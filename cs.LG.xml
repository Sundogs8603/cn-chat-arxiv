<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#24341;&#20837;&#21333;&#27493;&#25193;&#25955;&#27169;&#22411;&#24182;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#30446;&#26631;&#35843;&#25972;&#21040;&#26032;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29616;&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23616;&#38480;&#24615;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26080;&#37197;&#23545;&#21644;&#37197;&#23545;&#35774;&#32622;&#19979;&#30340;&#22270;&#20687;&#32763;&#35793;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.12036</link><description>&lt;p&gt;
&#19968;&#27493;&#22270;&#20687;&#32763;&#35793;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
One-Step Image Translation with Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12036
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21333;&#27493;&#25193;&#25955;&#27169;&#22411;&#24182;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#30446;&#26631;&#35843;&#25972;&#21040;&#26032;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29616;&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23616;&#38480;&#24615;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26080;&#37197;&#23545;&#21644;&#37197;&#23545;&#35774;&#32622;&#19979;&#30340;&#22270;&#20687;&#32763;&#35793;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#20004;&#20010;&#23616;&#38480;&#24615;&#65306;&#30001;&#20110;&#36845;&#20195;&#21435;&#22122;&#36807;&#31243;&#32780;&#23548;&#33268;&#30340;&#25512;&#26029;&#36895;&#24230;&#24930;&#20197;&#21450;&#23545;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#30340;&#20381;&#36182;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#30446;&#26631;&#23558;&#21333;&#27493;&#25193;&#25955;&#27169;&#22411;&#35843;&#25972;&#21040;&#26032;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21508;&#27169;&#22359;&#25972;&#21512;&#21040;&#19968;&#20010;&#20855;&#26377;&#23567;&#21487;&#35757;&#32451;&#26435;&#37325;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#22120;&#32593;&#32476;&#20013;&#65292;&#22686;&#24378;&#20102;&#20854;&#20445;&#30041;&#36755;&#20837;&#22270;&#20687;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#26080;&#37197;&#23545;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411; CycleGAN-Turbo &#22312;&#21508;&#31181;&#22330;&#26223;&#32763;&#35793;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#22914;&#26085;&#22812;&#36716;&#25442;&#20197;&#21450;&#28155;&#21152;/&#31227;&#38500;&#38654;&#12289;&#38634;&#21644;&#38632;&#31561;&#22825;&#27668;&#25928;&#26524;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#37197;&#23545;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411; pix2pix-Turbo &#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12036v1 Announce Type: cross  Abstract: In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning. To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives. Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29983;&#25104;&#22823;&#35268;&#27169;3D&#25968;&#25454;&#38598;&#30340;VFusion3D&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.12034</link><description>&lt;p&gt;
VFusion3D: &#20174;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20013;&#23398;&#20064;&#21487;&#25193;&#23637;&#30340;3D&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12034
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29983;&#25104;&#22823;&#35268;&#27169;3D&#25968;&#25454;&#38598;&#30340;VFusion3D&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;3D&#29983;&#25104;&#27169;&#22411;&#12290;&#26500;&#24314;&#22522;&#30784;3D&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;3D&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#19982;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#35270;&#39057;&#19981;&#21516;&#65292;3D&#25968;&#25454;&#19981;&#23481;&#26131;&#33719;&#21462;&#19988;&#38590;&#20197;&#33719;&#24471;&#65292;&#36825;&#23548;&#33268;&#19982;&#20854;&#20182;&#31867;&#22411;&#25968;&#25454;&#30340;&#25968;&#37327;&#30456;&#27604;&#23384;&#22312;&#26174;&#30528;&#30340;&#35268;&#27169;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#36890;&#36807;&#22823;&#37327;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;3D&#25968;&#25454;&#30340;&#30693;&#35782;&#28304;&#12290;&#36890;&#36807;&#24494;&#35843;&#35299;&#38145;&#20854;&#22810;&#35270;&#35282;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21069;&#39304;3D&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12034v1 Announce Type: cross  Abstract: This paper presents a novel paradigm for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compare
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2403.12031</link><description>&lt;p&gt;
ROUTERBENCH&#65306;&#29992;&#20110;&#22810;LLM&#36335;&#30001;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ROUTERBENCH: A Benchmark for Multi-LLM Routing System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#19981;&#26029;&#25193;&#22823;&#65292;&#23545;&#26377;&#25928;&#30340;&#26381;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#22810;&#26679;&#24615;&#65292;&#20294;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#26368;&#20248;&#22320;&#35299;&#20915;&#25152;&#26377;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#24179;&#34913;&#24615;&#33021;&#21644;&#25104;&#26412;&#20043;&#38388;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#38480;&#21046;&#65292;&#21457;&#23637;&#20102;LLM&#36335;&#30001;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#32467;&#21512;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#20811;&#26381;&#21333;&#20010;LLMs&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#22120;&#24615;&#33021;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#30340;&#21151;&#25928;&#65292;&#20197;&#21450;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#20195;&#34920;&#24615;LLMs&#30340;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36335;&#30001;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12031v1 Announce Type: cross  Abstract: As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and del
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22686;&#37327;&#31867;&#21035;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27599;&#20010;&#26032;&#20219;&#21153;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#27169;&#22359;&#26469;&#21019;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26356;&#26032;&#32780;&#19981;&#25439;&#23475;&#20808;&#21069;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.12030</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22686;&#37327;&#31867;&#21035;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#23376;&#31354;&#38388;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22686;&#37327;&#31867;&#21035;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27599;&#20010;&#26032;&#20219;&#21153;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#27169;&#22359;&#26469;&#21019;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26356;&#26032;&#32780;&#19981;&#25439;&#23475;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12030v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35201;&#27714;&#23398;&#20064;&#31995;&#32479;&#19981;&#26029;&#23398;&#20064;&#26032;&#31867;&#21035;&#32780;&#19981;&#20250;&#36951;&#24536;&#26087;&#30693;&#35782;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;CIL&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#23398;&#20064;&#26032;&#31867;&#21035;&#36890;&#24120;&#20250;&#23548;&#33268;&#26087;&#31867;&#21035;&#30340;&#35206;&#30422;&#12290;&#32593;&#32476;&#36807;&#24230;&#20462;&#25913;&#20250;&#23548;&#33268;&#36951;&#24536;&#65292;&#32780;&#26368;&#23567;&#35843;&#25972;&#20250;&#23548;&#33268;&#26032;&#31867;&#21035;&#25311;&#21512;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#24076;&#26395;&#25214;&#21040;&#19968;&#31181;&#26377;&#25928;&#30340;&#27169;&#22411;&#26356;&#26032;&#26041;&#24335;&#65292;&#26082;&#19981;&#25439;&#23475;&#20808;&#21069;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22522;&#20110;PTM&#30340;CIL&#30340;Extended Subspace Ensemble&#65288;EASE&#65289;&#12290;&#20026;&#20102;&#20351;&#27169;&#22411;&#26356;&#26032;&#19981;&#20914;&#31361;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#29420;&#29305;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#26088;&#22312;&#21019;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23376;&#31354;&#38388;&#12290;&#36825;&#20123;&#36866;&#37197;&#22120;&#36328;&#36234;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#36328;&#22810;&#20010;&#23376;&#31354;&#38388;&#30340;&#32852;&#21512;&#20915;&#31574;&#12290;&#38543;&#30528;&#25968;&#25454;&#30340;&#28436;&#21464;&#65292;&#19981;&#26029;&#25193;&#23637;&#30340;&#23376;&#31354;&#38388;&#20351;&#26087;&#31867;&#21035;&#20998;&#31867;&#22120;&#19982;&#26032;&#31867;&#21035;&#19981;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12030v1 Announce Type: cross  Abstract: Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones. Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes. As a result, it is desired to figure out a way of efficient model updating without harming former knowledge. In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces. These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces. As data evolves, the expanding subspaces render the old class classifiers incompatible with new
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12026</link><description>&lt;p&gt;
FlexCap&#65306;&#22312;&#22270;&#20687;&#20013;&#29983;&#25104;&#20016;&#23500;&#12289;&#26412;&#22320;&#21270;&#21644;&#28789;&#27963;&#30340;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
FlexCap: Generating Rich, Localized, and Flexible Captions in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12026
&lt;/p&gt;
&lt;p&gt;
FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;$\textit{&#28789;&#27963;&#23383;&#24149;}$&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#38271;&#24230;&#19981;&#21516;&#30340;&#29305;&#23450;&#21306;&#22495;&#25551;&#36848;&#12290;&#35813;&#27169;&#22411;FlexCap&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20026;&#36755;&#20837;&#30340;&#36793;&#30028;&#26694;&#29983;&#25104;&#38271;&#24230;&#26465;&#20214;&#30340;&#23383;&#24149;&#65292;&#20174;&#32780;&#21487;&#20197;&#25511;&#21046;&#20854;&#36755;&#20986;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#25551;&#36848;&#33539;&#22260;&#20174;&#31616;&#27905;&#30340;&#23545;&#35937;&#26631;&#31614;&#21040;&#35814;&#32454;&#30340;&#23383;&#24149;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#24102;&#23383;&#24149;&#30340;&#22270;&#20687;&#24320;&#22987;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#21306;&#22495;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#23383;&#24149;&#21151;&#33021;&#26377;&#20960;&#20010;&#23453;&#36149;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;FlexCap&#22312;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;FlexCap&#29983;&#25104;&#26412;&#22320;&#21270;&#25551;&#36848;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#26469;&#26500;&#24314;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#31995;&#32479;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#22312;&#35768;&#22810;VQ&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#20197;&#21450;EquityMedQA&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.12025</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#20197;&#21450;EquityMedQA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#30528;&#20026;&#22797;&#26434;&#30340;&#20581;&#24247;&#20449;&#24687;&#38656;&#27714;&#25552;&#20379;&#26381;&#21153;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#26377;&#21487;&#33021;&#24341;&#20837;&#21361;&#23475;&#24182;&#21152;&#21095;&#20581;&#24247;&#19981;&#24179;&#31561;&#12290;&#21487;&#38752;&#22320;&#35780;&#20272;&#19982;&#20844;&#24179;&#30456;&#20851;&#30340;&#27169;&#22411;&#22833;&#28789;&#26159;&#21457;&#23637;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#21487;&#33021;&#23548;&#33268;LLM&#29983;&#25104;&#30340;&#38271;&#31687;&#31572;&#26696;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#21361;&#23475;&#30340;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Med-PaLM 2&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22312;&#35813;&#39046;&#22495;&#36827;&#34892;&#30340;&#26368;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#65292;&#20197;&#21450;EquityMedQA&#65292;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#26032;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#65292;&#20854;&#20013;&#26082;&#21253;&#25324;&#25163;&#21160;&#31574;&#21010;&#21448;&#21253;&#25324;LLM&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20016;&#23500;&#20102;&#23545;&#25239;&#24615;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#35774;&#35745;&#36807;&#31243;&#37117;&#26681;&#26893;&#20110;&#23454;&#38469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12025v1 Announce Type: cross  Abstract: Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries. Both our human assessment framework and dataset design process are grounde
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#35265;&#35299;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#23545;&#40784;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.12017</link><description>&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#20316;&#20026;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Fine-Tuning as Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#35265;&#35299;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#23545;&#40784;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#40784;&#30340;&#20027;&#27969;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#31867;&#25110;AI&#21453;&#39304;&#65292;&#24182;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#29305;&#23450;&#31867;&#22411;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#21508;&#31181;&#24773;&#26223;&#19979;&#19982;&#19987;&#23478;&#28436;&#31034;&#23545;&#40784;&#26356;&#20026;&#29616;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#26694;&#26550;&#65292;&#20197;&#28436;&#31034;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#26469;&#35268;&#21010;&#23545;&#40784;LLMs&#30340;&#38382;&#39064;&#12290;&#20511;&#37492;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;LLM&#23545;&#40784;&#20219;&#21153;&#20013;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#19981;&#21516;&#26041;&#27861;&#30340;&#35206;&#30422;&#29575;&#21644;&#23547;&#25214;&#27169;&#24335;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#32463;&#20856;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#30340;&#21033;&#24330;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19981;&#21516;&#26041;&#27861;&#34920;&#29616;&#31361;&#20986;&#30340;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12017v1 Announce Type: cross  Abstract: The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.
&lt;/p&gt;</description></item><item><title>EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12014</link><description>&lt;p&gt;
EnvGen: &#36890;&#36807;LLMs&#29983;&#25104;&#21644;&#35843;&#25972;&#29615;&#22659;&#20197;&#35757;&#32451;&#20855;&#36523;&#20307;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12014
&lt;/p&gt;
&lt;p&gt;
EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#36890;&#36807;&#20114;&#21160;&#36827;&#34892;&#20855;&#36523;&#20307;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#30452;&#25509;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#65292;&#20197;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#19979;&#19968;&#27493;&#12290;LLM&#20195;&#29702;&#30001;&#20110;&#20854;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20197;&#24448;&#36739;&#23567;&#30340;&#20195;&#29702;&#34920;&#29616;&#26356;&#24378;&#65307;&#20294;&#39057;&#32321;&#35843;&#29992;LLMs&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;EnvGen&#65292;&#19968;&#20010;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#31034;&#19968;&#20010;LLM&#29983;&#25104;&#35757;&#32451;&#29615;&#22659;&#65292;&#20351;&#20195;&#29702;&#21487;&#20197;&#24555;&#36895;&#24182;&#34892;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#33719;&#24471;&#20219;&#21153;&#25551;&#36848;&#21644;&#27169;&#25311;&#22120;&#30446;&#26631;&#65292;&#28982;&#21518;&#34987;&#35201;&#27714;&#29983;&#25104;&#19968;&#32452;&#29615;&#22659;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12014v1 Announce Type: cross  Abstract: Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Lie&#32676;&#30340;&#21160;&#21147;&#23398;Langevin Monte Carlo&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21644;&#31934;&#32454;&#31163;&#25955;&#21270;&#23454;&#29616;&#20102;Lie&#32676;&#32467;&#26500;&#30340;&#20445;&#25345;&#65292;&#24182;&#22312;W2&#36317;&#31163;&#19979;&#35777;&#26126;&#20102;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#31163;&#25955;&#37319;&#26679;&#22120;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12012</link><description>&lt;p&gt;
&#22522;&#20110;Lie&#32676;&#30340;&#21160;&#21147;&#23398;Langevin Monte Carlo&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of Kinetic Langevin Monte Carlo on Lie groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Lie&#32676;&#30340;&#21160;&#21147;&#23398;Langevin Monte Carlo&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21644;&#31934;&#32454;&#31163;&#25955;&#21270;&#23454;&#29616;&#20102;Lie&#32676;&#32467;&#26500;&#30340;&#20445;&#25345;&#65292;&#24182;&#22312;W2&#36317;&#31163;&#19979;&#35777;&#26126;&#20102;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#31163;&#25955;&#37319;&#26679;&#22120;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#21464;&#20998;&#20248;&#21270;&#21644;&#24038;&#24179;&#20961;&#21270;&#31561;&#25216;&#26415;&#26500;&#24314;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#12289;&#22522;&#20110;&#21160;&#37327;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#29992;&#20110;&#20248;&#21270;&#23450;&#20041;&#22312;Lie&#32676;&#19978;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36866;&#24403;&#22320;&#20026;&#20248;&#21270;&#21160;&#21147;&#23398;&#28155;&#21152;&#21487;&#22788;&#29702;&#30340;&#22122;&#22768;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#37319;&#26679;&#21160;&#21147;&#23398;&#65292;&#21033;&#29992;&#21160;&#37327;&#21464;&#37327;&#26159;&#27431;&#20960;&#37324;&#24471;&#30340;&#36825;&#19968;&#26377;&#21033;&#29305;&#24615;&#65292;&#23613;&#31649;&#28508;&#22312;&#20989;&#25968;&#23384;&#22312;&#20110;&#27969;&#24418;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#31163;&#25955;&#21270;&#23548;&#33268;&#30340;&#21160;&#21147;&#23398;&#37319;&#26679;&#21160;&#21147;&#23398;&#25552;&#20986;&#20102;&#19968;&#20010;Lie&#32676;MCMC&#37319;&#26679;&#22120;&#12290;&#36825;&#31181;&#31163;&#25955;&#21270;&#23436;&#20840;&#20445;&#25345;&#20102;Lie&#32676;&#32467;&#26500;&#12290;&#22312;W2&#36317;&#31163;&#19979;&#65292;&#20998;&#21035;&#23545;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#31163;&#25955;&#37319;&#26679;&#22120;&#35777;&#26126;&#20102;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#20854;&#20013;&#21482;&#38656;&#35201;Lie&#32676;&#30340;&#32039;&#33268;&#24615;&#21644;&#28508;&#22312;&#20989;&#25968;&#30340;&#27979;&#22320;L-&#20809;&#28369;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;&#21160;&#21147;&#23398;Langevin&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12012v1 Announce Type: cross  Abstract: Explicit, momentum-based dynamics for optimizing functions defined on Lie groups was recently constructed, based on techniques such as variational optimization and left trivialization. We appropriately add tractable noise to the optimization dynamics to turn it into a sampling dynamics, leveraging the advantageous feature that the momentum variable is Euclidean despite that the potential function lives on a manifold. We then propose a Lie-group MCMC sampler, by delicately discretizing the resulting kinetic-Langevin-type sampling dynamics. The Lie group structure is exactly preserved by this discretization. Exponential convergence with explicit convergence rate for both the continuous dynamics and the discrete sampler are then proved under W2 distance. Only compactness of the Lie group and geodesically L-smoothness of the potential function are needed. To the best of our knowledge, this is the first convergence result for kinetic Langev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#36890;&#36807;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#25903;&#25345;&#30284;&#30151;&#24739;&#32773;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;&#65292;&#21457;&#29616;&#21307;&#29983;&#22788;&#26041;&#26174;&#33879;&#22686;&#21152;&#24739;&#32773;&#23545;&#31227;&#21160;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#30340;&#25345;&#32493;&#21442;&#19982;&#65292;&#21516;&#26102;&#25351;&#20986;&#27599;&#21608;&#21442;&#19982;&#19968;&#27425;&#24050;&#36275;&#20197;&#32500;&#25345;&#31119;&#31049;&#65292;&#20294;&#20869;&#22312;&#21160;&#26426;&#21487;&#33021;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#12290;</title><link>https://arxiv.org/abs/2403.12007</link><description>&lt;p&gt;
&#30830;&#23450;&#36890;&#36807;&#31227;&#21160;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#25552;&#39640;&#30284;&#30151;&#24739;&#32773;&#31119;&#31049;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#36890;&#36807;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#25903;&#25345;&#30284;&#30151;&#24739;&#32773;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;&#65292;&#21457;&#29616;&#21307;&#29983;&#22788;&#26041;&#26174;&#33879;&#22686;&#21152;&#24739;&#32773;&#23545;&#31227;&#21160;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#30340;&#25345;&#32493;&#21442;&#19982;&#65292;&#21516;&#26102;&#25351;&#20986;&#27599;&#21608;&#21442;&#19982;&#19968;&#27425;&#24050;&#36275;&#20197;&#32500;&#25345;&#31119;&#31049;&#65292;&#20294;&#20869;&#22312;&#21160;&#26426;&#21487;&#33021;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#65288;DBCI&#65289;&#27491;&#22312;&#25903;&#25345;&#26032;&#20581;&#24247;&#34892;&#20026;&#30340;&#21457;&#23637;&#12290;&#35780;&#20272;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#23545;&#20110;&#25913;&#36827;&#23427;&#20204;&#21644;&#29702;&#35299;&#25104;&#21151;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29305;&#21035;&#26159;&#22312;&#21463;&#20262;&#29702;&#38480;&#21046;&#30340;&#23567;&#35268;&#27169;&#30740;&#31350;&#20013;&#65292;&#24320;&#21457;&#32773;&#30340;&#20840;&#38754;&#25351;&#23548;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;CAPABLE&#39033;&#30446;&#65292;&#26088;&#22312;&#23450;&#20041;&#36890;&#36807;DBCI&#25903;&#25345;&#30284;&#30151;&#24739;&#32773;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#34913;&#37327;&#21442;&#19982;&#24230;&#30340;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#24739;&#32773;&#21644;&#20020;&#24202;&#21307;&#29983;&#23545;DBCI&#30340;&#20852;&#36259;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#35780;&#20272;DBCI&#24433;&#21709;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21307;&#29983;&#30340;&#22788;&#26041;&#26174;&#30528;&#22686;&#21152;&#20102;&#24739;&#32773;&#23545;&#31227;&#21160;DBCI&#30340;&#25345;&#32493;&#21442;&#19982;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#27599;&#21608;&#19968;&#27425;&#21442;&#19982;DBCI&#23601;&#36275;&#20197;&#32500;&#25345;&#31119;&#31049;&#65292;&#20294;&#20174;&#22806;&#22312;&#21160;&#26426;&#21521;&#20869;&#22312;&#21160;&#26426;&#30340;&#36716;&#21464;&#21487;&#33021;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12007v1 Announce Type: cross  Abstract: Digital Behavior Change Interventions (DBCIs) are supporting development of new health behaviors. Evaluating their effectiveness is crucial for their improvement and understanding of success factors. However, comprehensive guidance for developers, particularly in small-scale studies with ethical constraints, is limited. Building on the CAPABLE project, this study aims to define effective engagement with DBCIs for supporting cancer patients in enhancing their quality of life. We identify metrics for measuring engagement, explore the interest of both patients and clinicians in DBCIs, and propose hypotheses for assessing the impact of DBCIs in such contexts. Our findings suggest that clinician prescriptions significantly increase sustained engagement with mobile DBCIs. In addition, while one weekly engagement with a DBCI is sufficient to maintain well-being, transitioning from extrinsic to intrinsic motivation may require a higher level o
&lt;/p&gt;</description></item><item><title>2023&#24180;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35270;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20026;&#21508;&#31181;&#39046;&#22495;&#25552;&#20379;&#20102;&#36235;&#21183;&#12289;&#35265;&#35299;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12005</link><description>&lt;p&gt;
2023&#24180;&#26426;&#22120;&#23398;&#20064;&#20013;&#20449;&#20219;&#21487;&#35270;&#21270;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Visualization for Trust in Machine Learning Revisited: The State of the Field in 2023
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12005
&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35270;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20026;&#21508;&#31181;&#39046;&#22495;&#25552;&#20379;&#20102;&#36235;&#21183;&#12289;&#35265;&#35299;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35270;&#21270;&#20173;&#28982;&#26159;&#20449;&#24687;&#21487;&#35270;&#21270;&#21644;&#35270;&#35273;&#20998;&#26512;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#21644;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#28041;&#21450;&#21307;&#23398;&#12289;&#37329;&#34701;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#25105;&#20204;2020&#24180;&#30340;&#26368;&#26032;&#25253;&#21578;&#20013;&#65292;&#21253;&#25324;&#20102;200&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#22362;&#25345;&#25910;&#38598;&#21516;&#34892;&#35780;&#23457;&#30340;&#25991;&#31456;&#65292;&#25551;&#36848;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#26681;&#25454;&#20808;&#21069;&#24314;&#31435;&#30340;&#21253;&#21547;119&#20010;&#31867;&#21035;&#30340;&#20998;&#31867;&#27169;&#24335;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#22312;&#32447;&#35843;&#26597;&#27983;&#35272;&#22120;&#20013;&#25552;&#20379;&#20102;542&#31181;&#25216;&#26415;&#30340;&#32467;&#26524;&#38598;&#12290;&#22312;&#26412;&#35843;&#26597;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25130;&#33267;2023&#24180;&#31179;&#23395;&#20851;&#20110;&#36825;&#19968;&#25968;&#25454;&#38598;&#30340;&#26032;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#21487;&#35270;&#21270;&#30340;&#36235;&#21183;&#12289;&#35265;&#35299;&#21644;&#20843;&#20010;&#24320;&#25918;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;&#21487;&#35270;&#21270;&#25216;&#26415;&#22312;&#22686;&#21152;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20449;&#20219;&#26041;&#38754;&#21576;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12005v1 Announce Type: cross  Abstract: Visualization for explainable and trustworthy machine learning remains one of the most important and heavily researched fields within information visualization and visual analytics with various application domains, such as medicine, finance, and bioinformatics. After our 2020 state-of-the-art report comprising 200 techniques, we have persistently collected peer-reviewed articles describing visualization techniques, categorized them based on the previously established categorization schema consisting of 119 categories, and provided the resulting collection of 542 techniques in an online survey browser. In this survey article, we present the updated findings of new analyses of this dataset as of fall 2023 and discuss trends, insights, and eight open challenges for using visualizations in machine learning. Our results corroborate the rapidly growing trend of visualization techniques for increasing trust in machine learning models in the p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26426;&#26800;&#20027;&#20041;&#21644;&#21151;&#33021;&#20027;&#20041;&#20004;&#31181;&#26041;&#27861;&#20197;&#23398;&#20064;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#26435;&#37325;&#30340;&#26377;&#29992;&#34920;&#31034;&#65292;&#24182;&#21457;&#23637;&#20102;&#26694;&#26550;&#26469;&#29983;&#25104;&#26377;&#21161;&#20110;&#30830;&#23450;RNN&#34892;&#20026;&#30340;&#20016;&#23500;&#34920;&#31034;</title><link>https://arxiv.org/abs/2403.11998</link><description>&lt;p&gt;
&#23398;&#20064;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30697;&#38453;&#30340;&#26377;&#29992;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Useful Representations of Recurrent Neural Network Weight Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26426;&#26800;&#20027;&#20041;&#21644;&#21151;&#33021;&#20027;&#20041;&#20004;&#31181;&#26041;&#27861;&#20197;&#23398;&#20064;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#26435;&#37325;&#30340;&#26377;&#29992;&#34920;&#31034;&#65292;&#24182;&#21457;&#23637;&#20102;&#26694;&#26550;&#26469;&#29983;&#25104;&#26377;&#21161;&#20110;&#30830;&#23450;RNN&#34892;&#20026;&#30340;&#20016;&#23500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNNs)&#26159;&#36890;&#29992;&#30340;&#24182;&#34892;&#20018;&#34892;&#35745;&#31639;&#26426;&#12290; RNN&#30340;&#31243;&#24207;&#26159;&#20854;&#26435;&#37325;&#30697;&#38453;&#12290; &#22914;&#20309;&#23398;&#20064;&#26377;&#21161;&#20110;RNN&#20998;&#26512;&#20197;&#21450;&#19979;&#28216;&#20219;&#21153;&#30340;RNN&#26435;&#37325;&#30340;&#26377;&#29992;&#34920;&#31034;&#65311; &#23613;&#31649;&#26426;&#26800;&#20027;&#20041;&#26041;&#27861;&#30452;&#25509;&#26597;&#30475;&#19968;&#20123;RNN&#30340;&#26435;&#37325;&#26469;&#39044;&#27979;&#20854;&#34892;&#20026;&#65292;&#21151;&#33021;&#20027;&#20041;&#26041;&#27861;&#20998;&#26512;&#20854;&#25972;&#20307;&#21151;&#33021;--&#20855;&#20307;&#26469;&#35828;&#26159;&#20854;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#12290; &#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#36866;&#29992;&#20110;RNN&#26435;&#37325;&#30340;&#26426;&#26800;&#20027;&#20041;&#26041;&#27861;&#65292;&#24182;&#20026;RNN&#24341;&#20837;&#20102;&#32622;&#25442;&#31561;&#21464;&#30340;&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#23618;&#12290;&#25105;&#20204;&#30340;&#20004;&#31181;&#26032;&#39062;&#30340;&#21151;&#33021;&#20027;&#20041;&#26041;&#27861;&#36890;&#36807;&#8220;&#35810;&#38382;&#8221;&#36755;&#20837;&#32780;&#20174;RNN&#26435;&#37325;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21151;&#33021;&#20027;&#20041;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#21161;&#20110;&#30830;&#23450;RNN&#34892;&#20026;&#30340;&#20016;&#23500;&#34920;&#31034;&#30340;&#26465;&#20214;&#12290; &#25105;&#20204;&#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#20004;&#20010;&#8220;&#27169;&#22411;&#21160;&#29289;&#22253;&#8221;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;RNN&#26435;&#37325;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11998v1 Announce Type: new  Abstract: Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.11996</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#30693;&#35782;&#25552;&#21462;&#12289;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#26234;&#33021;&#22270;&#25512;&#29702;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11996
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#23558;&#19968;&#32452;&#28041;&#21450;&#29983;&#29289;&#26448;&#26009;&#39046;&#22495;&#30340;1,000&#31687;&#31185;&#23398;&#35770;&#25991;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26412;&#20307;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24615;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#32452;&#21512;&#25490;&#21517;&#65292;&#25506;&#27979;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#22270;&#36941;&#21382;&#36335;&#24452;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#20837;&#30340;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26597;&#35810;&#65292;&#35782;&#21035;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#26448;&#26009;&#35774;&#35745;&#21450;&#20854;&#34892;&#20026;&#12290;&#19968;&#39033;&#27604;&#36739;&#25581;&#31034;&#20102;&#29983;&#29289;&#26448;&#26009;&#21644;&#36125;&#22810;&#33452;&#31532;&#20061;&#20132;&#21709;&#26354;&#20043;&#38388;&#30340;&#35814;&#32454;&#32467;&#26500;&#30456;&#20284;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#36890;&#36807;&#21516;&#26500;&#26144;&#23556;&#20849;&#20139;&#22797;&#26434;&#24615;&#27169;&#24335;&#12290;&#35813;&#31639;&#27861;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20998;&#32423;&#33740;&#19997;&#20307;&#30340;&#22797;&#21512;&#26448;&#26009;&#65292;&#23558;&#22270;&#37319;&#26679;&#30340;&#32852;&#21512;&#21512;&#25104;&#21407;&#29702;&#19982;&#24247;&#23450;&#26031;&#22522;&#12298;&#31532;&#19971;&#32452;&#25104;&#12299;&#20013;&#25552;&#21462;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#21435;&#22122;&#20316;&#20026;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#30340;&#35748;&#35777;&#38450;&#24481;&#65292;&#33021;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#21040;0-16%&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#24433;&#21709;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#21644;&#21033;&#29992;&#35813;&#38450;&#24481;&#25514;&#26045;&#20316;&#20026;&#24378;&#26377;&#21147;&#22522;&#30784;&#25552;&#20379;&#20102;&#37325;&#35201;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.11981</link><description>&lt;p&gt;
&#25193;&#25955;&#21435;&#22122;&#20316;&#20026;&#19968;&#31181;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#30340;&#35748;&#35777;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Diffusion Denoising as a Certified Defense against Clean-label Poisoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11981
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#21435;&#22122;&#20316;&#20026;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#30340;&#35748;&#35777;&#38450;&#24481;&#65292;&#33021;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#21040;0-16%&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#24433;&#21709;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#21644;&#21033;&#29992;&#35813;&#38450;&#24481;&#25514;&#26045;&#20316;&#20026;&#24378;&#26377;&#21147;&#22522;&#30784;&#25552;&#20379;&#20102;&#37325;&#35201;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#25915;&#20987;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#23569;&#37327;&#30340;&#27602;&#23475;&#26679;&#26412;&#65288;&#20363;&#22914;1%&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;$p$-&#33539;&#25968;&#21463;&#38480;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#20174;&#32780;&#35825;&#23548;&#23545;&#27979;&#35797;&#36755;&#20837;&#30340;&#30446;&#26631;&#35823;&#20998;&#31867;&#12290;&#21463;&#21040;$&#21435;&#22122;&#24179;&#28369;$&#23454;&#29616;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#23545;&#31713;&#25913;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#28040;&#27602;&#12290;&#25105;&#20204;&#24191;&#27867;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#38450;&#24481;&#25514;&#26045;&#23545;&#19971;&#31181;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#25252;&#25928;&#26524;&#65292;&#24182;&#19988;&#23558;&#23427;&#20204;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#21040;0-16%&#65292;&#21516;&#26102;&#27979;&#35797;&#20934;&#30830;&#24615;&#20960;&#20046;&#27809;&#26377;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#38450;&#24481;&#19982;&#29616;&#26377;&#30340;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#30340;&#23545;&#31574;&#36827;&#34892;&#27604;&#36739;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#38450;&#24481;&#25928;&#26524;&#26368;&#20339;&#65292;&#24182;&#25552;&#20379;&#26368;&#20339;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#26410;&#26469;&#38656;&#35201;&#24320;&#23637;&#26356;&#24378;&#22823;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#35748;&#35777;&#20294;&#23454;&#29992;&#30340;&#38450;&#24481;&#20316;&#20026;&#31283;&#22266;&#22522;&#30784;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11981v1 Announce Type: cross  Abstract: We present a certified defense to clean-label poisoning attacks. These attacks work by injecting a small number of poisoning samples (e.g., 1%) that contain $p$-norm bounded adversarial perturbations into the training data to induce a targeted misclassification of a test-time input. Inspired by the adversarial robustness achieved by $denoised$ $smoothing$, we show how an off-the-shelf diffusion model can sanitize the tampered training data. We extensively test our defense against seven clean-label poisoning attacks and reduce their attack success to 0-16% with only a negligible drop in the test time accuracy. We compare our defense with existing countermeasures against clean-label poisoning, showing that the defense reduces the attack success the most and offers the best model utility. Our results highlight the need for future work on developing stronger clean-label attacks and using our certified yet practical defense as a strong base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#23574;&#38160;&#32479;&#35745;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#24179;&#28369;&#24230;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#39062;&#30340;&#25193;&#25955;&#27888;&#21202;&#36924;&#36817;&#25216;&#26415;&#22312;&#29702;&#35770;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11968</link><description>&lt;p&gt;
&#25581;&#31034;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#20010;&#23574;&#38160;&#30340;&#32479;&#35745;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#23574;&#38160;&#32479;&#35745;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#24179;&#28369;&#24230;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#39062;&#30340;&#25193;&#25955;&#27888;&#21202;&#36924;&#36817;&#25216;&#26415;&#22312;&#29702;&#35770;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11968v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#20855;&#26377;&#20998;&#31867;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#29616;&#20195;&#22270;&#20687;&#21512;&#25104;&#30340;&#22522;&#30784;&#65292;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#21508;&#31181;&#26465;&#20214;&#20449;&#24687;&#65292;&#22914;&#25552;&#31034;&#36755;&#20837;&#65292;&#20197;&#24341;&#23548;&#26679;&#26412;&#29983;&#25104;&#21040;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290;&#23613;&#31649;&#32463;&#39564;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#29702;&#35770;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#19981;&#23436;&#22791;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#24067;&#20272;&#35745;&#30340;&#23574;&#38160;&#32479;&#35745;&#29702;&#35770;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#19968;&#20010;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#24179;&#28369;&#24230;&#24182;&#21305;&#37197;&#26497;&#23567;&#26497;&#20540;&#19979;&#38480;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#25105;&#20204;&#29702;&#35770;&#21457;&#23637;&#30340;&#20851;&#38190;&#22312;&#20110;&#26465;&#20214;&#35780;&#20998;&#20989;&#25968;&#30340;&#36924;&#36817;&#32467;&#26524;&#65292;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27888;&#21202;&#36924;&#36817;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32479;&#35745;&#29702;&#35770;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11968v1 Announce Type: new  Abstract: Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning. In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties. Despite the empirical success, theory of conditional diffusion models is largely missing. This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models. Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound. The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique. Moreover, we demonstrate the utility of our statistical theory in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#30693;&#24335;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.11966</link><description>&lt;p&gt;
&#36890;&#30693;&#35889;&#24402;&#19968;&#21270;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Informed Spectral Normalized Gaussian Processes for Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11966
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#30693;&#24335;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#21442;&#25968;&#20998;&#24067;&#20026;&#36890;&#30693;&#24335;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#24335;&#65292;&#20197;&#34920;&#31034;&#20808;&#39564;&#19987;&#23478;&#21644;&#19990;&#30028;&#30693;&#35782;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#26679;&#30340;&#20449;&#24687;&#20808;&#39564;&#26469;&#35268;&#33539;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20250;&#22686;&#21152;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#29575;DL&#27169;&#22411;&#30340;&#24120;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36817;&#20284;&#26041;&#27861;&#21487;&#33021;&#35745;&#31639;&#26114;&#36149;&#65292;&#38656;&#35201;&#22810;&#27425;&#25512;&#29702;&#21644;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#35745;&#31639;&#39640;&#25928;&#30340;&#26368;&#21518;&#19968;&#23618;&#26680;&#36924;&#36817;&#22914;&#35889;&#24402;&#19968;&#21270;&#39640;&#26031;&#36807;&#31243;&#65288;SNGPs&#65289;&#26159;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;SNGPs&#30340;&#22522;&#20110;&#26032;&#39062;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#20195;&#34920;&#20808;&#21069;&#20219;&#21153;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#36890;&#30693;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#24314;&#31435;&#22312;&#25104;&#29087;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#19981;&#38656;&#35201;&#35760;&#24518;&#25110;&#21442;&#25968;&#25193;&#23637;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#36890;&#30693;SNGP&#27169;&#22411;&#24212;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11966v1 Announce Type: cross  Abstract: Prior parameter distributions provide an elegant way to represent prior expert and world knowledge for informed learning. Previous work has shown that using such informative priors to regularize probabilistic deep learning (DL) models increases their performance and data-efficiency. However, commonly used sampling-based approximations for probabilistic DL models can be computationally expensive, requiring multiple inference passes and longer training times. Promising alternatives are compute-efficient last layer kernel approximations like spectral normalized Gaussian processes (SNGPs). We propose a novel regularization-based continual learning method for SNGPs, which enables the use of informative priors that represent prior knowledge learned from previous tasks. Our proposal builds upon well-established methods and requires no rehearsal memory or parameter expansion. We apply our informed SNGP model to the trajectory prediction proble
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Quantile Recalibration Training&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#65292;&#23558;&#21518;&#22788;&#29702;&#26657;&#20934;&#30452;&#25509;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#65292;&#23637;&#31034;&#20986;&#22312;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#20013;&#25552;&#39640;&#26657;&#20934;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11964</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#30340;&#35774;&#35745;&#27010;&#29575;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Calibration by Design for Neural Network Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Quantile Recalibration Training&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#65292;&#23558;&#21518;&#22788;&#29702;&#26657;&#20934;&#30452;&#25509;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#65292;&#23637;&#31034;&#20986;&#22312;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#20013;&#25552;&#39640;&#26657;&#20934;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36827;&#34892;&#26368;&#20339;&#20915;&#31574;&#65292;&#20026;&#22238;&#24402;&#38382;&#39064;&#29983;&#25104;&#32463;&#36807;&#26657;&#20934;&#19988;&#31934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20998;&#24067;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#25913;&#21892;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#35757;&#32451;&#21518;&#35843;&#25972;&#39044;&#27979;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#21644;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#25805;&#20316;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#34429;&#28982;&#19982;&#27491;&#21017;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;&#21518;&#22788;&#29702;&#26041;&#27861;&#22312;&#26657;&#20934;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25913;&#36827;&#65292;&#20294;&#21518;&#22788;&#29702;&#27493;&#39588;&#19982;&#27169;&#22411;&#35757;&#32451;&#23436;&#20840;&#29420;&#31435;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Quantile Recalibration Training&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#65292;&#23558;&#21518;&#22788;&#29702;&#26657;&#20934;&#30452;&#25509;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31639;&#27861;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#20854;&#20182;&#21518;&#22788;&#29702;&#26041;&#27861;&#20197;&#21450;&#27491;&#21017;&#21270;&#26041;&#27861;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#21253;&#21547;&#22312;&#20869;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11964v1 Announce Type: new  Abstract: Generating calibrated and sharp neural network predictive distributions for regression problems is essential for optimal decision-making in many real-world applications. To address the miscalibration issue of neural networks, various methods have been proposed to improve calibration, including post-hoc methods that adjust predictions after training and regularization methods that act during training. While post-hoc methods have shown better improvement in calibration compared to regularization methods, the post-hoc step is completely independent of model training. We introduce a novel end-to-end model training procedure called Quantile Recalibration Training, integrating post-hoc calibration directly into the training process without additional parameters. We also present a unified algorithm that includes our method and other post-hoc and regularization methods, as particular cases. We demonstrate the performance of our method in a large
&lt;/p&gt;</description></item><item><title>&#20302;&#27425;&#22810;&#39033;&#24335;&#20272;&#35745;&#31867;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#22312;&#38750;&#24120;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#20302;&#27425;&#22810;&#39033;&#24335;&#26469;&#35828;&#38750;&#24179;&#20961;&#30340;&#36801;&#31227;&#23398;&#20064;&#26159;&#21487;&#33021;&#30340;&#65292;&#36229;&#36234;&#20102;$dQ/dP$&#26377;&#30028;&#30340;&#32463;&#20856;&#20551;&#35774;</title><link>https://arxiv.org/abs/2403.11963</link><description>&lt;p&gt;
&#36229;&#36234;&#26377;&#30028;&#23494;&#24230;&#27604;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning Beyond Bounded Density Ratios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11963
&lt;/p&gt;
&lt;p&gt;
&#20302;&#27425;&#22810;&#39033;&#24335;&#20272;&#35745;&#31867;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#22312;&#38750;&#24120;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#20302;&#27425;&#22810;&#39033;&#24335;&#26469;&#35828;&#38750;&#24179;&#20961;&#30340;&#36801;&#31227;&#23398;&#20064;&#26159;&#21487;&#33021;&#30340;&#65292;&#36229;&#36234;&#20102;$dQ/dP$&#26377;&#30028;&#30340;&#32463;&#20856;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36801;&#31227;&#23398;&#20064;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#23398;&#20064;&#31639;&#27861;&#20174;&#26576;&#20010;&#28304;&#20998;&#24067;$P$&#25910;&#38598;&#25968;&#25454;&#65292;&#20294;&#38656;&#35201;&#22312;&#19981;&#21516;&#30340;&#30446;&#26631;&#20998;&#24067;$Q$&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#26631;&#20934;&#30340;&#27979;&#24230;&#21464;&#25442;&#35770;&#35777;&#34920;&#26126;&#65292;&#24403;&#23494;&#24230;&#27604;$dQ/dP$&#26377;&#30028;&#26102;&#21457;&#29983;&#36801;&#31227;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;Kpotufe&#21644;Martinet(2018&#24180;COLT)&#20197;&#21450;Hanneke&#21644;Kpotufe(2019&#24180;NeurIPS)&#20043;&#21069;&#24341;&#20154;&#28145;&#24605;&#30340;&#20316;&#21697;&#23637;&#31034;&#20102;&#19968;&#20123;&#24773;&#20917;&#65292;&#20854;&#20013;&#27604;&#29575;$dQ/dP$&#26159;&#26080;&#30028;&#30340;&#65292;&#20294;&#36801;&#31227;&#23398;&#20064;&#26159;&#21487;&#33021;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#20302;&#27425;&#22810;&#39033;&#24335;&#20272;&#35745;&#31867;&#19978;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#22312;&#23450;&#20041;&#22495;$\mathbb{R}^n$&#19978;&#30340;&#19968;&#33324;&#36801;&#31227;&#19981;&#31561;&#24335;&#65292;&#35777;&#26126;&#20102;&#22312;&#38750;&#24120;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#20302;&#27425;&#22810;&#39033;&#24335;&#26469;&#35828;&#38750;&#24179;&#20961;&#30340;&#36801;&#31227;&#23398;&#20064;&#26159;&#21487;&#33021;&#30340;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;$dQ/dP$&#34987;&#26377;&#30028;&#30340;&#32463;&#20856;&#20551;&#35774;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;$Q$&#26159;&#23545;&#25968;&#20985;&#27979;&#24230;&#65292;&#21017;&#22987;&#32456;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11963v1 Announce Type: new  Abstract: We study the fundamental problem of transfer learning where a learning algorithm collects data from some source distribution $P$ but needs to perform well with respect to a different target distribution $Q$. A standard change of measure argument implies that transfer learning happens when the density ratio $dQ/dP$ is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet (COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where the ratio $dQ/dP$ is unbounded, but transfer learning is possible.   In this work, we focus on transfer learning over the class of low-degree polynomial estimators. Our main result is a general transfer inequality over the domain $\mathbb{R}^n$, proving that non-trivial transfer learning for low-degree polynomials is possible under very mild assumptions, going well beyond the classical assumption that $dQ/dP$ is bounded. For instance, it always applies if $Q$ is a log-concave measur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36816;&#21160;&#34917;&#20607;&#26469;&#22686;&#24378;&#37325;&#24314;&#36136;&#37327;&#65292;&#25552;&#20986;&#23558;&#36755;&#20837;&#24103;&#21644;&#31232;&#30095;&#32534;&#30721;&#36827;&#34892;&#21464;&#25442;&#65292;&#24182;&#23558;&#27969;&#32593;&#32476;&#19982;CISTA-LSTC&#38598;&#25104;&#65292;&#24418;&#25104;CISTA-Flow&#32593;&#32476;&#65292;&#20351;&#31995;&#32479;&#20165;&#20381;&#36182;&#20107;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.11961</link><description>&lt;p&gt;
&#36890;&#36807;&#36816;&#21160;&#34917;&#20607;&#22686;&#24378;&#20107;&#20214;&#39537;&#21160;&#35270;&#39057;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Enhanced Event-Based Video Reconstruction with Motion Compensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11961
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36816;&#21160;&#34917;&#20607;&#26469;&#22686;&#24378;&#37325;&#24314;&#36136;&#37327;&#65292;&#25552;&#20986;&#23558;&#36755;&#20837;&#24103;&#21644;&#31232;&#30095;&#32534;&#30721;&#36827;&#34892;&#21464;&#25442;&#65292;&#24182;&#23558;&#27969;&#32593;&#32476;&#19982;CISTA-LSTC&#38598;&#25104;&#65292;&#24418;&#25104;CISTA-Flow&#32593;&#32476;&#65292;&#20351;&#31995;&#32479;&#20165;&#20381;&#36182;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#35270;&#39057;&#37325;&#24314;&#36890;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#20869;&#23384;&#38656;&#27714;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#32593;&#32476;CISTA-LSTC&#65292;&#34920;&#26126;&#36890;&#36807;&#31995;&#32479;&#35774;&#35745;&#20854;&#26550;&#26500;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#20854;&#24314;&#27169;&#20551;&#35774;&#36755;&#20837;&#20449;&#21495;&#21644;&#36755;&#20986;&#37325;&#24314;&#24103;&#20849;&#20139;&#30456;&#21516;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#24573;&#35270;&#20102;&#36816;&#21160;&#23548;&#33268;&#30340;&#20301;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23545;&#36755;&#20837;&#24378;&#24230;&#24103;&#21644;&#31232;&#30095;&#32534;&#30721;&#36827;&#34892;&#21464;&#25442;&#20197;&#22686;&#24378;&#37325;&#24314;&#36136;&#37327;&#12290;&#36890;&#36807;&#23558;&#27969;&#32593;&#32476;&#19982;CISTA-LSTC&#38598;&#25104;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;CISTA-Flow&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#34917;&#20607;&#12290;&#31995;&#32479;&#20165;&#20381;&#36182;&#20107;&#20214;&#65292;&#20854;&#20013;&#39044;&#27979;&#30340;&#27969;&#26377;&#21161;&#20110;&#37325;&#24314;&#65292;&#28982;&#21518;&#37325;&#24314;&#30340;&#24103;&#29992;&#20110;&#20419;&#36827;&#27969;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#20026;&#35813;&#32452;&#21512;&#31995;&#32479;&#24341;&#20837;&#20102;&#19968;&#20010;&#36845;&#20195;&#35757;&#32451;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11961v1 Announce Type: cross  Abstract: Deep neural networks for event-based video reconstruction often suffer from a lack of interpretability and have high memory demands. A lightweight network called CISTA-LSTC has recently been introduced showing that high-quality reconstruction can be achieved through the systematic design of its architecture. However, its modelling assumption that input signals and output reconstructed frame share the same sparse representation neglects the displacement caused by motion. To address this, we propose warping the input intensity frames and sparse codes to enhance reconstruction quality. A CISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC for motion compensation. The system relies solely on events, in which predicted flow aids in reconstruction and then reconstructed frames are used to facilitate flow estimation. We also introduce an iterative training framework for this combined system. Results demonstrate tha
&lt;/p&gt;</description></item><item><title>CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11960</link><description>&lt;p&gt;
CASPER&#65306;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11960
&lt;/p&gt;
&lt;p&gt;
CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#26159;&#29702;&#35299;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#24433;&#21709;&#30340;&#22522;&#30784;&#65292;&#36890;&#24120;&#36890;&#36807;&#25918;&#32622;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#25910;&#38598;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#30001;&#20110;&#21508;&#31181;&#25925;&#38556;&#32780;&#23548;&#33268;&#30340;&#32570;&#22833;&#20540;&#65292;&#36825;&#23545;&#25968;&#25454;&#20998;&#26512;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#22312;&#24674;&#22797;&#29305;&#23450;&#25968;&#25454;&#28857;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#32771;&#34385;&#19982;&#35813;&#28857;&#30456;&#20851;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#19968;&#20123;&#26410;&#30693;&#28151;&#26434;&#22240;&#32032;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#26500;&#24314;&#30340;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#38750;&#22240;&#26524;&#24555;&#25463;&#36793;&#12290;&#36825;&#20123;&#28151;&#26434;&#22240;&#32032;&#21487;&#33021;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24320;&#36767;&#21453;&#21521;&#36335;&#24452;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#24314;&#31435;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#38750;&#22240;&#26524;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 Announce Type: new  Abstract: Spatiotemporal time series is the foundation of understanding human activities and their impacts, which is usually collected via monitoring sensors placed at different locations. The collected data usually contains missing values due to various failures, which have significant impact on data analysis. To impute the missing values, a lot of methods have been introduced. When recovering a specific data point, most existing methods tend to take into consideration all the information relevant to that point regardless of whether they have a cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths between the input and output, in other words, they establish non-causal correlations between the input and output. Over-exploiting these non-causa
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#20197;&#25552;&#39640;&#23398;&#20064;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#25110;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.11948</link><description>&lt;p&gt;
&#23398;&#20064;&#32534;&#30721;&#31354;&#38388;&#26354;&#29575;&#20869;&#38750;&#32447;&#24615;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamical Systems Encoding Non-Linearity within Space Curvature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11948
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#20197;&#25552;&#39640;&#23398;&#20064;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#25110;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#21147;&#23398;&#31995;&#32479;&#65288;DS&#65289;&#26159;&#22609;&#36896;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#39640;&#32423;&#31574;&#30053;&#30340;&#26377;&#25928;&#21644;&#24378;&#22823;&#25163;&#27573;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#31283;&#20581;&#21644;&#21453;&#24212;&#28789;&#25935;&#30340;&#25511;&#21046;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#39537;&#21160;&#21521;&#37327;&#22330;&#30340;&#31283;&#23450;&#24615;&#12290;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#26085;&#30410;&#22797;&#26434;&#24615;&#38656;&#35201;&#20855;&#26377;&#26356;&#39640;&#31243;&#24230;&#38750;&#32447;&#24615;&#30340;DS&#65292;&#20197;&#21450;&#36866;&#24212;&#29615;&#22659;&#26465;&#20214;&#21464;&#21270;&#65288;&#22914;&#38556;&#30861;&#29289;&#65289;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;DS&#30340;&#23398;&#20064;&#31574;&#30053;&#24448;&#24448;&#38656;&#35201;&#26435;&#34913;&#65292;&#35201;&#20040;&#29306;&#29298;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#35201;&#20040;&#29306;&#29298;&#31163;&#32447;&#35745;&#31639;&#25928;&#29575;&#65292;&#20197;&#22686;&#24378;&#25152;&#23398;DS&#30340;&#33021;&#21147;&#12290;&#22312;&#32447;&#23545;&#29615;&#22659;&#21464;&#21270;&#36827;&#34892;&#23616;&#37096;&#36866;&#24212;&#35201;&#20040;&#27809;&#26377;&#34987;&#32771;&#34385;&#65292;&#35201;&#20040;&#34987;&#35270;&#20026;&#19968;&#20010;&#21333;&#29420;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20171;&#32461;&#19968;&#31181;&#26041;&#27861;&#65292;&#22686;&#24378;&#23398;&#20064;DS&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#24433;&#21709;&#25928;&#29575;&#25110;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11948v1 Announce Type: cross  Abstract: Dynamical Systems (DS) are an effective and powerful means of shaping high-level policies for robotics control. They provide robust and reactive control while ensuring the stability of the driving vector field. The increasing complexity of real-world scenarios necessitates DS with a higher degree of non-linearity, along with the ability to adapt to potential changes in environmental conditions, such as obstacles. Current learning strategies for DSs often involve a trade-off, sacrificing either stability guarantees or offline computational efficiency in order to enhance the capabilities of the learned DS. Online local adaptation to environmental changes is either not taken into consideration or treated as a separate problem. In this paper, our objective is to introduce a method that enhances the complexity of the learned DS without compromising efficiency during training or stability guarantees. Furthermore, we aim to provide a unified 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#24615;&#20915;&#31574;&#26641;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23478;&#24237;&#33021;&#28304;&#31649;&#29702;&#20013;&#26377;&#25928;&#31649;&#29702;&#33021;&#32791;&#30340;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.11947</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#23478;&#24237;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11947
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#24615;&#20915;&#31574;&#26641;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23478;&#24237;&#33021;&#28304;&#31649;&#29702;&#20013;&#26377;&#25928;&#31649;&#29702;&#33021;&#32791;&#30340;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33021;&#28304;&#36716;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#38656;&#27714;&#20391;&#28789;&#27963;&#24615;&#25104;&#20026;&#29616;&#20195;&#30005;&#32593;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#25552;&#20379;&#30005;&#32593;&#25903;&#25345;&#24182;&#20419;&#36827;&#21487;&#25345;&#32493;&#33021;&#28304;&#30340;&#36827;&#19968;&#27493;&#25972;&#21512;&#12290;&#38500;&#20102;&#20256;&#32479;&#33021;&#28304;&#26469;&#28304;&#22806;&#65292;&#20303;&#23429;&#37096;&#38376;&#26159;&#21478;&#19968;&#20010;&#20027;&#35201;&#19988;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#28789;&#27963;&#24615;&#26469;&#28304;&#65292;&#20027;&#35201;&#21463;&#21040;&#22826;&#38451;&#33021;&#20809;&#20239;&#12289;&#23478;&#24237;&#30005;&#27744;&#21644;&#30005;&#21160;&#27773;&#36710;&#30340;&#26085;&#30410;&#26222;&#21450;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;&#37322;&#25918;&#20303;&#23429;&#36825;&#31181;&#33021;&#28304;&#28789;&#27963;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#26377;&#25928;&#31649;&#29702;&#23478;&#24237;&#33021;&#28304;&#28040;&#32791;&#24182;&#22312;&#20445;&#25345;&#29992;&#25143;&#33298;&#36866;&#30340;&#21516;&#26102;&#33021;&#22815;&#22312;&#19981;&#21516;&#21508;&#31181;&#25151;&#23627;&#20043;&#38388;&#23481;&#26131;&#25193;&#23637;&#30340;&#25511;&#21046;&#26694;&#26550;&#12290;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20280;&#32553;&#24615;&#19982;&#21487;&#35299;&#37322;&#24615;&#30340;&#65288;&#21487;&#24494;&#20998;&#65289;&#20915;&#31574;&#26641;&#30456;&#32467;&#21512;&#12290;&#36825;&#26679;&#23601;&#20135;&#29983;&#20102;&#19968;&#20010;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11947v1 Announce Type: cross  Abstract: With the ongoing energy transition, demand-side flexibility has become an important aspect of the modern power grid for providing grid support and allowing further integration of sustainable energy sources. Besides traditional sources, the residential sector is another major and largely untapped source of flexibility, driven by the increased adoption of solar PV, home batteries, and EVs. However, unlocking this residential flexibility is challenging as it requires a control framework that can effectively manage household energy consumption, and maintain user comfort while being readily scalable across different, diverse houses. We aim to address this challenging problem and introduce a reinforcement learning-based approach using differentiable decision trees. This approach integrates the scalability of data-driven reinforcement learning with the explainability of (differentiable) decision trees. This leads to a controller that can be e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#35266;&#27979;&#31354;&#38388;&#21040;&#31616;&#21270;&#25511;&#21046;&#30456;&#20851;&#21464;&#37327;&#31354;&#38388;&#30340;&#32534;&#30721;&#22120;&#23398;&#20064;&#65292;AC-State&#26041;&#27861;&#26159;&#19968;&#20010;&#22810;&#27493;&#21453;&#21521;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11940</link><description>&lt;p&gt;
&#22810;&#27493;&#21453;&#21521;&#19981;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Multistep Inverse Is Not All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#35266;&#27979;&#31354;&#38388;&#21040;&#31616;&#21270;&#25511;&#21046;&#30456;&#20851;&#21464;&#37327;&#31354;&#38388;&#30340;&#32534;&#30721;&#22120;&#23398;&#20064;&#65292;AC-State&#26041;&#27861;&#26159;&#19968;&#20010;&#22810;&#27493;&#21453;&#21521;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25511;&#21046;&#35774;&#32622;&#20013;&#65292;&#35266;&#27979;&#31354;&#38388;&#36890;&#24120;&#26159;&#19981;&#24517;&#35201;&#30340;&#39640;&#32500;&#19988;&#21463;&#21040;&#26102;&#38388;&#30456;&#20851;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#30340;&#21487;&#25511;&#21160;&#24577;&#36890;&#24120;&#36828;&#27604;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#30340;&#21160;&#24577;&#31616;&#21333;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#23558;&#35266;&#27979;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#21253;&#21547;&#25511;&#21046;&#30456;&#20851;&#21464;&#37327;&#30340;&#31616;&#21270;&#31354;&#38388;&#26159;&#21487;&#21462;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#30001;Efroni&#31561;&#20154;&#65288;2022&#24180;&#65289;&#39318;&#27425;&#25552;&#20986;&#30340;Ex-BMDP&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#35266;&#27979;&#21487;&#20197;&#20998;&#35299;&#20026;&#20381;&#36182;&#20110;&#21160;&#20316;&#30340;&#28508;&#22312;&#29366;&#24577;&#21644;&#29420;&#31435;&#20110;&#21160;&#20316;&#30340;&#26102;&#38388;&#30456;&#20851;&#22122;&#22768;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#33021;&#22815;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#22330;&#26223;&#12290;Lamb&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#20102;&#8220;AC-State&#8221;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#20174;&#36825;&#20123;&#38382;&#39064;&#20013;&#30340;&#35266;&#27979;&#20013;&#25552;&#21462;&#21253;&#21547;&#23436;&#25972;&#20381;&#36182;&#20110;&#21160;&#20316;&#30340;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#12290;AC-State&#26159;&#19968;&#20010;&#22810;&#27493;&#21453;&#21521;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#36335;&#24452;&#20013;&#31532;&#19968;&#20010;&#21644;&#26368;&#21518;&#19968;&#20010;&#29366;&#24577;&#30340;&#32534;&#30721;&#26469;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11940v1 Announce Type: new  Abstract: In real-world control settings, the observation space is often unnecessarily high-dimensional and subject to time-correlated noise. However, the controllable dynamics of the system are often far simpler than the dynamics of the raw observations. It is therefore desirable to learn an encoder to map the observation space to a simpler space of control-relevant variables. In this work, we consider the Ex-BMDP model, first proposed by Efroni et al. (2022), which formalizes control problems where observations can be factorized into an action-dependent latent state which evolves deterministically, and action-independent time-correlated noise. Lamb et al. (2022) proposes the "AC-State" method for learning an encoder to extract a complete action-dependent latent state representation from the observations in such problems. AC-State is a multistep-inverse method, in that it uses the encoding of the the first and last state in a path to predict the 
&lt;/p&gt;</description></item><item><title>&#20174;&#25511;&#21046;&#29702;&#35770;&#30340;&#35282;&#24230;&#65292;&#25552;&#20379;&#20102;Roesser&#31867;&#22411;&#30340;2-D&#21367;&#31215;&#23618;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#20855;&#26377;&#26368;&#23567;&#21270;&#30340;&#29366;&#24577;&#25968;&#37327;&#65292;&#22312;$c_\mathrm{in}=c_\mathrm{out}$&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#25193;&#24352;&#12289;&#36328;&#36234;&#21644;N-D&#21367;&#31215;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.11938</link><description>&lt;p&gt;
Roesser&#31867;&#22411;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#29992;&#20110;&#21367;&#31215;&#23618;
&lt;/p&gt;
&lt;p&gt;
State space representations of the Roesser type for convolutional layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11938
&lt;/p&gt;
&lt;p&gt;
&#20174;&#25511;&#21046;&#29702;&#35770;&#30340;&#35282;&#24230;&#65292;&#25552;&#20379;&#20102;Roesser&#31867;&#22411;&#30340;2-D&#21367;&#31215;&#23618;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#20855;&#26377;&#26368;&#23567;&#21270;&#30340;&#29366;&#24577;&#25968;&#37327;&#65292;&#22312;$c_\mathrm{in}=c_\mathrm{out}$&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#25193;&#24352;&#12289;&#36328;&#36234;&#21644;N-D&#21367;&#31215;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25511;&#21046;&#29702;&#35770;&#30340;&#35282;&#24230;&#30475;&#65292;&#21367;&#31215;&#23618;&#65288;&#31070;&#32463;&#32593;&#32476;&#30340;&#65289;&#26159;2-D&#65288;&#25110;N-D&#65289;&#32447;&#24615;&#26102;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#12290;&#21367;&#31215;&#23618;&#36890;&#24120;&#36890;&#36807;&#21367;&#31215;&#26680;&#34920;&#31034;&#65292;&#23545;&#24212;&#20110;&#21160;&#24577;&#31995;&#32479;&#36890;&#36807;&#20854;&#33033;&#20914;&#21709;&#24212;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25511;&#21046;&#29702;&#35770;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#20363;&#22914;&#28041;&#21450;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#30340;&#24037;&#20855;&#65292;&#38656;&#35201;&#19968;&#20010;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26126;&#30830;&#25552;&#20379;&#20102;Roesser&#31867;&#22411;&#30340;2-D&#21367;&#31215;&#23618;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#20855;&#26377;$c_\mathrm{in}r_1+c_\mathrm{out}r_2$&#20010;&#29366;&#24577;&#65292;&#20854;&#20013;$c_\mathrm{in}/c_\mathrm{out}$&#26159;&#23618;&#30340;&#36755;&#20837;/&#36755;&#20986;&#36890;&#36947;&#25968;&#65292;$r_1/r_2$ &#34920;&#31034;&#21367;&#31215;&#26680;&#30340;&#23485;&#24230;/&#38271;&#24230;&#12290;&#23545;&#20110;$c_\mathrm{in}=c_\mathrm{out}$&#65292;&#24050;&#32463;&#35777;&#26126;&#36825;&#31181;&#34920;&#31034;&#26159;&#26368;&#23567;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#25193;&#24352;&#12289;&#36328;&#36234;&#21644;N-D&#21367;&#31215;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11938v1 Announce Type: cross  Abstract: From the perspective of control theory, convolutional layers (of neural networks) are 2-D (or N-D) linear time-invariant dynamical systems. The usual representation of convolutional layers by the convolution kernel corresponds to the representation of a dynamical system by its impulse response. However, many analysis tools from control theory, e.g., involving linear matrix inequalities, require a state space representation. For this reason, we explicitly provide a state space representation of the Roesser type for 2-D convolutional layers with $c_\mathrm{in}r_1 + c_\mathrm{out}r_2$ states, where $c_\mathrm{in}$/$c_\mathrm{out}$ is the number of input/output channels of the layer and $r_1$/$r_2$ characterizes the width/length of the convolution kernel. This representation is shown to be minimal for $c_\mathrm{in} = c_\mathrm{out}$. We further construct state space representations for dilated, strided, and N-D convolutions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.11925</link><description>&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#32780;&#26080;&#38656;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#65306;&#22522;&#20110;&#22810;&#32423;Actor-Critic&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11925
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#20110;&#28151;&#21512;&#26102;&#38388;&#30340;&#39044;&#27979;&#30340;oracle&#30693;&#35782;&#35201;&#27714;&#65292;&#21363;&#24230;&#37327;&#39532;&#23572;&#21487;&#22827;&#38142;&#22312;&#22266;&#23450;&#31574;&#30053;&#19979;&#36798;&#21040;&#20854;&#31283;&#24577;&#20998;&#24067;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#23545;&#20110;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#29699;&#25910;&#25947;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#32423;Actor-Critic&#65288;MAC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#65288;MLMC&#65289;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#28151;&#21512;&#26102;&#38388;&#30693;&#35782;&#30340;&#20381;&#36182;&#24615;&#30340;&#26377;&#25928;&#20943;&#36731;&#65292;&#36825;&#26159;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;$\mathcal{O}$&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11925v1 Announce Type: new  Abstract: In the context of average-reward reinforcement learning, the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods. This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications. To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator. With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward MDPs global convergence. Furthermore, our approach exhibits the tightest-available dependence of $\mathcal{O
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;Agent Actor Critic&#27169;&#22411;&#65292;&#26088;&#22312;&#21033;&#29992;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#33258;&#20027;&#36710;&#36742;&#30340;&#21435;&#20013;&#24515;&#21270;&#21512;&#20316;&#39550;&#39542;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23545;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#30340;&#24191;&#27867;&#35780;&#20272;&#23637;&#29616;&#20102;&#25913;&#21892;&#36947;&#36335;&#31995;&#32479;&#20869;&#19981;&#21516;&#29942;&#39048;&#20301;&#32622;&#20132;&#36890;&#27969;&#37327;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11914</link><description>&lt;p&gt;
&#21333;Agent Actor Critic&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#21512;&#20316;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Single-Agent Actor Critic for Decentralized Cooperative Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;Agent Actor Critic&#27169;&#22411;&#65292;&#26088;&#22312;&#21033;&#29992;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#33258;&#20027;&#36710;&#36742;&#30340;&#21435;&#20013;&#24515;&#21270;&#21512;&#20316;&#39550;&#39542;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23545;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#30340;&#24191;&#27867;&#35780;&#20272;&#23637;&#29616;&#20102;&#25913;&#21892;&#36947;&#36335;&#31995;&#32479;&#20869;&#19981;&#21516;&#29942;&#39048;&#20301;&#32622;&#20132;&#36890;&#27969;&#37327;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#20132;&#36890;&#31649;&#29702;&#32467;&#21512;&#33258;&#20027;&#36710;&#36742;&#65288;AVs&#65289;&#25215;&#35834;&#26410;&#26469;&#25317;&#26377;&#20943;&#23569;&#25317;&#22581;&#21644;&#22686;&#24378;&#20132;&#36890;&#27969;&#37327;&#12290;&#28982;&#32780;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#24320;&#21457;&#31639;&#27861;&#38656;&#35201;&#35299;&#20915;&#36830;&#32493;&#20132;&#36890;&#27969;&#37327;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25512;&#21160;&#20027;&#21160;&#20132;&#36890;&#31649;&#29702;&#39046;&#22495;&#26397;&#30528;&#26356;&#22823;&#31243;&#24230;&#30340;&#21435;&#20013;&#24515;&#21270;&#21457;&#23637;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;actor-critic&#27169;&#22411;&#65292;&#26088;&#22312;&#21033;&#29992;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#33258;&#20027;&#36710;&#36742;&#30340;&#21435;&#20013;&#24515;&#21270;&#21512;&#20316;&#39550;&#39542;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20855;&#26377;&#25513;&#30721;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#23454;&#38469;&#20132;&#36890;&#27969;&#37327;&#30340;&#21160;&#24577;&#29305;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#20013;&#38024;&#23545;&#22522;&#32447;&#25511;&#21046;&#22120;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#22312;&#36947;&#36335;&#31995;&#32479;&#20869;&#19981;&#21516;&#29942;&#39048;&#20301;&#32622;&#25913;&#21892;&#20132;&#36890;&#27969;&#37327;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11914v1 Announce Type: new  Abstract: Active traffic management incorporating autonomous vehicles (AVs) promises a future with diminished congestion and enhanced traffic flow. However, developing algorithms for real-world application requires addressing the challenges posed by continuous traffic flow and partial observability. To bridge this gap and advance the field of active traffic management towards greater decentralization, we introduce a novel asymmetric actor-critic model aimed at learning decentralized cooperative driving policies for autonomous vehicles using single-agent reinforcement learning. Our approach employs attention neural networks with masking to handle the dynamic nature of real-world traffic flow and partial observability. Through extensive evaluations against baseline controllers across various traffic scenarios, our model shows great potential for improving traffic flow at diverse bottleneck locations within the road system. Additionally, we explore t
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#22312;&#33021;&#28304;&#24212;&#29992;&#25511;&#21046;&#22120;&#20013;&#30340;&#21019;&#26032;&#26159;&#36890;&#36807;&#25552;&#20986;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26469;&#35299;&#20915;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20303;&#23429;&#36164;&#20135;&#30828;&#20214;&#33021;&#21147;&#21463;&#38480;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.11907</link><description>&lt;p&gt;
Distill2Explain: &#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#22312;&#33021;&#28304;&#24212;&#29992;&#25511;&#21046;&#22120;&#20013;&#30340;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11907
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#22312;&#33021;&#28304;&#24212;&#29992;&#25511;&#21046;&#22120;&#20013;&#30340;&#21019;&#26032;&#26159;&#36890;&#36807;&#25552;&#20986;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26469;&#35299;&#20915;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20303;&#23429;&#36164;&#20135;&#30828;&#20214;&#33021;&#21147;&#21463;&#38480;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#20391;&#28789;&#27963;&#24615;&#20316;&#20026;&#33021;&#28304;&#36807;&#28193;&#36827;&#31243;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#26085;&#30410;&#37325;&#35201;&#12290;&#20316;&#20026;&#20840;&#29699;&#26368;&#32456;&#33021;&#28304;&#28040;&#32791;&#30340;&#32422;25&#65285;&#65292;&#20303;&#23429;&#37096;&#38376;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#65288;&#28508;&#22312;&#30340;&#65289;&#33021;&#28304;&#28789;&#27963;&#24615;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#21457;&#25496;&#36825;&#31181;&#28789;&#27963;&#24615;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#25511;&#21046;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#25151;&#23627;&#20043;&#38388;&#23481;&#26131;&#25193;&#23637;&#65292;&#26131;&#20110;&#32500;&#25252;&#65292;&#24182;&#19988;&#23545;&#32456;&#31471;&#29992;&#25143;&#31616;&#21333;&#26131;&#25026;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#30340;&#28508;&#22312;&#25511;&#21046;&#26694;&#26550;&#26159;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#65292;&#29305;&#21035;&#26159;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#36825;&#31181;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#23398;&#20064;&#33391;&#22909;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#23436;&#20840;&#22522;&#20110;&#25968;&#25454;&#23398;&#20064;&#65292;&#24182;&#19988;&#20154;&#31867;&#24178;&#39044;&#26368;&#23567;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#22952;&#30861;&#20102;&#29992;&#25143;&#25509;&#21463;&#12290;&#27492;&#22806;&#65292;&#20303;&#23429;&#36164;&#20135;&#30340;&#26377;&#38480;&#30828;&#20214;&#33021;&#21147;&#26500;&#25104;&#20102;&#38459;&#30861;&#65288;&#20363;&#22914;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11907v1 Announce Type: cross  Abstract: Demand-side flexibility is gaining importance as a crucial element in the energy transition process. Accounting for about 25% of final energy consumption globally, the residential sector is an important (potential) source of energy flexibility. However, unlocking this flexibility requires developing a control framework that (1) easily scales across different houses, (2) is easy to maintain, and (3) is simple to understand for end-users. A potential control framework for such a task is data-driven control, specifically model-free reinforcement learning (RL). Such RL-based controllers learn a good control policy by interacting with their environment, learning purely based on data and with minimal human intervention. Yet, they lack explainability, which hampers user acceptance. Moreover, limited hardware capabilities of residential assets forms a hurdle (e.g., using deep neural networks). To overcome both those challenges, we propose a no
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.11904</link><description>&lt;p&gt;
CICLe: &#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#27745;&#26579;&#25110;&#25530;&#20551;&#39135;&#21697;&#23545;&#20154;&#31867;&#20581;&#24247;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#22312;&#32473;&#23450;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#32593;&#32476;&#25991;&#26412;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#33258;&#21160;&#26816;&#27979;&#36825;&#31181;&#39118;&#38505;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;7,546&#20010;&#25551;&#36848;&#20844;&#20849;&#39135;&#21697;&#21484;&#22238;&#20844;&#21578;&#30340;&#30701;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25991;&#26412;&#37117;&#32463;&#36807;&#25163;&#21160;&#26631;&#35760;&#65292;&#20998;&#20026;&#20004;&#20010;&#31890;&#24230;&#32423;&#21035;&#65288;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#21484;&#22238;&#23545;&#24212;&#30340;&#39135;&#21697;&#20135;&#21697;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#24182;&#23545;&#26420;&#32032;&#12289;&#20256;&#32479;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;tf-idf&#34920;&#31034;&#30340;&#36923;&#36753;&#22238;&#24402;&#22312;&#25903;&#25345;&#36739;&#20302;&#30340;&#31867;&#21035;&#19978;&#20248;&#20110;RoBERTa&#21644;XLM-R&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#19982;&#26222;&#36890;&#25552;&#31034;&#30456;&#27604;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
&lt;/p&gt;</description></item><item><title>Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.11901</link><description>&lt;p&gt;
Larimar: &#20855;&#26377;&#24773;&#33410;&#35760;&#24518;&#25511;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Larimar: Large Language Models with Episodic Memory Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11901
&lt;/p&gt;
&lt;p&gt;
Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Larimar - &#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#12290; Larimar&#30340;&#35760;&#24518;&#20801;&#35768;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#26356;&#26032;&#30693;&#35782;&#65292;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#20107;&#23454;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Larimar&#22312;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322; - &#26681;&#25454;&#22522;&#30784;LLM&#30340;&#19981;&#21516;&#65292;&#36895;&#24230;&#25552;&#21319;&#20026;4-10&#20493;&#65292;&#24182;&#19988;&#30001;&#20110;&#25552;&#20986;&#30340;&#26550;&#26500;&#31616;&#21333;&#12289;&#19981;&#20381;&#36182;&#20110;LLM&#65292;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#20107;&#23454;&#36951;&#24536;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#27010;&#25324;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11901v1 Announce Type: cross  Abstract: Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#24179;&#21488;&#20197;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#20154;&#20195;&#29702;&#25554;&#25300;USB&#30005;&#32518;&#65292;&#23454;&#29616;&#20102;&#22312;&#24494;&#32454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11898</link><description>&lt;p&gt;
&#35270;&#35273;-&#35302;&#35273;&#39044;&#35757;&#32451;&#29992;&#20110;&#25554;&#25300;&#30005;&#32518;
&lt;/p&gt;
&lt;p&gt;
Visuo-Tactile Pretraining for Cable Plugging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#24179;&#21488;&#20197;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#20154;&#20195;&#29702;&#25554;&#25300;USB&#30005;&#32518;&#65292;&#23454;&#29616;&#20102;&#22312;&#24494;&#32454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#35273;&#20449;&#24687;&#26159;&#36827;&#34892;&#31934;&#32454;&#25805;&#32437;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#20316;&#20026;&#20154;&#31867;&#65292;&#25105;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#35302;&#35273;&#20449;&#24687;&#26469;&#29702;&#35299;&#21608;&#22260;&#30340;&#29289;&#20307;&#20197;&#21450;&#22914;&#20309;&#19982;&#20854;&#20114;&#21160;&#12290;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#35302;&#25720;&#26469;&#25191;&#34892;&#25805;&#32437;&#20219;&#21153;&#65292;&#36824;&#29992;&#23427;&#26469;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#21019;&#24314;&#33021;&#22815;&#23398;&#20064;&#20197;&#20154;&#31867;&#25110;&#36229;&#20154;&#31867;&#27700;&#24179;&#23436;&#25104;&#25805;&#32437;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#25105;&#20204;&#38656;&#35201;&#27491;&#30830;&#22320;&#23558;&#35302;&#35273;&#20449;&#24687;&#34701;&#20837;&#25216;&#33021;&#25191;&#34892;&#21644;&#25216;&#33021;&#23398;&#20064;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#24179;&#21488;&#20197;&#25552;&#39640;&#22797;&#26434;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30528;&#25163;&#35299;&#20915;&#25554;&#25300;USB&#30005;&#32518;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#39033;&#20381;&#36182;&#20110;&#24494;&#35266;&#35270;&#35273;-&#35302;&#35273;&#21327;&#20316;&#30340;&#29087;&#32451;&#25805;&#32437;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#20154;&#20195;&#29702;&#25554;&#25300;USB&#30005;&#32518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11898v1 Announce Type: cross  Abstract: Tactile information is a critical tool for fine-grain manipulation. As humans, we rely heavily on tactile information to understand objects in our environments and how to interact with them. We use touch not only to perform manipulation tasks but also to learn how to perform these tasks. Therefore, to create robotic agents that can learn to complete manipulation tasks at a human or super-human level of performance, we need to properly incorporate tactile information into both skill execution and skill learning. In this paper, we investigate how we can incorporate tactile information into imitation learning platforms to improve performance on complex tasks. To do this, we tackle the challenge of plugging in a USB cable, a dexterous manipulation task that relies on fine-grain visuo-tactile serving. By incorporating tactile information into imitation learning frameworks, we are able to train a robotic agent to plug in a USB cable - a firs
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.11894</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#21040;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#23454;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DL&#30340;NLP&#26041;&#27861;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#36879;&#26126;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#25110;&#33267;&#23569;&#26159;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#23545;&#21307;&#30103;&#20581;&#24247;NLP&#20013;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;DL&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#33539;&#22260;&#23457;&#26597;&#12290;&#24341;&#20837;&#20102;&#26415;&#35821;&#8220;XIAI&#8221;&#65288;eXplainable&#21644;Interpretable Artificial Intelligence&#65289;&#20197;&#21306;&#20998;XAI&#21644;IAI&#12290;&#26041;&#27861;&#26681;&#25454;&#20854;&#21151;&#33021;&#65288;&#27169;&#22411;&#12289;&#36755;&#20837;&#12289;&#36755;&#20986;&#20026;&#22522;&#30784;&#65289;&#21644;&#33539;&#22260;&#65288;&#23616;&#37096;&#12289;&#20840;&#23616;&#65289;&#36827;&#19968;&#27493;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27880;&#24847;&#26426;&#21046;&#26159;&#26368;&#20027;&#35201;&#30340;&#26032;&#20852;IAI&#12290;&#27492;&#22806;&#65292;IAI&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#23545;&#25239;XAI&#12290;&#30830;&#23450;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;XIAI&#19981;&#25506;&#32034;&#8220;&#20840;&#23616;&#8221;&#24314;&#27169;&#36807;&#31243;&#65292;&#32570;&#20047;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#19988;&#38656;&#35201;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
&lt;/p&gt;</description></item><item><title>FKD&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#27010;&#24565;&#65292;&#23581;&#35797;&#22312;&#35299;&#20915;FL&#20013;&#30340;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11892</link><description>&lt;p&gt;
KnFu: &#26377;&#25928;&#30340;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
KnFu: Effective Knowledge Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11892
&lt;/p&gt;
&lt;p&gt;
FKD&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#27010;&#24565;&#65292;&#23581;&#35797;&#22312;&#35299;&#20915;FL&#20013;&#30340;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11892v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#20256;&#32479;&#38598;&#20013;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#31181;&#31361;&#20986;&#26367;&#20195;&#26041;&#26696;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;FL&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#65292;&#20801;&#35768;&#36328;&#22810;&#20010;&#26412;&#22320;&#33410;&#28857;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#21516;&#26102;&#21033;&#29992;&#21508;&#31181;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;FL&#23481;&#26131;&#21463;&#21040;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#65292;&#24378;&#21046;&#22312;&#26412;&#22320;&#27169;&#22411;&#19978;&#23454;&#26045;&#32479;&#19968;&#26550;&#26500;&#65292;&#24182;&#19988;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#27169;&#22411;&#24322;&#26500;&#24615;&#65288;&#27169;&#22411;&#28418;&#31227;&#65289;&#12290;&#20026;&#20102;&#20943;&#36731;&#20854;&#20013;&#19968;&#20123;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#32852;&#37030;&#30693;&#35782;&#33976;&#39311;&#65288;FKD&#65289;&#30340;&#26032;&#33539;&#24335;&#12290;FKD&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#27010;&#24565;&#24320;&#21457;&#65292;&#20854;&#20013;&#28041;&#21450;&#20174;&#32463;&#39564;&#20016;&#23500;&#24182;&#35757;&#32451;&#33391;&#22909;&#30340;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#21644;&#36716;&#31227;&#30693;&#35782;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;FKD&#20173;&#28982;&#38754;&#20020;&#27169;&#22411;&#28418;&#31227;&#38382;&#39064;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#30693;&#35782;&#37117;&#26159;&#32479;&#19968;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11892v1 Announce Type: new  Abstract: Federated Learning (FL) has emerged as a prominent alternative to the traditional centralized learning approach. Generally speaking, FL is a decentralized approach that allows for collaborative training of Machine Learning (ML) models across multiple local nodes, ensuring data privacy and security while leveraging diverse datasets. Conventional FL, however, is susceptible to gradient inversion attacks, restrictively enforces a uniform architecture on local models, and suffers from model heterogeneity (model drift) due to non-IID local datasets. To mitigate some of these challenges, the new paradigm of Federated Knowledge Distillation (FKD) has emerged. FDK is developed based on the concept of Knowledge Distillation (KD), which involves extraction and transfer of a large and well-trained teacher model's knowledge to lightweight student models. FKD, however, still faces the model drift issue. Intuitively speaking, not all knowledge is univ
&lt;/p&gt;</description></item><item><title>SuperLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#19988;&#39640;&#24230;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25216;&#24039;&#25193;&#23637;&#20102;&#19981;&#21516;&#30340;LoRA&#21464;&#20307;&#65292;&#22312;&#26497;&#23569;&#21442;&#25968;&#24773;&#20917;&#19979;&#29305;&#21035;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.11887</link><description>&lt;p&gt;
SuperLoRA: &#22810;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#21442;&#25968;&#39640;&#25928;&#32479;&#19968;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11887
&lt;/p&gt;
&lt;p&gt;
SuperLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#19988;&#39640;&#24230;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25216;&#24039;&#25193;&#23637;&#20102;&#19981;&#21516;&#30340;LoRA&#21464;&#20307;&#65292;&#22312;&#26497;&#23569;&#21442;&#25968;&#24773;&#20917;&#19979;&#29305;&#21035;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#21450;&#20854;&#21464;&#20307;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24494;&#35843;&#22823;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SuperLoRA&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#32479;&#19968;&#24182;&#25193;&#23637;&#20102;&#19981;&#21516;&#30340;LoRA&#21464;&#20307;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#23454;&#29616;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#32452;&#12289;&#25240;&#21472;&#12289;&#27927;&#29260;&#12289;&#25237;&#24433;&#21644;&#24352;&#37327;&#22240;&#23376;&#21270;&#65292;SuperLoRA&#30456;&#27604;&#20854;&#20182;LoRA&#21464;&#20307;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#22312;&#38750;&#24120;&#23569;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#29305;&#21035;&#22312;&#20256;&#36882;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11887v1 Announce Type: cross  Abstract: Low-rank adaptation (LoRA) and its variants are widely employed in fine-tuning large models, including large language models for natural language processing and diffusion models for computer vision. This paper proposes a generalized framework called SuperLoRA that unifies and extends different LoRA variants, which can be realized under different hyper-parameter settings. Introducing grouping, folding, shuffling, projecting, and tensor factoring, SuperLoRA offers high flexibility compared with other LoRA variants and demonstrates superior performance for transfer learning tasks especially in the extremely few-parameter regimes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31532;&#22235;&#20195;&#21306;&#22495;&#20379;&#28909;&#32593;&#26684;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#28909;&#21151;&#29575;&#27969;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20195;&#29702;&#20998;&#24067;&#35206;&#30422;&#30456;&#20851;&#20379;&#38656;&#20540;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#36991;&#20813;&#27714;&#35299;&#28909;&#32593;&#26684;&#26041;&#31243;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11877</link><description>&lt;p&gt;
&#38024;&#23545;&#31532;&#22235;&#20195;&#21306;&#22495;&#20379;&#28909;&#32593;&#26684;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#28909;&#21151;&#29575;&#27969;&#25928;&#29575;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11877
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31532;&#22235;&#20195;&#21306;&#22495;&#20379;&#28909;&#32593;&#26684;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#28909;&#21151;&#29575;&#27969;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20195;&#29702;&#20998;&#24067;&#35206;&#30422;&#30456;&#20851;&#20379;&#38656;&#20540;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#36991;&#20813;&#27714;&#35299;&#28909;&#32593;&#26684;&#26041;&#31243;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#21151;&#29575;&#27969;&#65288;TPF&#65289;&#26159;&#31532;&#22235;&#20195;&#21306;&#22495;&#20379;&#28909;&#32593;&#26684;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#35813;&#32593;&#26684;&#20855;&#26377;&#22810;&#20010;&#20998;&#25955;&#30340;&#28909;&#28304;&#21644;&#32593;&#26684;&#32467;&#26500;&#12290;&#35745;&#31639;TPF&#21363;&#30830;&#23450;&#32593;&#26684;&#29366;&#24577;&#65288;&#21253;&#25324;&#28201;&#24230;&#12289;&#21387;&#21147;&#21644;&#36136;&#37327;&#27969;&#65289;&#20197;&#28385;&#36275;&#32473;&#23450;&#20379;&#38656;&#20540;&#30340;&#25511;&#21046;&#30446;&#30340;&#65292;&#36890;&#24120;&#36890;&#36807;&#27714;&#35299;&#38750;&#32447;&#24615;&#28909;&#32593;&#26684;&#26041;&#31243;&#26469;&#23454;&#29616;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31561;&#23398;&#20064;&#27169;&#22411;&#26469;&#21152;&#36895;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#28085;&#30422;&#30456;&#20851;&#20379;&#38656;&#20540;&#30340;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26159;&#20174;&#37319;&#26679;&#30340;&#20379;&#38656;&#20540;&#20013;&#29983;&#25104;&#35757;&#32451;&#31034;&#20363;&#65292;&#32780;&#26159;&#20174;&#20195;&#29702;&#20998;&#24067;&#29983;&#25104;&#35757;&#32451;&#31034;&#20363;&#65292;&#35813;&#20195;&#29702;&#20998;&#24067;&#35206;&#30422;&#21457;&#30005;&#26426;&#21644;&#28040;&#36153;&#32773;&#36136;&#37327;&#27969;&#65292;&#30465;&#30053;&#20102;&#35299;&#20915;&#28909;&#32593;&#26684;&#26041;&#31243;&#25152;&#38656;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#30830;&#20999;&#20294;&#30053;&#26377;&#19981;&#21516;&#30340;&#35757;&#32451;&#31034;&#20363;&#21487;&#20197;&#21152;&#26435;&#34920;&#31034;&#21407;&#22987;&#35757;&#32451;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11877v1 Announce Type: new  Abstract: Thermal power flow (TPF) is an important task for various control purposes in 4 Th generation district heating grids with multiple decentral heat sources and meshed grid structures. Computing the TPF, i.e., determining the grid state consisting of temperatures, pressures, and mass flows for given supply and demand values, is classically done by solving the nonlinear heat grid equations, but can be sped up by orders of magnitude using learned models such as neural networks. We propose a novel, efficient scheme to generate a sufficiently large training data set covering relevant supply and demand values. Instead of sampling supply and demand values, our approach generates training examples from a proxy distribution over generator and consumer mass flows, omitting the iterations needed for solving the heat grid equations. The exact, but slightly different, training examples can be weighted to represent the original training distribution. We
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#36234;&#37326;&#22320;&#22270;&#30340;&#21046;&#20316;&#65292;&#20026;&#38271;&#31243;&#39044;&#27979;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.11876</link><description>&lt;p&gt;
&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#29992;&#20110;&#33258;&#30417;&#30563;&#12289;&#39640;&#20998;&#36776;&#29575;&#12289;&#36234;&#37326;&#22320;&#22270;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#36234;&#37326;&#22320;&#22270;&#30340;&#21046;&#20316;&#65292;&#20026;&#38271;&#31243;&#39044;&#27979;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#21463;&#38480;&#30340;&#36234;&#37326;&#36710;&#36742;&#30340;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#26377;&#38480;&#65292;&#36825;&#32473;&#21487;&#38752;&#30340;&#36234;&#37326;&#33258;&#20027;&#24615;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34701;&#21512;&#26410;&#26469;&#20449;&#24687;&#65288;&#21363;&#26410;&#26469;&#34701;&#21512;&#65289;&#36827;&#34892;&#33258;&#30417;&#30563;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#26410;&#26469;&#20449;&#24687;&#20197;&#21450;&#25163;&#24037;&#21046;&#20316;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30452;&#25509;&#30417;&#30563;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#21487;&#31359;&#36234;&#24615;&#20272;&#35745;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#26356;&#20026;&#36890;&#29992;&#30340;&#21457;&#23637;&#26041;&#21521; - &#36890;&#36807;&#26410;&#26469;&#34701;&#21512;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#26102;&#38388;&#39640;&#25928;&#22320;&#23436;&#25104;&#26368;&#39640;&#20998;&#36776;&#29575;&#65288;&#21363;&#27599;&#20687;&#32032;2&#21400;&#31859;&#65289;BEV&#22320;&#22270;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#38271;&#31243;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#26410;&#26469;&#34701;&#21512;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#65288;RGB / &#39640;&#24230;&#65289;&#21407;&#22987;&#31232;&#30095;&#22122;&#38899;&#36755;&#20837;&#21644;&#22522;&#20110;&#22320;&#22270;&#30340;&#23494;&#38598;&#26631;&#31614;&#30340;&#25104;&#23545;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#36866;&#24212;&#20256;&#24863;&#22120;&#30340;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11876v1 Announce Type: cross  Abstract: The limited sensing resolution of resource-constrained off-road vehicles poses significant challenges towards reliable off-road autonomy. To overcome this limitation, we propose a general framework based on fusing the future information (i.e. future fusion) for self-supervision. Recent approaches exploit this future information alongside the hand-crafted heuristics to directly supervise the targeted downstream tasks (e.g. traversability estimation). However, in this paper, we opt for a more general line of development - time-efficient completion of the highest resolution (i.e. 2cm per pixel) BEV map in a self-supervised manner via future fusion, which can be used for any downstream tasks for better longer range prediction. To this end, first, we create a high-resolution future-fusion dataset containing pairs of (RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to accommodate the noise and sparsity of the sens
&lt;/p&gt;</description></item><item><title>NuGraph2 &#26159;&#19968;&#31181;&#29992;&#20110;&#28082;&#27689;&#26102;&#38388;&#25237;&#24433;&#23460;&#25506;&#27979;&#22120;&#20013;&#27169;&#25311;&#20013;&#24494;&#23376;&#30456;&#20114;&#20316;&#29992;&#20302;&#32423;&#37325;&#24314;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#20256;&#36882;&#26426;&#21046;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32972;&#26223;&#36807;&#28388;&#21644;&#35821;&#20041;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2403.11872</link><description>&lt;p&gt;
NuGraph2&#65306;&#29992;&#20110;&#20013;&#24494;&#23376;&#29289;&#29702;&#20107;&#20214;&#37325;&#24314;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
NuGraph2: A Graph Neural Network for Neutrino Physics Event Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11872
&lt;/p&gt;
&lt;p&gt;
NuGraph2 &#26159;&#19968;&#31181;&#29992;&#20110;&#28082;&#27689;&#26102;&#38388;&#25237;&#24433;&#23460;&#25506;&#27979;&#22120;&#20013;&#27169;&#25311;&#20013;&#24494;&#23376;&#30456;&#20114;&#20316;&#29992;&#20302;&#32423;&#37325;&#24314;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#20256;&#36882;&#26426;&#21046;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32972;&#26223;&#36807;&#28388;&#21644;&#35821;&#20041;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11872v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#28082;&#27689;&#26102;&#38388;&#25237;&#24433;&#23460;&#65288;LArTPC&#65289;&#25506;&#27979;&#22120;&#25216;&#26415;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#39640;&#20998;&#36776;&#29575;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#20449;&#24687;&#65292;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#38656;&#35201;&#20808;&#36827;&#30340;&#33258;&#21160;&#37325;&#24314;&#25216;&#26415;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;NuGraph2&#65292;&#19968;&#31181;&#29992;&#20110;LArTPC&#25506;&#27979;&#22120;&#20013;&#27169;&#25311;&#20013;&#24494;&#23376;&#30456;&#20114;&#20316;&#29992;&#20302;&#32423;&#37325;&#24314;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;MicroBooNE&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27169;&#25311;&#20013;&#24494;&#23376;&#30456;&#20114;&#20316;&#29992;&#34987;&#25551;&#36848;&#20026;&#24322;&#36136;&#22270;&#65292;&#27599;&#20010;&#25506;&#27979;&#22120;&#24179;&#38754;&#19978;&#30340;&#33021;&#37327;&#27785;&#31215;&#24418;&#25104;&#24179;&#38754;&#23376;&#22270;&#19978;&#30340;&#33410;&#28857;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#20256;&#36882;&#26426;&#21046;&#23545;&#36825;&#20123;&#22270;&#33410;&#28857;&#25191;&#34892;&#32972;&#26223;&#36807;&#28388;&#21644;&#35821;&#20041;&#26631;&#35760;&#65292;&#20197;98.0\%&#30340;&#25928;&#29575;&#35782;&#21035;&#19982;&#20027;&#35201;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#32852;&#30340;&#33410;&#28857;&#65292;&#24182;&#20197;94.9\%&#30340;&#25928;&#29575;&#26681;&#25454;&#31890;&#23376;&#31867;&#22411;&#23558;&#20854;&#26631;&#35760;&#12290;&#35813;&#32593;&#32476;&#30452;&#25509;&#22312;&#25506;&#27979;&#22120;&#21487;&#35266;&#23519;&#37327;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11872v1 Announce Type: cross  Abstract: Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a wealth of high-resolution information on particle interactions, and leveraging that information to its full potential requires sophisticated automated reconstruction techniques. This article describes NuGraph2, a Graph Neural Network (GNN) for low-level reconstruction of simulated neutrino interactions in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE detector geometry are described as heterogeneous graphs, with energy depositions on each detector plane forming nodes on planar subgraphs. The network utilizes a multi-head attention message-passing mechanism to perform background filtering and semantic labelling on these graph nodes, identifying those associated with the primary physics interaction with 98.0\% efficiency and labelling them according to particle type with 94.9\% efficiency. The network operates directly on detector observables
&lt;/p&gt;</description></item><item><title>&#23558;&#20108;&#20803;&#20998;&#31867;&#22120;&#23450;&#20041;&#20026;&#28909;&#24102;&#26377;&#29702;&#20989;&#25968;&#30340;&#31526;&#21495;&#65292;&#21457;&#29616;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#31354;&#38388;&#21547;&#20110;&#20854;&#20869;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#30340;&#20004;&#31181;&#19981;&#21516;&#32454;&#20998;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;0/1&#25439;&#22833;&#20989;&#25968;&#30340;&#23376;&#27700;&#24179;&#38598;&#20197;&#21450;&#20998;&#31867;&#39118;&#25159;&#30340;&#20960;&#20309;&#21644;&#32452;&#21512;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11871</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#38469;&#28909;&#24102;&#20960;&#20309;&#23398;
&lt;/p&gt;
&lt;p&gt;
The Real Tropical Geometry of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11871
&lt;/p&gt;
&lt;p&gt;
&#23558;&#20108;&#20803;&#20998;&#31867;&#22120;&#23450;&#20041;&#20026;&#28909;&#24102;&#26377;&#29702;&#20989;&#25968;&#30340;&#31526;&#21495;&#65292;&#21457;&#29616;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#31354;&#38388;&#21547;&#20110;&#20854;&#20869;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#30340;&#20004;&#31181;&#19981;&#21516;&#32454;&#20998;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;0/1&#25439;&#22833;&#20989;&#25968;&#30340;&#23376;&#27700;&#24179;&#38598;&#20197;&#21450;&#20998;&#31867;&#39118;&#25159;&#30340;&#20960;&#20309;&#21644;&#32452;&#21512;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23558;&#20108;&#20803;&#20998;&#31867;&#22120;&#23450;&#20041;&#20026;&#28909;&#24102;&#26377;&#29702;&#20989;&#25968;&#30340;&#31526;&#21495;&#65292;&#21363;&#20004;&#20010;&#20984;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#24046;&#12290;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#31354;&#38388;&#34987;&#21253;&#21547;&#22312;&#28909;&#24102;&#26377;&#29702;&#20989;&#25968;&#21442;&#25968;&#31354;&#38388;&#30340;&#21322;&#20195;&#25968;&#38598;&#20013;&#12290;&#25105;&#20204;&#21551;&#21160;&#20102;&#23545;&#21442;&#25968;&#31354;&#38388;&#30340;&#20004;&#31181;&#19981;&#21516;&#32454;&#20998;&#30340;&#30740;&#31350;&#65306;&#19968;&#31181;&#32454;&#20998;&#20026;&#21322;&#20195;&#25968;&#38598;&#65292;&#20854;&#20013;&#20915;&#31574;&#36793;&#30028;&#30340;&#32452;&#21512;&#31867;&#22411;&#26159;&#22266;&#23450;&#30340;&#65292;&#21478;&#19968;&#31181;&#32454;&#20998;&#20026;&#19968;&#20010;&#22810;&#38754;&#20307;&#39118;&#25159;&#65292;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#32452;&#21512;&#12290;0/1&#25439;&#22833;&#20989;&#25968;&#30340;&#23376;&#27700;&#24179;&#38598;&#20986;&#29616;&#20026;&#36825;&#31181;&#20998;&#31867;&#39118;&#25159;&#30340;&#23376;&#39118;&#25159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27700;&#24179;&#38598;&#19981;&#19968;&#23450;&#26159;&#36830;&#25509;&#30340;&#12290;&#25105;&#20204;&#25551;&#36848;&#20998;&#31867;&#39118;&#25159;i) &#20960;&#20309;&#19978;&#65292;&#20316;&#20026;&#28608;&#27963;&#22810;&#38754;&#20307;&#30340;&#27861;&#32447;&#39118;&#25159;&#65292;&#20197;&#21450;ii) &#32452;&#21512;&#19978;&#65292;&#36890;&#36807;&#20851;&#32852;&#20108;&#20998;&#22270;&#30340;&#24615;&#36136;&#21015;&#34920;&#65292;&#31867;&#27604;&#20110;&#26377;&#21521;&#24615;&#30340;&#32447;&#24615;&#20195;&#25968;&#20844;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11871v1 Announce Type: cross  Abstract: We consider a binary classifier defined as the sign of a tropical rational function, that is, as the difference of two convex piecewise linear functions. The parameter space of ReLU neural networks is contained as a semialgebraic set inside the parameter space of tropical rational functions. We initiate the study of two different subdivisions of this parameter space: a subdivision into semialgebraic sets, on which the combinatorial type of the decision boundary is fixed, and a subdivision into a polyhedral fan, capturing the combinatorics of the partitions of the dataset. The sublevel sets of the 0/1-loss function arise as subfans of this classification fan, and we show that the level-sets are not necessarily connected. We describe the classification fan i) geometrically, as normal fan of the activation polytope, and ii) combinatorially through a list of properties of associated bipartite graphs, in analogy to covector axioms of orient
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26230;&#20307;&#21333;&#20301;&#32990;&#30340;&#21608;&#26399;&#27169;&#24335;&#24314;&#31435;&#26230;&#26684;&#34920;&#31034;&#36827;&#34892;&#26230;&#20307;&#39640;&#25928;&#22270;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#26230;&#20307;&#26448;&#26009;&#30340;SE(3) transformer&#65292;&#21253;&#25324;iComFormer&#21644;eComFormer&#20004;&#20010;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.11857</link><description>&lt;p&gt;
&#23436;&#22791;&#39640;&#25928;&#30340;&#29992;&#20110;&#26230;&#20307;&#26448;&#26009;&#24615;&#36136;&#39044;&#27979;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Complete and Efficient Graph Transformers for Crystal Material Property Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26230;&#20307;&#21333;&#20301;&#32990;&#30340;&#21608;&#26399;&#27169;&#24335;&#24314;&#31435;&#26230;&#26684;&#34920;&#31034;&#36827;&#34892;&#26230;&#20307;&#39640;&#25928;&#22270;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#26230;&#20307;&#26448;&#26009;&#30340;SE(3) transformer&#65292;&#21253;&#25324;iComFormer&#21644;eComFormer&#20004;&#20010;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#32467;&#26500;&#30001;&#27839;&#30528;&#25972;&#20010;&#19977;&#32500;&#31354;&#38388;&#23450;&#26399;&#37325;&#22797;&#30340;&#21407;&#32990;&#20869;&#30340;&#21407;&#23376;&#22522;&#22242;&#29305;&#24449;&#21270;&#12290;&#26230;&#20307;&#30340;&#21608;&#26399;&#24615;&#21644;&#26080;&#38480;&#24615;&#36136;&#20026;&#20960;&#20309;&#22270;&#34920;&#31034;&#23398;&#20064;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26500;&#24314;&#33021;&#22815;&#26377;&#25928;&#25429;&#33719;&#26230;&#20307;&#23436;&#25972;&#20960;&#20309;&#20449;&#24687;&#24182;&#22788;&#29702;&#25163;&#24615;&#26230;&#20307;&#30340;&#22270;&#24418;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#20301;&#32990;&#30340;&#21608;&#26399;&#27169;&#24335;&#20026;&#27599;&#20010;&#21407;&#23376;&#24314;&#31435;&#22522;&#20110;&#26230;&#26684;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26230;&#20307;&#30340;&#39640;&#25928;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#22270;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ComFormer&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#26230;&#20307;&#26448;&#26009;&#35774;&#35745;&#30340;SE(3)&#21464;&#21387;&#22120;&#12290;ComFormer&#21253;&#25324;&#20004;&#20010;&#21464;&#20307;&#65292;&#21363;&#20351;&#29992;&#19981;&#21464;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#35282;&#24230;&#30340;iComFormer&#20197;&#21450;&#21033;&#29992;&#31561;&#21464;&#30690;&#37327;&#34920;&#31034;&#30340;eComFormer&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11857v1 Announce Type: new  Abstract: Crystal structures are characterized by atomic bases within a primitive unit cell that repeats along a regular lattice throughout 3D space. The periodic and infinite nature of crystals poses unique challenges for geometric graph representation learning. Specifically, constructing graphs that effectively capture the complete geometric information of crystals and handle chiral crystals remains an unsolved and challenging problem. In this paper, we introduce a novel approach that utilizes the periodic patterns of unit cells to establish the lattice-based representation for each atom, enabling efficient and expressive graph representations of crystals. Furthermore, we propose ComFormer, a SE(3) transformer designed specifically for crystalline materials. ComFormer includes two variants; namely, iComFormer that employs invariant geometric descriptors of Euclidean distances and angles, and eComFormer that utilizes equivariant vector representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#34920;&#24449;&#19982;&#26368;&#20248;&#23545;&#20598;&#21464;&#37327;&#30456;&#20851;&#30340;Lagrange&#26368;&#23567;&#21270;&#22120;&#30340;&#32422;&#26463;&#36829;&#21453;&#26469;&#24357;&#21512;&#23454;&#36341;&#19982;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.11844</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;&#23398;&#20064;&#38382;&#39064;&#30340;&#36817;&#20284;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Solutions of Constrained Learning Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#34920;&#24449;&#19982;&#26368;&#20248;&#23545;&#20598;&#21464;&#37327;&#30456;&#20851;&#30340;Lagrange&#26368;&#23567;&#21270;&#22120;&#30340;&#32422;&#26463;&#36829;&#21453;&#26469;&#24357;&#21512;&#23454;&#36341;&#19982;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#38480;&#21046;&#23427;&#20204;&#34892;&#20026;&#30340;&#38656;&#27714;&#21464;&#24471;&#26085;&#30410;&#26126;&#26174;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#23545;&#28385;&#36275;&#40065;&#26834;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#20844;&#24179;&#24615;&#35201;&#27714;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#35201;&#27714;&#21487;&#20197;&#36890;&#36807;&#21046;&#23450;&#21463;&#38480;&#21046;&#30340;&#23398;&#20064;&#38382;&#39064;&#24182;&#36890;&#36807;&#23545;&#20598;&#19978;&#21319;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#20197;&#23454;&#29616;&#27867;&#21270;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#25910;&#25947;&#20110;&#30446;&#26631;&#20540;&#65292;&#20063;&#26080;&#27861;&#20445;&#35777;&#20854;&#32467;&#26524;&#26159;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#38656;&#35201;&#22312;&#25152;&#26377;&#36845;&#20195;&#19978;&#36827;&#34892;&#38543;&#26426;&#21270;&#65292;&#36825;&#22312;&#20219;&#20309;&#29616;&#20195;&#24212;&#29992;&#20013;&#20960;&#20046;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#30340;&#26368;&#32456;&#36845;&#20195;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#34920;&#24449;&#19982;&#26368;&#20248;&#23545;&#20598;&#21464;&#37327;&#30456;&#20851;&#30340;Lagrange&#26368;&#23567;&#21270;&#22120;&#30340;&#32422;&#26463;&#36829;&#21453;&#65292;&#23613;&#31649;&#32570;&#20047;&#20984;&#24615;&#65292;&#26469;&#35299;&#20915;&#23454;&#36341;&#19982;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11844v1 Announce Type: new  Abstract: With the widespread adoption of machine learning systems, the need to curtail their behavior has become increasingly apparent. This is evidenced by recent advancements towards developing models that satisfy robustness, safety, and fairness requirements. These requirements can be imposed (with generalization guarantees) by formulating constrained learning problems that can then be tackled by dual ascent algorithms. Yet, though these algorithms converge in objective value, even in non-convex settings, they cannot guarantee that their outcome is feasible. Doing so requires randomizing over all iterates, which is impractical in virtually any modern applications. Still, final iterates have been observed to perform well in practice. In this work, we address this gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables, despite lack of convexity. To do this, we le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#31895;&#31961;&#38598;&#21512;&#24230;&#37327;&#30340;&#26032;&#22411;Choquet&#36317;&#31163;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20351;&#24471;&#36317;&#31163;&#24230;&#37327;&#26356;&#21152;&#28789;&#27963;&#19982;&#20934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2403.11843</link><description>&lt;p&gt;
&#27169;&#31946;&#31895;&#31961;Choquet&#36317;&#31163;&#29992;&#20110;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Rough Choquet Distances for Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#31895;&#31961;&#38598;&#21512;&#24230;&#37327;&#30340;&#26032;&#22411;Choquet&#36317;&#31163;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20351;&#24471;&#36317;&#31163;&#24230;&#37327;&#26356;&#21152;&#28789;&#27963;&#19982;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#31895;&#31961;&#38598;&#21512;&#24230;&#37327;&#30340;&#26032;&#22411;Choquet&#36317;&#31163;&#12290;&#25152;&#25552;&#20986;&#30340;&#36317;&#31163;&#24230;&#37327;&#32467;&#21512;&#20102;&#20174;&#27169;&#31946;&#31895;&#31961;&#38598;&#29702;&#35770;&#20013;&#33719;&#24471;&#30340;&#23646;&#24615;&#20449;&#24687;&#21644;Choquet&#31215;&#20998;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#36866;&#24212;&#24615;&#22320;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#35748;&#35782;&#21040;&#26465;&#20214;&#23646;&#24615;&#19982;&#20915;&#31574;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23454;&#29616;&#26356;&#28789;&#27963;&#19982;&#20934;&#30830;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#24378;&#35843;&#22522;&#20110;&#36317;&#31163;&#30340;&#20998;&#31867;&#26041;&#27861;&#65288;&#20363;&#22914;k&#26368;&#36817;&#37051;&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#22522;&#20110;&#27491;&#21306;&#22495;&#30340;&#27169;&#31946;&#31895;&#31961;&#38598;&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#20004;&#31181;&#26681;&#25454;&#27169;&#31946;&#31895;&#31961;&#38598;&#29702;&#35770;&#23548;&#20986;&#30340;&#29992;&#20110;&#36866;&#29992;Choquet&#31215;&#20998;&#30340;&#21333;&#35843;&#21270;&#31243;&#24207;&#65292;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11843v1 Announce Type: cross  Abstract: This paper introduces a novel Choquet distance using fuzzy rough set based measures. The proposed distance measure combines the attribute information received from fuzzy rough set theory with the flexibility of the Choquet integral. This approach is designed to adeptly capture non-linear relationships within the data, acknowledging the interplay of the conditional attributes towards the decision attribute and resulting in a more flexible and accurate distance. We explore its application in the context of machine learning, with a specific emphasis on distance-based classification approaches (e.g. k-nearest neighbours). The paper examines two fuzzy rough set based measures that are based on the positive region. Moreover, we explore two procedures for monotonizing the measures derived from fuzzy rough set theory, making them suitable for use with the Choquet integral, and investigate their differences.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;PESCAL&#65292;&#21033;&#29992;&#22522;&#20110;&#21069;&#38376;&#26631;&#20934;&#30340;&#20013;&#20171;&#21464;&#37327;&#28040;&#38500;&#28151;&#26434;&#20559;&#24046;&#65292;&#24182;&#37319;&#29992;&#24754;&#35266;&#21407;&#21017;&#22788;&#29702;&#20505;&#36873;&#31574;&#30053;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.11841</link><description>&lt;p&gt;
&#22522;&#20110;&#20013;&#20171;&#22240;&#32032;&#30340;&#24754;&#35266;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28151;&#26434;&#30340;&#31163;&#32447;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11841
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;PESCAL&#65292;&#21033;&#29992;&#22522;&#20110;&#21069;&#38376;&#26631;&#20934;&#30340;&#20013;&#20171;&#21464;&#37327;&#28040;&#38500;&#28151;&#26434;&#20559;&#24046;&#65292;&#24182;&#37319;&#29992;&#24754;&#35266;&#21407;&#21017;&#22788;&#29702;&#20505;&#36873;&#31574;&#30053;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#38543;&#26426;&#23454;&#39564;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#21463;&#21040;&#26102;&#38388;&#21644;&#39044;&#31639;&#38480;&#21046;&#32780;&#35268;&#27169;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#25104;&#20026;&#23454;&#29616;&#39640;&#36136;&#37327;&#31574;&#30053;&#23398;&#20064;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#20851;&#38190;&#20551;&#35774;-- &#38750;&#28151;&#26434;&#24615;&#21644;&#27491;&#24615;-- &#36825;&#20004;&#20010;&#20551;&#35774;&#22312;&#35266;&#27979;&#25968;&#25454;&#29615;&#22659;&#20013;&#32463;&#24120;&#19981;&#25104;&#31435;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#24754;&#35266;&#22240;&#26524;&#23398;&#20064;&#65288;PESCAL&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#21069;&#38376;&#26631;&#20934;&#30340;&#20013;&#20171;&#21464;&#37327;&#26469;&#28040;&#38500;&#28151;&#26434;&#20559;&#24046;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#24754;&#35266;&#21407;&#21017;&#26469;&#35299;&#20915;&#30001;&#20505;&#36873;&#31574;&#30053;&#24341;&#36215;&#30340;&#21160;&#20316;&#20998;&#24067;&#19982;&#29983;&#25104;&#35266;&#27979;&#25968;&#25454;&#30340;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#36890;&#36807;&#34701;&#21512;&#36741;&#21161;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11841v1 Announce Type: cross  Abstract: In real-world scenarios, datasets collected from randomized experiments are often constrained by size, due to limitations in time and budget. As a result, leveraging large observational datasets becomes a more attractive option for achieving high-quality policy learning. However, most existing offline reinforcement learning (RL) methods depend on two key assumptions--unconfoundedness and positivity--which frequently do not hold in observational data contexts. Recognizing these challenges, we propose a novel policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the mediator variable based on front-door criterion to remove the confounding bias; additionally, we adopt the pessimistic principle to address the distributional shift between the action distributions induced by candidate policies, and the behavior policy that generates the observational data. Our key observation is that, by incorporating auxiliary variable
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#21644;&#32467;&#26500;&#30340;&#20505;&#36873;&#27169;&#22411;&#22312;&#22810;&#20010;&#31185;&#23398;&#12289;&#29702;&#35770;&#21644;&#23454;&#36341;&#26631;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.11840</link><description>&lt;p&gt;
&#23558;&#22810;&#26631;&#20934;&#27604;&#36739;&#20316;&#20026;&#25512;&#21160;&#30693;&#35782;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Criteria Comparison as a Method of Advancing Knowledge-Guided Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11840
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#21644;&#32467;&#26500;&#30340;&#20505;&#36873;&#27169;&#22411;&#22312;&#22810;&#20010;&#31185;&#23398;&#12289;&#29702;&#35770;&#21644;&#23454;&#36341;&#26631;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#35780;&#20272;AI/ML&#27169;&#22411;&#22312;&#21253;&#25324;&#26680;&#24515;&#31185;&#23398;&#21407;&#21017;&#21644;&#26356;&#23454;&#38469;&#32467;&#26524;&#22312;&#20869;&#30340;&#22810;&#20010;&#26631;&#20934;&#19978;&#12290;&#35813;&#26041;&#27861;&#28304;&#20110;&#24515;&#29702;&#23398;&#21644;&#20915;&#31574;&#31185;&#23398;&#20013;&#30340;&#39044;&#27979;&#31454;&#36187;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#19981;&#21516;&#31867;&#22411;&#21644;&#32467;&#26500;&#30340;&#20505;&#36873;&#27169;&#22411;&#22312;&#22810;&#20010;&#31185;&#23398;&#12289;&#29702;&#35770;&#21644;&#23454;&#36341;&#26631;&#20934;&#19978;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#39046;&#22495;&#30340;&#25237;&#31080;&#35268;&#21017;&#26469;&#35780;&#20272;&#26631;&#20934;&#24471;&#20998;&#30340;&#24207;&#25968;&#25490;&#21517;&#65292;&#20174;&#32780;&#27604;&#36739;&#22810;&#26679;&#21270;&#25514;&#26045;&#21644;&#31867;&#22411;&#30340;&#27169;&#22411;&#22312;&#25972;&#20307;&#35780;&#20272;&#20013;&#30340;&#25928;&#26524;&#12290;&#36824;&#35752;&#35770;&#20102;&#39069;&#22806;&#30340;&#20248;&#21183;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11840v1 Announce Type: new  Abstract: This paper describes a generalizable model evaluation method that can be adapted to evaluate AI/ML models across multiple criteria including core scientific principles and more practical outcomes. Emerging from prediction competitions in Psychology and Decision Science, the method evaluates a group of candidate models of varying type and structure across multiple scientific, theoretic, and practical criteria. Ordinal ranking of criteria scores are evaluated using voting rules from the field of computational social choice and allow the comparison of divergent measures and types of models in a holistic evaluation. Additional advantages and applications are discussed.
&lt;/p&gt;</description></item><item><title>&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#26377;&#21161;&#20110;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26500;&#25104;&#27010;&#25324;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.11834</link><description>&lt;p&gt;
&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26500;&#25104;&#27010;&#25324;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Relationship between In-context Learning and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11834
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#26377;&#21161;&#20110;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26500;&#25104;&#27010;&#25324;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#26500;&#25104;&#27010;&#25324;&#21407;&#21017;&#65292;&#22797;&#26434;&#34920;&#36798;&#30340;&#21547;&#20041;&#21487;&#20197;&#29702;&#35299;&#20026;&#20854;&#37096;&#20998;&#21547;&#20041;&#21450;&#23427;&#20204;&#22914;&#20309;&#32452;&#21512;&#30340;&#20989;&#25968;&#12290;&#36825;&#19968;&#21407;&#21017;&#23545;&#20110;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#65292;&#21487;&#20197;&#35828;&#23545;&#20110;&#38754;&#23545;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;NLP&#27169;&#22411;&#20063;&#26159;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;Transformer&#65292;&#22312;&#26500;&#25104;&#27010;&#25324;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20551;&#35774;&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#24402;&#32435;&#20559;&#35265;&#20197;&#20419;&#36827;&#26500;&#25104;&#27010;&#25324;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20351;&#26222;&#36890;&#23398;&#20064;&#38750;&#24120;&#22256;&#38590;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#20102;&#19968;&#20010;&#22240;&#26524;Transformer&#65306;&#25105;&#20204;&#21521;&#20854;&#25552;&#20379;&#19981;&#21516;&#25490;&#24207;&#30340;&#35757;&#32451;&#23454;&#20363;&#24182;&#27927;&#29260;&#23454;&#20363;&#26631;&#31614;&#12290;&#36825;&#30456;&#24403;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#27169;&#22411;&#35299;&#20915;&#25152;&#26377;&#21487;&#33021;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#12290;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#65292;&#28982;&#32780;&#65292;&#36890;&#36807;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11834v1 Announce Type: new  Abstract: According to the principle of compositional generalization, the meaning of a complex expression can be understood as a function of the meaning of its parts and of how they are combined. This principle is crucial for human language processing and also, arguably, for NLP models in the face of out-of-distribution data. However, many neural network models, including Transformers, have been shown to struggle with compositional generalization. In this paper, we hypothesize that forcing models to in-context learn can provide an inductive bias to promote compositional generalization. To test this hypothesis, we train a causal Transformer in a setting that renders ordinary learning very difficult: we present it with different orderings of the training instance and shuffle instance labels. This corresponds to training the model on all possible few-shot learning problems attainable from the dataset. The model can solve the task, however, by utilizi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SSCAE&#30340;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#38408;&#20540;&#21644;&#26412;&#22320;&#36138;&#23146;&#25628;&#32034;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.11833</link><description>&lt;p&gt;
SSCAE -- &#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SSCAE&#30340;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#38408;&#20540;&#21644;&#26412;&#22320;&#36138;&#23146;&#25628;&#32034;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;AEs&#65289;&#30340;&#24433;&#21709;&#12290;&#29992;AEs&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#20854;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24320;&#21457;&#39640;&#36136;&#37327;AEs&#30340;&#27169;&#22411;&#27604;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#35201;&#24930;&#24471;&#22810;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SSCAE&#30340;&#23454;&#29992;&#21644;&#39640;&#25928;&#30340;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#28982;&#35821;&#35328;AE&#29983;&#25104;&#22120;&#12290;SSCAE&#35782;&#21035;&#37325;&#35201;&#21333;&#35789;&#24182;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26089;&#26399;&#26367;&#25442;&#38598;&#12290;&#25509;&#30528;&#65292;&#20351;&#29992;&#20004;&#20010;&#33879;&#21517;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#21021;&#22987;&#38598;&#21512;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#65288;1&#65289;&#21160;&#24577;&#38408;&#20540;&#26469;&#25429;&#33719;&#26356;&#39640;&#25928;&#30340;&#25200;&#21160;&#20197;&#21450;&#65288;2&#65289;&#26412;&#22320;&#36138;&#23146;&#25628;&#32034;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11833v1 Announce Type: new  Abstract: Machine learning models are vulnerable to maliciously crafted Adversarial Examples (AEs). Training a machine learning model with AEs improves its robustness and stability against adversarial attacks. It is essential to develop models that produce high-quality AEs. Developing such models has been much slower in natural language processing (NLP) than in areas such as computer vision. This paper introduces a practical and efficient adversarial attack model called SSCAE for \textbf{S}emantic, \textbf{S}yntactic, and \textbf{C}ontext-aware natural language \textbf{AE}s generator. SSCAE identifies important words and uses a masked language model to generate an early set of substitutions. Next, two well-known language models are employed to evaluate the initial set in terms of semantic and syntactic characteristics. We introduce (1) a dynamic threshold to capture more efficient perturbations and (2) a local greedy search to generate high-qualit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;&#20219;&#21153;&#25193;&#23637;&#20026;&#20855;&#26377;&#36317;&#31163;&#20272;&#35745;&#30340;3D SELD&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#38598;&#25104;&#36317;&#31163;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;Ambisonic&#21644;&#21452;&#32819;&#29256;&#26412;&#30340;&#22768;&#38899;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.11827</link><description>&lt;p&gt;
&#20855;&#26377;&#36317;&#31163;&#20272;&#35745;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Sound Event Detection and Localization with Distance Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;&#20219;&#21153;&#25193;&#23637;&#20026;&#20855;&#26377;&#36317;&#31163;&#20272;&#35745;&#30340;3D SELD&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#38598;&#25104;&#36317;&#31163;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;Ambisonic&#21644;&#21452;&#32819;&#29256;&#26412;&#30340;&#22768;&#38899;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;(SELD)&#26159;&#35782;&#21035;&#22768;&#38899;&#20107;&#20214;&#21450;&#20854;&#23545;&#24212;&#21040;&#36798;&#26041;&#21521;(DOA)&#30340;&#32508;&#21512;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#19968;&#20219;&#21153;&#22312;&#36817;&#24180;&#26469;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#24182;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20294;&#23427;&#26410;&#33021;&#25552;&#20379;&#26377;&#20851;&#22768;&#28304;&#20301;&#32622;&#30340;&#23436;&#25972;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20219;&#21153;&#25193;&#23637;&#20026;&#20855;&#26377;&#36317;&#31163;&#20272;&#35745;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#12289;&#23450;&#20301;(3D SELD)&#26469;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#38598;&#25104;&#36317;&#31163;&#20272;&#35745;&#22312;SELD&#26680;&#24515;&#20013;&#30340;&#26041;&#27861; - &#19968;&#31181;&#26159;&#22810;&#20219;&#21153;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#29420;&#27169;&#22411;&#36755;&#20986;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#23558;&#22810;ACCDOA&#26041;&#27861;&#25193;&#23637;&#20197;&#21253;&#25324;&#36317;&#31163;&#20449;&#24687;&#32780;&#33719;&#24471;&#30340;&#21333;&#20219;&#21153;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;Ambisonic&#21644;&#21452;&#32819;&#29256;&#26412;&#30340;STARSS23&#65306;Sony-TAU Realistic Spatial Soundscapes 2023&#24320;&#23637;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#19982;&#36317;&#31163;&#20272;&#35745;&#37096;&#20998;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11827v1 Announce Type: cross  Abstract: Sound Event Detection and Localization (SELD) is a combined task of identifying sound events and their corresponding direction-of-arrival (DOA). While this task has numerous applications and has been extensively researched in recent years, it fails to provide full information about the sound source position. In this paper, we overcome this problem by extending the task to Sound Event Detection, Localization with Distance Estimation (3D SELD). We study two ways of integrating distance estimation within the SELD core - a multi-task approach, in which the problem is tackled by a separate model output, and a single-task approach obtained by extending the multi-ACCDOA method to include distance information. We investigate both methods for the Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial Soundscapes 2023. Moreover, our study involves experiments on the loss function related to the distance estimation part. Our resu
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#33014;&#22218;&#23618;&#30340;&#26550;&#26500;&#20462;&#25913;&#65292;&#37197;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23558;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#29305;&#24449;&#25972;&#21512;&#36827;&#20998;&#26512;&#65292;&#20026;&#39640;&#32423;&#23545;&#35937;&#26631;&#35760;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.11826</link><description>&lt;p&gt;
CapsLorentzNet: &#23558;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#29305;&#24449;&#19982;&#22270;&#21367;&#31215;&#30456;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11826
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#33014;&#22218;&#23618;&#30340;&#26550;&#26500;&#20462;&#25913;&#65292;&#37197;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23558;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#29305;&#24449;&#25972;&#21512;&#36827;&#20998;&#26512;&#65292;&#20026;&#39640;&#32423;&#23545;&#35937;&#26631;&#35760;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#25552;&#21319;&#23545;&#35937;&#26631;&#35760;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19982;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#20860;&#23481;&#30340;&#26032;&#39062;&#26550;&#26500;&#20462;&#25913;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#24352;&#22312;&#26631;&#20934;GNN&#20013;&#26367;&#25442;&#20256;&#32479;&#35299;&#30721;&#22359;&#20197;&#38598;&#25104;&#33014;&#22218;&#23618;&#12290;&#36825;&#20123;&#33014;&#22218;&#26159;&#20855;&#26377;&#21521;&#37327;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#32452;&#12290;&#36825;&#20123;&#21521;&#37327;&#30340;&#26041;&#21521;&#34920;&#31034;&#34987;&#30740;&#31350;&#23545;&#35937;&#30340;&#37325;&#35201;&#23646;&#24615;&#65292;&#20854;&#22823;&#23567;&#34920;&#24449;&#34987;&#30740;&#31350;&#23545;&#35937;&#26159;&#21542;&#23646;&#20110;&#30001;&#33014;&#22218;&#20195;&#34920;&#30340;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#33014;&#22218;&#32593;&#32476;&#32467;&#21512;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26500;&#26426;&#21046;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20419;&#36827;&#20102;&#19987;&#23478;&#35774;&#35745;&#30340;&#39640;&#32423;&#29305;&#24449;&#26080;&#32541;&#34701;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#24050;&#32463;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#19982;LorentzNet&#26550;&#26500;&#22312;&#22840;&#20811;&#33014;&#23376;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11826v1 Announce Type: cross  Abstract: With the advent of advanced machine learning techniques, boosted object tagging has witnessed significant progress. In this article, we take this field further by introducing novel architectural modifications compatible with a wide array of Graph Neural Network (GNN) architectures. Our approach advocates for integrating capsule layers, replacing the conventional decoding blocks in standard GNNs. These capsules are a group of neurons with vector activations. The orientation of these vectors represents important properties of the objects under study, with their magnitude characterizing whether the object under study belongs to the class represented by the capsule. Moreover, capsule networks incorporate a regularization by reconstruction mechanism, facilitating the seamless integration of expert-designed high-level features into the analysis. We have studied the usefulness of our architecture with the LorentzNet architecture for quark-glu
&lt;/p&gt;</description></item><item><title>ZIP-DL&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.11795</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low-Cost Privacy-Aware Decentralized Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11795
&lt;/p&gt;
&lt;p&gt;
ZIP-DL&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;ZIP-DL&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#12290;&#36825;&#31181;&#25216;&#26415;&#30830;&#20445;&#20102;&#30001;&#20110;&#20854;&#30456;&#20851;&#24615;&#65292;&#22312;&#32858;&#21512;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#20960;&#20046;&#30456;&#20114;&#25269;&#28040;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;ZIP-DL&#19981;&#38656;&#35201;&#22810;&#27425;&#36890;&#20449;&#36718;&#36827;&#34892;&#22122;&#22768;&#25269;&#28040;&#65292;&#35299;&#20915;&#20102;&#38544;&#31169;&#20445;&#25252;&#19982;&#36890;&#20449;&#24320;&#38144;&#20043;&#38388;&#30340;&#24120;&#35265;&#26435;&#34913;&#12290;&#25105;&#20204;&#20026;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20174;&#32780;&#20351;ZIP-DL&#21487;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;ZIP-DL&#22312;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#26435;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#22522;&#32447;DL&#30456;&#27604;&#65292;ZIP-DL&#65288;i&#65289;&#23558;&#21487;&#36861;&#36394;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#38477;&#20302;&#20102;&#22810;&#36798;52&#20010;&#28857;&#65292;&#65288;ii&#65289;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;37&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11795v1 Announce Type: new  Abstract: This paper introduces ZIP-DL, a novel privacy-aware decentralized learning (DL) algorithm that relies on adding correlated noise to each model update during the model training process. This technique ensures that the added noise almost neutralizes itself during the aggregation process due to its correlation, thus minimizing the impact on model accuracy. In addition, ZIP-DL does not require multiple communication rounds for noise cancellation, addressing the common trade-off between privacy protection and communication overhead. We provide theoretical guarantees for both convergence speed and privacy guarantees, thereby making ZIP-DL applicable to practical scenarios. Our extensive experimental study shows that ZIP-DL achieves the best trade-off between vulnerability and accuracy. In particular, ZIP-DL (i) reduces the effectiveness of a linkability attack by up to 52 points compared to baseline DL, and (ii) achieves up to 37 more accuracy
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11782</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#20174;&#20559;&#22909;&#21644;&#36873;&#25321;&#20013;&#23398;&#20064;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A tutorial on learning from preferences and choices with Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24314;&#27169;&#20301;&#20110;&#32463;&#27982;&#23398;&#12289;&#20915;&#31574;&#29702;&#35770;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#30340;&#20559;&#22909;&#21450;&#20854;&#36873;&#25321;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#26356;&#25509;&#36817;&#20182;&#20204;&#26399;&#26395;&#30340;&#20135;&#21697;&#65292;&#20026;&#36328;&#39046;&#22495;&#30340;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#24212;&#29992;&#38138;&#24179;&#36947;&#36335;&#12290;&#27492;&#25945;&#31243;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#36830;&#36143;&#12289;&#20840;&#38754;&#30340;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#28436;&#31034;&#22914;&#20309;&#23558;&#29702;&#24615;&#21407;&#21017;&#65288;&#26469;&#33258;&#32463;&#27982;&#23398;&#21644;&#20915;&#31574;&#29702;&#35770;&#65289;&#26080;&#32541;&#22320;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#21512;&#36866;&#22320;&#23450;&#21046;&#20284;&#28982;&#20989;&#25968;&#65292;&#36825;&#19968;&#26694;&#26550;&#20351;&#24471;&#33021;&#22815;&#26500;&#24314;&#28085;&#30422;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#12289;&#36776;&#35782;&#38480;&#21046;&#21644;&#23545;&#35937;&#21644;&#26631;&#31614;&#20559;&#22909;&#30340;&#22810;&#37325;&#20914;&#31361;&#25928;&#29992;&#24773;&#26223;&#30340;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11780</link><description>&lt;p&gt;
Prompt-Singer: &#24102;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#21487;&#25511;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;(SVS)&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#33258;&#28982;&#24230;&#65292;&#28982;&#32780;&#23427;&#20204;&#32570;&#20047;&#26174;&#24335;&#25511;&#21046;&#21512;&#25104;&#21809;&#27468;&#39118;&#26684;&#23646;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;SVS&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20165;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#20855;&#26377;&#22810;&#23610;&#24230;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#31163;&#38899;&#39640;&#34920;&#31034;&#30340;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#22495;&#25511;&#21046;&#21516;&#26102;&#20445;&#25345;&#20102;&#26059;&#24459;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#24494;&#35843;&#65292;&#20197;&#21450;&#24341;&#20837;&#35821;&#38899;&#25968;&#25454;&#20197;&#20943;&#36731;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26088;&#22312;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#38899;&#39057;&#36136;&#37327;&#12290;&#38899;&#39057;&#31034;&#20363;&#21487;&#35775;&#38382; http://prompt-singer.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#23454;&#26102;&#36890;&#20449;&#24179;&#21488;&#20013;&#20351;&#29992;&#38745;&#24577;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#20004;&#20010;&#22522;&#20110;Resnet&#21644;LCNN&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#24179;&#21488;&#20013;&#30340;&#23454;&#26102;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#65292;&#20026;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#30830;&#20445;&#36890;&#20449;&#23433;&#20840;&#25552;&#20379;&#20102;&#31574;&#30053;&#21644;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.11778</link><description>&lt;p&gt;
&#22312;&#36890;&#20449;&#24179;&#21488;&#20013;&#24320;&#21457;&#23454;&#26102;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31995;&#32479;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11778
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#23454;&#26102;&#36890;&#20449;&#24179;&#21488;&#20013;&#20351;&#29992;&#38745;&#24577;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#20004;&#20010;&#22522;&#20110;Resnet&#21644;LCNN&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#24179;&#21488;&#20013;&#30340;&#23454;&#26102;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#65292;&#20026;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#30830;&#20445;&#36890;&#20449;&#23433;&#20840;&#25552;&#20379;&#20102;&#31574;&#30053;&#21644;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#22312;&#36890;&#20449;&#24179;&#21488;&#20013;&#26500;&#25104;&#19981;&#26029;&#19978;&#21319;&#30340;&#23041;&#32961;&#65292;&#22240;&#27492;&#38656;&#35201;&#23454;&#26102;&#26816;&#27979;&#20197;&#30830;&#20445;&#38899;&#39057;&#27969;&#30340;&#23436;&#25972;&#24615;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#23454;&#26102;&#36890;&#20449;&#24179;&#21488;&#20013;&#20351;&#29992;&#38745;&#24577;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#24320;&#21457;&#20102;&#21487;&#22312;&#19981;&#21516;&#24179;&#21488;&#19978;&#36816;&#34892;&#30340;&#21487;&#25191;&#34892;&#36719;&#20214;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25191;&#34892;&#12290;&#22522;&#20110;Resnet&#21644;LCNN&#26550;&#26500;&#30340;&#20004;&#20010;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#27169;&#22411;&#20351;&#29992;ASVspoof 2019&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#29616;&#65292;&#30456;&#36739;&#20110;ASVspoof 2019&#25361;&#25112;&#22522;&#20934;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#31574;&#30053;&#21644;&#26694;&#26550;&#65292;&#20026;&#22312;&#36890;&#20449;&#24179;&#21488;&#20013;&#23454;&#29616;&#23454;&#26102;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36825;&#39033;&#24037;&#20316;&#20419;&#36827;&#20102;&#38899;&#39057;&#27969;&#23433;&#20840;&#24615;&#30340;&#21457;&#23637;&#65292;&#30830;&#20445;&#22312;&#21160;&#24577;&#30340;&#23454;&#26102;&#36890;&#20449;&#22330;&#26223;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11778v1 Announce Type: cross  Abstract: Deepfake audio poses a rising threat in communication platforms, necessitating real-time detection for audio stream integrity. Unlike traditional non-real-time approaches, this study assesses the viability of employing static deepfake audio detection models in real-time communication platforms. An executable software is developed for cross-platform compatibility, enabling real-time execution. Two deepfake audio detection models based on Resnet and LCNN architectures are implemented using the ASVspoof 2019 dataset, achieving benchmark performances compared to ASVspoof 2019 challenge baselines. The study proposes strategies and frameworks for enhancing these models, paving the way for real-time deepfake audio detection in communication platforms. This work contributes to the advancement of audio stream security, ensuring robust detection capabilities in dynamic, real-time communication scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#23454;&#29616;&#33041;&#30005;&#20449;&#21495;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;Signal-JEPA&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31934;&#30830;&#19979;&#28216;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11772</link><description>&lt;p&gt;
S-JEPA&#65306;&#36890;&#36807;&#21160;&#24577;&#31354;&#38388;&#27880;&#24847;&#21147;&#23454;&#29616;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#23454;&#29616;&#33041;&#30005;&#20449;&#21495;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;Signal-JEPA&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31934;&#30830;&#19979;&#28216;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#33041;&#30005;&#20449;&#21495;&#22788;&#29702;&#20013;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#36801;&#31227;&#23398;&#20064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#33041;&#30005;&#20449;&#21495;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#30340;Signal-JEPA&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#29305;&#23450;&#31354;&#38388;&#22359;&#25513;&#34109;&#31574;&#30053;&#21644;&#19977;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#19979;&#28216;&#20998;&#31867;&#30340;&#26550;&#26500;&#12290;&#35813;&#30740;&#31350;&#22312;&#19968;&#20010;54&#20010;&#21463;&#35797;&#32773;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;BCI&#33539;&#24335;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;&#36816;&#21160;&#24819;&#35937;&#12289;ERP&#21644;SSVEP&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;JEPAs&#22312;&#33041;&#30005;&#20449;&#21495;&#32534;&#30721;&#20013;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#31354;&#38388;&#28388;&#27874;&#23545;&#20934;&#30830;&#19979;&#28216;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11772v1 Announce Type: cross  Abstract: Motivated by the challenge of seamless cross-dataset transfer in EEG signal processing, this article presents an exploratory study on the use of Joint Embedding Predictive Architectures (JEPAs). In recent years, self-supervised learning has emerged as a promising approach for transfer learning in various domains. However, its application to EEG signals remains largely unexplored. In this article, we introduce Signal-JEPA for representing EEG recordings which includes a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification. The study is conducted on a 54~subjects dataset and the downstream performance of the models is evaluated on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study provides preliminary evidence for the potential of JEPAs in EEG signal encoding. Notably, our results highlight the importance of spatial filtering for accurate downstream classific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#38899;&#39057;&#35270;&#35273;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#20272;&#35745;&#30340;&#39640;&#25928;&#29305;&#24449;&#25552;&#21462;&#21644;&#24310;&#36831;&#34701;&#21512;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.11757</link><description>&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#20272;&#35745;&#30340;&#39640;&#25928;&#29305;&#24449;&#25552;&#21462;&#21644;&#24310;&#36831;&#34701;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Feature Extraction and Late Fusion Strategy for Audiovisual Emotional Mimicry Intensity Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#38899;&#39057;&#35270;&#35273;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#20272;&#35745;&#30340;&#39640;&#25928;&#29305;&#24449;&#25552;&#21462;&#21644;&#24310;&#36831;&#34701;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#65288;EMI&#65289;&#20272;&#35745;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#25361;&#25112;&#26159;&#31532;&#20845;&#23626;&#38754;&#21521;&#37326;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#65288;ABAW&#65289;&#31454;&#36187;&#30340;&#19968;&#37096;&#20998;&#12290;EMI&#20272;&#35745;&#25361;&#25112;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20174;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#24773;&#32490;&#31867;&#21035;&#65288;&#21363;&#65292;&#8220;&#23815;&#25308;&#8221;&#65292;&#8220;&#23089;&#20048;&#8221;&#65292;&#8220;&#20915;&#24515;&#8221;&#65292;&#8220;&#20849;&#24773;&#24615;&#30140;&#30171;&#8221;&#65292;&#8220;&#20852;&#22859;&#8221;&#21644;&#8220;&#21916;&#24742;&#8221;&#65289;&#20013;&#35780;&#20272;&#23427;&#20204;&#26469;&#35780;&#20272;&#31181;&#23376;&#35270;&#39057;&#30340;&#24773;&#24863;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11757v1 Announce Type: cross  Abstract: In this paper, we present the solution to the Emotional Mimicry Intensity (EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis in-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to evaluate the emotional intensity of seed videos by assessing them from a set of predefined emotion categories (i.e., "Admiration," "Amusement," "Determination," "Empathic Pain," "Excitement," and "Joy").
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11755</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#25552;&#31034;&#33258;&#21160;&#21270;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#30340;&#25552;&#31034;&#38598;&#25104;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#20026;LLMs&#35774;&#35745;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;VLM&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#25163;&#21160;&#32534;&#20889;&#36825;&#20123;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65292;&#32780;&#19988;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#19982;&#24863;&#20852;&#36259;&#31867;&#21035;&#30456;&#20851;&#30340;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#20219;&#21153;&#29305;&#23450;&#39118;&#26684;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#25490;&#38500;&#22312;&#24490;&#29615;&#20043;&#22806;&#65292;&#24182;&#23436;&#20840;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#20803;&#25552;&#31034;&#65288;MPVR&#65289;&#12290;&#20165;&#20197;&#30446;&#26631;&#20219;&#21153;&#30340;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24418;&#24335;&#20197;&#21450;&#19968;&#31995;&#21015;&#30456;&#20851;&#31867;&#21035;&#26631;&#31614;&#20316;&#20026;&#36755;&#20837;&#65292;MPVR&#33258;&#21160;&#20135;&#29983;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#31867;&#21035;&#25552;&#31034;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36716;&#23548;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;PARMESAN&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26080;&#21442;&#25968;&#20869;&#23384;&#25628;&#32034;&#21644;&#36716;&#23548;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#26080;&#38656;&#36830;&#32493;&#35757;&#32451;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.11743</link><description>&lt;p&gt;
PARMESAN: &#29992;&#20110;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26080;&#21442;&#25968;&#20869;&#23384;&#25628;&#32034;&#19982;&#36716;&#23548;
&lt;/p&gt;
&lt;p&gt;
PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11743
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36716;&#23548;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;PARMESAN&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26080;&#21442;&#25968;&#20869;&#23384;&#25628;&#32034;&#21644;&#36716;&#23548;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#26080;&#38656;&#36830;&#32493;&#35757;&#32451;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36716;&#23548;&#25512;&#29702;&#26469;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#28789;&#27963;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PARMESAN&#65288;&#26080;&#21442;&#25968;&#20869;&#23384;&#25628;&#32034;&#19982;&#36716;&#23548;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36716;&#23548;&#26041;&#27861;&#65292;&#21033;&#29992;&#20869;&#23384;&#27169;&#22359;&#26469;&#35299;&#20915;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#20869;&#23384;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#34987;&#25628;&#32034;&#20197;&#25214;&#21040;&#30456;&#24212;&#30340;&#31034;&#20363;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;PARMESAN&#36890;&#36807;&#20462;&#25913;&#20869;&#23384;&#20869;&#23481;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#36830;&#32493;&#35757;&#32451;&#25110;&#24494;&#35843;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24120;&#29992;&#30340;&#31070;&#32463;&#32467;&#26500;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11743v1 Announce Type: new  Abstract: In this work we address flexibility in deep learning by means of transductive reasoning. For adaptation to new tasks or new data, existing methods typically involve tuning of learnable parameters or even complete re-training from scratch, rendering such approaches unflexible in practice. We argue that the notion of separating computation from memory by the means of transduction can act as a stepping stone for solving these issues. We therefore propose PARMESAN (parameter-free memory search and transduction), a scalable transduction method which leverages a memory module for solving dense prediction tasks. At inference, hidden representations in memory are being searched to find corresponding examples. In contrast to other methods, PARMESAN learns without the requirement for any continuous training or fine-tuning of learnable parameters simply by modifying the memory content. Our method is compatible with commonly used neural architecture
&lt;/p&gt;</description></item><item><title>LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2403.11735</link><description>&lt;p&gt;
LSKNet&#65306;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#30340;&#36731;&#37327;&#32423;&#22522;&#30784;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
LSKNet: A Foundation Lightweight Backbone for Remote Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11735
&lt;/p&gt;
&lt;p&gt;
LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#23545;&#19979;&#28216;&#20219;&#21153;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#36965;&#24863;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#24573;&#35270;&#20102;&#23884;&#20837;&#22312;&#36965;&#24863;&#22330;&#26223;&#20013;&#30340;&#23453;&#36149;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#21487;&#33021;&#20250;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#21442;&#32771;&#36275;&#22815;&#38271;&#31243;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#36965;&#24863;&#23545;&#35937;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#35782;&#21035;&#65292;&#32780;&#36825;&#21487;&#20197;&#22240;&#19981;&#21516;&#23545;&#35937;&#32780;&#24322;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#65288;LSKNet&#65289;&#39592;&#24178;&#32593;&#32476;&#12290;LSKNet&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20808;&#21069;&#23578;&#26410;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#25506;&#32034;&#36807;&#22823;&#22411;&#21644;&#36873;&#25321;&#24615;&#26680;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#27809;&#26377;&#22826;&#22810;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11735v1 Announce Type: cross  Abstract: Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightw
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#29256;&#26412;&#30340;&#20851;&#31995;GNNs&#65292;&#36890;&#36807;&#22312;$t$&#20026;&#26080;&#31351;&#22823;&#26102;&#20165;&#20351;&#29992;&#20108;&#27425;&#31354;&#38388;&#30340;&#23884;&#20837;&#26469;&#36817;&#20284;$3$-GNNs&#65292;&#23545;&#20110;&#36739;&#20302;&#30340;$t$&#20540;&#65292;&#36890;&#36807;&#20132;&#25442;&#36739;&#23569;&#30340;&#28040;&#24687;&#23454;&#29616;&#24369;&#30340;&#36817;&#20284;&#65292;&#21516;&#26102;&#36890;&#24120;&#20135;&#29983;&#20102;&#20960;&#20010;&#35268;&#21010;&#39046;&#22495;&#20013;&#25152;&#38656;&#30340;$C_3$&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.11734</link><description>&lt;p&gt;
&#23398;&#20064;&#21476;&#20856;&#35268;&#21010;&#39046;&#22495;&#30340;&#36890;&#29992;&#31574;&#30053;&#65306;&#36229;&#36234;$C_2$
&lt;/p&gt;
&lt;p&gt;
Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#29256;&#26412;&#30340;&#20851;&#31995;GNNs&#65292;&#36890;&#36807;&#22312;$t$&#20026;&#26080;&#31351;&#22823;&#26102;&#20165;&#20351;&#29992;&#20108;&#27425;&#31354;&#38388;&#30340;&#23884;&#20837;&#26469;&#36817;&#20284;$3$-GNNs&#65292;&#23545;&#20110;&#36739;&#20302;&#30340;$t$&#20540;&#65292;&#36890;&#36807;&#20132;&#25442;&#36739;&#23569;&#30340;&#28040;&#24687;&#23454;&#29616;&#24369;&#30340;&#36817;&#20284;&#65292;&#21516;&#26102;&#36890;&#24120;&#20135;&#29983;&#20102;&#20960;&#20010;&#35268;&#21010;&#39046;&#22495;&#20013;&#25152;&#38656;&#30340;$C_3$&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#36328;&#35268;&#21010;&#39046;&#22495;&#30340;&#36890;&#29992;&#31574;&#30053;&#21463;&#21040;$C_2$&#34920;&#36798;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#21363;&#19968;&#38454;&#36923;&#36753;&#21482;&#33021;&#21253;&#21547;&#20004;&#20010;&#21464;&#37327;&#21644;&#35745;&#25968;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#20197;&#36890;&#36807;&#36716;&#21521;$k$-GNNs&#65292;&#20854;&#20013;$k=3$&#65292;&#20854;&#20013;&#29289;&#20307;&#23884;&#20837;&#34987;&#19977;&#20803;&#32452;&#23884;&#20837;&#25152;&#26367;&#25442;&#65292;&#26469;&#20811;&#26381;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;$3$-GNNs&#20855;&#26377;$C_3$&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#19981;&#21516;&#20110;&#21463;&#38480;&#20110;$C_2$&#30340;$1$-&#21644;$2$-GNNs&#65292;&#23427;&#20204;&#38656;&#35201;&#22235;&#27425;&#26102;&#38388;&#36827;&#34892;&#28040;&#24687;&#20132;&#25442;&#21644;&#19977;&#27425;&#31354;&#38388;&#36827;&#34892;&#23884;&#20837;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#29256;&#26412;&#30340;&#20851;&#31995;GNNs&#12290;&#24403;$t$&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;R-GNN[$t$]&#20165;&#20351;&#29992;&#20108;&#27425;&#31354;&#38388;&#30340;&#23884;&#20837;&#26469;&#36817;&#20284;$3$-GNNs&#12290;&#23545;&#20110;&#36739;&#20302;&#30340;$t$&#20540;&#65292;&#20363;&#22914;$t=1$&#21644;$t=2$&#65292;R-GNN[$t$]&#36890;&#36807;&#20132;&#25442;&#36739;&#23569;&#30340;&#28040;&#24687;&#23454;&#29616;&#20102;&#26356;&#24369;&#30340;&#36817;&#20284;&#65292;&#20294;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#24120;&#20135;&#29983;&#20102;&#22312;&#20960;&#20010;&#35268;&#21010;&#39046;&#22495;&#20013;&#25152;&#38656;&#30340;$C_3$&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;R-GNN[$t$] ar
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11734v1 Announce Type: new  Abstract: GNN-based approaches for learning general policies across planning domains are limited by the expressive power of $C_2$, namely; first-order logic with two variables and counting. This limitation can be overcomed by transitioning to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet embeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$- and $2$-GNNs that are confined to $C_2$, they require quartic time for message exchange and cubic space for embeddings, rendering them impractical. In this work, we introduce a parameterized version of relational GNNs. When $t$ is infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for embeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by exchanging fewer messages, yet interestingly, often yield the $C_3$ features required in several planning domains. Furthermore, the new R-GNN[$t$] ar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#36712;&#36857;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;PITA&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#21160;&#21147;&#23398;&#27169;&#22411;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#36712;&#36857;&#26356;&#21152;&#24179;&#28369;&#19988;&#20855;&#26377;&#29289;&#29702;&#21512;&#29702;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11728</link><description>&lt;p&gt;
PITA: &#29289;&#29702;&#20449;&#24687;&#36712;&#36857;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PITA: Physics-Informed Trajectory Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#36712;&#36857;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;PITA&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#21160;&#21147;&#23398;&#27169;&#22411;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#36712;&#36857;&#26356;&#21152;&#24179;&#28369;&#19988;&#20855;&#26377;&#29289;&#29702;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#38656;&#35201;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#21253;&#25324;&#32597;&#35265;&#30340;&#36793;&#32536;&#24773;&#20917;&#65292;&#36825;&#20123;&#24773;&#20917;&#19981;&#22826;&#21487;&#33021;&#21457;&#29983;&#65292;&#38656;&#35201;&#36890;&#36807;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#27979;&#35797;&#26469;&#34917;&#20805;&#29616;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#12290;&#29983;&#25104;&#27169;&#22411;&#21487;&#29992;&#20110;&#36890;&#36807;&#22312;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#23558;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19982;&#29983;&#25104;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#29983;&#25104;&#36793;&#32536;&#24773;&#20917;&#22330;&#26223;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20174;&#36739;&#20302;&#32500;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#37325;&#24314;&#36755;&#20837;&#25968;&#25454;&#26469;&#23398;&#20064;&#29305;&#23450;&#39046;&#22495;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#36712;&#36857;&#19981;&#19968;&#23450;&#26159;&#29289;&#29702;&#19978;&#21512;&#29702;&#30340;&#65292;&#32780;&#36890;&#24120;&#21253;&#21547;&#36755;&#20837;&#36712;&#36857;&#20013;&#27809;&#26377;&#30340;&#22122;&#22768;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#29289;&#29702;&#20449;&#24687;&#36712;&#36857;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;PITA&#65289;&#26550;&#26500;&#65292;&#23558;&#29289;&#29702;&#21160;&#21147;&#23398;&#27169;&#22411;&#32435;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#36825;&#23548;&#33268;&#24179;&#28369;&#36712;&#36857;&#65292;&#19981;&#20165;&#37325;&#24314;&#20102;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11728v1 Announce Type: new  Abstract: Validating robotic systems in safety-critical appli-cations requires testing in many scenarios including rare edgecases that are unlikely to occur, requiring to complement real-world testing with testing in simulation. Generative models canbe used to augment real-world datasets with generated data toproduce edge case scenarios by sampling in a learned latentspace. Autoencoders can learn said latent representation for aspecific domain by learning to reconstruct the input data froma lower-dimensional intermediate representation. However, theresulting trajectories are not necessarily physically plausible, butinstead typically contain noise that is not present in the inputtrajectory. To resolve this issue, we propose the novel Physics-Informed Trajectory Autoencoder (PITA) architecture, whichincorporates a physical dynamics model into the loss functionof the autoencoder. This results in smooth trajectories that notonly reconstruct the input 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#20803;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#22235;&#20803;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#30041;&#29305;&#24449;&#20043;&#38388;&#20851;&#31995;&#30340;&#21516;&#26102;&#22312;&#25925;&#38556;&#20998;&#31867;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11722</link><description>&lt;p&gt;
&#20351;&#29992;&#22235;&#20803;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#22235;&#20803;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11722
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#20803;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#22235;&#20803;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#30041;&#29305;&#24449;&#20043;&#38388;&#20851;&#31995;&#30340;&#21516;&#26102;&#22312;&#25925;&#38556;&#20998;&#31867;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22235;&#20803;&#25968;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#38271;&#26102;&#38388;&#24207;&#21015;&#21010;&#20998;&#20026;&#25968;&#25454;&#27573;&#65292;&#25552;&#21462;&#36825;&#20123;&#22359;&#30340;&#26368;&#23567;&#20540;&#12289;&#26368;&#22823;&#20540;&#12289;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#20316;&#20026;&#20195;&#34920;&#24615;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#23553;&#35013;&#22312;&#22235;&#20803;&#25968;&#20013;&#65292;&#24471;&#21040;&#19968;&#20010;&#22235;&#20803;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#12290;&#36825;&#20010;&#26102;&#38388;&#24207;&#21015;&#20351;&#29992;&#22235;&#20803;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21704;&#23494;&#39039;&#31215;&#26469;&#20445;&#30041;&#36825;&#20123;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20010;&#22235;&#20803;&#25968;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20351;&#29992;GHR&#24494;&#31215;&#20998;&#30340;&#22235;&#20803;&#25968;&#21453;&#21521;&#20256;&#25773;&#65292;&#36825;&#23545;&#20110;&#22235;&#20803;&#25968;&#31354;&#38388;&#20013;&#30340;&#26377;&#25928;&#20056;&#31215;&#21644;&#38142;&#35268;&#21017;&#26159;&#24517;&#38656;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25512;&#23548;&#26356;&#26032;&#35268;&#21017;&#19982;&#33258;&#21160;&#24494;&#20998;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;Tennessee Eastman&#25968;&#25454;&#38598;&#65292;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#20351;&#29992;&#21387;&#32553;&#25968;&#25454;&#36827;&#34892;&#25925;&#38556;&#20998;&#31867;&#65306;&#19968;&#20010;&#23436;&#20840;&#30417;&#30563;&#30340;&#35774;&#32622;&#21644;&#21478;&#19968;&#20010;&#21322;&#30417;&#30563;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11722v1 Announce Type: new  Abstract: We propose a novel quaternionic time-series compression methodology where we divide a long time-series into segments of data, extract the min, max, mean and standard deviation of these chunks as representative features and encapsulate them in a quaternion, yielding a quaternion valued time-series. This time-series is processed using quaternion valued neural network layers, where we aim to preserve the relation between these features through the usage of the Hamilton product. To train this quaternion neural network, we derive quaternion backpropagation employing the GHR calculus, which is required for a valid product and chain rule in quaternion space. Furthermore, we investigate the connection between the derived update rules and automatic differentiation. We apply our proposed compression method on the Tennessee Eastman Dataset, where we perform fault classification using the compressed data in two settings: a fully supervised one and i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22810;&#28304;&#25193;&#25955;&#27169;&#22411;&#65288;MSDM&#65289;&#25512;&#24191;&#21040;&#20219;&#24847;&#26102;&#38388;&#22495;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#25991;&#26412;&#23884;&#20837;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#19981;&#38656;&#35201;&#20998;&#31163;&#25968;&#25454;&#35757;&#32451;&#65292;&#21487;&#20197;&#21442;&#25968;&#21270;&#20219;&#24847;&#25968;&#37327;&#28304;&#65292;&#24182;&#23454;&#29616;&#20016;&#23500;&#35821;&#20041;&#25511;&#21046;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11706</link><description>&lt;p&gt;
&#36890;&#29992;&#25991;&#26412;&#26465;&#20214;&#38899;&#20048;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#28304;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22810;&#28304;&#25193;&#25955;&#27169;&#22411;&#65288;MSDM&#65289;&#25512;&#24191;&#21040;&#20219;&#24847;&#26102;&#38388;&#22495;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#25991;&#26412;&#23884;&#20837;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#19981;&#38656;&#35201;&#20998;&#31163;&#25968;&#25454;&#35757;&#32451;&#65292;&#21487;&#20197;&#21442;&#25968;&#21270;&#20219;&#24847;&#25968;&#37327;&#28304;&#65292;&#24182;&#23454;&#29616;&#20016;&#23500;&#35821;&#20041;&#25511;&#21046;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#25193;&#25955;&#27169;&#22411;&#65288;MSDM&#65289;&#29992;&#20110;&#38899;&#20048;&#29983;&#25104;&#20219;&#21153;&#65306;&#29983;&#25104;&#19968;&#32452;&#36830;&#36143;&#30340;&#28304;&#65292;&#21019;&#24314;&#20276;&#22863;&#20197;&#21450;&#25191;&#34892;&#28304;&#20998;&#31163;&#12290;&#26412;&#25991;&#23558;MSDM&#25512;&#24191;&#21040;&#26465;&#20214;&#20110;&#25991;&#26412;&#23884;&#20837;&#30340;&#20219;&#24847;&#26102;&#38388;&#22495;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#38656;&#35201;&#20998;&#31163;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#23545;&#20219;&#24847;&#25968;&#37327;&#30340;&#28304;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#20801;&#35768;&#20016;&#23500;&#30340;&#35821;&#20041;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#36807;&#31243;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#21644;&#20276;&#22863;&#30340;&#36830;&#36143;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#25972;&#20102;MSDM&#30340;Dirac&#20998;&#38548;&#22120;&#20197;&#25191;&#34892;&#28304;&#20998;&#31163;&#12290;&#25105;&#20204;&#23454;&#39564;&#20102;&#22312;Slakh2100&#21644;MTG-Jamendo&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11706v1 Announce Type: cross  Abstract: Multi-Source Diffusion Models (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation. Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time. This paper generalizes MSDM to arbitrary time-domain diffusion models conditioned on text embeddings. These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control. We propose an inference procedure enabling the coherent generation of sources and accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform source separation. We experiment with diffusion models trained on Slakh2100 and MTG-Jamendo, showcasing competitive gene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#27169;&#25311;&#22312;&#19977;&#35282;&#26684;&#23376;&#20013;&#31283;&#23450;&#25163;&#24449;&#30913;&#24615;&#39046;&#22495;&#30340;&#22797;&#26434;&#30005;&#23376;&#20171;&#23548;&#30340;&#33258;&#26059;-&#33258;&#26059;&#30456;&#20114;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#30913;&#24615;&#39046;&#22495;&#22312;&#28909;&#28140;&#21518;&#30340;&#31895;&#21270;&#36807;&#31243;</title><link>https://arxiv.org/abs/2403.11705</link><description>&lt;p&gt;
&#22312;&#20256;&#36882;&#30005;&#23376;&#30913;&#20307;&#20013;&#25163;&#24449;&#39046;&#22495;&#30340;&#31895;&#21270;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11705
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#27169;&#25311;&#22312;&#19977;&#35282;&#26684;&#23376;&#20013;&#31283;&#23450;&#25163;&#24449;&#30913;&#24615;&#39046;&#22495;&#30340;&#22797;&#26434;&#30005;&#23376;&#20171;&#23548;&#30340;&#33258;&#26059;-&#33258;&#26059;&#30456;&#20114;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#30913;&#24615;&#39046;&#22495;&#22312;&#28909;&#28140;&#21518;&#30340;&#31895;&#21270;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25387;&#20256;&#36882;&#30913;&#20307;&#36890;&#24120;&#34920;&#29616;&#20986;&#22797;&#26434;&#30340;&#38750;&#20849;&#32447;&#25110;&#38750;&#20849;&#38754;&#30913;&#24207;&#65292;&#25903;&#25345;&#25299;&#25169;&#30005;&#23376;&#32467;&#26500;&#12290;&#19968;&#20010;&#20856;&#22411;&#20363;&#23376;&#26159;&#30001;&#19977;&#35282;&#26684;&#23376;&#19978;&#30340;&#30005;&#23376;&#33258;&#26059;&#30456;&#20114;&#20316;&#29992;&#31283;&#23450;&#30340;&#25163;&#24449;&#33258;&#26059;&#24207;&#30340;&#24322;&#24120;&#37327;&#23376;&#38669;&#23572;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#22312;&#19977;&#35282;&#26684;&#23376;&#20013;&#31283;&#23450;&#25163;&#24449;&#30913;&#24615;&#39046;&#22495;&#30340;&#22797;&#26434;&#30340;&#30005;&#23376;&#20171;&#23548;&#30340;&#33258;&#26059;-&#33258;&#26059;&#30456;&#20114;&#20316;&#29992;&#12290;&#21033;&#29992;ML&#21147;&#22330;&#27169;&#22411;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#21160;&#21147;&#23398;&#27169;&#25311;&#30740;&#31350;&#20102;&#28909;&#28140;&#21518;&#25163;&#24449;&#39046;&#22495;&#30340;&#31895;&#21270;&#12290;&#34429;&#28982;&#25163;&#24449;&#30456;&#30001;&#30772;&#32570;&#30340;$Z_2$ Ising&#22411;&#23545;&#31216;&#24615;&#25551;&#36848;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29305;&#24449;&#23610;&#23544;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11705v1 Announce Type: cross  Abstract: Frustrated itinerant magnets often exhibit complex noncollinear or noncoplanar magnetic orders which support topological electronic structures. A canonical example is the anomalous quantum Hall state with a chiral spin order stabilized by electron-spin interactions on a triangular lattice. While a long-range magnetic order cannot survive thermal fluctuations in two dimensions, the chiral order which results from the breaking of a discrete Ising symmetry persists even at finite temperatures. We present a scalable machine learning (ML) framework to model the complex electron-mediated spin-spin interactions that stabilize the chiral magnetic domains in a triangular lattice. Large-scale dynamical simulations, enabled by the ML force-field models, are performed to investigate the coarsening of chiral domains after a thermal quench. While the chiral phase is described by a broken $Z_2$ Ising-type symmetry, we find that the characteristic siz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20351;&#29992;&#20809;&#35889;&#31639;&#27861;&#26469;&#35757;&#32451;&#26680;&#65292;&#25512;&#23548;&#20986;&#27867;&#21270;&#35823;&#24046;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#25439;&#22833;&#28176;&#36817;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;&#25439;&#22833;&#22312;&#29305;&#23450;&#39057;&#35889;&#23610;&#24230;&#19978;&#30340;&#23616;&#37096;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.11696</link><description>&lt;p&gt;
&#20809;&#35889;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Generalization error of spectral algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20351;&#29992;&#20809;&#35889;&#31639;&#27861;&#26469;&#35757;&#32451;&#26680;&#65292;&#25512;&#23548;&#20986;&#27867;&#21270;&#35823;&#24046;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#25439;&#22833;&#28176;&#36817;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;&#25439;&#22833;&#22312;&#29305;&#23450;&#39057;&#35889;&#23610;&#24230;&#19978;&#30340;&#23616;&#37096;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#27880;&#26680;&#26041;&#27861;&#27867;&#21270;&#30340;&#28176;&#36817;&#31934;&#30830;&#20272;&#35745;&#65292;&#28304;&#20110;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#30456;&#20851;&#26680;&#20043;&#38388;&#30340;&#31867;&#27604;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#26159;&#36890;&#36807;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#25512;&#23548;&#20986;&#36825;&#20123;&#20272;&#35745;&#20540;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20351;&#29992;&#30001;&#37197;&#32622;&#25991;&#20214;$h(\lambda)$&#25351;&#23450;&#30340;&#19968;&#31995;&#21015;&#8220;&#20809;&#35889;&#31639;&#27861;&#8221;&#26469;&#35757;&#32451;&#26680;&#65292;&#20854;&#20013;&#21253;&#25324;KRR&#21644;GD&#20316;&#20026;&#29305;&#20363;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20851;&#20110;&#20004;&#31181;&#25968;&#25454;&#27169;&#22411;&#30340;&#23398;&#20064;&#37197;&#32622;&#25991;&#20214;$h(\lambda)$&#30340;&#27867;&#21270;&#35823;&#24046;&#20989;&#25968;&#65306;&#39640;&#32500;&#39640;&#26031;&#27169;&#22411;&#21644;&#20302;&#32500;&#24179;&#31227;&#19981;&#21464;&#27169;&#22411;&#12290;&#22312;&#23545;&#26680;&#21644;&#30446;&#26631;&#30340;&#39057;&#35889;&#36827;&#34892;&#24130;&#24459;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#26469;(i)&#20026;&#26377;&#22122;&#21644;&#26080;&#22122;&#35266;&#27979;&#25552;&#20379;&#23436;&#25972;&#30340;&#25439;&#22833;&#28176;&#36817;&#34892;&#20026;&#65307;(ii)&#23637;&#31034;&#25439;&#22833;&#20986;&#29616;&#22312;&#26576;&#20123;&#39057;&#35889;&#23610;&#24230;&#19978;&#30340;&#23616;&#37096;&#21270;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11696v1 Announce Type: new  Abstract: The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of $\textit{spectral algorithms}$ specified by profile $h(\lambda)$, and including KRR and GD as special cases. Then, we derive the generalization error as a functional of learning profile $h(\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model. Under power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, givi
&lt;/p&gt;</description></item><item><title>&#22312;&#38750;&#20809;&#28369;&#35774;&#32622;&#19979;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#20869;&#26144;&#23556;&#30340;&#22806;&#26144;&#23556;&#22266;&#23450;&#28857;&#30340;&#38544;&#24335;&#23548;&#25968;&#30340;&#26032;&#26041;&#27861;NSID&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#36845;&#20195;&#24494;&#20998;&#65288;ITD&#65289;&#21644;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#30340;&#25913;&#36827;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.11687</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#38544;&#24335;&#24494;&#20998;&#65306;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11687
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#20809;&#28369;&#35774;&#32622;&#19979;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#20869;&#26144;&#23556;&#30340;&#22806;&#26144;&#23556;&#22266;&#23450;&#28857;&#30340;&#38544;&#24335;&#23548;&#25968;&#30340;&#26032;&#26041;&#27861;NSID&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#36845;&#20195;&#24494;&#20998;&#65288;ITD&#65289;&#21644;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#30340;&#25913;&#36827;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#25928;&#35745;&#31639;&#21442;&#25968;&#21270;&#19981;&#21487;&#24494;&#25910;&#32553;&#26144;&#23556;&#22266;&#23450;&#28857;&#23548;&#25968;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65306;&#36845;&#20195;&#24494;&#20998;&#65288;ITD&#65289;&#21644;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#12290;&#22312;&#38750;&#20809;&#28369;&#35774;&#32622;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38142;&#35268;&#21017;&#19981;&#20877;&#25104;&#31435;&#12290;&#22312;Bolte&#31561;&#20154;&#65288;2022&#65289;&#26368;&#36817;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#19981;&#21487;&#24494;&#20998;ITD&#30340;&#32447;&#24615;&#25910;&#25947;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;ITD&#21644;AID&#30340;&#25913;&#36827;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;NSID&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22266;&#23450;&#28857;&#34987;&#23450;&#20041;&#20026;&#21482;&#36890;&#36807;&#38543;&#26426;&#26080;&#20559;&#20272;&#35745;&#22120;&#35775;&#38382;&#30340;&#22806;&#26144;&#23556;&#21644;&#20869;&#26144;&#23556;&#30340;&#32452;&#21512;&#26102;&#35745;&#31639;&#38544;&#24335;&#23548;&#25968;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11687v1 Announce Type: cross  Abstract: We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of 
&lt;/p&gt;</description></item><item><title>Crystalformer&#26159;&#19968;&#31181;&#29992;&#20110;&#26230;&#20307;&#32467;&#26500;&#30340;Transformer-based&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#26080;&#38480;&#36830;&#25509;&#27880;&#24847;&#21147;&#36827;&#34892;&#26080;&#38480;&#30340;&#21407;&#23376;&#38388;&#21183;&#27714;&#21644;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.11686</link><description>&lt;p&gt;
Crystalformer&#65306;&#29992;&#20110;&#21608;&#26399;&#32467;&#26500;&#32534;&#30721;&#30340;&#26080;&#38480;&#36830;&#25509;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11686
&lt;/p&gt;
&lt;p&gt;
Crystalformer&#26159;&#19968;&#31181;&#29992;&#20110;&#26230;&#20307;&#32467;&#26500;&#30340;Transformer-based&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#26080;&#38480;&#36830;&#25509;&#27880;&#24847;&#21147;&#36827;&#34892;&#26080;&#38480;&#30340;&#21407;&#23376;&#38388;&#21183;&#27714;&#21644;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#20174;&#23427;&#20204;&#30340;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#26448;&#26009;&#30340;&#29289;&#29702;&#24615;&#36136;&#12290;&#22312;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#31561;&#36793;&#32536;&#39046;&#22495;&#65292;&#20840;&#36830;&#25509;&#27880;&#24847;&#21147;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#20123;&#26377;&#38480;&#21407;&#23376;&#25490;&#21015;&#19981;&#21516;&#65292;&#26230;&#20307;&#32467;&#26500;&#26159;&#26080;&#38480;&#37325;&#22797;&#30340;&#65292;&#21608;&#26399;&#24615;&#30340;&#21407;&#23376;&#25490;&#21015;&#65292;&#20854;&#20840;&#36830;&#25509;&#27880;&#24847;&#21147;&#23548;&#33268;&#26080;&#38480;&#36830;&#25509;&#27880;&#24847;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26080;&#38480;&#36830;&#25509;&#27880;&#24847;&#21147;&#21487;&#20197;&#23548;&#33268;&#19968;&#20010;&#21487;&#35745;&#31639;&#30340;&#20844;&#24335;&#24418;&#24335;&#65292;&#35299;&#37322;&#20026;&#31070;&#32463;&#21183;&#27714;&#21644;&#65292;&#22312;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#29305;&#24449;&#31354;&#38388;&#20013;&#25191;&#34892;&#26080;&#38480;&#30340;&#21407;&#23376;&#38388;&#21183;&#27714;&#21644;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#30340;&#26230;&#20307;&#32467;&#26500;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#31216;&#20026;Crystalformer&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20165;&#38656;&#35201;29.4%&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;wit
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11686v1 Announce Type: new  Abstract: Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention. In this work, we show that this infinitely connected attention can lead to a computationally tractable formulation, interpreted as neural potential summation, that performs infinite interatomic potential summations in a deeply learned feature space. We then propose a simple yet effective Transformer-based encoder architecture for crystal structures called Crystalformer. Compared to an existing Transformer-based model, the proposed model requires only 29.4% of the number of parameters, wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;3D&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#21644;&#36328;&#22330;&#26223;&#20849;&#20139;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;NeRFs&#26377;&#25928;&#23398;&#20064;&#22823;&#37327;&#35821;&#20041;&#30456;&#20284;&#22330;&#26223;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;</title><link>https://arxiv.org/abs/2403.11678</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;3D&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#65292;&#20197;&#26377;&#25928;&#23398;&#20064;&#22810;&#20010;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11678
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;3D&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#21644;&#36328;&#22330;&#26223;&#20849;&#20139;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;NeRFs&#26377;&#25928;&#23398;&#20064;&#22823;&#37327;&#35821;&#20041;&#30456;&#20284;&#22330;&#26223;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#25216;&#26415;&#26469;&#25913;&#21892;&#27599;&#20010;&#22330;&#26223;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#23558;NeRFs&#26080;&#32541;&#25193;&#23637;&#21040;&#23398;&#20064;&#22823;&#37327;&#35821;&#20041;&#30456;&#20284;&#30340;&#22330;&#26223;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;3D&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#65292;&#22312;&#20854;&#20013;&#35757;&#32451;&#19977;&#24179;&#38754;&#22330;&#26223;&#34920;&#31034;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23398;&#20064;&#22330;&#26223;&#30340;&#20998;&#36776;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22330;&#26223;&#20849;&#20139;&#36890;&#29992;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23398;&#20064;&#29305;&#23450;&#22330;&#26223;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#24403;&#35757;&#32451;1000&#20010;&#22330;&#26223;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#22330;&#26223;&#30340;&#20869;&#23384;&#25104;&#26412;&#38477;&#20302;&#20102;44%&#65292;&#26102;&#38388;&#25104;&#26412;&#38477;&#20302;&#20102;86%&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;&#20301;&#20110;https://3da-ae.github.io&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11678v1 Announce Type: cross  Abstract: We present a method enabling the scaling of NeRFs to learn a large number of semantically-similar scenes. We combine two techniques to improve the required training time and memory cost per scene. First, we learn a 3D-aware latent space in which we train Tri-Plane scene representations, hence reducing the resolution at which scenes are learned. Moreover, we present a way to share common information across scenes, hence allowing for a reduction of model complexity to learn a particular scene. Our method reduces effective per-scene memory costs by 44% and per-scene time costs by 86% when training 1000 scenes. Our project page can be found at https://3da-ae.github.io .
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36741;&#21161;&#30340;HDL&#35843;&#35797;&#26694;&#26550; HDLdebugger&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#29983;&#25104;HDL&#35843;&#35797;&#25968;&#25454;&#65292;&#25552;&#20379;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#25628;&#32034;&#24341;&#25806;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;LLM&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;HDL&#35843;&#35797;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.11671</link><description>&lt;p&gt;
HDLdebugger: &#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;HDL&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;
HDLdebugger: Streamlining HDL debugging with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36741;&#21161;&#30340;HDL&#35843;&#35797;&#26694;&#26550; HDLdebugger&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#29983;&#25104;HDL&#35843;&#35797;&#25968;&#25454;&#65292;&#25552;&#20379;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#25628;&#32034;&#24341;&#25806;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;LLM&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;HDL&#35843;&#35797;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#65292;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65288;HDLs&#65289;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;HDL&#30340;&#22797;&#26434;&#35821;&#27861;&#21644;&#22312;&#32447;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#21363;&#20351;&#26159;&#32463;&#39564;&#20016;&#23500;&#30340;&#24037;&#31243;&#24072;&#65292;&#35843;&#35797;HDL&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#39033;&#22256;&#38590;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#21270;HDL&#20195;&#30721;&#35843;&#35797;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#30828;&#20214;&#24037;&#31243;&#24072;&#30340;&#36127;&#25285;&#12290;&#23613;&#31649;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#12289;&#23436;&#25104;&#21644;&#35843;&#35797;&#36719;&#20214;&#20195;&#30721;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#19987;&#38376;&#39046;&#22495;&#30340;HDL&#35843;&#35797;&#20013;&#30340;&#21033;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#65292;&#24182;&#19988;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36741;&#21161;&#30340;HDL&#35843;&#35797;&#26694;&#26550;&#65292;&#21363;HDLdebugger&#65292;&#23427;&#21253;&#25324;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#29983;&#25104;HDL&#35843;&#35797;&#25968;&#25454;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;&#20197;&#21450;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;LLM&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11671v1 Announce Type: cross  Abstract: In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#65292;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#20132;&#36890;&#21442;&#19982;&#32773;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#20248;&#20110;&#22810;&#31181;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.11643</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29615;&#22659;&#24863;&#30693;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Based Environment-Aware Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#65292;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#20132;&#36890;&#21442;&#19982;&#32773;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#20248;&#20110;&#22810;&#31181;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#26410;&#26469;&#36712;&#36857;&#30340;&#33021;&#21147;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#20132;&#36890;&#21442;&#19982;&#32773;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#20934;&#30830;&#23398;&#20064;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#12290;&#36890;&#36807;&#22312;&#27169;&#22411;&#36755;&#20986;&#19978;&#24341;&#20837;&#24494;&#20998;&#36816;&#21160;&#32422;&#26463;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#36924;&#30495;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#24863;&#30693;&#20132;&#20114;&#24341;&#23548;&#20449;&#21495;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#39044;&#27979;&#19981;&#22826;&#21512;&#20316;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11643v1 Announce Type: cross  Abstract: The ability to predict the future trajectories of traffic participants is crucial for the safe and efficient operation of autonomous vehicles. In this paper, a diffusion-based generative model for multi-agent trajectory prediction is proposed. The model is capable of capturing the complex interactions between traffic participants and the environment, accurately learning the multimodal nature of the data. The effectiveness of the approach is assessed on large-scale datasets of real-world traffic scenarios, showing that our model outperforms several well-established methods in terms of prediction accuracy. By the incorporation of differential motion constraints on the model output, we illustrate that our model is capable of generating a diverse set of realistic future trajectories. Through the use of an interaction-aware guidance signal, we further demonstrate that the model can be adapted to predict the behavior of less cooperative agen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#36816;&#34892;&#26102;&#30340;&#26102;&#38388;&#32422;&#26463;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#25972;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#25216;&#26415;&#65292;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#20013;&#24341;&#20837;&#20102;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.11642</link><description>&lt;p&gt;
&#24341;&#23548;&#22522;&#20110;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#21487;&#35299;&#37322;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#36827;&#34892;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#36816;&#34892;&#26102;&#30340;&#26102;&#38388;&#32422;&#26463;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#25972;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#25216;&#26415;&#65292;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#20013;&#24341;&#20837;&#20102;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#25351;&#20986;&#20102;&#20462;&#25913;&#36755;&#20837;&#23454;&#20363;&#20197;&#25913;&#21464;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#32467;&#26524;&#24212;&#35813;&#26377;&#20160;&#20040;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#39046;&#22495;&#22788;&#29702;&#21453;&#20107;&#23454;&#35299;&#37322;&#26102;&#65292;&#24517;&#39035;&#20180;&#32454;&#32771;&#34385;&#20107;&#20214;&#20043;&#38388;&#30340;&#25511;&#21046;&#27969;&#20851;&#31995;&#12290;&#30830;&#23454;&#65292;&#19968;&#20010;&#21453;&#20107;&#23454;&#19981;&#24212;&#36829;&#21453;&#27963;&#21160;&#20043;&#38388;&#30340;&#25511;&#21046;&#27969;&#20851;&#31995;&#65288;&#21363;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#65289;&#12290;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#30340;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#26377;&#19968;&#31995;&#21015;&#20851;&#20110;&#22522;&#20110;&#32467;&#26524;&#30340;&#39044;&#27979;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20316;&#21697;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#22312;&#29983;&#25104;&#36825;&#20123;&#21453;&#20107;&#23454;&#26102;&#32771;&#34385;&#20102;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#30340;&#21253;&#21547;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20197;&#32771;&#34385;&#19968;&#31995;&#21015;&#36816;&#34892;&#26102;&#30340;&#26102;&#38388;&#32422;&#26463;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#20551;&#23450;&#36825;&#31181;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11642v1 Announce Type: new  Abstract: Counterfactual explanations suggest what should be different in the input instance to change the outcome of an AI system. When dealing with counterfactual explanations in the field of Predictive Process Monitoring, however, control flow relationships among events have to be carefully considered. A counterfactual, indeed, should not violate control flow relationships among activities (temporal background knowledege). Within the field of Explainability in Predictive Process Monitoring, there have been a series of works regarding counterfactual explanations for outcome-based predictions. However, none of them consider the inclusion of temporal background knowledge when generating these counterfactuals. In this work, we adapt state-of-the-art techniques for counterfactual generation in the domain of XAI that are based on genetic algorithms to consider a series of temporal constraints at runtime. We assume that this temporal background knowle
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#37096;&#20998;&#26410;&#26469;&#22870;&#21169;&#20808;&#30693;&#30340;&#20215;&#20540;&#65292;&#36890;&#36807;&#31454;&#20105;&#24615;&#20998;&#26512;&#24471;&#20986;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#22870;&#21169;&#26399;&#26395;&#30340;&#31934;&#30830;&#27604;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.11637</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#26410;&#26469;&#22870;&#21169;&#20808;&#30693;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Value of Reward Lookahead in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11637
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#37096;&#20998;&#26410;&#26469;&#22870;&#21169;&#20808;&#30693;&#30340;&#20215;&#20540;&#65292;&#36890;&#36807;&#31454;&#20105;&#24615;&#20998;&#26512;&#24471;&#20986;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#22870;&#21169;&#26399;&#26395;&#30340;&#31934;&#30830;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#20195;&#29702;&#20204;&#19982;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#36827;&#34892;&#39034;&#24207;&#20132;&#20114;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#33719;&#24471;&#30340;&#22870;&#21169;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22870;&#21169;&#20165;&#22312;&#34892;&#21160;&#21518;&#34987;&#35266;&#23519;&#21040;&#65292;&#22240;&#27492;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22870;&#21169;&#20449;&#24687;&#26159;&#25552;&#21069;&#35266;&#23519;&#21040;&#30340; -- &#20132;&#26131;&#21069;&#35266;&#23519;&#21040;&#20215;&#26684;&#65307;&#20102;&#35299;&#37096;&#20998;&#38468;&#36817;&#20132;&#36890;&#20449;&#24687;&#65307;&#32463;&#24120;&#22312;&#20114;&#21160;&#20043;&#21069;&#20026;&#20195;&#29702;&#20998;&#37197;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#24615;&#20998;&#26512;&#30340;&#35270;&#35282;&#65292;&#23450;&#37327;&#20998;&#26512;&#36825;&#31181;&#26410;&#26469;&#22870;&#21169;&#20449;&#24687;&#30340;&#20215;&#20540;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#26631;&#20934;RL&#20195;&#29702;&#30340;&#20215;&#20540;&#19982;&#20855;&#26377;&#37096;&#20998;&#26410;&#26469;&#22870;&#21169;&#20808;&#30693;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#21051;&#30011;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#22870;&#21169;&#20998;&#24067;&#65292;&#24182;&#25512;&#23548;&#20986;&#26368;&#22351;&#24773;&#20917;&#19979;&#22870;&#21169;&#26399;&#26395;&#30340;&#31934;&#30830;&#27604;&#29575;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#32467;&#26524;&#27604;&#29575;&#19982;&#31163;&#32447;RL&#21644;r&#20013;&#24050;&#30693;&#30340;&#25968;&#37327;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11637v1 Announce Type: new  Abstract: In reinforcement learning (RL), agents sequentially interact with changing environments while aiming to maximize the obtained rewards. Usually, rewards are observed only after acting, and so the goal is to maximize the expected cumulative reward. Yet, in many practical settings, reward information is observed in advance -- prices are observed before performing transactions; nearby traffic information is partially known; and goals are oftentimes given to agents prior to the interaction. In this work, we aim to quantifiably analyze the value of such future reward information through the lens of competitive analysis. In particular, we measure the ratio between the value of standard RL agents and that of agents with partial future-reward lookahead. We characterize the worst-case reward distribution and derive exact ratios for the worst-case reward expectations. Surprisingly, the resulting ratios relate to known quantities in offline RL and r
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11624</link><description>&lt;p&gt;
&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Channel Multiplex Graph Neural Networks for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#25429;&#25417;&#21453;&#26144;&#20010;&#20154;&#20559;&#22909;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23646;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#25512;&#33616;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#19978;&#30340;&#28857;&#20987;&#12289;&#26631;&#35760;&#25910;&#34255;&#21644;&#36141;&#20080;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#65306;(1) &#19981;&#36275;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#22810;&#36890;&#36335;&#20851;&#31995;&#24418;&#25104;&#30340;&#21508;&#31181;&#34892;&#20026;&#27169;&#24335;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;(2) &#24573;&#30053;&#20102;&#34892;&#20026;&#27169;&#24335;&#20013;&#19981;&#21516;&#20851;&#31995;&#23545;&#25512;&#33616;&#31995;&#32479;&#22330;&#26223;&#20013;&#30446;&#26631;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#22810;&#20154;&#21512;&#20316;&#22810;&#33218;&#32769;&#34382;&#26426;&#27169;&#22411;&#65292;&#35774;&#35745;&#20102;&#20998;&#24067;&#24335;&#21512;&#20316;&#36172;&#21338;&#31639;&#27861;DC-ULCB&#65292;&#33021;&#22815;&#22312;&#26368;&#22823;&#21270;&#25968;&#25454;&#36895;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#36873;&#25321;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#39564;&#35777;&#65292;&#35777;&#26126;&#22312;&#22870;&#21169;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11603</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#38024;&#23545;&#32593;&#32476;&#30340;&#20844;&#24179;&#20998;&#24067;&#24335;&#21512;&#20316;&#36172;&#21338;&#23398;&#20064;&#65288;&#25216;&#26415;&#25253;&#21578;&#65289;
&lt;/p&gt;
&lt;p&gt;
Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#22810;&#20154;&#21512;&#20316;&#22810;&#33218;&#32769;&#34382;&#26426;&#27169;&#22411;&#65292;&#35774;&#35745;&#20102;&#20998;&#24067;&#24335;&#21512;&#20316;&#36172;&#21338;&#31639;&#27861;DC-ULCB&#65292;&#33021;&#22815;&#22312;&#26368;&#22823;&#21270;&#25968;&#25454;&#36895;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#36873;&#25321;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#39564;&#35777;&#65292;&#35777;&#26126;&#22312;&#22870;&#21169;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#31995;&#32479;&#20013;&#65292;&#32593;&#32476;&#20869;&#30340;&#36793;&#32536;&#26381;&#21153;&#22120;&#19982;&#20854;&#37051;&#23621;&#20132;&#25442;&#20449;&#24687;&#24182;&#20174;&#20256;&#24863;&#22120;&#25910;&#38598;&#25968;&#25454;&#20197;&#23436;&#25104;&#20132;&#20184;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20154;&#22810;&#33218;&#32769;&#34382;&#26426;&#27169;&#22411;&#65292;&#26088;&#22312;&#20026;&#26234;&#33021;IoT&#31995;&#32479;&#30340;&#25968;&#25454;&#25910;&#38598;&#25552;&#20379;&#20415;&#21033;&#65292;&#24182;&#32435;&#20837;&#20844;&#24179;&#32771;&#34385;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#24110;&#21161;&#26381;&#21153;&#22120;&#19982;&#20854;&#37051;&#23621;&#21512;&#20316;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#21512;&#20316;&#36172;&#21338;&#31639;&#27861;&#65292;DC-ULCB&#65292;&#20351;&#26381;&#21153;&#22120;&#33021;&#22815;&#21327;&#20316;&#36873;&#25321;&#20256;&#24863;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#25968;&#25454;&#36895;&#29575;&#21516;&#26102;&#20445;&#25345;&#36873;&#25321;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#23545;DC-ULCB&#30340;&#22870;&#21169;&#36951;&#25022;&#21644;&#20844;&#24179;&#36951;&#25022;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20004;&#31181;&#36951;&#25022;&#22343;&#20855;&#26377;&#23545;&#25968;&#23454;&#20363;&#30456;&#20851;&#30340;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;DC-ULCB&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#30830;&#20445;&#20844;&#24179;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11603v1 Announce Type: cross  Abstract: In intelligent Internet of Things (IoT) systems, edge servers within a network exchange information with their neighbors and collect data from sensors to complete delivered tasks. In this paper, we propose a multiplayer multi-armed bandit model for intelligent IoT systems to facilitate data collection and incorporate fairness considerations. In our model, we establish an effective communication protocol that helps servers cooperate with their neighbors. Then we design a distributed cooperative bandit algorithm, DC-ULCB, enabling servers to collaboratively select sensors to maximize data rates while maintaining fairness in their choices. We conduct an analysis of the reward regret and fairness regret of DC-ULCB, and prove that both regrets have logarithmic instance-dependent upper bounds. Additionally, through extensive simulations, we validate that DC-ULCB outperforms existing algorithms in maximizing reward and ensuring fairness.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#24555;/&#24930;&#21160;&#21147;&#23398;ODE&#31995;&#32479;&#30340;&#24930;&#19981;&#21464;&#27969;&#24418;&#65292;&#33021;&#22815;&#21516;&#26102;&#20998;&#35299;&#30690;&#37327;&#22330;&#20026;&#24555;&#24930;&#32452;&#20998;&#24182;&#20197;&#38381;&#21512;&#24418;&#24335;&#25552;&#20379;&#19979;&#23618;SIM&#30340;&#27867;&#20989;&#12290;</title><link>https://arxiv.org/abs/2403.11591</link><description>&lt;p&gt;
&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#36817;&#20284;&#19968;&#33324;&#31867;&#21035;&#30340;&#21018;&#24615;ODE&#31995;&#32479;&#30340;&#24930;&#19981;&#21464;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
A physics-informed neural network method for the approximation of slow invariant manifolds for the general class of stiff systems of ODEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11591
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#24555;/&#24930;&#21160;&#21147;&#23398;ODE&#31995;&#32479;&#30340;&#24930;&#19981;&#21464;&#27969;&#24418;&#65292;&#33021;&#22815;&#21516;&#26102;&#20998;&#35299;&#30690;&#37327;&#22330;&#20026;&#24555;&#24930;&#32452;&#20998;&#24182;&#20197;&#38381;&#21512;&#24418;&#24335;&#25552;&#20379;&#19979;&#23618;SIM&#30340;&#27867;&#20989;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#24555;/&#24930;&#21160;&#21147;&#23398;ODE&#31995;&#32479;&#30340;&#24930;&#19981;&#21464;&#27969;&#24418;&#65288;SIMs&#65289;&#12290;&#19982;&#20854;&#20182;&#21033;&#29992;&#31616;&#21333;&#22238;&#24402;&#26500;&#24314;&#38477;&#38454;&#40657;&#30418;&#20195;&#29702;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;&#25110;&#32773;&#38656;&#35201;&#23545;&#24555;&#24930;&#21464;&#37327;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#23558;&#30690;&#37327;&#22330;&#20998;&#35299;&#20026;&#24555;&#24930;&#32452;&#20998;&#65292;&#24182;&#20197;&#38381;&#21512;&#24418;&#24335;&#25552;&#20379;&#19979;&#23618;SIM&#30340;&#27867;&#20989;&#12290;&#36890;&#36807;&#25214;&#21040;&#29366;&#24577;&#21464;&#37327;&#21040;&#24555;&#24930;&#21464;&#37327;&#30340;&#36716;&#25442;&#26469;&#23454;&#29616;&#20998;&#35299;&#65292;&#20174;&#32780;&#33021;&#22815;&#25512;&#23548;&#20986;&#26174;&#24335;&#30340;&#12289;&#20197;&#24555;&#21464;&#37327;&#20026;&#22522;&#30784;&#30340;SIM&#27867;&#20989;&#12290;&#21518;&#32773;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#31526;&#21495;&#24494;&#20998;&#30340;&#21333;&#23618;&#21069;&#21521;&#31070;&#32463;&#32593;&#32476;&#22312;&#20960;&#20309;&#22855;&#24322;&#25668;&#21160;&#29702;&#35770;&#65288;GSPT&#65289;&#20013;&#35299;&#20915;&#19982;&#19981;&#21464;&#24615;&#26041;&#31243;&#30456;&#23545;&#24212;&#30340;PDE&#32780;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11591v1 Announce Type: cross  Abstract: We present a physics-informed neural network (PINN) approach for the discovery of slow invariant manifolds (SIMs), for the most general class of fast/slow dynamical systems of ODEs. In contrast to other machine learning (ML) approaches that construct reduced order black box surrogate models using simple regression, and/or require a priori knowledge of the fast and slow variables, our approach, simultaneously decomposes the vector field into fast and slow components and provides a functional of the underlying SIM in a closed form. The decomposition is achieved by finding a transformation of the state variables to the fast and slow ones, which enables the derivation of an explicit, in terms of fast variables, SIM functional. The latter is obtained by solving a PDE corresponding to the invariance equation within the Geometric Singular Perturbation Theory (GSPT) using a single-layer feedforward neural network with symbolic differentiation.
&lt;/p&gt;</description></item><item><title>Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11585</link><description>&lt;p&gt;
Linguacodus&#65306;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#36827;&#34892;&#21464;&#38761;&#24615;&#20195;&#30721;&#29983;&#25104;&#30340;&#21327;&#21516;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11585
&lt;/p&gt;
&lt;p&gt;
Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26080;&#32541;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Linguacodus&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#21160;&#24577;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39640;&#32423;&#25968;&#25454;&#22609;&#24418;&#25351;&#20196;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36845;&#20195;&#22320;&#36716;&#25442;&#20026;&#20195;&#30721;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;Linguacodus&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#35780;&#20272;&#21508;&#31181;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#29305;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#20195;&#30721;&#12290;Linguacodus&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#20219;&#21153;&#25551;&#36848;&#21644;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#23545;&#25512;&#36827;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#31163;&#32447;&#22810;&#20219;&#21153;&#20302;&#31209;RL&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORL&#30340;&#26032;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.11574</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Multitask Representation Learning for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11574
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31163;&#32447;&#22810;&#20219;&#21153;&#20302;&#31209;RL&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORL&#30340;&#26032;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#32447;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#34987;&#25552;&#20379;&#26469;&#33258;&#20849;&#20139;&#36890;&#29992;&#34920;&#31034;&#30340;&#19981;&#21516;&#20219;&#21153;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#24182;&#34987;&#35201;&#27714;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#23545;&#31163;&#32447;&#22810;&#20219;&#21153;&#20302;&#31209;RL&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORL&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22870;&#21169;&#20813;&#36153;&#12289;&#31163;&#32447;&#21644;&#22312;&#32447;&#22330;&#26223;&#20013;&#30740;&#31350;&#20102;&#19979;&#28216;RL&#65292;&#20854;&#20013;&#21521;&#20195;&#29702;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#19982;&#19978;&#28216;&#31163;&#32447;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20174;&#19978;&#28216;&#31163;&#32447;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#22909;&#22788;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23398;&#20064;&#20302;&#31209;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11574v1 Announce Type: new  Abstract: We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is asked to learn the shared representation. We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning. Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DSM&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20854;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#27425;&#26799;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11565</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#27861;&#30340;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DSM&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20854;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#27425;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20855;&#26377;&#38750;&#20984;&#21644;&#38750;&#24179;&#28369;&#30446;&#26631;&#20989;&#25968;&#30340;&#21435;&#20013;&#24515;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#20851;&#27880;&#38750;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;DSM&#65292;&#29992;&#20110;&#20998;&#26512;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#36890;&#36807;&#24314;&#31435;&#29983;&#25104;&#24207;&#21015;&#28176;&#36817;&#36924;&#36817;&#20854;&#20851;&#32852;&#24494;&#20998;&#21253;&#21547;&#30340;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#27425;&#26799;&#24230;&#26041;&#27861;&#65292;&#21253;&#25324;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#19979;&#38477;&#65288;DSGD&#65289;&#65292;&#20855;&#26377;&#26799;&#24230;&#36319;&#36394;&#25216;&#26415;&#30340;DSGD&#65288;DSGD-T&#65289;&#21644;&#24102;&#21160;&#37327;&#30340;DSGD&#65288;DSGDm&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;SignSGD&#65292;&#37319;&#29992;&#31526;&#21495;&#26144;&#23556;&#26469;&#27491;&#21017;&#21270;DSGDm&#20013;&#30340;&#26356;&#26032;&#26041;&#21521;&#65292;&#24182;&#34920;&#26126;&#23427;&#34987;&#21253;&#21547;&#22312;&#25105;&#20204;&#30340;&#25552;&#35758;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11565v1 Announce Type: cross  Abstract: In this paper, we concentrate on decentralized optimization problems with nonconvex and nonsmooth objective functions, especially on the decentralized training of nonsmooth neural networks. We introduce a unified framework, named DSM, to analyze the global convergence of decentralized stochastic subgradient methods. We prove the global convergence of our proposed framework under mild conditions, by establishing that the generated sequence asymptotically approximates the trajectories of its associated differential inclusion. Furthermore, we establish that our proposed framework encompasses a wide range of existing efficient decentralized subgradient methods, including decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In addition, we introduce SignSGD employing the sign map to regularize the update directions in DSGDm, and show it is enclosed in our propos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#25968;&#23383;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#65292;&#36890;&#36807;&#28151;&#21512;&#20449;&#21495;&#35774;&#35745;&#26041;&#27861;&#23558;&#33041;&#32534;&#30721;&#21333;&#20803;&#65288;BCU&#65289;&#21644;&#22522;&#26412;&#32534;&#30721;&#21333;&#20803;&#65288;FCU&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#25552;&#21319;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#30340;&#35745;&#31639;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11563</link><description>&lt;p&gt;
&#25512;&#36827;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#65306;&#21033;&#29992;&#33041;&#32534;&#30721;&#21333;&#20803;&#21644;&#22522;&#26412;&#32534;&#30721;&#21333;&#20803;&#30340;&#28151;&#21512;&#20449;&#21495;&#35774;&#35745;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Advancing Neuromorphic Computing: Mixed-Signal Design Techniques Leveraging Brain Code Units and Fundamental Code Units
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#25968;&#23383;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#65292;&#36890;&#36807;&#28151;&#21512;&#20449;&#21495;&#35774;&#35745;&#26041;&#27861;&#23558;&#33041;&#32534;&#30721;&#21333;&#20803;&#65288;BCU&#65289;&#21644;&#22522;&#26412;&#32534;&#30721;&#21333;&#20803;&#65288;FCU&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#25552;&#21319;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#30340;&#35745;&#31639;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#25968;&#23383;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#65292;&#21019;&#26032;&#22320;&#21033;&#29992;&#20102;&#28151;&#21512;&#20449;&#21495;&#35774;&#35745;&#26041;&#27861;&#23558;&#33041;&#32534;&#30721;&#21333;&#20803;&#65288;Brain Code Unit&#65292;BCU&#65289;&#21644;&#22522;&#26412;&#32534;&#30721;&#21333;&#20803;&#65288;Fundamental Code Unit&#65292;FCU&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#21033;&#29992;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644;&#26368;&#26032;&#30340;&#26448;&#26009;&#31185;&#23398;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#25552;&#21319;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#30340;&#35745;&#31639;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#23558;&#25968;&#23383;&#31995;&#32479;&#30340;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#19982;&#27169;&#25311;&#22788;&#29702;&#30340;&#31283;&#20581;&#24615;&#21644;&#33021;&#37327;&#25928;&#29575;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#21508;&#20010;&#25351;&#26631;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;BCU&#23454;&#29616;&#20102;88.0%&#30340;&#20934;&#30830;&#29575;&#21644;20.0 GOP/s/W&#30340;&#21151;&#32791;&#25928;&#29575;&#65292;&#32780;FCU&#35760;&#24405;&#20102;86.5%&#30340;&#20934;&#30830;&#29575;&#21644;18.5 GOP/s/W&#30340;&#21151;&#32791;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#20449;&#21495;&#35774;&#35745;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#24310;&#36831;&#21644;&#21534;&#21520;&#37327;&#65292;&#23454;&#29616;&#20102;&#20302;&#33267;0.75&#27627;&#31186;&#30340;&#24310;&#36831;&#21644;&#39640;&#36798;213&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11563v1 Announce Type: cross  Abstract: This paper introduces a groundbreaking digital neuromorphic architecture that innovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU) using mixedsignal design methodologies. Leveraging open-source datasets and the latest advances in materials science, our research focuses on enhancing the computational efficiency, accuracy, and adaptability of neuromorphic systems. The core of our approach lies in harmonizing the precision and scalability of digital systems with the robustness and energy efficiency of analog processing. Through experimentation, we demonstrate the effectiveness of our system across various metrics. The BCU achieved an accuracy of 88.0% and a power efficiency of 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a power efficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantly improved latency and throughput, achieving a latency as low as 0.75 ms and throughput up to 213 
&lt;/p&gt;</description></item><item><title>&#22312;&#23616;&#37096;&#35775;&#38382;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#24341;&#20837;Lin-Confident-FTRL&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#26356;&#20248;&#31934;&#24230;&#30028;&#38480;&#21644;&#26356;&#22909;&#25193;&#23637;&#24615;&#30340;&#22343;&#20540;&#24179;&#34913;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11544</link><description>&lt;p&gt;
&#29420;&#31435;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#19982;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65306;&#22312;&#23616;&#37096;&#35775;&#38382;&#27169;&#22411;&#19979;&#25913;&#36827;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
RL en Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11544
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23616;&#37096;&#35775;&#38382;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#24341;&#20837;Lin-Confident-FTRL&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#26356;&#20248;&#31934;&#24230;&#30028;&#38480;&#21644;&#26356;&#22909;&#25193;&#23637;&#24615;&#30340;&#22343;&#20540;&#24179;&#34913;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#33324;&#21644;Markov&#21338;&#24328;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#20855;&#26377;&#22823;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22343;&#20540;&#24179;&#34913;&#65292;&#21516;&#26102;&#20811;&#26381;&#22810;&#26041;&#20195;&#29702;&#30340;&#22256;&#22659;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#31867;&#26469;&#36924;&#36817;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#36793;&#38469;$Q$&#20540;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26679;&#19968;&#20010;&#26694;&#26550;&#19979;&#29616;&#26377;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#23545;&#25152;&#38656;&#31934;&#24230;$\varepsilon$&#25110;&#21160;&#20316;&#31354;&#38388;&#20855;&#26377;&#27425;&#20248;&#20381;&#36182;&#24615;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;Lin-Confident-FTRL&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#23616;&#37096;&#23545;&#27169;&#25311;&#22120;&#30340;&#35775;&#38382;&#26435;&#38480;&#30340;&#31895;&#31890;&#24230;&#30456;&#20851;&#22343;&#34913;&#65288;CCE&#65289;&#65292;&#21363;&#21487;&#20197;&#19982;&#35775;&#38382;&#29366;&#24577;&#30340;&#22522;&#30784;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#23545;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#36827;&#34892;&#23545;&#25968;&#30456;&#20851;&#24615;&#30340;&#21516;&#26102;&#65292;Lin-Confident-FTRL&#23398;&#20064;$\epsilon$-CCE&#65292;&#24182;&#33719;&#24471;&#20855;&#26377;&#22791;&#20221;&#30340;&#26368;&#20339;&#31934;&#24230;&#30028;&#38480;$O(\epsilon^{-2})&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#21160;&#20316;&#31354;&#38388;&#30340;&#32447;&#24615;&#20381;&#23384;&#20851;&#31995;&#65292;&#21516;&#26102;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11544v1 Announce Type: new  Abstract: Efficiently learning equilibria with large state and action spaces in general-sum Markov games while overcoming the curse of multi-agency is a challenging problem. Recent works have attempted to solve this problem by employing independent linear function classes to approximate the marginal $Q$-value for each agent. However, existing sample complexity bounds under such a framework have a suboptimal dependency on the desired accuracy $\varepsilon$ or the action space. In this work, we introduce a new algorithm, Lin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local access to the simulator, i.e., one can interact with the underlying environment on the visited states. Up to a logarithmic dependence on the size of the state space, Lin-Confident-FTRL learns $\epsilon$-CCE with a provable optimal accuracy bound $O(\epsilon^{-2})$ and gets rids of the linear dependency on the action space, while scaling polynomially with 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#20196;&#29260;&#30340;&#35821;&#20041;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;I-Prompt&#65292;&#26088;&#22312;&#28040;&#38500;&#20219;&#21153;&#39044;&#27979;&#65292;&#36890;&#36807;&#35821;&#20041;&#25552;&#31034;&#21305;&#37197;&#21644;&#22270;&#20687;&#20196;&#29260;&#32423;&#25552;&#31034;&#26469;&#36873;&#25321;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.11537</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#20196;&#29260;&#30340;&#35821;&#20041;&#25552;&#31034;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic Prompting with Image-Token for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#20196;&#29260;&#30340;&#35821;&#20041;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;I-Prompt&#65292;&#26088;&#22312;&#28040;&#38500;&#20219;&#21153;&#39044;&#27979;&#65292;&#36890;&#36807;&#35821;&#20041;&#25552;&#31034;&#21305;&#37197;&#21644;&#22270;&#20687;&#20196;&#29260;&#32423;&#25552;&#31034;&#26469;&#36873;&#25321;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#22312;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#20026;&#26032;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#32463;&#20986;&#29616;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#31034;&#23398;&#20064;&#21518;&#32493;&#20219;&#21153;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#37325;&#22797;&#32531;&#20914;&#21306;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#21462;&#20915;&#20110;&#21069;&#32622;&#20219;&#21153;&#36873;&#25321;&#36807;&#31243;&#26469;&#36873;&#25321;&#36866;&#24403;&#30340;&#25552;&#31034;&#12290;&#20294;&#26159;&#65292;&#22312;&#20219;&#21153;&#36873;&#25321;&#20013;&#30340;&#19981;&#23436;&#32654;&#21487;&#33021;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#20219;&#21153;&#25968;&#37327;&#36739;&#22823;&#25110;&#20219;&#21153;&#20998;&#24067;&#19981;&#22343;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;I-Prompt&#65292;&#36825;&#26159;&#19968;&#31181;&#20851;&#27880;&#22270;&#20687;&#20196;&#29260;&#35270;&#35273;&#35821;&#20041;&#20449;&#24687;&#20197;&#28040;&#38500;&#20219;&#21153;&#39044;&#27979;&#30340;&#26080;&#20851;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#35821;&#20041;&#25552;&#31034;&#21305;&#37197;&#65292;&#26681;&#25454;&#20196;&#29260;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30830;&#23450;&#25552;&#31034;&#65292;&#20197;&#21450;&#22270;&#20687;&#20196;&#29260;&#32423;&#25552;&#31034;&#65292;&#24212;&#29992;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11537v1 Announce Type: cross  Abstract: Continual learning aims to refine model parameters for new tasks while retaining knowledge from previous tasks. Recently, prompt-based learning has emerged to leverage pre-trained models to be prompted to learn subsequent tasks without the reliance on the rehearsal buffer. Although this approach has demonstrated outstanding results, existing methods depend on preceding task-selection process to choose appropriate prompts. However, imperfectness in task-selection may lead to negative impacts on the performance particularly in the scenarios where the number of tasks is large or task distributions are imbalanced. To address this issue, we introduce I-Prompt, a task-agnostic approach focuses on the visual semantic information of image tokens to eliminate task prediction. Our method consists of semantic prompt matching, which determines prompts based on similarities between tokens, and image token-level prompting, which applies prompts dire
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#22806;&#37096;&#27169;&#24577;&#24341;&#23548;&#30340;&#25968;&#25454;&#25366;&#25496;&#26694;&#26550;&#20197;&#35299;&#20915;&#33258;&#21160;&#20809;&#23398;&#26816;&#39564;&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#38754;&#20020;&#30340;&#27169;&#22411;&#37096;&#32626;&#25361;&#25112;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11536</link><description>&lt;p&gt;
OCR&#21363;&#20026;&#25152;&#38656;&#65306;&#23558;&#22810;&#27169;&#24577;&#24615;&#24341;&#20837;&#22522;&#20110;&#22270;&#20687;&#30340;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11536
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22806;&#37096;&#27169;&#24577;&#24341;&#23548;&#30340;&#25968;&#25454;&#25366;&#25496;&#26694;&#26550;&#20197;&#35299;&#20915;&#33258;&#21160;&#20809;&#23398;&#26816;&#39564;&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#38754;&#20020;&#30340;&#27169;&#22411;&#37096;&#32626;&#25361;&#25112;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11536v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#21160;&#20809;&#23398;&#26816;&#39564;&#65288;AOI&#65289;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20027;&#35201;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#20202;&#22120;&#36827;&#34892;&#25195;&#25551;&#12290;&#23427;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#32441;&#29702;&#25110;&#22270;&#26696;&#26469;&#26816;&#27979;&#24322;&#24120;&#65292;&#20174;&#32780;&#25104;&#20026;&#24037;&#19994;&#21046;&#36896;&#21644;&#36136;&#37327;&#25511;&#21046;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23613;&#31649;&#23427;&#30340;&#37325;&#35201;&#24615;&#65292;&#29992;&#20110;AOI&#30340;&#27169;&#22411;&#37096;&#32626;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#26679;&#26412;&#37327;&#38459;&#30861;&#20102;&#26377;&#25928;&#29305;&#24449;&#23398;&#20064;&#65292;&#28304;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#22312;&#25104;&#20687;&#36807;&#31243;&#20013;&#20809;&#29031;&#21644;&#25668;&#20687;&#26426;&#20301;&#32622;&#21464;&#21270;&#23545;&#20854;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#22240;&#32032;&#20849;&#21516;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;AOI&#36890;&#24120;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#26469;&#33258;&#26426;&#22120;&#25110;&#22270;&#20687;&#20869;&#37096;&#30340;&#20016;&#23500;&#26426;&#21046;&#21442;&#25968;&#20449;&#24687;&#65292;&#21253;&#25324;&#32463;&#24120;&#26377;&#30410;&#20110;AOI&#20998;&#31867;&#30340;&#32479;&#35745;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#22806;&#37096;&#27169;&#24577;&#24341;&#23548;&#30340;&#25968;&#25454;&#25366;&#25496;&#26694;&#26550;&#65292;&#20027;&#35201;&#26681;&#26893;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11536v1 Announce Type: cross  Abstract: Automatic optical inspection (AOI) plays a pivotal role in the manufacturing process, predominantly leveraging high-resolution imaging instruments for scanning purposes. It detects anomalies by analyzing image textures or patterns, making it an essential tool in industrial manufacturing and quality control. Despite its importance, the deployment of models for AOI often faces challenges. These include limited sample sizes, which hinder effective feature learning, variations among source domains, and sensitivities to changes in lighting and camera positions during imaging. These factors collectively compromise the accuracy of model predictions. Traditional AOI often fails to capitalize on the rich mechanism-parameter information from machines or inside images, including statistical parameters, which typically benefit AOI classification. To address this, we introduce an external modality-guided data mining framework, primarily rooted in o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#35780;&#20272;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#20013;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#26032;&#30340;&#31526;&#21512;AUROC&#21644;&#31526;&#21512;FRP@TPR95&#25351;&#26631;&#65292;&#20026;OOD&#21644;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#25552;&#20379;&#20102;&#27010;&#29575;&#20445;&#23432;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.11532</link><description>&lt;p&gt;
&#24212;&#35813;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#36827;&#34892;&#20998;&#24067;&#22806;&#26816;&#27979;&#65288;&#21453;&#20043;&#20134;&#28982;&#65311;&#65289;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#35780;&#20272;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#20013;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#26032;&#30340;&#31526;&#21512;AUROC&#21644;&#31526;&#21512;FRP@TPR95&#25351;&#26631;&#65292;&#20026;OOD&#21644;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#25552;&#20379;&#20102;&#27010;&#29575;&#20445;&#23432;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26500;&#24314;&#26377;&#25928;&#21306;&#20998;OOD&#25968;&#25454;&#21644;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#25968;&#25454;&#30340;&#20998;&#25968;&#19978;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#20351;&#29992;&#38750;&#19968;&#33268;&#24615;&#20998;&#25968;&#26500;&#24314;&#20855;&#26377;&#27010;&#29575;&#35206;&#30422;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;CP&#26356;&#22909;&#22320;&#35780;&#20272;OOD&#20998;&#25968;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#26679;&#26412;&#22823;&#23567;&#65292;&#35780;&#20272;&#25351;&#26631;&#21487;&#33021;&#36807;&#20110;&#20048;&#35266;&#12290;&#22522;&#20110;&#65288;Bates&#31561;&#20154;&#65292;2022&#65289;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#26032;&#30340;&#31526;&#21512;AUROC&#21644;&#31526;&#21512;FRP@TPR95&#25351;&#26631;&#65292;&#36825;&#20123;&#20462;&#27491;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#25351;&#26631;&#21464;&#24322;&#24615;&#30340;&#27010;&#29575;&#20445;&#23432;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20462;&#27491;&#23545;&#20004;&#20010;&#21442;&#32771;OOD&#21644;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;OpenOOD&#65288;Yang&#31561;&#20154;&#65292;2022&#65289;&#21644;AD-Bench&#65288;Han&#31561;&#20154;&#65292;2022&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20316;&#29992;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11532v1 Announce Type: cross  Abstract: Research on Out-Of-Distribution (OOD) detection focuses mainly on building scores that efficiently distinguish OOD data from In Distribution (ID) data. On the other hand, Conformal Prediction (CP) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In this work, we propose to use CP to better assess the efficiency of OOD scores. Specifically, we emphasize that in standard OOD benchmark settings, evaluation metrics can be overly optimistic due to the finite sample size of the test dataset. Based on the work of (Bates et al., 2022), we define new conformal AUROC and conformal FRP@TPR95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics. We show the effect of these corrections on two reference OOD and anomaly detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al., 2022). We also show that the benefits of us
&lt;/p&gt;</description></item><item><title>LOOPer&#26159;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#24314;&#31435;&#25104;&#26412;&#27169;&#22411;&#26469;&#25351;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#25628;&#32034;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#32534;&#35793;&#22120;&#22312;&#36873;&#25321;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.11522</link><description>&lt;p&gt;
LOOPer: &#19968;&#20010;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11522
&lt;/p&gt;
&lt;p&gt;
LOOPer&#26159;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#24314;&#31435;&#25104;&#26412;&#27169;&#22411;&#26469;&#25351;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#25628;&#32034;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#32534;&#35793;&#22120;&#22312;&#36873;&#25321;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#22312;&#23454;&#29616;&#39640;&#32423;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#22312;&#36873;&#25321;&#33021;&#22815;&#24102;&#26469;&#26368;&#20339;&#21152;&#36895;&#30340;&#26368;&#26377;&#21033;&#36716;&#25442;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20419;&#20351;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#25104;&#26412;&#27169;&#22411;&#26469;&#24341;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#30340;&#25628;&#32034;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#24050;&#32463;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#27010;&#24565;&#39564;&#35777;&#12290;&#34429;&#28982;&#36825;&#31181;&#27010;&#24565;&#39564;&#35777;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#21482;&#25903;&#25345;&#23569;&#37327;&#20223;&#23556;&#21464;&#25442;&#30340;&#23376;&#38598;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#29992;&#22797;&#26434;&#20195;&#30721;&#21464;&#25442;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#36824;&#21482;&#25903;&#25345;&#20855;&#26377;&#21333;&#20010;&#24490;&#29615;&#23884;&#22871;&#21644;&#30697;&#24418;&#36845;&#20195;&#22495;&#30340;&#31616;&#21333;&#31243;&#24207;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#35768;&#22810;&#31243;&#24207;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#26174;&#33879;&#24433;&#21709;&#20102;&#36825;&#26679;&#30340;&#32534;&#35793;&#22120;&#21644;&#33258;&#21160;&#35843;&#24230;&#22120;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11522v1 Announce Type: cross  Abstract: While polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. This has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. While such a proof-of-concept has shown promise, it still has significant limitations. State-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. They also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. These limitations significantly impact the generality of such compilers and autoschedulers and put in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#24310;&#23884;&#20837;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#21450;&#31283;&#20581;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#20419;&#36827;&#31232;&#30095;&#24230;&#20934;&#21017;&#65292;&#33021;&#22815;&#33258;&#21160;&#21644;&#26368;&#20248;&#22320;&#36873;&#25321;&#31232;&#30095;&#27169;&#24577;&#65292;&#21152;&#24555;&#27668;&#21160;&#24377;&#24615;&#27169;&#24577;&#35782;&#21035;&#21644;&#20998;&#26512;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11521</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#24555;&#36895;&#26816;&#27979;&#27668;&#21160;&#24377;&#24615;&#25391;&#21160;&#27169;&#24577;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data-driven Approach for Rapid Detection of Aeroelastic Modes from Flutter Flight Test Based on Limited Sensor Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#24310;&#23884;&#20837;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#21450;&#31283;&#20581;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#20419;&#36827;&#31232;&#30095;&#24230;&#20934;&#21017;&#65292;&#33021;&#22815;&#33258;&#21160;&#21644;&#26368;&#20248;&#22320;&#36873;&#25321;&#31232;&#30095;&#27169;&#24577;&#65292;&#21152;&#24555;&#27668;&#21160;&#24377;&#24615;&#27169;&#24577;&#35782;&#21035;&#21644;&#20998;&#26512;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38453;&#25391;&#39134;&#34892;&#35797;&#39564;&#28041;&#21450;&#36890;&#36807;&#23545;&#39134;&#26426;&#21319;&#38477;&#38754;&#26045;&#21152;&#20154;&#24037;&#28608;&#21169;&#26469;&#35780;&#20272;&#26426;&#20307;&#30340;&#27668;&#21160;&#24377;&#24615;&#31283;&#23450;&#24615;&#12290;&#38543;&#21518;&#25429;&#33719;&#21644;&#20998;&#26512;&#21709;&#24212;&#20197;&#25552;&#21462;&#31995;&#32479;&#30340;&#39057;&#29575;&#21644;&#38459;&#23612;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#22122;&#22768;&#27745;&#26579;&#12289;&#28237;&#27969;&#12289;&#27169;&#24335;&#38750;&#26368;&#20339;&#28608;&#21169;&#20197;&#21450;&#19968;&#20010;&#25110;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#23548;&#33268;&#35813;&#36807;&#31243;&#32791;&#26102;&#24182;&#30772;&#22351;&#25552;&#21462;&#36807;&#31243;&#12290;&#20026;&#20102;&#21152;&#24555;&#35782;&#21035;&#21644;&#20998;&#26512;&#27668;&#21160;&#24377;&#24615;&#27169;&#24577;&#30340;&#36807;&#31243;&#65292;&#26412;&#30740;&#31350;&#23454;&#26045;&#20102;&#19968;&#31181;&#26102;&#24310;&#23884;&#20837;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#36741;&#20043;&#20197;&#31283;&#20581;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#21644;&#19968;&#20010;&#31232;&#30095;&#24615;&#20419;&#36827;&#26631;&#20934;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31232;&#30095;&#27169;&#24577;&#30340;&#33258;&#21160;&#21644;&#26368;&#20339;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#30340;&#31532;&#20116;&#20316;&#32773;&#25552;&#20379;&#30340;&#21311;&#21517;&#21270;&#38453;&#25391;&#39134;&#34892;&#35797;&#39564;&#25968;&#25454;&#22312;&#27492;&#23454;&#26045;&#20013;&#34987;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11521v1 Announce Type: cross  Abstract: Flutter flight test involves the evaluation of the airframes aeroelastic stability by applying artificial excitation on the aircraft lifting surfaces. The subsequent responses are captured and analyzed to extract the frequencies and damping characteristics of the system. However, noise contamination, turbulence, non-optimal excitation of modes, and sensor malfunction in one or more sensors make it time-consuming and corrupt the extraction process. In order to expedite the process of identifying and analyzing aeroelastic modes, this study implements a time-delay embedded Dynamic Mode Decomposition technique. This approach is complemented by Robust Principal Component Analysis methodology, and a sparsity promoting criterion which enables the automatic and optimal selection of sparse modes. The anonymized flutter flight test data, provided by the fifth author of this research paper, is utilized in this implementation. The methodology assu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20998;&#31163;&#29366;&#24577;SARSA&#65288;SS-SARSA&#65289;&#31639;&#27861;&#65292;&#38024;&#23545;&#24674;&#22797;&#32769;&#34382;&#26426;&#22330;&#26223;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#36718;&#25968;&#35270;&#20026;&#29366;&#24577;&#65292;&#38477;&#20302;Q-learning/SARSA&#25152;&#38656;&#30340;&#29366;&#24577;&#32452;&#21512;&#25968;&#37327;&#65292;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#65292;&#24182;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#28176;&#36817;&#25910;&#25947;&#33267;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11520</link><description>&lt;p&gt;
&#20998;&#31163;&#29366;&#24577;SARSA: &#19968;&#31181;&#20855;&#26377;&#24674;&#22797;&#22870;&#21169;&#30340;&#23454;&#29992;&#24207;&#36143;&#20915;&#31574;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11520
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20998;&#31163;&#29366;&#24577;SARSA&#65288;SS-SARSA&#65289;&#31639;&#27861;&#65292;&#38024;&#23545;&#24674;&#22797;&#32769;&#34382;&#26426;&#22330;&#26223;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#36718;&#25968;&#35270;&#20026;&#29366;&#24577;&#65292;&#38477;&#20302;Q-learning/SARSA&#25152;&#38656;&#30340;&#29366;&#24577;&#32452;&#21512;&#25968;&#37327;&#65292;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#65292;&#24182;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#28176;&#36817;&#25910;&#25947;&#33267;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35768;&#22810;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#20551;&#35774;&#25152;&#26377;&#33218;&#30340;&#22870;&#21169;&#22312;&#21508;&#36718;&#20043;&#38388;&#20445;&#25345;&#19981;&#21464;&#65292;&#20294;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#24182;&#19981;&#25104;&#31435;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#24674;&#22797;&#32769;&#34382;&#26426;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#22870;&#21169;&#21462;&#20915;&#20110;&#33258;&#19978;&#27425;&#25289;&#21160;&#33218;&#20197;&#26469;&#32463;&#36807;&#30340;&#36718;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#21517;&#20026;&#20998;&#31163;&#29366;&#24577;SARSA&#65288;SS-SARSA&#65289;&#31639;&#27861;&#65292;&#20854;&#20013;&#23558;&#21508;&#36718;&#35270;&#20026;&#29366;&#24577;&#12290; SS-SARSA&#31639;&#27861;&#36890;&#36807;&#20943;&#23569;Q-learning/SARSA&#25152;&#38656;&#30340;&#29366;&#24577;&#32452;&#21512;&#25968;&#37327;&#26469;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;RL&#38382;&#39064;&#20013;&#32463;&#24120;&#36935;&#21040;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#23545;&#22870;&#21169;&#32467;&#26500;&#36827;&#34892;&#26368;&#23569;&#20551;&#35774;&#24182;&#25552;&#20379;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#28176;&#36817;&#25910;&#25947;&#33267;&#26368;&#20248;&#31574;&#30053;&#12290;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11520v1 Announce Type: new  Abstract: While many multi-armed bandit algorithms assume that rewards for all arms are constant across rounds, this assumption does not hold in many real-world scenarios. This paper considers the setting of recovering bandits (Pike-Burke &amp; Grunewalder, 2019), where the reward depends on the number of rounds elapsed since the last time an arm was pulled. We propose a new reinforcement learning (RL) algorithm tailored to this setting, named the State-Separate SARSA (SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm achieves efficient learning by reducing the number of state combinations required for Q-learning/SARSA, which often suffers from combinatorial issues for large-scale RL problems. Additionally, it makes minimal assumptions about the reward structure and offers lower computational complexity. Furthermore, we prove asymptotic convergence to an optimal policy under mild assumptions. Simulation studies demonstrate the
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31649;&#36947;&#65292;&#32467;&#21512;EfficientNet&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#32954;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;COVID-19&#65292;&#24182;&#22312;&#31454;&#36187;&#25968;&#25454;&#38598;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;</title><link>https://arxiv.org/abs/2403.11505</link><description>&lt;p&gt;
&#20351;&#29992;EfficientNet&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;CT&#25195;&#25551;&#20013;&#26816;&#27979;Covid-19
&lt;/p&gt;
&lt;p&gt;
Covid-19 detection from CT scans using EfficientNet and Attention mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11505
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31649;&#36947;&#65292;&#32467;&#21512;EfficientNet&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#32954;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;COVID-19&#65292;&#24182;&#22312;&#31454;&#36187;&#25968;&#25454;&#38598;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#31649;&#36947;&#30340;COVID-19&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32954;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;COVID-19&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;EfficientNet&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#21435;&#24180;&#31454;&#36187;&#25968;&#25454;&#38598;&#39564;&#35777;&#38598;&#19978;&#30340;&#20854;&#20182;&#22242;&#38431;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11505v1 Announce Type: cross  Abstract: Manual diagnosis and analysis of COVID-19 through the examination of lung Computed Tomography (CT) scan images by physicians tends to result in inefficiency, especially with high patient volumes and numerous images per patient. We address the need for automation by developing a deep learning model-based pipeline for COVID-19 detection from CT scan images of the lungs. The Domain adaptation, Explainability, and Fairness in AI for Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D) provides an opportunity to assess our designed pipeline for COVID-19 detection from CT scan images. The proposed pipeline incorporates EfficientNet with an Attention mechanism with a pre-processing step. Our pipeline outperforms last year's teams on the validation set of the competition dataset.
&lt;/p&gt;</description></item><item><title>MLVICX&#26159;&#19968;&#31181;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#26041;&#24046;&#21644;&#21327;&#26041;&#24046;&#25506;&#32034;&#31574;&#30053;&#26469;&#25429;&#33719;&#20016;&#23500;&#30340;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.11504</link><description>&lt;p&gt;
MLVICX: &#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#22810;&#32423;&#26041;&#24046;-&#21327;&#26041;&#24046;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11504
&lt;/p&gt;
&lt;p&gt;
MLVICX&#26159;&#19968;&#31181;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#26041;&#24046;&#21644;&#21327;&#26041;&#24046;&#25506;&#32034;&#31574;&#30053;&#26469;&#25429;&#33719;&#20016;&#23500;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#20943;&#23569;&#25163;&#21160;&#27880;&#37322;&#38656;&#27714;&#12289;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#29992;&#36884;&#12290;&#36890;&#36807;&#21033;&#29992;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#26080;&#38656;&#25110;&#21482;&#38656;&#23569;&#37327;&#24494;&#35843;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#22797;&#26434;&#35299;&#21078;&#32467;&#26500;&#21644;&#22810;&#26679;&#20020;&#24202;&#30149;&#24773;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#22914;&#33016;&#37096;X&#23556;&#32447;&#65292;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#32534;&#30721;&#32454;&#31890;&#24230;&#32454;&#33410;&#24182;&#20445;&#30041;&#26356;&#24191;&#27867;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#34920;&#24449;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MLVICX&#65288;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#22810;&#32423;&#26041;&#24046;-&#21327;&#26041;&#24046;&#25506;&#32034;&#65289;&#65292;&#19968;&#31181;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25429;&#33719;&#23500;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32423;&#26041;&#24046;&#21644;&#21327;&#26041;&#24046;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#20854;&#26356;&#26377;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11504v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) is potentially useful in reducing the need for manual annotation and making deep learning models accessible for medical image analysis tasks. By leveraging the representations learned from unlabeled data, self-supervised models perform well on tasks that require little to no fine-tuning. However, for medical images, like chest X-rays, which are characterized by complex anatomical structures and diverse clinical conditions, there arises a need for representation learning techniques that can encode fine-grained details while preserving the broader contextual information. In this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning), an approach to capture rich representations in the form of embeddings from chest X-ray images. Central to our approach is a novel multi-level variance and covariance exploration strategy that empowers t
&lt;/p&gt;</description></item><item><title>CLIP&#27169;&#22411;&#22312;&#38754;&#23545;&#20998;&#24067;&#36716;&#31227;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;CounterAnimal&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11497</link><description>&lt;p&gt;
CLIP&#24635;&#26159;&#27604;ImageNet&#27169;&#22411;&#27867;&#21270;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do CLIPs Always Generalize Better than ImageNet Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11497
&lt;/p&gt;
&lt;p&gt;
CLIP&#27169;&#22411;&#22312;&#38754;&#23545;&#20998;&#24067;&#36716;&#31227;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;CounterAnimal&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;CLIP&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#12290;CLIP&#23637;&#31034;&#20102;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;CLIP&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#20026;ImageNet&#22522;&#20934;&#32780;&#35774;&#35745;&#30340;&#21464;&#31181;&#65292;&#21487;&#33021;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;CLIP&#22312;LAION&#31561;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;CounterAnimal&#65292;&#20854;&#20013;&#21253;&#21547;&#21160;&#29289;&#29031;&#29255;&#20013;&#21457;&#29616;&#30340;&#29616;&#23454;&#34394;&#20551;&#29305;&#24449;&#12290;CounterAnimal&#21253;&#25324;a&#65289;&#24120;&#35265;&#32452;&#65306;&#21253;&#25324;&#24120;&#35265;&#32972;&#26223;&#30340;&#21160;&#29289;&#65292;&#24182;&#19988; b) &#23545;&#29031;&#32452;&#65306;&#21253;&#25324;&#22312;&#19981;&#23547;&#24120;&#32972;&#26223;&#19979;&#30340;&#21160;&#29289;&#12290;&#20174;&#24120;&#35265;&#32452;&#21040;&#23545;&#29031;&#32452;&#30340;&#24615;&#33021;&#19979;&#38477;&#37327;&#21270;&#20102;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#65288;&#21363;&#32972;&#26223;&#65289;&#39044;&#27979;&#21160;&#29289;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;LAION&#25110;OpenAI&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;CLIP&#21363;&#27809;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11497v1 Announce Type: cross  Abstract: Large vision language models, such as CLIPs, have revolutionized modern machine learning. CLIPs have demonstrated great generalizability under distribution shifts, supported by an increasing body of literature. However, the evaluation datasets for CLIPs are variations primarily designed for ImageNet benchmarks, which may not fully reflect the extent to which CLIPs, e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap, we collect a real-world dataset called CounterAnimal that contains realistic spurious features found in animal photos. CounterAnimal consists of a) the common group: comprising animals on common backgrounds, and b) the counter group: including animals on unusual backgrounds. The performance drops from the common to counter groups quantify the reliance of models on spurious features (i.e., backgrounds) to predict the animals. We find that CLIPs trained on either LAION or the OpenAI data exhibit no
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Toast&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#21450;&#22686;&#24378;&#29256;DyToast&#65292;&#29992;&#20110;&#23398;&#20064;&#36335;&#32593;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#24182;&#22686;&#24378;&#20102;&#26102;&#38388;&#21160;&#24577;&#30340;&#25972;&#21512;&#65292;&#20197;&#25552;&#39640;&#21508;&#31181;&#26102;&#38388;&#25935;&#24863;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11495</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#38388;&#21160;&#24577;&#30340;&#36335;&#32593;&#35821;&#20041;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11495
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Toast&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#21450;&#22686;&#24378;&#29256;DyToast&#65292;&#29992;&#20110;&#23398;&#20064;&#36335;&#32593;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#24182;&#22686;&#24378;&#20102;&#26102;&#38388;&#21160;&#24577;&#30340;&#25972;&#21512;&#65292;&#20197;&#25552;&#39640;&#21508;&#31181;&#26102;&#38388;&#25935;&#24863;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Toast&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36335;&#32593;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#21450;&#20854;&#22686;&#24378;&#29256;DyToast&#65292;&#26088;&#22312;&#22686;&#24378;&#26102;&#38388;&#21160;&#24577;&#30340;&#25972;&#21512;&#65292;&#25552;&#39640;&#21508;&#31181;&#26102;&#38388;&#25935;&#24863;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#32534;&#30721;&#36335;&#32593;&#22266;&#26377;&#30340;&#20004;&#20010;&#20851;&#38190;&#35821;&#20041;&#29305;&#24449;&#65306;&#20132;&#36890;&#27169;&#24335;&#21644;&#34892;&#39542;&#35821;&#20041;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#32435;&#20837;&#26088;&#22312;&#39044;&#27979;&#19982;&#30446;&#26631;&#36335;&#27573;&#30456;&#20851;&#30340;&#20132;&#36890;&#19978;&#19979;&#25991;&#30340;&#36741;&#21161;&#30446;&#26631;&#65292;&#25913;&#36827;&#20102;skip-gram&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36712;&#36857;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#22312;&#36335;&#32593;&#19978;&#25552;&#28860;&#34892;&#39542;&#35821;&#20041;&#12290;DyToast&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#30410;&#22788;&#29305;&#24615;&#30340;&#32479;&#19968;&#19977;&#35282;&#20989;&#25968;&#36827;&#19968;&#27493;&#22686;&#36827;&#20102;&#35813;&#26694;&#26550;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#36335;&#32593;&#30340;&#26102;&#38388;&#28436;&#21464;&#21644;&#21160;&#24577;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11495v1 Announce Type: cross  Abstract: In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road netwo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25239;&#36951;&#24536;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;EATA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#20027;&#21160;&#26679;&#26412;&#36873;&#25321;&#26631;&#20934;&#21644;&#24341;&#20837;Fisher&#27491;&#21017;&#21270;&#32422;&#26463;&#37325;&#35201;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#19981;&#20250;&#24536;&#35760;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#27979;&#35797;&#26102;&#38388;&#27169;&#22411;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.11491</link><description>&lt;p&gt;
&#19981;&#20250;&#24536;&#21364;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#27979;&#35797;&#26102;&#38388;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11491
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25239;&#36951;&#24536;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;EATA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#20027;&#21160;&#26679;&#26412;&#36873;&#25321;&#26631;&#20934;&#21644;&#24341;&#20837;Fisher&#27491;&#21017;&#21270;&#32422;&#26463;&#37325;&#35201;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#19981;&#20250;&#24536;&#35760;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#27979;&#35797;&#26102;&#38388;&#27169;&#22411;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26088;&#22312;&#36890;&#36807;&#26681;&#25454;&#20219;&#20309;&#27979;&#35797;&#26679;&#26412;&#35843;&#25972;&#32473;&#23450;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#28508;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;TTA&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#20808;&#21069;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#23548;&#33268;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#26080;&#27861;&#25215;&#21463;&#30340;&#20248;&#21270;&#25104;&#26412;&#65307;2&#65289;&#34429;&#28982;&#29616;&#26377;&#30340;TTA&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22312;TTA&#21518;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#36973;&#21463;&#20005;&#37325;&#24615;&#33021;&#19979;&#38477;&#65288;&#21363;&#25152;&#35859;&#30340;&#36951;&#24536;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25239;&#36951;&#24536;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;EATA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#20027;&#21160;&#26679;&#26412;&#36873;&#25321;&#26631;&#20934;&#65292;&#20197;&#35782;&#21035;&#21487;&#38752;&#19988;&#38750;&#20887;&#20313;&#30340;&#26679;&#26412;&#36827;&#34892;&#22312;&#27979;&#35797;&#26102;&#38388;&#30340;&#29109;&#26368;&#23567;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36951;&#24536;&#65292;EATA&#24341;&#20837;&#20102;&#20174;&#27979;&#35797;&#26679;&#26412;&#20013;&#20272;&#35745;&#30340;Fisher&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#32422;&#26463;&#37325;&#35201;&#30340;&#27169;&#22411;&#21442;&#25968;&#20813;&#20110;&#24613;&#21095;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11491v1 Announce Type: new  Abstract: Test-time adaptation (TTA) seeks to tackle potential distribution shifts between training and test data by adapting a given model w.r.t. any test sample. Although recent TTA has shown promising performance, we still face two key challenges: 1) prior methods perform backpropagation for each test sample, resulting in unbearable optimization costs to many applications; 2) while existing TTA can significantly improve the test performance on out-of-distribution data, they often suffer from severe performance degradation on in-distribution data after TTA (known as forgetting). To this end, we have proposed an Efficient Anti-Forgetting Test-Time Adaptation (EATA) method which develops an active sample selection criterion to identify reliable and non-redundant samples for test-time entropy minimization. To alleviate forgetting, EATA introduces a Fisher regularizer estimated from test samples to constrain important model parameters from drastic c
&lt;/p&gt;</description></item><item><title>&#26041;&#24046;&#19981;&#24179;&#34913;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25552;&#20986;&#19968;&#31181;&#19981;&#20381;&#36182;&#39044;&#35757;&#32451;&#22270;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.11483</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Open-World Semi-Supervised Learning for Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11483
&lt;/p&gt;
&lt;p&gt;
&#26041;&#24046;&#19981;&#24179;&#34913;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25552;&#20986;&#19968;&#31181;&#19981;&#20381;&#36182;&#39044;&#35757;&#32451;&#22270;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#23398;&#20064; (Open-world SSL) &#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#65292;&#22312;&#22270;&#24418;&#31038;&#21306;&#20013;&#26159;&#19968;&#20010;&#23454;&#29992;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#23427;&#23558;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#31867;&#20026;&#24050;&#35265;&#31867;&#25110;&#22810;&#20010;&#26032;&#39062;&#31867;&#12290;&#26681;&#25454;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26041;&#24046;&#19981;&#24179;&#34913;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#39044;&#35757;&#32451;&#29305;&#24449;&#32534;&#30721;&#22120;&#21487;&#20197;&#36890;&#36807;&#20026;&#26032;&#39062;&#31867;&#29983;&#25104;&#32039;&#20945;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;&#21508;&#31181;&#31867;&#22411;&#30340;&#22270;&#24418;&#25968;&#25454;&#21019;&#24314;&#36890;&#29992;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#19981;&#20381;&#36182;&#39044;&#35757;&#32451;&#22270;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11483v1 Announce Type: cross  Abstract: Open-world semi-supervised learning (Open-world SSL) for node classification, that classifies unlabeled nodes into seen classes or multiple novel classes, is a practical but under-explored problem in the graph community. As only seen classes have human labels, they are usually better learned than novel classes, and thus exhibit smaller intra-class variances within the embedding space (named as imbalance of intra-class variances between seen and novel classes). Based on empirical and theoretical analysis, we find the variance imbalance can negatively impact the model performance. Pre-trained feature encoders can alleviate this issue via producing compact representations for novel classes. However, creating general pre-trained encoders for various types of graph data has been proven to be challenging. As such, there is a demand for an effective method that does not rely on pre-trained graph encoders. In this paper, we propose an IMbalanc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;3D&#22320;&#38663;&#25968;&#25454;&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#22797;&#26434;&#32570;&#22833;&#27169;&#24335;&#26102;&#25552;&#39640;&#37325;&#24314;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.11482</link><description>&lt;p&gt;
SeisFusion: &#24102;&#26377;&#36755;&#20837;&#25351;&#23548;&#30340;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;3D&#22320;&#38663;&#25968;&#25454;&#25554;&#20540;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;3D&#22320;&#38663;&#25968;&#25454;&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#22797;&#26434;&#32570;&#22833;&#27169;&#24335;&#26102;&#25552;&#39640;&#37325;&#24314;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#12289;&#29289;&#29702;&#25110;&#32463;&#27982;&#32422;&#26463;&#36890;&#24120;&#23548;&#33268;&#22320;&#38663;&#25968;&#25454;&#20013;&#23384;&#22312;&#32570;&#22833;&#30340;&#30165;&#36857;&#65292;&#20351;&#24471;&#37325;&#24314;&#23436;&#25972;&#30340;&#22320;&#38663;&#25968;&#25454;&#25104;&#20026;&#22320;&#38663;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20256;&#32479;&#30340;&#22320;&#38663;&#25968;&#25454;&#37325;&#24314;&#26041;&#27861;&#38656;&#35201;&#36873;&#25321;&#22810;&#20010;&#32463;&#39564;&#21442;&#25968;&#65292;&#24182;&#19988;&#38590;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#36830;&#32493;&#32570;&#22833;&#25968;&#25454;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#37325;&#24314;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20195;&#34920;&#20102;&#19968;&#31181;&#28857;&#23545;&#28857;&#30340;&#37325;&#24314;&#26041;&#27861;&#65292;&#21487;&#33021;&#26080;&#27861;&#35206;&#30422;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#24403;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#32570;&#22833;&#27169;&#24335;&#30340;&#22320;&#38663;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#33021;&#20250;&#32463;&#21382;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;3D&#22320;&#38663;&#25968;&#25454;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11482v1 Announce Type: new  Abstract: Geographical, physical, or economic constraints often result in missing traces within seismic data, making the reconstruction of complete seismic data a crucial step in seismic data processing. Traditional methods for seismic data reconstruction require the selection of multiple empirical parameters and struggle to handle large-scale continuous missing data. With the development of deep learning, various neural networks have demonstrated powerful reconstruction capabilities. However, these convolutional neural networks represent a point-to-point reconstruction approach that may not cover the entire distribution of the dataset. Consequently, when dealing with seismic data featuring complex missing patterns, such networks may experience varying degrees of performance degradation. In response to this challenge, we propose a novel diffusion model reconstruction framework tailored for 3D seismic data. To constrain the results generated by the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#24369;&#36890;&#20449;MDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#20026; $\tilde{O}(SA\frac{H}{\epsilon^2})$&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#26159;&#22312;&#25152;&#26377;&#21442;&#25968;&#19978;&#26368;&#23567;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.11477</link><description>&lt;p&gt;
&#24369;&#36890;&#20449;&#21644;&#19968;&#33324;&#24179;&#22343;&#22870;&#36175;MDPs&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#24369;&#36890;&#20449;MDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#20026; $\tilde{O}(SA\frac{H}{\epsilon^2})$&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#26159;&#22312;&#25152;&#26377;&#21442;&#25968;&#19978;&#26368;&#23567;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#27169;&#22411;&#19979;&#23398;&#20064;&#24179;&#22343;&#22870;&#36175;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;$\epsilon$-&#26368;&#20339;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;&#24369;&#36890;&#20449;MDPs&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22797;&#26434;&#24230;&#30028;&#38480;&#20026;$\tilde{O}(SA\frac{H}{\epsilon^2})$&#65292;&#20854;&#20013;$H$&#26159;&#26368;&#20248;&#31574;&#30053;&#30340;&#20559;&#24046;&#20989;&#25968;&#30340;&#36328;&#24230;&#65292;$SA$&#26159;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22522;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22312;&#25152;&#26377;&#21442;&#25968;$S,A,H$&#21644;$\epsilon$&#19978;&#65288;&#26368;&#22810;&#23545;&#25968;&#22240;&#23376;&#65289;&#26368;&#23567;&#26368;&#20248;&#30340;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#29616;&#26377;&#24037;&#20316;&#35201;&#20040;&#20551;&#35774;&#25152;&#26377;&#31574;&#30053;&#30340;&#28151;&#21512;&#26102;&#38388;&#22343;&#21248;&#26377;&#30028;&#65292;&#35201;&#20040;&#23545;&#21442;&#25968;&#26377;&#27425;&#20248;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#19968;&#33324;&#65288;&#38750;&#24369;&#36890;&#20449;&#65289;&#24179;&#22343;&#22870;&#36175;MDPs&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#20010;&#26032;&#30340;&#30636;&#24577;&#26102;&#38388;&#21442;&#25968;$B$&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;$\tilde{O}(SA\frac{B+H}{\epsilon^2})$&#30340;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#21305;&#37197;&#30340;&#65288;&#26368;&#22810;&#23545;&#25968;&#22240;&#23376;&#65289;&#26368;&#23567;&#26368;&#20248;&#19979;&#30028;&#12290;&#36825;&#20004;&#20010;&#32467;&#26524;&#37117;&#26159;&#22522;&#20110;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11477v1 Announce Type: new  Abstract: We study the sample complexity of learning an $\epsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\tilde{O}(SA\frac{H}{\epsilon^2})$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\epsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We further investigate sample complexity in general (non-weakly-communicating) average-reward MDPs. We argue a new transient time parameter $B$ is necessary, establish an $\tilde{O}(SA\frac{B+H}{\epsilon^2})$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#21152;&#36895;&#23398;&#20064;&#32034;&#24341;&#32467;&#26500;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#23383;&#31526;&#20018;&#38190;&#22312;&#23398;&#20064;&#32034;&#24341;&#20013;&#24615;&#33021;&#29942;&#39048;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11472</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#35760;&#24518;&#30340;&#22686;&#37327;&#35757;&#32451;&#21152;&#36895;&#23383;&#31526;&#20018;&#38190;&#30340;&#23398;&#20064;&#32034;&#24341;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Accelerating String-Key Learned Index Structures via Memoization-based Incremental Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11472
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#21152;&#36895;&#23398;&#20064;&#32034;&#24341;&#32467;&#26500;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#23383;&#31526;&#20018;&#38190;&#22312;&#23398;&#20064;&#32034;&#24341;&#20013;&#24615;&#33021;&#29942;&#39048;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32034;&#24341;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23398;&#20064;&#38190;&#21644;&#23427;&#20204;&#22312;&#38190;-&#20540;&#32034;&#24341;&#20013;&#23545;&#24212;&#20301;&#32622;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#20123;&#32034;&#24341;&#20351;&#29992;&#26144;&#23556;&#20449;&#24687;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#23398;&#20064;&#32034;&#24341;&#38656;&#35201;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;&#23427;&#20204;&#30340;&#27169;&#22411;&#20197;&#25972;&#21512;&#26356;&#26032;&#26597;&#35810;&#24341;&#20837;&#30340;&#26356;&#25913;&#12290;&#20026;&#20102;&#26377;&#25928;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#32034;&#24341;&#31995;&#32479;&#32463;&#24120;&#21033;&#29992;&#19968;&#31181;&#32447;&#24615;&#20195;&#25968;QR&#20998;&#35299;&#25216;&#26415;&#26469;&#25191;&#34892;&#30697;&#38453;&#20998;&#35299;&#12290;&#36825;&#31181;&#22240;&#23376;&#21270;&#26041;&#27861;&#22312;&#27599;&#27425;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#20013;&#22788;&#29702;&#25152;&#26377;&#38190;-&#20301;&#32622;&#23545;&#65292;&#23548;&#33268;&#38543;&#30528;&#38190;&#21644;&#23427;&#20204;&#38271;&#24230;&#30340;&#24635;&#25968;&#32447;&#24615;&#22686;&#38271;&#30340;&#35745;&#31639;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;&#37325;&#26032;&#35757;&#32451;&#20250;&#36896;&#25104;&#20005;&#37325;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21487;&#21464;&#38271;&#24230;&#30340;&#23383;&#31526;&#20018;&#38190;&#65292;&#32780;&#37325;&#26032;&#35757;&#32451;&#23545;&#20110;&#20445;&#25345;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#20197;&#21450;&#30830;&#20445;&#20302;&#26597;&#35810;&#26381;&#21153;&#24310;&#36831;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11472v1 Announce Type: new  Abstract: Learned indexes use machine learning models to learn the mappings between keys and their corresponding positions in key-value indexes. These indexes use the mapping information as training data. Learned indexes require frequent retrainings of their models to incorporate the changes introduced by update queries. To efficiently retrain the models, existing learned index systems often harness a linear algebraic QR factorization technique that performs matrix decomposition. This factorization approach processes all key-position pairs during each retraining, resulting in compute operations that grow linearly with the total number of keys and their lengths. Consequently, the retrainings create a severe performance bottleneck, especially for variable-length string keys, while the retrainings are crucial for maintaining high prediction accuracy and in turn, ensuring low query service latency.   To address this performance problem, we develop an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26426;&#21046;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#22312;&#38750;iid&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11464</link><description>&lt;p&gt;
FedSPU&#65306;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#30340;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11464
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26426;&#21046;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#22312;&#38750;iid&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#37327;&#30340;&#38750;iid&#23458;&#25143;&#31471;&#25968;&#25454;&#65292;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#25317;&#26377;&#30340;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#21487;&#33021;&#26045;&#21152;&#19981;&#21516;&#31243;&#24230;&#30340;&#36164;&#28304;&#32422;&#26463;&#65292;&#32473;PFL&#36896;&#25104;&#35745;&#31639;&#21644;&#36890;&#20449;&#29942;&#39048;&#12290;&#32852;&#37030;Dropout&#24050;&#25104;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#27969;&#34892;&#31574;&#30053;&#65292;&#20854;&#20013;&#20165;&#22312;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#30340;&#19968;&#20010;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;Dropout&#30340;&#27169;&#22411;&#20462;&#21098;&#31574;&#30053;&#21487;&#33021;&#24341;&#20837;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;iid&#26412;&#22320;&#25968;&#25454;&#12290;&#24403;&#26377;&#20559;&#35265;&#30340;&#23376;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#39640;&#24230;&#20998;&#25955;&#21442;&#25968;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FedSPU&#65289;&#12290;&#19982;&#19987;&#38376;&#20026;&#23567;&#22411;&#26412;&#22320;&#23376;&#27169;&#22411;&#23450;&#21046;&#20840;&#23616;&#27169;&#22411;&#30340;Dropout&#19981;&#21516;&#65292;FedSPU&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26426;&#21046;&#65292;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11464v1 Announce Type: new  Abstract: Personalized Federated Learning (PFL) is widely employed in IoT applications to handle high-volume, non-iid client data while ensuring data privacy. However, heterogeneous edge devices owned by clients may impose varying degrees of resource constraints, causing computation and communication bottlenecks for PFL. Federated Dropout has emerged as a popular strategy to address this challenge, wherein only a subset of the global model, i.e. a \textit{sub-model}, is trained on a client's device, thereby reducing computation and communication overheads. Nevertheless, the dropout-based model-pruning strategy may introduce bias, particularly towards non-iid local data. When biased sub-models absorb highly divergent parameters from other clients, performance degradation becomes inevitable. In response, we propose federated learning with stochastic parameter update (FedSPU). Unlike dropout that tailors the global model to small-size local sub-model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#22240;&#26524;&#21457;&#29616;&#21151;&#33021;&#30340;&#22270;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#37096;&#20998;&#26631;&#35760;&#23398;&#20064;&#29615;&#22659;&#20013;&#26377;&#25928;&#23398;&#20064;&#21306;&#20998;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.11449</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#22240;&#26524;&#21457;&#29616;&#21151;&#33021;&#30340;&#22270;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Partial Label Learning with Potential Cause Discovering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11449
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#22240;&#26524;&#21457;&#29616;&#21151;&#33021;&#30340;&#22270;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#37096;&#20998;&#26631;&#35760;&#23398;&#20064;&#29615;&#22659;&#20013;&#26377;&#25928;&#23398;&#20064;&#21306;&#20998;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22240;&#20854;&#22312;&#35299;&#20915;&#21508;&#39046;&#22495;&#22797;&#26434;&#22270;&#32467;&#26500;&#25968;&#25454;&#25361;&#25112;&#20013;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#26631;&#27880;&#22270;&#25968;&#25454;&#20197;&#36827;&#34892;&#35757;&#32451;&#30001;&#20110;&#22270;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#30456;&#20114;&#20851;&#32852;&#24615;&#32780;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#24471;GNN&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21306;&#20998;&#20449;&#24687;&#65292;&#21363;&#20351;&#22312;&#37096;&#20998;&#26631;&#35760;&#23398;&#20064;&#65288;PLL&#65289;&#30340;&#29615;&#22659;&#20013;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#12290; PLL&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#19982;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614;&#30456;&#20851;&#32852;&#65292;&#21253;&#25324;&#30495;&#23454;&#26631;&#31614;&#21644;&#39069;&#22806;&#30340;&#22122;&#22768;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28508;&#22312;&#22240;&#26524;&#25552;&#21462;&#26469;&#33719;&#21462;&#20855;&#26377;&#26356;&#39640;&#22240;&#26524;&#20851;&#31995;&#21487;&#33021;&#24615;&#30340;&#22270;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#25552;&#21462;&#30340;&#22270;&#30340;&#36741;&#21161;&#35757;&#32451;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11449v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have gained considerable attention for their potential in addressing challenges posed by complex graph-structured data in diverse domains. However, accurately annotating graph data for training is difficult due to the inherent complexity and interconnectedness of graphs. To tackle this issue, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information even in the presence of noisy labels within the context of Partially Labeled Learning (PLL). PLL is a critical weakly supervised learning problem, where each training instance is associated with a set of candidate labels, including both the true label and additional noisy labels. Our approach leverages potential cause extraction to obtain graph data that exhibit a higher likelihood of possessing a causal relationship with the labels. By incorporating auxiliary training based on the extracted gra
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#24341;&#23548;&#36827;&#21270;"&#65288;GE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#20462;&#25913;&#20195;&#30721;&#65292;&#37319;&#29992;&#33258;&#25105;&#32500;&#25345;&#30340;&#21453;&#39304;&#24490;&#29615;&#22686;&#24378;&#27169;&#22411;&#36827;&#21270;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.11446</link><description>&lt;p&gt;
LLM&#24341;&#23548;&#36827;&#21270;-&#27169;&#22411;&#25512;&#36827;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
LLM Guided Evolution - The Automation of Models Advancing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#24341;&#23548;&#36827;&#21270;"&#65288;GE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#20462;&#25913;&#20195;&#30721;&#65292;&#37319;&#29992;&#33258;&#25105;&#32500;&#25345;&#30340;&#21453;&#39304;&#24490;&#29615;&#22686;&#24378;&#27169;&#22411;&#36827;&#21270;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#27169;&#22411;&#21457;&#23637;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#65288;&#22914;AutoML&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#23618;&#23618;&#25277;&#35937;&#65292;&#22914;&#22522;&#20110;&#26641;&#25110;Cartesian&#36951;&#20256;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8220;&#24341;&#23548;&#36827;&#21270;&#8221;&#65288;GE&#65289;&#65292;&#23427;&#19981;&#21516;&#20110;&#36825;&#20123;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#20462;&#25913;&#20195;&#30721;&#12290;GE&#21033;&#29992;LLMs&#36827;&#34892;&#26356;&#26234;&#33021;&#30340;&#12289;&#26377;&#30417;&#30563;&#30340;&#36827;&#21270;&#36807;&#31243;&#65292;&#24341;&#23548;&#21464;&#24322;&#21644;&#20132;&#21449;&#12290;&#25105;&#20204;&#29420;&#29305;&#30340;&#8220;&#24605;&#24819;&#36827;&#21270;&#8221;&#65288;EoT&#65289;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;GE&#65292;&#20351;LLMs&#33021;&#22815;&#21453;&#24605;&#21644;&#20174;&#20808;&#21069;&#21464;&#24322;&#30340;&#32467;&#26524;&#20013;&#23398;&#20064;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#33258;&#25105;&#32500;&#25345;&#30340;&#21453;&#39304;&#24490;&#29615;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#36827;&#21270;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#29983;&#25104;&#23545;&#31934;&#24515;&#21046;&#20316;&#30340;&#25552;&#31034;&#30340;&#22810;&#26679;&#21709;&#24212;&#21644;&#35843;&#33410;&#27169;&#22411;&#28201;&#24230;&#26469;&#32500;&#25345;&#36951;&#20256;&#22810;&#26679;&#24615;&#65292;GE&#32500;&#25345;&#30528;&#23545;&#36827;&#21270;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#30340;&#36951;&#20256;&#22810;&#26679;&#24615;&#12290;&#36825;&#19981;&#20165;&#21152;&#36895;&#20102;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11446v1 Announce Type: cross  Abstract: In the realm of machine learning, traditional model development and automated approaches like AutoML typically rely on layers of abstraction, such as tree-based or Cartesian genetic programming. Our study introduces "Guided Evolution" (GE), a novel framework that diverges from these methods by utilizing Large Language Models (LLMs) to directly modify code. GE leverages LLMs for a more intelligent, supervised evolutionary process, guiding mutations and crossovers. Our unique "Evolution of Thought" (EoT) technique further enhances GE by enabling LLMs to reflect on and learn from the outcomes of previous mutations. This results in a self-sustaining feedback loop that augments decision-making in model evolution. GE maintains genetic diversity, crucial for evolutionary algorithms, by leveraging LLMs' capability to generate diverse responses from expertly crafted prompts and modulate model temperature. This not only accelerates the evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#28155;&#21152;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11432</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#30340;&#25581;&#31192;
&lt;/p&gt;
&lt;p&gt;
Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#28155;&#21152;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20986;&#29616;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#25968;&#37327;&#28608;&#22686;&#12290;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#24050;&#25104;&#20026;&#20854;&#20013;&#19968;&#39033;&#20027;&#35201;&#24212;&#29992;&#65292;&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#25110;&#39640;&#38454;&#36816;&#21160;&#23398;&#21464;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#31163;&#25955;&#36873;&#25321;&#25110;&#36830;&#32493;&#25511;&#21046;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;DRL&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#20102;&#22522;&#20110;&#36830;&#32493;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;DRL&#31639;&#27861;&#20316;&#20026;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#20998;&#26512;&#25216;&#26415;&#26469;&#35752;&#35770;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11432v1 Announce Type: cross  Abstract: With the advent of universal function approximators in the domain of reinforcement learning, the number of practical applications leveraging deep reinforcement learning (DRL) has exploded. Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output. However, the black-box nature of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs). Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework. We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV simulation environment. We provide some analytical techniques for discussing the interpretability of the trained mode
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11425</link><description>&lt;p&gt;
&#21465;&#20107;&#29305;&#24449;&#36824;&#26159;&#32467;&#26500;&#29305;&#24449;&#65311;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11425
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#27835;&#30103;&#24050;&#30693;&#20250;&#24341;&#20837;&#24515;&#27602;&#24615;&#65292;&#23545;&#39044;&#21518;&#21644;&#29983;&#23384;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#23545;&#20110;&#25913;&#21892;&#30284;&#30151;&#27835;&#30103;&#32467;&#26524;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#21253;&#25324;&#20256;&#32479;ML&#12289;&#26102;&#38388;&#24863;&#30693;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;T-LSTM&#65289;&#21644;&#20351;&#29992;&#20174;&#32467;&#26500;&#21270;&#21307;&#23398;&#20195;&#30721;&#34893;&#29983;&#30340;&#26032;&#39062;&#21465;&#36848;&#29305;&#24449;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35782;&#21035;&#24739;HF&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#12290;&#25105;&#20204;&#20174;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#35782;&#21035;&#20102;&#19968;&#32452;&#21253;&#25324;12,806&#21517;&#32954;&#30284;&#12289;&#20083;&#33146;&#30284;&#21644;&#32467;&#30452;&#32928;&#30284;&#24739;&#32773;&#30340;&#30284;&#30151;&#38431;&#21015;&#65292;&#20854;&#20013;1,602&#20154;&#22312;&#30284;&#30151;&#21518;&#21457;&#23637;&#20026;HF&#12290;LLM GatorTron-3.9B&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;F1&#20998;&#25968;&#65292;&#27604;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#39640;&#20986;39%&#65292;&#27604;T-LSTM&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;7%&#65292;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;BERT&#39640;&#20986;5.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26144;&#23556;&#26041;&#27861;&#65292;&#23558;&#24191;&#27867;&#31867;&#30340;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#65292;&#20174;&#32780;&#21487;&#20197;&#23545;Feynman&#36335;&#24452;&#31215;&#20998;&#20013;&#30340;&#20219;&#24847;&#36335;&#24452;&#36827;&#34892;&#32479;&#35745;&#27714;&#21644;&#12290;</title><link>https://arxiv.org/abs/2403.11420</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#37327;&#23376;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Neural network representation of quantum systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11420
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26144;&#23556;&#26041;&#27861;&#65292;&#23558;&#24191;&#27867;&#31867;&#30340;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#65292;&#20174;&#32780;&#21487;&#20197;&#23545;Feynman&#36335;&#24452;&#31215;&#20998;&#20013;&#30340;&#20219;&#24847;&#36335;&#24452;&#36827;&#34892;&#32479;&#35745;&#27714;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#65292;&#25509;&#36817;&#39640;&#26031;&#36807;&#31243;&#30340;&#38543;&#26426;&#23485;&#31070;&#32463;&#32593;&#32476;&#26159;&#22260;&#32469;&#39640;&#26031;&#22266;&#23450;&#28857;&#30340;&#37327;&#23376;&#22330;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26144;&#23556;&#65292;&#36890;&#36807;&#35813;&#26144;&#23556;&#65292;&#21487;&#20197;&#23558;&#19968;&#22823;&#31867;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#34920;&#36798;&#20026;&#20855;&#26377;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#32479;&#35745;&#27714;&#21644;&#24418;&#24335;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;&#24605;&#24819;&#26159;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#29983;&#25104;&#36153;&#26364;&#36335;&#24452;&#31215;&#20998;&#20013;&#30340;&#20219;&#24847;&#36335;&#24452;&#12290;&#36825;&#31181;&#26144;&#23556;&#21487;&#20197;&#24212;&#29992;&#20110;&#30456;&#20114;&#20316;&#29992;&#30340;&#37327;&#23376;&#31995;&#32479;/&#22330;&#35770;&#65292;&#21363;&#20351;&#36828;&#31163;&#39640;&#26031;&#26497;&#38480;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20351;&#26426;&#22120;&#23398;&#20064;&#19982;&#37327;&#23376;&#19990;&#30028;&#26356;&#21152;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11420v1 Announce Type: cross  Abstract: It has been proposed that random wide neural networks near Gaussian process are quantum field theories around Gaussian fixed points. In this paper, we provide a novel map with which a wide class of quantum mechanical systems can be cast into the form of a neural network with a statistical summation over network parameters. Our simple idea is to use the universal approximation theorem of neural networks to generate arbitrary paths in the Feynman's path integral. The map can be applied to interacting quantum systems / field theories, even away from the Gaussian limit. Our findings bring machine learning closer to the quantum world.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26174;&#24335;&#21442;&#25968;&#21270;&#36807;&#28193;&#20989;&#25968;&#26469;&#23398;&#20064;&#36712;&#36857;&#20998;&#24067;&#30340;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#36712;&#36857;&#30340;&#39640;&#25928;&#21512;&#25104;&#21644;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.11418</link><description>&lt;p&gt;
&#26102;&#38388;&#36712;&#36857;&#30340;&#21464;&#20998;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Variational Sampling of Temporal Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26174;&#24335;&#21442;&#25968;&#21270;&#36807;&#28193;&#20989;&#25968;&#26469;&#23398;&#20064;&#36712;&#36857;&#20998;&#24067;&#30340;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#36712;&#36857;&#30340;&#39640;&#25928;&#21512;&#25104;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#30830;&#23450;&#24615;&#30340;&#26102;&#38388;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#20854;&#36712;&#36857;&#26469;&#30830;&#23450;&#65292;&#35813;&#36712;&#36857;&#26159;(a)&#21021;&#22987;&#26465;&#20214;$z_0 \in \mathcal{Z}$&#21644;(b)&#36807;&#28193;&#20989;&#25968;$f:(\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$&#30340;&#20056;&#31215;&#31354;&#38388;&#20013;&#30340;&#20803;&#32032;&#65292;&#24448;&#24448;&#21463;&#24213;&#23618;&#21160;&#21147;&#31995;&#32479;&#25511;&#21046;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#36807;&#28193;&#20989;&#25968;&#24314;&#27169;&#20026;&#24494;&#20998;&#26041;&#31243;&#25110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#39044;&#27979;&#26410;&#26469;&#27979;&#37327;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#20294;&#24456;&#23569;&#26377;&#32467;&#26524;&#25104;&#21151;&#24314;&#31435;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36712;&#36857;&#37319;&#26679;&#21644;&#32479;&#35745;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#21442;&#25968;&#21270;&#26041;&#38754;&#30340;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#36807;&#28193;&#20989;&#25968;$f$&#26174;&#24335;&#22320;&#21442;&#25968;&#21270;&#20026;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#20803;&#32032;&#26469;&#23398;&#20064;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#26377;&#25928;&#22320;&#21512;&#25104;&#26032;&#39062;&#30340;&#36712;&#36857;&#65292;&#21516;&#26102;&#36824;&#30452;&#25509;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#20415;&#30340;&#24037;&#20855;&#26469;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11418v1 Announce Type: cross  Abstract: A deterministic temporal process can be determined by its trajectory, an element in the product space of (a) initial condition $z_0 \in \mathcal{Z}$ and (b) transition function $f: (\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$ often influenced by the control of the underlying dynamical system. Existing methods often model the transition function as a differential equation or as a recurrent neural network. Despite their effectiveness in predicting future measurements, few results have successfully established a method for sampling and statistical inference of trajectories using neural networks, partially due to constraints in the parameterization. In this work, we introduce a mechanism to learn the distribution of trajectories by parameterizing the transition function $f$ explicitly as an element in a function space. Our framework allows efficient synthesis of novel trajectories, while also directly providing a convenient tool for inferen
&lt;/p&gt;</description></item><item><title>DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11415</link><description>&lt;p&gt;
DreamSampler&#65306;&#32479;&#19968;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#20197;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11415
&lt;/p&gt;
&lt;p&gt;
DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#24050;&#25104;&#20026;&#26368;&#36817;&#20960;&#24180;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LDM&#26550;&#26500;&#25110;&#29305;&#24449;&#24037;&#31243;&#65292;&#20998;&#25968;&#33976;&#39311;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20294;&#24448;&#24448;&#23481;&#26131;&#21457;&#29983;&#27169;&#24335;&#23849;&#28291;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#24182;&#21033;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;DreamSampler&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#28508;&#22312;&#20248;&#21270;&#30340;&#35270;&#35282;&#26080;&#32541;&#22320;&#25972;&#21512;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#20998;&#25968;&#33976;&#39311;&#65292;DreamSampler&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;LDM&#26550;&#26500;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20294;&#23427;&#20801;&#35768;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#36827;&#34892;&#33976;&#39311;&#21644;&#21453;&#21521;&#37319;&#26679;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#28041;&#21450;&#22270;&#20687;&#32534;&#36753;&#12289;SVG&#37325;&#26500;&#31561;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11415v1 Announce Type: cross  Abstract: Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent diffusion models (LDMs). While reverse diffusion sampling often requires adjustments of LDM architecture or feature engineering, score distillation offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing. To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization. Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both distillation and reverse sampling with additional guidance for image editing and reconstruction. Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#22810;&#26679;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#20013;&#36873;&#25321;&#24615;&#37319;&#26679;&#36127;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#22810;&#23618;GNNs&#20013;&#23454;&#29616;&#20102;&#36880;&#23618;&#36127;&#26679;&#26412;&#22810;&#26679;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11408</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#22810;&#26679;&#36127;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Layer-diverse Negative Sampling for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11408
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#22810;&#26679;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#20013;&#36873;&#25321;&#24615;&#37319;&#26679;&#36127;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#22810;&#23618;GNNs&#20013;&#23454;&#29616;&#20102;&#36880;&#23618;&#36127;&#26679;&#26412;&#22810;&#26679;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26159;&#21508;&#31181;&#32467;&#26500;&#23398;&#20064;&#24212;&#29992;&#30340;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#22270;&#25968;&#25454;&#30340;&#34920;&#31034;&#33021;&#21147;&#24378;&#22823;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;GNNs&#20381;&#36182;&#20110;&#20165;&#20174;&#19968;&#38454;&#37051;&#23621;&#65288;&#21363;&#27491;&#26679;&#26412;&#65289;&#25910;&#38598;&#20449;&#24687;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#30340;&#23618;&#22810;&#26679;&#36127;&#37319;&#26679;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#37319;&#26679;&#30697;&#38453;&#65292;&#23558;&#20505;&#36873;&#38598;&#36716;&#25442;&#20026;&#19968;&#20010;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#31354;&#38388;&#36873;&#25321;&#24615;&#22320;&#23545;&#20854;&#36827;&#34892;&#37319;&#26679;&#20197;&#29983;&#25104;&#36127;&#26679;&#26412;&#12290;&#20026;&#20102;&#22312;&#27599;&#20010;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#36827;&#19968;&#27493;&#22686;&#24378;&#36127;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31354;&#38388;&#21387;&#32553;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22810;&#23618;GNNs&#20013;&#30340;&#36880;&#23618;&#22810;&#26679;&#24615;&#12290;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#22270;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11408v1 Announce Type: new  Abstract: Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20808;&#39564;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20998;&#38548;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.11407</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20998;&#38548;&#21518;&#39564;&#37319;&#26679;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20808;&#39564;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20998;&#38548;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#20316;&#20026;&#36870;&#36125;&#21494;&#26031;&#38382;&#39064;&#27714;&#35299;&#30340;&#20808;&#39564;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20174;&#32467;&#26524;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#26679;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36817;&#20284;&#26041;&#27861;&#26469;&#20559;&#32622;&#25193;&#25955;&#30340;&#28418;&#31227;&#39033;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;DDM&#20808;&#39564;&#30340;&#29305;&#23450;&#32467;&#26500;&#26469;&#23450;&#20041;&#19968;&#32452;&#20013;&#38388;&#21644;&#26356;&#31616;&#21333;&#30340;&#21518;&#39564;&#25277;&#26679;&#38382;&#39064;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#20363;&#23376;&#21644;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#26469;&#23454;&#35777;&#22320;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#23545;&#20110;&#19968;&#33324;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#37325;&#26500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11407v1 Announce Type: cross  Abstract: Interest in the use of Denoising Diffusion Models (DDM) as priors for solving inverse Bayesian problems has recently increased significantly. However, sampling from the resulting posterior distribution poses a challenge. To solve this problem, previous works have proposed approximations to bias the drift term of the diffusion. In this work, we take a different approach and utilize the specific structure of the DDM prior to define a set of intermediate and simpler posterior sampling problems, resulting in a lower approximation error compared to previous methods. We empirically demonstrate the reconstruction capability of our method for general linear inverse problems using synthetic examples and various image restoration tasks.
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#20852;&#36215;&#39537;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#25968;&#25454;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11395</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automated data processing and feature engineering for deep learning and big data applications: a survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11395
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#20852;&#36215;&#39537;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#25968;&#25454;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#24182;&#22312;AI&#30340;&#21457;&#23637;&#20013;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#29305;&#21035;&#26159;&#22312;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#12290;&#23427;&#20063;&#31616;&#21270;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#22240;&#20026;&#23398;&#20064;&#36807;&#31243;&#26159;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#37117;&#24050;&#33258;&#21160;&#21270;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#24517;&#39035;&#22312;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20043;&#21069;&#32463;&#36807;&#25163;&#21160;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#36827;&#19968;&#27493;&#25193;&#23637;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#29992;&#20110;&#33258;&#21160;&#21270;&#36825;&#20123;&#20219;&#21153;&#30340;&#29305;&#27530;&#25216;&#26415;&#12290;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#39537;&#21160;&#21147;&#26159;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#12289;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#12290;&#22914;&#20170;&#65292;&#22522;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;A
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11395v1 Announce Type: cross  Abstract: Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (A
&lt;/p&gt;</description></item><item><title>&#25237;&#24433;&#22836;&#25216;&#26415;&#33021;&#22815;&#36890;&#36807;&#36880;&#23618;&#28176;&#36827;&#30340;&#29305;&#24449;&#21152;&#26435;&#21644;&#26356;&#20026;&#24402;&#19968;&#21270;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11391</link><description>&lt;p&gt;
&#25506;&#31350;&#25237;&#24433;&#22836;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Investigating the Benefits of Projection Head for Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11391
&lt;/p&gt;
&lt;p&gt;
&#25237;&#24433;&#22836;&#25216;&#26415;&#33021;&#22815;&#36890;&#36807;&#36880;&#23618;&#28176;&#36827;&#30340;&#29305;&#24449;&#21152;&#26435;&#21644;&#26356;&#20026;&#24402;&#19968;&#21270;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#33719;&#21462;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#26377;&#25928;&#25216;&#26415;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#32534;&#30721;&#22120;&#39030;&#37096;&#28155;&#21152;&#19968;&#20010;&#25237;&#24433;&#22836;&#65292;&#28982;&#21518;&#20002;&#24323;&#23427;&#24182;&#20351;&#29992;&#39044;&#25237;&#24433;&#34920;&#31034;&#12290;&#23613;&#31649;&#35813;&#25216;&#26415;&#34987;&#35777;&#26126;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#65292;&#20294;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#19981;&#26126;&#30830;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#20005;&#35880;&#30340;&#29702;&#35770;&#22238;&#31572;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35757;&#32451;&#31639;&#27861;&#30340;&#38544;&#24335;&#20559;&#24046;&#23548;&#33268;&#20102;&#36880;&#23618;&#28176;&#36827;&#30340;&#29305;&#24449;&#21152;&#26435;&#65292;&#38543;&#30528;&#23618;&#28145;&#20837;&#65292;&#29305;&#24449;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#22343;&#34913;&#12290;&#22240;&#27492;&#65292;&#36739;&#20302;&#23618;&#24448;&#24448;&#20855;&#26377;&#26356;&#22810;&#24402;&#19968;&#21270;&#21644;&#26356;&#23569;&#19987;&#38376;&#21270;&#34920;&#31034;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#34920;&#24449;&#20102;&#36825;&#31181;&#34920;&#31034;&#26356;&#20026;&#36866;&#29992;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11391v1 Announce Type: new  Abstract: An effective technique for obtaining high-quality representations is adding a projection head on top of the encoder during training, then discarding it and using the pre-projection representations. Despite its proven practical effectiveness, the reason behind the success of this technique is poorly understood. The pre-projection representations are not directly optimized by the loss function, raising the question: what makes them better? In this work, we provide a rigorous theoretical answer to this question. We start by examining linear models trained with self-supervised contrastive loss. We reveal that the implicit bias of training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers. Consequently, lower layers tend to have more normalized and less specialized representations. We theoretically characterize scenarios where such representations are more 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32593;&#26684;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23380;&#22495;&#20013;&#30340;&#26925;&#22278;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#38543;&#26426;&#34920;&#31034;&#26041;&#27861;&#21644;Feynman-Kac&#20844;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#22810;&#23610;&#24230;&#24615;&#36136;&#24182;&#22788;&#29702;&#30028;&#38754;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.11385</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#23380;&#22495;&#20013;&#26925;&#22278;&#38382;&#39064;&#30340;&#38543;&#26426;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic approach for elliptic problems in perforated domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32593;&#26684;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23380;&#22495;&#20013;&#30340;&#26925;&#22278;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#38543;&#26426;&#34920;&#31034;&#26041;&#27861;&#21644;Feynman-Kac&#20844;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#22810;&#23610;&#24230;&#24615;&#36136;&#24182;&#22788;&#29702;&#30028;&#38754;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#19982;&#24037;&#31243;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#28041;&#21450;&#24102;&#26377;&#23380;&#38553;&#30340;&#22495;&#20013;&#30340;PDE&#27169;&#22411;&#65292;&#20363;&#22914;&#22810;&#23380;&#37329;&#23646;&#25110;&#31354;&#27668;&#36807;&#28388;&#22120;&#12290;&#35299;&#20915;&#36825;&#31867;&#22810;&#23380;&#22495;&#38382;&#39064;&#21463;&#21040;&#19982;&#35299;&#20915;&#23380;&#38553;&#20960;&#20309;&#23610;&#24230;&#30456;&#20851;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32593;&#26684;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#23380;&#22495;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#25429;&#25417;&#21508;&#31181;&#37197;&#32622;&#23610;&#24230;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#28041;&#21450;&#30001;&#23567;&#23380;&#24341;&#36215;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#30340;&#35299;&#30340;&#24179;&#22343;&#23439;&#35266;&#34892;&#20026;&#12290;&#26032;&#26041;&#27861;&#32467;&#21512;&#20102;&#20351;&#29992;&#38543;&#26426;&#34920;&#31034;&#25110;Feynman-Kac&#20844;&#24335;&#30340;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#29992;&#20110;&#22788;&#29702;&#22495;&#19982;&#23380;&#38553;&#20043;&#38388;&#30028;&#38754;&#30340;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#30340;Neumann&#36793;&#30028;&#26465;&#20214;&#12290;&#25552;&#20379;&#20102;&#19968;&#22871;&#20005;&#26684;&#30340;&#25968;&#20540;&#27979;&#35797;&#26469;&#25903;&#25345;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11385v1 Announce Type: cross  Abstract: A wide range of applications in science and engineering involve a PDE model in a domain with perforations, such as perforated metals or air filters. Solving such perforated domain problems suffers from computational challenges related to resolving the scale imposed by the geometries of perforations. We propose a neural network-based mesh-free approach for perforated domain problems. The method is robust and efficient in capturing various configuration scales, including the averaged macroscopic behavior of the solution that involves a multiscale nature induced by small perforations. The new approach incorporates the derivative-free loss method that uses a stochastic representation or the Feynman-Kac formulation. In particular, we implement the Neumann boundary condition for the derivative-free loss method to handle the interface between the domain and perforations. A suite of stringent numerical tests is provided to support the proposed
&lt;/p&gt;</description></item><item><title>Path-GPTOmic&#26694;&#26550;&#36890;&#36807;&#35843;&#33410;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#21644;&#25552;&#20986;&#26799;&#24230;&#35843;&#25972;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30284;&#30151;&#29983;&#23384;&#39044;&#27979;&#20013;&#23384;&#22312;&#30340;&#30149;&#29702;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11375</link><description>&lt;p&gt;
Path-GPTOmic&#65306;&#19968;&#31181;&#29992;&#20110;&#29983;&#23384;&#32467;&#26524;&#39044;&#27979;&#30340;&#24179;&#34913;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11375
&lt;/p&gt;
&lt;p&gt;
Path-GPTOmic&#26694;&#26550;&#36890;&#36807;&#35843;&#33410;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#21644;&#25552;&#20986;&#26799;&#24230;&#35843;&#25972;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30284;&#30151;&#29983;&#23384;&#39044;&#27979;&#20013;&#23384;&#22312;&#30340;&#30149;&#29702;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#27979;&#30284;&#30151;&#29983;&#23384;&#32467;&#26524;&#65292;&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#26631;&#20934;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#20004;&#31181;&#20027;&#35201;&#27169;&#24577;&#65306;&#29992;&#20110;&#35266;&#23519;&#32454;&#32990;&#24418;&#24577;&#29305;&#24449;&#30340;&#30149;&#29702;&#22270;&#20687;&#65292;&#20197;&#21450;&#29992;&#20110;&#37327;&#21270;&#22522;&#22240;&#34920;&#36798;&#30340;&#22522;&#22240;&#32452;&#23398;&#65288;&#22914;&#25209;&#37327;RNA&#27979;&#24207;&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30149;&#29702;&#22522;&#22240;&#32452;&#22810;&#27169;&#24577;&#31639;&#27861;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65306;&#65288;1&#65289;&#26377;&#20215;&#20540;&#30340;&#20851;&#20110;&#22522;&#22240;&#21644;&#22522;&#22240;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;&#32463;&#24120;&#34987;&#24573;&#35270;&#65307;&#65288;2&#65289;&#19968;&#20010;&#27169;&#24577;&#36890;&#24120;&#20027;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#23548;&#33268;&#23545;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#35757;&#32451;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#8220;Path-GPTOmic&#8221;&#26694;&#26550;&#29992;&#20110;&#30284;&#30151;&#29983;&#23384;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;&#65292;&#25105;&#20204;&#35843;&#33410;&#22522;&#30784;&#27169;&#22411;scGPT&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#35813;&#27169;&#22411;&#26368;&#21021;&#26159;&#22312;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25209;&#37327;RNA&#27979;&#24207;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#35299;&#20915;&#27169;&#24577;&#38388;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26799;&#24230;&#35843;&#25972;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#36335;&#24452;&#36951;&#20256;&#23398;&#25216;&#26415;&#65288;&#19968;&#31181;&#36716;&#23548;&#25216;&#26415;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11375v1 Announce Type: cross  Abstract: For predicting cancer survival outcomes, standard approaches in clinical research are often based on two main modalities: pathology images for observing cell morphology features, and genomic (e.g., bulk RNA-seq) for quantifying gene expressions. However, existing pathology-genomic multi-modal algorithms face significant challenges: (1) Valuable biological insights regarding genes and gene-gene interactions are frequently overlooked; (2) one modality often dominates the optimization process, causing inadequate training for the other modality. In this paper, we introduce a new multi-modal ``Path-GPTOmic" framework for cancer survival outcome prediction. First, to extract valuable biological insights, we regulate the embedding space of a foundation model, scGPT, initially trained on single-cell RNA-seq data, making it adaptable for bulk RNA-seq data. Second, to address the imbalance-between-modalities problem, we propose a gradient modula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#65292;&#36890;&#36807;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#21644;&#24352;&#37327;&#20998;&#29255;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#31649;&#29702;&#65292;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;RAG&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11366</link><description>&lt;p&gt;
JORA: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;
&lt;/p&gt;
&lt;p&gt;
JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11366
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#65292;&#36890;&#36807;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#21644;&#24352;&#37327;&#20998;&#29255;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#31649;&#29702;&#65292;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;RAG&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;JORA: JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#12299;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20219;&#21153;&#30340;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#29420;&#29305;&#22320;&#21033;&#29992;&#20102;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#65288;JIT&#65289;&#21644;&#24352;&#37327;&#20998;&#29255;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#30340;&#39640;&#25928;&#31649;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#36825;&#19968;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29992;&#20110;&#22797;&#26434;RAG&#24212;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#29978;&#33267;&#22312;GPU&#36164;&#28304;&#26377;&#38480;&#30340;&#31995;&#32479;&#19978;&#20063;&#33021;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11366v1 Announce Type: cross  Abstract: The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 1
&lt;/p&gt;</description></item><item><title>IGANN Sparse&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#36873;&#25321;&#20419;&#36827;&#31232;&#30095;&#24615;&#65292;&#30830;&#20445;&#22312;&#19981;&#24433;&#21709;&#39044;&#27979;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11363</link><description>&lt;p&gt;
IGANN Sparse: &#29992;&#38750;&#32447;&#24615;&#27934;&#23519;&#21147;&#36830;&#25509;&#31232;&#30095;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear Insight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11363
&lt;/p&gt;
&lt;p&gt;
IGANN Sparse&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#36873;&#25321;&#20419;&#36827;&#31232;&#30095;&#24615;&#65292;&#30830;&#20445;&#22312;&#19981;&#24433;&#21709;&#39044;&#27979;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#39044;&#27979;&#24615;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;IGANN Sparse&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#26469;&#33258;&#24191;&#20041;&#21152;&#27861;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#36873;&#25321;&#20419;&#36827;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#30830;&#20445;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#31232;&#30095;&#24615;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#32780;&#19981;&#24433;&#21709;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11363v1 Announce Type: cross  Abstract: Feature selection is a critical component in predictive analytics that significantly affects the prediction accuracy and interpretability of models. Intrinsic methods for feature selection are built directly into model learning, providing a fast and attractive option for large amounts of data. Machine learning algorithms, such as penalized regression models (e.g., lasso) are the most common choice when it comes to in-built feature selection. However, they fail to capture non-linear relationships, which ultimately affects their ability to predict outcomes in intricate datasets. In this paper, we propose IGANN Sparse, a novel machine learning model from the family of generalized additive models, which promotes sparsity through a non-linear feature selection process during training. This ensures interpretability through improved model sparsity without sacrificing predictive performance. Moreover, IGANN Sparse serves as an exploratory tool
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.11353</link><description>&lt;p&gt;
&#28342;&#21058;&#24863;&#30693;&#30340;2D&#26680;&#30913;&#20849;&#25391;&#39044;&#27979;&#65306;&#21033;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#36845;&#20195;&#33258;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#20998;&#23376;&#30340;&#32467;&#26500;&#20449;&#24687;&#12289;&#30005;&#23376;&#24615;&#36136;&#21644;&#21160;&#24577;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20934;&#30830;&#30340;NMR&#20809;&#35889;&#39044;&#27979;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20505;&#36873;&#20998;&#23376;&#65292;&#20351;&#21270;&#23398;&#23478;&#33021;&#22815;&#23558;&#23427;&#20204;&#19982;&#23454;&#38469;&#23454;&#39564;&#20809;&#35889;&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#35748;&#20998;&#23376;&#32467;&#26500;&#25110;&#25351;&#20986;&#24046;&#24322;&#65292;&#24341;&#23548;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20998;&#23376;&#32467;&#26500;&#39044;&#27979;&#20998;&#23376;&#30340;&#21407;&#23376;NMR&#21270;&#23398;&#20301;&#31227;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#19968;&#32500;&#65288;1D&#65289;NMR&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20108;&#32500;&#65288;2D&#65289;NMR&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#27880;&#30340;NMR&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33258;&#35757;&#32451;&#65288;IST&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21407;&#23376;2DNMR&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;SDP&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11351</link><description>&lt;p&gt;
&#22522;&#20110;SDP&#30340;&#20108;&#20998;&#22270;&#32858;&#31867;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An SDP-based Branch-and-Cut Algorithm for Biclustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11351
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;SDP&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20998;&#22270;&#32858;&#31867;&#65292;&#20063;&#31216;&#20026;&#20849;&#32858;&#31867;&#12289;&#22359;&#32858;&#31867;&#25110;&#21452;&#21521;&#32858;&#31867;&#65292;&#28041;&#21450;&#23558;&#25968;&#25454;&#30697;&#38453;&#30340;&#34892;&#21644;&#21015;&#21516;&#26102;&#32858;&#31867;&#25104;&#19981;&#21516;&#30340;&#32452;&#65292;&#20351;&#24471;&#21516;&#19968;&#32452;&#20869;&#30340;&#34892;&#21644;&#21015;&#26174;&#31034;&#20986;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#20316;&#20026;&#20108;&#20998;&#22270;&#32858;&#31867;&#30340;&#27169;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#21152;&#26435;&#23436;&#20840;&#20108;&#20998;&#22270;&#20013;&#35782;&#21035; $k$ &#20010;&#19981;&#30456;&#20132;&#30340;&#23436;&#20840;&#20108;&#37096;&#23376;&#22270;&#65288;&#31216;&#20026;&#21452;&#22242;&#65289;&#65292;&#20351;&#23427;&#20204;&#30340;&#23494;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#12290;&#23545;&#20110;&#19978;&#30028;&#20363;&#31243;&#65292;&#25105;&#20204;&#32771;&#34385;&#21322;&#23450;&#35268;&#21010;&#25918;&#26494;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21152;&#24378;&#30028;&#38480;&#30340;&#26377;&#25928;&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#19968;&#38454;&#26041;&#27861;&#20197;&#20999;&#24179;&#38754;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#25918;&#26494;&#38382;&#39064;&#12290;&#23545;&#20110;&#19979;&#30028;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21033;&#29992;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#22823;&#26435;&#21305;&#37197;&#33293;&#20837;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11351v1 Announce Type: cross  Abstract: Biclustering, also called co-clustering, block clustering, or two-way clustering, involves the simultaneous clustering of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns. As a model problem for biclustering, we consider the $k$-densest-disjoint biclique problem, whose goal is to identify $k$ disjoint complete bipartite subgraphs (called bicliques) of a given weighted complete bipartite graph such that the sum of their densities is maximized. To address this problem, we present a tailored branch-and-cut algorithm. For the upper bound routine, we consider a semidefinite programming relaxation and propose valid inequalities to strengthen the bound. We solve this relaxation in a cutting-plane fashion using a first-order method. For the lower bound, we design a maximum weight matching rounding procedure that exploits the solution of the relaxation solved
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#26377;&#38480;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#20013;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#33021;&#26356;&#31283;&#23450;&#22320;&#37325;&#26500;&#26356;&#22810;&#20449;&#24687;</title><link>https://arxiv.org/abs/2403.11350</link><description>&lt;p&gt;
&#26377;&#38480;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#20013;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of the data-driven approach in limited angle tomography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11350
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#26377;&#38480;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#20013;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#33021;&#26356;&#31283;&#23450;&#22320;&#37325;&#26500;&#26356;&#22810;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#35282;&#24230;Radon&#21464;&#25442;&#30001;&#20110;&#20854;&#19981;&#36870;&#38382;&#39064;&#32780;&#38395;&#21517;&#20110;&#19990;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#35299;&#37322;&#65292;&#21363;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#20197;&#26356;&#31283;&#23450;&#30340;&#26041;&#24335;&#37325;&#26500;&#26356;&#22810;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11350v1 Announce Type: cross  Abstract: The limited angle Radon transform is notoriously difficult to invert due to the ill-posedness. In this work, we give a mathematical explanation that the data-driven approach based on deep neural networks can reconstruct more information in a stable way compared to traditional methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#65292;&#25552;&#20986;&#20102;COLEP&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#23454;&#40065;&#26834;&#23398;&#20064;&#25512;&#29702;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#20854;&#29305;&#28857;&#22312;&#20110;&#35757;&#32451;&#32479;&#35745;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#27010;&#29575;&#30005;&#36335;&#23454;&#29616;&#31934;&#30830;&#39640;&#25928;&#25512;&#29702;</title><link>https://arxiv.org/abs/2403.11348</link><description>&lt;p&gt;
COLEP: &#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23454;&#29616;&#21487;&#35777;&#23454;&#40065;&#26834;&#23398;&#20064;&#25512;&#29702;&#19968;&#33268;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#65292;&#25552;&#20986;&#20102;COLEP&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#23454;&#40065;&#26834;&#23398;&#20064;&#25512;&#29702;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#20854;&#29305;&#28857;&#22312;&#20110;&#35757;&#32451;&#32479;&#35745;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#27010;&#29575;&#30005;&#36335;&#23454;&#29616;&#31934;&#30830;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#19968;&#33268;&#24615;&#39044;&#27979;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20026;&#20219;&#24847;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#32479;&#35745;&#20005;&#35880;&#30340;&#39044;&#27979;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20551;&#35774;&#25968;&#25454;&#26159;&#21487;&#20132;&#25442;&#30340;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#24494;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#20063;&#21487;&#33021;&#36829;&#21453;&#21487;&#20132;&#25442;&#24615;&#20551;&#35774;&#65292;&#25361;&#25112;&#35206;&#30422;&#29575;&#20445;&#35777;&#65292;&#24182;&#23548;&#33268;&#21518;&#32493;&#23454;&#35777;&#35206;&#30422;&#29575;&#30340;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23454;&#29616;&#21487;&#35777;&#23454;&#40065;&#26834;&#23398;&#20064;&#25512;&#29702;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#26694;&#26550;&#65288;COLEP&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#32452;&#20214;&#65292;&#29992;&#20110;&#35757;&#32451;&#32479;&#35745;&#27169;&#22411;&#20197;&#23398;&#20064;&#19981;&#21516;&#30340;&#35821;&#20041;&#27010;&#24565;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#32534;&#30721;&#30693;&#35782;&#24182;&#34920;&#24449;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#25512;&#29702;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#22312;&#25512;&#29702;&#32452;&#20214;&#20869;&#37096;&#20351;&#29992;&#20102;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#39044;&#27979;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11348v1 Announce Type: cross  Abstract: Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary black-box machine learning models, assuming the data is exchangeable. However, even small adversarial perturbations during the inference can violate the exchangeability assumption, challenge the coverage guarantees, and result in a subsequent decline in empirical coverage. In this work, we propose a certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits, which comprise a data-driven learning component that trains statistical models to learn different semantic concepts, and a reasoning component that encodes knowledge and characterizes the relationships among the trained models for logic reasoning. To achieve exact and efficient reasoning, we employ probabilistic circuits (PCs) within the reasoning component. Theoretically, we provide end-to-end certification of predict
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11345</link><description>&lt;p&gt;
&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#20316;&#31454;&#20105;Agent&#65306;&#22343;&#22330;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#25104;&#22242;&#38431;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#27599;&#20010;&#22242;&#38431;&#20869;&#37096;&#23384;&#22312;&#21512;&#20316;&#65292;&#20294;&#19981;&#21516;&#22242;&#38431;&#20043;&#38388;&#23384;&#22312;&#38750;&#38646;&#21644;&#30340;&#31454;&#20105;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#31181;&#21487;&#20197;&#26126;&#30830;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30001;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24341;&#36215;&#30340;&#38750;&#31283;&#24577;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#27599;&#20010;&#22242;&#38431;&#20869;&#20195;&#29702;&#25968;&#37327;&#26080;&#38480;&#30340;&#24773;&#20917;&#65292;&#21363;&#22343;&#22330;&#35774;&#32622;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#30340;LQ&#22343;&#22330;&#31867;&#22411;&#21338;&#24328;&#65288;GS-MFTGs&#65289;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#36870;&#21487;&#36870;&#26465;&#20214;&#19979;&#34920;&#24449;&#20102;GS-MFTG&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#12290;&#28982;&#21518;&#35777;&#26126;&#20102;&#36825;&#20010;MFTG NE&#22312;&#26377;&#38480;&#20154;&#21475;&#21338;&#24328;&#20013;&#20026;$\mathcal{O}(1/M)$-NE&#65292;&#20854;&#20013;$M$&#26159;&#27599;&#20010;&#22242;&#38431;&#20013;&#20195;&#29702;&#25968;&#37327;&#30340;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26500;&#24615;&#32467;&#26524;&#25512;&#21160;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#29609;&#23478;&#36882;&#36827;&#24335;&#33258;&#28982;Pol&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11345v1 Announce Type: cross  Abstract: We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Pol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.11343</link><description>&lt;p&gt;
&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Transfer Learning with Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#38544;&#31169;&#24615;&#26159;&#20004;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#20869;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#36981;&#23432;&#38544;&#31169;&#32422;&#26463;&#12290;&#25105;&#20204;&#20005;&#26684;&#21046;&#23450;&#20102;\textit{&#32852;&#37030;&#24046;&#20998;&#38544;&#31169;}&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#26377;&#19968;&#20010;&#21463;&#20449;&#20219;&#30340;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#22312;&#36825;&#20010;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#32463;&#20856;&#30340;&#32479;&#35745;&#38382;&#39064;&#65292;&#21363;&#21333;&#21464;&#37327;&#22343;&#20540;&#20272;&#35745;&#12289;&#20302;&#32500;&#32447;&#24615;&#22238;&#24402;&#21644;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#12290;&#36890;&#36807;&#30740;&#31350;&#26497;&#23567;&#20540;&#29575;&#24182;&#30830;&#23450;&#36825;&#20123;&#38382;&#39064;&#30340;&#38544;&#31169;&#25104;&#26412;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32852;&#37030;&#24046;&#20998;&#38544;&#31169;&#26159;&#24050;&#24314;&#31435;&#30340;&#23616;&#37096;&#21644;&#20013;&#22830;&#27169;&#22411;&#20043;&#38388;&#30340;&#19968;&#31181;&#20013;&#38388;&#38544;&#31169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11343v1 Announce Type: new  Abstract: Federated learning is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges. In this paper, we address both issues within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of \textit{federated differential privacy}, which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and identifying the costs of privacy for these problems, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26368;&#26032;&#30340;CNN-based&#20998;&#21106;&#26550;&#26500;PDAtt-Unet&#65292;&#32467;&#21512;3D CNN&#39592;&#24178;&#32593;&#32476;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#38024;&#23545;Covid-19&#26816;&#27979;&#21644;Covid-19&#39046;&#22495;&#33258;&#36866;&#24212;&#25361;&#25112;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.11338</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#21644;&#27979;&#35797;&#22686;&#24378;&#30340;3D CT&#25195;&#25551;&#36827;&#34892;Covid-19&#26816;&#27979;&#21644;Covid-19&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Ensembling and Test Augmentation for Covid-19 Detection and Covid-19 Domain Adaptation from 3D CT-Scans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26368;&#26032;&#30340;CNN-based&#20998;&#21106;&#26550;&#26500;PDAtt-Unet&#65292;&#32467;&#21512;3D CNN&#39592;&#24178;&#32593;&#32476;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#38024;&#23545;Covid-19&#26816;&#27979;&#21644;Covid-19&#39046;&#22495;&#33258;&#36866;&#24212;&#25361;&#25112;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2019&#24180;&#24213;Covid-19&#20986;&#29616;&#20197;&#26469;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#21033;&#29992;CT&#25195;&#25551;&#25104;&#20687;&#36827;&#34892;&#30142;&#30149;&#35786;&#26029;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#31532;4&#23626;COV19D&#31454;&#36187;&#65292;&#37325;&#28857;&#20851;&#27880;Covid-19&#26816;&#27979;&#21644;Covid-19&#39046;&#22495;&#36866;&#24212;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#20013;&#22312;&#32954;&#37096;&#20998;&#21106;&#21644;Covid-19&#24863;&#26579;&#20998;&#21106;&#65292;&#20351;&#29992;&#26368;&#36817;&#30340;&#22522;&#20110;CNN&#30340;&#20998;&#21106;&#26550;&#26500;PDAtt-Unet&#65292;&#21516;&#26102;&#20998;&#21106;&#32954;&#37096;&#21306;&#22495;&#21644;&#24863;&#26579;&#21306;&#22495;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#20999;&#29255;&#65288;&#28784;&#24230;&#22270;&#20687;&#65289;&#19982;&#20998;&#21106;&#30340;&#32954;&#37096;&#21644;&#24863;&#26579;&#37096;&#20301;&#36830;&#25509;&#36215;&#26469;&#65292;&#29983;&#25104;&#31867;&#20284;&#20110;&#24425;&#33394;&#36890;&#36947;&#30340;&#19977;&#20010;&#36755;&#20837;&#36890;&#36947;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19977;&#31181;3D CNN&#39592;&#24178;&#32593;&#32476;Customized Hybrid-DeCoVNet&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;3D-Resnet-18&#21644;3D-Resnet-50&#27169;&#22411;&#26469;&#35757;&#32451;&#20004;&#20010;&#25361;&#25112;&#30340;Covid-19&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11338v1 Announce Type: cross  Abstract: Since the emergence of Covid-19 in late 2019, medical image analysis using artificial intelligence (AI) has emerged as a crucial research area, particularly with the utility of CT-scan imaging for disease diagnosis. This paper contributes to the 4th COV19D competition, focusing on Covid-19 Detection and Covid-19 Domain Adaptation Challenges. Our approach centers on lung segmentation and Covid-19 infection segmentation employing the recent CNN-based segmentation architecture PDAtt-Unet, which simultaneously segments lung regions and infections. Departing from traditional methods, we concatenate the input slice (grayscale) with segmented lung and infection, generating three input channels akin to color channels. Additionally, we employ three 3D CNN backbones Customized Hybrid-DeCoVNet, along with pretrained 3D-Resnet-18 and 3D-Resnet-50 models to train Covid-19 recognition for both challenges. Furthermore, we explore ensemble approaches 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#30452;&#25509;&#21644;&#21516;&#34892;&#25928;&#24212;&#65292;&#22788;&#29702;&#32593;&#32476;&#28151;&#26434;&#22240;&#32032;&#65292;&#24182;&#19968;&#33268;&#22320;&#20272;&#35745;&#25152;&#38656;&#30340;&#22240;&#26524;&#25928;&#24212;</title><link>https://arxiv.org/abs/2403.11332</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#22240;&#26524;&#25928;&#24212;&#21452;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#30452;&#25509;&#21644;&#21516;&#34892;&#25928;&#24212;&#65292;&#22788;&#29702;&#32593;&#32476;&#28151;&#26434;&#22240;&#32032;&#65292;&#24182;&#19968;&#33268;&#22320;&#20272;&#35745;&#25152;&#38656;&#30340;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#25928;&#24212;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25968;&#25454;&#20855;&#26377;&#20010;&#20307;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#21333;&#20301;&#20043;&#38388;&#19981;&#29420;&#31435;&#12289;&#24178;&#25200;&#65288;&#21333;&#20301;&#30340;&#32467;&#26524;&#21463;&#37051;&#23621;&#30340;&#22788;&#29702;&#24433;&#21709;&#65289;&#20197;&#21450;&#24341;&#20837;&#26469;&#33258;&#37051;&#36817;&#21333;&#20301;&#30340;&#39069;&#22806;&#28151;&#26434;&#22240;&#32032;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#21333;&#20010;&#35266;&#27979;&#31038;&#20132;&#32593;&#32476;&#20934;&#30830;&#39640;&#25928;&#22320;&#20272;&#35745;&#30452;&#25509;&#21644;&#21516;&#20276;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#19982;&#21452;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35843;&#25972;&#32593;&#32476;&#28151;&#26434;&#22240;&#32032;&#24182;&#19968;&#33268;&#22320;&#20272;&#35745;&#25152;&#38656;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#26082;&#20855;&#26377;&#28176;&#36817;&#27491;&#24577;&#24615;&#21448;&#21322;&#21442;&#25968;&#39640;&#25928;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#21322;&#21512;&#25104;&#29366;&#24577;&#19979;&#30340;&#22235;&#31181;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11332v1 Announce Type: new  Abstract: Our paper addresses the challenge of inferring causal effects in social network data, characterized by complex interdependencies among individuals resulting in challenges such as non-independence of units, interference (where a unit's outcome is affected by neighbors' treatments), and introduction of additional confounding factors from neighboring units. We propose a novel methodology combining graph neural networks and double machine learning, enabling accurate and efficient estimation of direct and peer effects using a single observational social network. Our approach utilizes graph isomorphism networks in conjunction with double machine learning to effectively adjust for network confounders and consistently estimate the desired causal effects. We demonstrate that our estimator is both asymptotically normal and semiparametrically efficient. A comprehensive evaluation against four state-of-the-art baseline methods using three semi-synth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#29983;&#24577;&#21644;&#27700;&#25991;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#26469;&#25913;&#21892;&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24357;&#34917;&#20102;&#30446;&#21069;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#30340;&#22320;&#29702;&#22806;&#25512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11331</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#29983;&#24577;&#21644;&#27700;&#25991;&#23398;&#20013;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#28508;&#21147;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Potential of Domain Adaptation in Machine Learning in Ecology and Hydrology to Improve Model Extrapolability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#29983;&#24577;&#21644;&#27700;&#25991;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#26469;&#25913;&#21892;&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24357;&#34917;&#20102;&#30446;&#21069;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#30340;&#22320;&#29702;&#22806;&#25512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#24577;&#21644;&#27700;&#25991;&#22320;&#38754;&#23454;&#38469;&#35266;&#27979;&#30340;&#20840;&#29699;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#26410;&#30693;&#20301;&#32622;&#26102;&#21487;&#33021;&#20855;&#26377;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#65292;&#20063;&#31216;&#20026;&#24369;&#22806;&#25512;&#33021;&#21147;&#12290;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#36890;&#36807;&#35843;&#25972;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#20998;&#24067;&#30340;&#24046;&#24322;&#25110;&#19981;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#22240;&#22320;&#29702;&#22806;&#25512;&#33021;&#21147;&#38382;&#39064;&#32780;&#21463;&#21040;&#36136;&#30097;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23569;&#34987;&#26126;&#30830;&#22320;&#24212;&#29992;&#20110;&#20840;&#29699;&#33539;&#22260;&#20869;&#29983;&#24577;&#21644;&#27700;&#25991;&#23398;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#31616;&#35201;&#25551;&#36848;&#20102;&#24403;&#21069;&#29983;&#24577;&#21644;&#27700;&#25991;&#23398;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20840;&#29699;&#35266;&#27979;&#20998;&#24067;&#30340;&#20195;&#34920;&#24615;&#21644;&#30001;&#27492;&#36896;&#25104;&#30340;&#23616;&#38480;&#24615;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11331v1 Announce Type: cross  Abstract: Due to the heterogeneity of the global distribution of ecological and hydrological ground-truth observations, machine learning models can have limited adaptability when applied to unknown locations, which is referred to as weak extrapolability. Domain adaptation techniques have been widely used in machine learning domains such as image classification, which can improve the model generalization ability by adjusting the difference or inconsistency of the domain distribution between the training and test sets. However, this approach has rarely been used explicitly in machine learning models in ecology and hydrology at the global scale, although these models have often been questioned due to geographic extrapolability issues. This paper briefly describes the shortcomings of current machine learning models of ecology and hydrology in terms of the global representativeness of the distribution of observations and the resulting limitations of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.11330</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#19968;&#20010;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#26469;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11330
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#65288;&#21363;&#65292;&#23545;&#35805;&#32423;&#65289;&#22870;&#21169;&#23545;&#40784;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#33258;&#28982;&#21457;&#29983;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#21517;&#20026;GELI&#65289;&#36890;&#36807;&#23558;&#20154;&#31867;&#25552;&#20379;&#30340;&#20840;&#23616;&#26126;&#30830;&#65288;GE&#65289;&#20250;&#35805;&#32423;&#22870;&#21169;&#25286;&#20998;&#65292;&#21033;&#29992;&#26412;&#22320;&#38544;&#24335;&#65288;LI&#65289;&#22810;&#27169;&#24577;&#22870;&#21169;&#20449;&#21495;&#26469;&#36328;&#27169;&#24577;&#22320;&#22609;&#36896;&#22870;&#21169;&#20998;&#35299;&#27493;&#39588;&#12290;&#28982;&#21518;&#23558;&#36825;&#31181;&#20998;&#35299;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#26631;&#20934;RHLF&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#26469;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;GELI&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;Transformer&#27169;&#22411;&#20013;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#35777;&#26126;&#26469;&#35757;&#32451;&#20004;&#31181;&#27169;&#22411;&#65292;&#25104;&#21151;&#36991;&#20813;&#20102;&#20266;&#30456;&#20851;&#24615;&#21644;&#25512;&#29702;&#25463;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.11314</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#36827;&#34892;&#25512;&#29702;-&#20943;&#23569;&#20266;&#30456;&#20851;&#24615;&#21644;&#25512;&#29702;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11314
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;Transformer&#27169;&#22411;&#20013;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#35777;&#26126;&#26469;&#35757;&#32451;&#20004;&#31181;&#27169;&#22411;&#65292;&#25104;&#21151;&#36991;&#20813;&#20102;&#20266;&#30456;&#20851;&#24615;&#21644;&#25512;&#29702;&#25463;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#26159;&#29992;&#20110;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35768;&#22810;&#38656;&#35201;&#36923;&#36753;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34987;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#21487;&#33021;&#20250;&#36731;&#26131;&#23398;&#20064;&#21040;&#25968;&#25454;&#20013;&#30340;&#20266;&#27169;&#24335;&#65292;&#20174;&#32780;&#32469;&#36807;&#23454;&#38469;&#25512;&#29702;&#36807;&#31243;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#34987;&#35757;&#32451;&#26469;a) &#36817;&#20284;&#21629;&#39064;&#36923;&#36753;&#25512;&#29702;&#65292;&#21516;&#26102;b) &#36991;&#20813;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;&#24341;&#36215;&#30340;&#24050;&#30693;&#25512;&#29702;&#25463;&#24452;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30495;&#23454;&#24615;&#19982;&#38382;&#39064;&#20013;&#35268;&#21017;&#25968;&#37327;&#31561;&#20043;&#38388;&#23384;&#22312;&#24050;&#30693;&#30340;&#20266;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#22686;&#24378;&#20102;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#20102;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#29983;&#25104;&#24335;Transformer&#65292;WP-BART&#65292;&#23427;&#22312;&#38382;&#39064;&#21450;&#20854;&#23436;&#25972;&#35777;&#26126;&#19978;&#36827;&#34892;&#35757;&#32451;&#65307;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;SIP-BART&#65292;&#23427;&#22312;&#21333;&#20010;&#35777;&#26126;&#27493;&#39588;&#19978;&#35757;&#32451;&#65292;&#24182;&#23558;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;BART&#19982;&#31526;&#21495;&#25512;&#29702;&#26816;&#26597;&#22120;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;SIP-BART&#25104;&#21151;&#22320;&#36991;&#20813;&#20102;&#25512;&#29702;&#25463;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11314v1 Announce Type: cross  Abstract: Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning. However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning. In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker. We find that SIP-BART succeeds in avoiding reasoning 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11299</link><description>&lt;p&gt;
SQ-LLaVA&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#30528;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#40511;&#27807;&#25104;&#20026;&#25972;&#20010;&#32593;&#32476;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#25913;&#21892;&#36328;&#27169;&#24577;&#23545;&#40784;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#32771;&#34385;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35270;&#35273;&#20219;&#21153;&#33539;&#22260;&#30340;&#26356;&#22810;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38382;&#31572;&#65292;&#20294;&#36825;&#31181;&#25805;&#20316;&#25104;&#26412;&#36739;&#39640;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#21253;&#21547;&#22823;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#19968;&#30452;&#40092;&#26377;&#20154;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20869;&#37096;&#34987;&#24573;&#35270;&#30340;&#19978;&#19979;&#25991;&#65292;&#35757;&#32451;&#27169;&#22411;&#33258;&#25105;&#35757;&#32451;'&#23398;&#20064;'&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;&#12290;SQ-LLaVA&#22312;&#29983;&#25104;&#28789;&#27963;&#19988;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35789;&#36827;&#34892;&#20462;&#25913;&#65292;&#26088;&#22312;&#27450;&#39575;&#20998;&#31867;&#27169;&#22411;&#19988;&#20445;&#25345;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#22312;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#30340;&#21516;&#26102;&#36991;&#24320;&#20102;&#20998;&#31867;&#31995;&#32479;&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.11297</link><description>&lt;p&gt;
&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#31181;&#20462;&#25913;&#30340;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Modified Word Saliency-Based Adversarial Attack on Text Classification Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35789;&#36827;&#34892;&#20462;&#25913;&#65292;&#26088;&#22312;&#27450;&#39575;&#20998;&#31867;&#27169;&#22411;&#19988;&#20445;&#25345;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#22312;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#30340;&#21516;&#26102;&#36991;&#24320;&#20102;&#20998;&#31867;&#31995;&#32479;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;&#20462;&#25913;&#30340;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#65288;MWSAA&#65289;&#12290;&#35813;&#25216;&#26415;&#22522;&#20110;&#35789;&#26174;&#33879;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25112;&#30053;&#24615;&#22320;&#25200;&#20081;&#36755;&#20837;&#25991;&#26412;&#65292;&#26088;&#22312;&#35823;&#23548;&#20998;&#31867;&#27169;&#22411;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#36830;&#36143;&#24615;&#12290;&#36890;&#36807;&#25913;&#36827;&#20256;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;MWSAA&#26174;&#33879;&#22686;&#24378;&#20102;&#20854;&#35268;&#36991;&#20998;&#31867;&#31995;&#32479;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#26174;&#33879;&#24615;&#20272;&#35745;&#36807;&#31243;&#35782;&#21035;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#26174;&#33879;&#35789;&#65292;&#36825;&#20123;&#35789;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#26368;&#22823;&#24433;&#21709;&#12290;&#38543;&#21518;&#65292;&#36825;&#20123;&#26174;&#33879;&#35789;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#20462;&#25913;&#65292;&#24341;&#23548;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#30830;&#20445;&#25913;&#21464;&#30340;&#25991;&#26412;&#20445;&#25345;&#36830;&#36143;&#24182;&#20445;&#30041;&#20854;&#21407;&#22987;&#21547;&#20041;&#12290;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11297v1 Announce Type: new  Abstract: This paper introduces a novel adversarial attack method targeting text classification models, termed the Modified Word Saliency-based Adversarial At-tack (MWSAA). The technique builds upon the concept of word saliency to strategically perturb input texts, aiming to mislead classification models while preserving semantic coherence. By refining the traditional adversarial attack approach, MWSAA significantly enhances its efficacy in evading detection by classification systems. The methodology involves first identifying salient words in the input text through a saliency estimation process, which prioritizes words most influential to the model's decision-making process. Subsequently, these salient words are subjected to carefully crafted modifications, guided by semantic similarity metrics to ensure that the altered text remains coherent and retains its original meaning. Empirical evaluations conducted on diverse text classification datasets
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOOD&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#39046;&#22495;&#22806;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20851;&#31995;&#23884;&#20837;&#32858;&#21512;&#35774;&#35745;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.11292</link><description>&lt;p&gt;
&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39046;&#22495;&#22806;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOOD&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#39046;&#22495;&#22806;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20851;&#31995;&#23884;&#20837;&#32858;&#21512;&#35774;&#35745;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22810;&#20851;&#31995;&#22270;&#26159;&#19968;&#31181;&#29992;&#20110;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#25968;&#25454;&#30340;&#34920;&#36798;&#24418;&#24335;&#65292;&#22312;&#36825;&#31181;&#25968;&#25454;&#20013;&#65292;&#20851;&#31995;&#20801;&#35768;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#35299;&#20915;&#36825;&#31867;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20219;&#21153;&#38656;&#35201;&#33021;&#22815;&#25214;&#21040;&#25429;&#25417;&#25152;&#28041;&#21450;&#20851;&#31995;&#22810;&#26679;&#24615;&#20197;&#21450;&#21160;&#24577;&#28436;&#21464;&#30340;&#32467;&#26500;&#23884;&#20837;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#21160;&#24577;&#22810;&#20851;&#31995;&#22270;&#24314;&#31435;&#20102;&#19968;&#31867;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#39046;&#22495;&#22806;&#38142;&#25509;&#39044;&#27979;&#65292;&#20854;&#20013;&#24453;&#39044;&#27979;&#30340;&#20851;&#31995;&#22312;&#36755;&#20837;&#22270;&#20013;&#19981;&#21487;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;GOOD&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#39046;&#22495;&#22806;&#27867;&#21270;&#38382;&#39064;&#12290;GOOD&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20851;&#31995;&#23884;&#20837;&#32858;&#21512;&#35774;&#35745;&#27010;&#24565;&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#24403;&#33021;&#22815;&#35299;&#24320;&#19981;&#21516;&#20851;&#31995;&#28151;&#21512;&#27604;&#20363;&#26102;&#65292;&#23884;&#20837;&#25165;&#26159;&#22909;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11292v1 Announce Type: cross  Abstract: Dynamic multi-relational graphs are an expressive relational representation for data enclosing entities and relations of different types, and where relationships are allowed to vary in time. Addressing predictive tasks over such data requires the ability to find structure embeddings that capture the diversity of the relationships involved, as well as their dynamic evolution. In this work, we establish a novel class of challenging tasks for dynamic multi-relational graphs involving out-of-domain link prediction, where the relationship being predicted is not available in the input graph. We then introduce a novel Graph Neural Network model, named GOOD, designed specifically to tackle the out-of-domain generalization problem. GOOD introduces a novel design concept for multi-relation embedding aggregation, based on the idea that good representations are such when it is possible to disentangle the mixing proportions of the different relatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#21644;&#36793;&#32536;&#26816;&#27979;&#31639;&#27861;&#25552;&#21462;&#29289;&#29702;&#35774;&#35745;&#22270;&#32440;&#30340;&#20449;&#24687;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;CAD&#26684;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.11291</link><description>&lt;p&gt;
&#29289;&#29702;&#35774;&#35745;&#22270;&#32440;&#30340;&#39640;&#32423;&#30693;&#35782;&#25552;&#21462;&#12289;&#32763;&#35793;&#21644;&#36716;&#25442;&#20026;CAD&#26684;&#24335;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advanced Knowledge Extraction of Physical Design Drawings, Translation and conversion to CAD formats using Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11291
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#21644;&#36793;&#32536;&#26816;&#27979;&#31639;&#27861;&#25552;&#21462;&#29289;&#29702;&#35774;&#35745;&#22270;&#32440;&#30340;&#20449;&#24687;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;CAD&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24418;&#24335;&#19979;&#35774;&#35745;&#22270;&#32440;&#30340;&#32500;&#25252;&#12289;&#23384;&#26723;&#21644;&#20351;&#29992;&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#38271;&#26102;&#38388;&#20197;&#26469;&#21464;&#24471;&#32321;&#29712;&#12290;&#31616;&#21333;&#25195;&#25551;&#22270;&#32440;&#24456;&#38590;&#25552;&#21462;&#20449;&#24687;&#12290;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#31561;&#25968;&#23383;&#26684;&#24335;&#65292;&#24182;&#36827;&#34892;&#24517;&#35201;&#30340;&#30693;&#35782;&#25552;&#21462;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23558;&#36825;&#20123;&#26426;&#22120;&#22270;&#32440;&#36716;&#25442;&#20026;&#25968;&#23383;&#24418;&#24335;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#65292;&#22914;Yolov7&#12289;Faster R-CNN&#65292;&#26469;&#26816;&#27979;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#29289;&#29702;&#32472;&#22270;&#23545;&#35937;&#65292;&#38543;&#21518;&#20351;&#29992;&#36793;&#32536;&#26816;&#27979;&#31639;&#27861;&#65292;&#22914;Canny&#28388;&#27874;&#22120;&#65292;&#20174;&#22270;&#32440;&#21306;&#22495;&#25552;&#21462;&#21644;&#31934;&#21270;&#35782;&#21035;&#30340;&#32447;&#26465;&#65292;&#20197;&#21450;&#26354;&#32447;&#26816;&#27979;&#25216;&#26415;&#26469;&#26816;&#27979;&#22278;&#12290;&#36824;&#25552;&#21462;&#22270;&#32440;&#20013;&#30340;&#35013;&#39280;&#29289;&#65288;&#22797;&#26434;&#24418;&#29366;&#65289;&#12290;&#20026;&#30830;&#20445;&#20840;&#38754;&#24615;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11291v1 Announce Type: cross  Abstract: The maintenance, archiving and usage of the design drawings is cumbersome in physical form in different industries for longer period. It is hard to extract information by simple scanning of drawing sheets. Converting them to their digital formats such as Computer-Aided Design (CAD), with needed knowledge extraction can solve this problem. The conversion of these machine drawings to its digital form is a crucial challenge which requires advanced techniques. This research proposes an innovative methodology utilizing Deep Learning methods. The approach employs object detection model, such as Yolov7, Faster R-CNN, to detect physical drawing objects present in the images followed by, edge detection algorithms such as canny filter to extract and refine the identified lines from the drawing region and curve detection techniques to detect circle. Also ornaments (complex shapes) within the drawings are extracted. To ensure comprehensive convers
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.11265</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#21892;&#20316;&#32773;&#39564;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11265
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20851;&#27880;&#30340;&#26159;&#25512;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#19968;&#20010;&#29305;&#23450;&#20316;&#32773;&#25776;&#20889;&#36824;&#26159;&#30001;&#20854;&#20182;&#20154;&#25776;&#20889;&#12290;&#24050;&#32463;&#26174;&#31034;&#35768;&#22810;AV&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#24694;&#24847;&#20316;&#32773;&#31215;&#26497;&#23581;&#35797;&#27450;&#39575;&#20998;&#31867;&#22120;&#65292;&#26041;&#27861;&#26159;&#38544;&#34255;&#20182;&#20204;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#25110;&#32773;&#27169;&#20223;&#21478;&#19968;&#20301;&#20316;&#32773;&#30340;&#39118;&#26684;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20998;&#31867;&#22120;&#35757;&#32451;&#38598;&#19982;&#65288;&#36127;&#38754;&#30340;&#65289;&#21512;&#25104;&#31034;&#20363;&#36827;&#34892;&#22686;&#24378;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#36825;&#20123;&#21512;&#25104;&#31034;&#20363;&#26159;&#20026;&#20102;&#27169;&#20223;&#24863;&#20852;&#36259;&#30340;&#20316;&#32773;&#30340;&#39118;&#26684;&#32780;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22686;&#24378;&#23545;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#30340;AV&#20219;&#21153;&#20013;&#24102;&#26469;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#25913;&#36827;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65288;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#23567;&#35268;&#27169;transformers&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#27969;&#34892;&#30340;GPT&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11265v1 Announce Type: cross  Abstract: Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting. In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36153;&#26364;&#30340;&#36335;&#24452;&#31215;&#20998;&#24341;&#20837;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#24418;&#24335;&#65292;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20840;&#38754;&#25551;&#36848;&#65292;&#24182;&#35782;&#21035;&#20986;&#36830;&#25509;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#37319;&#26679;&#26041;&#26696;&#30340;&#25554;&#20540;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.11262</link><description>&lt;p&gt;
&#36890;&#36807;&#36153;&#26364;&#30340;&#36335;&#24452;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Understanding Diffusion Models by Feynman's Path Integral
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11262
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36153;&#26364;&#30340;&#36335;&#24452;&#31215;&#20998;&#24341;&#20837;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#24418;&#24335;&#65292;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20840;&#38754;&#25551;&#36848;&#65292;&#24182;&#35782;&#21035;&#20986;&#36830;&#25509;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#37319;&#26679;&#26041;&#26696;&#30340;&#25554;&#20540;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65307;&#28982;&#32780;&#65292;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#65288;&#21363;&#27010;&#29575;&#27969;ODEs&#65289;&#37319;&#26679;&#26041;&#26696;&#20043;&#38388;&#24615;&#33021;&#24046;&#24322;&#30340;&#28508;&#22312;&#22240;&#32032;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#36153;&#26364;&#30340;&#36335;&#24452;&#31215;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#24418;&#24335;&#65292;&#36825;&#26159;&#26368;&#21021;&#20026;&#37327;&#23376;&#29289;&#29702;&#23398;&#24320;&#21457;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#24418;&#24335;&#25552;&#20379;&#20102;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20840;&#38754;&#25551;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#25512;&#23548;&#12290;&#36825;&#31181;&#24418;&#24335;&#23481;&#32435;&#20102;&#19968;&#20010;&#25554;&#20540;&#21442;&#25968;&#65292;&#36830;&#25509;&#20102;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#37319;&#26679;&#26041;&#26696;&#65292;&#25105;&#20204;&#30830;&#23450;&#36825;&#20010;&#21442;&#25968;&#23601;&#20687;&#26159;&#37327;&#23376;&#29289;&#29702;&#23398;&#20013;&#30340;&#26222;&#26391;&#20811;&#24120;&#25968;&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#31181;&#31867;&#27604;&#20351;&#25105;&#20204;&#33021;&#22815;&#24212;&#29992;&#28201;&#31574;-&#20811;&#25289;&#22696;-&#24067;&#37324;&#28170;&#65288;WKB&#65289;&#23637;&#24320;&#65292;&#36825;&#26159;&#37327;&#23376;&#29289;&#29702;&#23398;&#20013;&#19968;&#20010;&#25104;&#29087;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11262v1 Announce Type: cross  Abstract: Score-based diffusion models have proven effective in image generation and have gained widespread usage; however, the underlying factors contributing to the performance disparity between stochastic and deterministic (i.e., the probability flow ODEs) sampling schemes remain unclear. We introduce a novel formulation of diffusion models using Feynman's path integral, which is a formulation originally developed for quantum physics. We find this formulation providing comprehensive descriptions of score-based generative models, and demonstrate the derivation of backward stochastic differential equations and loss functions.The formulation accommodates an interpolating parameter connecting stochastic and deterministic sampling schemes, and we identify this parameter as a counterpart of Planck's constant in quantum physics. This analogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a well-established technique in quantum ph
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;Lie&#32676;&#19978;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20026;&#40654;&#26364;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;RBN&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;Lie&#32676;&#21040;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#19977;&#31867;&#21442;&#25968;&#21270;Lie&#32676;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.11261</link><description>&lt;p&gt;
&#19968;&#31181;Lie&#32676;&#26041;&#27861;&#24212;&#29992;&#20110;&#40654;&#26364;&#25209;&#37327;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Lie Group Approach to Riemannian Batch Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;Lie&#32676;&#19978;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20026;&#40654;&#26364;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;RBN&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;Lie&#32676;&#21040;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#19977;&#31867;&#21442;&#25968;&#21270;Lie&#32676;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#23384;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#27969;&#24418;&#20540;&#27979;&#37327;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25193;&#23637;&#21040;&#27969;&#24418;&#65292;&#24182;&#19988;&#21516;&#26102;&#65292;&#24402;&#19968;&#21270;&#25216;&#26415;&#20063;&#24050;&#32463;&#34987;&#24212;&#29992;&#21040;&#20960;&#20010;&#27969;&#24418;&#65292;&#31216;&#20026;&#40654;&#26364;&#24402;&#19968;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#40654;&#26364;&#24402;&#19968;&#21270;&#26041;&#27861;&#26159;&#20197;&#20020;&#26102;&#26041;&#24335;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#27969;&#24418;&#12290;&#26412;&#25991;&#22312;Lie&#32676;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#40654;&#26364;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;RBN&#65289;&#25216;&#26415;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#25511;&#21046;&#40654;&#26364;&#24179;&#22343;&#20540;&#21644;&#26041;&#24046;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;Lie&#32676;&#32467;&#26500;&#30340;&#23545;&#31216;&#27491;&#23450;(SPD)&#27969;&#24418;&#12290;&#21033;&#29992;&#21464;&#24418;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;SPD&#27969;&#24418;&#19978;&#30340;Lie&#32676;&#25512;&#24191;&#25104;&#19977;&#31867;&#21442;&#25968;&#21270;Lie&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11261v1 Announce Type: cross  Abstract: Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups. Specific normalization l
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#65292;&#20197;&#35299;&#20915;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.11259</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11259
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#65292;&#20197;&#35299;&#20915;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#26381;&#21153;&#22120;&#20013;&#25918;&#32622;&#24212;&#29992;&#31243;&#24207;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#35768;&#22810;&#26381;&#21153;&#22120;&#12289;&#29992;&#25143;&#21450;&#20854;&#35831;&#27714;&#12290;&#29616;&#26377;&#31639;&#27861;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#35299;&#20915;&#20855;&#26377;&#37325;&#22823;&#19981;&#30830;&#23450;&#24615;&#24773;&#26223;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26368;&#22823;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#21516;&#26102;&#32771;&#34385;&#25152;&#26377;&#25216;&#26415;&#32422;&#26463;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#65292;&#23427;&#27169;&#25311;&#20102;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#20013;&#37096;&#32626;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#35745;&#23558;&#23398;&#20064;&#22914;&#20309;&#26681;&#25454;&#29992;&#25143;&#21644;&#26381;&#21153;&#22120;&#30340;&#31354;&#38388;&#20301;&#32622;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#12290;&#26412;&#30740;&#31350;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#36890;&#36807;&#21464;&#21270;&#21442;&#25968;&#22914;&#29992;&#25143;&#20301;&#32622;&#12289;&#35831;&#27714;&#36895;&#29575;&#21644;&#35299;&#20915;&#20248;&#21270;&#27169;&#22411;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#35760;&#24405;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#36317;&#31163;&#21487;&#29992;&#26381;&#21153;&#22120;&#30340;&#36317;&#31163;&#29305;&#24449;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11259v1 Announce Type: cross  Abstract: Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests. Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios. Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints. One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers. Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers. In this study, the problem is formulated as a two-stage stochastic programming. A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model. Then, based on the distance features of each user from the available servers and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;CT&#25195;&#25551;&#22270;&#20687;&#30340;&#39640;&#32423;&#31354;&#38388;&#20999;&#29255;&#29305;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36807;&#28388;&#24322;&#24120;&#25968;&#25454;&#21644;&#20943;&#23569;&#25968;&#25454;&#20887;&#20313;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11230</link><description>&lt;p&gt;
&#22522;&#20110;&#31616;&#21333;2D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;COVID-19&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple 2D Convolutional Neural Network-based Approach for COVID-19 Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;CT&#25195;&#25551;&#22270;&#20687;&#30340;&#39640;&#32423;&#31354;&#38388;&#20999;&#29255;&#29305;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36807;&#28388;&#24322;&#24120;&#25968;&#25454;&#21644;&#20943;&#23569;&#25968;&#25454;&#20887;&#20313;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#32954;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;CT&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#20999;&#29255;&#35745;&#25968;&#21644;&#20998;&#36776;&#29575;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#31181;&#24046;&#24322;&#26469;&#33258;&#20110;&#20351;&#29992;&#21508;&#31181;&#25195;&#25551;&#35774;&#22791;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23545;&#21333;&#20010;&#20999;&#29255;&#36827;&#34892;&#39044;&#27979;&#65292;&#28982;&#21518;&#23558;&#20854;&#32452;&#21512;&#20197;&#33719;&#24471;&#20840;&#38754;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#27809;&#26377;&#21253;&#21547;&#29305;&#23450;&#20110;&#27599;&#20010;&#20999;&#29255;&#30340;&#23398;&#20064;&#29305;&#24449;&#65292;&#23548;&#33268;&#25928;&#26524;&#19978;&#30340;&#22949;&#21327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;CT&#25195;&#25551;&#23450;&#21046;&#30340;&#39640;&#32423;&#31354;&#38388;&#20999;&#29255;&#29305;&#24449;&#23398;&#20064;&#65288;SSFL++&#65289;&#26694;&#26550;&#12290;&#23427;&#26088;&#22312;&#36807;&#28388;&#20986;&#25972;&#20010;CT&#25195;&#25551;&#20013;&#30340;&#24322;&#24120;&#25968;&#25454;&#65292;&#36890;&#36807;&#20943;&#23569;70%&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36873;&#25321;&#36827;&#34892;&#20998;&#26512;&#30340;&#20851;&#38190;&#31354;&#38388;&#20999;&#29255;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23494;&#24230;&#30340;&#20999;&#29255;&#37319;&#26679;&#65288;KDS&#65289;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11230v1 Announce Type: cross  Abstract: This study explores the use of deep learning techniques for analyzing lung Computed Tomography (CT) images. Classic deep learning approaches face challenges with varying slice counts and resolutions in CT images, a diversity arising from the utilization of assorted scanning equipment. Typically, predictions are made on single slices which are then combined for a comprehensive outcome. Yet, this method does not incorporate learning features specific to each slice, leading to a compromise in effectiveness. To address these challenges, we propose an advanced Spatial-Slice Feature Learning (SSFL++) framework specifically tailored for CT scans. It aims to filter out out-of-distribution (OOD) data within the entire CT scan, allowing us to select essential spatial-slice features for analysis by reducing data redundancy by 70\%. Additionally, we introduce a Kernel-Density-based slice Sampling (KDS) method to enhance stability during training a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20020;&#24202;&#26631;&#35760;&#30340;&#24265;&#20215;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#21462;&#20142;&#28857;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11227</link><description>&lt;p&gt;
&#20174;&#25991;&#23383;&#20013;&#25552;&#21462;&#20020;&#24202;&#26631;&#35760;&#30340;&#24265;&#20215;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cheap Ways of Extracting Clinical Markers from Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20020;&#24202;&#26631;&#35760;&#30340;&#24265;&#20215;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#21462;&#20142;&#28857;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;UniBuc&#32771;&#21476;&#22242;&#38431;&#20026;CLPsych 2024&#20849;&#20139;&#20219;&#21153;&#25152;&#36827;&#34892;&#30340;&#24037;&#20316;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#22312;&#25991;&#26412;&#20013;&#23547;&#25214;&#25903;&#25345;&#20998;&#37197;&#30340;&#33258;&#26432;&#39118;&#38505;&#27700;&#24179;&#30340;&#35777;&#25454;&#12290;&#38656;&#35201;&#20004;&#31181;&#31867;&#22411;&#30340;&#35777;&#25454;&#65306;&#20142;&#28857;&#65288;&#25552;&#21462;&#25991;&#26412;&#20013;&#30340;&#30456;&#20851;&#29255;&#27573;&#65289;&#21644;&#25688;&#35201;&#65288;&#23558;&#35777;&#25454;&#32858;&#21512;&#25104;&#32508;&#21512;&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32780;&#19981;&#26159;&#19968;&#31181;&#26356;&#33410;&#30465;&#20869;&#23384;&#21644;&#36164;&#28304;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#37319;&#29992;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;GOML&#65289;&#31649;&#36947;&#65292;&#21253;&#25324;tf-idf&#21521;&#37327;&#21270;&#22120;&#21644;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#65292;&#20854;&#20195;&#34920;&#24615;&#29305;&#24449;&#29992;&#20110;&#25552;&#21462;&#30456;&#20851;&#30340;&#20142;&#28857;&#12290;&#31532;&#20108;&#31181;&#26356;&#28040;&#32791;&#36164;&#28304;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#29983;&#25104;&#25688;&#35201;&#65292;&#24182;&#30001;&#24605;&#32500;&#38142;&#25351;&#23548;&#25552;&#20379;&#25351;&#31034;&#20020;&#24202;&#26631;&#35760;&#30340;&#25991;&#26412;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11227v1 Announce Type: new  Abstract: This paper describes the work of the UniBuc Archaeology team for CLPsych's 2024 Shared Task, which involved finding evidence within the text supporting the assigned suicide risk level. Two types of evidence were required: highlights (extracting relevant spans within the text) and summaries (aggregating evidence into a synthesis). Our work focuses on evaluating Large Language Models (LLM) as opposed to an alternative method that is much more memory and resource efficient. The first approach employs a good old-fashioned machine learning (GOML) pipeline consisting of a tf-idf vectorizer with a logistic regression classifier, whose representative features are used to extract relevant highlights. The second, more resource intensive, uses an LLM for generating the summaries and is guided by chain-of-thought to provide sequences of text indicating clinical markers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.11220</link><description>&lt;p&gt;
CPA-Enhancer&#65306;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#21333;&#19968;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#26469;&#30830;&#23450;&#36864;&#21270;&#31867;&#22411;&#65292;&#24182;&#20026;&#27599;&#31181;&#31867;&#22411;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CPA-Enhancer&#22312;CoT&#25552;&#31034;&#30340;&#36880;&#27493;&#25351;&#23548;&#19979;&#36880;&#27493;&#35843;&#25972;&#20854;&#22686;&#24378;&#31574;&#30053;&#65292;&#36825;&#20123;&#25552;&#31034;&#32534;&#30721;&#20102;&#19982;&#36864;&#21270;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#21033;&#29992;CoT&#25552;&#31034;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CPA-Enhancer&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#22312;&#19981;&#20107;&#20808;&#30693;&#36947;&#36864;&#21270;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21463;&#25439;&#22270;&#20687;&#19978;&#23454;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CPA-E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ANN&#30340;&#33258;&#36866;&#24212;&#20998;&#31867;&#26041;&#27861;CBR&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35782;&#21035;&#26032;&#30340;&#21644;&#29616;&#26377;&#30340;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.11206</link><description>&lt;p&gt;
CBR - &#36890;&#36807;&#26816;&#32034;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#22686;&#24378;&#33258;&#36866;&#24212;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CBR - Boosting Adaptive Classification By Retrieval of Encrypted Network Traffic with Out-of-distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ANN&#30340;&#33258;&#36866;&#24212;&#20998;&#31867;&#26041;&#27861;CBR&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35782;&#21035;&#26032;&#30340;&#21644;&#29616;&#26377;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#21644;&#19981;&#21516;&#30340;&#30446;&#26631;&#35299;&#20915;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#22266;&#23450;&#25968;&#37327;&#30340;&#31867;&#21035;&#65292;&#23548;&#33268;&#24403;&#36755;&#20837;&#19968;&#20010;&#26410;&#30693;&#31867;&#21035;&#26102;&#21457;&#29983;&#38169;&#35823;&#20998;&#31867;&#12290;&#22788;&#29702;&#26410;&#30693;&#31867;&#21035;&#30340;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#27599;&#27425;&#27169;&#22411;&#21464;&#24471;&#36807;&#26102;&#37117;&#37325;&#26032;&#35757;&#32451;&#26082;&#32791;&#36153;&#36164;&#28304;&#21448;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#35753;&#20998;&#31867;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#26816;&#27979;&#21644;&#36866;&#24212;&#26032;&#31867;&#21035;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#32780;&#26159;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26469;&#26816;&#27979;&#26032;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#33258;&#36866;&#24212;&#20998;&#31867;&#26041;&#27861;CBR&#65292;&#29992;&#20110;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22522;&#20110;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#30340;&#21644;&#29616;&#26377;&#30340;&#31867;&#21035;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11206v1 Announce Type: new  Abstract: Encrypted network traffic Classification tackles the problem from different approaches and with different goals. One of the common approaches is using Machine learning or Deep Learning-based solutions on a fixed number of classes, leading to misclassification when an unknown class is given as input. One of the solutions for handling unknown classes is to retrain the model, however, retraining models every time they become obsolete is both resource and time-consuming. Therefore, there is a growing need to allow classification models to detect and adapt to new classes dynamically, without retraining, but instead able to detect new classes using few shots learning [1]. In this paper, we introduce Adaptive Classification By Retrieval CBR, a novel approach for encrypted network traffic classification. Our new approach is based on an ANN-based method, which allows us to effectively identify new and existing classes without retraining the model
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040;&#19981;&#21516;GPU&#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11204</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#36827;&#34892;&#20998;&#21306;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Partitioned Neural Network Training via Synthetic Intermediate Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040;&#19981;&#21516;GPU&#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26222;&#21450;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290; GPU &#20869;&#23384;&#32422;&#26463;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#36825;&#20123;&#24222;&#22823;&#27169;&#22411;&#30340;&#19968;&#20010;&#26126;&#26174;&#29942;&#39048;&#12290;&#29616;&#26377;&#31574;&#30053;&#65292;&#21253;&#25324;&#25968;&#25454;&#24182;&#34892;&#12289;&#27169;&#22411;&#24182;&#34892;&#12289;&#27969;&#27700;&#32447;&#24182;&#34892;&#21644;&#23436;&#20840;&#20998;&#29255;&#25968;&#25454;&#24182;&#34892;&#65292;&#25552;&#20379;&#20102;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#12290; &#29305;&#21035;&#26159;&#27169;&#22411;&#24182;&#34892;&#20801;&#35768;&#23558;&#25972;&#20010;&#27169;&#22411;&#20998;&#24067;&#22312;&#22810;&#20010; GPU &#19978;&#65292;&#20294;&#38543;&#21518;&#30340;&#36825;&#20123;&#20998;&#21306;&#20043;&#38388;&#30340;&#25968;&#25454;&#36890;&#20449;&#20943;&#24930;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#20026;&#22312;&#27599;&#20010; GPU &#19978;&#23384;&#20648;&#36741;&#21161;&#21442;&#25968;&#25152;&#38656;&#30340;&#22823;&#37327;&#20869;&#23384;&#24320;&#38144;&#22686;&#21152;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290; &#26412;&#30740;&#31350;&#20027;&#24352;&#19981;&#20351;&#29992;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26159;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040; GPU &#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#12290; &#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#36825;&#20123;&#26631;&#31614;&#20943;&#32531;&#20102;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11204v1 Announce Type: cross  Abstract: The proliferation of extensive neural network architectures, particularly deep learning models, presents a challenge in terms of resource-intensive training. GPU memory constraints have become a notable bottleneck in training such sizable models. Existing strategies, including data parallelism, model parallelism, pipeline parallelism, and fully sharded data parallelism, offer partial solutions. Model parallelism, in particular, enables the distribution of the entire model across multiple GPUs, yet the ensuing data communication between these partitions slows down training. Additionally, the substantial memory overhead required to store auxiliary parameters on each GPU compounds computational demands. Instead of using the entire model for training, this study advocates partitioning the model across GPUs and generating synthetic intermediate labels to train individual segments. These labels, produced through a random process, mitigate me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUMP&#30340;&#22270;&#21333;&#20803;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#26469;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11199</link><description>&lt;p&gt;
&#22270;&#21333;&#20803;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Graph Unitary Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUMP&#30340;&#22270;&#21333;&#20803;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#26469;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25913;&#21892;&#22270;&#35889;&#30340;&#37325;&#36830;&#25216;&#26415;&#12289;&#30772;&#22351;&#22270;&#20013;&#30340;&#32467;&#26500;&#20559;&#35265;&#26469;&#25269;&#21046;&#36807;&#24230;&#21387;&#32553;&#65292;&#28982;&#32780;&#22312;&#36807;&#24230;&#21387;&#32553;&#24230;&#37327;&#26041;&#38754;&#23545;&#36807;&#24230;&#21387;&#32553;&#30340;&#25913;&#36827;&#26377;&#25152;&#38480;&#21046;&#12290;&#21463;&#21040;&#21333;&#20803;RNN&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#21333;&#20803;&#28040;&#24687;&#20256;&#36882;&#65288;GUMP&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#26469;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#20026;&#35774;&#35745;GUMP&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#26222;&#36890;&#22270;&#20855;&#26377;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#24182;&#20445;&#25345;&#20854;&#32467;&#26500;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#30340;&#22266;&#26377;&#32467;&#26500;&#23454;&#29616;&#21333;&#20301;&#21270;&#25237;&#24433;&#31639;&#27861;&#33719;&#24471;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#65292;&#24182;&#20801;&#35768;GUMP&#26159;&#32622;&#25442;&#31561;&#21464;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;GUMP&#22312;&#25913;&#21892;&#21508;&#31181;&#24212;&#29992;&#20219;&#21153;&#19978;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11199v1 Announce Type: cross  Abstract: Message passing mechanism contributes to the success of GNNs in various applications, but also brings the oversquashing problem. Recent works combat oversquashing by improving the graph spectrums with rewiring techniques, disrupting the structural bias in graphs, and having limited improvement on oversquashing in terms of oversquashing measure. Motivated by unitary RNN, we propose Graph Unitary Message Passing (GUMP) to alleviate oversquashing in GNNs by applying unitary adjacency matrix for message passing. To design GUMP, a transformation is first proposed to make general graphs have unitary adjacency matrix and keep its structural bias. Then, unitary adjacency matrix is obtained with a unitary projection algorithm, which is implemented by utilizing the intrinsic structure of unitary adjacency matrix and allows GUMP to be permutation-equivariant. Experimental results show the effectiveness of GUMP in improving the performance on vari
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;IDS&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;IDS&#22312;&#26816;&#27979;&#38646;&#26085;&#25110;&#26410;&#30693;&#25915;&#20987;&#26041;&#38754;&#30340;&#19981;&#36275;</title><link>https://arxiv.org/abs/2403.11180</link><description>&lt;p&gt;
&#22522;&#20110;usfAD&#30340;&#26377;&#25928;&#26410;&#30693;&#25915;&#20987;&#26816;&#27979;&#20026;&#37325;&#28857;&#30340;IDS&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
usfAD Based Effective Unknown Attack Detection Focused IDS Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;IDS&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;IDS&#22312;&#26816;&#27979;&#38646;&#26085;&#25110;&#26410;&#30693;&#25915;&#20987;&#26041;&#38754;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#21508;&#31181;&#32593;&#32476;&#31995;&#32479;&#30340;&#24555;&#36895;&#25193;&#24352;&#65292;&#21253;&#25324;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#21644;&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#65292;&#23548;&#33268;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#32593;&#32476;&#23041;&#32961;&#12290;&#30830;&#20445;&#23545;&#25239;&#36825;&#20123;&#23041;&#32961;&#30340;&#31283;&#20581;&#20445;&#25252;&#38656;&#35201;&#23454;&#26045;&#19968;&#31181;&#26377;&#25928;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#12290;&#30740;&#31350;&#20154;&#21592;&#21313;&#22810;&#24180;&#26469;&#19968;&#30452;&#28145;&#20837;&#30740;&#31350;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#24320;&#21457;IDS&#26469;&#23545;&#27491;&#24120;&#21644;&#25915;&#20987;&#27969;&#37327;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#26377;&#25928;&#30340;IDS&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#33391;&#24615;&#21644;&#25915;&#20987;&#26679;&#26412;&#12290;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#25910;&#38598;&#36275;&#22815;&#25968;&#37327;&#30340;&#25915;&#20987;&#26679;&#26412;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#32593;&#32476;&#25915;&#20987;&#20598;&#23572;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#20110;&#24050;&#30693;&#25968;&#25454;&#38598;&#19978;&#30340;IDS&#26080;&#27861;&#26816;&#27979;&#21040;&#38646;&#26085;&#25110;&#26410;&#30693;&#25915;&#20987;&#65292;&#21407;&#22240;&#26159;&#25915;&#20987;&#27169;&#24335;&#30340;&#36805;&#36895;&#28436;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;IDS&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11180v1 Announce Type: cross  Abstract: The rapid expansion of varied network systems, including the Internet of Things (IoT) and Industrial Internet of Things (IIoT), has led to an increasing range of cyber threats. Ensuring robust protection against these threats necessitates the implementation of an effective Intrusion Detection System (IDS). For more than a decade, researchers have delved into supervised machine learning techniques to develop IDS to classify normal and attack traffic. However, building effective IDS models using supervised learning requires a substantial number of benign and attack samples. To collect a sufficient number of attack samples from real-life scenarios is not possible since cyber attacks occur occasionally. Further, IDS trained and tested on known datasets fails in detecting zero-day or unknown attacks due to the swift evolution of attack patterns. To address this challenge, we put forth two strategies for semi-supervised learning based IDS wh
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20808;&#39564;&#20381;&#36182;&#24615;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#23545;&#21518;&#39564;&#25277;&#26679;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#25913;&#36827;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#22522;&#20934;&#30340;&#26041;&#27861;&#35770;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.11175</link><description>&lt;p&gt;
&#20808;&#39564;&#20381;&#36182;&#24615;&#20998;&#26512;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#30340;&#21518;&#39564;&#25277;&#26679;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prior-dependent analysis of posterior sampling reinforcement learning with function approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11175
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20808;&#39564;&#20381;&#36182;&#24615;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#23545;&#21518;&#39564;&#25277;&#26679;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#25913;&#36827;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#22522;&#20934;&#30340;&#26041;&#27861;&#35770;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#23545;&#32447;&#24615;&#28151;&#21512;MDPs&#24314;&#27169;&#30340;&#20989;&#25968;&#36924;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#25512;&#36827;&#20102;&#38543;&#26426;&#25506;&#32034;&#12290;&#25105;&#20204;&#20026;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;RL&#24314;&#31435;&#20102;&#39318;&#20010;&#20808;&#39564;&#20381;&#36182;&#24615;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#65307;&#24182;&#19988;&#25913;&#36827;&#20102;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#24378;&#21270;&#23398;&#20064;&#65288;PSRL&#65289;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#30028;&#20026;${\mathcal{O}}(d\sqrt{H^3 T \log T})$&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;$d$&#34920;&#31034;&#36716;&#31227;&#26680;&#30340;&#32500;&#24230;&#65292;$H$&#34920;&#31034;&#35268;&#21010;&#35270;&#37326;&#65292;$T$&#34920;&#31034;&#24635;&#20132;&#20114;&#27425;&#25968;&#12290; &#36825;&#34920;&#31034;&#36890;&#36807;&#20248;&#21270;$\mathcal{O}(\sqrt{\log T})$&#22240;&#23376;&#65292;&#25105;&#20204;&#22312;&#20043;&#21069;&#38024;&#23545;&#32447;&#24615;&#28151;&#21512;MDPs&#30340;&#22522;&#20934;&#65288;Osband&#21644;Van Roy&#65292;2014&#65289;&#19978;&#21462;&#24471;&#20102;&#26041;&#27861;&#35770;&#19978;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20215;&#20540;&#23450;&#21521;&#27169;&#22411;&#23398;&#20064;&#30340;&#35270;&#35282;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#35770;&#35777;&#21644;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#20998;&#26512;&#20381;&#36182;&#20110;&#32622;&#20449;&#21306;&#38388;&#21644;&#38598;&#20013;&#19981;&#31561;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11175v1 Announce Type: cross  Abstract: This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs. We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of ${\mathcal{O}}(d\sqrt{H^3 T \log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\mathcal{O}(\sqrt{\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#38024;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#22635;&#34917;&#20102;&#29616;&#20195;NAS&#26041;&#27861;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.11173</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#38024;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#22635;&#34917;&#20102;&#29616;&#20195;NAS&#26041;&#27861;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26550;&#26500;&#35774;&#35745;&#26159;&#19968;&#39033;&#19981;&#23481;&#26131;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26088;&#22312;&#33258;&#21160;&#21270;NN&#26550;&#26500;&#30340;&#35774;&#35745;&#65292;&#24182;&#24050;&#35777;&#26126;&#22312;&#33258;&#21160;&#26597;&#25214;&#24615;&#33021;&#20248;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;NN&#26550;&#26500;&#26041;&#38754;&#26159;&#25104;&#21151;&#30340;&#12290;NN&#26550;&#26500;&#24615;&#33021;&#21487;&#20197;&#22522;&#20110;&#22810;&#20010;&#30446;&#26631;&#36827;&#34892;&#37327;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#22411;&#20934;&#30830;&#24230;&#21644;&#19968;&#20123;NN&#26550;&#26500;&#22797;&#26434;&#24615;&#30446;&#26631;&#31561;&#12290;&#22823;&#22810;&#25968;&#32771;&#34385;&#22810;&#20010;&#30446;&#26631;&#36827;&#34892;NN&#26550;&#26500;&#24615;&#33021;&#35780;&#20272;&#30340;&#29616;&#20195;NAS&#26041;&#27861;&#20851;&#27880;&#20110;&#33258;&#21160;&#21270;&#21069;&#39304;NN&#26550;&#26500;&#35774;&#35745;&#65292;&#32780;&#24573;&#30053;&#20102;&#22810;&#30446;&#26631;&#33258;&#21160;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;&#35774;&#35745;&#12290;RNN&#23545;&#20110;&#24314;&#27169;&#24207;&#21015;&#25968;&#25454;&#38598;&#38750;&#24120;&#37325;&#35201;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20869;&#21344;&#26377;&#26174;&#33879;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11173v1 Announce Type: cross  Abstract: Artificial neural network (NN) architecture design is a nontrivial and time-consuming task that often requires a high level of human expertise. Neural architecture search (NAS) serves to automate the design of NN architectures and has proven to be successful in automatically finding NN architectures that outperform those manually designed by human experts. NN architecture performance can be quantified based on multiple objectives, which include model accuracy and some NN architecture complexity objectives, among others. The majority of modern NAS methods that consider multiple objectives for NN architecture performance evaluation are concerned with automated feed forward NN architecture design, which leaves multi-objective automated recurrent neural network (RNN) architecture design unexplored. RNNs are important for modeling sequential datasets, and prominent within the natural language processing domain. It is often the case in real 
&lt;/p&gt;</description></item><item><title>Pencil&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#21327;&#20316;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#25361;&#25112;&#30340;&#26694;&#26550;&#65292;&#20860;&#39038;&#27169;&#22411;&#21644;&#25968;&#25454;&#38544;&#31169;&#65292;&#19981;&#20381;&#36182;&#38750;&#20849;&#35851;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2403.11166</link><description>&lt;p&gt;
&#38085;&#31508;&#65306;&#26080;&#38656;&#38750;&#20849;&#35851;&#20551;&#35774;&#30340;&#31169;&#23494;&#21487;&#25193;&#23637;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11166
&lt;/p&gt;
&lt;p&gt;
Pencil&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#21327;&#20316;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#25361;&#25112;&#30340;&#26694;&#26550;&#65292;&#20860;&#39038;&#27169;&#22411;&#21644;&#25968;&#25454;&#38544;&#31169;&#65292;&#19981;&#20381;&#36182;&#38750;&#20849;&#35851;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20851;&#27880;&#19981;&#26029;&#25552;&#21319;&#65292;&#21327;&#20316;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#20854;&#20013;&#25968;&#25454;&#25152;&#26377;&#26435;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;/&#37096;&#32626;&#36131;&#20219;&#20998;&#23646;&#20110;&#19981;&#21516;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#31038;&#21306;&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#21644;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;MPC&#65289;&#31561;&#23494;&#30721;&#26500;&#36896;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;FL&#23436;&#20840;&#24573;&#35270;&#20102;&#27169;&#22411;&#38544;&#31169;&#65292;HE&#30340;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#65288;&#20165;&#38480;&#20110;&#19968;&#23478;&#25968;&#25454;&#25552;&#20379;&#32773;&#65289;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;MPC&#26694;&#26550;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#21534;&#21520;&#37327;&#24182;&#21516;&#26102;&#30830;&#20445;&#20102;&#27169;&#22411;/&#25968;&#25454;&#38544;&#31169;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#35745;&#31639;&#26381;&#21153;&#22120;&#19978;&#20551;&#35774;&#30340;&#20851;&#38190;&#38750;&#20849;&#35851;&#20551;&#35774;&#65292;&#25918;&#26494;&#36825;&#19968;&#20551;&#35774;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pencil&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#21327;&#20316;&#35757;&#32451;&#30340;&#31169;&#23494;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11166v1 Announce Type: cross  Abstract: The escalating focus on data privacy poses significant challenges for collaborative neural network training, where data ownership and model training/deployment responsibilities reside with distinct entities. Our community has made substantial contributions to addressing this challenge, proposing various approaches such as federated learning (FL) and privacy-preserving machine learning based on cryptographic constructs like homomorphic encryption (HE) and secure multiparty computation (MPC). However, FL completely overlooks model privacy, and HE has limited extensibility (confined to only one data provider). While the state-of-the-art MPC frameworks provide reasonable throughput and simultaneously ensure model/data privacy, they rely on a critical non-colluding assumption on the computing servers, and relaxing this assumption is still an open problem.   In this paper, we present Pencil, the first private training framework for collabora
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36873;&#25321;&#24615;&#32508;&#36848;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#35745;&#31639;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#23376;&#37319;&#26679;&#20197;&#21450;&#23567;&#25209;&#37327;&#26799;&#24230;&#25216;&#26415;&#36825;&#19977;&#31867;&#32479;&#35745;&#35745;&#31639;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11163</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#25968;&#25454;&#35745;&#31639;&#30340;&#32479;&#35745;&#26041;&#27861;&#36873;&#25321;&#24615;&#32508;&#36848;&#65306;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#23376;&#37319;&#26679;&#21644;&#23567;&#25209;&#37327;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Selective Review on Statistical Methods for Massive Data Computation: Distributed Computing, Subsampling, and Minibatch Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36873;&#25321;&#24615;&#32508;&#36848;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#35745;&#31639;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#23376;&#37319;&#26679;&#20197;&#21450;&#23567;&#25209;&#37327;&#26799;&#24230;&#25216;&#26415;&#36825;&#19977;&#31867;&#32479;&#35745;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#30340;&#32479;&#35745;&#35745;&#31639;&#26041;&#27861;&#36827;&#34892;&#36873;&#25321;&#24615;&#32508;&#36848;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#36805;&#36895;&#21457;&#23637;&#20102;&#22823;&#37327;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#35745;&#31639;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#19977;&#31867;&#32479;&#35745;&#35745;&#31639;&#26041;&#27861;&#65306;&#65288;1&#65289;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#65288;2&#65289;&#23376;&#37319;&#26679;&#26041;&#27861;&#65292;&#21644;&#65288;3&#65289;&#23567;&#25209;&#37327;&#26799;&#24230;&#25216;&#26415;&#12290;&#31532;&#19968;&#31867;&#25991;&#29486;&#20851;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#37325;&#28857;&#26159;&#22312;&#25968;&#25454;&#38598;&#22826;&#22823;&#26080;&#27861;&#34987;&#21333;&#21488;&#35745;&#31639;&#26426;&#36731;&#26494;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22810;&#21488;&#35745;&#31639;&#26426;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#31995;&#32479;&#12290;&#31532;&#20108;&#31867;&#25991;&#29486;&#20851;&#20110;&#23376;&#37319;&#26679;&#26041;&#27861;&#65292;&#20851;&#27880;&#30340;&#26159;&#26679;&#26412;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#36275;&#22815;&#23567;&#65292;&#21487;&#20197;&#25918;&#22312;&#19968;&#21488;&#35745;&#31639;&#26426;&#19978;&#65292;&#20294;&#22826;&#22823;&#20197;&#33267;&#20110;&#26080;&#27861;&#25972;&#20307;&#22788;&#29702;&#20854;&#20869;&#23384;&#12290;&#26368;&#21518;&#19968;&#31867;&#25991;&#29486;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#26799;&#24230;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11163v1 Announce Type: cross  Abstract: This paper presents a selective review of statistical computation methods for massive data analysis. A huge amount of statistical methods for massive data computation have been rapidly developed in the past decades. In this work, we focus on three categories of statistical computation methods: (1) distributed computing, (2) subsampling methods, and (3) minibatch gradient techniques. The first class of literature is about distributed computing and focuses on the situation, where the dataset size is too huge to be comfortably handled by one single computer. In this case, a distributed computation system with multiple computers has to be utilized. The second class of literature is about subsampling methods and concerns about the situation, where the sample size of dataset is small enough to be placed on one single computer but too large to be easily processed by its memory as a whole. The last class of literature studies those minibatch g
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#26799;&#24230;&#21453;&#36716;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#23383;&#29256;&#26435;&#35748;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#20043;&#38388;&#30340;&#27010;&#24565;&#24046;&#24322;&#26469;&#24674;&#22797;&#22270;&#20687;&#30340;&#32570;&#22833;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2403.11162</link><description>&lt;p&gt;
CGI-DM&#65306;&#36890;&#36807;&#23545;&#27604;&#26799;&#24230;&#21453;&#36716;&#36827;&#34892;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#23383;&#29256;&#26435;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#26799;&#24230;&#21453;&#36716;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#23383;&#29256;&#26435;&#35748;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#20043;&#38388;&#30340;&#27010;&#24565;&#24046;&#24322;&#26469;&#24674;&#22797;&#22270;&#20687;&#30340;&#32570;&#22833;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#24050;&#32463;&#21457;&#23637;&#25104;&#20026;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#26679;&#26412;&#29983;&#25104;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#24494;&#35843;&#21040;&#19968;&#23567;&#32452;&#22270;&#20687;&#19978;&#20197;&#25429;&#25417;&#29305;&#23450;&#39118;&#26684;&#25110;&#23545;&#35937;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#23545;&#28508;&#22312;&#30340;&#29256;&#26435;&#20405;&#29359;&#38382;&#39064;&#34920;&#31034;&#25285;&#24551;&#65292;&#36825;&#20027;&#35201;&#28304;&#20110;&#22312;&#27492;&#36807;&#31243;&#20013;&#20351;&#29992;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23545;&#27604;&#26799;&#24230;&#21453;&#36716;&#36827;&#34892;&#25193;&#25955;&#27169;&#22411;&#65288;CGI-DM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#29983;&#21160;&#30340;&#35270;&#35273;&#22270;&#20687;&#65292;&#29992;&#20110;&#25968;&#23383;&#29256;&#26435;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21024;&#38500;&#22270;&#20687;&#30340;&#37096;&#20998;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#20043;&#38388;&#30340;&#27010;&#24565;&#24046;&#24322;&#24674;&#22797;&#32570;&#22833;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#27169;&#22411;&#30340;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#26500;&#24314;&#20026;&#32473;&#23450;&#30456;&#21516;&#36755;&#20837;&#22270;&#20687;&#26102;&#30340;KL&#25955;&#24230;&#65292;&#21487;&#20197;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#21644;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#36827;&#34892;&#26368;&#22823;&#21270;&#12290;&#21407;&#22270;&#20687;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11162v1 Announce Type: cross  Abstract: Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and
&lt;/p&gt;</description></item><item><title>Mamba&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#25429;&#25417;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12289;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#20197;&#21450;&#24615;&#33021;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11144</link><description>&lt;p&gt;
Mamba&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Mamba Effective for Time Series Forecasting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11144
&lt;/p&gt;
&lt;p&gt;
Mamba&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#25429;&#25417;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12289;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#20197;&#21450;&#24615;&#33021;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;Transformer&#27169;&#22411;&#33021;&#22815;&#32858;&#28966;&#20840;&#23616;&#29615;&#22659;&#65292;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#36776;&#21035;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#23427;&#19968;&#30452;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;Transformer&#27169;&#22411;&#30340;&#20302;&#25928;&#29575;&#21644;&#20851;&#20110;&#20854;&#25429;&#25417;&#20381;&#36182;&#20851;&#31995;&#33021;&#21147;&#30340;&#36136;&#30097;&#65292;&#23545;Transformer&#26550;&#26500;&#30340;&#19981;&#26029;&#23436;&#21892;&#24037;&#20316;&#20173;&#22312;&#36827;&#34892;&#20013;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22914;Mamba&#22240;&#20854;&#33021;&#22815;&#20687;Transformer&#19968;&#26679;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#21448;&#20445;&#25345;&#36817;&#32447;&#24615;&#30340;&#22797;&#26434;&#24230;&#32780;&#22791;&#21463;&#25512;&#23815;&#12290;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;Mamba&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#33410;&#32422;&#25104;&#26412;&#65292;&#23454;&#29616;&#21452;&#36194;&#23616;&#38754;&#12290;&#36825;&#24341;&#36215;&#20102;&#25105;&#20204;&#23545;&#25506;&#32034;SSM&#22312;TSF&#20219;&#21153;&#20013;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;SSM&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;S-Mamba&#21644;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11144v1 Announce Type: new  Abstract: In the realm of time series forecasting (TSF), the Transformer has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables. However, due to the inefficiencies of the Transformer model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the Transformer architecture persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the Transformer, while maintaining near-linear complexity. In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation. This has piqued our interest in exploring SSM's potential in TSF tasks. In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#23450;&#29702;&#25506;&#35752;&#20102;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#65292;&#21253;&#25324;&#32771;&#34385;&#21644;&#24573;&#30053;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#39034;&#24207;&#22810;&#20010;&#35757;&#32451;&#26679;&#26412;&#22686;&#30410;&#30340;&#29702;&#35770;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11125</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based system reliability analysis with Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#23450;&#29702;&#25506;&#35752;&#20102;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#65292;&#21253;&#25324;&#32771;&#34385;&#21644;&#24573;&#30053;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#39034;&#24207;&#22810;&#20010;&#35757;&#32451;&#26679;&#26412;&#22686;&#30410;&#30340;&#29702;&#35770;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11125v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#35768;&#22810;&#26377;&#25928;&#30340;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;&#35745;&#31639;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#24456;&#23569;&#26377;&#20154;&#25506;&#35752;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#23450;&#29702;&#26469;&#20419;&#36827;&#36825;&#31181;&#25506;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#32771;&#34385;&#21644;&#24573;&#30053;&#20505;&#36873;&#35774;&#35745;&#26679;&#26412;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340; U &#23398;&#20064;&#20989;&#25968;&#21487;&#20197;&#37325;&#26032;&#21046;&#23450;&#20026;&#22312;&#24573;&#30053; Kriging &#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#23398;&#20064;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#36890;&#36807;&#24102;&#26377;&#30456;&#24212;&#25439;&#22833;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#25968;&#23398;&#19978;&#25506;&#35752;&#20102;&#39034;&#24207;&#22810;&#20010;&#35757;&#32451;&#26679;&#26412;&#22686;&#30410;&#30340;&#29702;&#35770;&#19978;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11125v1 Announce Type: cross  Abstract: Machine learning-based reliability analysis methods have shown great advancements for their computational efficiency and accuracy. Recently, many efficient learning strategies have been proposed to enhance the computational performance. However, few of them explores the theoretical optimal learning strategy. In this article, we propose several theorems that facilitates such exploration. Specifically, cases that considering and neglecting the correlations among the candidate design samples are well elaborated. Moreover, we prove that the well-known U learning function can be reformulated to the optimal learning function for the case neglecting the Kriging correlation. In addition, the theoretical optimal learning strategy for sequential multiple training samples enrichment is also mathematically explored through the Bayesian estimate with the corresponding lost functions. Simulation results show that the optimal learning strategy consid
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Phasic Diversity Optimization (PDO)&#31639;&#27861;&#65292;&#37319;&#29992;&#31181;&#32676;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#22870;&#21169;&#21644;&#22810;&#26679;&#24615;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#38454;&#27573;&#65292;&#24182;&#22312;&#36741;&#21161;&#38454;&#27573;&#23454;&#29616;&#28608;&#36827;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.11114</link><description>&lt;p&gt;
&#31181;&#32676;&#24378;&#21270;&#23398;&#20064;&#30340;&#30456;&#20301;&#22810;&#26679;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Phasic Diversity Optimization for Population-Based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11114
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Phasic Diversity Optimization (PDO)&#31639;&#27861;&#65292;&#37319;&#29992;&#31181;&#32676;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#22870;&#21169;&#21644;&#22810;&#26679;&#24615;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#38454;&#27573;&#65292;&#24182;&#22312;&#36741;&#21161;&#38454;&#27573;&#23454;&#29616;&#28608;&#36827;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Phasic Diversity Optimization (PDO)&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#31181;&#32676;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#22870;&#21169;&#21644;&#22810;&#26679;&#24615;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#32780;&#19981;&#26159;&#20248;&#21270;&#22810;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#36741;&#21161;&#38454;&#27573;&#65292;&#34920;&#29616;&#36739;&#24046;&#30340;agent&#36890;&#36807;&#20915;&#31574;&#32773;&#36827;&#34892;&#22810;&#26679;&#21270;&#65292;&#19981;&#20250;&#21462;&#20195;&#23384;&#26723;&#20013;&#26356;&#22909;&#30340;agent&#12290;&#22870;&#21169;&#21644;&#22810;&#26679;&#24615;&#30340;&#35299;&#32806;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#36741;&#21161;&#38454;&#27573;&#20351;&#29992;&#28608;&#36827;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11114v1 Announce Type: cross  Abstract: Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.Generally,diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;(SQAKD)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20840;&#31934;&#24230;&#21644;&#20302;&#27604;&#29305;&#27169;&#22411;&#20043;&#38388;&#30340;KL&#25439;&#22833;&#20197;&#21450;&#37327;&#21270;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#22797;&#26434;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11106</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Quantization-Aware Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11106
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;(SQAKD)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20840;&#31934;&#24230;&#21644;&#20302;&#27604;&#29305;&#27169;&#22411;&#20043;&#38388;&#30340;KL&#25439;&#22833;&#20197;&#21450;&#37327;&#21270;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#22797;&#26434;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#25022;&#22320;&#65292;&#29616;&#26377;&#24037;&#20316;&#23558;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#38656;&#35201;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26469;&#24179;&#34913;&#19981;&#21516;&#25439;&#22833;&#39033;&#30340;&#26435;&#37325;&#65292;&#20551;&#23450;&#26377;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#65292;&#24182;&#19988;&#38656;&#35201;&#22797;&#26434;&#12289;&#35745;&#31639;&#23494;&#38598;&#30340;&#35757;&#32451;&#31243;&#24207;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;(SQAKD)&#26694;&#26550;&#12290;SQAKD&#39318;&#20808;&#32479;&#19968;&#20102;&#21508;&#31181;&#37327;&#21270;&#20989;&#25968;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#21160;&#24577;&#65292;&#20351;&#20854;&#21487;&#20197;&#28789;&#27963;&#22320;&#25972;&#21512;&#21508;&#31181;QAT&#24037;&#20316;&#12290;&#28982;&#21518;&#65292;&#23427;&#23558;QAT&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#29992;&#20110;KD&#30340;&#20840;&#31934;&#24230;&#27169;&#22411;&#21644;&#20302;&#27604;&#29305;&#27169;&#22411;&#20043;&#38388;&#30340;KL&#25439;&#22833;&#65292;&#20197;&#21450;&#29992;&#20110;&#37327;&#21270;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#32780;&#26080;&#38656;&#26469;&#33258;&#26631;&#31614;&#30340;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11106v1 Announce Type: cross  Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models. However, existing works applying KD to QAT require tedious hyper-parameter tuning to balance the weights of different loss terms, assume the availability of labeled training data, and require complex, computationally intensive training procedures for good performance. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and backward dynamics of various quantization functions, making it flexible for incorporating various QAT works. Then it formulates QAT as a co-optimization problem that simultaneously minimizes the KL-Loss between the full-precision and low-bit models for KD and the discretization error for quantization, without supervision from labels. A comprehensive e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#30465;&#29305;&#23450;&#39046;&#22495;&#65292;&#29983;&#25104;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#24182;&#21019;&#24314;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32469;&#36807;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#21019;&#26032;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11103</link><description>&lt;p&gt;
ProgGen:&#36890;&#36807;&#33258;&#21453;&#22823;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11103
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#30465;&#29305;&#23450;&#39046;&#22495;&#65292;&#29983;&#25104;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#24182;&#21019;&#24314;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32469;&#36807;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#30340;&#21019;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36328;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#25552;&#21462;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;NER&#65289;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#20855;&#26377;&#36866;&#24230;NER&#33021;&#21147;&#30340;LLMs&#26469;&#29983;&#25104;&#20248;&#31168;&#30340;NER&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#22522;&#26412;&#30340;&#31867;&#26465;&#20214;&#25552;&#31034;&#65292;&#32780;&#26159;&#25351;&#23548;LLMs&#23545;&#29305;&#23450;&#39046;&#22495;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#65292;&#20174;&#32780;&#29983;&#25104;&#20855;&#26377;&#39046;&#22495;&#30456;&#20851;&#23646;&#24615;&#65288;&#20363;&#22914;&#24433;&#35780;&#30340;&#31867;&#21035;&#21644;&#24773;&#24863;&#65289;&#30340;&#23646;&#24615;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39044;&#20808;&#29983;&#25104;&#23454;&#20307;&#26415;&#35821;&#65292;&#28982;&#21518;&#22260;&#32469;&#36825;&#20123;&#23454;&#20307;&#24320;&#21457;NER&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#26377;&#25928;&#35268;&#36991;&#20102;LLMs&#23545;&#22797;&#26434;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#36890;&#29992;&#21644;&#19987;&#19994;&#39046;&#22495;&#23637;&#24320;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11103v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being mor
&lt;/p&gt;</description></item><item><title>&#22270;&#25193;&#23637;&#30340;&#24615;&#36136;&#26159;&#35813;&#30740;&#31350;&#30340;&#20851;&#38190;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20462;&#21098;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#20445;&#25345;&#22270;&#30340;&#25193;&#23637;&#24615;&#33021;&#21487;&#20197;&#32500;&#25345;RNN&#21644;LSTM&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11100</link><description>&lt;p&gt;
&#22312;&#20462;&#21098;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#20445;&#25345;&#24615;&#33021;&#30340;&#22270;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11100
&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#23637;&#30340;&#24615;&#36136;&#26159;&#35813;&#30740;&#31350;&#30340;&#20851;&#38190;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20462;&#21098;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#20445;&#25345;&#22270;&#30340;&#25193;&#23637;&#24615;&#33021;&#21487;&#20197;&#32500;&#25345;RNN&#21644;LSTM&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#30340;&#25193;&#23637;&#24615;&#36136;&#25351;&#30340;&#26159;&#20854;&#24378;&#36830;&#36890;&#24615;&#21644;&#31232;&#30095;&#24615;&#12290;&#24050;&#32463;&#25253;&#21578;&#31216;&#21487;&#20197;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20462;&#21098;&#65292;&#20351;&#20854;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#20855;&#26377;&#39640;&#24230;&#30340;&#31232;&#30095;&#24615;&#12290;&#36825;&#31181;&#20462;&#21098;&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#24179;&#21488;&#19978;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#23454;&#26102;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23545;&#35832;&#22914;RNN&#21644;LSTM&#30340;&#24490;&#29615;&#32593;&#32476;&#36827;&#34892;&#20462;&#21098;&#65292;&#20445;&#25345;&#24213;&#23618;&#22270;&#30340;&#36739;&#22823;&#35889;&#38388;&#38548;&#65292;&#24182;&#30830;&#20445;&#23427;&#20204;&#30340;&#36880;&#23618;&#25193;&#23637;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#20174;&#20854;&#20108;&#20998;&#22270;&#23618;&#38754;&#30740;&#31350;&#26102;&#38388;&#23637;&#24320;&#30340;&#24490;&#29615;&#32593;&#32476;&#22270;&#30340;&#24615;&#36136;&#12290;&#38024;&#23545;&#22522;&#20934;&#24207;&#21015;MNIST&#12289;CIFAR-10&#21644;Google&#35821;&#38899;&#21629;&#20196;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#23637;&#22120;&#22270;&#24418;&#23646;&#24615;&#26159;&#20445;&#25345;RNN&#21644;LSTM&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11100v1 Announce Type: new  Abstract: Expansion property of a graph refers to its strong connectivity as well as sparseness. It has been reported that deep neural networks can be pruned to a high degree of sparsity while maintaining their performance. Such pruning is essential for performing real time sequence learning tasks using recurrent neural networks in resource constrained platforms. We prune recurrent networks such as RNNs and LSTMs, maintaining a large spectral gap of the underlying graphs and ensuring their layerwise expansion properties. We also study the time unfolded recurrent network graphs in terms of the properties of their bipartite layers. Experimental results for the benchmark sequence MNIST, CIFAR-10, and Google speech command data show that expander graph properties are key to preserving classification accuracy of RNN and LSTM.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#23450;&#20215;&#21644;&#21305;&#37197;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#24179;&#21488;&#21033;&#28070;&#65292;&#22312;&#26410;&#30693;&#38656;&#27714;&#21644;&#20379;&#24212;&#20989;&#25968;&#19979;&#65292;&#20445;&#25345;&#39038;&#23458;&#21644;&#26381;&#21153;&#22120;&#38431;&#21015;&#38271;&#24230;&#20302;&#20110;&#38408;&#20540;</title><link>https://arxiv.org/abs/2403.11093</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#21452;&#36793;&#38431;&#21015;&#23450;&#20215;&#21644;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Pricing and Matching for Two-Sided Queues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11093
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#23450;&#20215;&#21644;&#21305;&#37197;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#24179;&#21488;&#21033;&#28070;&#65292;&#22312;&#26410;&#30693;&#38656;&#27714;&#21644;&#20379;&#24212;&#20989;&#25968;&#19979;&#65292;&#20445;&#25345;&#39038;&#23458;&#21644;&#26381;&#21153;&#22120;&#38431;&#21015;&#38271;&#24230;&#20302;&#20110;&#38408;&#20540;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20855;&#26377;&#22810;&#31181;&#31867;&#22411;&#39038;&#23458;&#21644;&#26381;&#21153;&#22120;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#27599;&#31181;&#31561;&#24453;&#30340;&#39038;&#23458;&#25110;&#26381;&#21153;&#22120;&#21152;&#20837;&#19968;&#20010;&#21333;&#29420;&#30340;&#38431;&#21015;&#65292;&#24418;&#25104;&#19968;&#20010;&#20855;&#26377;&#39038;&#23458;&#38431;&#21015;&#21644;&#26381;&#21153;&#22120;&#38431;&#21015;&#30340;&#20108;&#37096;&#22270;&#12290;&#24179;&#21488;&#21487;&#20197;&#21305;&#37197;&#26381;&#21153;&#22120;&#21644;&#39038;&#23458;&#65292;&#22914;&#26524;&#23427;&#20204;&#30340;&#31867;&#22411;&#26159;&#20860;&#23481;&#30340;&#12290;&#21305;&#37197;&#30340;&#23545;&#23558;&#31163;&#24320;&#31995;&#32479;&#12290;&#24179;&#21488;&#23558;&#26681;&#25454;&#39038;&#23458;&#30340;&#31867;&#22411;&#25910;&#21462;&#19968;&#20010;&#20215;&#26684;&#65292;&#24403;&#23427;&#20204;&#21040;&#36798;&#26102;&#65292;&#24182;&#26681;&#25454;&#20854;&#31867;&#22411;&#21521;&#26381;&#21153;&#22120;&#25903;&#20184;&#19968;&#20010;&#20215;&#26684;&#12290;&#27599;&#20010;&#38431;&#21015;&#30340;&#21040;&#36798;&#29575;&#21462;&#20915;&#20110;&#26576;&#20123;&#26410;&#30693;&#30340;&#38656;&#27714;&#25110;&#20379;&#24212;&#20989;&#25968;&#25353;&#20215;&#26684;&#30830;&#23450;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#23450;&#20215;&#21644;&#21305;&#37197;&#31639;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#24179;&#21488;&#22312;&#26410;&#30693;&#38656;&#27714;&#21644;&#20379;&#24212;&#20989;&#25968;&#19979;&#30340;&#21033;&#28070;&#65292;&#21516;&#26102;&#20445;&#25345;&#39038;&#23458;&#21644;&#26381;&#21153;&#22120;&#30340;&#38431;&#21015;&#38271;&#24230;&#20302;&#20110;&#39044;&#23450;&#38408;&#20540;&#12290;&#36825;&#20010;&#31995;&#32479;&#21487;&#20197;&#29992;&#26469;&#24314;&#27169;&#20687;&#20056;&#36710;&#20849;&#20139;&#24066;&#22330;&#36825;&#26679;&#30340;&#21452;&#36793;&#24066;&#22330;&#65292;&#26377;&#20056;&#23458;&#21644;&#21496;&#26426;&#12290;&#25361;&#25112;&#22312;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11093v1 Announce Type: cross  Abstract: We consider a dynamic system with multiple types of customers and servers. Each type of waiting customer or server joins a separate queue, forming a bipartite graph with customer-side queues and server-side queues. The platform can match the servers and customers if their types are compatible. The matched pairs then leave the system. The platform will charge a customer a price according to their type when they arrive and will pay a server a price according to their type. The arrival rate of each queue is determined by the price according to some unknown demand or supply functions. Our goal is to design pricing and matching algorithms to maximize the profit of the platform with unknown demand and supply functions, while keeping queue lengths of both customers and servers below a predetermined threshold. This system can be used to model two-sided markets such as ride-sharing markets with passengers and drivers. The difficulties of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BoS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#24179;&#38754;&#19978;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#27969;&#37327;&#20998;&#26512;&#65292;&#20197;&#25512;&#21160;&#26234;&#33021;&#32593;&#32476;&#25968;&#25454;&#24179;&#38754;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11090</link><description>&lt;p&gt;
&#22823;&#33041;&#24320;&#20851;&#65306;&#36890;&#36807;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#32447;&#36895;&#27969;&#37327;&#20998;&#26512;&#23454;&#29616;&#20808;&#36827;&#26234;&#33021;&#32593;&#32476;&#25968;&#25454;&#24179;&#38754;
&lt;/p&gt;
&lt;p&gt;
Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via NN-Driven Traffic Analysis at Line-Speed
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BoS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#24179;&#38754;&#19978;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#27969;&#37327;&#20998;&#26512;&#65292;&#20197;&#25512;&#21160;&#26234;&#33021;&#32593;&#32476;&#25968;&#25454;&#24179;&#38754;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#21487;&#32534;&#31243;&#32593;&#32476;&#24341;&#21457;&#20102;&#23545;&#26234;&#33021;&#32593;&#32476;&#25968;&#25454;&#24179;&#38754;&#65288;INDP&#65289;&#30340;&#37325;&#35201;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#32447;&#36895;&#27969;&#37327;&#20998;&#26512;&#12290;&#20808;&#21069;&#30340;INDP&#37325;&#28857;&#30740;&#31350;&#22312;&#25968;&#25454;&#24179;&#38754;&#19978;&#37096;&#32626;&#26641;/&#26862;&#26519;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#26641;&#30340;INDP&#26041;&#27861;&#23384;&#22312;&#26681;&#26412;&#23616;&#38480;&#24615;&#65306;&#34429;&#28982;&#22312;&#25968;&#25454;&#24179;&#38754;&#19978;&#21487;&#20197;&#34920;&#31034;&#26356;&#22823;&#30340;&#26641;/&#26862;&#26519;&#34920;&#65292;&#20294;&#25968;&#25454;&#24179;&#38754;&#19978;&#21487;&#35745;&#31639;&#30340;&#27969;&#29305;&#24449;&#21463;&#30828;&#20214;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BoS&#65292;&#36890;&#36807;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#39537;&#21160;&#30340;&#32447;&#36895;&#27969;&#37327;&#20998;&#26512;&#26469;&#25512;&#21160;INDP&#30340;&#36793;&#30028;&#12290;&#35768;&#22810;&#31867;&#22411;&#30340;NN&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;transformers&#65289;&#19987;&#20026;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#32780;&#35774;&#35745;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#23558;&#21407;&#22987;&#32593;&#32476;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#26080;&#38656;&#21363;&#26102;&#36827;&#34892;&#22797;&#26434;&#30340;&#29305;&#24449;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11090v1 Announce Type: cross  Abstract: The emerging programmable networks sparked significant research on Intelligent Network Data Plane (INDP), which achieves learning-based traffic analysis at line-speed. Prior art in INDP focus on deploying tree/forest models on the data plane. We observe a fundamental limitation in tree-based INDP approaches: although it is possible to represent even larger tree/forest tables on the data plane, the flow features that are computable on the data plane are fundamentally limited by hardware constraints. In this paper, we present BoS to push the boundaries of INDP by enabling Neural Network (NN) driven traffic analysis at line-speed. Many types of NNs (such as Recurrent Neural Network (RNN), and transformers) that are designed to work with sequential data have advantages over tree-based models, because they can take raw network data as input without complex feature computations on the fly. However, the challenge is significant: the recurrent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#21033;&#29992;&#22270;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#20114;&#20449;&#24687;&#26497;&#22823;&#21270;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#20102;&#22270;&#32423;&#21644;&#33410;&#28857;&#32423;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.11087</link><description>&lt;p&gt;
&#32467;&#21512;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Incorporating Higher-order Structural Information for Graph Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#21033;&#29992;&#22270;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#20114;&#20449;&#24687;&#26497;&#22823;&#21270;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#20102;&#22270;&#32423;&#21644;&#33410;&#28857;&#32423;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#20855;&#26377;&#28145;&#36828;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#32858;&#31867;&#24037;&#20855;&#23853;&#38706;&#22836;&#35282;&#65292;&#38598;&#25104;&#20102;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#33410;&#28857;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#22270;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#12290;&#26174;&#28982;&#65292;&#20301;&#20110;&#21516;&#19968;&#32858;&#31867;&#20013;&#30340;&#33410;&#28857;&#21487;&#20197;&#24314;&#31435;&#36828;&#36317;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#36890;&#24120;&#24212;&#29992;&#33258;&#30417;&#30563;&#27169;&#22359;&#26469;&#30417;&#25511;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20165;&#20851;&#27880;&#33410;&#28857;&#23646;&#24615;&#32780;&#24573;&#30053;&#22270;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#20102;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#20114;&#20449;&#24687;&#26497;&#22823;&#21270;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#20102;&#22270;&#32423;&#21644;&#33410;&#28857;&#32423;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20803;&#33258;&#30417;&#30563;&#27169;&#22359;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11087v1 Announce Type: new  Abstract: Clustering holds profound significance in data mining. In recent years, graph convolutional network (GCN) has emerged as a powerful tool for deep clustering, integrating both graph structural information and node attributes. However, most existing methods ignore the higher-order structural information of the graph. Evidently, nodes within the same cluster can establish distant connections. Besides, recent deep clustering methods usually apply a self-supervised module to monitor the training process of their model, focusing solely on node attributes without paying attention to graph structure. In this paper, we propose a novel graph clustering network to make full use of graph structural information. To capture the higher-order structural information, we design a graph mutual infomax module, effectively maximizing mutual information between graph-level and node-level representations, and employ a trinary self-supervised module that includ
&lt;/p&gt;</description></item><item><title>RobustSentEmbed&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#25991;&#26412;&#34920;&#31034;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.11082</link><description>&lt;p&gt;
RobustSentEmbed&#65306;&#20351;&#29992;&#23545;&#25239;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#31283;&#20581;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11082
&lt;/p&gt;
&lt;p&gt;
RobustSentEmbed&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#25991;&#26412;&#34920;&#31034;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#22522;&#20110;PLM&#30340;&#34920;&#31034;&#36890;&#24120;&#22312;&#23545;&#25239;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;RobustSentEmbed&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#20197;&#21450;&#23545;&#25239;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#29983;&#25104;&#39640;&#39118;&#38505;&#30340;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#29992;&#20110;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;RobustSentEmbed&#24039;&#22937;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#21644;&#31283;&#20581;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;RobustSentEmbed&#20248;&#20110;&#26368;&#20808;&#36827;&#34920;&#31034;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#38477;&#20302;&#20102;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#38477;&#20302;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11082v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#34701;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#19987;&#23478;&#30693;&#35782;&#30340;&#31616;&#21333;&#27169;&#22411;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#22797;&#26434;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#21363;&#26102;&#32570;&#38519;&#39044;&#27979;&#30340;&#26356;&#22909;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11079</link><description>&lt;p&gt;
&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#21363;&#26102;&#32570;&#38519;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bridging Expert Knowledge with Deep Learning Techniques for Just-In-Time Defect Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11079
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#34701;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#19987;&#23478;&#30693;&#35782;&#30340;&#31616;&#21333;&#27169;&#22411;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#22797;&#26434;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#21363;&#26102;&#32570;&#38519;&#39044;&#27979;&#30340;&#26356;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#26102;&#32570;&#38519;&#39044;&#27979;&#26088;&#22312;&#33258;&#21160;&#39044;&#27979;&#25552;&#20132;&#26159;&#21542;&#23384;&#22312;&#32570;&#38519;&#65292;&#36817;&#24180;&#26469;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;1)&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#25163;&#24037;&#35774;&#35745;&#29305;&#24449;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;2)&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#25552;&#21462;&#25552;&#20132;&#20869;&#23481;&#29305;&#24449;&#30340;&#22797;&#26434;&#27169;&#22411;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#21644;&#35821;&#20041;&#24847;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#34701;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#29305;&#24449;&#32423;&#21035;&#19978;&#21516;&#26102;&#37319;&#29992;&#26089;&#26399;&#34701;&#21512;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11079v1 Announce Type: cross  Abstract: Just-In-Time (JIT) defect prediction aims to automatically predict whether a commit is defective or not, and has been widely studied in recent years. In general, most studies can be classified into two categories: 1) simple models using traditional machine learning classifiers with hand-crafted features, and 2) complex models using deep learning techniques to automatically extract features from commit contents. Hand-crafted features used by simple models are based on expert knowledge but may not fully represent the semantic meaning of the commits. On the other hand, deep learning-based features used by complex models represent the semantic meaning of commits but may not reflect useful expert knowledge. Simple models and complex models seem complementary to each other to some extent. To utilize the advantages of both simple and complex models, we propose a model fusion framework that adopts both early fusions on the feature level and la
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39118;&#38505;&#20013;&#24615;&#31574;&#30053;&#21644;&#21487;&#35843;&#25972;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;CVaR&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.11062</link><description>&lt;p&gt;
&#29992;&#20110;&#25552;&#39640;CVaR&#20248;&#21270;&#26679;&#26412;&#25928;&#29575;&#30340;&#31616;&#21333;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39118;&#38505;&#20013;&#24615;&#31574;&#30053;&#21644;&#21487;&#35843;&#25972;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;CVaR&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;(PG)&#20248;&#21270;&#26465;&#20214;&#20540;&#39118;&#38505;(CVaR)&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#39118;&#38505;&#20013;&#24615;&#31574;&#30053;&#19982;&#21487;&#35843;&#25972;&#31574;&#30053;&#25972;&#21512;&#20026;&#19968;&#20010;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#25152;&#26377;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#37117;&#21487;&#20197;&#29992;&#20110;&#31574;&#30053;&#26356;&#26032;&#65292;&#24182;&#19988;&#36890;&#36807;&#39118;&#38505;&#20013;&#24615;&#32452;&#20214;&#21050;&#28608;&#26356;&#39640;&#30340;&#22238;&#25253;&#65292;&#20174;&#32780;&#25552;&#21319;&#23614;&#37096;&#24182;&#38450;&#27490;&#25153;&#24179;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#28151;&#21512;&#21442;&#25968;&#21270;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11062v1 Announce Type: new  Abstract: Reinforcement learning algorithms utilizing policy gradients (PG) to optimize Conditional Value at Risk (CVaR) face significant challenges with sample inefficiency, hindering their practical applications. This inefficiency stems from two main facts: a focus on tail-end performance that overlooks many sampled trajectories, and the potential of gradient vanishing when the lower tail of the return distribution is overly flat. To address these challenges, we propose a simple mixture policy parameterization. This method integrates a risk-neutral policy with an adjustable policy to form a risk-averse policy. By employing this strategy, all collected trajectories can be utilized for policy updating, and the issue of vanishing gradients is counteracted by stimulating higher returns through the risk-neutral component, thus lifting the tail and preventing flatness. Our empirical study reveals that this mixture parameterization is uniquely effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;JustQ&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;NISQ&#35745;&#31639;&#26426;&#19978;&#37096;&#32626;&#20844;&#24179;&#20934;&#30830;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;NISQ&#38169;&#35823;&#27169;&#22411;&#12289;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#37096;&#32626;&#21644;&#21516;&#26102;&#32771;&#34385;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#28789;&#27963;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;JustQ&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.11048</link><description>&lt;p&gt;
JustQ&#65306;&#20844;&#24179;&#20934;&#30830;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;JustQ&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;NISQ&#35745;&#31639;&#26426;&#19978;&#37096;&#32626;&#20844;&#24179;&#20934;&#30830;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;NISQ&#38169;&#35823;&#27169;&#22411;&#12289;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#37096;&#32626;&#21644;&#21516;&#26102;&#32771;&#34385;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#28789;&#27963;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;JustQ&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#20844;&#24179;&#24615;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#65292;&#22240;&#20026;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#20110;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;QNN&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;QNN&#37096;&#32626;&#21644;&#37327;&#23376;&#22122;&#22768;&#23545;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#36941;&#21382;&#24191;&#38420;&#30340;QNN&#37096;&#32626;&#35774;&#35745;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JustQ&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;NISQ&#35745;&#31639;&#26426;&#19978;&#37096;&#32626;&#20844;&#24179;&#20934;&#30830;QNN&#30340;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#23436;&#25972;&#30340;NISQ&#38169;&#35823;&#27169;&#22411;&#12289;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#37096;&#32626;&#20197;&#21450;&#19968;&#20010;&#28789;&#27963;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#21516;&#26102;&#32771;&#34385;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JustQ&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#26412;&#30740;&#31350;&#24320;&#21019;&#20102;NISQ&#35745;&#31639;&#26426;&#19978;&#20844;&#24179;QNN&#35774;&#35745;&#30340;&#20808;&#27827;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11048v1 Announce Type: cross  Abstract: Despite the success of Quantum Neural Networks (QNNs) in decision-making systems, their fairness remains unexplored, as the focus primarily lies on accuracy. This work conducts a design space exploration, unveiling QNN unfairness, and highlighting the significant influence of QNN deployment and quantum noise on accuracy and fairness. To effectively navigate the vast QNN deployment design space, we propose JustQ, a framework for deploying fair and accurate QNNs on NISQ computers. It includes a complete NISQ error model, reinforcement learning-based deployment, and a flexible optimization objective incorporating both fairness and accuracy. Experimental results show JustQ outperforms previous methods, achieving superior accuracy and fairness. This work pioneers fair QNN design on NISQ computers, paving the way for future investigations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#20449;&#24687;&#38388;&#31454;&#20105;&#65292;&#25552;&#20986;&#21033;&#29992;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#20316;&#20026;&#26377;&#25928;&#20943;&#36731;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#24230;&#35880;&#24910;&#65292;&#38656;&#37325;&#26032;&#35780;&#20272;&#30417;&#31649;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11046</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#38388;&#31454;&#20105;&#35843;&#25511;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Regulating Chatbot Output via Inter-Informational Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#20449;&#24687;&#38388;&#31454;&#20105;&#65292;&#25552;&#20986;&#21033;&#29992;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#20316;&#20026;&#26377;&#25928;&#20943;&#36731;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#24230;&#35880;&#24910;&#65292;&#38656;&#37325;&#26032;&#35780;&#20272;&#30417;&#31649;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#19968;&#24180;&#22810;&#30340;&#30417;&#31649;&#29378;&#28526;&#12290;&#28982;&#32780;&#65292;&#23569;&#25968;&#29616;&#26377;&#30740;&#31350;&#20005;&#26684;&#36136;&#30097;&#20102;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65306;&#22914;&#26524;&#19981;&#32463;&#35268;&#33539;&#65292;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#36755;&#20986;&#20250;&#23545;&#20154;&#31867;&#20107;&#21153;&#36896;&#25104;&#23454;&#36136;&#19988;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#20851;&#38190;&#21487;&#33021;&#24615;&#65292;&#24182;&#22240;&#27492;&#20542;&#21521;&#20110;&#20351;&#29992;&#30417;&#31649;&#24037;&#20855;&#30452;&#25509;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;&#21508;&#31181;&#28192;&#36947;&#20043;&#38388;&#30340;&#20449;&#24687;&#31454;&#20105;&#65292;&#21457;&#23637;&#20102;&#19968;&#20010;&#37325;&#26032;&#35780;&#20272;AI&#30456;&#20851;&#20869;&#23481;&#39118;&#38505;&#21644;&#30456;&#24212;&#30417;&#31649;&#25552;&#35758;&#30340;&#26631;&#20934;&#12290;&#38271;&#36798;&#25968;&#21313;&#24180;&#30340;&#20449;&#24687;&#21644;&#36890;&#20449;&#25216;&#26415;&#30417;&#31649;&#21490;&#34920;&#26126;&#65292;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#20110;&#35880;&#24910;&#65292;&#24182;&#22312;&#25552;&#20986;&#36807;&#24230;&#30340;&#30417;&#31649;&#25514;&#26045;&#26102;&#29359;&#38169;&#35823;&#12290;&#20107;&#23454;&#19978;&#65292;&#20016;&#23500;&#30340;&#32463;&#39564;&#25968;&#25454;&#25903;&#25345;&#20102;&#20449;&#24687;&#24066;&#22330;&#26426;&#21046;&#22312;&#20449;&#24687;&#30417;&#31649;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11046v1 Announce Type: cross  Abstract: The advent of ChatGPT has sparked over a year of regulatory frenzy. However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI chatbot's output would inflict tangible, severe real harm on human affairs. Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly. This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets. The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies. In fact, a trove of empiric
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MTASA&#30340;&#38598;&#25104;&#35745;&#31639;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#22810;&#22788;&#29702;&#24341;&#25806;&#25552;&#39640;&#35745;&#31639;&#36164;&#28304;&#21033;&#29992;&#29575;&#65292;&#35299;&#20915;&#20102;&#35780;&#20272;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30456;&#20284;&#24615;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.11044</link><description>&lt;p&gt;
&#25512;&#36827;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#35780;&#20272;&#65306;&#19968;&#31181;&#38598;&#25104;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advancing multivariate time series similarity assessment: an integrated computational approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11044
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MTASA&#30340;&#38598;&#25104;&#35745;&#31639;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#22810;&#22788;&#29702;&#24341;&#25806;&#25552;&#39640;&#35745;&#31639;&#36164;&#28304;&#21033;&#29992;&#29575;&#65292;&#35299;&#20915;&#20102;&#35780;&#20272;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30456;&#20284;&#24615;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Data mining&#22312;&#25552;&#21462;&#22797;&#26434;&#31995;&#32479;&#35265;&#35299;&#21644;&#25903;&#25345;&#19981;&#21516;&#39046;&#22495;&#30340;&#20915;&#31574;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#32780;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#26512;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#23384;&#22312;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12289;&#35299;&#20915;&#26102;&#38388;&#19978;&#30340;&#38169;&#20301;&#38382;&#39064;&#20197;&#21450;&#38656;&#35201;&#39640;&#25928;&#20840;&#38754;&#30340;&#20998;&#26512;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#23545;&#40784;&#21644;&#30456;&#20284;&#24615;&#35780;&#20272;&#65288;MTASA&#65289;&#30340;&#26032;&#22411;&#38598;&#25104;&#35745;&#31639;&#26041;&#27861;&#12290;MTASA&#24314;&#31435;&#22312;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#23398;&#30340;&#22522;&#30784;&#19978;&#65292;&#26088;&#22312;&#20248;&#21270;&#26102;&#38388;&#24207;&#21015;&#23545;&#40784;&#65292;&#36741;&#20197;&#19968;&#20010;&#25552;&#39640;&#35745;&#31639;&#36164;&#28304;&#21033;&#29992;&#29575;&#30340;&#22810;&#22788;&#29702;&#24341;&#25806;&#12290;&#36825;&#31181;&#38598;&#25104;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#37117;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#35780;&#20272;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11044v1 Announce Type: new  Abstract: Data mining, particularly the analysis of multivariate time series data, plays a crucial role in extracting insights from complex systems and supporting informed decision-making across diverse domains. However, assessing the similarity of multivariate time series data presents several challenges, including dealing with large datasets, addressing temporal misalignments, and the need for efficient and comprehensive analytical frameworks. To address all these challenges, we propose a novel integrated computational approach known as Multivariate Time series Alignment and Similarity Assessment (MTASA). MTASA is built upon a hybrid methodology designed to optimize time series alignment, complemented by a multiprocessing engine that enhances the utilization of computational resources. This integrated approach comprises four key components, each addressing essential aspects of time series similarity assessment, thereby offering a comprehensive f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;FL&#26041;&#27861;FAGH&#65292;&#21033;&#29992;&#36817;&#20284;&#20840;&#23616;Hessian&#21644;&#20840;&#23616;&#26799;&#24230;&#30340;&#19968;&#38454;&#30697;&#26469;&#21152;&#36895;&#20840;&#23616;&#27169;&#22411;&#35757;&#32451;&#65292;&#38477;&#20302;&#36890;&#20449;&#27425;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.11041</link><description>&lt;p&gt;
FAGH&#65306;&#21033;&#29992;&#36817;&#20284;&#20840;&#23616;Hessian&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FAGH: Accelerating Federated Learning with Approximated Global Hessian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;FL&#26041;&#27861;FAGH&#65292;&#21033;&#29992;&#36817;&#20284;&#20840;&#23616;Hessian&#21644;&#20840;&#23616;&#26799;&#24230;&#30340;&#19968;&#38454;&#30697;&#26469;&#21152;&#36895;&#20840;&#23616;&#27169;&#22411;&#35757;&#32451;&#65292;&#38477;&#20302;&#36890;&#20449;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#30001;&#20110;&#20840;&#23616;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#24930;&#32780;&#23548;&#33268;&#30340;&#26174;&#30528;&#36890;&#20449;&#24320;&#38144;&#26500;&#25104;&#20102;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAGH&#30340;FL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#36817;&#20284;&#20840;&#23616;Hessian&#30340;&#19968;&#38454;&#30697;&#21644;&#20840;&#23616;&#26799;&#24230;&#30340;&#19968;&#38454;&#30697;&#26469;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;Hessian&#30340;&#26354;&#29575;&#65292;&#21152;&#36895;&#20840;&#23616;&#27169;&#22411;&#35757;&#32451;&#30340;&#25910;&#25947;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11041v1 Announce Type: new  Abstract: In federated learning (FL), the significant communication overhead due to the slow convergence speed of training the global model poses a great challenge. Specifically, a large number of communication rounds are required to achieve the convergence in FL. One potential solution is to employ the Newton-based optimization method for training, known for its quadratic convergence rate. However, the existing Newton-based FL training methods suffer from either memory inefficiency or high computational costs for local clients or the server. To address this issue, we propose an FL with approximated global Hessian (FAGH) method to accelerate FL training. FAGH leverages the first moment of the approximated global Hessian and the first moment of the global gradient to train the global model. By harnessing the approximated global Hessian curvature, FAGH accelerates the convergence of global model training, leading to the reduced number of communicati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;FH-TabNet&#65292;&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23478;&#26063;&#24615;&#39640;&#32966;&#22266;&#37255;&#34880;&#30151;&#30340;&#22810;&#31867;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.11032</link><description>&lt;p&gt;
FH-TabNet&#65306;&#22810;&#38454;&#27573;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#22810;&#31867;&#23478;&#26063;&#24615;&#39640;&#32966;&#22266;&#37255;&#34880;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
FH-TabNet: Multi-Class Familial Hypercholesterolemia Detection via a Multi-Stage Tabular Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;FH-TabNet&#65292;&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23478;&#26063;&#24615;&#39640;&#32966;&#22266;&#37255;&#34880;&#30151;&#30340;&#22810;&#31867;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23478;&#26063;&#24615;&#39640;&#32966;&#22266;&#37255;&#34880;&#30151;&#65288;FH&#65289;&#26159;&#19968;&#31181;&#36951;&#20256;&#30142;&#30149;&#65292;&#20854;&#29305;&#24449;&#26159;&#20302;&#23494;&#24230;&#33026;&#34507;&#30333;&#65288;LDL&#65289;&#32966;&#22266;&#37255;&#25110;&#20854;&#30456;&#20851;&#22522;&#22240;&#27700;&#24179;&#21319;&#39640;&#12290;&#26089;&#26399;&#21644;&#20934;&#30830;&#20998;&#31867;FH&#23545;&#20110;&#21450;&#26102;&#24178;&#39044;&#20197;&#20943;&#36731;&#21361;&#21450;&#29983;&#21629;&#30340;&#29366;&#20917;&#39118;&#38505;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35786;&#26029;&#26041;&#27861;&#22797;&#26434;&#12289;&#26114;&#36149;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#20020;&#24202;&#21307;&#29983;&#26469;&#35828;&#20063;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35299;&#37322;&#20219;&#21153;&#65292;&#23548;&#33268;&#39640;&#30340;&#28431;&#35786;&#29575;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#23545;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#36827;&#34892;&#26089;&#26399;FH&#26816;&#27979;&#20135;&#29983;&#20102;&#20852;&#36259;&#28608;&#22686;&#65292;&#20294;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20165;&#32771;&#34385;&#20351;&#29992;&#20256;&#32479;ML&#27169;&#22411;&#36827;&#34892;&#20108;&#36827;&#21046;&#20998;&#31867;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#24213;&#23618;&#20020;&#24202;&#25968;&#25454;&#30340;&#20998;&#31867;&#24615;&#36136;&#65292;&#22240;&#27492;&#38024;&#23545;FH&#26816;&#27979;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#30740;&#31350;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;FH-TabNet&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;FH-TabNet&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#34920;&#26684;DL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11032v1 Announce Type: new  Abstract: Familial Hypercholesterolemia (FH) is a genetic disorder characterized by elevated levels of Low-Density Lipoprotein (LDL) cholesterol or its associated genes. Early-stage and accurate categorization of FH is of significance allowing for timely interventions to mitigate the risk of life-threatening conditions. Conventional diagnosis approach, however, is complex, costly, and a challenging interpretation task even for experienced clinicians resulting in high underdiagnosis rates. Although there has been a recent surge of interest in using Machine Learning (ML) models for early FH detection, existing solutions only consider a binary classification task solely using classical ML models. Despite its significance, application of Deep Learning (DL) for FH detection is in its infancy, possibly, due to categorical nature of the underlying clinical data. The paper addresses this gap by introducing the FH-TabNet, which is a multi-stage tabular DL 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31354;&#38388;&#25277;&#35937;&#21152;&#36895;&#26679;&#26412;&#36873;&#25321;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#24182;&#22312;&#24191;&#27867;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.11020</link><description>&lt;p&gt;
&#21033;&#29992;&#31354;&#38388;&#25277;&#35937;&#21152;&#36895;&#26679;&#26412;&#36873;&#25321;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Accelerating prototype selection with spatial abstraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31354;&#38388;&#25277;&#35937;&#21152;&#36895;&#26679;&#26412;&#36873;&#25321;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#24182;&#22312;&#24191;&#27867;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#21644;&#31038;&#20250;&#30340;&#25968;&#23383;&#21270;&#22686;&#38271;&#23548;&#33268;&#20102;&#21487;&#20379;&#22788;&#29702;&#21644;&#21033;&#29992;&#30340;&#25968;&#25454;&#19981;&#26029;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#25968;&#25454;&#38656;&#35201;&#30456;&#24403;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26679;&#26412;&#36873;&#25321;&#25216;&#26415;&#24050;&#32463;&#34987;&#24212;&#29992;&#26469;&#38477;&#20302;&#36825;&#20123;&#25216;&#26415;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#29616;&#26377;&#26679;&#26412;&#36873;&#25321;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#31354;&#38388;&#21010;&#20998;&#30340;&#27010;&#24565;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#25277;&#35937;&#34920;&#31034;&#12290;&#31532;&#20108;&#27493;&#21033;&#29992;&#36825;&#31181;&#25277;&#35937;&#34920;&#31034;&#26377;&#25928;&#22320;&#21098;&#26525;&#25628;&#32034;&#31354;&#38388;&#24182;&#36873;&#25321;&#19968;&#32452;&#20505;&#36873;&#21407;&#22411;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#30340;&#20505;&#36873;&#26679;&#26412;&#21487;&#20197;&#24212;&#29992;&#19968;&#20123;&#20256;&#32479;&#30340;&#26679;&#26412;&#36873;&#25321;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20116;&#31181;&#20256;&#32479;&#30340;&#26679;&#26412;&#36873;&#25321;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;14&#20010;&#24191;&#20026;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11020v1 Announce Type: new  Abstract: The increasing digitalization in industry and society leads to a growing abundance of data available to be processed and exploited. However, the high volume of data requires considerable computational resources for applying machine learning approaches. Prototype selection techniques have been applied to reduce the requirements of computational resources that are needed by these techniques. In this paper, we propose an approach for speeding up existing prototype selection techniques. It builds an abstract representation of the dataset, using the notion of spatial partition. The second step uses this abstract representation to prune the search space efficiently and select a set of candidate prototypes. After, some conventional prototype selection algorithms can be applied to the candidates selected by our approach. Our approach was integrated with five conventional prototype selection algorithms and tested on 14 widely recognized datasets 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20266;&#28857;SPA&#31639;&#27861;&#65292;&#21033;&#29992;&#26497;&#20540;&#29702;&#35770;&#25512;&#23548;&#35823;&#24046;&#30028;&#38480;&#65292;&#30456;&#27604;&#21407;&#22987;SPA&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.11013</link><description>&lt;p&gt;
&#25552;&#21319;&#36830;&#32493;&#25237;&#24433;&#31639;&#27861;&#21644;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Improved Algorithm and Bounds for Successive Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11013
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20266;&#28857;SPA&#31639;&#27861;&#65292;&#21033;&#29992;&#26497;&#20540;&#29702;&#35770;&#25512;&#23548;&#35823;&#24046;&#30028;&#38480;&#65292;&#30456;&#27604;&#21407;&#22987;SPA&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;$d$&#32500;&#31354;&#38388;&#20013;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;$K$&#20010;&#39030;&#28857;&#30340;&#21333;&#32431;&#24418;&#65292;&#20551;&#35774;&#25105;&#20204;&#22312;&#21333;&#32431;&#24418;&#19978;&#27979;&#37327;$n$&#20010;&#24102;&#26377;&#22122;&#22768;&#30340;&#28857;&#65288;&#22240;&#27492;&#65292;&#19968;&#20123;&#35266;&#27979;&#28857;&#33853;&#22312;&#21333;&#32431;&#24418;&#20043;&#22806;&#65289;&#12290;&#39030;&#28857;&#25628;&#32034;&#26159;&#20272;&#35745;&#21333;&#32431;&#24418;&#30340;$K$&#20010;&#39030;&#28857;&#30340;&#38382;&#39064;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#39030;&#28857;&#25628;&#32034;&#31639;&#27861;&#26159;&#36830;&#32493;&#25237;&#24433;&#31639;&#27861;&#65288;SPA&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#22122;&#22768;&#25110;&#24322;&#24120;&#20540;&#19979;&#65292;&#35266;&#23519;&#21040;SPA&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20266;&#28857;SPA&#65288;pp-SPA&#65289;&#12290;&#23427;&#20351;&#29992;&#25237;&#24433;&#27493;&#39588;&#21644;&#21435;&#22122;&#27493;&#39588;&#29983;&#25104;&#20266;&#28857;&#65292;&#24182;&#23558;&#23427;&#20204;&#36755;&#20837;&#21040;SPA&#20013;&#36827;&#34892;&#39030;&#28857;&#25628;&#32034;&#12290;&#25105;&#20204;&#22522;&#20110;&#65288;&#21487;&#33021;&#26159;&#65289;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#26497;&#20540;&#29702;&#35770;&#25512;&#23548;&#20102;pp-SPA&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;pp-SPA&#27604;SPA&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;&#23545;&#21407;&#22987;SPA&#30340;&#25913;&#36827;&#38750;&#28176;&#36817;&#30028;&#38480;&#65292;&#36825;&#20855;&#26377;&#29420;&#31435;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11013v1 Announce Type: new  Abstract: Given a $K$-vertex simplex in a $d$-dimensional space, suppose we measure $n$ points on the simplex with noise (hence, some of the observed points fall outside the simplex). Vertex hunting is the problem of estimating the $K$ vertices of the simplex. A popular vertex hunting algorithm is successive projection algorithm (SPA). However, SPA is observed to perform unsatisfactorily under strong noise or outliers. We propose pseudo-point SPA (pp-SPA). It uses a projection step and a denoise step to generate pseudo-points and feed them into SPA for vertex hunting. We derive error bounds for pp-SPA, leveraging on extreme value theory of (possibly) high-dimensional random vectors. The results suggest that pp-SPA has faster rates and better numerical performances than SPA. Our analysis includes an improved non-asymptotic bound for the original SPA, which is of independent interest.
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20294;&#20854;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#21069;&#21521;&#27491;&#21521;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11004</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Forward Learning of Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11004
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20294;&#20854;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#21069;&#21521;&#27491;&#21521;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#38382;&#31572;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#22312;GNNs&#30340;&#25104;&#21151;&#32972;&#21518;&#65292;&#26159;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;BP&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#36824;&#26159;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#19981;&#20165;&#22312;&#29983;&#29289;&#19978;&#19981;&#21512;&#29702;&#65292;&#32780;&#19988;&#38480;&#21046;&#20102;&#23398;&#20064;NNs&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#24182;&#34892;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26368;&#36817;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#25552;&#20986;&#20102;&#21069;&#21521;&#27491;&#21521;&#65288;FF&#65289;&#31639;&#27861;&#20316;&#20026;BP&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27491;&#36127;&#25968;&#25454;&#19978;&#25191;&#34892;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#26469;&#35757;&#32451;NNs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11004v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#25299;&#25169;&#20445;&#30495;&#22810;&#31867;&#21035;&#20998;&#21106;&#30340;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23558;N&#31867;&#21035;&#20998;&#21106;&#38382;&#39064;&#20998;&#35299;&#20026;N&#20010;&#21333;&#31867;&#21035;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#39564;&#35777;&#20102;&#22312;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.11001</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#25299;&#25169;&#20445;&#30495;&#30340;&#22810;&#31867;&#21035;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Topologically faithful multi-class segmentation in medical images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11001
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#25299;&#25169;&#20445;&#30495;&#22810;&#31867;&#21035;&#20998;&#21106;&#30340;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23558;N&#31867;&#21035;&#20998;&#21106;&#38382;&#39064;&#20998;&#35299;&#20026;N&#20010;&#21333;&#31867;&#21035;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#39564;&#35777;&#20102;&#22312;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#25299;&#25169;&#31934;&#24230;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#23646;&#24615;&#65292;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#22914;&#32593;&#32476;&#20998;&#26512;&#21644;&#34880;&#31649;&#25110;&#32454;&#32990;&#35745;&#25968;&#20013;&#30340;&#27969;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#37325;&#35201;&#30340;&#26041;&#27861;&#35770;&#36827;&#27493;&#23558;&#20195;&#25968;&#25299;&#25169;&#20013;&#25166;&#23454;&#30340;&#27010;&#24565;&#24102;&#21040;&#20102;&#20108;&#20540;&#20998;&#21106;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#31867;&#21035;&#20998;&#21106;&#22330;&#26223;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#25299;&#25169;&#38169;&#35823;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25299;&#25169;&#20445;&#30495;&#30340;&#22810;&#31867;&#21035;&#20998;&#21106;&#65292;&#25193;&#23637;&#20102;&#26368;&#36817;&#22522;&#20110;&#25345;&#20037;&#26465;&#30721;&#30340;Betti&#21305;&#37197;&#27010;&#24565;&#12290;&#25105;&#20204;&#23558;N&#31867;&#21035;&#20998;&#21106;&#38382;&#39064;&#25237;&#24433;&#21040;N&#20010;&#21333;&#31867;&#21035;&#20998;&#21106;&#20219;&#21153;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#19968;&#21442;&#25968;&#25345;&#20037;&#21516;&#35843;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#21253;&#21547;&#39640;&#24230;&#19981;&#21516;&#25299;&#25169;&#29305;&#24449;&#30340;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11001v1 Announce Type: cross  Abstract: Topological accuracy in medical image segmentation is a highly important property for downstream applications such as network analysis and flow modeling in vessels or cell counting. Recently, significant methodological advancements have brought well-founded concepts from algebraic topology to binary segmentation. However, these approaches have been underexplored in multi-class segmentation scenarios, where topological errors are common. We propose a general loss function for topologically faithful multi-class segmentation extending the recent Betti matching concept, which is based on induced matchings of persistence barcodes. We project the N-class segmentation problem to N single-class segmentation tasks, which allows us to use 1-parameter persistent homology making training of neural networks computationally feasible. We validate our method on a comprehensive set of four medical datasets with highly variant topological characteristic
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.10997</link><description>&lt;p&gt;
&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#23618;&#27425;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10997
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29702;&#35299;&#22810;&#23618;&#25277;&#35937;&#30340;&#22797;&#26434;&#22330;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330; (N2F2)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#30417;&#30563;&#26469;&#23398;&#20064;&#21333;&#20010;&#29305;&#24449;&#22330;&#65292;&#22312;&#21516;&#19968;&#39640;&#32500;&#29305;&#24449;&#20013;&#30340;&#19981;&#21516;&#32500;&#24230;&#32534;&#30721;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#28789;&#27963;&#23450;&#20041;&#23618;&#27425;&#65292;&#21487;&#20197;&#26681;&#25454;&#29289;&#29702;&#32500;&#24230;&#12289;&#35821;&#20041;&#32500;&#24230;&#25110;&#20004;&#32773;&#22343;&#21305;&#37197;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22330;&#26223;&#30340;&#20840;&#38754;&#21644;&#32454;&#33268;&#29702;&#35299;&#12290;&#25105;&#20204;&#21033;&#29992;2D&#31867;&#21035;&#26080;&#20851;&#20998;&#21106;&#27169;&#22411;&#22312;&#22270;&#20687;&#31354;&#38388;&#30340;&#20219;&#24847;&#23610;&#24230;&#25552;&#20379;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20687;&#32032;&#20998;&#32452;&#65292;&#24182;&#26597;&#35810;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#20026;&#36825;&#20123;&#27573;&#33853;&#20013;&#30340;&#27599;&#20010;&#37096;&#20998;&#33719;&#24471;&#19982;&#35821;&#35328;&#23545;&#40784;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#23618;&#30417;&#30563;&#26041;&#27861;&#23558;&#19981;&#21516;&#30340;&#23884;&#22871;&#29305;&#24449;&#22330;&#32500;&#24230;&#20998;&#37197;&#32473;&#25552;&#21462;C
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10997v1 Announce Type: cross  Abstract: Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the C
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10996</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#25345;&#32493;Sim2Real&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#25353;&#38656;&#25193;&#23637;&#24182;&#34892;&#21270;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#21033;&#29992;&#26368;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#21551;&#21160;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#36716;&#31227;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#20102;4&#21488;&#21512;&#20316;&#36710;&#36742;(Nigel)&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#30340;&#20132;&#21449;&#36941;&#21382;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#30740;&#31350;&#20102;2&#36742;&#36710;(F1TENTH)&#30340;&#23545;&#25239;&#24615;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#20801;&#35768;&#23545;&#31574;&#30053;&#36827;&#34892;&#26377;&#21147;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10996v1 Announce Type: cross  Abstract: This work presents a sustainable multi-agent deep reinforcement learning framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent reinforcement learning policies from simulation to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;GNN&#35757;&#32451;&#31639;&#27861;Eclipse&#65292;&#36890;&#36807;&#35266;&#23519;&#22270;&#32467;&#26500;&#20013;&#37051;&#25509;&#30697;&#38453;&#30340;&#20302;&#31209;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25252;&#36793;&#32536;&#38544;&#31169;&#30340;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33391;&#22909;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10995</link><description>&lt;p&gt;
&#20855;&#26377;&#22855;&#24322;&#20540;&#25200;&#21160;&#30340;&#36793;&#32536;&#31169;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Edge Private Graph Neural Networks with Singular Value Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;GNN&#35757;&#32451;&#31639;&#27861;Eclipse&#65292;&#36890;&#36807;&#35266;&#23519;&#22270;&#32467;&#26500;&#20013;&#37051;&#25509;&#30697;&#38453;&#30340;&#20302;&#31209;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25252;&#36793;&#32536;&#38544;&#31169;&#30340;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33391;&#22909;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#20174;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;GNN&#35757;&#32451;&#27969;&#31243;&#24050;&#34987;&#26174;&#31034;&#23481;&#26131;&#21463;&#21040;&#33410;&#28857;&#29305;&#24449;&#27844;&#28431;&#21644;&#36793;&#25552;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24773;&#26223;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#20174;&#35757;&#32451;&#36807;&#30340;GNN&#27169;&#22411;&#20013;&#24674;&#22797;&#31169;&#26377;&#36793;&#32536;&#20449;&#24687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30452;&#25509;&#21521;&#37051;&#25509;&#30697;&#38453;&#25110;&#32039;&#20945;&#22270;&#34920;&#31034;&#28155;&#21152;&#22122;&#22768;&#12290;&#28155;&#21152;&#30340;&#25200;&#21160;&#23548;&#33268;&#22270;&#32467;&#26500;&#34987;&#22823;&#24133;&#25913;&#21464;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;GNN&#35757;&#32451;&#31639;&#27861;Eclipse&#65292;&#35813;&#31639;&#27861;&#22312;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;Eclipse&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#12290;&#31532;&#19968;&#65292;&#22270;&#32467;&#26500;&#20013;&#30340;&#37051;&#25509;&#30697;&#38453;&#34920;&#29616;&#20986;&#20302;&#31209;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;Eclipse&#20351;&#29992;&#20302;&#31209;&#30340;&#26041;&#24335;&#35757;&#32451;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10995v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) play a key role in learning representations from graph-structured data and are demonstrated to be useful in many applications. However, the GNN training pipeline has been shown to be vulnerable to node feature leakage and edge extraction attacks. This paper investigates a scenario where an attacker aims to recover private edge information from a trained GNN model. Previous studies have employed differential privacy (DP) to add noise directly to the adjacency matrix or a compact graph representation. The added perturbations cause the graph structure to be substantially morphed, reducing the model utility. We propose a new privacy-preserving GNN training algorithm, Eclipse, that maintains good model utility while providing strong privacy protection on edges. Eclipse is based on two key observations. First, adjacency matrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains GNNs with a low-r
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\carb &#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;-&#21551;&#29992;&#28145;&#24230;&#23398;&#20064;&#20013;&#31934;&#30830;&#20272;&#31639;&#30899;&#36275;&#36857;&#65292;&#23637;&#31034;&#20102;&#19982;&#23454;&#38469;&#27979;&#37327;&#20540;&#30456;&#27604;&#26368;&#22823;$\pm21\%$&#30340;&#30899;&#36275;&#36857;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.10984</link><description>&lt;p&gt;
IoTCO2&#65306;&#35780;&#20272;&#29289;&#32852;&#32593;-&#21551;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10984
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\carb &#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;-&#21551;&#29992;&#28145;&#24230;&#23398;&#20064;&#20013;&#31934;&#30830;&#20272;&#31639;&#30899;&#36275;&#36857;&#65292;&#23637;&#31034;&#20102;&#19982;&#23454;&#38469;&#27979;&#37327;&#20540;&#30456;&#27604;&#26368;&#22823;$\pm21\%$&#30340;&#30899;&#36275;&#36857;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#38544;&#31169;&#24615;&#21644;&#30830;&#20445;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#19978;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#65292;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#19982;IoT&#19978;DL&#30456;&#20851;&#30340;&#30899;&#36275;&#36857;&#65292;&#28085;&#30422;&#20102;&#25805;&#20316;&#21644;&#23454;&#20307;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#25805;&#20316;&#33021;&#37327;&#39044;&#27979;&#22120;&#32463;&#24120;&#24573;&#30053;&#20102;&#37327;&#21270;&#30340;DL&#27169;&#22411;&#21644;&#26032;&#20852;&#30340;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#65288;NPUs&#65289;&#65292;&#32780;&#23454;&#20307;&#30899;&#36275;&#36857;&#24314;&#27169;&#24037;&#20855;&#24573;&#30053;&#20102;IoT&#35774;&#22791;&#20013;&#24120;&#35265;&#30340;&#38750;&#35745;&#31639;&#30828;&#20214;&#32452;&#20214;&#65292;&#23548;&#33268;&#20102;&#29289;&#32852;&#32593;DL&#20934;&#30830;&#30899;&#36275;&#36857;&#24314;&#27169;&#24037;&#20855;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;\textit{\carb}&#65292;&#19968;&#31181;&#29992;&#20110;&#31934;&#30830;&#20272;&#31639;&#29289;&#32852;&#32593;DL&#20013;&#30899;&#36275;&#36857;&#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#24037;&#20855;&#65292;&#23637;&#31034;&#20102;&#19982;&#21508;&#31181;DL&#27169;&#22411;&#30340;&#23454;&#38469;&#27979;&#37327;&#20540;&#30456;&#27604;&#26368;&#22823;$\pm21\%$&#30340;&#30899;&#36275;&#36857;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;\carb&#30340;&#23454;&#38469;&#24212;&#29992;&#36890;&#36807;&#22810;&#20010;&#29992;&#25143;&#26696;&#20363;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10984v1 Announce Type: cross  Abstract: To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook quantized DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \textit{\carb}, an end-to-end modeling tool for precise carbon footprint estimation in IoT-enabled DL, demonstrating a maximum $\pm21\%$ deviation in carbon footprint values compared to actual measurements across various DL models. Additionally, practical applications of \carb are showcased through multiple user case studies.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38598;&#20307;&#26234;&#24935;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#22686;&#24378;&#29289;&#32852;&#32593;&#23545;&#25239;DDoS&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10968</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22686;&#24378;&#23545;&#25239; DDoS &#25915;&#20987;&#30340;&#29289;&#32852;&#32593;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Enhancing IoT Security Against DDoS Attacks through Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10968
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38598;&#20307;&#26234;&#24935;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#22686;&#24378;&#29289;&#32852;&#32593;&#23545;&#25239;DDoS&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#36805;&#36895;&#26222;&#21450;&#24341;&#20837;&#20102;&#29289;&#29702;&#35774;&#22791;&#19982;&#25968;&#23383;&#19990;&#30028;&#20043;&#38388;&#30340;&#21464;&#38761;&#24615;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#19981;&#26029;&#21319;&#32423;&#65292;&#21361;&#21450;&#29289;&#32852;&#32593;&#32593;&#32476;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#20256;&#32479;&#30340;DDoS&#32531;&#35299;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#33021; compromise &#25968;&#25454;&#38544;&#31169;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#22686;&#24378;&#29289;&#32852;&#32593;&#32593;&#32476;&#23545;&#25239;DDoS&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#65292;&#20801;&#35768;&#22810;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#25110;&#36793;&#32536;&#33410;&#28857;&#21327;&#20316;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#32852;&#37030;&#23398;&#20064;&#22312;&#26816;&#27979;&#21644;&#32531;&#35299;&#29289;&#32852;&#32593;&#20013;DDoS&#25915;&#20987;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38598;&#20307;&#26234;&#24935;&#36827;&#34892;&#23454;&#26102;&#25915;&#20987;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10968v1 Announce Type: cross  Abstract: The rapid proliferation of the Internet of Things (IoT) has ushered in transformative connectivity between physical devices and the digital realm. Nonetheless, the escalating threat of Distributed Denial of Service (DDoS) attacks jeopardizes the integrity and reliability of IoT networks. Conventional DDoS mitigation approaches are ill-equipped to handle the intricacies of IoT ecosystems, potentially compromising data privacy. This paper introduces an innovative strategy to bolster the security of IoT networks against DDoS attacks by harnessing the power of Federated Learning that allows multiple IoT devices or edge nodes to collaboratively build a global model while preserving data privacy and minimizing communication overhead. The research aims to investigate Federated Learning's effectiveness in detecting and mitigating DDoS attacks in IoT. Our proposed framework leverages IoT devices' collective intelligence for real-time attack det
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#19979;&#30340;&#38646;&#26679;&#28857;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10967</link><description>&lt;p&gt;
&#26790;&#24819;&#20013;&#30340;&#35768;&#22810;&#19990;&#30028;&#65306;&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#38646;&#26679;&#28857;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10967
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#19979;&#30340;&#38646;&#26679;&#28857;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#28857;&#27867;&#21270;&#65288;Zero-shot generalization&#65292;ZSG&#65289;&#21040;&#26410;&#35265;&#36807;&#30340;&#21160;&#24577;&#23545;&#20110;&#21019;&#24314;&#20855;&#26377;&#26222;&#36941;&#33021;&#21147;&#30340;&#20307;&#31995;&#20195;&#29702;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;contextual reinforcement learning&#65292;cRL&#65289;&#30340;&#31616;&#21333;&#35774;&#32622;&#24320;&#22987;&#65292;&#20551;&#35774;&#21487;&#35266;&#23519;&#21040;&#21442;&#25968;&#21270;&#31995;&#32479;&#21160;&#24577;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#20540;&#65292;&#22914;&#26426;&#22120;&#20154;&#30340;&#36136;&#37327;&#25110;&#23610;&#23544;&#65292;&#32780;&#19981;&#23545;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#30340;&#21487;&#35266;&#23519;&#24615;&#20570;&#36827;&#19968;&#27493;&#31616;&#21270;&#20551;&#35774;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;ZSG&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24490;&#29615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;contextual recurrent state-space model&#65292;cRSSM&#65289;&#65292;&#23427;&#23545;Dreamer&#65288;v3&#65289;&#65288;Hafner&#31561;&#20154;&#65292;2023&#24180;&#65289;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20351;&#24471;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#34701;&#20837;&#19978;&#19979;&#25991;&#20197;&#20174;&#35266;&#23519;&#20013;&#25512;&#26029;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24182;&#24314;&#27169;&#28508;&#22312;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#31995;&#32479;&#24615;&#22320;&#23558;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#20013;&#25552;&#39640;&#20102;&#22312;&#8220;&#26790;&#22659;&#8221;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;ZSG&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10967v1 Announce Type: cross  Abstract: Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ``dreams
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20854;&#26174;&#33879;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.10961</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#21450;&#20854;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models with Applications to Speech and Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10961
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20854;&#26174;&#33879;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20063;&#31216;&#20026;&#38543;&#26426;&#22330;&#21644;&#26080;&#21521;&#22270;&#27169;&#22411;&#12290;EBMs&#26159;&#38750;&#35268;&#33539;&#21270;&#30340;&#65292;&#22240;&#27492;&#19982;&#20854;&#20182;&#27969;&#34892;&#30340;&#33258;&#24402;&#19968;&#21270;&#27010;&#29575;&#27169;&#22411;&#65288;&#22914;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMMs&#65289;&#12289;&#33258;&#22238;&#24402;&#27169;&#22411;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#26377;&#26681;&#26412;&#30340;&#19981;&#21516;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#37325;&#35201;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#36827;&#23637;&#65292;EBMs&#19981;&#20165;&#21560;&#24341;&#20102;&#26680;&#24515;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#65292;&#36824;&#21560;&#24341;&#20102;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#35821;&#38899;&#12289;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#65289;&#30340;&#20852;&#36259;&#12290;&#35821;&#38899;&#21644;&#35821;&#35328;&#30340;&#39034;&#24207;&#24615;&#36136;&#20063;&#25552;&#20379;&#20102;&#29305;&#27530;&#25361;&#25112;&#65292;&#38656;&#35201;&#19982;&#22788;&#29702;&#22266;&#23450;&#32500;&#24230;&#25968;&#25454;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#26377;&#25152;&#19981;&#21516;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#19987;&#33879;&#26088;&#22312;&#31995;&#32479;&#20171;&#32461;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#31639;&#27861;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10961v1 Announce Type: cross  Abstract: Energy-Based Models (EBMs) are an important class of probabilistic models, also known as random fields and undirected graphical models. EBMs are un-normalized and thus radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets (GANs) and variational auto-encoders (VAEs). Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as speech, vision, natural language processing (NLP) and so on, due to significant theoretical and algorithmic progress. The sequential nature of speech and language also presents special challenges and needs a different treatment from processing fix-dimensional data (e.g., images). Therefore, the purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.10949</link><description>&lt;p&gt;
SelfIE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SelfIE: Self-Interpretation of Large Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#33719;&#24471;&#31572;&#26696;&#65311;&#35299;&#37322;&#21644;&#25511;&#21046;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#26410;&#26469;&#27169;&#22411;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SelfIE&#65288;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;LLMs&#21709;&#24212;&#20851;&#20110;&#32473;&#23450;&#27573;&#33853;&#30340;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#33258;&#24049;&#30340;&#23884;&#20837;&#12290;SelfIE&#33021;&#22815;&#35299;&#37322;&#38544;&#34255;&#23884;&#20837;&#20013;&#30340;&#24320;&#25918;&#19990;&#30028;&#27010;&#24565;&#65292;&#22312;&#26696;&#20363;&#20013;&#25581;&#31034;LLM&#30340;&#20869;&#37096;&#25512;&#29702;&#65292;&#22914;&#20570;&#20986;&#36947;&#24503;&#20915;&#31574;&#12289;&#20869;&#21270;&#25552;&#31034;&#27880;&#20837;&#21644;&#22238;&#24819;&#26377;&#23475;&#30693;&#35782;&#12290;SelfIE&#23545;&#38544;&#34255;&#23884;&#20837;&#30340;&#25991;&#26412;&#25551;&#36848;&#20063;&#24320;&#36767;&#20102;&#25511;&#21046;LLM&#25512;&#29702;&#30340;&#26032;&#36884;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#25511;&#21046;&#65292;&#23427;&#20801;&#35768;&#32534;&#36753;&#24320;&#25918;&#24335;&#27010;&#24565;&#65292;&#32780;&#21482;&#38656;&#35201;&#35745;&#31639;&#21333;&#20010;&#23618;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23558;RLHF&#25193;&#23637;&#21040;&#38544;&#34255;&#30340;&#23884;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#24378;&#21270;&#25511;&#21046;&#26469;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#24207;&#21015;&#20219;&#21153;&#35774;&#32622;&#19979;&#26368;&#23567;&#21270;&#23616;&#37096;&#36951;&#25022;&#30340;&#35884;&#35823;&#65292;&#25581;&#31034;&#20102;&#36817;&#35270;&#22320;&#26368;&#23567;&#21270;&#36951;&#25022;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10946</link><description>&lt;p&gt;
&#22312;&#24207;&#21015;&#20219;&#21153;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#23616;&#37096;&#36951;&#25022;&#30340;&#35884;&#35823;
&lt;/p&gt;
&lt;p&gt;
The Fallacy of Minimizing Local Regret in the Sequential Task Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10946
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#24207;&#21015;&#20219;&#21153;&#35774;&#32622;&#19979;&#26368;&#23567;&#21270;&#23616;&#37096;&#36951;&#25022;&#30340;&#35884;&#35823;&#65292;&#25581;&#31034;&#20102;&#36817;&#35270;&#22320;&#26368;&#23567;&#21270;&#36951;&#25022;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#32463;&#24120;&#34987;&#27010;&#24565;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#31639;&#27861;&#19982;&#26410;&#30693;&#29615;&#22659;&#20132;&#20114;&#20197;&#26368;&#23567;&#21270;&#32047;&#31215;&#36951;&#25022;&#12290;&#22312;&#38745;&#24577;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#22914;&#27425;&#32447;&#24615;&#65288;$\sqrt{T}$&#65289;&#36951;&#25022;&#30028;&#38480;&#65292;&#36890;&#24120;&#24847;&#21619;&#30528;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#24182;&#20572;&#27490;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29702;&#35770;&#35774;&#32622;&#36890;&#24120;&#36807;&#20998;&#31616;&#21270;&#20102;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20013;&#36935;&#21040;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#20219;&#21153;&#25353;&#39034;&#24207;&#21040;&#36798;&#65292;&#20219;&#21153;&#20043;&#38388;&#26377;&#37325;&#22823;&#21464;&#21270;&#65292;&#24182;&#19988;&#31639;&#27861;&#21487;&#33021;&#19981;&#20801;&#35768;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#36229;&#20986;&#32467;&#26524;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#28085;&#30422;&#22870;&#21169;&#35774;&#35745;&#65288;&#20174;&#32467;&#26524;&#21040;&#22870;&#21169;&#30340;&#26144;&#23556;&#65289;&#21644;&#20801;&#35768;&#30340;&#31574;&#30053;&#31354;&#38388;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#36817;&#35270;&#22320;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#35884;&#35823;&#65306;&#33719;&#24471;&#26368;&#20248;&#36951;&#25022;r
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10946v1 Announce Type: cross  Abstract: In the realm of Reinforcement Learning (RL), online RL is often conceptualized as an optimization problem, where an algorithm interacts with an unknown environment to minimize cumulative regret. In a stationary setting, strong theoretical guarantees, like a sublinear ($\sqrt{T}$) regret bound, can be obtained, which typically implies the convergence to an optimal policy and the cessation of exploration. However, these theoretical setups often oversimplify the complexities encountered in real-world RL implementations, where tasks arrive sequentially with substantial changes between tasks and the algorithm may not be allowed to adaptively learn within certain tasks. We study the changes beyond the outcome distributions, encompassing changes in the reward designs (mappings from outcomes to rewards) and the permissible policy spaces. Our results reveal the fallacy of myopically minimizing regret within each task: obtaining optimal regret r
&lt;/p&gt;</description></item><item><title>ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10940</link><description>&lt;p&gt;
ViSaRL&#65306;&#21463;&#20154;&#31867;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ViSaRL: Visual Reinforcement Learning Guided by Human Saliency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10940
&lt;/p&gt;
&lt;p&gt;
ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#39640;&#32500;&#20687;&#32032;&#36755;&#20837;&#22521;&#35757;&#26426;&#22120;&#20154;&#25191;&#34892;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22270;&#20687;&#35266;&#23519;&#20027;&#35201;&#30001;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#32452;&#25104;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#33021;&#22815;&#22312;&#35270;&#35273;&#19978;&#20851;&#27880;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;ViSaRL&#65289;&#12290;&#20351;&#29992;ViSaRL&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26174;&#30528;&#25552;&#39640;&#20102;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;DeepMind&#25511;&#21046;&#22522;&#20934;&#12289;&#20223;&#30495;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26174;&#33879;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#32534;&#30721;&#22120;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20351;&#29992;ViSaRL&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#21508;&#31181;&#35270;&#35273;&#25200;&#21160;&#65292;&#21253;&#25324;&#24863;&#30693;&#22122;&#22768;&#21644;&#22330;&#26223;&#21464;&#21270;&#65292;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;ViSaRL&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#25104;&#21151;&#29575;&#20960;&#20046;&#32763;&#20102;&#19968;&#30058;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10940v1 Announce Type: cross  Abstract: Training robots to perform complex control tasks from high-dimensional pixel input using reinforcement learning (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement Learning (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control benchmark, robot manipulation in simulation and on a real robot. We present approaches for incorporating saliency into both CNN and Transformer-based encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#23567;&#21270;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#35299;&#30721;&#65292;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;ASR&#20013;&#26230;&#26684;&#37325;&#26032;&#25171;&#20998;&#30340;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20943;&#23569;&#20102;&#27888;&#21346;&#22266;&#35821;21.8%&#21644;&#21345;&#32435;&#36798;&#35821;41.8%&#30340;&#35789;&#35823;&#24046;&#29575;&#65292;&#21516;&#26102;&#20165;&#28040;&#32791;1/8&#20869;&#23384;&#12290;</title><link>https://arxiv.org/abs/2403.10937</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21270;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#35299;&#30721;&#20197;&#25913;&#36827;&#20302;&#36164;&#28304;ASR&#20013;&#30340;&#26230;&#26684;&#37325;&#26032;&#25171;&#20998;
&lt;/p&gt;
&lt;p&gt;
Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10937
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21270;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#35299;&#30721;&#65292;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;ASR&#20013;&#26230;&#26684;&#37325;&#26032;&#25171;&#20998;&#30340;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20943;&#23569;&#20102;&#27888;&#21346;&#22266;&#35821;21.8%&#21644;&#21345;&#32435;&#36798;&#35821;41.8%&#30340;&#35789;&#35823;&#24046;&#29575;&#65292;&#21516;&#26102;&#20165;&#28040;&#32791;1/8&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22522;&#32447;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#20840;&#38754;&#26230;&#26684;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36890;&#36807;&#26230;&#26684;&#37325;&#26032;&#25171;&#20998;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22522;&#32447;&#35821;&#35328;&#27169;&#22411;&#26368;&#23567;&#31243;&#24230;&#22320;&#22686;&#24378;&#20026;&#30446;&#26631;&#35821;&#35328;&#26356;&#22823;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#20986;&#29616;&#20294;&#22522;&#32447;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#30340;&#35789;&#30340;&#35789;&#39057;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#22686;&#24378;&#30340;&#22522;&#32447;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#29983;&#25104;&#30340;&#26230;&#26684;&#26356;&#21152;&#20840;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#35789;&#35823;&#24046;&#29575;&#30456;&#23545;&#20943;&#23569;&#20102;21.8%&#65288;&#27888;&#21346;&#22266;&#35821;&#65289;&#21644;41.8%&#65288;&#21345;&#32435;&#36798;&#35821;&#65289;&#12290;&#19982;&#20351;&#29992;&#23436;&#25972;&#32500;&#22522;&#30334;&#31185;&#25991;&#26412;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#35789;&#35823;&#24046;&#29575;&#26041;&#38754;&#30456;&#24403;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#28040;&#32791;1/8&#30340;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10937v1 Announce Type: cross  Abstract: This paper addresses the problem of improving speech recognition accuracy with lattice rescoring in low-resource languages where the baseline language model is insufficient for generating inclusive lattices. We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline. The lattices generated after decoding with such an augmented baseline language model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method. This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory. We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24207;&#21015;&#23398;&#20064;&#20013;&#26377;&#25928;&#25972;&#21512;&#26032;&#25968;&#25454;&#24182;&#20445;&#30041;&#20808;&#21069;&#30693;&#35782;&#65292;&#21516;&#26102;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21512;&#24182;&#26032;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.10929</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#21442;&#25968;&#21270;&#29992;&#20110;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Function-space Parameterization of Neural Networks for Sequential Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10929
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24207;&#21015;&#23398;&#20064;&#20013;&#26377;&#25928;&#25972;&#21512;&#26032;&#25968;&#25454;&#24182;&#20445;&#30041;&#20808;&#21069;&#30693;&#35782;&#65292;&#21516;&#26102;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21512;&#24182;&#26032;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#26799;&#24230;&#19979;&#38477;&#28145;&#24230;&#23398;&#20064;&#20013;&#38590;&#20197;&#25972;&#21512;&#26032;&#25968;&#25454;&#24182;&#20445;&#30041;&#20808;&#21069;&#30693;&#35782;&#65292;&#39034;&#24207;&#23398;&#20064;&#33539;&#24335;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#34429;&#28982;&#39640;&#26031;&#36807;&#31243;&#20248;&#38597;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#22312;&#22788;&#29702;&#35832;&#22914;&#22270;&#20687;&#20043;&#31867;&#30340;&#20016;&#23500;&#36755;&#20837;&#21644;&#20280;&#32553;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#32593;&#32476;&#20174;&#26435;&#37325;&#31354;&#38388;&#36716;&#25442;&#21040;&#20989;&#25968;&#31354;&#38388;&#30340;&#25216;&#26415;&#65292;&#21363;&#21452;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#21442;&#25968;&#21270;&#25552;&#20379;&#20102;&#65306;(i) &#36890;&#36807;&#31232;&#30095;&#21270;&#23558;&#20989;&#25968;&#31354;&#38388;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#25968;&#25454;&#38598;&#30340;&#36884;&#24452;&#65292;(ii) &#22312;&#35775;&#38382;&#36807;&#21435;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#20808;&#21069;&#30693;&#35782;&#65292;&#20197;&#21450;(iii) &#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21512;&#24182;&#26032;&#25968;&#25454;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#20445;&#30041;&#30693;&#35782;&#65292;&#24182;&#26377;&#25928;&#22320;&#21512;&#24182;&#26032;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#24341;&#23548;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#20013;&#25506;&#32034;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10929v1 Announce Type: cross  Abstract: Sequential learning paradigms pose challenges for gradient-based deep learning due to difficulties incorporating new data and retaining prior knowledge. While Gaussian processes elegantly tackle these problems, they struggle with scalability and handling rich inputs, such as images. To address these issues, we introduce a technique that converts neural networks from weight space to function space, through a dual parameterization. Our parameterization offers: (i) a way to scale function-space methods to large data sets via sparsification, (ii) retention of prior knowledge when access to past data is limited, and (iii) a mechanism to incorporate new data without retraining. Our experiments demonstrate that we can retain knowledge in continual learning and incorporate new data efficiently. We further show its strengths in uncertainty quantification and guiding exploration in model-based RL. Further information and code is available on the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#31354;&#22320;&#21327;&#21516; MEC &#30340;&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#21160;&#24577;&#21368;&#36733;&#35843;&#24230;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;MORL&#21644;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;n&#27493;&#22238;&#25253;&#26469;&#22788;&#29702;&#31215;&#21387;&#20013;&#30340;&#27874;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.10927</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#21160;&#24577;&#21368;&#36733;&#35843;&#24230;&#29992;&#20110;&#31354;&#22320;&#21327;&#21516; MEC
&lt;/p&gt;
&lt;p&gt;
Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground Cooperative MEC
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10927
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#31354;&#22320;&#21327;&#21516; MEC &#30340;&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#21160;&#24577;&#21368;&#36733;&#35843;&#24230;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;MORL&#21644;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;n&#27493;&#22238;&#25253;&#26469;&#22788;&#29702;&#31215;&#21387;&#20013;&#30340;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#19982;&#36793;&#32536;&#26381;&#21153;&#22120;&#21327;&#21161;&#22320;&#38754;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30830;&#23450;&#24615;&#20248;&#21270;&#25110;&#21333;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#26080;&#27861;&#22312;&#39640;&#24230;&#21160;&#24577;&#32593;&#32476;&#29615;&#22659;&#20013;&#20943;&#23569;&#20219;&#21153;&#27604;&#29305;&#30340;&#31215;&#21387;&#65292;&#24182;&#21516;&#26102;&#25552;&#39640;&#33021;&#25928;&#65292;&#35774;&#35745;&#38382;&#39064;&#31561;&#21516;&#20110;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#19988;&#22788;&#29702;&#22686;&#21152;&#30340;&#22320;&#38754;&#29992;&#25143;&#25968;&#37327;&#24341;&#20837;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#65288;MO&#65289;&#21160;&#24577;&#36712;&#36857;&#35268;&#21010;&#21644;&#21368;&#36733;&#35843;&#24230;&#26041;&#26696;&#65292;&#25972;&#21512;&#20102;MORL&#21644;&#26680;&#26041;&#27861;&#12290;n&#27493;&#22238;&#25253;&#30340;&#35774;&#35745;&#20063;&#34987;&#24212;&#29992;&#20110;&#20943;&#23569;&#31215;&#21387;&#20013;&#30340;&#27874;&#21160;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;n&#27493;&#22238;&#25253;&#21487;&#20197;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#26680;&#26041;&#27861;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10927v1 Announce Type: cross  Abstract: Utilizing unmanned aerial vehicles (UAVs) with edge server to assist terrestrial mobile edge computing (MEC) has attracted tremendous attention. Nevertheless, state-of-the-art schemes based on deterministic optimizations or single-objective reinforcement learning (RL) cannot reduce the backlog of task bits and simultaneously improve energy efficiency in highly dynamic network environments, where the design problem amounts to a sequential decision-making problem. In order to address the aforementioned problems, as well as the curses of dimensionality introduced by the growing number of terrestrial terrestrial users, this paper proposes a distributed multi-objective (MO) dynamic trajectory planning and offloading scheduling scheme, integrated with MORL and the kernel method. The design of n-step return is also applied to average fluctuations in the backlog. Numerical results reveal that the n-step return can benefit the proposed kernel-b
&lt;/p&gt;</description></item><item><title>TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.10923</link><description>&lt;p&gt;
TabPFN&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for TabPFN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10923
&lt;/p&gt;
&lt;p&gt;
TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;Prior-Data Fitted Networks&#65288;PFNs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#20855;&#26377;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#32467;&#26524;&#12290;TabPFN&#27169;&#22411;&#26159;PFN&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#22312;&#19981;&#38656;&#35201;&#23398;&#20064;&#21442;&#25968;&#25110;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#22312;&#30701;&#30701;&#20960;&#31186;&#38047;&#20869;&#23454;&#29616;&#22810;&#31181;&#20998;&#31867;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;TabPFN&#22240;&#27492;&#25104;&#20026;&#20102;&#35768;&#22810;&#39046;&#22495;&#24212;&#29992;&#20013;&#38750;&#24120;&#21560;&#24341;&#20154;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;TabPFN&#19987;&#38376;&#35774;&#35745;&#30340;&#27969;&#34892;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#21033;&#29992;&#35813;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#25913;&#36827;&#20801;&#35768;&#27604;&#29616;&#26377;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36991;&#20813;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10923v1 Announce Type: cross  Abstract: The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes. The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by in-context learning without the need for learning parameters or hyperparameter tuning. This makes TabPFN a very attractive option for a wide range of domain applications. However, a major drawback of the method is its lack of interpretability. Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN. By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations. In particular, we show how in-context learning facilitates the estimation of Shapley values by avoid
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20102;&#38024;&#23545;&#21360;&#24230;&#22478;&#24066;&#22270;&#20687;&#30340;&#33258;&#21160;&#20301;&#32622;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#65288;&#26222;&#36890;CNN&#21644;VGG16&#27169;&#22411;&#65289;&#33719;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#65292;&#24182;&#31361;&#20986;&#20102;&#20248;&#21183;&#21644;&#25913;&#36827;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10912</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#20301;&#32622;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic location detection based on deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10912
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20102;&#38024;&#23545;&#21360;&#24230;&#22478;&#24066;&#22270;&#20687;&#30340;&#33258;&#21160;&#20301;&#32622;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#65288;&#26222;&#36890;CNN&#21644;VGG16&#27169;&#22411;&#65289;&#33719;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#65292;&#24182;&#31361;&#20986;&#20102;&#20248;&#21183;&#21644;&#25913;&#36827;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22270;&#20687;&#30340;&#26222;&#21450;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#20026;&#21508;&#20010;&#39046;&#22495;&#24102;&#26469;&#20102;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#23545;&#38024;&#23545;&#21360;&#24230;&#22478;&#24066;&#22270;&#20687;&#36827;&#34892;&#35782;&#21035;&#21644;&#20998;&#31867;&#30340;&#22270;&#20687;&#20998;&#31867;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#21644;&#23454;&#26045;&#12290;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22270;&#20687;&#20998;&#31867;&#20026;&#20116;&#20010;&#20027;&#35201;&#30340;&#21360;&#24230;&#22478;&#24066;&#65306;&#33406;&#21704;&#36808;&#36798;&#24052;&#24503;&#12289;&#24503;&#37324;&#12289;&#21888;&#25289;&#25289;&#12289;&#21152;&#23572;&#21508;&#31572;&#21644;&#23391;&#20080;&#65292;&#20197;&#35782;&#21035;&#27599;&#20010;&#22478;&#24066;/&#37030;&#30340;&#29420;&#29305;&#29305;&#24449;&#21644;&#29305;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#26041;&#27861;&#12290;&#39318;&#20808;&#26159;&#19968;&#20010;&#26222;&#36890;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#28982;&#21518;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#21033;&#29992;VGG16&#27169;&#22411;&#26469;&#23454;&#29616;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#21147;&#12290;&#26222;&#36890;CNN&#23454;&#29616;&#20102;&#21487;&#35266;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;VGG16&#27169;&#22411;&#23454;&#29616;&#20102;63.6%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#12290;&#35780;&#20272;&#31361;&#20986;&#20102;&#20248;&#21183;&#21644;&#25913;&#36827;&#28508;&#21147;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10912v1 Announce Type: cross  Abstract: The proliferation of digital images and the advancements in deep learning have paved the way for innovative solutions in various domains, especially in the field of image classification. Our project presents an in-depth study and implementation of an image classification system specifically tailored to identify and classify images of Indian cities. Drawing from an extensive dataset, our model classifies images into five major Indian cities: Ahmedabad, Delhi, Kerala, Kolkata, and Mumbai to recognize the distinct features and characteristics of each city/state. To achieve high precision and recall rates, we adopted two approaches. The first, a vanilla Convolutional Neural Network (CNN) and then we explored the power of transfer learning by leveraging the VGG16 model. The vanilla CNN achieved commendable accuracy and the VGG16 model achieved a test accuracy of 63.6%. Evaluations highlighted the strengths and potential areas of improvement
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;L20&#33539;&#25968;&#32422;&#26463;&#30340;&#22270;&#27491;&#21017;&#21270;NMF&#29992;&#20110;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#65292;&#26088;&#22312;&#22686;&#24378;&#29305;&#24449;&#31232;&#30095;&#24615;&#21644;&#20943;&#36731;&#22122;&#22768;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.10910</link><description>&lt;p&gt;
&#20855;&#26377;L20&#33539;&#25968;&#30340;&#22270;&#27491;&#21017;&#21270;NMF&#29992;&#20110;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Regularized NMF with L20-norm for Unsupervised Feature Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10910
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;L20&#33539;&#25968;&#32422;&#26463;&#30340;&#22270;&#27491;&#21017;&#21270;NMF&#29992;&#20110;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#65292;&#26088;&#22312;&#22686;&#24378;&#29305;&#24449;&#31232;&#30095;&#24615;&#21644;&#20943;&#36731;&#22122;&#22768;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#12290;&#22270;&#27491;&#21017;&#21270;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;GNMF&#65289;&#26159;NMF&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#23427;&#21253;&#21547;&#20102;&#22270;&#27491;&#21017;&#21270;&#32422;&#26463;&#12290;GNMF&#22312;&#32858;&#31867;&#21644;&#38477;&#32500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#23884;&#20837;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20302;&#32500;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;GNMF&#23545;&#22122;&#22768;&#30340;&#25935;&#24863;&#24615;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#22686;&#24378;&#29305;&#24449;&#30340;&#31232;&#30095;&#24615;&#24182;&#20943;&#36731;&#22122;&#22768;&#23545;&#20854;&#24433;&#21709;&#65292;&#21516;&#26102;&#22312;&#25968;&#25454;&#20013;&#25366;&#25496;&#34892;&#31232;&#30095;&#27169;&#24335;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29305;&#24449;&#36873;&#25321;&#65292;&#25105;&#20204;&#23558;L20&#33539;&#25968;&#32422;&#26463;&#24341;&#20837;&#20316;&#20026;GNMF&#30340;&#31232;&#30095;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNMF\_$\ell_{20}$&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;PALM&#21450;&#20854;&#21152;&#36895;&#29256;&#26412;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10910v1 Announce Type: new  Abstract: Nonnegative Matrix Factorization (NMF) is a widely applied technique in the fields of machine learning and data mining. Graph Regularized Non-negative Matrix Factorization (GNMF) is an extension of NMF that incorporates graph regularization constraints. GNMF has demonstrated exceptional performance in clustering and dimensionality reduction, effectively discovering inherent low-dimensional structures embedded within high-dimensional spaces. However, the sensitivity of GNMF to noise limits its stability and robustness in practical applications. In order to enhance feature sparsity and mitigate the impact of noise while mining row sparsity patterns in the data for effective feature selection, we introduce the $\ell_{2,0}$-norm constraint as the sparsity constraints for GNMF. We propose an unsupervised feature learning framework based on GNMF\_$\ell_{20}$ and devise an algorithm based on PALM and its accelerated version to address this prob
&lt;/p&gt;</description></item><item><title>DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.10903</link><description>&lt;p&gt;
DTOR&#65306;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#29992;&#20110;&#35299;&#37322;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
DTOR: Decision Tree Outlier Regressor to explain anomalies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10903
&lt;/p&gt;
&lt;p&gt;
DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#20197;&#21450;&#20854;&#20135;&#29983;&#26426;&#21046;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#25925;&#38556;&#12289;&#27450;&#35784;&#12289;&#23041;&#32961;&#31561;&#38382;&#39064;&#65292;&#38500;&#20102;&#34987;&#27491;&#30830;&#35782;&#21035;&#20043;&#22806;&#65292;&#36890;&#24120;&#38656;&#35201;&#26377;&#25928;&#30340;&#35299;&#37322;&#20197;&#26377;&#25928;&#25191;&#34892;&#21487;&#25805;&#20316;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#20351;&#24471;&#36825;&#26679;&#30340;&#35299;&#37322;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65288;DTOR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#36825;&#26159;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#26469;&#35745;&#31639;&#20272;&#35745;&#20998;&#25968;&#65292;&#28982;&#21518;&#25552;&#21462;&#19982;&#25968;&#25454;&#28857;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#30456;&#23545;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;DTOR&#30340;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#21015;&#34920;&#23398;&#20064;&#20013;&#22343;&#21248;&#25910;&#25947;&#21644;&#26679;&#26412;&#21387;&#32553;&#21407;&#21017;&#30340;&#36866;&#29992;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#21015;&#34920;PAC&#23398;&#20064;&#20013;&#22343;&#21248;&#25910;&#25947;&#20173;&#28982;&#31561;&#20215;&#20110;&#21487;&#23398;&#20064;&#24615;</title><link>https://arxiv.org/abs/2403.10889</link><description>&lt;p&gt;
&#21015;&#34920;&#26679;&#26412;&#21387;&#32553;&#21644;&#22343;&#21248;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
List Sample Compression and Uniform Convergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10889
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#21015;&#34920;&#23398;&#20064;&#20013;&#22343;&#21248;&#25910;&#25947;&#21644;&#26679;&#26412;&#21387;&#32553;&#21407;&#21017;&#30340;&#36866;&#29992;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#21015;&#34920;PAC&#23398;&#20064;&#20013;&#22343;&#21248;&#25910;&#25947;&#20173;&#28982;&#31561;&#20215;&#20110;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21015;&#34920;&#23398;&#20064;&#26159;&#30417;&#30563;&#20998;&#31867;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#22312;&#36825;&#31181;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#22120;&#20026;&#27599;&#20010;&#23454;&#20363;&#36755;&#20986;&#22810;&#20010;&#21487;&#33021;&#30340;&#26631;&#31614;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#21015;&#34920;&#23398;&#20064;&#19978;&#30340;&#27867;&#21270;&#30456;&#20851;&#30340;&#32463;&#20856;&#21407;&#21017;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#22312;&#21015;&#34920;PAC&#23398;&#20064;&#39046;&#22495;&#65292;PAC&#35774;&#32622;&#20013;&#30340;&#32463;&#20856;&#21407;&#21017;&#26159;&#21542;&#20445;&#30041;&#20854;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#22343;&#21248;&#25910;&#25947;&#65288;&#36825;&#26159;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#22522;&#30784;&#65289;&#21644;&#26679;&#26412;&#21387;&#32553;&#65288;&#36825;&#26159;Occam's Razor&#30340;&#19968;&#20010;&#24378;&#22823;&#20307;&#29616;&#65289;&#12290;&#22312;&#32463;&#20856;PAC&#23398;&#20064;&#20013;&#65292;&#22343;&#21248;&#25910;&#25947;&#21644;&#26679;&#26412;&#21387;&#32553;&#37117;&#28385;&#36275;&#19968;&#31181;&#8220;&#23436;&#22791;&#24615;&#8221;&#24418;&#24335;&#65306;&#27599;&#24403;&#19968;&#20010;&#31867;&#26159;&#21487;&#23398;&#20064;&#30340;&#26102;&#20505;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#36981;&#24490;&#36825;&#20123;&#21407;&#21017;&#30340;&#23398;&#20064;&#35268;&#21017;&#26469;&#23398;&#20064;&#23427;&#12290;&#25105;&#20204;&#25506;&#35752;&#22312;&#21015;&#34920;&#23398;&#20064;&#29615;&#22659;&#20013;&#26159;&#21542;&#20063;&#23384;&#22312;&#30456;&#21516;&#30340;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;&#21015;&#34920;PAC&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#22343;&#21248;&#25910;&#25947;&#20173;&#28982;&#31561;&#20215;&#20110;&#21487;&#23398;&#20064;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10889v1 Announce Type: new  Abstract: List learning is a variant of supervised classification where the learner outputs multiple plausible labels for each instance rather than just one. We investigate classical principles related to generalization within the context of list learning. Our primary goal is to determine whether classical principles in the PAC setting retain their applicability in the domain of list PAC learning. We focus on uniform convergence (which is the basis of Empirical Risk Minimization) and on sample compression (which is a powerful manifestation of Occam's Razor). In classical PAC learning, both uniform convergence and sample compression satisfy a form of `completeness': whenever a class is learnable, it can also be learned by a learning rule that adheres to these principles. We ask whether the same completeness holds true in the list learning setting.   We show that uniform convergence remains equivalent to learnability in the list PAC learning setting
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#38750;&#23545;&#31216;&#30456;&#20284;&#24615;&#20989;&#25968;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#27010;&#29575;&#19990;&#30028;&#21160;&#24577;&#30340;&#20960;&#20309;&#25277;&#35937;&#23884;&#20837;&#21040;&#34920;&#24449;&#31354;&#38388;&#20013;&#65292;&#24182;&#23454;&#29616;&#22810;&#21521;&#27010;&#29575;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.10875</link><description>&lt;p&gt;
&#29992;&#38750;&#23545;&#31216;&#36317;&#31163;&#24230;&#37327;&#36827;&#34892;&#27010;&#29575;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic World Modeling with Asymmetric Distance Measure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10875
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#38750;&#23545;&#31216;&#30456;&#20284;&#24615;&#20989;&#25968;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#27010;&#29575;&#19990;&#30028;&#21160;&#24577;&#30340;&#20960;&#20309;&#25277;&#35937;&#23884;&#20837;&#21040;&#34920;&#24449;&#31354;&#38388;&#20013;&#65292;&#24182;&#23454;&#29616;&#22810;&#21521;&#27010;&#29575;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#20013;&#25581;&#31034;&#32467;&#26500;&#20197;&#20415;&#20419;&#36827;&#21518;&#32493;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#38543;&#26426;&#19990;&#30028;&#20013;&#35745;&#21010;&#21644;&#25512;&#29702;&#30340;&#22909;&#30340;&#34920;&#24449;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#23398;&#20064;&#36317;&#31163;&#20989;&#25968;&#23545;&#20110;&#22312;&#34920;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#35745;&#21010;&#21644;&#25512;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#23558;&#27010;&#29575;&#19990;&#30028;&#21160;&#24577;&#30340;&#20960;&#20309;&#25277;&#35937;&#23884;&#20837;&#21040;&#34920;&#24449;&#31354;&#38388;&#20013;&#12290;&#19982;&#20043;&#21069;&#20851;&#27880;&#23398;&#20064;&#30456;&#20114;&#30456;&#20284;&#24615;&#25110;&#20860;&#23481;&#24615;&#24230;&#37327;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#21453;&#26144;&#29366;&#24577;&#36798;&#21040;&#24615;&#21644;&#20801;&#35768;&#22810;&#21521;&#27010;&#29575;&#25512;&#29702;&#30340;&#38750;&#23545;&#31216;&#30456;&#20284;&#24615;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20197;&#19968;&#20010;&#20849;&#21516;&#30340;&#21442;&#32771;&#29366;&#24577;&#65288;&#20363;&#22914;&#35266;&#23519;&#32773;&#30340;&#24403;&#21069;&#29366;&#24577;&#65289;&#20026;&#26465;&#20214;&#65292;&#25152;&#23398;&#20064;&#30340;&#34920;&#24449;&#31354;&#38388;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#20960;&#20309;&#19978;&#26174;&#33879;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10875v1 Announce Type: new  Abstract: Representation learning is a fundamental task in machine learning, aiming at uncovering structures from data to facilitate subsequent tasks. However, what is a good representation for planning and reasoning in a stochastic world remains an open problem. In this work, we posit that learning a distance function is essential to allow planning and reasoning in the representation space. We show that a geometric abstraction of the probabilistic world dynamics can be embedded into the representation space through asymmetric contrastive learning. Unlike previous approaches that focus on learning mutual similarity or compatibility measures, we instead learn an asymmetric similarity function that reflects the state reachability and allows multi-way probabilistic inference. Moreover, by conditioning on a common reference state (e.g. the observer's current state), the learned representation space allows us to discover the geometrically salient state
&lt;/p&gt;</description></item><item><title>stMCDI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23450;&#20301;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#26469;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.10863</link><description>&lt;p&gt;
stMCDI: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#25513;&#30721;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22635;&#34917;
&lt;/p&gt;
&lt;p&gt;
stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10863
&lt;/p&gt;
&lt;p&gt;
stMCDI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23450;&#20301;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#26469;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#23450;&#20301;&#36716;&#24405;&#32452;&#23398;&#36890;&#36807;&#25552;&#20379;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#21450;&#20854;&#30456;&#24212;&#30340;&#29289;&#29702;&#20301;&#32622;&#65292;&#20026;&#21333;&#32454;&#32990;&#20998;&#26512;&#24102;&#26469;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39640;&#24230;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#21364;&#24102;&#26469;&#20102;&#19968;&#20010;&#32570;&#28857;&#65292;&#21363;&#30001;&#20110;&#32570;&#22833;&#20540;&#30340;&#39640;&#21457;&#29983;&#29575;&#65292;&#23548;&#33268;&#20102;&#32454;&#32990;&#27700;&#24179;&#19978;&#30340;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#26126;&#26174;&#21463;&#21040;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22635;&#34917;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#19981;&#21516;&#28857;&#20043;&#38388;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#35201;&#20040;&#29306;&#29298;&#25972;&#20307;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#26377;&#25928;&#21033;&#29992;&#31354;&#38388;&#23450;&#20301;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#26469;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;stMCDI&#65292;&#21033;&#29992;&#20351;&#29992;&#38543;&#26426;&#25513;&#30721;&#25968;&#25454;&#37096;&#20998;&#20316;&#20026;&#25351;&#23548;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10863v1 Announce Type: cross  Abstract: Spatially resolved transcriptomics represents a significant advancement in single-cell analysis by offering both gene expression data and their corresponding physical locations. However, this high degree of spatial resolution entails a drawback, as the resulting spatial transcriptomic data at the cellular level is notably plagued by a high incidence of missing values. Furthermore, most existing imputation methods either overlook the spatial information between spots or compromise the overall gene expression data distribution. To address these challenges, our primary focus is on effectively utilizing the spatial location information within spatial transcriptomic data to impute missing values, while preserving the overall data distribution. We introduce \textbf{stMCDI}, a novel conditional diffusion model for spatial transcriptomics data imputation, which employs a denoising network trained using randomly masked data portions as guidance
&lt;/p&gt;</description></item><item><title>FedQNN&#26694;&#26550;&#34701;&#21512;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#21407;&#21017;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21512;&#20316;&#23398;&#20064;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10861</link><description>&lt;p&gt;
FedQNN: &#20351;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedQNN: Federated Learning using Quantum Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10861
&lt;/p&gt;
&lt;p&gt;
FedQNN&#26694;&#26550;&#34701;&#21512;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#21407;&#21017;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21512;&#20316;&#23398;&#20064;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#30340;&#21019;&#26032;&#39046;&#22495;&#65292;&#20316;&#20026;&#36890;&#36807;&#20998;&#24067;&#24335;&#32593;&#32476;&#35757;&#32451;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32852;&#37030;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FedQNN&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#23574;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;QML&#30340;&#29420;&#29305;&#29305;&#24449;&#19982;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;&#12290;&#36825;&#39033;&#24037;&#20316;&#24443;&#24213;&#35843;&#26597;&#20102;QFL&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20445;&#25252;&#25968;&#25454;&#22788;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#20419;&#36827;&#21512;&#20316;&#23398;&#20064;&#32780;&#26080;&#38656;&#30452;&#25509;&#20849;&#20139;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#22522;&#22240;&#32452;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#27010;&#24565;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;FedQNN&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#32467;&#26524;&#22312;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20445;&#25345;&#22312;86%&#20197;&#19978;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10861v1 Announce Type: cross  Abstract: In this study, we explore the innovative domain of Quantum Federated Learning (QFL) as a framework for training Quantum Machine Learning (QML) models via distributed networks. Conventional machine learning models frequently grapple with issues about data privacy and the exposure of sensitive information. Our proposed Federated Quantum Neural Network (FedQNN) framework emerges as a cutting-edge solution, integrating the singular characteristics of QML with the principles of classical federated learning. This work thoroughly investigates QFL, underscoring its capability to secure data handling in a distributed environment and facilitate cooperative learning without direct data sharing. Our research corroborates the concept through experiments across varied datasets, including genomics and healthcare, thereby validating the versatility and efficacy of our FedQNN framework. The results consistently exceed 86% accuracy across three distinct
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;CME&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26680;&#26465;&#20214;&#22343;&#20540;&#23884;&#20837;&#38754;&#20020;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#34920;&#29616;&#21147;&#25361;&#25112;&#65292;&#24182;&#22312;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10859</link><description>&lt;p&gt;
&#31070;&#32463;&#26680;&#26465;&#20214;&#22343;&#20540;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Neural-Kernel Conditional Mean Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10859
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;CME&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26680;&#26465;&#20214;&#22343;&#20540;&#23884;&#20837;&#38754;&#20020;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#34920;&#29616;&#21147;&#25361;&#25112;&#65292;&#24182;&#22312;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26465;&#20214;&#22343;&#20540;&#23884;&#20837;&#65288;CME&#65289;&#20026;&#34920;&#31034;&#26465;&#20214;&#20998;&#24067;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20294;&#36890;&#24120;&#38754;&#20020;&#21487;&#20280;&#32553;&#24615;&#21644;&#34920;&#29616;&#21147;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;CME&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#31181;&#35774;&#35745;&#36991;&#20813;&#20102;&#24403;&#21069;CME&#26041;&#27861;&#25152;&#38656;&#30340;&#35745;&#31639;&#26114;&#36149;&#30340;Gram&#30697;&#38453;&#27714;&#36870;&#12290;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#20248;&#21270;&#21097;&#20313;&#30340;&#26680;&#36229;&#21442;&#25968;&#12290;&#22312;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;NN-CME&#28151;&#21512;&#26041;&#27861;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#24120;&#24120;&#36229;&#36807;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26080;&#32541;&#38598;&#25104;&#21040;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#20013;&#30340;&#21331;&#36234;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10859v1 Announce Type: cross  Abstract: Kernel conditional mean embeddings (CMEs) offer a powerful framework for representing conditional distribution, but they often face scalability and expressiveness challenges. In this work, we propose a new method that effectively combines the strengths of deep learning with CMEs in order to address these challenges. Specifically, our approach leverages the end-to-end neural network (NN) optimization framework using a kernel-based objective. This design circumvents the computationally expensive Gram matrix inversion required by current CME methods. To further enhance performance, we provide efficient strategies to optimize the remaining kernel hyperparameters. In conditional density estimation tasks, our NN-CME hybrid achieves competitive performance and often surpasses existing deep learning-based methods. Lastly, we showcase its remarkable versatility by seamlessly integrating it into reinforcement learning (RL) contexts. Building on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36873;&#39033;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;Hierarchical Policy learning&#26469;&#35299;&#20915;&#39640;&#32500;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10855</link><description>&lt;p&gt;
&#20351;&#29992;&#36873;&#39033;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Options
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36873;&#39033;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;Hierarchical Policy learning&#26469;&#35299;&#20915;&#39640;&#32500;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#24182;&#24314;&#31435;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#25913;&#36827;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#39640;&#32500;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20197;&#20998;&#23618;&#26041;&#24335;&#20998;&#35299;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65292;&#26469;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#31532;&#19968;&#31456;&#25105;&#20204;&#29087;&#24713;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#19968;&#20123;&#26368;&#36817;&#30340;&#25216;&#26415;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20998;&#23618;&#31574;&#30053;&#23398;&#20064;&#65292;&#20197;&#24212;&#23545;&#21333;&#20010;&#22522;&#26412;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20010;&#23618;&#27425;&#32467;&#26500;&#30001;&#39030;&#23618;&#30340;&#31649;&#29702;&#20195;&#29702;&#21644;&#24213;&#23618;&#30340;&#21592;&#24037;&#20195;&#29702;&#32452;&#25104;&#12290;&#22312;&#26368;&#21518;&#19968;&#31456;&#65292;&#20063;&#26159;&#26412;&#35770;&#25991;&#30340;&#26680;&#24515;&#37096;&#20998;&#65292;&#25105;&#20204;&#23581;&#35797;&#29420;&#31435;&#20110;&#31649;&#29702;&#32423;&#21035;&#23398;&#20064;&#23618;&#27425;&#32467;&#26500;&#30340;&#20302;&#23618;&#20803;&#32032;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;Eigenoption&#8221;&#12290;&#22522;&#20110;&#29615;&#22659;&#30340;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10855v1 Announce Type: new  Abstract: The current thesis aims to explore the reinforcement learning field and build on existing methods to produce improved ones to tackle the problem of learning in high-dimensional and complex environments. It addresses such goals by decomposing learning tasks in a hierarchical fashion known as Hierarchical Reinforcement Learning.   We start in the first chapter by getting familiar with the Markov Decision Process framework and presenting some of its recent techniques that the following chapters use. We then proceed to build our Hierarchical Policy learning as an answer to the limitations of a single primitive policy. The hierarchy is composed of a manager agent at the top and employee agents at the lower level.   In the last chapter, which is the core of this thesis, we attempt to learn lower-level elements of the hierarchy independently of the manager level in what is known as the "Eigenoption". Based on the graph structure of the environm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10853</link><description>&lt;p&gt;
&#21482;&#35828;&#21517;&#31216;&#65306;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#23454;&#29616;&#20165;&#21033;&#29992;&#31867;&#21035;&#21517;&#31216;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Just Say the Name: Online Continual Learning with Category Names Only via Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#22823;&#35268;&#27169;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#35832;&#22914;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#20351;&#29992;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#32493;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550; - &#20165;&#20351;&#29992;&#21517;&#31216;&#30340;&#29983;&#25104;&#24335;&#36830;&#32493;&#23398;&#20064;&#65288;G-NoCL&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;G-NoCL&#20351;&#29992;&#19968;&#32452;&#29983;&#25104;&#22120;G&#20197;&#21450;&#23398;&#20064;&#32773;&#12290;&#24403;&#36935;&#21040;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#31867;&#21035;&#65289;&#26102;&#65292;G-NoCL&#37319;&#29992;&#26032;&#39062;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;DIverSity and COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#20174;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#26368;&#20248;&#25277;&#26679;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DISCOBER&#22312;G-NoCL&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#28085;&#30422;&#20102;In-Distributi&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.10842</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#30340;&#21452;Transformer&#22312;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65288;FDD&#65289;&#23545;&#20110;&#30830;&#20445;&#24037;&#19994;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FDD&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#65288;TEP&#65289;&#65292;&#36825;&#26159;&#21270;&#24037;&#36807;&#31243;&#25511;&#21046;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;Transformer&#20998;&#25903;&#65292;&#33021;&#22815;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#65288;GDLAttention&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#38376;&#25511;&#26426;&#21046;&#21644;&#21160;&#24577;&#23398;&#20064;&#33021;&#21147;&#12290;&#38376;&#25511;&#26426;&#21046;&#35843;&#33410;&#27880;&#24847;&#26435;&#37325;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#36755;&#20837;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#12290;&#21160;&#24577;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;&#27880;&#24847;&#26426;&#21046;&#20351;&#29992;&#21452;&#32447;&#24615;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#26469;&#25429;&#25417;&#26597;&#35810;&#21644;&#36755;&#20837;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
&lt;/p&gt;</description></item><item><title>SF(DA)$^2$&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26500;&#24314;&#22686;&#24378;&#22270;&#24182;&#21033;&#29992;&#35889;&#37051;&#22495;&#32858;&#31867;&#26469;&#35782;&#21035;&#20998;&#21306;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#32780;&#36991;&#20813;&#20102;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.10834</link><description>&lt;p&gt;
SF(DA)$^2$: &#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#22312;&#25968;&#25454;&#22686;&#24378;&#30340;&#35270;&#35282;&#19979;
&lt;/p&gt;
&lt;p&gt;
SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10834
&lt;/p&gt;
&lt;p&gt;
SF(DA)$^2$&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26500;&#24314;&#22686;&#24378;&#22270;&#24182;&#21033;&#29992;&#35889;&#37051;&#22495;&#32858;&#31867;&#26469;&#35782;&#21035;&#20998;&#21306;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#32780;&#36991;&#20813;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#23545;&#22495;&#20559;&#31227;&#30340;&#33030;&#24369;&#24615;&#26102;&#65292;&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#30446;&#26631;&#22495;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#28304;&#22495;&#25968;&#25454;&#12290;&#23613;&#31649;&#23558;&#25968;&#25454;&#22686;&#24378;&#24212;&#29992;&#20110;SFDA&#30340;&#28508;&#22312;&#22909;&#22788;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#20063;&#20250;&#24102;&#26469;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#20381;&#36182;&#20110;&#20445;&#23384;&#31867;&#21035;&#21464;&#25442;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#21450;&#22686;&#21152;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SF(DA)$^2$&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#24182;&#36991;&#20813;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#26500;&#24314;&#19968;&#20010;&#22686;&#24378;&#22270;&#65292;&#21033;&#29992;&#30446;&#26631;&#29305;&#24449;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#35889;&#37051;&#22495;&#32858;&#31867;&#26469;&#35782;&#21035;&#39044;&#27979;&#31354;&#38388;&#20013;&#30340;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10834v1 Announce Type: cross  Abstract: In the face of the deep learning model's vulnerability to domain shift, source-free domain adaptation (SFDA) methods have been proposed to adapt models to new, unseen target domains without requiring access to source domain data. Although the potential benefits of applying data augmentation to SFDA are attractive, several challenges arise such as the dependence on prior knowledge of class-preserving transformations and the increase in memory and computational requirements. In this paper, we propose Source-free Domain Adaptation Through the Lens of Data Augmentation (SF(DA)$^2$), a novel approach that leverages the benefits of data augmentation without suffering from these challenges. We construct an augmentation graph in the feature space of the pretrained model using the neighbor relationships between target features and propose spectral neighborhood clustering to identify partitions in the prediction space. Furthermore, we propose im
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LLM&#20195;&#29702;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#29616;&#23454;&#26102;&#30340;&#20154;&#31867;&#35282;&#33394;&#25198;&#28436;&#65292;&#20445;&#30041;&#29420;&#29305;&#19978;&#19979;&#25991;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#25110;&#39044;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.10824</link><description>&lt;p&gt;
LookALike: &#22522;&#20110;&#20154;&#31867;&#27169;&#20223;&#30340;&#21327;&#20316;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LookALike: Human Mimicry based collaborative decision making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10824
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LLM&#20195;&#29702;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#29616;&#23454;&#26102;&#30340;&#20154;&#31867;&#35282;&#33394;&#25198;&#28436;&#65292;&#20445;&#30041;&#29420;&#29305;&#19978;&#19979;&#25991;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#25110;&#39044;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#22312;&#21521;&#20854;&#20182;&#31995;&#32479;&#20256;&#36798;&#29305;&#23450;&#35282;&#33394;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26500;&#24314;&#33021;&#22815;&#30456;&#20114;&#27807;&#36890;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#33258;&#20027;LLM&#20195;&#29702;&#26102;&#65292;&#36825;&#19968;&#19981;&#36275;&#26356;&#20026;&#26126;&#26174;&#12290;&#20154;&#31867;&#33021;&#22815;&#20256;&#36798;&#19978;&#19979;&#25991;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#24494;&#23567;&#24046;&#21035;&#20197;&#21450;&#30693;&#35782;&#65292;&#36825;&#23548;&#33268;&#20102;&#25216;&#33021;&#30340;&#36827;&#19968;&#27493;&#23436;&#21892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20445;&#30041;&#29420;&#29305;&#19978;&#19979;&#25991;&#32780;&#26080;&#38656;&#20381;&#36182;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#25110;&#39044;&#35757;&#32451;&#30340;&#23454;&#26102;&#20154;&#31867;&#35282;&#33394;&#25198;&#28436;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#27169;&#25311;&#23454;&#38469;&#20219;&#21153;&#20013;&#30456;&#27604;&#26368;&#20808;&#36827;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10824v1 Announce Type: cross  Abstract: Artificial General Intelligence falls short when communicating role specific nuances to other systems. This is more pronounced when building autonomous LLM agents capable and designed to communicate with each other for real world problem solving. Humans can communicate context and domain specific nuances along with knowledge, and that has led to refinement of skills. In this work we propose and evaluate a novel method that leads to knowledge distillation among LLM agents leading to realtime human role play preserving unique contexts without relying on any stored data or pretraining. We also evaluate how our system performs better in simulated real world tasks compared to state of the art.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#38543;&#26426;&#36172;&#21338;&#26426;&#30340;&#28608;&#21169;&#25506;&#32034;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#38543;&#26102;&#38388;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#21644;&#34917;&#20607;</title><link>https://arxiv.org/abs/2403.10819</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#38543;&#26426;&#36172;&#21338;&#26426;&#30340;&#28608;&#21169;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Incentivized Exploration of Non-Stationary Stochastic Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#38543;&#26426;&#36172;&#21338;&#26426;&#30340;&#28608;&#21169;&#25506;&#32034;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#38543;&#26102;&#38388;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#21644;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#38382;&#39064;&#20013;&#30340;&#28608;&#21169;&#25506;&#32034;&#65292;&#20854;&#20013;&#29609;&#23478;&#36890;&#36807;&#25506;&#32034;&#38500;&#20102;&#36138;&#23146;&#36873;&#25321;&#20043;&#22806;&#30340;&#33218;&#33719;&#24471;&#34917;&#20607;&#65292;&#24182;&#19988;&#21487;&#33021;&#23545;&#22870;&#21169;&#25552;&#20379;&#20559;&#20506;&#21453;&#39304;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#65306;&#31361;&#21464;&#21644;&#25345;&#32493;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#28608;&#21169;&#25506;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#38543;&#26102;&#38388;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#21644;&#34917;&#20607;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#28608;&#21169;&#20102;&#25506;&#32034;&#65292;&#23613;&#31649;&#23384;&#22312;&#38750;&#24179;&#31283;&#24615;&#21644;&#20559;&#20506;&#25110;&#28418;&#31227;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10819v1 Announce Type: cross  Abstract: We study incentivized exploration for the multi-armed bandit (MAB) problem with non-stationary reward distributions, where players receive compensation for exploring arms other than the greedy choice and may provide biased feedback on the reward. We consider two different non-stationary environments: abruptly-changing and continuously-changing, and propose respective incentivized exploration algorithms. We show that the proposed algorithms achieve sublinear regret and compensation over time, thus effectively incentivizing exploration despite the nonstationarity and the biased or drifted feedback.
&lt;/p&gt;</description></item><item><title>FlyKD&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39134;&#34892;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#32467;&#21512;&#35838;&#31243;&#23398;&#20064;&#65292;&#33021;&#22815;&#29983;&#25104;&#20960;&#20046;&#26080;&#38480;&#25968;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#26497;&#22823;&#32531;&#35299;&#20102;&#22312;&#22024;&#26434;&#20266;&#26631;&#31614;&#19978;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#20934;KD&#21644;&#23616;&#37096;&#32467;&#26500;&#20445;&#25345;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10807</link><description>&lt;p&gt;
FlyKD: &#39134;&#34892;&#20013;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#19982;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10807
&lt;/p&gt;
&lt;p&gt;
FlyKD&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39134;&#34892;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#32467;&#21512;&#35838;&#31243;&#23398;&#20064;&#65292;&#33021;&#22815;&#29983;&#25104;&#20960;&#20046;&#26080;&#38480;&#25968;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#26497;&#22823;&#32531;&#35299;&#20102;&#22312;&#22024;&#26434;&#20266;&#26631;&#31614;&#19978;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#20934;KD&#21644;&#23616;&#37096;&#32467;&#26500;&#20445;&#25345;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26088;&#22312;&#23558;&#26356;&#24378;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#32473;&#26356;&#36731;&#20415;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20351;&#20854;&#26356;&#24555;&#36895;&#21644;&#26356;&#26131;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#27169;&#22411;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#38754;&#20020;&#30001;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#30340;&#22024;&#26434;&#20266;&#26631;&#31614;&#30340;&#22256;&#38590;&#65292;&#32780;&#33021;&#22815;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#37327;&#21463;&#21040;&#20869;&#23384;&#19981;&#36275;&#65288;OOM&#65289;&#38169;&#35823;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FlyKD&#65288;&#39134;&#34892;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#65289;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#20960;&#20046;&#26080;&#38480;&#25968;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#32467;&#21512;&#35838;&#31243;&#23398;&#20064;&#22823;&#22823;&#32531;&#35299;&#20102;&#23545;&#22024;&#26434;&#20266;&#26631;&#31614;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FlyKD&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;KD&#21644;&#33879;&#21517;&#30340;&#23616;&#37096;&#32467;&#26500;&#20445;&#25345;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;LSPGCN&#65289;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#25913;&#21892;&#23545;&#22024;&#26434;&#20266;&#26631;&#31614;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10807v1 Announce Type: new  Abstract: Knowledge Distillation (KD) aims to transfer a more capable teacher model's knowledge to a lighter student model in order to improve the efficiency of the model, making it faster and more deployable. However, the student model's optimization process over the noisy pseudo labels (generated by the teacher model) is tricky and the amount of pseudo labels one can generate is limited due to Out of Memory (OOM) error. In this paper, we propose FlyKD (Knowledge Distillation on the Fly) which enables the generation of virtually unlimited number of pseudo labels, coupled with Curriculum Learning that greatly alleviates the optimization process over the noisy pseudo labels. Empirically, we observe that FlyKD outperforms vanilla KD and the renown Local Structure Preserving Graph Convolutional Network (LSPGCN). Lastly, with the success of Curriculum Learning, we shed light on a new research direction of improving optimization over noisy pseudo label
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLOD&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27979;&#35797;&#36807;&#31243;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#29305;&#24449;&#20013;&#35782;&#21035;&#27979;&#35797;&#26679;&#26412;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#26080;&#38656;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.10803</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#22810;&#27979;&#35797;&#30340;&#36880;&#23618;&#29305;&#24449;&#34701;&#21512;&#22686;&#24378;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10803
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLOD&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27979;&#35797;&#36807;&#31243;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#29305;&#24449;&#20013;&#35782;&#21035;&#27979;&#35797;&#26679;&#26412;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#26080;&#38656;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#20250;&#36935;&#21040;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;&#36935;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#21508;&#31181;&#27979;&#35797;&#36755;&#20837;&#65292;&#36825;&#20123;&#36229;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#21487;&#33021;&#22312;&#23616;&#37096;&#25110;&#20840;&#23616;&#29305;&#24449;&#19978;&#19982;&#35757;&#32451;&#20998;&#24067;&#26377;&#25152;&#20559;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#22810;&#27979;&#35797;&#30340;&#36880;&#23618;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#65288;MLOD&#65289;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#22810;&#20010;&#27979;&#35797;&#36807;&#31243;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#29305;&#24449;&#20013;&#37492;&#21035;&#27979;&#35797;&#26679;&#26412;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20462;&#25913;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10803v1 Announce Type: cross  Abstract: Deploying machine learning in open environments presents the challenge of encountering diverse test inputs that differ significantly from the training data. These out-of-distribution samples may exhibit shifts in local or global features compared to the training distribution. The machine learning (ML) community has responded with a number of methods aimed at distinguishing anomalous inputs from original training data. However, the majority of previous studies have primarily focused on the output layer or penultimate layer of pre-trained deep neural networks. In this paper, we propose a novel framework, Multitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), to identify distributional shifts in test samples at different levels of features through rigorous multiple testing procedure. Our approach distinguishes itself from existing methods as it does not require modifying the structure or fine-tuning of the pre-trained c
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#38548;&#31163;&#26426;&#21046;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#20302;&#20869;&#23384;&#20351;&#29992;&#12289;&#39640;&#21487;&#20280;&#32553;&#24615;&#12289;&#23545;&#22122;&#22768;&#21644;&#26080;&#20851;&#29305;&#24449;&#30340;&#31283;&#20581;&#24615;&#65292;&#20197;&#21450;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#32321;&#37325;&#21442;&#25968;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2403.10802</link><description>&lt;p&gt;
&#22522;&#20110;&#38548;&#31163;&#26426;&#21046;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection Based on Isolation Mechanisms: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10802
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38548;&#31163;&#26426;&#21046;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#20302;&#20869;&#23384;&#20351;&#29992;&#12289;&#39640;&#21487;&#20280;&#32553;&#24615;&#12289;&#23545;&#22122;&#22768;&#21644;&#26080;&#20851;&#29305;&#24449;&#30340;&#31283;&#20581;&#24615;&#65292;&#20197;&#21450;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#32321;&#37325;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#38271;&#26399;&#24182;&#19988;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#37329;&#34701;&#12289;&#23433;&#20840;&#21644;&#21046;&#36896;&#31561;&#39046;&#22495;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#21463;&#21040;&#22823;&#35268;&#27169;&#12289;&#39640;&#32500;&#21644;&#24322;&#26500;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#38548;&#31163;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#35782;&#21035;&#25968;&#25454;&#20013;&#24322;&#24120;&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#24322;&#24120;&#24456;&#23569;&#19988;&#19982;&#27491;&#24120;&#23454;&#20363;&#19981;&#21516;&#30340;&#29702;&#24565;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#20998;&#21306;&#36731;&#26494;&#38548;&#31163;&#24322;&#24120;&#20540;&#12290;&#22522;&#20110;&#38548;&#31163;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#22914;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#20302;&#20869;&#23384;&#20351;&#29992;&#12289;&#39640;&#21487;&#20280;&#32553;&#24615;&#12289;&#23545;&#22122;&#22768;&#21644;&#26080;&#20851;&#29305;&#24449;&#30340;&#31283;&#20581;&#24615;&#65292;&#20197;&#21450;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#32321;&#37325;&#21442;&#25968;&#35843;&#25972;&#12290;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#38548;&#31163;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#25968;&#25454;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10802v1 Announce Type: new  Abstract: Anomaly detection is a longstanding and active research area that has many applications in domains such as finance, security, and manufacturing. However, the efficiency and performance of anomaly detection algorithms are challenged by the large-scale, high-dimensional, and heterogeneous data that are prevalent in the era of big data. Isolation-based unsupervised anomaly detection is a novel and effective approach for identifying anomalies in data. It relies on the idea that anomalies are few and different from normal instances, and thus can be easily isolated by random partitioning. Isolation-based methods have several advantages over existing methods, such as low computational complexity, low memory usage, high scalability, robustness to noise and irrelevant features, and no need for prior knowledge or heavy parameter tuning. In this survey, we review the state-of-the-art isolation-based anomaly detection methods, including their data p
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861; Reprogrammer &#22312;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#20013;&#30340;&#24212;&#29992;&#20248;&#20110;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#22312;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#25968;&#25454;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.10800</link><description>&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#20248;&#20110;&#22312;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#20013;&#38024;&#23545;&#20998;&#24067;&#22806;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10800
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861; Reprogrammer &#22312;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#20013;&#30340;&#24212;&#29992;&#20248;&#20110;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#22312;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#25968;&#25454;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35780;&#20272;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26102;&#65292;&#19981;&#20165;&#38656;&#35201;&#35780;&#20272;&#19979;&#28216;&#27169;&#22411;&#30340;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#20934;&#30830;&#24615;&#65292;&#36824;&#38656;&#35201;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#24182;&#35782;&#21035;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#20405;&#20837;&#24615;&#24494;&#35843;&#25216;&#26415;&#25152;&#24102;&#26469;&#30340;&#38544;&#34255;&#25104;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24120;&#29992;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#20165;&#25197;&#26354;&#20102;&#29992;&#20110;&#27867;&#21270;&#21040;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;OOD&#26679;&#26412;&#65288;OOD&#27867;&#21270;&#65289;&#25152;&#38656;&#30340;&#34920;&#31034;&#65292;&#36824;&#25197;&#26354;&#20102;&#29992;&#20110;&#26816;&#27979;&#22312;&#35821;&#20041;&#19978;&#36716;&#31227;&#30340;OOD&#26679;&#26412;&#65288;OOD&#26816;&#27979;&#65289;&#25152;&#38656;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#29992;&#20110;&#24494;&#35843;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#37325;&#26032;&#32534;&#31243;&#22120;&#12290;&#37325;&#26032;&#32534;&#31243;&#22120;&#26088;&#22312;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#22312;ID&#12289;OOD&#27867;&#21270;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#26174;&#31034;&#37325;&#26032;&#32534;&#31243;&#22120;&#20248;&#20110;&#23545;&#25239;&#24615;&#24494;&#35843;&#19982;&#21407;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#12289;&#26377;&#25928;&#19988;&#24378;&#22823;&#30340;&#24037;&#20855;&#26469;&#24212;&#23545;OOD&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10800v1 Announce Type: new  Abstract: When evaluating the performance of a pre-trained model transferred to a downstream task, it is imperative to assess not only the in-distribution (ID) accuracy of the downstream model but also its capacity to generalize and identify out-of-distribution (OOD) samples. In this paper, we unveil the hidden costs associated with intrusive fine-tuning techniques. Specifically, we demonstrate that commonly used fine-tuning methods not only distort the representations necessary for generalizing to covariate-shifted OOD samples (OOD generalization) but also distort the representations necessary for detecting semantically-shifted OOD samples (OOD detection). To address these challenges, we introduce a new model reprogramming approach for fine-tuning, which we name Reprogrammer. Reprogrammer aims to improve the holistic performance of the downstream model across ID, OOD generalization, and OOD detection tasks. Our empirical evidence reveals that Rep
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.10795</link><description>&lt;p&gt;
&#20174;&#21333;&#35789;&#21040;&#36335;&#24452;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
From Words to Routes: Applying Large Language Models to Vehicle Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65288;&#20363;&#22914;&#25805;&#20316;&#21644;&#23548;&#33322;&#65289;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20854;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#12290;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#35753;&#25105;&#20204;&#24605;&#32771;&#65306;LLMs&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;VRPs&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#19977;&#27493;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;21&#31181;&#21333;&#36710;&#25110;&#22810;&#36710;&#36335;&#24452;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22235;&#31181;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#27599;&#31181;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20174;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#20195;&#30721;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#23545;&#20110;GPT-4&#25928;&#26524;&#26368;&#20339;&#65292;&#23454;&#29616;&#20102;56%&#30340;&#21487;&#34892;&#24615;&#65292;40%&#30340;&#20248;&#21270;&#24615;&#21644;53%&#30340;&#25928;&#29575;&#12290;&#31532;&#19977;&#65292;&#22522;&#20110;&#35266;&#23519;&#21040;LLMs&#21487;&#33021;&#26080;&#27861;&#22312;&#21021;&#22987;&#23581;&#35797;&#20013;&#25552;&#20379;&#27491;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10795v1 Announce Type: cross  Abstract: LLMs have shown impressive progress in robotics (e.g., manipulation and navigation) with natural language task descriptions. The success of LLMs in these tasks leads us to wonder: What is the ability of LLMs to solve vehicle routing problems (VRPs) with natural language task descriptions? In this work, we study this question in three steps. First, we construct a dataset with 21 types of single- or multi-vehicle routing problems. Second, we evaluate the performance of LLMs across four basic prompt paradigms of text-to-code generation, each involving different types of text input. We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency. Third, based on the observation that LLMs may not be able to provide correct solutions at the initial attempt, we propose a framework that enables LLMs to refin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#24212;&#29992;&#25193;&#25955;-&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39640;&#32423;&#25193;&#25955;&#27169;&#22411;&#21644;&#20302;&#32423;RL&#31639;&#27861;&#65292;&#23454;&#29616;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#39640;&#25928;&#29575;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.10794</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#30340;&#25193;&#25955;-&#24378;&#21270;&#23398;&#20064;&#20998;&#23618;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10794
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#24212;&#29992;&#25193;&#25955;-&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39640;&#32423;&#25193;&#25955;&#27169;&#22411;&#21644;&#20302;&#32423;RL&#31639;&#27861;&#65292;&#23454;&#29616;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#39640;&#25928;&#29575;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;-&#22522;&#20110;&#30340;&#36816;&#21160;&#35268;&#21010;&#26368;&#36817;&#23637;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20174;&#33258;&#20027;&#23548;&#33322;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#37096;&#20998;&#21487;&#35266;&#23519;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#36861;&#36880;&#36867;&#36991;&#28216;&#25103;&#65288;PEG&#65289;&#20013;&#23545;&#36867;&#36991;&#30446;&#26631;&#30340;&#36816;&#21160;&#35268;&#21010;&#20219;&#21153;&#12290;&#36825;&#20123;&#36861;&#36880;&#36867;&#36991;&#38382;&#39064;&#19982;&#21508;&#31181;&#24212;&#29992;&#30456;&#20851;&#65292;&#20363;&#22914;&#25628;&#32034;&#21644;&#25937;&#25588;&#34892;&#21160;&#20197;&#21450;&#30417;&#35270;&#26426;&#22120;&#20154;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#24517;&#39035;&#26377;&#25928;&#35268;&#21010;&#20182;&#20204;&#30340;&#34892;&#21160;&#26469;&#25910;&#38598;&#24773;&#25253;&#25110;&#23436;&#25104;&#20219;&#21153;&#65292;&#21516;&#26102;&#36991;&#20813;&#34987;&#20390;&#26597;&#25110;&#34987;&#20440;&#34383;&#33258;&#24049;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#25972;&#21512;&#20102;&#19968;&#20010;&#39640;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35268;&#21010;&#23545;&#29615;&#22659;&#25968;&#25454;&#25935;&#24863;&#30340;&#20840;&#23616;&#36335;&#24452;&#65292;&#21516;&#26102;&#20302;&#32423;RL&#31639;&#27861;&#25512;&#29702;&#38378;&#36991;&#34892;&#20026;&#19982;&#20840;&#23616;&#36335;&#24452;&#36319;&#38543;&#34892;&#20026;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;51.2&#65285;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10794v1 Announce Type: cross  Abstract: Reinforcement Learning- (RL-)based motion planning has recently shown the potential to outperform traditional approaches from autonomous navigation to robot manipulation. In this work, we focus on a motion planning task for an evasive target in a partially observable multi-agent adversarial pursuit-evasion games (PEG). These pursuit-evasion problems are relevant to various applications, such as search and rescue operations and surveillance robots, where robots must effectively plan their actions to gather intelligence or accomplish mission tasks while avoiding detection or capture themselves. We propose a hierarchical architecture that integrates a high-level diffusion model to plan global paths responsive to environment data while a low-level RL algorithm reasons about evasive versus global path-following behavior. Our approach outperforms baselines by 51.2% by leveraging the diffusion model to guide the RL algorithm for more efficien
&lt;/p&gt;</description></item><item><title>QuantumLeak&#26159;&#19968;&#31181;&#26377;&#25928;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22522;&#20110;&#20113;&#30340;NISQ&#26426;&#22120;&#20013;&#25552;&#21462;QNN&#27169;&#22411;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#32463;&#20856;&#27169;&#22411;&#25552;&#21462;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;VQC&#26550;&#26500;&#19979;&#25552;&#39640;&#26412;&#22320;VQC&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10790</link><description>&lt;p&gt;
QuantumLeak&#65306;&#20174;&#22522;&#20110;&#20113;&#30340;NISQ&#26426;&#22120;&#20013;&#31363;&#21462;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10790
&lt;/p&gt;
&lt;p&gt;
QuantumLeak&#26159;&#19968;&#31181;&#26377;&#25928;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22522;&#20110;&#20113;&#30340;NISQ&#26426;&#22120;&#20013;&#25552;&#21462;QNN&#27169;&#22411;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#32463;&#20856;&#27169;&#22411;&#25552;&#21462;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;VQC&#26550;&#26500;&#19979;&#25552;&#39640;&#26412;&#22320;VQC&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;VQC&#65289;&#24050;&#25104;&#20026;&#23454;&#29616;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#22797;&#26434;&#38382;&#39064;&#12290;&#31934;&#24515;&#35757;&#32451;&#30340;VQC&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#36164;&#20135;&#25176;&#31649;&#22312;&#22522;&#20110;&#20113;&#30340;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35745;&#31639;&#26426;&#19978;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;VQC&#31363;&#21462;&#25915;&#20987;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;QuantumLeak&#65292;&#19968;&#31181;&#20174;&#22522;&#20110;&#20113;&#30340;NISQ&#26426;&#22120;&#20013;&#26377;&#25928;&#20934;&#30830;&#25552;&#21462;QNN&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#29992;&#20110;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20256;&#32479;&#27169;&#22411;&#25552;&#21462;&#25216;&#26415;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;VQC&#26550;&#26500;&#19979;&#65292;QuantumLeak&#23558;&#26412;&#22320;VQC&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.99%\~7.35%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10790v1 Announce Type: cross  Abstract: Variational quantum circuits (VQCs) have become a powerful tool for implementing Quantum Neural Networks (QNNs), addressing a wide range of complex problems. Well-trained VQCs serve as valuable intellectual assets hosted on cloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them susceptible to malicious VQC stealing attacks. However, traditional model extraction techniques designed for classical machine learning models encounter challenges when applied to NISQ computers due to significant noise in current devices. In this paper, we introduce QuantumLeak, an effective and accurate QNN model extraction technique from cloud-based NISQ machines. Compared to existing classical model stealing techniques, QuantumLeak improves local VQC accuracy by 4.99\%$\sim$7.35\% across diverse datasets and VQC architectures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCOTT&#30340;&#20855;&#26377;&#30417;&#30563;&#23545;&#27604;&#21464;&#25442;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;Temporal Convolutional Networks&#20197;&#23398;&#20064;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#31616;&#21270;&#20102;&#29992;&#20110;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.10787</link><description>&lt;p&gt;
&#20855;&#26377;&#30417;&#30563;&#23545;&#27604;&#26102;&#38388;&#21464;&#25442;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time Series Representation Learning with Supervised Contrastive Temporal Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10787
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCOTT&#30340;&#20855;&#26377;&#30417;&#30563;&#23545;&#27604;&#21464;&#25442;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;Temporal Convolutional Networks&#20197;&#23398;&#20064;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#31616;&#21270;&#20102;&#29992;&#20110;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26377;&#25928;&#34920;&#31034;&#26159;&#19968;&#39033;&#26377;&#29992;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26377;&#20123;&#24037;&#20316;&#21033;&#29992;&#33258;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#21487;&#29992;&#30340;&#26631;&#31614;&#20449;&#24687;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#31216;&#20026;&#65306;\textbf{S}upervised \textbf{CO}ntrastive \textbf{T}emporal \textbf{T}ransformer (SCOTT)&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21512;&#36866;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#23398;&#20064;&#20855;&#26377;&#21464;&#21270;&#19981;&#21464;&#24615;&#30340;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20197;&#31616;&#21333;&#30340;&#26041;&#24335;&#32467;&#21512;&#20102;Transformer&#21644;Temporal Convolutional Networks&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;&#29992;&#20110;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10787v1 Announce Type: cross  Abstract: Finding effective representations for time series data is a useful but challenging task. Several works utilize self-supervised or unsupervised learning methods to address this. However, there still remains the open question of how to leverage available label information for better representations. To answer this question, we exploit pre-existing techniques in time series and representation learning domains and develop a simple, yet novel fusion model, called: \textbf{S}upervised \textbf{CO}ntrastive \textbf{T}emporal \textbf{T}ransformer (SCOTT). We first investigate suitable augmentation methods for various types of time series data to assist with learning change-invariant representations. Secondly, we combine Transformer and Temporal Convolutional Networks in a simple way to efficiently learn both global and local features. Finally, we simplify Supervised Contrastive Loss for representation learning of labelled time series data. We p
&lt;/p&gt;</description></item><item><title>ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10786</link><description>&lt;p&gt;
ContourDiff&#65306;&#24102;&#36718;&#24275;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#26080;&#37197;&#23545;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10786
&lt;/p&gt;
&lt;p&gt;
ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#65288;&#20363;&#22914;&#20174;CT&#21040;MRI&#65289;&#23545;&#20110;&#35768;&#22810;&#20020;&#24202;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContourDiff&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#26131;&#20110;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#65292;&#20294;&#23545;&#20854;&#35299;&#21078;&#20869;&#23481;&#24418;&#25104;&#31934;&#30830;&#30340;&#31354;&#38388;&#32422;&#26463;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#20219;&#24847;&#36755;&#20837;&#39046;&#22495;&#30340;&#22270;&#20687;&#30340;&#36718;&#24275;&#34920;&#31034;&#36716;&#25442;&#20026;&#36755;&#20986;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10786v1 Announce Type: cross  Abstract: Accurately translating medical images across different modalities (e.g., CT to MRI) has numerous downstream clinical and machine learning applications. While several methods have been proposed to achieve this, they often prioritize perceptual quality with respect to output domain features over preserving anatomical fidelity. However, maintaining anatomy during translation is essential for many tasks, e.g., when leveraging masks from the input domain to develop a segmentation model with images translated to the output domain. To address these challenges, we propose ContourDiff, a novel framework that leverages domain-invariant anatomical contour representations of images. These representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. We introduce a diffusion model that converts contour representations of images from arbitrary input domains into images in the output domain of in
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#29983;&#25104;&#24335;AI&#20013;&#30340;&#31038;&#20250;&#26377;&#23475;&#24433;&#21709;&#65292;&#25552;&#20986;&#23545;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#20851;&#20999;&#65292;&#24182;&#24341;&#39046;&#35752;&#35770;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.10776</link><description>&lt;p&gt;
&#20174;&#22823;&#29076;&#28809;&#21040;&#35823;&#20256;&#65306;&#25506;&#35752;&#29983;&#25104;&#24335;AI&#20013;&#30340;&#26377;&#23475;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
From Melting Pots to Misrepresentations: Exploring Harms in Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10776
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#29983;&#25104;&#24335;AI&#20013;&#30340;&#31038;&#20250;&#26377;&#23475;&#24433;&#21709;&#65292;&#25552;&#20986;&#23545;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#20851;&#20999;&#65292;&#24182;&#24341;&#39046;&#35752;&#35770;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Gemini&#21644;GPT&#31561;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#32435;&#20837;AI&#20316;&#20026;&#26381;&#21153;&#65288;AIaaS&#65289;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#65292;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#31038;&#20250;&#20154;&#21475;&#32500;&#24230;&#19978;&#30340;&#27495;&#35270;&#20542;&#21521;&#20173;&#23384;&#22312;&#20851;&#20999;&#65292;&#23588;&#20854;&#26159;&#20542;&#21521;&#20110;&#26576;&#20123;&#8220;&#22810;&#25968;&#32676;&#20307;&#8221;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#12290;&#23613;&#31649;&#26377;&#20851;&#23186;&#20307;&#21628;&#21505;&#22810;&#26679;&#21270;&#20195;&#34920;&#30340;&#24191;&#27867;&#65292;&#20294;&#22312;AIaaS&#32972;&#26223;&#19979;&#65292;&#36793;&#32536;&#21270;&#31181;&#26063;&#21644;&#27665;&#26063;&#32676;&#20307;&#20173;&#28982;&#38754;&#20020;&#25345;&#32493;&#25197;&#26354;&#12289;&#21051;&#26495;&#21360;&#35937;&#21644;&#24573;&#35270;&#12290;&#26412;&#25991;&#23545;&#31038;&#20250;&#26377;&#23475;&#30740;&#31350;&#29616;&#29366;&#36827;&#34892;&#20102;&#20851;&#38190;&#24635;&#32467;&#65292;&#24341;&#39046;&#35752;&#35770;&#37325;&#28857;&#25918;&#22312;&#23427;&#20204;&#30340;&#24433;&#21709;&#19978;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26412;&#35752;&#35770;&#25351;&#24341;&#19979;&#30340;&#24320;&#25918;&#24335;&#30740;&#31350;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;&#30830;&#23450;&#26410;&#26469;&#30740;&#31350;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10776v1 Announce Type: cross  Abstract: With the widespread adoption of advanced generative models such as Gemini and GPT, there has been a notable increase in the incorporation of such models into sociotechnical systems, categorized under AI-as-a-Service (AIaaS). Despite their versatility across diverse sectors, concerns persist regarding discriminatory tendencies within these models, particularly favoring selected `majority' demographics across various sociodemographic dimensions. Despite widespread calls for diversification of media representations, marginalized racial and ethnic groups continue to face persistent distortion, stereotyping, and neglect within the AIaaS context. In this work, we provide a critical summary of the state of research in the context of social harms to lead the conversation to focus on their implications. We also present open-ended research questions, guided by our discussion, to help define future research pathways.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#8220;&#30417;&#30563;&#24494;&#35843;+&#20154;&#31867;&#27604;&#36739;&#8221;&#26694;&#26550;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#20154;&#31867;&#27604;&#36739;&#26469;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#22024;&#26434;&#25968;&#25454;&#21644;&#39640;&#32500;&#27169;&#22411;&#26102;&#12290;</title><link>https://arxiv.org/abs/2403.10771</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#20154;&#31867;&#27604;&#36739;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Approach for Alignment with Human Comparisons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10771
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#8220;&#30417;&#30563;&#24494;&#35843;+&#20154;&#31867;&#27604;&#36739;&#8221;&#26694;&#26550;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#20154;&#31867;&#27604;&#36739;&#26469;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#22024;&#26434;&#25968;&#25454;&#21644;&#39640;&#32500;&#27169;&#22411;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22686;&#38271;&#30340;&#36235;&#21183;&#26159;&#23558;&#20154;&#31867;&#30693;&#35782;&#25972;&#21512;&#21040;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#24494;&#22937;&#30340;&#20154;&#31867;&#21453;&#39304;&#26469;&#23436;&#21892;AI&#27169;&#22411;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#23578;&#26410;&#24320;&#21457;&#20986;&#25551;&#36848;&#20154;&#31867;&#27604;&#36739;&#20309;&#26102;&#25913;&#21892;&#20256;&#32479;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#30340;&#29305;&#23450;&#26465;&#20214;&#30340;&#20840;&#38754;&#29702;&#35770;&#26694;&#26550;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#25928;&#21033;&#29992;&#20154;&#31867;&#27604;&#36739;&#26469;&#35299;&#20915;&#30001;&#22024;&#26434;&#25968;&#25454;&#21644;&#39640;&#32500;&#27169;&#22411;&#24341;&#36215;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#36890;&#36807;&#27010;&#29575;&#20108;&#20998;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#30340;&#20004;&#38454;&#27573;&#8220;&#30417;&#30563;&#24494;&#35843;+&#20154;&#31867;&#27604;&#36739;&#8221;&#65288;SFT+HC&#65289;&#26694;&#26550;&#12290;&#36825;&#20004;&#38454;&#27573;&#26694;&#26550;&#39318;&#20808;&#36890;&#36807;SFT&#36807;&#31243;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#28982;&#21518;&#21033;&#29992;&#20154;&#31867;&#27604;&#36739;&#26469;&#25913;&#36827;&#27169;&#22411;&#23545;&#40784;&#12290;&#20026;&#20102;&#26816;&#39564;&#23545;&#40784;&#38454;&#27573;&#30340;&#25928;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#31216;&#20026;&#8220;&#26631;&#31614;&#22122;&#22768;&#21040;&#19968;&#33268;&#24615;&#8221;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10771v1 Announce Type: new  Abstract: A growing trend involves integrating human knowledge into learning frameworks, leveraging subtle human feedback to refine AI models. Despite these advances, no comprehensive theoretical framework describing the specific conditions under which human comparisons improve the traditional supervised fine-tuning process has been developed. To bridge this gap, this paper studies the effective use of human comparisons to address limitations arising from noisy data and high-dimensional models. We propose a two-stage "Supervised Fine Tuning+Human Comparison" (SFT+HC) framework connecting machine learning with human feedback through a probabilistic bisection approach. The two-stage framework first learns low-dimensional representations from noisy-labeled data via an SFT procedure, and then uses human comparisons to improve the model alignment. To examine the efficacy of the alignment phase, we introduce a novel concept termed the "label-noise-to-co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32437;&#21521;&#35774;&#32622;&#20013;&#20351;&#29992;&#23553;&#38381;&#24418;&#24335;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#35268;&#21017;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10766</link><description>&lt;p&gt;
&#27178;&#21521;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#25512;&#26029;&#30340;ODE&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32437;&#21521;&#35774;&#32622;&#20013;&#20351;&#29992;&#23553;&#38381;&#24418;&#24335;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#65292;&#25512;&#26029;&#26080;&#20559;&#22788;&#29702;&#25928;&#24212;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#25105;&#20204;&#30340;&#31038;&#21306;&#38024;&#23545;&#26631;&#20934;&#35774;&#32622;&#12289;&#39640;&#32500;&#22788;&#29702;&#35774;&#32622;&#29978;&#33267;&#32437;&#21521;&#35774;&#32622;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#22810;&#26679;&#21270;&#65292;&#20294;&#22823;&#22810;&#20173;&#28982;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#26029;&#21644;&#21516;&#26102;&#32416;&#27491;&#20998;&#37197;&#20559;&#24046;&#12290;&#26032;&#26041;&#27861;&#36890;&#24120;&#26159;&#22312;&#20043;&#21069;&#30340;&#26041;&#27861;&#22522;&#30784;&#19978;&#25552;&#20986;&#26032;&#30340;&#65288;&#25110;&#25913;&#36827;&#30340;&#65289;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#32456;&#32467;&#26524;&#8212;&#8212;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#26426;&#22120;&#8212;&#8212;&#23578;&#26410;&#21463;&#21040;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32437;&#21521;&#35774;&#32622;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#34429;&#28982;&#25105;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#36830;&#32493;&#20248;&#21270;&#26469;&#23398;&#20064;ODE&#65292;&#20294;&#24471;&#21040;&#30340;&#25512;&#26029;&#26426;&#22120;&#19981;&#20877;&#26159;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#26679;&#20570;&#24102;&#26469;&#20102;&#20960;&#20010;&#20248;&#21183;&#65292;&#27604;&#22914;&#21487;&#35299;&#37322;&#24615;&#65292;&#19981;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10766v1 Announce Type: new  Abstract: Inferring unbiased treatment effects has received widespread attention in the machine learning community. In recent years, our community has proposed numerous solutions in standard settings, high-dimensional treatment settings, and even longitudinal settings. While very diverse, the solution has mostly relied on neural networks for inference and simultaneous correction of assignment bias. New approaches typically build on top of previous approaches by proposing new (or refined) architectures and learning algorithms. However, the end result -- a neural-network-based inference machine -- remains unchallenged. In this paper, we introduce a different type of solution in the longitudinal setting: a closed-form ordinary differential equation (ODE). While we still rely on continuous optimization to learn an ODE, the resulting inference machine is no longer a neural network. Doing so yields several advantages such as interpretability, irregular 
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.10763</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26356;&#24555;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Primal-Dual Algorithm for Faster Distributionally Robust Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10763
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24102;&#26377;&#38381;&#21512;&#12289;&#20984;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#24809;&#32602;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#38382;&#39064;&#65292;&#36825;&#20010;&#35774;&#32622;&#21253;&#25324;&#20102;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;$f$-DRO&#12289;Wasserstein-DRO&#21644;&#35889;/$L$-&#39118;&#38505;&#20844;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Drago&#65292;&#19968;&#31181;&#38543;&#26426;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#22312;&#24378;&#20984;-&#24378;&#20985;DRO&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;&#35813;&#26041;&#27861;&#23558;&#38543;&#26426;&#21270;&#21644;&#24490;&#29615;&#32452;&#20214;&#19982;&#23567;&#25209;&#37327;&#32467;&#21512;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;DRO&#20013;&#21407;&#22987;&#21644;&#23545;&#20598;&#38382;&#39064;&#30340;&#29420;&#29305;&#19981;&#23545;&#31216;&#24615;&#36136;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#31867;&#21644;&#22238;&#24402;&#20013;&#30340;&#25968;&#20540;&#22522;&#20934;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10763v1 Announce Type: cross  Abstract: We consider the penalized distributionally robust optimization (DRO) problem with a closed, convex uncertainty set, a setting that encompasses the $f$-DRO, Wasserstein-DRO, and spectral/$L$-risk formulations used in practice. We present Drago, a stochastic primal-dual algorithm that achieves a state-of-the-art linear convergence rate on strongly convex-strongly concave DRO problems. The method combines both randomized and cyclic components with mini-batching, which effectively handles the unique asymmetric nature of the primal and dual problems in DRO. We support our theoretical results with numerical benchmarks in classification and regression.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23545;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;&#36827;&#34892;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#20154;&#26426;&#22312;&#35266;&#23519;&#20219;&#21153;&#20013;&#30340;&#33322;&#32447;&#35268;&#21010;&#21644;&#20805;&#30005;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10761</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;
&lt;/p&gt;
&lt;p&gt;
Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23545;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;&#36827;&#34892;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#20154;&#26426;&#22312;&#35266;&#23519;&#20219;&#21153;&#20013;&#30340;&#33322;&#32447;&#35268;&#21010;&#21644;&#20805;&#30005;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#23545;&#20351;&#29992;&#26080;&#32447;&#20805;&#30005;&#22120;&#24310;&#38271;&#26080;&#20154;&#26426;&#25805;&#20316;&#23551;&#21629;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#21463;&#21040;&#20805;&#30005;&#22120;&#36741;&#21161;&#30340;&#26080;&#20154;&#26426;&#24212;&#29992;&#65306;&#26080;&#20154;&#26426;&#37096;&#32626;&#21040;&#35266;&#23519;&#19968;&#32452;&#20852;&#36259;&#28857;&#65292;&#32780;&#20805;&#30005;&#22120;&#21487;&#20197;&#31227;&#21160;&#20197;&#20026;&#26080;&#20154;&#26426;&#20805;&#30005;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;&#30340;&#33322;&#32447;&#21644;&#20805;&#30005;&#35745;&#21010;&#65292;&#20197;&#22312;&#23613;&#21487;&#33021;&#30701;&#30340;&#26102;&#38388;&#20869;&#33719;&#24471;&#39640;&#35266;&#23519;&#25928;&#29992;&#65292;&#21516;&#26102;&#30830;&#20445;&#26080;&#20154;&#26426;&#22312;&#20219;&#21153;&#25191;&#34892;&#26399;&#38388;&#20445;&#25345;&#21487;&#25805;&#20316;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26080;&#20154;&#26426;-&#20805;&#30005;&#22120;&#35843;&#24230;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#65292;&#22312;&#35813;&#36807;&#31243;&#20013;&#65292;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;&#20805;&#24403;&#20004;&#20010;&#20195;&#29702;&#21830;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#12290;&#36825;&#20004;&#20010;&#20195;&#29702;&#21830;&#30340;&#31163;&#25955;&#36830;&#32493;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#22312;&#25105;&#20204;&#30340;&#38382;&#39064;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#21160;&#20316;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10761v1 Announce Type: new  Abstract: Recently there has been a growing interest in industry and academia, regarding the use of wireless chargers to prolong the operational longevity of unmanned aerial vehicles (commonly knowns as drones). In this paper we consider a charger-assisted drone application: a drone is deployed to observe a set points of interest, while a charger can move to recharge the drone's battery. We focus on the route and charging schedule of the drone and the mobile charger, to obtain high observation utility with the shortest possible time, while ensuring the drone remains operational during task execution. Essentially, this proposed drone-charger scheduling problem is a multi-stage decision-making process, in which the drone and the mobile charger act as two agents who cooperate to finish a task. The discrete-continuous hybrid action space of the two agents poses a significant challenge in our problem. To address this issue, we present a hybrid-action d
&lt;/p&gt;</description></item><item><title>Latent Space Dynamics Identification (LaSDI)&#26694;&#26550;&#33021;&#23558;&#39640;&#20445;&#30495;&#25968;&#25454;&#36716;&#25442;&#20026;&#31616;&#21333;&#30340;ODEs&#65292;&#24182;&#19988;&#27599;&#20010;&#26500;&#20214;&#21487;&#20197;&#26681;&#25454;&#24212;&#29992;&#28789;&#27963;&#35843;&#33410;&#65292;&#20026;&#31616;&#32422;&#24314;&#27169;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10748</link><description>&lt;p&gt;
&#23545;&#20110;&#20405;&#20837;&#24615;&#21644;&#38750;&#20405;&#20837;&#24615;&#31616;&#32422;&#24314;&#27169;&#20013;&#28508;&#31354;&#38388;&#21160;&#24577;&#35782;&#21035;&#31639;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Latent Space Dynamics Identification Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10748
&lt;/p&gt;
&lt;p&gt;
Latent Space Dynamics Identification (LaSDI)&#26694;&#26550;&#33021;&#23558;&#39640;&#20445;&#30495;&#25968;&#25454;&#36716;&#25442;&#20026;&#31616;&#21333;&#30340;ODEs&#65292;&#24182;&#19988;&#27599;&#20010;&#26500;&#20214;&#21487;&#20197;&#26681;&#25454;&#24212;&#29992;&#28789;&#27963;&#35843;&#33410;&#65292;&#20026;&#31616;&#32422;&#24314;&#27169;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#27169;&#25311;&#29289;&#29702;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20027;&#35201;&#29942;&#39048;&#65292;&#36825;&#20419;&#20351;&#31616;&#21270;&#27169;&#22411;&#65288;ROMs&#65289;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;ROMs&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#27969;&#34892;&#65292;&#24182;&#19988;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;ROM&#26041;&#27861;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#29305;&#21035;&#26159;&#23545;&#20110;&#23545;&#27969;&#20027;&#23548;&#31995;&#32479;&#32780;&#35328;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#31216;&#20026;&#28508;&#31354;&#38388;&#21160;&#24577;&#35782;&#21035;&#65288;LaSDI&#65289;&#30340;&#29305;&#23450;&#26694;&#26550;&#65292;&#23427;&#23558;&#30001;PDE&#25511;&#21046;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#36716;&#25442;&#20026;&#30001;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#25511;&#21046;&#30340;&#26356;&#31616;&#21333;&#21644;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#25968;&#25454;&#12290;&#36825;&#20123;ODEs&#21487;&#20197;&#34987;&#23398;&#20064;&#65292;&#38543;&#21518;&#34987;&#25554;&#20540;&#20197;&#36827;&#34892;ROM&#39044;&#27979;&#12290;LaSDI&#30340;&#27599;&#20010;&#26500;&#20214;&#37117;&#21487;&#20197;&#26681;&#25454;&#24212;&#29992;&#31243;&#24207;&#36731;&#26494;&#35843;&#33410;&#65292;&#36825;&#20351;&#24471;LaSDI&#25104;&#20026;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10748v1 Announce Type: cross  Abstract: Numerical solvers of partial differential equations (PDEs) have been widely employed for simulating physical systems. However, the computational cost remains a major bottleneck in various scientific and engineering applications, which has motivated the development of reduced-order models (ROMs). Recently, machine-learning-based ROMs have gained significant popularity and are promising for addressing some limitations of traditional ROM methods, especially for advection dominated systems. In this chapter, we focus on a particular framework known as Latent Space Dynamics Identification (LaSDI), which transforms the high-fidelity data, governed by a PDE, to simpler and low-dimensional latent-space data, governed by ordinary differential equations (ODEs). These ODEs can be learned and subsequently interpolated to make ROM predictions. Each building block of LaSDI can be easily modulated depending on the application, which makes the LaSDI fr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26080;&#35270;&#35268;&#21010;&#26102;Horizon&#30340;&#30028;&#38480;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#30452;&#25509;&#20272;&#35745;&#20540;&#20989;&#25968;&#21644;&#32622;&#20449;&#21306;&#38388;&#65292;&#36991;&#20813;&#26174;&#24335;&#20272;&#35745;&#36716;&#25442;&#27169;&#22411;&#21644;&#35745;&#31639;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#38750;&#40784;&#27425;&#20540;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.10738</link><description>&lt;p&gt;
&#26080;&#35270;&#35268;&#21010;&#26102;Horizon&#30340;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Horizon-Free Regret for Linear Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26080;&#35270;&#35268;&#21010;&#26102;Horizon&#30340;&#30028;&#38480;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#30452;&#25509;&#20272;&#35745;&#20540;&#20989;&#25968;&#21644;&#32622;&#20449;&#21306;&#38388;&#65292;&#36991;&#20813;&#26174;&#24335;&#20272;&#35745;&#36716;&#25442;&#27169;&#22411;&#21644;&#35745;&#31639;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#38750;&#40784;&#27425;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31995;&#21015;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#36951;&#25022;&#30028;&#38480;&#21487;&#20197;&#65288;&#20960;&#20046;&#65289;&#29420;&#31435;&#20110;&#35268;&#21010;&#26102;Horizon&#65292;&#20063;&#23601;&#26159;&#26080;&#35270;Horizon&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36951;&#25022;&#30028;&#38480;&#20165;&#36866;&#29992;&#20110;&#20801;&#35768;&#23545;&#36716;&#25442;&#27169;&#22411;&#22823;&#23567;&#36827;&#34892;&#22810;&#39033;&#24335;&#20381;&#36182;&#30340;&#35774;&#32622;&#65292;&#20363;&#22914;&#34920;&#26684;&#22411;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21644;&#32447;&#24615;&#28151;&#21512;MDP&#12290;&#25105;&#20204;&#20026;&#27969;&#34892;&#30340;&#32447;&#24615;MDP&#35774;&#32622;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#35270;Horizon&#30340;&#30028;&#38480;&#65292;&#20854;&#20013;&#36716;&#25442;&#27169;&#22411;&#30340;&#22823;&#23567;&#21487;&#20197;&#26159;&#25351;&#25968;&#32423;&#22823;&#29978;&#33267;&#19981;&#21487;&#25968;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#26126;&#30830;&#20272;&#35745;&#36716;&#25442;&#27169;&#22411;&#24182;&#35745;&#31639;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#38750;&#40784;&#27425;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#30452;&#25509;&#20272;&#35745;&#20540;&#20989;&#25968;&#21644;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#65306;&#65288;1&#65289;&#32500;&#25252;&#29992;&#20110;&#20540;&#20989;&#25968;&#30340;&#22810;&#20010;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65307;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#32467;&#26500;&#24341;&#29702;&#26469;&#33719;&#24471;&#26080;&#35270;Horizon&#30340;&#30028;&#38480;&#65292;&#35813;&#24341;&#29702;&#34920;&#26126;&#38750;&#40784;&#27425;&#20540;&#20989;&#25968;&#30340;&#26368;&#22823;&#24635;&#21464;&#24322;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10738v1 Announce Type: new  Abstract: A recent line of works showed regret bounds in reinforcement learning (RL) can be (nearly) independent of planning horizon, a.k.a.~the horizon-free bounds. However, these regret bounds only apply to settings where a polynomial dependency on the size of transition model is allowed, such as tabular Markov Decision Process (MDP) and linear mixture MDP. We give the first horizon-free bound for the popular linear MDP setting where the size of the transition model can be exponentially large or even uncountable. In contrast to prior works which explicitly estimate the transition model and compute the inhomogeneous value functions at different time steps, we directly estimate the value functions and confidence sets. We obtain the horizon-free bound by: (1) maintaining multiple weighted least square estimators for the value functions; and (2) a structural lemma which shows the maximal total variation of the inhomogeneous value functions is bounde
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#21033;&#29992;&#22870;&#21169;&#20998;&#24067;&#26041;&#24046;&#21644;&#21464;&#21270;&#39044;&#31639;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#19978;&#38480;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.10732</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#24179;&#31283;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Variance-Dependent Regret Bounds for Non-stationary Linear Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#21033;&#29992;&#22870;&#21169;&#20998;&#24067;&#26041;&#24046;&#21644;&#21464;&#21270;&#39044;&#31639;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#19978;&#38480;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#24179;&#31283;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#20998;&#24067;&#27599;&#19968;&#36718;&#37117;&#22312;&#28436;&#21464;&#12290;&#29616;&#26377;&#31639;&#27861;&#36890;&#36807;&#24635;&#21464;&#21270;&#39044;&#31639;$B_K$&#26469;&#34920;&#24449;&#38750;&#24179;&#31283;&#24615;&#65292;&#35813;&#39044;&#31639;&#26159;&#32447;&#24615;&#36172;&#21338;&#26426;&#27599;$K$&#36718;&#36830;&#32493;&#29305;&#24449;&#21521;&#37327;&#21464;&#21270;&#30340;&#24635;&#21644;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#37327;&#21482;&#34913;&#37327;&#20102;&#30456;&#23545;&#20110;&#22870;&#21169;&#20998;&#24067;&#26399;&#26395;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#36825;&#20351;&#24471;&#29616;&#26377;&#31639;&#27861;&#22312;&#19968;&#33324;&#38750;&#24179;&#31283;&#20998;&#24067;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#22870;&#21169;&#20998;&#24067;&#26041;&#24046;&#20197;&#21450;$B_K$&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#19978;&#38480;&#30028;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;: &#37325;&#26032;&#21551;&#21160;&#30340;&#21152;&#26435;$\text{OFUL}^+$&#21644;&#37325;&#26032;&#21551;&#21160;&#30340;$\text{SAVE}^+$&#12290;&#36825;&#20123;&#31639;&#27861;&#20998;&#21035;&#22788;&#29702;&#20102;&#22870;&#21169;&#26041;&#24046;&#20449;&#24687;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10732v1 Announce Type: cross  Abstract: We investigate the non-stationary stochastic linear bandit problem where the reward distribution evolves each round. Existing algorithms characterize the non-stationarity by the total variation budget $B_K$, which is the summation of the change of the consecutive feature vectors of the linear bandits over $K$ rounds. However, such a quantity only measures the non-stationarity with respect to the expectation of the reward distribution, which makes existing algorithms sub-optimal under the general non-stationary distribution setting. In this work, we propose algorithms that utilize the variance of the reward distribution as well as the $B_K$, and show that they can achieve tighter regret upper bounds. Specifically, we introduce two novel algorithms: Restarted Weighted$\text{OFUL}^+$ and Restarted $\text{SAVE}^+$. These algorithms address cases where the variance information of the rewards is known and unknown, respectively. Notably, when
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26465;&#20214;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#39318;&#20808;&#35757;&#32451;&#25163;&#37096;&#29983;&#25104;&#22120;&#20135;&#29983;&#25163;&#37096;&#22270;&#20687;&#21644;&#20998;&#21106;&#25513;&#27169;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25913;&#36827;&#30340; ControlNet &#27169;&#22411;&#32472;&#21046;&#29983;&#25104;&#25163;&#37096;&#21608;&#22260;&#30340;&#36523;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.10731</link><description>&lt;p&gt;
&#19968;&#31181;&#20026;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#24110;&#21161;&#30340;&#26041;&#27861;&#65306;&#25913;&#36827;&#26465;&#20214;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10731
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26465;&#20214;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#39318;&#20808;&#35757;&#32451;&#25163;&#37096;&#29983;&#25104;&#22120;&#20135;&#29983;&#25163;&#37096;&#22270;&#20687;&#21644;&#20998;&#21106;&#25513;&#27169;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25913;&#36827;&#30340; ControlNet &#27169;&#22411;&#32472;&#21046;&#29983;&#25104;&#25163;&#37096;&#21608;&#22260;&#30340;&#36523;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#27493;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#26041;&#27861;&#22312;&#29983;&#25104;&#19968;&#33268;&#30340;&#25163;&#37096;&#35299;&#21078;&#32467;&#26500;&#26102;&#36935;&#21040;&#25361;&#25112;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#22270;&#20687;&#36890;&#24120;&#32570;&#20047;&#23545;&#25163;&#37096;&#23039;&#21183;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#23039;&#21183;&#26465;&#20214;&#30340;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#65292;&#23558;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#25163;&#30340;&#29983;&#25104;&#21644;&#38543;&#21518;&#22260;&#32469;&#25163;&#37096;&#36827;&#34892;&#36523;&#20307;&#22806;&#37096;&#32472;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22810;&#20219;&#21153;&#35774;&#32622;&#35757;&#32451;&#25163;&#37096;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#25163;&#37096;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#36866;&#24212;&#30340; ControlNet &#27169;&#22411;&#26469;&#32472;&#21046;&#21608;&#22260;&#30340;&#36523;&#20307;&#65292;&#29983;&#25104;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#25216;&#26415;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20445;&#30041;&#25163;&#37096;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10731v1 Announce Type: cross  Abstract: Recent years have seen significant progress in human image generation, particularly with the advancements in diffusion models. However, existing diffusion methods encounter challenges when producing consistent hand anatomy and the generated images often lack precise control over the hand pose. To address this limitation, we introduce a novel approach to pose-conditioned human image generation, dividing the process into two stages: hand generation and subsequent body out-painting around the hands. We propose training the hand generator in a multi-task setting to produce both hand images and their corresponding segmentation masks, and employ the trained model in the first stage of generation. An adapted ControlNet model is then used in the second stage to outpaint the body around the generated hands, producing the final result. A novel blending technique is introduced to preserve the hand details during the second stage that combines the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32933;&#26009;&#21709;&#24212;&#24615;&#30340;&#31649;&#29702;&#21306;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#27694;&#21709;&#24212;&#26354;&#32447;&#24182;&#36827;&#34892;&#29305;&#24449;&#21270;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.10730</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21019;&#24314;&#32933;&#26009;&#31649;&#29702;&#21306;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Analysis of Neural Networks Used to Create Fertilizer Management Zones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32933;&#26009;&#21709;&#24212;&#24615;&#30340;&#31649;&#29702;&#21306;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#27694;&#21709;&#24212;&#26354;&#32447;&#24182;&#36827;&#34892;&#29305;&#24449;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#20934;&#20892;&#19994;&#20013;&#65292;&#32771;&#34385;&#21040;&#30000;&#38388;&#21464;&#24322;&#24615;&#30340;&#31649;&#29702;&#21306;&#65288;MZs&#65289;&#30340;&#21033;&#29992;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;&#32933;&#26009;&#31649;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32933;&#26009;&#21709;&#24212;&#24615;&#30340;MZ&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20026;&#30000;&#22320;&#20013;&#30340;&#25152;&#26377;&#31449;&#28857;&#29983;&#25104;&#20102;&#27694;&#65288;N&#65289;&#21709;&#24212;&#26354;&#32447;&#65292;&#28982;&#21518;&#21033;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#20998;&#26512;&#23545;&#36817;&#20284;&#30340;N&#21709;&#24212;&#26354;&#32447;&#36827;&#34892;&#29305;&#24449;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10730v1 Announce Type: new  Abstract: In Precision Agriculture, the utilization of management zones (MZs) that take into account within-field variability facilitates effective fertilizer management. This approach enables the optimization of nitrogen (N) rates to maximize crop yield production and enhance agronomic use efficiency. However, existing works often neglect the consideration of responsivity to fertilizer as a factor influencing MZ determination. In response to this gap, we present a MZ clustering method based on fertilizer responsivity. We build upon the statement that the responsivity of a given site to the fertilizer rate is described by the shape of its corresponding N fertilizer-yield response (N-response) curve. Thus, we generate N-response curves for all sites within the field using a convolutional neural network (CNN). The shape of the approximated N-response curves is then characterized using functional principal component analysis. Subsequently, a counterf
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#27604;&#20363;&#39044;&#27979;&#19968;&#33268;&#24615;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#34987;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#35782;&#21035;&#21518;&#38376;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#24178;&#20928;&#25968;&#25454;&#25110;&#25163;&#21160;&#23450;&#20041;&#21518;&#38376;&#26816;&#27979;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.10717</link><description>&lt;p&gt;
&#25581;&#31034;&#21518;&#38376;&#31192;&#23494;&#65306;&#21033;&#29992;&#20248;&#21270;&#30340;&#27604;&#20363;&#39044;&#27979;&#19968;&#33268;&#24615;&#35782;&#21035;&#21518;&#38376;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10717
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#27604;&#20363;&#39044;&#27979;&#19968;&#33268;&#24615;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#34987;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#35782;&#21035;&#21518;&#38376;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#24178;&#20928;&#25968;&#25454;&#25110;&#25163;&#21160;&#23450;&#20041;&#21518;&#38376;&#26816;&#27979;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#24120;&#38656;&#35201;&#20511;&#21161;&#22806;&#37096;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20570;&#27861;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#27602;&#21270;&#25915;&#20987;&#12290;&#20808;&#21069;&#30340;&#21518;&#38376;&#38450;&#24481;&#31574;&#30053;&#20027;&#35201;&#38598;&#20013;&#22312;&#35782;&#21035;&#34987;&#26893;&#20837;&#21518;&#38376;&#30340;&#27169;&#22411;&#25110;&#34987;&#27602;&#21270;&#25968;&#25454;&#30340;&#29305;&#24449;&#19978;&#65292;&#36890;&#24120;&#22312;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#24178;&#20928;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36816;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#30456;&#23545;&#19981;&#22826;&#34987;&#25506;&#32034;&#30340;&#25361;&#25112;&#65306;&#22312;&#34987;&#27602;&#21270;&#30340;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#35782;&#21035;&#21518;&#38376;&#25968;&#25454;&#65292;&#32780;&#19988;&#22312;&#29616;&#23454;&#26465;&#20214;&#19979;&#65292;&#21363;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24178;&#20928;&#25968;&#25454;&#25110;&#19981;&#38656;&#35201;&#25163;&#21160;&#23450;&#20041;&#21518;&#38376;&#26816;&#27979;&#30340;&#38408;&#20540;&#12290;&#25105;&#20204;&#20174;&#27604;&#20363;&#39044;&#27979;&#19968;&#33268;&#24615;&#65288;SPC&#65289;&#25216;&#26415;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#34987;&#27602;&#21270;&#25968;&#25454;&#23545;&#36755;&#20837;&#32553;&#25918;&#22240;&#23376;&#30340;&#39044;&#27979;&#19981;&#21464;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#23558;&#21518;&#38376;&#25968;&#25454;&#35782;&#21035;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#23618;&#25968;&#25454;&#21010;&#20998;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10717v1 Announce Type: cross  Abstract: Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, i.e., without the need for additional clean data or without manually defining a threshold for backdoor detection. We draw an inspiration from the scaled prediction consistency (SPC) technique, which exploits the prediction invariance of poisoned data to an input scaling factor. Based on this, we pose the backdoor data identification problem as a hierarchical data splitting optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10707</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#38598;&#25104;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#30340;&#28508;&#22312;&#20027;&#39064;&#65306;&#27668;&#20505;&#36816;&#21160;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25581;&#31034;&#21644;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#37492;&#20110;&#20256;&#32479;&#20027;&#39064;&#32423;&#20998;&#26512;&#30340;&#23616;&#38480;&#24615;&#65292;&#24448;&#24448;&#21482;&#25429;&#25417;&#21040;&#25972;&#20307;&#27169;&#24335;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#26356;&#31934;&#32454;&#12289;&#20027;&#39064;&#32858;&#28966;&#30340;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#28041;&#21450;&#25163;&#21160;&#27969;&#31243;&#21644;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#22312;&#20280;&#32553;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36164;&#28304;&#24378;&#24230;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20808;&#36827;&#21151;&#33021;&#30340;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#26356;&#28145;&#20837;&#22320;&#35843;&#26597;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#30340;&#20027;&#39064;&#26041;&#38754;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#22810;&#26679;&#30340;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#26356;&#24191;&#27867;&#20027;&#39064;&#20869;&#26377;&#30340;&#24494;&#22937;&#32454;&#33410;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;PERL&#65289;&#65292;&#33021;&#22815;&#22312;&#19982;&#20256;&#32479;RLHF&#35774;&#32622;&#30456;&#24403;&#30340;&#24615;&#33021;&#19979;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10704</link><description>&lt;p&gt;
PERL: &#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PERL: Parameter Efficient Reinforcement Learning from Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10704
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;PERL&#65289;&#65292;&#33021;&#22815;&#22312;&#19982;&#20256;&#32479;RLHF&#35774;&#32622;&#30456;&#24403;&#30340;&#24615;&#33021;&#19979;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;RLHF&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#19988;&#25972;&#20010;&#36807;&#31243;&#22797;&#26434;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#65292;&#20854;&#20013;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#32993;&#31561;&#20154;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#8220;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#8221;&#65288;PERL&#65289;&#30340;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;LoRA&#36827;&#34892;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;PERL&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#65288;&#20840;&#35843;&#65289;&#22312;&#21253;&#25324;2&#20010;&#26032;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;7&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#22870;&#21169;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#21508;&#31181;&#37197;&#32622;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;PERL&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#30340;RLHF&#35774;&#32622;&#30456;&#24403;&#65292;&#21516;&#26102;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23569;&#12290;&#36825;&#20351;&#24471;RLHF&#20855;&#26377;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10704v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of "Parameter Efficient Reinforcement Learning" (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational 
&lt;/p&gt;</description></item><item><title>[V]-Mamba&#30456;&#23545;&#20110;ViTs&#22312;&#21033;&#29992;&#32447;&#24615;&#25506;&#26597;&#36827;&#34892;&#36801;&#31227;&#26102;&#23637;&#29616;&#20986;&#26356;&#20248;&#31168;&#25110;&#31561;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#22312;&#37319;&#29992;&#35270;&#35273;&#25552;&#31034;&#20316;&#20026;&#36801;&#31227;&#26041;&#27861;&#26102;&#34920;&#29616;&#20986;&#36739;&#24369;&#25110;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.10696</link><description>&lt;p&gt;
&#35770;[V]-Mamba&#30340;&#20302;&#26679;&#26412;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the low-shot transferability of [V]-Mamba
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10696
&lt;/p&gt;
&lt;p&gt;
[V]-Mamba&#30456;&#23545;&#20110;ViTs&#22312;&#21033;&#29992;&#32447;&#24615;&#25506;&#26597;&#36827;&#34892;&#36801;&#31227;&#26102;&#23637;&#29616;&#20986;&#26356;&#20248;&#31168;&#25110;&#31561;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#22312;&#37319;&#29992;&#35270;&#35273;&#25552;&#31034;&#20316;&#20026;&#36801;&#31227;&#26041;&#27861;&#26102;&#34920;&#29616;&#20986;&#36739;&#24369;&#25110;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#36866;&#24212;&#23569;&#37327;&#31034;&#20363;&#30340;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;Vision Transformers&#65288;ViTs&#65289;&#22312;&#19981;&#21516;&#32422;&#26463;&#19979;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20294;&#26412;&#30740;&#31350;&#23558;&#37325;&#28857;&#36716;&#31227;&#21040;&#20102;[V]-Mamba&#30340;&#36801;&#31227;&#23398;&#20064;&#28508;&#21147;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20854;&#22312;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#39044;&#31639;&#21644;&#39640;&#25928;&#36801;&#31227;&#26041;&#27861;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#20110;[V]-Mamba&#22312;&#23569;&#26679;&#26412;&#36801;&#31227;&#24615;&#33021;&#26041;&#38754;&#30340;&#20851;&#38190;&#35265;&#35299;&#65306;(a) &#22312;&#21033;&#29992;&#32447;&#24615;&#25506;&#26597;&#65288;LP&#65289;&#36827;&#34892;&#36801;&#31227;&#26102;&#65292;[V]-Mamba&#30456;&#36739;&#20110;ViTs&#23637;&#29616;&#20986;&#20102;&#26356;&#20248;&#31168;&#25110;&#31561;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;(b) &#30456;&#21453;&#65292;&#24403;&#37319;&#29992;&#35270;&#35273;&#25552;&#31034;&#65288;VP&#65289;&#20316;&#20026;&#36801;&#31227;&#26041;&#27861;&#26102;&#65292;[V]-Mamba&#23637;&#31034;&#20986;&#36739;&#24369;&#25110;&#31867;&#20284;&#20110;ViTs&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;(c) &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#36801;&#31227;&#24615;&#33021;&#24046;&#36317;&#26041;&#38754;&#23384;&#22312;&#30528;&#24494;&#24369;&#30340;&#27491;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10696v1 Announce Type: cross  Abstract: The strength of modern large-scale neural networks lies in their ability to efficiently adapt to new tasks with few examples. Although extensive research has investigated the transferability of Vision Transformers (ViTs) to various downstream tasks under diverse constraints, this study shifts focus to explore the transfer learning potential of [V]-Mamba. We compare its performance with ViTs across different few-shot data budgets and efficient transfer methods. Our analysis yields three key insights into [V]-Mamba's few-shot transfer performance: (a) [V]-Mamba demonstrates superior or equivalent few-shot learning capabilities compared to ViTs when utilizing linear probing (LP) for transfer, (b) Conversely, [V]-Mamba exhibits weaker or similar few-shot learning performance compared to ViTs when employing visual prompting (VP) as the transfer method, and (c) We observe a weak positive correlation between the performance gap in transfer vi
&lt;/p&gt;</description></item><item><title>MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2403.10691</link><description>&lt;p&gt;
MYTE&#65306;&#24418;&#24577;&#23398;&#39537;&#21160;&#30340;&#23383;&#33410;&#32534;&#30721;&#65292;&#29992;&#20110;&#26356;&#22909;&#12289;&#26356;&#20844;&#24179;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10691
&lt;/p&gt;
&lt;p&gt;
MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#22914;&#20309;&#26368;&#22909;&#22320;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#35789;&#27719;&#21644;&#25991;&#23383;&#30340;&#35821;&#35328;&#12290;&#23613;&#31649;&#24403;&#20195;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#19990;&#30028;&#25991;&#23383;&#31995;&#32479;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20559;&#21521;&#20110;&#20840;&#29699;&#35199;&#26041;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23569;&#25968;&#35821;&#35328;&#30340;&#25991;&#26412;&#24448;&#24448;&#34987;&#20998;&#21106;&#20026;&#19968;&#38271;&#20018;&#22312;&#35821;&#35328;&#23398;&#19978;&#27627;&#26080;&#24847;&#20041;&#30340;&#21333;&#20803;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#19981;&#24179;&#31561;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#36328;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#32534;&#30721;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#32422;&#23450;&#65288;MYTE&#65289;&#22522;&#20110;&#24418;&#24577;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24211;&#23384;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#27604;&#23383;&#31526;&#26356;&#24179;&#34913;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#23383;&#31526;&#12290;&#25105;&#20204;&#23637;&#31034;MYTE&#20026;&#25152;&#26377;99&#31181;&#20998;&#26512;&#35821;&#35328;&#20135;&#29983;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#20854;&#20013;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;&#36825;&#36827;&#32780;&#25913;&#21892;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#21040;&#35302;&#35273;-&#38899;&#39057;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28508;&#22312;&#29289;&#20307;&#29305;&#24449;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.10689</link><description>&lt;p&gt;
&#20855;&#26377;&#35270;&#35273;&#21040;&#35302;&#35273;-&#38899;&#39057;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#30340;&#28508;&#22312;&#29289;&#20307;&#29305;&#24449;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-modal Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10689
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#21040;&#35302;&#35273;-&#38899;&#39057;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28508;&#22312;&#29289;&#20307;&#29305;&#24449;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#26426;&#22120;&#20154;&#22788;&#29702;&#29289;&#20307;&#29305;&#24449;&#23545;&#20110;&#35843;&#25972;&#21160;&#20316;&#20197;&#30830;&#20445;&#19982;&#23481;&#22120;&#31283;&#23450;&#26377;&#25928;&#22320;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#28508;&#22312;&#30340;&#19981;&#21487;&#35265;&#29289;&#20307;&#29305;&#24449;&#65292;&#36890;&#36807;&#35270;&#35273;&#21040;&#35302;&#35273;-&#38899;&#39057;&#30340;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#31354;&#38388;&#36716;&#31227;&#21040;&#20165;&#20351;&#29992;&#35302;&#35273;-&#38899;&#39057;&#21644;&#36816;&#21160;&#25968;&#25454;&#35757;&#32451;&#30340;&#31532;&#20108;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10689v1 Announce Type: cross  Abstract: Recognising the characteristics of objects while a robot handles them is crucial for adjusting motions that ensure stable and efficient interactions with containers. Ahead of realising stable and efficient robot motions for handling/transferring the containers, this work aims to recognise the latent unobservable object characteristics. While vision is commonly used for object recognition by robots, it is ineffective for detecting hidden objects. However, recognising objects indirectly using other sensors is a challenging task. To address this challenge, we propose a cross-modal transfer learning approach from vision to haptic-audio. We initially train the model with vision, directly observing the target object. Subsequently, we transfer the latent space learned from vision to a second module, trained only with haptic-audio and motor data. This transfer learning framework facilitates the representation of object characteristics using in
&lt;/p&gt;</description></item><item><title>AutoHLS&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;HLS&#30828;&#20214;&#35774;&#35745;&#20248;&#21270;&#65292;&#23454;&#29616;&#39640;&#36798;70&#20493;&#30340;&#25506;&#32034;&#26102;&#38388;&#21152;&#36895;</title><link>https://arxiv.org/abs/2403.10686</link><description>&lt;p&gt;
AutoHLS&#65306;&#23398;&#20064;&#21152;&#36895;HLS&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10686
&lt;/p&gt;
&lt;p&gt;
AutoHLS&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;HLS&#30828;&#20214;&#35774;&#35745;&#20248;&#21270;&#65292;&#23454;&#29616;&#39640;&#36798;70&#20493;&#30340;&#25506;&#32034;&#26102;&#38388;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#32508;&#21512;&#65288;HLS&#65289;&#26159;&#19968;&#31181;&#35774;&#35745;&#27969;&#31243;&#65292;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#29305;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22914;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#65292;&#32487;&#25215;&#65292;&#27169;&#26495;&#31561;&#65292;&#24555;&#36895;&#21407;&#22411;&#21270;&#30828;&#20214;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#25506;&#32034;&#21508;&#31181;&#35774;&#35745;&#31354;&#38388;&#21442;&#25968;&#21487;&#33021;&#38656;&#35201;&#30828;&#20214;&#24037;&#31243;&#24072;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#35774;&#35745;&#35268;&#26684;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AutoHLS&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#38598;&#25104;&#65292;&#21152;&#36895;HLS&#30828;&#20214;&#35774;&#35745;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#19987;&#27880;&#20110;HLS pragma&#25506;&#32034;&#21644;&#25805;&#20316;&#36716;&#25442;&#12290;&#23427;&#21033;&#29992;&#38598;&#25104;&#30340;DNN&#26469;&#39044;&#27979;&#22312;&#32473;&#23450;FPGA&#36164;&#28304;&#39044;&#31639;&#19979;&#30340;&#32508;&#21512;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#30340;&#28508;&#21147;&#65292;&#21462;&#20195;AutoHLS&#27969;&#27700;&#32447;&#20013;&#30340;&#32463;&#20856;DNNs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25506;&#32034;&#26102;&#38388;&#19978;&#21152;&#36895;&#20102;&#22810;&#36798;70&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10686v1 Announce Type: cross  Abstract: High-level synthesis (HLS) is a design flow that leverages modern language features and flexibility, such as complex data structures, inheritance, templates, etc., to prototype hardware designs rapidly. However, exploring various design space parameters can take much time and effort for hardware engineers to meet specific design specifications. This paper proposes a novel framework called AutoHLS, which integrates a deep neural network (DNN) with Bayesian optimization (BO) to accelerate HLS hardware design optimization. Our tool focuses on HLS pragma exploration and operation transformation. It utilizes integrated DNNs to predict synthesizability within a given FPGA resource budget. We also investigate the potential of emerging quantum neural networks (QNNs) instead of classical DNNs for the AutoHLS pipeline. Our experimental results demonstrate up to a 70-fold speedup in exploration time.
&lt;/p&gt;</description></item><item><title>&#23545;GlassNet&#27169;&#22411;&#22312;&#39044;&#27979;&#29627;&#29827;&#31283;&#23450;&#24615;&#21442;&#25968;&#26041;&#38754;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#36825;&#20123;&#21442;&#25968;&#26469;&#20272;&#35745;&#29627;&#29827;&#30340;&#24418;&#25104;&#33021;&#21147;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10682</link><description>&lt;p&gt;
&#23545;GlassNet&#22312;&#29627;&#29827;&#31283;&#23450;&#24615;&#21644;&#24418;&#25104;&#33021;&#21147;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GlassNet for physics-informed machine learning of glass stability and glass-forming ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10682
&lt;/p&gt;
&lt;p&gt;
&#23545;GlassNet&#27169;&#22411;&#22312;&#39044;&#27979;&#29627;&#29827;&#31283;&#23450;&#24615;&#21442;&#25968;&#26041;&#38754;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#36825;&#20123;&#21442;&#25968;&#26469;&#20272;&#35745;&#29627;&#29827;&#30340;&#24418;&#25104;&#33021;&#21147;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29627;&#29827;&#26500;&#25104;&#20102;&#35768;&#22810;&#29616;&#20195;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#20063;&#22312;&#26410;&#26469;&#21307;&#30103;&#21644;&#29615;&#22659;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#24222;&#22823;&#30340;&#32452;&#25104;&#31354;&#38388;&#20351;&#24471;&#23545;&#26576;&#20123;&#24212;&#29992;&#36827;&#34892;&#35774;&#35745;&#21644;&#20248;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29627;&#29827;&#21152;&#24037;&#29305;&#21035;&#37325;&#35201;&#30340;&#26159;&#20272;&#35745;&#32473;&#23450;&#32452;&#25104;&#30340;&#29627;&#29827;&#25104;&#24418;&#33021;&#21147;&#65288;GFA&#65289;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#29627;&#29827;&#24418;&#25104;&#30340;&#29289;&#29702;&#26426;&#21046;&#20173;&#23384;&#22312;&#35768;&#22810;&#26410;&#35299;&#20043;&#35868;&#65292;&#29305;&#21035;&#26159;&#22312;&#27687;&#21270;&#29627;&#29827;&#20013;&#12290;&#26174;&#32780;&#26131;&#35265;&#65292;&#29992;&#20110;&#20272;&#35745;GFA&#30340;&#20195;&#29702;&#23646;&#24615;&#23558;&#22312;&#29627;&#29827;&#21152;&#24037;&#21644;&#35774;&#35745;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#23547;&#25214;&#36825;&#26679;&#19968;&#20010;&#26367;&#20195;&#24615;&#23646;&#24615;&#24050;&#34987;&#35777;&#26126;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#39044;&#35757;&#32451;NN&#27169;&#22411;GlassNet&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#35745;&#31639;&#29627;&#29827;&#31283;&#23450;&#24615;&#65288;GS&#65289;&#25152;&#38656;&#30340;&#29305;&#24449;&#28201;&#24230;&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#36825;&#20123;&#29289;&#29702;&#21551;&#21457;&#24335;ML&#65288;PIML&#65289;&#39044;&#27979;&#30340;GS&#21442;&#25968;&#26469;&#20272;&#35745;&#24418;&#25104;&#33021;&#21147;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10682v1 Announce Type: cross  Abstract: Glasses form the basis of many modern applications and also hold great potential for future medical and environmental applications. However, their structural complexity and large composition space make design and optimization challenging for certain applications. Of particular importance for glass processing is an estimate of a given composition's glass-forming ability (GFA). However, there remain many open questions regarding the physical mechanisms of glass formation, especially in oxide glasses. It is apparent that a proxy for GFA would be highly useful in glass processing and design, but identifying such a surrogate property has proven itself to be difficult. Here, we explore the application of an open-source pre-trained NN model, GlassNet, that can predict the characteristic temperatures necessary to compute glass stability (GS) and assess the feasibility of using these physics-informed ML (PIML)-predicted GS parameters to estimat
&lt;/p&gt;</description></item><item><title>RFMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#27969;&#21305;&#37197;&#30340;&#20248;&#21183;&#22312;&#26426;&#22120;&#20154;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#20013;&#20855;&#26377;&#39640;&#25928;&#35757;&#32451;&#21644;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#20960;&#20309;&#24847;&#35782;&#65292;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#21160;&#20316;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2403.10672</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#30340;&#40654;&#26364;&#27969;&#21305;&#37197;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Riemannian Flow Matching Policy for Robot Motion Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10672
&lt;/p&gt;
&lt;p&gt;
RFMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#27969;&#21305;&#37197;&#30340;&#20248;&#21183;&#22312;&#26426;&#22120;&#20154;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#20013;&#20855;&#26377;&#39640;&#25928;&#35757;&#32451;&#21644;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#20960;&#20309;&#24847;&#35782;&#65292;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#21160;&#20316;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#40654;&#26364;&#27969;&#21305;&#37197;&#31574;&#30053;&#65288;RFMP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#21512;&#25104;&#26426;&#22120;&#20154;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#12290;RFMP&#21033;&#29992;&#20102;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#39640;&#25928;&#35757;&#32451;&#21644;&#25512;&#26029;&#33021;&#21147;&#12290;&#36890;&#36807;&#35774;&#35745;&#65292;RFMP&#32487;&#25215;&#20102;&#27969;&#21305;&#37197;&#30340;&#20248;&#21183;&#65306;&#33021;&#22815;&#32534;&#30721;&#39640;&#32500;&#24230;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24120;&#35265;&#65292;&#24182;&#19988;&#20855;&#26377;&#38750;&#24120;&#31616;&#21333;&#21644;&#24555;&#36895;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RFMP&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#35270;&#35273;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#31574;&#30053;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#26426;&#22120;&#20154;&#29366;&#24577;&#23384;&#22312;&#20110;&#40654;&#26364;&#27969;&#24418;&#19978;&#65292;RFMP&#22312;&#26412;&#36136;&#19978;&#34701;&#21512;&#20102;&#20960;&#20309;&#24847;&#35782;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;RFMP&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#27010;&#24565;&#39564;&#35777;&#23454;&#39564;&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#25193;&#25955;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23613;&#31649;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#25152;&#32771;&#34385;&#30340;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;RFMP&#25552;&#20379;&#20102;&#26356;&#24179;&#28369;&#30340;&#21160;&#20316;&#36712;&#36857;&#65292;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10672v1 Announce Type: cross  Abstract: We introduce Riemannian Flow Matching Policies (RFMP), a novel model for learning and synthesizing robot visuomotor policies. RFMP leverages the efficient training and inference capabilities of flow matching methods. By design, RFMP inherits the strengths of flow matching: the ability to encode high-dimensional multimodal distributions, commonly encountered in robotic tasks, and a very simple and fast inference process. We demonstrate the applicability of RFMP to both state-based and vision-conditioned robot motion policies. Notably, as the robot state resides on a Riemannian manifold, RFMP inherently incorporates geometric awareness, which is crucial for realistic robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments, comparing its performance against Diffusion Policies. Although both approaches successfully learn the considered tasks, our results show that RFMP provides smoother action trajectories with signifi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;Hessian&#35745;&#31639;&#21644;&#27714;&#36870;&#30340;Hessian-Free Laplace&#36817;&#20284;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25968;&#21518;&#39564;&#21644;&#32593;&#32476;&#39044;&#27979;&#30340;&#26354;&#29575;&#26469;&#20272;&#35745;&#21518;&#39564;&#30340;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.10671</link><description>&lt;p&gt;
Bayesian&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26080;Hessian-Laplace
&lt;/p&gt;
&lt;p&gt;
Hessian-Free Laplace in Bayesian Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;Hessian&#35745;&#31639;&#21644;&#27714;&#36870;&#30340;Hessian-Free Laplace&#36817;&#20284;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25968;&#21518;&#39564;&#21644;&#32593;&#32476;&#39044;&#27979;&#30340;&#26354;&#29575;&#26469;&#20272;&#35745;&#21518;&#39564;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;Laplace&#36817;&#20284;&#65288;LA&#65289;&#26159;&#20197;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#20026;&#20013;&#24515;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#23427;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21560;&#24341;&#21147;&#28304;&#20110;&#33021;&#22815;&#22312;&#26631;&#20934;&#32593;&#32476;&#21442;&#25968;&#20248;&#21270;&#20043;&#21518;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65288;&#21363;&#20107;&#21518;&#65289;&#65292;&#20174;&#36817;&#20284;&#21518;&#39564;&#20013;&#25277;&#26679;&#30340;&#20415;&#21033;&#24615;&#20197;&#21450;&#27169;&#22411;&#35777;&#25454;&#30340;&#35299;&#26512;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;LA&#30340;&#19968;&#20010;&#37325;&#35201;&#35745;&#31639;&#29942;&#39048;&#26159;&#24517;&#39035;&#35745;&#31639;&#21644;&#27714;&#36870;&#23545;&#25968;&#21518;&#39564;&#30340;Hessian&#30697;&#38453;&#12290;Hessian&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#36817;&#20284;&#65292;&#36136;&#37327;&#19982;&#32593;&#32476;&#12289;&#25968;&#25454;&#38598;&#21644;&#25512;&#26029;&#20219;&#21153;&#31561;&#22810;&#20010;&#22240;&#32032;&#26377;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32469;&#36807;Hessian&#35745;&#31639;&#21644;&#27714;&#36870;&#30340;&#26367;&#20195;&#26694;&#26550;&#12290;&#26080;Hessian-Laplace&#65288;HFL&#65289;&#36817;&#20284;&#20351;&#29992;&#23545;&#25968;&#21518;&#39564;&#21644;&#32593;&#32476;&#39044;&#27979;&#30340;&#26354;&#29575;&#26469;&#20272;&#35745;&#20854;&#26041;&#24046;&#12290;&#21482;&#38656;&#35201;&#20004;&#20010;&#28857;&#20272;&#35745;&#65306;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#21644;&#31561;&#20215;&#30340;&#26354;&#29575;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10671v1 Announce Type: cross  Abstract: The Laplace approximation (LA) of the Bayesian posterior is a Gaussian distribution centered at the maximum a posteriori estimate. Its appeal in Bayesian deep learning stems from the ability to quantify uncertainty post-hoc (i.e., after standard network parameter optimization), the ease of sampling from the approximate posterior, and the analytic form of model evidence. However, an important computational bottleneck of LA is the necessary step of calculating and inverting the Hessian matrix of the log posterior. The Hessian may be approximated in a variety of ways, with quality varying with a number of factors including the network, dataset, and inference task. In this paper, we propose an alternative framework that sidesteps Hessian calculation and inversion. The Hessian-free Laplace (HFL) approximation uses curvature of both the log posterior and network prediction to estimate its variance. Only two point estimates are needed: the st
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#27700;&#21360;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#23545;&#28304;&#27169;&#22411;&#21151;&#33021;&#30340;&#31363;&#21462;&#25915;&#20987;</title><link>https://arxiv.org/abs/2403.10663</link><description>&lt;p&gt;
&#19981;&#20165;&#25913;&#21464;&#26631;&#31614;&#65292;&#23398;&#20064;&#29305;&#24449;&#65306;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10663
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#27700;&#21360;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#23545;&#28304;&#27169;&#22411;&#21151;&#33021;&#30340;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#24179;&#21488;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27700;&#21360;&#25216;&#26415;&#12290;&#36825;&#20123;&#26041;&#27861;&#29992;&#20110;&#39564;&#35777;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#12290;&#26412;&#25991;&#39318;&#20808;&#20174;&#29305;&#24449;&#23398;&#20064;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35302;&#21457;&#38598;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#36873;&#25321;&#23637;&#31034;&#22810;&#20010;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#20063;&#34987;&#31216;&#20026;$\textit{&#22810;&#35270;&#35282;&#25968;&#25454;}$&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10663v1 Announce Type: cross  Abstract: With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as $\textit{multi-view data}$, it becomes feasible to effectively defend 
&lt;/p&gt;</description></item><item><title>InterLUDE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#37096;&#20998;&#30456;&#20114;&#20316;&#29992;&#26469;&#22686;&#24378;SSL&#65292;&#21253;&#25324;&#23884;&#20837;&#34701;&#21512;&#21644;&#22522;&#20110;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#21307;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.10658</link><description>&lt;p&gt;
InterLUDE&#65306;&#26631;&#35760;&#25968;&#25454;&#19982;&#26410;&#26631;&#35760;&#25968;&#25454;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20197;&#22686;&#24378;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10658
&lt;/p&gt;
&lt;p&gt;
InterLUDE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#37096;&#20998;&#30456;&#20114;&#20316;&#29992;&#26469;&#22686;&#24378;SSL&#65292;&#21253;&#25324;&#23884;&#20837;&#34701;&#21512;&#21644;&#22522;&#20110;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#21307;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#26631;&#35760;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#22270;&#20687;&#20998;&#31867;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#20013;&#23558;&#30417;&#30563;&#20998;&#31867;&#30446;&#26631;&#19982;&#20165;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#23548;&#20986;&#30340;&#27491;&#21017;&#21270;&#39033;&#30456;&#21152;&#12290; &#36825;&#31181;&#34920;&#36798;&#26041;&#24335;&#24573;&#30053;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#22270;&#20687;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;InterLUDE&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;SSL&#26041;&#27861;&#65292;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#21463;&#30410;&#20110;&#26631;&#35760;-&#26410;&#26631;&#35760;&#20132;&#20114;&#12290; &#31532;&#19968;&#37096;&#20998;&#26159;&#23884;&#20837;&#34701;&#21512;&#65292;&#23427;&#22312;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#23884;&#20837;&#20043;&#38388;&#25554;&#20540;&#20197;&#25913;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290; &#31532;&#20108;&#37096;&#20998;&#26159;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22522;&#20110;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21407;&#21017;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#27169;&#22411;&#22312;&#26631;&#35760;&#19982;&#26410;&#26631;&#35760;&#36755;&#20837;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#24322;&#12290; &#22312;&#26631;&#20934;&#38381;&#38598;SSL&#22522;&#20934;&#27979;&#35797;&#21644;&#21307;&#23398;SSL&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;InterLUDE&#33021;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10658v1 Announce Type: cross  Abstract: Semi-supervised learning (SSL) seeks to enhance task performance by training on both labeled and unlabeled data. Mainstream SSL image classification methods mostly optimize a loss that additively combines a supervised classification objective with a regularization term derived solely from unlabeled data. This formulation neglects the potential for interaction between labeled and unlabeled images. In this paper, we introduce InterLUDE, a new approach to enhance SSL made of two parts that each benefit from labeled-unlabeled interaction. The first part, embedding fusion, interpolates between labeled and unlabeled embeddings to improve representation learning. The second part is a new loss, grounded in the principle of consistency regularization, that aims to minimize discrepancies in the model's predictions between labeled versus unlabeled inputs. Experiments on standard closed-set SSL benchmarks and a medical SSL task with an uncurated u
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23376;&#32676;&#38408;&#20540;&#20248;&#21270;&#65288;STO&#65289;&#25216;&#26415;&#65292;&#22312;&#19981;&#25913;&#21160;&#35757;&#32451;&#25968;&#25454;&#21644;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#21508;&#20010;&#23376;&#32676;&#30340;&#20998;&#31867;&#38408;&#20540;&#65292;&#20197;&#26368;&#23567;&#21270;&#25972;&#20307;&#27495;&#35270;&#20998;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20449;&#36151;&#20511;&#36151;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10652</link><description>&lt;p&gt;
&#20351;&#29992;&#23376;&#32676;&#38408;&#20540;&#20248;&#21270;&#25552;&#39640;&#20449;&#36151;&#20511;&#36151;&#27169;&#22411;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10652
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23376;&#32676;&#38408;&#20540;&#20248;&#21270;&#65288;STO&#65289;&#25216;&#26415;&#65292;&#22312;&#19981;&#25913;&#21160;&#35757;&#32451;&#25968;&#25454;&#21644;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#21508;&#20010;&#23376;&#32676;&#30340;&#20998;&#31867;&#38408;&#20540;&#65292;&#20197;&#26368;&#23567;&#21270;&#25972;&#20307;&#27495;&#35270;&#20998;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20449;&#36151;&#20511;&#36151;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25552;&#39640;&#20449;&#36151;&#20511;&#36151;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#65292;&#35768;&#22810;&#37329;&#34701;&#26426;&#26500;&#29616;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#39044;&#27979;&#26377;&#21487;&#33021;&#23545;&#20154;&#21475;&#30340;&#26576;&#20123;&#23376;&#32676;&#20307;&#23384;&#22312;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20960;&#31181;&#25216;&#26415;&#26469;&#28040;&#38500;&#20559;&#35265;&#65292;&#25552;&#39640;&#39044;&#27979;&#30340;&#25972;&#20307;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Subgroup Threshold Optimizer&#8221;&#65288;&#31616;&#31216;STO&#65289;&#30340;&#26032;&#30340;&#20844;&#24179;&#24615;&#25216;&#26415;&#65292;&#23427;&#19981;&#38656;&#35201;&#23545;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#65292;&#20063;&#19981;&#38656;&#35201;&#23545;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20219;&#20309;&#26356;&#25913;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#19968;&#36215;&#20351;&#29992;&#12290;STO&#36890;&#36807;&#20248;&#21270;&#21508;&#20010;&#23376;&#32676;&#30340;&#20998;&#31867;&#38408;&#20540;&#65292;&#20197;&#26368;&#23567;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#25972;&#20307;&#27495;&#35270;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10652v1 Announce Type: new  Abstract: In an effort to improve the accuracy of credit lending decisions, many financial intuitions are now using predictions from machine learning models. While such predictions enjoy many advantages, recent research has shown that the predictions have the potential to be biased and unfair towards certain subgroups of the population. To combat this, several techniques have been introduced to help remove the bias and improve the overall fairness of the predictions. We introduce a new fairness technique, called \textit{Subgroup Threshold Optimizer} (\textit{STO}), that does not require any alternations to the input training data nor does it require any changes to the underlying machine learning algorithm, and thus can be used with any existing machine learning pipeline. STO works by optimizing the classification thresholds for individual subgroups in order to minimize the overall discrimination score between them. Our experiments on a real-world 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.10650</link><description>&lt;p&gt;
PALM&#65306;&#25512;&#36827;&#29992;&#20110;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#27169;&#22411;&#38754;&#20020;&#39046;&#22495;&#20998;&#24067;&#30340;&#24555;&#36895;&#36716;&#21464;&#65292;&#23548;&#33268;&#35782;&#21035;&#24615;&#33021;&#19979;&#38477;&#12290;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTTA&#65289;&#30452;&#25509;&#26681;&#25454;&#27979;&#35797;&#25968;&#25454;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#28304;&#21028;&#21035;&#27169;&#22411;&#20197;&#36866;&#24212;&#36825;&#20123;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#12290;&#19968;&#31181;&#39640;&#24230;&#26377;&#25928;&#30340;CTTA&#26041;&#27861;&#28041;&#21450;&#24212;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#35843;&#25972;&#39044;&#35757;&#32451;&#23618;&#12290;&#28982;&#32780;&#65292;&#23427;&#21463;&#21040;&#39046;&#22495;&#36716;&#31227;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#30001;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#25152;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#23618;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#23618;&#65292;&#32780;&#26080;&#39035;&#20381;&#36182;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#30340;&#22823;&#23567;&#20316;&#20026;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;softmax&#36755;&#20986;&#19982;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#35745;&#31639;&#65292;&#20197;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#12290;&#38543;&#21518;&#65292;&#20165;&#23646;&#20110;&#36825;&#20123;&#23618;&#30340;&#21442;&#25968;&#23558;&#34987;&#36827;&#19968;&#27493;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10650v1 Announce Type: cross  Abstract: Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Continual test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains using test data. A highly effective CTTA method involves applying layer-wise adaptive learning rates, and selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. In this work, we aim to overcome these limitations by identifying layers through the quantification of model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#28304;&#20195;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#65292;&#32780;&#20998;&#35789;&#22120;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#26159;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10646</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#28304;&#20195;&#30721;&#34920;&#31034;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#28304;&#20195;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#65292;&#32780;&#20998;&#35789;&#22120;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#26159;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10646v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28304;&#20195;&#30721;&#30340;&#34920;&#31034;&#26159;&#35813;&#25216;&#26415;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#23398;&#20064;&#28304;&#20195;&#30721;&#29305;&#24449;&#30340;&#26041;&#24335;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#36825;&#20123;&#25216;&#26415;&#34987;&#24320;&#21457;&#65292;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#23545;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#24050;&#26377;&#20869;&#23481;&#21644;&#23578;&#26410;&#23384;&#22312;&#30340;&#20869;&#23481;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#26412;&#25991;&#23545;&#36825;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;ML&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#21644;&#32534;&#31243;&#35821;&#35328;&#25152;&#20351;&#29992;&#30340;&#34920;&#31034;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#34920;&#31034;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#31867;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#34920;&#31034;&#31867;&#21035;&#65292;&#32780;&#20998;&#35789;&#22120;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;ASTs&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#26368;&#21463;&#27426;&#36814;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10646v1 Announce Type: new  Abstract: Machine learning techniques for cybersecurity-related software engineering tasks are becoming increasingly popular. The representation of source code is a key portion of the technique that can impact the way the model is able to learn the features of the source code. With an increasing number of these techniques being developed, it is valuable to see the current state of the field to better understand what exists and what's not there yet. This paper presents a study of these existing ML-based approaches and demonstrates what type of representations were used for different cybersecurity tasks and programming languages. Additionally, we study what types of models are used with different representations. We have found that graph-based representations are the most popular category of representation, and Tokenizers and Abstract Syntax Trees (ASTs) are the two most popular representations overall. We also found that the most popular cybersecur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#31070;&#32463;&#31639;&#23376;&#26469;&#25552;&#39640;&#23545;&#21306;&#22495;&#22806;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;OOD&#27979;&#35797;&#36755;&#20837;&#19978;&#30340;&#22833;&#36133;</title><link>https://arxiv.org/abs/2403.10642</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#34920;&#24449;&#21644;&#25913;&#36827;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#21306;&#22495;&#22806;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#31070;&#32463;&#31639;&#23376;&#26469;&#25552;&#39640;&#23545;&#21306;&#22495;&#22806;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;OOD&#27979;&#35797;&#36755;&#20837;&#19978;&#30340;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#39046;&#22495;&#20013;&#30340;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#35299;&#31639;&#31526;&#21487;&#20197;&#20026;&#32463;&#20856;&#25968;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27714;&#35299;&#22120;&#25552;&#20379;&#19968;&#20010;&#24555;&#36895;&#30340;&#36817;&#20284;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#20854;&#20013;&#65292;&#31070;&#32463;&#31639;&#23376;&#65288;NOs&#65289;&#24050;&#32463;&#34987;&#35748;&#20026;&#23588;&#20026;&#20855;&#26377;&#21069;&#26223;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#21306;&#22495;&#22806;&#65288;OOD&#65289;&#27979;&#35797;&#36755;&#20837;&#65292;&#20960;&#31181;NOs&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#29978;&#33267;&#22312;&#27169;&#22411;&#23545;&#20110;&#22495;&#20869;&#20219;&#21153;&#30340;&#35299;&#36817;&#20284;&#33391;&#22909;&#26102;&#20063;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38598;&#25104;&#20960;&#20010;NOs&#21487;&#20197;&#35782;&#21035;&#39640;&#35823;&#24046;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#33391;&#22909;&#19982;&#39044;&#27979;&#35823;&#24046;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;DiverseNO&#65292;&#36890;&#36807;&#40723;&#21169;&#20854;&#26368;&#21518;&#21069;&#21521;&#20256;&#25773;&#23618;&#20013;&#30340;&#22810;&#20010;&#22836;&#37096;&#36827;&#34892;&#22810;&#26679;&#21270;&#39044;&#27979;&#26469;&#27169;&#25311;&#38598;&#25104;&#30340;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10642v1 Announce Type: new  Abstract: Existing work in scientific machine learning (SciML) has shown that data-driven learning of solution operators can provide a fast approximate alternative to classical numerical partial differential equation (PDE) solvers. Of these, Neural Operators (NOs) have emerged as particularly promising. We observe that several uncertainty quantification (UQ) methods for NOs fail for test inputs that are even moderately out-of-domain (OOD), even when the model approximates the solution well for in-domain tasks. To address this limitation, we show that ensembling several NOs can identify high-error regions and provide good uncertainty estimates that are well-correlated with prediction errors. Based on this, we propose a cost-effective alternative, DiverseNO, that mimics the properties of the ensemble by encouraging diverse predictions from its multiple heads in the last feed-forward layer. We then introduce Operator-ProbConserv, a method that uses t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#26080;&#23478;&#21487;&#24402;&#32773;&#21644;&#39135;&#21697;&#38134;&#34892;&#30340;&#36164;&#28304;&#21463;&#38480;&#22806;&#23637;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Thompson&#25277;&#26679;&#19982;&#39532;&#23572;&#21487;&#22827;&#38142;&#24674;&#22797;&#30340;&#31639;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10638</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#26080;&#23478;&#21487;&#24402;&#32773;&#34903;&#22836;&#22806;&#23637;&#21644;&#25910;&#38598;&#21487;&#39135;&#39135;&#29289;&#30340;&#36164;&#28304;&#21463;&#38480;&#38543;&#26426;&#35843;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A resource-constrained stochastic scheduling algorithm for homeless street outreach and gleaning edible food
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#26080;&#23478;&#21487;&#24402;&#32773;&#21644;&#39135;&#21697;&#38134;&#34892;&#30340;&#36164;&#28304;&#21463;&#38480;&#22806;&#23637;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Thompson&#25277;&#26679;&#19982;&#39532;&#23572;&#21487;&#22827;&#38142;&#24674;&#22797;&#30340;&#31639;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#31038;&#20250;&#21464;&#38761;&#32452;&#32455;&#22312;&#36164;&#28304;&#21463;&#38480;&#22806;&#23637;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#32452;&#32455;&#25317;&#26377;&#19981;&#21516;&#30340;&#20351;&#21629;&#21644;&#36816;&#33829;&#26041;&#24335;&#65306;Breaking Ground&#8212;&#8212;&#19968;&#23478;&#24110;&#21161;&#32445;&#32422;&#26080;&#23478;&#21487;&#24402;&#32773;&#36807;&#28193;&#33267;&#27704;&#20037;&#20303;&#25151;&#30340;&#32452;&#32455;&#21644;&#20197;&#33394;&#21015;&#20840;&#22269;&#39135;&#21697;&#38134;&#34892;Leket&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#38024;&#23545;&#37096;&#20998;&#35266;&#27979;&#30340;&#21608;&#26399;&#24615;&#19981;&#23433;&#23450;&#36172;&#24466;&#38382;&#39064;&#22312;$k$-&#27493;&#36716;&#31227;&#19979;&#24320;&#21457;&#20102;&#19968;&#31181;&#20272;&#35745;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;Thompson&#25277;&#26679;&#19982;&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#38142;&#24674;&#22797;&#65288;&#36890;&#36807;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65289;&#30340;&#31639;&#27861;&#22312;&#20004;&#20010;&#32452;&#32455;&#30340;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#12290;&#25105;&#20204;&#20197;&#36828;&#26223;&#26041;&#24335;&#36827;&#34892;&#20102;&#36825;&#39033;&#24037;&#20316;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#36275;&#22815;&#28789;&#27963;&#20294;&#20063;&#36275;&#22815;&#26377;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#20811;&#26381;&#23545;&#25968;&#25454;&#21487;&#25345;&#32493;&#24433;&#21709;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10638v1 Announce Type: new  Abstract: We developed a common algorithmic solution addressing the problem of resource-constrained outreach encountered by social change organizations with different missions and operations: Breaking Ground -- an organization that helps individuals experiencing homelessness in New York transition to permanent housing and Leket -- the national food bank of Israel that rescues food from farms and elsewhere to feed the hungry. Specifically, we developed an estimation and optimization approach for partially-observed episodic restless bandits under $k$-step transitions. The results show that our Thompson sampling with Markov chain recovery (via Stein variational gradient descent) algorithm significantly outperforms baselines for the problems of both organizations. We carried out this work in a prospective manner with the express goal of devising a flexible-enough but also useful-enough solution that can help overcome a lack of sustainable impact in da
&lt;/p&gt;</description></item><item><title>&#20272;&#35745;&#20013;&#20301;&#25968;&#24046;&#24322;&#27604;&#20272;&#35745;&#20013;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#26356;&#23481;&#26131;&#30340;&#38382;&#39064;&#26159;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10618</link><description>&lt;p&gt;
&#36817;&#20284;&#20013;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Limits of Approximating the Median Treatment Effect
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10618
&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#20013;&#20301;&#25968;&#24046;&#24322;&#27604;&#20272;&#35745;&#20013;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#26356;&#23481;&#26131;&#30340;&#38382;&#39064;&#26159;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#30340;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#19968;&#20010;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#24182;&#19981;&#19968;&#23450;&#33021;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#20272;&#35745;&#20998;&#20301;&#22788;&#29702;&#25928;&#24212;&#12290;&#22312;&#21253;&#21547;$n$&#20010;&#20010;&#20307;&#30340;&#26377;&#38480;&#20154;&#32676;&#35774;&#32622;&#20013;&#65292;&#27835;&#30103;&#21644;&#23545;&#29031;&#20540;&#29992;&#28508;&#22312;&#32467;&#26524;&#21521;&#37327;$\mathbf{a}, \mathbf{b}$&#34920;&#31034;&#65292;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#20272;&#35745;median$(\mathbf{a}) -$ median$(\mathbf{b})$&#65292;&#20854;&#20013;median($\mathbf x$)&#34920;&#31034;&#21521;&#37327;$\mathbf x$&#20013;&#25152;&#26377;&#20540;&#25490;&#24207;&#21518;&#30340;&#20013;&#20301;&#25968;&#20540;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20272;&#35745;&#20013;&#20301;&#25968;&#24046;&#24322;&#27604;&#20272;&#35745;&#20013;&#20301;&#25968;$(\mathbf{a-b})$&#26356;&#23481;&#26131;&#65292;&#21363;&#25152;&#35859;&#30340;&#20013;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#65288;MTE&#65289;&#12290;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#23545;&#20110;&#27599;&#20010;&#20010;&#20307;$i$&#65292;&#25105;&#20204;&#21482;&#33021;&#35266;&#27979;&#21040;&#28508;&#22312;&#32467;&#26524;&#20540;&#20043;&#19968;&#65292;&#21363;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10618v1 Announce Type: cross  Abstract: Average Treatment Effect (ATE) estimation is a well-studied problem in causal inference. However, it does not necessarily capture the heterogeneity in the data, and several approaches have been proposed to tackle the issue, including estimating the Quantile Treatment Effects. In the finite population setting containing $n$ individuals, with treatment and control values denoted by the potential outcome vectors $\mathbf{a}, \mathbf{b}$, much of the prior work focused on estimating median$(\mathbf{a}) -$ median$(\mathbf{b})$, where median($\mathbf x$) denotes the median value in the sorted ordering of all the values in vector $\mathbf x$. It is known that estimating the difference of medians is easier than the desired estimand of median$(\mathbf{a-b})$, called the Median Treatment Effect (MTE). The fundamental problem of causal inference -- for every individual $i$, we can only observe one of the potential outcome values, i.e., either the
&lt;/p&gt;</description></item><item><title>DiPaCo&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#27169;&#22359;&#21270;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36335;&#24452;&#20998;&#21457;&#35745;&#31639;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#26080;&#38656;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.10616</link><description>&lt;p&gt;
DiPaCo: &#20998;&#24067;&#24335;&#36335;&#24452;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
DiPaCo: Distributed Path Composition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10616
&lt;/p&gt;
&lt;p&gt;
DiPaCo&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#27169;&#22359;&#21270;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36335;&#24452;&#20998;&#21457;&#35745;&#31639;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#26080;&#38656;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24471;&#30410;&#20110;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36825;&#31181;&#25193;&#23637;&#26159;&#36890;&#36807;&#19981;&#26029;&#22766;&#20030;&#30340;&#24037;&#31243;&#21162;&#21147;&#23454;&#29616;&#30340;&#65292;&#20197;&#36866;&#24212;&#38656;&#35201;&#35774;&#22791;&#20043;&#38388;&#39640;&#24102;&#23485;&#36890;&#20449;&#30340;&#24182;&#34892;ML&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;ML&#27169;&#22411;&#35774;&#35745;&#30340;&#21327;&#21516;&#27169;&#22359;&#21270;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;DIstributed PAth COmposition&#65288;DiPaCo&#65289;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;DiPaCo&#36890;&#36807;&#19968;&#32452;&#20849;&#20139;&#27169;&#22359;&#30340;&#36335;&#24452;&#36827;&#34892;&#35745;&#31639;&#20998;&#21457;&#12290;&#32467;&#21512;&#19968;&#31181;&#21463;Local-SGD&#21551;&#21457;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;DiLoCo&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22823;&#24133;&#20943;&#23569;&#36890;&#20449;&#26469;&#30830;&#20445;&#27169;&#22359;&#21516;&#27493;&#65292;&#20419;&#36827;&#20102;&#36328;&#36830;&#25509;&#19981;&#20339;&#19988;&#24322;&#26500;&#30340;&#24037;&#20316;&#33410;&#28857;&#30340;&#35757;&#32451;&#65292;&#20854;&#35774;&#35745;&#30830;&#20445;&#20102;&#23545;&#24037;&#20316;&#33410;&#28857;&#25925;&#38556;&#21644;&#25250;&#21344;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;&#27599;&#20010;&#36755;&#20837;&#21482;&#38656;&#35201;&#25191;&#34892;&#19968;&#26465;&#36335;&#24452;&#65292;&#26080;&#38656;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#31181;&#39318;&#21019;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10616v1 Announce Type: cross  Abstract: Progress in machine learning (ML) has been fueled by scaling neural network models. This scaling has been enabled by ever more heroic feats of engineering, necessary for accommodating ML approaches that require high bandwidth communication between devices working in parallel. In this work, we propose a co-designed modular architecture and training approach for ML models, dubbed DIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes computation by paths through a set of shared modules. Together with a Local-SGD inspired optimization (DiLoCo) that keeps modules in sync with drastically reduced communication, Our approach facilitates training across poorly connected and heterogeneous workers, with a design that ensures robustness to worker failures and preemptions. At inference time, only a single path needs to be executed for each input, without the need for any model compression. We consider this approach as a first 
&lt;/p&gt;</description></item><item><title>LightIt&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26174;&#24335;&#29031;&#26126;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#29031;&#26126;&#25511;&#21046;&#65292;&#21516;&#26102;&#35757;&#32451;&#20102;&#19968;&#20010;&#36523;&#20221;&#20445;&#25345;&#30340;&#37325;&#29031;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.10615</link><description>&lt;p&gt;
LightIt&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#29031;&#26126;&#24314;&#27169;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LightIt: Illumination Modeling and Control for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10615
&lt;/p&gt;
&lt;p&gt;
LightIt&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26174;&#24335;&#29031;&#26126;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#29031;&#26126;&#25511;&#21046;&#65292;&#21516;&#26102;&#35757;&#32451;&#20102;&#19968;&#20010;&#36523;&#20221;&#20445;&#25345;&#30340;&#37325;&#29031;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LightIt&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26174;&#24335;&#29031;&#26126;&#25511;&#21046;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#29983;&#25104;&#26041;&#27861;&#32570;&#20047;&#29031;&#26126;&#25511;&#21046;&#65292;&#32780;&#36825;&#23545;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#35768;&#22810;&#33402;&#26415;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#27604;&#22914;&#35774;&#32622;&#25972;&#20307;&#24773;&#32490;&#25110;&#30005;&#24433;&#22806;&#35266;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20197;&#36974;&#34109;&#21644;&#27861;&#32447;&#22270;&#20026;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#27425;&#21453;&#23556;&#36974;&#34109;&#26469;&#24314;&#27169;&#29031;&#26126;&#65292;&#21253;&#25324;&#25237;&#23556;&#38452;&#24433;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#36974;&#34109;&#20272;&#35745;&#27169;&#22359;&#26469;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#36974;&#34109;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20272;&#35745;&#30340;&#36974;&#34109;&#21644;&#27861;&#32447;&#20316;&#20026;&#36755;&#20837;&#35757;&#32451;&#25511;&#21046;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#29031;&#26126;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#20010;&#20445;&#25345;&#36523;&#20221;&#30340;&#37325;&#29031;&#27169;&#22411;&#65292;&#20197;&#22270;&#20687;&#21644;&#30446;&#26631;&#36974;&#34109;&#20026;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#12289;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10615v1 Announce Type: cross  Abstract: We introduce LightIt, a method for explicit illumination control for image generation. Recent generative methods lack lighting control, which is crucial to numerous artistic aspects of image generation such as setting the overall mood or cinematic appearance. To overcome these limitations, we propose to condition the generation on shading and normal maps. We model the lighting with single bounce shading, which includes cast shadows. We first train a shading estimation module to generate a dataset of real-world images and shading pairs. Then, we train a control network using the estimated shading and normals as input. Our method demonstrates high-quality image generation and lighting control in numerous scenes. Additionally, we use our generated dataset to train an identity-preserving relighting model, conditioned on an image and a target shading. Our method is the first that enables the generation of images with controllable, consisten
&lt;/p&gt;</description></item><item><title>&#29992;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#20272;&#35745;inclusive KL&#25955;&#24230;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20559;&#24046;&#26799;&#24230;&#21644;&#39640;&#24230;&#38598;&#20013;&#21464;&#20998;&#20998;&#24067;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10610</link><description>&lt;p&gt;
&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#22312;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#20013;&#21253;&#23481;KL&#26368;&#23567;&#21270;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sequential Monte Carlo for Inclusive KL Minimization in Amortized Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10610
&lt;/p&gt;
&lt;p&gt;
&#29992;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#20272;&#35745;inclusive KL&#25955;&#24230;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20559;&#24046;&#26799;&#24230;&#21644;&#39640;&#24230;&#38598;&#20013;&#21464;&#20998;&#20998;&#24067;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#26469;&#35757;&#32451;&#32534;&#30721;&#22120;&#32593;&#32476;&#25191;&#34892;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#65292;&#20174;&#31934;&#30830;&#21518;&#39564;&#20998;&#24067;&#21040;&#20854;&#36817;&#20284;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#65292;&#21363;&#21253;&#23481;&#25110;&#27491;&#21521;KL&#65292;&#26159;&#22240;&#20854;&#26497;&#23567;&#21270;&#20540;&#30340;&#21306;&#22495;&#35206;&#30422;&#24615;&#36136;&#32780;&#25104;&#20026;&#21464;&#20998;&#30446;&#26631;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#26368;&#23567;&#21270;&#36825;&#19968;&#30446;&#26631;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SMC-Wake&#65292;&#35813;&#36807;&#31243;&#29992;&#20110;&#25311;&#21512;&#19968;&#31181;&#25674;&#38144;&#21464;&#20998;&#36817;&#20284;&#65292;&#20854;&#20351;&#29992;&#35843;&#33410;&#20284;&#28982;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#22120;&#26469;&#20272;&#35745;&#21253;&#23481;KL&#25955;&#24230;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20854;&#20013;&#23545;&#36845;&#20195;&#27425;&#25968;&#26159;&#28176;&#36817;&#26080;&#20559;&#30340;&#20004;&#31181;&#26159;&#24378;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20132;&#26367;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10610v1 Announce Type: new  Abstract: For training an encoder network to perform amortized variational inference, the Kullback-Leibler (KL) divergence from the exact posterior to its approximation, known as the inclusive or forward KL, is an increasingly popular choice of variational objective due to the mass-covering property of its minimizer. However, minimizing this objective is challenging. A popular existing approach, Reweighted Wake-Sleep (RWS), suffers from heavily biased gradients and a circular pathology that results in highly concentrated variational distributions. As an alternative, we propose SMC-Wake, a procedure for fitting an amortized variational approximation that uses likelihood-tempered sequential Monte Carlo samplers to estimate the gradient of the inclusive KL divergence. We propose three gradient estimators, all of which are asymptotically unbiased in the number of iterations and two of which are strongly consistent. Our method interleaves stochastic gr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SurvRNC&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#26469;&#33719;&#24471;&#22522;&#20110;&#29983;&#23384;&#26102;&#38388;&#30340;&#26377;&#24207;&#34920;&#31034;&#65292;&#33021;&#22788;&#29702;&#34987;&#25130;&#23614;&#25968;&#25454;&#65292;&#24182;&#21487;&#25972;&#21512;&#21040;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.10603</link><description>&lt;p&gt;
SurvRNC&#65306;&#20351;&#29992;Rank-N-Contrast&#23398;&#20064;&#26377;&#24207;&#34920;&#31034;&#26469;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SurvRNC: Learning Ordered Representations for Survival Prediction using Rank-N-Contrast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SurvRNC&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#26469;&#33719;&#24471;&#22522;&#20110;&#29983;&#23384;&#26102;&#38388;&#30340;&#26377;&#24207;&#34920;&#31034;&#65292;&#33021;&#22788;&#29702;&#34987;&#25130;&#23614;&#25968;&#25454;&#65292;&#24182;&#21487;&#25972;&#21512;&#21040;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#29983;&#23384;&#30340;&#21487;&#33021;&#24615;&#23545;&#20110;&#34987;&#35786;&#26029;&#20026;&#30284;&#30151;&#30340;&#20010;&#20307;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#20851;&#20110;&#26089;&#26399;&#39044;&#21518;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#36825;&#31181;&#30693;&#35782;&#20351;&#24471;&#21046;&#23450;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#24102;&#26469;&#25913;&#21892;&#24739;&#32773;&#32467;&#23616;&#30340;&#32467;&#26524;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20026;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#20197;&#20272;&#35745;&#30284;&#30151;&#39118;&#38505;&#20998;&#25968;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#23398;&#20064;&#20855;&#26377;&#22238;&#24402;&#24847;&#35782;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Survival Rank-N Contrast&#65288;SurvRNC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#26681;&#25454;&#29983;&#23384;&#26102;&#38388;&#33719;&#24471;&#19968;&#20010;&#26377;&#24207;&#34920;&#31034;&#12290;&#35813;&#20989;&#25968;&#21487;&#20197;&#22788;&#29702;&#34987;&#25130;&#23614;&#30340;&#25968;&#25454;&#65292;&#24182;&#21487;&#20197;&#34987;&#25972;&#21512;&#21040;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#20013;&#65292;&#20197;&#30830;&#20445;&#25152;&#23398;&#24471;&#30340;&#34920;&#31034;&#26159;&#39034;&#24207;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10603v1 Announce Type: cross  Abstract: Predicting the likelihood of survival is of paramount importance for individuals diagnosed with cancer as it provides invaluable information regarding prognosis at an early stage. This knowledge enables the formulation of effective treatment plans that lead to improved patient outcomes. In the past few years, deep learning models have provided a feasible solution for assessing medical images, electronic health records, and genomic data to estimate cancer risk scores. However, these models often fall short of their potential because they struggle to learn regression-aware feature representations. In this study, we propose Survival Rank-N Contrast (SurvRNC) method, which introduces a loss function as a regularizer to obtain an ordered representation based on the survival times. This function can handle censored data and can be incorporated into any survival model to ensure that the learned representation is ordinal. The model was extensi
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.10586</link><description>&lt;p&gt;
&#20174;&#31639;&#27861;&#21040;&#32467;&#26524;&#65306;&#23457;&#35270;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10586
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33152;&#33009;&#30284;&#26159;&#33521;&#22269;&#27599;&#22825;&#36896;&#25104;15&#20154;&#27515;&#20129;&#30340;&#39046;&#20808;&#27852;&#23615;&#36947;&#30284;&#30151;&#12290;&#36825;&#31181;&#30284;&#30151;&#20027;&#35201;&#34920;&#29616;&#20026;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#65288;NMIBC&#65289;&#65292;&#20854;&#29305;&#28857;&#26159;&#32959;&#30244;&#36824;&#26410;&#28183;&#36879;&#21040;&#33152;&#33009;&#22721;&#30340;&#32908;&#32905;&#23618;&#12290; NMIBC&#30340;&#22797;&#21457;&#29575;&#38750;&#24120;&#39640;&#65292;&#36798;&#21040;70-80&#65285;&#65292;&#22240;&#27492;&#27835;&#30103;&#25104;&#26412;&#26368;&#39640;&#12290;&#30446;&#21069;&#29992;&#20110;&#39044;&#27979;&#22797;&#21457;&#30340;&#24037;&#20855;&#20351;&#29992;&#35780;&#20998;&#31995;&#32479;&#26469;&#39640;&#20272;&#39118;&#38505;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#23545;&#22797;&#21457;&#30340;&#19981;&#20934;&#30830;&#21644;&#24310;&#36831;&#39044;&#27979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27515;&#20129;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#22797;&#21457;&#23545;&#20110;&#25104;&#26412;&#25928;&#30410;&#30340;&#31649;&#29702;&#21644;&#27835;&#30103;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#23601;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#20986;&#29616;&#30340;&#22320;&#26041;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#23376;&#21644;&#20020;&#24202;&#25968;&#25454;&#39044;&#27979;NMIBC&#22797;&#21457;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#27425;&#23457;&#26597;&#23545;&#39044;&#27979;NMIBC&#22797;&#21457;&#30340;ML&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35780;&#20272;&#20351;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10586v1 Announce Type: cross  Abstract: Bladder cancer, the leading urinary tract cancer, is responsible for 15 deaths daily in the UK. This cancer predominantly manifests as non-muscle-invasive bladder cancer (NMIBC), characterised by tumours not yet penetrating the muscle layer of the bladder wall. NMIBC is plagued by a very high recurrence rate of 70-80% and hence the costliest treatments. Current tools for predicting recurrence use scoring systems that overestimate risk and have poor accuracy. Inaccurate and delayed prediction of recurrence significantly elevates the likelihood of mortality. Accurate prediction of recurrence is hence vital for cost-effective management and treatment planning. This is where Machine learning (ML) techniques have emerged as a promising approach for predicting NMIBC recurrence by leveraging molecular and clinical data. This review provides a comprehensive analysis of ML approaches for predicting NMIBC recurrence. Our systematic evaluation de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#39564;&#25277;&#26679;&#35299;&#20915;&#19968;&#33324;&#24615;&#22122;&#22768;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#31574;&#30053;&#26799;&#24230;&#35270;&#35282;&#65292;&#36890;&#36807;&#25193;&#25955;&#31574;&#30053;&#26799;&#24230;&#65288;DPG&#65289;&#31934;&#30830;&#20272;&#35745;&#25351;&#23548;&#35780;&#20998;&#20989;&#25968;&#65292;&#23454;&#29616;&#23545;&#22810;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36870;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#35299;&#20915;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#24674;&#22797;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.10585</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#39564;&#25277;&#26679;&#35299;&#20915;&#19968;&#33324;&#24615;&#22122;&#22768;&#36870;&#38382;&#39064;&#65306;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10585
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#39564;&#25277;&#26679;&#35299;&#20915;&#19968;&#33324;&#24615;&#22122;&#22768;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#31574;&#30053;&#26799;&#24230;&#35270;&#35282;&#65292;&#36890;&#36807;&#25193;&#25955;&#31574;&#30053;&#26799;&#24230;&#65288;DPG&#65289;&#31934;&#30830;&#20272;&#35745;&#25351;&#23548;&#35780;&#20998;&#20989;&#25968;&#65292;&#23454;&#29616;&#23545;&#22810;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36870;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#35299;&#20915;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#24674;&#22797;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22270;&#20687;&#36870;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#65289;&#38656;&#35201;&#29983;&#25104;&#19968;&#20010;&#19982;&#32473;&#23450;&#36755;&#20837;&#65288;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#25110;&#36974;&#25377;&#22270;&#20687;&#65289;&#30456;&#21305;&#37197;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#22270;&#20687;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26469;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;&#36870;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#12290;&#20026;&#20102;&#31934;&#30830;&#20272;&#35745;&#36755;&#20837;&#22270;&#20687;&#30340;&#25351;&#23548;&#35780;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#31574;&#30053;&#26799;&#24230;&#65288;DPG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20013;&#38388;&#22122;&#22768;&#22270;&#20687;&#35270;&#20026;&#31574;&#30053;&#65292;&#23558;&#30446;&#26631;&#22270;&#20687;&#35270;&#20026;&#31574;&#30053;&#36873;&#25321;&#30340;&#29366;&#24577;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#22810;&#20010;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36870;&#20219;&#21153;&#19978;&#30340;&#39640;&#26031;&#21644;&#27850;&#26494;&#22122;&#22768;&#36864;&#21270;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#22312;FFHQ&#12289;ImageNet&#21644;LSUN&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#39640;&#30340;&#22270;&#20687;&#24674;&#22797;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10585v1 Announce Type: cross  Abstract: Solving image inverse problems (e.g., super-resolution and inpainting) requires generating a high fidelity image that matches the given input (the low-resolution image or the masked image). By using the input image as guidance, we can leverage a pretrained diffusion generative model to solve a wide range of image inverse tasks without task specific model fine-tuning. To precisely estimate the guidance score function of the input image, we propose Diffusion Policy Gradient (DPG), a tractable computation method by viewing the intermediate noisy images as policies and the target image as the states selected by the policy. Experiments show that our method is robust to both Gaussian and Poisson noise degradation on multiple linear and non-linear inverse tasks, resulting into a higher image restoration quality on FFHQ, ImageNet and LSUN datasets.
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;rPPG&#27169;&#22411;&#26159;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#21644;&#25163;&#25351;&#25509;&#35302;PPG&#27979;&#37327;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20294;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#30340;PPG&#20449;&#21495;&#20855;&#26377;&#19981;&#21516;&#30340;&#24418;&#24577;&#29305;&#24449;&#65292;&#36825;&#26679;&#35757;&#32451;&#38754;&#37096;&#35270;&#39057;&#30340;rPPG&#27169;&#22411;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.10582</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#30340;&#35270;&#39057;&#21644;&#30446;&#26631;&#35757;&#32451;rPPG&#27169;&#22411;&#26377;&#22810;&#27425;&#20248;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Suboptimal is Training rPPG Models with Videos and Targets from Different Body Sites?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10582
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;rPPG&#27169;&#22411;&#26159;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#21644;&#25163;&#25351;&#25509;&#35302;PPG&#27979;&#37327;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20294;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#30340;PPG&#20449;&#21495;&#20855;&#26377;&#19981;&#21516;&#30340;&#24418;&#24577;&#29305;&#24449;&#65292;&#36825;&#26679;&#35757;&#32451;&#38754;&#37096;&#35270;&#39057;&#30340;rPPG&#27169;&#22411;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#27861;&#65288;rPPG&#65289;&#36828;&#31243;&#25668;&#20687;&#22836;&#27979;&#37327;&#33033;&#25615;&#34880;&#23481;&#37327;&#26159;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#21487;&#20280;&#32553;&#12289;&#20302;&#25104;&#26412;&#21644;&#26131;&#33719;&#21462;&#30340;&#24515;&#34880;&#31649;&#20449;&#24687;&#35780;&#20272;&#12290;&#31070;&#32463;&#32593;&#32476;&#30446;&#21069;&#20026;&#27492;&#20219;&#21153;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#32780;&#30417;&#30563;&#35757;&#32451;&#25110;&#24494;&#35843;&#26159;&#21019;&#24314;&#36825;&#20123;&#27169;&#22411;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#27169;&#22411;&#26159;&#20351;&#29992;&#26469;&#33258;&#25163;&#25351;&#30340;&#25509;&#35302;PPG&#27979;&#37327;&#20316;&#20026;&#30446;&#26631;/&#26631;&#31614;&#65292;&#36890;&#36807;&#38754;&#37096;&#35270;&#39057;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#20844;&#20849;&#25968;&#25454;&#38598;&#34701;&#21512;&#20102;&#26469;&#33258;&#38754;&#37096;&#30340;&#25509;&#35302;PPG&#27979;&#37327;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#65292;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#30340;PPG&#20449;&#21495;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#24418;&#24577;&#29305;&#24449;&#12290;&#20351;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;&#20855;&#26377;&#21516;&#27493;&#25509;&#35302;PPG&#21644;&#35270;&#39057;&#27979;&#37327;&#25968;&#25454;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10582v1 Announce Type: cross  Abstract: Remote camera measurement of the blood volume pulse via photoplethysmography (rPPG) is a compelling technology for scalable, low-cost, and accessible assessment of cardiovascular information. Neural networks currently provide the state-of-the-art for this task and supervised training or fine-tuning is an important step in creating these models. However, most current models are trained on facial videos using contact PPG measurements from the fingertip as targets/ labels. One of the reasons for this is that few public datasets to date have incorporated contact PPG measurements from the face. Yet there is copious evidence that the PPG signals at different sites on the body have very different morphological features. Is training a facial video rPPG model using contact measurements from another site on the body suboptimal? Using a recently released unique dataset with synchronized contact PPG and video measurements from both the hand and fa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.10581</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#30340;ECG&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#30001;&#20110;&#20840;&#29699;&#27515;&#20129;&#29575;&#19981;&#26029;&#19978;&#21319;&#32780;&#26500;&#25104;&#37325;&#22823;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#36890;&#36807;&#26089;&#26399;&#35786;&#26029;&#21644;&#39044;&#38450;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#21487;&#26174;&#33879;&#20943;&#23569;&#30142;&#30149;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#20020;&#24202;&#33719;&#21462;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#36827;&#34892;HF&#39118;&#38505;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#26088;&#22312;&#25429;&#25417;&#23545;&#26089;&#26399;HF&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#22797;&#26434;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#23613;&#31649;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#24179;&#34913;&#12290;&#35813;&#32593;&#32476;&#20855;&#26377;&#19968;&#20010;&#36328;&#23548;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;12&#20010;&#23548;&#32852;&#29305;&#23450;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#25429;&#25417;&#20132;&#21449;&#23548;&#32852;&#20132;&#20114;&#20316;&#29992;&#21644;&#27599;&#20010;&#23548;&#32852;&#20869;&#30340;&#23616;&#37096;&#26102;&#38388;&#21160;&#24577;&#12290;&#20026;&#20102;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20844;&#20849;ECG-Report&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#36827;&#34892;ECG-&#25253;&#21578;&#23545;&#40784;&#20219;&#21153;&#12290;&#28982;&#21518;&#23545;&#32593;&#32476;&#36827;&#34892;fine-tune&#20197;&#29992;&#20110;HF&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#26657;&#20934;&#27969;&#20307;&#21160;&#21147;&#23398;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#22122;&#22768;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#25216;&#26415;&#21462;&#20195;&#20102;&#20197;&#24448;&#30340;PCA&#25216;&#26415;&#65292;&#36825;&#33021;&#22815;&#36991;&#20813;&#23545;&#22686;&#37327;&#26045;&#21152;&#39069;&#22806;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.10578</link><description>&lt;p&gt;
&#38543;&#26426;&#26059;&#36716;&#27973;&#27700;&#22122;&#22768;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modelling of Stochastic Rotating Shallow Water Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#26657;&#20934;&#27969;&#20307;&#21160;&#21147;&#23398;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#22122;&#22768;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#25216;&#26415;&#21462;&#20195;&#20102;&#20197;&#24448;&#30340;PCA&#25216;&#26415;&#65292;&#36825;&#33021;&#22815;&#36991;&#20813;&#23545;&#22686;&#37327;&#26045;&#21152;&#39069;&#22806;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#65292;&#20316;&#32773;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#26657;&#20934;&#27969;&#20307;&#21160;&#21147;&#23398;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#22122;&#22768;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20854;&#20013;&#38543;&#26426;&#24615;&#34987;&#24341;&#20837;&#20197;&#21442;&#25968;&#21270;&#27425;&#32593;&#26684;&#23610;&#24230;&#36807;&#31243;&#12290;&#23376;&#32593;&#26684;&#23610;&#24230;&#36807;&#31243;&#30340;&#38543;&#26426;&#21442;&#25968;&#21270;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#26159;&#24517;&#38656;&#30340;&#65292;&#20197;&#34920;&#31034;&#30001;&#23376;&#32593;&#26684;&#23610;&#24230;&#27874;&#21160;&#24341;&#36215;&#30340;&#31995;&#32479;&#27169;&#22411;&#35823;&#24046;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#21442;&#25968;&#21270;&#22686;&#37327;&#20026;&#27491;&#24577;&#20998;&#24067;&#30340;&#20551;&#35774;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;PCA&#25216;&#26415;&#34987;&#29983;&#25104;&#27169;&#22411;&#25216;&#26415;&#25152;&#21462;&#20195;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36991;&#20813;&#23545;&#22686;&#37327;&#26045;&#21152;&#39069;&#22806;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#27169;&#22411;&#39640;&#31243;&#21464;&#37327;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#30340;&#38543;&#26426;&#26059;&#36716;&#27973;&#27700;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10578v1 Announce Type: cross  Abstract: In recent work, the authors have developed a generic methodology for calibrating the noise in fluid dynamics stochastic partial differential equations where the stochasticity was introduced to parametrize subgrid-scale processes. The stochastic parameterization of sub-grid scale processes is required in the estimation of uncertainty in weather and climate predictions, to represent systematic model errors arising from subgrid-scale fluctuations. The previous methodology used a principal component analysis (PCA) technique based on the ansatz that the increments of the stochastic parametrization are normally distributed.   In this paper, the PCA technique is replaced by a generative model technique. This enables us to avoid imposing additional constraints on the increments. The methodology is tested on a stochastic rotating shallow water model with the elevation variable of the model used as input data. The numerical simulations show that
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24182;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.10576</link><description>&lt;p&gt;
&#24573;&#30053;&#25105;&#20294;&#19981;&#35201;&#26367;&#20195;&#25105;&#65306;&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10576
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24182;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32593;&#32476;&#23433;&#20840;&#20449;&#24687;&#36890;&#24120;&#25216;&#26415;&#22797;&#26434;&#19988;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20256;&#36882;&#65292;&#20351;&#24471;&#33258;&#21160;&#21270;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#28041;&#21450;&#39640;&#24230;&#19987;&#19994;&#30693;&#35782;&#30340;&#25991;&#26412;&#39046;&#22495;&#65292;&#22522;&#20110;&#39046;&#22495;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#19968;&#30452;&#26159;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#23433;&#20840;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#38750;&#35821;&#35328;&#20803;&#32032;&#65288;&#22914;URL&#21644;&#21704;&#24076;&#20540;&#65289;&#65292;&#36825;&#21487;&#33021;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#19981;&#36866;&#29992;&#12290;&#20808;&#21069;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#24037;&#20316;&#20013;&#65292;&#24050;&#23558;&#27492;&#31867;&#25991;&#26412;&#35270;&#20026;&#22122;&#38899;&#36827;&#34892;&#31227;&#38500;&#25110;&#36807;&#28388;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#21644;&#25506;&#27979;&#20219;&#21153;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#65288;&#36873;&#25321;&#24615;MLM&#21644;&#32852;&#21512;&#35757;&#32451;NLE&#26631;&#35760;&#20998;&#31867;&#65289;&#20248;&#20110;&#24120;&#29992;&#30340;&#26367;&#25442;&#38750;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10576v1 Announce Type: cross  Abstract: Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We propose different pretraining methodologies and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy (selective MLM and jointly training NLE token classification) outperforms the commonly taken approach of replacing non-l
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21307;&#23398;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#23519;&#35273;&#22122;&#22768;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.10573</link><description>&lt;p&gt;
&#21307;&#23398;&#19981;&#21487;&#23398;&#20064;&#30340;&#31034;&#20363;&#65306;&#36890;&#36807;&#31232;&#30095;&#24863;&#30693;&#26412;&#22320;&#33945;&#29256;&#20445;&#25252;&#21307;&#23398;&#25968;&#25454;&#20813;&#21463;&#26410;&#32463;&#25480;&#26435;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Medical Unlearnable Examples: Securing Medical Data from Unauthorized Traning via Sparsity-Aware Local Masking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10573
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21307;&#23398;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#23519;&#35273;&#22122;&#22768;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#25935;&#24863;&#21307;&#23398;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#23384;&#20648;&#26174;&#33879;&#22686;&#21152;&#12290;&#36825;&#31181;&#25968;&#25454;&#30340;&#20016;&#23500;&#37327;&#25512;&#21160;&#20102;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#21033;&#29992;&#65292;&#20363;&#22914;&#29992;&#20110;&#35757;&#32451;&#21830;&#19994;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#24120;&#24120;&#20351;&#30740;&#31350;&#20154;&#21592;&#26395;&#32780;&#21364;&#27493;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#24895;&#20844;&#24320;&#20854;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#38590;&#20197;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#40723;&#21169;&#21307;&#30103;&#26426;&#26500;&#20998;&#20139;&#25968;&#25454;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21521;&#25968;&#25454;&#20013;&#24341;&#20837;&#38590;&#20197;&#23519;&#35273;&#30340;&#22122;&#22768;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#27169;&#22411;&#27867;&#21270;&#20013;&#24341;&#20837;&#36864;&#21270;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#19968;&#33324;&#39046;&#22495;&#26174;&#31034;&#20986;&#20196;&#20154;&#38054;&#20329;&#30340;&#25968;&#25454;&#20445;&#25252;&#33021;&#21147;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21040;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10573v1 Announce Type: cross  Abstract: With the rapid growth of artificial intelligence (AI) in healthcare, there has been a significant increase in the generation and storage of sensitive medical data. This abundance of data, in turn, has propelled the advancement of medical AI technologies. However, concerns about unauthorized data exploitation, such as training commercial AI models, often deter researchers from making their invaluable datasets publicly available. In response to the need to protect this hard-to-collect data while still encouraging medical institutions to share it, one promising solution is to introduce imperceptible noise into the data. This method aims to safeguard the data against unauthorized training by inducing degradation in model generalization. Although existing methods have shown commendable data protection capabilities in general domains, they tend to fall short when applied to biomedical data, mainly due to their failure to account for the spar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#21464;&#37051;&#22495;&#27169;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37051;&#22495;&#20256;&#25773;&#27169;&#22359;&#21644;&#19981;&#21464;&#38750;&#21516;&#28304;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#38750;&#21516;&#28304;&#22270;&#19978;&#37051;&#22495;&#27169;&#24335;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10572</link><description>&lt;p&gt;
&#21457;&#29616;&#24322;&#24615;&#22270;&#30340;&#19981;&#21464;&#37051;&#22495;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Invariant Neighborhood Patterns for Heterophilic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#21464;&#37051;&#22495;&#27169;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37051;&#22495;&#20256;&#25773;&#27169;&#22359;&#21644;&#19981;&#21464;&#38750;&#21516;&#28304;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#38750;&#21516;&#28304;&#22270;&#19978;&#37051;&#22495;&#27169;&#24335;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21516;&#28304;&#22270;&#19978;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20381;&#36182;&#20110;&#21516;&#28304;&#20551;&#35774;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#34987;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#65292;&#36825;&#31181;&#21516;&#28304;&#24615;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#20808;&#21069;&#30340;&#26041;&#27861;&#20013;&#26410;&#33021;&#35299;&#37322;&#30340;&#26356;&#22797;&#26434;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22312;&#38750;&#21516;&#28304;&#22270;&#19978;&#65292;&#37051;&#22495;&#27169;&#24335;&#30340;&#20998;&#24067;&#20559;&#31227;&#26356;&#21152;&#22810;&#26679;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#21464;&#37051;&#22495;&#27169;&#24335;&#23398;&#20064;&#65288;INPL&#65289;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#38750;&#21516;&#28304;&#22270;&#19978;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#37051;&#22495;&#20256;&#25773;&#65288;ANP&#65289;&#27169;&#22359;&#26469;&#25429;&#33719;&#33258;&#36866;&#24212;&#30340;&#37051;&#22495;&#20449;&#24687;&#65292;&#36825;&#21487;&#20197;&#32531;&#35299;&#38750;&#21516;&#28304;&#22270;&#19978;&#30340;&#37051;&#22495;&#27169;&#24335;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21464;&#38750;&#21516;&#28304;&#22270;&#23398;&#20064;&#65288;INHGL&#65289;&#27169;&#22359;&#65292;&#20854;&#32422;&#26463;&#20102;ANP&#24182;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10572v1 Announce Type: new  Abstract: This paper studies the problem of distribution shifts on non-homophilous graphs Mosting existing graph neural network methods rely on the homophilous assumption that nodes from the same class are more likely to be linked. However, such assumptions of homophily do not always hold in real-world graphs, which leads to more complex distribution shifts unaccounted for in previous methods. The distribution shifts of neighborhood patterns are much more diverse on non-homophilous graphs. We propose a novel Invariant Neighborhood Pattern Learning (INPL) to alleviate the distribution shifts problem on non-homophilous graphs. Specifically, we propose the Adaptive Neighborhood Propagation (ANP) module to capture the adaptive neighborhood information, which could alleviate the neighborhood pattern distribution shifts problem on non-homophilous graphs. We propose Invariant Non-Homophilous Graph Learning (INHGL) module to constrain the ANP and learn in
&lt;/p&gt;</description></item><item><title>JaxDecompiler&#26159;&#19968;&#31181;&#23558;JAX&#20989;&#25968;&#36716;&#25442;&#20026;&#21487;&#32534;&#36753;Python&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#22522;&#20110;&#26799;&#24230;&#20449;&#24687;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#21453;&#21521;&#24037;&#31243;&#12289;&#29702;&#35299;&#12289;&#23450;&#21046;&#21644;&#20114;&#25805;&#20316;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.10571</link><description>&lt;p&gt;
JaxDecompiler&#65306;&#37325;&#26032;&#23450;&#20041;&#22522;&#20110;&#26799;&#24230;&#20449;&#24687;&#30340;&#36719;&#20214;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
JaxDecompiler: Redefining Gradient-Informed Software Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10571
&lt;/p&gt;
&lt;p&gt;
JaxDecompiler&#26159;&#19968;&#31181;&#23558;JAX&#20989;&#25968;&#36716;&#25442;&#20026;&#21487;&#32534;&#36753;Python&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#22522;&#20110;&#26799;&#24230;&#20449;&#24687;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#21453;&#21521;&#24037;&#31243;&#12289;&#29702;&#35299;&#12289;&#23450;&#21046;&#21644;&#20114;&#25805;&#20316;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33021;&#22815;&#35745;&#31639;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#25968;&#20540;&#24211;&#20013;&#65292;JAX&#20197;&#20854;&#25552;&#20379;&#26356;&#22810;&#21151;&#33021;&#32780;&#33073;&#39062;&#32780;&#20986;&#65292;&#20854;&#21152;&#36895;&#25928;&#26524;&#26469;&#33258;&#19968;&#20010;&#21517;&#20026;Jaxpr&#35821;&#35328;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#32534;&#36753;Jaxpr&#20195;&#30721;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;JaxDecompiler&#65292;&#19968;&#31181;&#23558;&#20219;&#20309;JAX&#20989;&#25968;&#36716;&#25442;&#20026;&#21487;&#32534;&#36753;Python&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#32534;&#36753;&#30001;&#26799;&#24230;&#20989;&#25968;&#29983;&#25104;&#30340;JAX&#20989;&#25968;&#12290;JaxDecompiler&#31616;&#21270;&#20102;&#36890;&#36807;JAX&#24320;&#21457;&#30340;&#36719;&#20214;&#30340;&#21453;&#21521;&#24037;&#31243;&#12289;&#29702;&#35299;&#12289;&#23450;&#21046;&#21644;&#20114;&#25805;&#20316;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#31361;&#20986;&#20102;&#23427;&#30340;&#21151;&#33021;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#26356;&#19968;&#33324;&#30340;&#22522;&#20110;&#26799;&#24230;&#20449;&#24687;&#30340;&#36719;&#20214;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#21453;&#32534;&#35793;&#20195;&#30721;&#30340;&#36895;&#24230;&#24615;&#33021;&#19982;&#21407;&#22987;&#20195;&#30721;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10571v1 Announce Type: cross  Abstract: Among numerical libraries capable of computing gradient descent optimization, JAX stands out by offering more features, accelerated by an intermediate representation known as Jaxpr language. However, editing the Jaxpr code is not directly possible. This article introduces JaxDecompiler, a tool that transforms any JAX function into an editable Python code, especially useful for editing the JAX function generated by the gradient function. JaxDecompiler simplifies the processes of reverse engineering, understanding, customizing, and interoperability of software developed by JAX. We highlight its capabilities, emphasize its practical applications especially in deep learning and more generally gradient-informed software, and demonstrate that the decompiled code speed performance is similar to the original.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;Xception&#19978;&#23454;&#26045;&#39640;&#25928;&#30340;&#21442;&#25968;&#32553;&#20943;&#31574;&#30053;&#65292;&#35813;&#30740;&#31350;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;DNN&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#21033;&#29992;&#65292;&#19988;&#22312;Caltech-101&#22270;&#20687;&#20998;&#31867;&#20013;&#34920;&#29616;&#20248;&#20110;&#21407;&#22987;Xception&#21644;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.10569</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#21033;&#29992;&#39640;&#25928;&#21442;&#25968;&#32553;&#20943;&#23454;&#29616;DNN&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10569
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;Xception&#19978;&#23454;&#26045;&#39640;&#25928;&#30340;&#21442;&#25968;&#32553;&#20943;&#31574;&#30053;&#65292;&#35813;&#30740;&#31350;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;DNN&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#21033;&#29992;&#65292;&#19988;&#22312;Caltech-101&#22270;&#20687;&#20998;&#31867;&#20013;&#34920;&#29616;&#20248;&#20110;&#21407;&#22987;Xception&#21644;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#25913;&#21892;&#20854;&#30828;&#20214;&#21033;&#29992;&#29575;&#65292;&#24182;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#25552;&#20379;&#20415;&#20110;&#35774;&#22791;&#19978;&#35757;&#32451;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;Xception&#19978;&#23454;&#26045;&#20102;&#39640;&#25928;&#30340;&#21442;&#25968;&#32553;&#20943;&#31574;&#30053;&#65292;&#32553;&#23567;&#27169;&#22411;&#22823;&#23567;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65306;Caltech-101&#22270;&#20687;&#20998;&#31867;&#21644;PCB&#32570;&#38519;&#26816;&#27979;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#21407;&#22987;&#30340;Xception&#21644;&#36731;&#37327;&#32423;&#27169;&#22411;EfficientNetV2B1&#21644;MobileNetV2&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;Caltech-101&#22270;&#20687;&#20998;&#31867;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65288;76.21%&#65289;&#65292;&#27604;Xception&#65288;75.89%&#65289;&#24179;&#22343;&#20351;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#65288;847.9MB&#27604;Xception&#30340;874.6MB&#65289;&#65292;&#24182;&#19988;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#26356;&#24555;&#12290;&#36731;&#37327;&#32423;&#27169;&#22411;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;EfficientNetV2B1&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20026;30.52%&#65292;MobileNetV2&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20026;58.11%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10569v1 Announce Type: cross  Abstract: This paper proposes an optimization of an existing Deep Neural Network (DNN) that improves its hardware utilization and facilitates on-device training for resource-constrained edge environments. We implement efficient parameter reduction strategies on Xception that shrink the model size without sacrificing accuracy, thus decreasing memory utilization during training. We evaluate our model in two experiments: Caltech-101 image classification and PCB defect detection and compare its performance against the original Xception and lightweight models, EfficientNetV2B1 and MobileNetV2. The results of the Caltech-101 image classification show that our model has a better test accuracy (76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than Xception (874.6MB), and has faster training and inference times. The lightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy and MobileNetV2 having a 58.11% test acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20061;&#31181;&#38598;&#25104;&#23398;&#20064;&#22120;&#24182;&#21033;&#29992;&#26032;&#39062;&#29305;&#24449;&#24037;&#31243;&#31574;&#30053;&#65292;&#32467;&#21512;&#22810;&#31181;&#20998;&#20301;&#25968;&#22238;&#24402;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#31354;&#38388;&#25554;&#20540;&#20013;&#38598;&#25104;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;</title><link>https://arxiv.org/abs/2403.10567</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#20013;&#30340;&#21355;&#26143;&#38477;&#27700;&#31354;&#38388;&#25554;&#20540;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation in spatial interpolation of satellite precipitation with ensemble learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10567
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20061;&#31181;&#38598;&#25104;&#23398;&#20064;&#22120;&#24182;&#21033;&#29992;&#26032;&#39062;&#29305;&#24449;&#24037;&#31243;&#31574;&#30053;&#65292;&#32467;&#21512;&#22810;&#31181;&#20998;&#20301;&#25968;&#22238;&#24402;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#31354;&#38388;&#25554;&#20540;&#20013;&#38598;&#25104;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10567v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#27010;&#29575;&#20998;&#24067;&#24418;&#24335;&#30340;&#39044;&#27979;&#23545;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20998;&#20301;&#25968;&#22238;&#24402;&#22312;&#31354;&#38388;&#25554;&#20540;&#35774;&#32622;&#20013;&#33021;&#22815;&#21512;&#24182;&#36965;&#24863;&#21644;&#38632;&#37327;&#25968;&#25454;&#65292;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#65292;&#20998;&#20301;&#25968;&#22238;&#24402;&#31639;&#27861;&#30340;&#38598;&#25104;&#23398;&#20064;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20061;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#38598;&#25104;&#23398;&#20064;&#22120;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#38477;&#27700;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#24037;&#31243;&#31574;&#30053;&#65292;&#23558;&#39044;&#27979;&#22240;&#23376;&#20943;&#23569;&#20026;&#30456;&#20851;&#20301;&#32622;&#30340;&#21152;&#26435;&#36317;&#31163;&#21355;&#26143;&#38477;&#27700;&#65292;&#32467;&#21512;&#20301;&#32622;&#39640;&#31243;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#23398;&#20064;&#22120;&#21253;&#25324;&#20845;&#31181;&#22534;&#21472;&#26041;&#27861;&#21644;&#19977;&#31181;&#31616;&#21333;&#26041;&#27861;&#65288;&#22343;&#20540;&#12289;&#20013;&#20301;&#25968;&#12289;&#26368;&#20339;&#32452;&#21512;&#22120;&#65289;&#65292;&#32467;&#21512;&#20102;&#20845;&#31181;&#20010;&#20307;&#31639;&#27861;&#65306;&#20998;&#20301;&#25968;&#22238;&#24402;(QR)&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;(QRF)&#12289;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;(GRF)&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;(GBM)&#12289;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;(LightGBM)&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10567v1 Announce Type: new  Abstract: Predictions in the form of probability distributions are crucial for decision-making. Quantile regression enables this within spatial interpolation settings for merging remote sensing and gauge precipitation data. However, ensemble learning of quantile regression algorithms remains unexplored in this context. Here, we address this gap by introducing nine quantile-based ensemble learners and applying them to large precipitation datasets. We employed a novel feature engineering strategy, reducing predictors to distance-weighted satellite precipitation at relevant locations, combined with location elevation. Our ensemble learners include six stacking and three simple methods (mean, median, best combiner), combining six individual algorithms: quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neur
&lt;/p&gt;</description></item><item><title>&#23548;&#20837;&#20919;&#21364;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;AI&#26041;&#27861;&#20248;&#21270;&#20102;&#30005;&#27744;&#21333;&#20803;&#24067;&#23616;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#21333;&#20803;&#30340;&#26368;&#39640;&#28201;&#24230;&#65292;&#24182;&#22312;&#20919;&#21364;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.10566</link><description>&lt;p&gt;
&#30005;&#27744;&#21333;&#20803;&#24067;&#23616;&#30340;&#20919;&#21364;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cooling-Guide Diffusion Model for Battery Cell Arrangement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10566
&lt;/p&gt;
&lt;p&gt;
&#23548;&#20837;&#20919;&#21364;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;AI&#26041;&#27861;&#20248;&#21270;&#20102;&#30005;&#27744;&#21333;&#20803;&#24067;&#23616;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#21333;&#20803;&#30340;&#26368;&#39640;&#28201;&#24230;&#65292;&#24182;&#22312;&#20919;&#21364;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#21033;&#29992;&#20919;&#21364;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#26469;&#20248;&#21270;&#30005;&#27744;&#21333;&#20803;&#30340;&#24067;&#23616;&#65292;&#36825;&#26159;&#22686;&#24378;&#30005;&#27744;&#28909;&#31649;&#29702;&#31995;&#32479;&#20919;&#21364;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#19982;&#20256;&#32479;&#35774;&#35745;&#27969;&#31243;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#20351;&#29992;&#21442;&#25968;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#19982;&#20998;&#31867;&#22120;&#21644;&#20919;&#21364;&#24341;&#23548;&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#20855;&#26377;&#22686;&#24378;&#20919;&#21364;&#36335;&#24452;&#30340;&#20248;&#21270;&#21333;&#20803;&#24067;&#23616;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#21333;&#20803;&#30340;&#26368;&#39640;&#28201;&#24230;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#20301;&#32622;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#65292;&#25105;&#20204;&#30830;&#20445;&#29983;&#25104;&#30340;&#24067;&#23616;&#30340;&#21487;&#34892;&#24615;&#12290;&#21516;&#26102;&#65292;&#20919;&#21364;&#24341;&#23548;&#30452;&#25509;&#20248;&#21270;&#20919;&#21364;&#25928;&#29575;&#65292;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#20855;&#25928;&#26524;&#12290;&#19982;&#20004;&#31181;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#29420;&#29305;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10566v1 Announce Type: cross  Abstract: Our study introduces a Generative AI method that employs a cooling-guided diffusion model to optimize the layout of battery cells, a crucial step for enhancing the cooling performance and efficiency of battery thermal management systems. Traditional design processes, which rely heavily on iterative optimization and extensive guesswork, are notoriously slow and inefficient, often leading to suboptimal solutions. In contrast, our innovative method uses a parametric denoising diffusion probabilistic model (DDPM) with classifier and cooling guidance to generate optimized cell layouts with enhanced cooling paths, significantly lowering the maximum temperature of the cells. By incorporating position-based classifier guidance, we ensure the feasibility of generated layouts. Meanwhile, cooling guidance directly optimizes cooling-efficiency, making our approach uniquely effective. When compared to two advanced models, the Tabular Denoising Diff
&lt;/p&gt;</description></item><item><title>&#26080;&#29366;&#24577;&#31574;&#30053;&#36890;&#36807;&#35780;&#20272;&#21453;&#26679;&#26412;&#23545;&#25239;&#40657;&#30418;&#26597;&#35810;&#65292;&#26377;&#25928;&#24341;&#20837;&#20102;&#38450;&#24481;&#32773;&#26377;&#21033;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#38450;&#24481;&#26041;&#27861;&#33021;&#22815;&#27450;&#39575;&#25915;&#20987;&#32773;&#23547;&#25214;&#23545;&#25239;&#24615;&#31034;&#20363;&#12289;&#20445;&#25345;&#27169;&#22411;&#22312;&#21512;&#27861;&#36755;&#20837;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#25915;&#20987;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.10562</link><description>&lt;p&gt;
Counter-Samples: &#19968;&#31181;&#25269;&#24481;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#30340;&#26080;&#29366;&#24577;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10562
&lt;/p&gt;
&lt;p&gt;
&#26080;&#29366;&#24577;&#31574;&#30053;&#36890;&#36807;&#35780;&#20272;&#21453;&#26679;&#26412;&#23545;&#25239;&#40657;&#30418;&#26597;&#35810;&#65292;&#26377;&#25928;&#24341;&#20837;&#20102;&#38450;&#24481;&#32773;&#26377;&#21033;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#38450;&#24481;&#26041;&#27861;&#33021;&#22815;&#27450;&#39575;&#25915;&#20987;&#32773;&#23547;&#25214;&#23545;&#25239;&#24615;&#31034;&#20363;&#12289;&#20445;&#25345;&#27169;&#22411;&#22312;&#21512;&#27861;&#36755;&#20837;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#25915;&#20987;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25269;&#24481;&#40657;&#30418;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#21463;&#23475;&#32773;&#27169;&#22411;&#20316;&#20026; Oracle &#26469;&#26500;&#24314;&#20182;&#20204;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#19982;&#20256;&#32479;&#30340;&#39044;&#22788;&#29702;&#38450;&#24481;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26080;&#29366;&#24577;&#31574;&#30053;&#23454;&#38469;&#19978;&#23545;&#25239;&#25915;&#20987;&#36807;&#31243;&#26412;&#36523;&#12290;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#26597;&#35810;&#37117;&#35780;&#20272;&#19968;&#20010;&#21453;&#26679;&#26412;&#65292;&#20854;&#20013;&#21453;&#26679;&#26412;&#26159;&#38024;&#23545;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#21407;&#22987;&#26679;&#26412;&#12290;&#36890;&#36807;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#30333;&#30418;&#20248;&#21270;&#26469;&#23545;&#25239;&#27599;&#19968;&#20010;&#40657;&#30418;&#26597;&#35810;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#26377;&#25928;&#22320;&#20026;&#38450;&#24481;&#32773;&#24341;&#20837;&#20102;&#23545;&#31216;&#24615;&#65292;&#20351;&#20854;&#22788;&#20110;&#26377;&#21033;&#22320;&#20301;&#12290;&#36825;&#31181;&#38450;&#24481;&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22320;&#35823;&#23548;&#25915;&#20987;&#32773;&#23547;&#25214;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36824;&#33021;&#20445;&#30041;&#22312;&#21512;&#27861;&#36755;&#20837;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#26368;&#20808;&#36827;&#30340;&#40657;&#30418;&#25915;&#20987;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312; C &#19978;&#32988;&#36807;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10562v1 Announce Type: cross  Abstract: Our paper presents a novel defence against black box attacks, where attackers use the victim model as an oracle to craft their adversarial examples. Unlike traditional preprocessing defences that rely on sanitizing input samples, our stateless strategy counters the attack process itself. For every query we evaluate a counter-sample instead, where the counter-sample is the original sample optimized against the attacker's objective. By countering every black box query with a targeted white box optimization, our strategy effectively introduces an asymmetry to the game to the defender's advantage. This defence not only effectively misleads the attacker's search for an adversarial example, it also preserves the model's accuracy on legitimate inputs and is generic to multiple types of attacks.   We demonstrate that our approach is remarkably effective against state-of-the-art black box attacks and outperforms existing defences for both the C
&lt;/p&gt;</description></item><item><title>AAAI 2024&#24180;&#20154;&#26412;&#20027;&#20041;&#34920;&#31034;&#23398;&#20064;&#30740;&#35752;&#20250;&#30340;&#34987;&#25509;&#21463;&#35770;&#25991;&#38598;&#21512;&#12290;&#37096;&#20998;&#35770;&#25991;&#36873;&#25321;&#36864;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.10561</link><description>&lt;p&gt;
AAAI 2024&#24180;&#20154;&#26412;&#20027;&#20041;&#34920;&#31034;&#23398;&#20064;&#30740;&#35752;&#20250;&#30340;&#34987;&#25509;&#21463;&#35770;&#25991;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10561
&lt;/p&gt;
&lt;p&gt;
AAAI 2024&#24180;&#20154;&#26412;&#20027;&#20041;&#34920;&#31034;&#23398;&#20064;&#30740;&#35752;&#20250;&#30340;&#34987;&#25509;&#21463;&#35770;&#25991;&#38598;&#21512;&#12290;&#37096;&#20998;&#35770;&#25991;&#36873;&#25321;&#36864;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10561v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39033;  &#25688;&#35201;: &#36825;&#20221;&#38750;&#23384;&#26723;&#32034;&#24341;&#24182;&#19981;&#23436;&#25972;&#65292;&#22240;&#20026;&#19968;&#20123;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#36873;&#25321;&#20102;&#36864;&#20986;&#12290;&#25152;&#26377;&#34987;&#25509;&#21463;&#35770;&#25991;&#30340;&#21015;&#34920;&#21487;&#22312;&#30740;&#35752;&#20250;&#32593;&#31449;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10561v1 Announce Type: cross  Abstract: This non-archival index is not complete, as some accepted papers chose to opt-out of inclusion. The list of all accepted papers is available on the workshop website.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#25972;&#21512;&#26377;&#26395;&#25552;&#21319;&#33258;&#21160;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#65292;&#23545;&#20132;&#36890;&#34892;&#19994;&#30340;&#23433;&#20840;&#21644;&#21019;&#26032;&#20855;&#26377;&#28508;&#22312;&#25512;&#21160;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10559</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65306;&#25506;&#32034;&#20132;&#36890;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10559
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#25972;&#21512;&#26377;&#26395;&#25552;&#21319;&#33258;&#21160;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#65292;&#23545;&#20132;&#36890;&#34892;&#19994;&#30340;&#23433;&#20840;&#21644;&#21019;&#26032;&#20855;&#26377;&#28508;&#22312;&#25512;&#21160;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#35843;&#26597;&#20102;&#29983;&#25104;&#27169;&#22411;&#21644;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#20004;&#31181;&#25512;&#21160;&#25216;&#26415;&#21644;&#20132;&#36890;&#36827;&#27493;&#30340;&#31361;&#30772;&#24615;&#21147;&#37327;&#30340;&#21382;&#21490;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#20851;&#27880;&#29983;&#25104;&#27169;&#22411;&#22312;CAVs&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#36825;&#31181;&#25972;&#21512;&#22914;&#20309;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#20132;&#36890;&#39046;&#22495;&#25972;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;CAV&#25216;&#26415;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#26088;&#22312;&#24378;&#35843;&#21462;&#24471;&#30340;&#36827;&#23637;&#12289;&#21097;&#20313;&#30340;&#38556;&#30861;&#20197;&#21450;&#22312;&#23433;&#20840;&#21644;&#21019;&#26032;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10559v1 Announce Type: cross  Abstract: This report investigates the history and impact of Generative Models and Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing progress in technology and transportation. By focusing on the application of generative models within the context of CAVs, the study aims to unravel how this integration could enhance predictive modeling, simulation accuracy, and decision-making processes in autonomous vehicles. This thesis discusses the benefits and challenges of integrating generative models and CAV technology in transportation. It aims to highlight the progress made, the remaining obstacles, and the potential for advancements in safety and innovation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#36974;&#30422;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#20351;&#29992;MixUp&#31574;&#30053;&#23545;&#20154;&#33080;&#22270;&#20687;&#36827;&#34892;&#36974;&#30422;&#65292;&#20197;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.10558</link><description>&lt;p&gt;
&#38024;&#23545;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#38544;&#31169;&#20445;&#25252;&#20154;&#33080;&#35782;&#21035;&#33258;&#36866;&#24212;&#28151;&#21512;&#36974;&#30422;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#36974;&#30422;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#20351;&#29992;MixUp&#31574;&#30053;&#23545;&#20154;&#33080;&#22270;&#20687;&#36827;&#34892;&#36974;&#30422;&#65292;&#20197;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10558v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22312;&#35757;&#32451;&#20154;&#33080;&#35782;&#21035;&#65288;FR&#65289;&#27169;&#22411;&#20013;&#21033;&#29992;&#20010;&#20154;&#25935;&#24863;&#25968;&#25454;&#23384;&#22312;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#65288;MIA&#65289;&#25512;&#26029;&#20986;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;&#25968;&#25454;&#22686;&#24378;&#21644;&#24046;&#20998;&#38544;&#31169;&#65292;&#24050;&#34987;&#29992;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#22312;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36798;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;MIA&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#36974;&#30422;&#31639;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#39057;&#22495;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;MixUp&#31574;&#30053;&#23545;&#20154;&#33080;&#22270;&#20687;&#36827;&#34892;&#36974;&#30422;&#12290;&#19982;&#20027;&#35201;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#20256;&#32479;MixUp&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20462;&#25913;&#30340;&#26041;&#27861;&#34701;&#21512;&#20102;&#39057;&#22495;&#28151;&#21512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#21152;MixUp&#20013;&#28151;&#21512;&#30340;&#22270;&#20687;&#25968;&#37327;&#21487;&#20197;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#20294;&#20250;&#38477;&#20302;&#20154;&#33080;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10558v1 Announce Type: cross  Abstract: The utilization of personal sensitive data in training face recognition (FR) models poses significant privacy concerns, as adversaries can employ model inversion attacks (MIA) to infer the original training data. Existing defense methods, such as data augmentation and differential privacy, have been employed to mitigate this issue. However, these methods often fail to strike an optimal balance between privacy and accuracy. To address this limitation, this paper introduces an adaptive hybrid masking algorithm against MIA. Specifically, face images are masked in the frequency domain using an adaptive MixUp strategy. Unlike the traditional MixUp algorithm, which is predominantly used for data augmentation, our modified approach incorporates frequency domain mixing. Previous studies have shown that increasing the number of images mixed in MixUp can enhance privacy preservation but at the expense of reduced face recognition accuracy. To ove
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20108;&#38454;&#20449;&#24687;&#65288;Hessian&#65289;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10557</link><description>&lt;p&gt;
&#20108;&#38454;&#20449;&#24687;&#24456;&#37325;&#35201;&#65306;&#37325;&#35775;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20108;&#38454;&#20449;&#24687;&#65288;Hessian&#65289;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;ChatGPT&#12289;LLaMa&#21644;Gemini&#31561;&#20027;&#35201;LLM&#20135;&#21697;&#20043;&#38388;&#30340;&#28608;&#28872;&#31454;&#20105;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#21508;&#31181;&#38382;&#39064;&#65288;&#22914;&#38544;&#31169;&#27844;&#38706;&#21644;&#29256;&#26435;&#20405;&#29359;&#65289;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#20197;LLM&#20174;&#19994;&#32773;&#30340;&#35270;&#35282;&#26469;&#30475;&#65292;&#22788;&#29702;&#36825;&#20123;&#24847;&#22806;&#30340;&#38544;&#31169;&#20405;&#29359;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#35299;&#20915;&#20102;LLMs&#30340;&#8220;&#36951;&#24536;&#8221;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#24320;&#38144;&#65292;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#25110;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#19982;&#22522;&#20110;&#19968;&#38454;&#20449;&#24687;&#30340;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#20108;&#38454;&#20449;&#24687;&#65288;Hessian&#65289;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#36951;&#24536;&#38382;&#39064;&#12290;&#21463;&#32463;&#20856;&#29275;&#39039;&#26356;&#26032;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#36951;&#24536;&#31639;&#27861;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10557v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), we have witnessed intense competition among the major LLM products like ChatGPT, LLaMa, and Gemini. However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of LLM practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning" problem of LLMs using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are 
&lt;/p&gt;</description></item><item><title>KARINA&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;ConvNext&#12289;SENet&#21644;Geocyclic&#22635;&#20805;&#65292;&#22312;2.5&#176;&#20998;&#36776;&#29575;&#19979;&#25552;&#21319;&#22825;&#27668;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21482;&#38656;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23637;&#31034;&#20986;&#19982;&#26356;&#39640;&#20998;&#36776;&#29575;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.10555</link><description>&lt;p&gt;
KARINA&#65306;&#19968;&#31181;&#29992;&#20110;&#20840;&#29699;&#22825;&#27668;&#39044;&#27979;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KARINA: An Efficient Deep Learning Model for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10555
&lt;/p&gt;
&lt;p&gt;
KARINA&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;ConvNext&#12289;SENet&#21644;Geocyclic&#22635;&#20805;&#65292;&#22312;2.5&#176;&#20998;&#36776;&#29575;&#19979;&#25552;&#21319;&#22825;&#27668;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21482;&#38656;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23637;&#31034;&#20986;&#19982;&#26356;&#39640;&#20998;&#36776;&#29575;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#27668;&#20505;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#29305;&#21035;&#26159;&#22312;&#20840;&#29699;&#22825;&#27668;&#39044;&#27979;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20197;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#20840;&#29699;&#22825;&#27668;&#25968;&#25454;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KARINA&#30340;&#26032;&#27169;&#22411;&#65292;&#20197;&#20811;&#26381;&#35813;&#39046;&#22495;&#20856;&#22411;&#30340;&#24040;&#22823;&#35745;&#31639;&#38656;&#27714;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21482;&#38656;4&#20010;NVIDIA A100 GPU&#21644;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#21363;&#21487;&#23454;&#29616;&#19982;&#26356;&#39640;&#20998;&#36776;&#29575;&#23545;&#25163;&#30456;&#23218;&#32654;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#12290;KARINA&#32467;&#21512;&#20102;ConvNext&#12289;SENet&#21644;Geocyclic&#22635;&#20805;&#20197;&#22686;&#24378;&#20197;2.5&#176;&#20998;&#36776;&#29575;&#36827;&#34892;&#30340;&#22825;&#27668;&#39044;&#27979;&#65292;&#21487;&#20197;&#36807;&#28388;&#25481;&#39640;&#39057;&#22122;&#22768;&#12290;&#22320;&#29702;&#21608;&#26399;&#22635;&#20805;&#20445;&#30041;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20391;&#36793;&#30028;&#20687;&#32032;&#65292;&#20174;&#32780;&#22312;&#29699;&#24418;&#22320;&#29699;&#20013;&#20445;&#25345;&#22823;&#27668;&#27969;&#30340;&#36830;&#32493;&#24615;&#12290;SENet&#21160;&#24577;&#25913;&#21892;&#29305;&#24449;&#21709;&#24212;&#65292;&#25512;&#36827;&#20102;&#22823;&#27668;&#36807;&#31243;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10555v1 Announce Type: cross  Abstract: Deep learning-based, data-driven models are gaining prevalence in climate research, particularly for global weather prediction. However, training the global weather data at high resolution requires massive computational resources. Therefore, we present a new model named KARINA to overcome the substantial computational demands typical of this field. This model achieves forecasting accuracy comparable to higher-resolution counterparts with significantly less computational resources, requiring only 4 NVIDIA A100 GPUs and less than 12 hours of training. KARINA combines ConvNext, SENet, and Geocyclic Padding to enhance weather forecasting at a 2.5{\deg} resolution, which could filter out high-frequency noise. Geocyclic Padding preserves pixels at the lateral boundary of the input image, thereby maintaining atmospheric flow continuity in the spherical Earth. SENet dynamically improves feature response, advancing atmospheric process modeling,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20449;&#21495;&#23884;&#20837;LLM&#30340;&#26435;&#37325;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#19968;&#31181;&#27169;&#22411;&#32423;&#27700;&#21360;&#65292;&#26377;&#25928;&#36861;&#36394;&#29983;&#25104;&#25991;&#26412;&#30340;&#28389;&#29992;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#27700;&#21360;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#19988;&#26356;&#36866;&#24212;&#26032;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.10553</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#32473;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#28155;&#21152;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Learning to Watermark LLM-generated Text via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10553
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20449;&#21495;&#23884;&#20837;LLM&#30340;&#26435;&#37325;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#19968;&#31181;&#27169;&#22411;&#32423;&#27700;&#21360;&#65292;&#26377;&#25928;&#36861;&#36394;&#29983;&#25104;&#25991;&#26412;&#30340;&#28389;&#29992;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#27700;&#21360;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#19988;&#26356;&#36866;&#24212;&#26032;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#32473;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#28155;&#21152;&#27700;&#21360;&#65292;&#21363;&#23558;&#21487;&#20197;&#36890;&#36807;&#31639;&#27861;&#26816;&#27979;&#21040;&#30340;&#20449;&#21495;&#23884;&#20837;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20197;&#36861;&#36394;&#28389;&#29992;&#24773;&#20917;&#12290;&#19982;&#24403;&#21069;&#20027;&#27969;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;LLM&#35843;&#20248;&#38454;&#27573;&#32435;&#20837;&#27700;&#21360;&#27969;&#31243;&#65292;&#25193;&#23637;&#20102;&#27700;&#21360;&#35774;&#35745;&#31354;&#38388;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#22312;&#36755;&#20986;&#20013;&#23884;&#20837;&#20449;&#21495;&#30340;&#26631;&#35760;&#32423;&#27700;&#21360;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27169;&#22411;&#32423;&#27700;&#21360;&#65292;&#23558;&#20449;&#21495;&#23884;&#20837;LLM&#30340;&#26435;&#37325;&#20013;&#65292;&#21487;&#20197;&#34987;&#37197;&#23545;&#30340;&#26816;&#27979;&#22120;&#26816;&#27979;&#21040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#36845;&#20195;&#22320;&#65288;1&#65289;&#35757;&#32451;&#19968;&#20010;&#26816;&#27979;&#22120;&#26469;&#26816;&#27979;&#29983;&#25104;&#30340;&#24102;&#27700;&#21360;&#25991;&#26412;&#65292;&#24182;&#65288;2&#65289;&#35843;&#25972;LLM&#20197;&#29983;&#25104;&#26816;&#27979;&#22120;&#36731;&#26494;&#26816;&#27979;&#21040;&#20294;&#20445;&#25345;&#27491;&#24120;&#25928;&#29992;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27700;&#21360;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#19988;&#26356;&#36866;&#24212;&#65288;&#24212;&#23545;&#26032;&#25915;&#20987;&#65289;&#12290;&#23427;&#36824;&#20801;&#35768;&#27700;&#21360;&#27169;&#22411;&#24320;&#28304;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#19982;al&#19968;&#36215;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10553v1 Announce Type: cross  Abstract: We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse. Unlike the current mainstream methods that work with a fixed LLM, we expand the watermark design space by including the LLM tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the LLM weights, and such signals can be detected by a paired detector. We propose a co-training framework based on reinforcement learning that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the LLM to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks). It also allows watermarked model open-sourcing. In addition, if used together with al
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24072;&#29983;&#26080;&#25968;&#25454;&#30693;&#35782;&#20256;&#36755;&#65292;&#23454;&#29616;&#22312;&#26410;&#30693;&#38476;&#29983;&#22320;&#28857;&#19978;&#35757;&#32451;&#33258;&#23450;&#20301;&#27169;&#22411;&#65292;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#32769;&#24072;&#65292;&#36824;&#26377;&#25928;&#36991;&#20813;&#20381;&#36182;&#20110;&#24072;&#29983;&#31169;&#20154;&#25968;&#25454;&#21487;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10552</link><description>&lt;p&gt;
&#36890;&#36807;&#24072;&#29983;&#26080;&#25968;&#25454;&#30693;&#35782;&#20256;&#36755;&#22312;&#26410;&#30693;&#38476;&#29983;&#22320;&#28857;&#19978;&#35757;&#32451;&#33258;&#23450;&#20301;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10552
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24072;&#29983;&#26080;&#25968;&#25454;&#30693;&#35782;&#20256;&#36755;&#65292;&#23454;&#29616;&#22312;&#26410;&#30693;&#38476;&#29983;&#22320;&#28857;&#19978;&#35757;&#32451;&#33258;&#23450;&#20301;&#27169;&#22411;&#65292;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#32769;&#24072;&#65292;&#36824;&#26377;&#25928;&#36991;&#20813;&#20381;&#36182;&#20110;&#24072;&#29983;&#31169;&#20154;&#25968;&#25454;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19968;&#20010;&#26426;&#22120;&#20154;&#22312;&#19968;&#20010;&#19968;&#33324;&#30340;&#24320;&#25918;&#19990;&#30028;&#20013;&#36816;&#34892;&#26102;&#65292;&#30446;&#21069;&#33258;&#23450;&#20301;&#27169;&#22411;&#30340;&#19968;&#20010;&#20856;&#22411;&#20551;&#35774;&#26159;&#30446;&#26631;&#24037;&#20316;&#31354;&#38388;&#20013;&#26377;&#19968;&#20010;&#24102;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#33324;&#24320;&#25918;&#19990;&#30028;&#20013;&#65292;&#36825;&#31181;&#24773;&#20917;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#19990;&#30028;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#26696;&#20013;&#65292;&#19968;&#20010;&#26426;&#22120;&#20154;&#65288;"&#23398;&#29983;"&#65289;&#21487;&#20197;&#21521;&#23427;&#22312;&#38476;&#29983;&#22320;&#28857;&#36935;&#21040;&#30340;&#20854;&#20182;&#26426;&#22120;&#20154;&#65288;"&#32769;&#24072;"&#65289;&#23547;&#27714;&#25351;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20174;&#32769;&#24072;&#27169;&#22411;&#37325;&#24314;&#19968;&#20010;&#20266;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#29992;&#20110;&#23398;&#29983;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#19982;&#20856;&#22411;&#30340;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#32769;&#24072;&#27169;&#22411;&#19978;&#21482;&#24341;&#20837;&#20102;&#26368;&#23567;&#30340;&#20551;&#35774;&#65292;&#20351;&#20854;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#32769;&#24072;&#65292;&#21253;&#25324;&#19981;&#21512;&#20316;&#30340;&#12289;&#26080;&#27861;&#35757;&#32451;&#30340;&#65288;&#22914;&#22270;&#20687;&#26816;&#32034;&#24341;&#25806;&#65289;&#21644;&#40657;&#21283;&#23376;&#32769;&#24072;&#65288;&#21363;&#25968;&#25454;&#38544;&#31169;&#65289;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20013;&#20381;&#36182;&#20110;&#32769;&#24072;&#30340;&#31169;&#20154;&#25968;&#25454;&#21487;&#29992;&#24615;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10552v1 Announce Type: cross  Abstract: A typical assumption in state-of-the-art self-localization models is that an annotated training dataset is available in the target workspace. However, this does not always hold when a robot travels in a general open-world. This study introduces a novel training scheme for open-world distributed robot systems. In our scheme, a robot ("student") can ask the other robots it meets at unfamiliar places ("teachers") for guidance. Specifically, a pseudo-training dataset is reconstructed from the teacher model and thereafter used for continual learning of the student model. Unlike typical knowledge transfer schemes, our scheme introduces only minimal assumptions on the teacher model, such that it can handle various types of open-set teachers, including uncooperative, untrainable (e.g., image retrieval engines), and blackbox teachers (i.e., data privacy). Rather than relying on the availability of private data of teachers as in existing methods
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#20266;&#24322;&#24120;&#26679;&#26412;&#21644;&#20351;&#29992;&#21452;&#21521;&#24402;&#19968;&#21270;&#27969;&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#24322;&#24120;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.10550</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24322;&#24120;&#27969;&#37327;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10550
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#20266;&#24322;&#24120;&#26679;&#26412;&#21644;&#20351;&#29992;&#21452;&#21521;&#24402;&#19968;&#21270;&#27969;&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#24322;&#24120;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21508;&#31181;&#31867;&#22411;&#30340;&#24322;&#24120;&#27969;&#37327;&#23041;&#32961;&#30528;&#32593;&#32476;&#23433;&#20840;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#24322;&#24120;&#32593;&#32476;&#27969;&#37327;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20165;&#20351;&#29992;&#27491;&#24120;&#27969;&#37327;&#30340;&#19977;&#38454;&#27573;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20266;&#24322;&#24120;&#26679;&#26412;&#65292;&#26080;&#38656;&#20808;&#21069;&#23545;&#24322;&#24120;&#36827;&#34892;&#20102;&#35299;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24322;&#24120;&#25968;&#25454;&#30340;&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#37325;&#26500;&#26041;&#27861;&#26469;&#23398;&#20064;&#27491;&#24120;&#26679;&#26412;&#30340;&#28145;&#23618;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#21452;&#21521;&#27969;&#27169;&#22359;&#23558;&#36825;&#20123;&#34920;&#31034;&#24402;&#19968;&#21270;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#20026;&#20102;&#27169;&#25311;&#24322;&#24120;&#26679;&#26412;&#65292;&#25105;&#20204;&#21521;&#24402;&#19968;&#21270;&#21518;&#30340;&#34920;&#31034;&#28155;&#21152;&#22122;&#22768;&#65292;&#28982;&#21518;&#23558;&#20854;&#36890;&#36807;&#21452;&#21521;&#27969;&#27169;&#22359;&#30340;&#29983;&#25104;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#22120;&#26469;&#21306;&#20998;&#27491;&#24120;&#26679;&#26412;&#21644;&#20266;&#24322;&#24120;&#26679;&#26412;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20165;&#38656;&#35201;&#20004;&#20010;&#27169;&#22359;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10550v1 Announce Type: cross  Abstract: With the rapid development of the Internet, various types of anomaly traffic are threatening network security. We consider the problem of anomaly network traffic detection and propose a three-stage anomaly detection framework using only normal traffic. Our framework can generate pseudo anomaly samples without prior knowledge of anomalies to achieve the detection of anomaly data. Firstly, we employ a reconstruction method to learn the deep representation of normal samples. Secondly, these representations are normalized to a standard normal distribution using a bidirectional flow module. To simulate anomaly samples, we add noises to the normalized representations which are then passed through the generation direction of the bidirectional flow module. Finally, a simple classifier is trained to differentiate the normal samples and pseudo anomaly samples in the latent space. During inference, our framework requires only two modules to detec
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#21151;&#32791;&#26497;&#31471;&#36793;&#32536;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#35774;&#22791;&#31471;&#22495;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;14%&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#20165;&#38656;&#23569;&#20110;10 kB&#30340;&#20869;&#23384;&#21644;100&#20010;&#26631;&#35760;&#30340;&#35805;&#35821;&#65292;&#22312;&#36866;&#24212;&#22797;&#26434;&#30340;&#35821;&#38899;&#22122;&#22768;&#21518;&#33021;&#22815;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10549</link><description>&lt;p&gt;
&#22312;&#20302;&#21151;&#32791;&#26497;&#31471;&#36793;&#32536;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#36827;&#34892;&#20851;&#38190;&#35789;&#26816;&#27979;&#30340;&#35774;&#22791;&#31471;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge Embedded Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10549
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#21151;&#32791;&#26497;&#31471;&#36793;&#32536;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#35774;&#22791;&#31471;&#22495;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;14%&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#20165;&#38656;&#23569;&#20110;10 kB&#30340;&#20869;&#23384;&#21644;100&#20010;&#26631;&#35760;&#30340;&#35805;&#35821;&#65292;&#22312;&#36866;&#24212;&#22797;&#26434;&#30340;&#35821;&#38899;&#22122;&#22768;&#21518;&#33021;&#22815;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#26292;&#38706;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#26102;&#65292;&#20851;&#38190;&#35789;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#20250;&#19979;&#38477;&#12290;&#29616;&#22330;&#23545;&#20043;&#21069;&#26410;&#35265;&#22122;&#22768;&#30340;&#33258;&#36866;&#24212;&#23545;&#20110;&#24674;&#22797;&#20934;&#30830;&#24615;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#38656;&#35201;&#35774;&#22791;&#31471;&#23398;&#20064;&#26469;&#30830;&#20445;&#33258;&#36866;&#24212;&#36807;&#31243;&#23436;&#20840;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#35774;&#22791;&#31471;&#22495;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;14%&#30340;&#20934;&#30830;&#24615;&#22686;&#30410;&#65292;&#36229;&#36807;&#20102;&#24050;&#32463;&#31283;&#20581;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21040;10 kB&#30340;&#20869;&#23384;&#23454;&#29616;&#20102;&#35774;&#22791;&#31471;&#23398;&#20064;&#65292;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#30340;&#35805;&#35821;&#23601;&#33021;&#22312;&#36866;&#24212;&#22797;&#26434;&#30340;&#35821;&#38899;&#22122;&#22768;&#21518;&#25552;&#39640;5%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#22495;&#33258;&#36866;&#24212;&#21487;&#20197;&#22312;&#36229;&#20302;&#21151;&#32791;&#30340;&#24494;&#25511;&#21046;&#22120;&#19978;&#23454;&#29616;&#65292;&#20165;&#38656;806 mJ&#65292;&#19988;&#22312;&#22987;&#32456;&#25171;&#24320;&#30340;&#12289;&#30005;&#27744;&#20379;&#30005;&#30340;&#35774;&#22791;&#19978;&#20165;&#38656;14&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10549v1 Announce Type: cross  Abstract: Keyword spotting accuracy degrades when neural networks are exposed to noisy environments. On-site adaptation to previously unseen noise is crucial to recovering accuracy loss, and on-device learning is required to ensure that the adaptation process happens entirely on the edge device. In this work, we propose a fully on-device domain adaptation system achieving up to 14% accuracy gains over already-robust keyword spotting models. We enable on-device learning with less than 10 kB of memory, using only 100 labeled utterances to recover 5% accuracy after adapting to the complex speech noise. We demonstrate that domain adaptation can be achieved on ultra-low-power microcontrollers with as little as 806 mJ in only 14 s on always-on, battery-operated devices.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#20013;&#23547;&#25214;SOSP&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#20197;\emph{&#29420;&#31435;&#20110;&#32500;&#24230;}&#30340;&#31934;&#24230;&#20445;&#35777;&#39640;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;SOSP&#65292;&#20855;&#26377;&#23545;&#25239;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#21457;&#23637;&#20102;&#33021;&#22815;&#23481;&#24525;&#25968;&#25454;&#30772;&#22351;&#30340;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10547</link><description>&lt;p&gt;
&#22362;&#38887;&#30340;&#20108;&#38454;&#38750;&#20984;&#20248;&#21270;&#21450;&#20854;&#22312;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10547
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#20013;&#23547;&#25214;SOSP&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#20197;\emph{&#29420;&#31435;&#20110;&#32500;&#24230;}&#30340;&#31934;&#24230;&#20445;&#35777;&#39640;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;SOSP&#65292;&#20855;&#26377;&#23545;&#25239;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#21457;&#23637;&#20102;&#33021;&#22815;&#23481;&#24525;&#25968;&#25454;&#30772;&#22351;&#30340;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#36817;&#20284;&#30340;&#20108;&#38454;&#31283;&#23450;&#28857;&#65288;SOSP&#65289;&#26159;&#38543;&#26426;&#38750;&#20984;&#20248;&#21270;&#20013;&#19968;&#20010;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#29702;&#35299;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#38750;&#20984;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#20013;&#23547;&#25214;SOSPs&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#19968;&#23450;&#37096;&#20998;&#30340;&#25968;&#25454;&#28857;&#34987;&#20219;&#24847;&#30772;&#22351;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23547;&#25214;&#36817;&#20284;SOSP&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#20855;&#26377;\emph{&#29420;&#31435;&#20110;&#32500;&#24230;}&#30340;&#31934;&#24230;&#20445;&#35777;&#65292;&#20351;&#29992;$\widetilde{O}({D^2}/{\epsilon})$&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;$D$&#26159;&#29615;&#22659;&#32500;&#24230;&#65292;$\epsilon$&#26159;&#34987;&#30772;&#22351;&#25968;&#25454;&#28857;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10547v1 Announce Type: cross  Abstract: Finding an approximate second-order stationary point (SOSP) is a well-studied and fundamental problem in stochastic nonconvex optimization with many applications in machine learning. However, this problem is poorly understood in the presence of outliers, limiting the use of existing nonconvex algorithms in adversarial settings.   In this paper, we study the problem of finding SOSPs in the strong contamination model, where a constant fraction of datapoints are arbitrarily corrupted. We introduce a general framework for efficiently finding an approximate SOSP with \emph{dimension-independent} accuracy guarantees, using $\widetilde{O}({D^2}/{\epsilon})$ samples where $D$ is the ambient dimension and $\epsilon$ is the fraction of corrupted datapoints.   As a concrete application of our framework, we apply it to the problem of low rank matrix sensing, developing efficient and provably robust algorithms that can tolerate corruptions in both 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;GNN&#30340;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24322;&#36136;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#30340;&#21306;&#20998;&#24230;&#65292;&#24182;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10543</link><description>&lt;p&gt;
&#36890;&#36807;GNN&#30340;&#21453;&#21521;&#36807;&#31243;&#21306;&#20998;&#24322;&#36136;&#22270;&#20013;&#30340;&#37051;&#23621;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Distinguishing Neighborhood Representations Through Reverse Process of GNNs for Heterophilic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10543
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;GNN&#30340;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24322;&#36136;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#30340;&#21306;&#20998;&#24230;&#65292;&#24182;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graph Neural Network&#65288;GNN&#65289;&#31867;&#20284;&#20110;&#25193;&#25955;&#36807;&#31243;&#65292;&#22312;&#22534;&#21472;&#35768;&#22810;&#23618;&#26102;&#23548;&#33268;&#23398;&#20064;&#34920;&#31034;&#36807;&#24230;&#24179;&#28369;&#12290;&#22240;&#27492;&#65292;&#28040;&#24687;&#20256;&#36882;&#30340;&#21453;&#21521;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#21453;&#36716;&#27491;&#21521;&#28040;&#24687;&#20256;&#25773;&#26469;&#38160;&#21270;&#33410;&#28857;&#34920;&#31034;&#12290;&#38160;&#21270;&#21518;&#30340;&#34920;&#31034;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#37051;&#23621;&#33410;&#28857;&#65292;&#20363;&#22914;&#22312;&#24322;&#36136;&#22270;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#21453;&#21521;&#36807;&#31243;&#30340;&#35774;&#35745;&#21407;&#21017;&#24212;&#29992;&#20110;GNN&#30340;&#19977;&#20010;&#21464;&#20307;&#12290;&#36890;&#36807;&#22312;&#24322;&#36136;&#22270;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#38656;&#35201;&#20855;&#26377;&#19981;&#21516;&#34920;&#31034;&#25165;&#33021;&#25104;&#21151;&#20998;&#31867;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21453;&#21521;&#36807;&#31243;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#26174;&#30528;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#21453;&#21521;&#26426;&#21046;&#21487;&#20197;&#20943;&#36731;&#25968;&#30334;&#23618;&#19978;&#30340;&#36807;&#24230;&#24179;&#28369;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10543v1 Announce Type: cross  Abstract: Graph Neural Network (GNN) resembles the diffusion process, leading to the over-smoothing of learned representations when stacking many layers. Hence, the reverse process of message passing can sharpen the node representations by inverting the forward message propagation. The sharpened representations can help us to better distinguish neighboring nodes with different labels, such as in heterophilic graphs. In this work, we apply the design principle of the reverse process to the three variants of the GNNs. Through the experiments on heterophilic graph data, where adjacent nodes need to have different representations for successful classification, we show that the reverse process significantly improves the prediction performance in many cases. Additional analysis reveals that the reverse mechanism can mitigate the over-smoothing over hundreds of layers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MATADOR&#65292;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#29255;&#19978;Tsetlin&#26426;&#35774;&#35745;&#29983;&#25104;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#23558;ML&#27169;&#22411;&#36716;&#25442;&#20026;SoC-FPGA&#35299;&#20915;&#26041;&#26696;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10538</link><description>&lt;p&gt;
MATADOR&#65306;&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#29255;&#19978;Tsetlin&#26426;&#35774;&#35745;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for Edge Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MATADOR&#65292;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#29255;&#19978;Tsetlin&#26426;&#35774;&#35745;&#29983;&#25104;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#23558;ML&#27169;&#22411;&#36716;&#25442;&#20026;SoC-FPGA&#35299;&#20915;&#26041;&#26696;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
System-on-Chip Field-Programmable Gate Array&#65288;SoC-FPGA&#65289;&#36890;&#36807;&#35774;&#35745;&#21327;&#22788;&#29702;&#22120;&#21152;&#36895;&#22120;&#31995;&#32479;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36793;&#32536;&#25512;&#26029;&#24212;&#29992;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#21534;&#21520;&#37327;&#22686;&#30410;&#12290;&#28982;&#32780;&#65292;&#23558;ML&#27169;&#22411;&#35757;&#32451;&#24182;&#32763;&#35793;&#20026;SoC-FPGA&#35299;&#20915;&#26041;&#26696;&#30340;&#35774;&#35745;&#24037;&#20316;&#21487;&#33021;&#26159;&#30456;&#24403;&#22823;&#30340;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#26469;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#12289;&#21151;&#32791;&#12289;&#24310;&#36831;&#21644;&#36164;&#28304;&#21033;&#29992;&#29575;&#20043;&#38388;&#30340; trade-offs&#12290;&#19982;&#20854;&#20182;ML&#31639;&#27861;&#30456;&#21453;&#65292;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#36890;&#36807;&#20174;Tsetlin&#33258;&#21160;&#26426;&#65288;&#23398;&#20064;&#20803;&#32032;&#65289;&#21644;&#24067;&#23572;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#24418;&#25104;&#36923;&#36753;&#21629;&#39064;&#26469;&#25191;&#34892;&#20998;&#31867;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;TM&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#24456;&#39640;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#19988;&#36825;&#20123;&#36923;&#36753;&#21629;&#39064;&#22312;&#31867;&#21035;&#20869;&#37096;&#21644;&#31867;&#21035;&#20043;&#38388;&#37117;&#26377;&#30456;&#24403;&#22823;&#30340;&#37325;&#21472;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#26497;&#23569;&#37327;&#30340;AND&#21644;NOT&#38376;&#36716;&#25442;&#20026;RTL&#32423;&#30340;&#35774;&#35745;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MATADOR&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24067;&#23572;&#21040;si
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10538v1 Announce Type: cross  Abstract: System-on-Chip Field-Programmable Gate Arrays (SoC-FPGAs) offer significant throughput gains for machine learning (ML) edge inference applications via the design of co-processor accelerator systems. However, the design effort for training and translating ML models into SoC-FPGA solutions can be substantial and requires specialist knowledge aware trade-offs between model performance, power consumption, latency and resource utilization. Contrary to other ML algorithms, Tsetlin Machine (TM) performs classification by forming logic proposition between boolean actions from the Tsetlin Automata (the learning elements) and boolean input features. A trained TM model, usually, exhibits high sparsity and considerable overlapping of these logic propositions both within and among the classes. The model, thus, can be translated to RTL-level design using a miniscule number of AND and NOT gates. This paper presents MATADOR, an automated boolean-to-si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22522;&#20110;&#36924;&#30495;3D&#22270;&#24418;&#24314;&#27169;&#12289;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#12289;&#24046;&#20998;&#31070;&#32463;&#28210;&#26579;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10075</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey of synthetic data augmentation methods in computer vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22522;&#20110;&#36924;&#30495;3D&#22270;&#24418;&#24314;&#27169;&#12289;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#12289;&#24046;&#20998;&#31070;&#32463;&#28210;&#26579;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#20351;&#29992;&#20195;&#34920;&#30446;&#26631;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#35757;&#32451;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#33719;&#21462;&#36275;&#22815;&#30340;&#30446;&#26631;&#20219;&#21153;&#22270;&#20687;&#25968;&#25454;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#26159;&#26126;&#30830;&#22320;&#20197;&#25152;&#38656;&#30340;&#26041;&#24335;&#36716;&#25442;&#29616;&#26377;&#22270;&#20687;&#65292;&#20197;&#21019;&#24314;&#23454;&#29616;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#24615;&#12290;&#22312;&#26080;&#27861;&#35775;&#38382;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#20174;&#38646;&#24320;&#22987;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;--&#21363;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24191;&#27867;&#35780;&#20272;&#12290;&#23427;&#28085;&#30422;&#20102;&#22522;&#20110;&#36924;&#30495;3D&#22270;&#24418;&#24314;&#27169;&#12289;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#65288;NST&#65289;&#12289;&#24046;&#20998;&#31070;&#32463;&#28210;&#26579;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10075v1 Announce Type: cross  Abstract: The standard approach to tackling computer vision problems is to train deep convolutional neural network (CNN) models using large-scale image datasets which are representative of the target task. However, in many scenarios, it is often challenging to obtain sufficient image data for the target task. Data augmentation is a way to mitigate this challenge. A common practice is to explicitly transform existing images in desired ways so as to create the required volume and variability of training data necessary to achieve good generalization performance. In situations where data for the target domain is not accessible, a viable workaround is to synthesize training data from scratch--i.e., synthetic data augmentation. This paper presents an extensive review of synthetic data augmentation techniques. It covers data synthesis approaches based on realistic 3D graphics modeling, neural style transfer (NST), differential neural rendering, and gen
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#21253;&#21547;&#22312;&#22312;&#32447;&#31639;&#27861;&#30340;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#36827;&#34892;&#21551;&#21160;&#65292;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#20110;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#24341;&#23548;&#22312;&#32447;&#25506;&#32034;&#30340;&#21487;&#35777;&#26126;&#25910;&#30410;&#65292;&#21363;&#20351;&#31163;&#32447;&#25968;&#25454;&#38598;&#27809;&#26377;&#21333;&#19968;&#31574;&#30053;&#21487;&#38598;&#20013;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09701</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#26377;&#38480;&#35206;&#30422;&#30340;&#28151;&#21512;RL&#22312;&#32447;&#31639;&#27861;&#30340;&#33258;&#28982;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
A Natural Extension To Online Algorithms For Hybrid RL With Limited Coverage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09701
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#21253;&#21547;&#22312;&#22312;&#32447;&#31639;&#27861;&#30340;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#36827;&#34892;&#21551;&#21160;&#65292;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#20110;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#24341;&#23548;&#22312;&#32447;&#25506;&#32034;&#30340;&#21487;&#35777;&#26126;&#25910;&#30410;&#65292;&#21363;&#20351;&#31163;&#32447;&#25968;&#25454;&#38598;&#27809;&#26377;&#21333;&#19968;&#31574;&#30053;&#21487;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32467;&#21512;&#22312;&#32447;&#21644;&#31163;&#32447;&#25968;&#25454;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20851;&#20110;&#20854;&#21487;&#35777;&#26126;&#30410;&#22788;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#28151;&#21512;RL&#31639;&#27861;&#23545;&#31163;&#32447;&#25968;&#25454;&#38598;&#26045;&#21152;&#35206;&#30422;&#20551;&#35774;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#36825;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#19968;&#20010;&#35774;&#35745;&#33391;&#22909;&#30340;&#22312;&#32447;&#31639;&#27861;&#24212;&#35813;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#8220;&#22635;&#34917;&#31354;&#30333;&#8221;&#65292;&#25506;&#32034;&#34892;&#20026;&#31574;&#30053;&#26410;&#25506;&#32034;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#20272;&#35745;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20197;&#24341;&#23548;&#22312;&#32447;&#25506;&#32034;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#34920;&#26126;&#23545;&#26631;&#20934;&#20048;&#35266;&#22312;&#32447;&#31639;&#27861;&#30340;&#19968;&#20010;&#33258;&#28982;&#25193;&#23637;&#8212;&#8212;&#36890;&#36807;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#21253;&#21547;&#22312;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#26469;&#21551;&#21160;&#23427;&#20204;&#8212;&#8212;&#21363;&#20351;&#31163;&#32447;&#25968;&#25454;&#38598;&#27809;&#26377;&#21333;&#19968;&#31574;&#30053;&#21487;&#38598;&#20013;&#24615;&#65292;&#20063;&#21487;&#23454;&#29616;&#28151;&#21512;&#25968;&#25454;&#30340;&#31867;&#20284;&#21487;&#35777;&#26126;&#25910;&#30410;&#12290;&#25105;&#20204;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09701v1 Announce Type: new  Abstract: Hybrid Reinforcement Learning (RL), leveraging both online and offline data, has garnered recent interest, yet research on its provable benefits remains sparse. Additionally, many existing hybrid RL algorithms (Song et al., 2023; Nakamoto et al., 2023; Amortila et al., 2024) impose coverage assumptions on the offline dataset, but we show that this is unnecessary. A well-designed online algorithm should "fill in the gaps" in the offline dataset, exploring states and actions that the behavior policy did not explore. Unlike previous approaches that focus on estimating the offline data distribution to guide online exploration (Li et al., 2023b), we show that a natural extension to standard optimistic online algorithms -- warm-starting them by including the offline dataset in the experience replay buffer -- achieves similar provable gains from hybrid data even when the offline dataset does not have single-policy concentrability. We accomplish
&lt;/p&gt;</description></item><item><title>Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09629</link><description>&lt;p&gt;
Quiet-STaR: &#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#24049;&#23398;&#20250;&#24605;&#32771;&#21518;&#20877;&#35828;&#35805;
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09629
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#21644;&#20132;&#35848;&#26102;&#65292;&#20154;&#20204;&#26377;&#26102;&#20250;&#20572;&#19979;&#26469;&#24605;&#32771;&#12290;&#23613;&#31649;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#25512;&#29702;&#26694;&#23450;&#20026;&#22238;&#31572;&#38382;&#39064;&#25110;&#23436;&#25104;&#20195;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20294;&#25512;&#29702;&#20960;&#20046;&#37117;&#38544;&#21547;&#22312;&#25152;&#26377;&#20070;&#38754;&#25991;&#26412;&#20013;&#12290;&#20363;&#22914;&#65292;&#36825;&#36866;&#29992;&#20110;&#35777;&#26126;&#20013;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#27493;&#39588;&#65292;&#20197;&#21450;&#25903;&#25745;&#23545;&#35805;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#22312;&#33258;&#23398;&#20064;&#25512;&#29702;&#32773;&#65288;STaR&#65292;Zelikman&#31561;&#65292;2022&#65289;&#20013;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#25512;&#26029;&#26469;&#33258;&#38382;&#31572;&#20013;&#26377;&#29992;&#30340;&#24605;&#32771;&#65292;&#24182;&#23398;&#20064;&#37027;&#20123;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#24605;&#32771;&#12290;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;--&#29702;&#24819;&#24773;&#20917;&#19979;, &#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20250;&#20174;&#20219;&#24847;&#25991;&#26412;&#20013;&#25512;&#26029;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#24605;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;Quiet-STaR&#65292;&#36825;&#26159;STaR&#30340;&#19968;&#20010;&#27867;&#21270;&#29256;&#26412;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#39044;&#27979;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;1&#65289;&#29983;&#25104;&#36830;&#32493;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.09603</link><description>&lt;p&gt;
&#25511;&#21046;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#36827;&#34892;&#20048;&#35266;&#21487;&#39564;&#35777;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Optimistic Verifiable Training by Controlling Hardware Nondeterminism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#26085;&#30410;&#22686;&#21152;&#30340;&#35745;&#31639;&#38656;&#27714;&#23548;&#33268;&#20102;&#20026;&#32570;&#20047;&#24517;&#35201;&#36164;&#28304;&#30340;&#23458;&#25143;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#26381;&#21153;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#35757;&#32451;&#30340;&#27491;&#30830;&#24615;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#35757;&#32451;&#26102;&#25915;&#20987;&#65292;&#20363;&#22914;&#25968;&#25454;&#27602;&#21270;&#65292;&#37117;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#21487;&#39564;&#35777;&#35757;&#32451;&#30340;&#24037;&#20316;&#20027;&#35201;&#20998;&#20026;&#20004;&#31867;&#65306;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#38656;&#35201;&#21152;&#23494;&#25216;&#26415;&#32780;&#38590;&#20197;&#25193;&#23637;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#19968;&#20010;&#21487;&#20449;&#31532;&#19977;&#26041;&#23457;&#35745;&#21592;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#30340;&#8220;&#20048;&#35266;&#8221;&#26041;&#27861;&#12290; &#21518;&#32773;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;GPU&#31867;&#22411;&#20043;&#38388;&#30340;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#38459;&#27490;&#23457;&#35745;&#21592;&#31934;&#30830;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#26041;&#26696;&#19981;&#22815;&#20581;&#22766;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#19979;&#36827;&#34892;&#65292;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#22235;&#33293;&#20116;&#20837;&#65292;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09603v1 Announce Type: cross  Abstract: The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LAN&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#27963;&#21160;&#32423;&#21035;&#36827;&#34892;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65292;&#24182;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.09209</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#36866;&#24212;&#37051;&#23621;&#20197;&#23454;&#26102;&#26816;&#27979;&#20869;&#37096;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LAN&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#27963;&#21160;&#32423;&#21035;&#36827;&#34892;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65292;&#24182;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#21644;&#32452;&#32455;&#38754;&#20020;&#26469;&#33258;&#20869;&#37096;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;&#20808;&#21069;&#20851;&#20110;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65288;ITD&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26816;&#27979;&#24322;&#24120;&#29992;&#25143;&#25110;&#24322;&#24120;&#26102;&#38388;&#27573;&#65288;&#20363;&#22914;&#65292;&#19968;&#21608;&#25110;&#19968;&#22825;&#65289;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21487;&#33021;&#22312;&#26085;&#24535;&#20013;&#26377;&#25968;&#21313;&#19975;&#26465;&#27963;&#21160;&#65292;&#21363;&#20351;&#22312;&#19968;&#22825;&#20869;&#65292;&#19968;&#20010;&#29992;&#25143;&#20063;&#21487;&#33021;&#23384;&#22312;&#25968;&#21315;&#26465;&#27963;&#21160;&#65292;&#36825;&#38656;&#35201;&#39640;&#26114;&#30340;&#35843;&#26597;&#39044;&#31639;&#26469;&#39564;&#35777;&#24322;&#24120;&#29992;&#25143;&#25110;&#27963;&#21160;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#26159;&#20107;&#21518;&#26041;&#27861;&#32780;&#19981;&#26159;&#23454;&#26102;&#26816;&#27979;&#65292;&#26080;&#27861;&#21450;&#26102;&#25253;&#21578;&#20869;&#37096;&#23041;&#32961;&#22312;&#24341;&#36215;&#25439;&#22833;&#20043;&#21069;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38024;&#23545;&#27963;&#21160;&#32423;&#21035;&#23454;&#26102;ITD&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#21644;&#39640;&#25928;&#30340;&#26694;&#26550;LAN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LAN&#21516;&#26102;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09209v1 Announce Type: cross  Abstract: Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between ac
&lt;/p&gt;</description></item><item><title>AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09113</link><description>&lt;p&gt;
AutoLoRA&#65306;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#33258;&#21160;&#35843;&#25972;&#30697;&#38453;&#31209;&#22312;&#20302;&#31209;&#36866;&#24212;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09113
&lt;/p&gt;
&lt;p&gt;
AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20043;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#23384;&#22312;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#21457;&#20102;&#20960;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#36890;&#36807;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#20043;&#19978;&#24494;&#35843;&#20302;&#31209;&#22686;&#37327;&#26356;&#26032;&#30697;&#38453;&#65292;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;LoRA&#22312;&#25152;&#26377;&#23618;&#20013;&#22343;&#21248;&#20998;&#37197;&#31209;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#31351;&#20030;&#25628;&#32034;&#26469;&#25214;&#21040;&#26368;&#20339;&#31209;&#65292;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24494;&#35843;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoLoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#12290;AutoLoRA&#23558;&#20302;&#31209;&#26356;&#26032;&#30697;&#38453;&#20013;&#30340;&#27599;&#20010;&#31209;&#20026;1&#30340;&#30697;&#38453;&#19982;&#36873;&#25321;&#21464;&#37327;&#30456;&#20851;&#32852;&#65292;&#35813;&#21464;&#37327;&#20915;&#23450;&#20102;&#31209;&#20026;1&#30340;&#30697;&#38453;&#26159;&#21542;&#24212;&#35813;&#34987;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.08333</link><description>&lt;p&gt;
&#24555;&#36895;&#25512;&#26029;&#22522;&#20110;&#31227;&#38500;&#30340;&#33410;&#28857;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fast Inference of Removal-Based Node Influence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25429;&#33719;&#22270;&#20013;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#36235;&#21183;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#35757;&#32451;&#22909;&#30340;GNN&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#19968;&#20010;&#30495;&#23454;&#24212;&#29992;&#26159;&#65292;&#8220;&#22312;&#39044;&#27979;Twitter&#36134;&#25143;&#26497;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#26524;&#31227;&#38500;&#29305;&#23450;&#36134;&#25143;&#65292;&#20854;&#20182;&#36134;&#25143;&#30340;&#26497;&#24615;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#8221;&#25105;&#20204;&#23558;GNN&#20316;&#20026;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#21487;&#20197;&#27169;&#25311;&#31227;&#38500;&#33410;&#28857;&#24341;&#36215;&#30340;&#33410;&#28857;&#25110;&#36793;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#24433;&#21709;&#65292;&#19968;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#20132;&#26367;&#31227;&#38500;&#27599;&#20010;&#33410;&#28857;&#65292;&#24182;&#22312;&#20462;&#25913;&#21518;&#30340;&#22270;&#19978;&#24212;&#29992;&#35757;&#32451;&#22909;&#30340;GNN&#12290;&#36825;&#26159;&#21487;&#38752;&#30340;&#20294;&#32791;&#26102;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#23376;&#27169;&#20989;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08199</link><description>&lt;p&gt;
&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Submodular Peripteral Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08199
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#23376;&#27169;&#20989;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#20989;&#25968;&#23545;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23454;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#33719;&#21462;&#23427;&#20204;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23376;&#27169;&#20989;&#25968;&#21442;&#25968;&#21270;&#26063;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#36830;&#25509;&#24182;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08199v1 Announce Type: cross  Abstract: Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style "peripteral" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;TTS&#31995;&#32479;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#32780;&#38750;&#36882;&#24402;&#21333;&#20803;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#32463;&#27982;&#25104;&#26412;</title><link>https://arxiv.org/abs/2403.08164</link><description>&lt;p&gt;
EM-TTS&#65306;&#39640;&#25928;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#33945;&#21476;&#35821;&#36731;&#37327;&#32423;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08164
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;TTS&#31995;&#32479;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#32780;&#38750;&#36882;&#24402;&#21333;&#20803;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#32463;&#27982;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#21462;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#32467;&#26524;&#12290;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;TTS&#31995;&#32479;&#20013;&#24207;&#21015;&#25968;&#25454;&#30340;&#26631;&#20934;&#24314;&#27169;&#25216;&#26415;&#65292;&#24182;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21253;&#21547;RNN&#32452;&#20214;&#30340;TTS&#27169;&#22411;&#38656;&#35201;&#24378;&#22823;&#30340;GPU&#24615;&#33021;&#24182;&#19988;&#26102;&#38388;&#38271;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;CNN&#30340;&#24207;&#21015;&#21512;&#25104;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;TTS&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#30001;&#20110;&#20854;&#39640;&#24182;&#34892;&#24615;&#65292;&#21487;&#20197;&#20445;&#35777;&#19968;&#23450;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#36731;&#36825;&#20123;&#35757;&#32451;&#30340;&#32463;&#27982;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;TTS&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;TTS&#27169;&#22411;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#36882;&#24402;&#21333;&#20803;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;Text2Spectrum &#21644; SSRN&#12290;&#21069;&#32773;&#29992;&#20110;&#23558;&#38899;&#32032;&#32534;&#30721;&#20026;&#31895;&#31961;&#30340;&#26757;&#23572;&#39057;&#35889;&#22270;&#65292;&#21518;&#32773;&#29992;&#20110;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08164v1 Announce Type: cross  Abstract: Recently, deep learning-based Text-to-Speech (TTS) systems have achieved high-quality speech synthesis results. Recurrent neural networks have become a standard modeling technique for sequential data in TTS systems and are widely used. However, training a TTS model which includes RNN components requires powerful GPU performance and takes a long time. In contrast, CNN-based sequence synthesis techniques can significantly reduce the parameters and training time of a TTS model while guaranteeing a certain performance due to their high parallelism, which alleviate these economic costs of training. In this paper, we propose a lightweight TTS system based on deep convolutional neural networks, which is a two-stage training end-to-end TTS model and does not employ any recurrent units. Our model consists of two stages: Text2Spectrum and SSRN. The former is used to encode phonemes into a coarse mel spectrogram and the latter is used to synthesi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32467;&#26500;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21344;&#29992;&#37327;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;SBI&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07454</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#23616;&#37096;&#32447;&#24615;&#26144;&#23556;&#36827;&#34892;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#36731;&#37327;&#32423;&#30340;&#39034;&#24207;&#20223;&#30495;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07454
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21344;&#29992;&#37327;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;SBI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07454v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#38024;&#23545;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21487;&#20197;&#20351;&#29992;&#22810;&#27425;&#35843;&#29992;&#35745;&#31639;&#27169;&#25311;&#22120;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290; &#36825;&#20123;&#26041;&#27861;&#34987;&#32479;&#31216;&#20026;&#8220;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#8221;&#65288;SBI&#65289;&#12290; &#26368;&#36817;&#30340;SBI&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25552;&#20379;&#36817;&#20284;&#20294;&#34920;&#36798;&#20016;&#23500;&#30340;&#26500;&#36896;&#65292;&#29992;&#20110;&#19981;&#21487;&#29992;&#30340;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#26368;&#20339;&#25240;&#34935;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#20379;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#36817;&#20284;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#27010;&#29575;&#20998;&#24067;&#28151;&#21512;&#29289;&#12290; &#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;NN&#30340;SBI&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#20934;&#30830;&#30340;&#21518;&#39564;&#25512;&#26029;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21344;&#29992;&#37327;&#12290; &#25105;&#20204;&#22312;SBI&#25991;&#29486;&#20013;&#30340;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07454v1 Announce Type: cross  Abstract: Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators. These approaches are collectively known as "simulation-based inference" (SBI). Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution. However, they do not generally achieve an optimal trade-off between accuracy and computational demand. In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions. Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint. We illustrate our results on several benchmark models from the SBI literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AuG-KD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#38170;&#28857;&#30340;&#28151;&#21512;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07030</link><description>&lt;p&gt;
AuG-KD: &#22522;&#20110;&#38170;&#28857;&#30340;&#28151;&#21512;&#29983;&#25104;&#29992;&#20110;&#39046;&#22495;&#20043;&#22806;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AuG-KD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#38170;&#28857;&#30340;&#28151;&#21512;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#25110;&#19987;&#21033;&#38382;&#39064;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22823;&#22411;&#27169;&#22411;&#21457;&#24067;&#26102;&#19981;&#25552;&#20379;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#36825;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#30693;&#35782;&#36716;&#31227;&#21464;&#24471;&#20302;&#25928;&#19988;&#38382;&#39064;&#22797;&#26434;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#26041;&#27861;&#20316;&#20026;&#30452;&#25509;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#37319;&#29992;&#20174;DFKD&#27966;&#29983;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#22240;&#20026;&#25945;&#24072;&#35757;&#32451;&#25968;&#25454;&#19982;&#23454;&#38469;&#22330;&#26223;&#65288;&#23398;&#29983;&#39046;&#22495;&#65289;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#12290;&#36825;&#31181;&#24615;&#33021;&#19979;&#38477;&#28304;&#20110;&#25945;&#24072;&#30693;&#35782;&#20013;&#19981;&#36866;&#29992;&#20110;&#23398;&#29983;&#39046;&#22495;&#30340;&#37096;&#20998;&#65292;&#36825;&#20123;&#30693;&#35782;&#26159;&#29305;&#23450;&#20110;&#25945;&#24072;&#39046;&#22495;&#30340;&#65292;&#20250;&#21066;&#24369;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;DFKD&#20013;&#65292;&#26377;&#36873;&#25321;&#22320;&#36716;&#31227;&#36866;&#29992;&#20110;&#23398;&#29983;&#39046;&#22495;&#30340;&#25945;&#24072;&#30693;&#35782;&#25104;&#20026;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;AuG-KD&#12290;&#23427;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#26679;&#26412;&#29305;&#23450;&#30340;&#38170;&#28857;&#26469;&#23545;&#40784;&#23398;&#29983;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07030v1 Announce Type: new  Abstract: Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers' training data and real-world scenarios (student domain). The degradation stems from the portions of teachers' knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students' performance. Hence, selectively transferring teachers' appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-doma
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22914;&#20309;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#21457;&#29616;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06880</link><description>&lt;p&gt;
&#25581;&#31034;&#21463;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22312;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06880
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22914;&#20309;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#21457;&#29616;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24188;&#20799;&#20174;&#31232;&#30095;&#21453;&#39304;&#30340;&#33258;&#30001;&#25506;&#32034;&#36880;&#28176;&#21457;&#23637;&#20026;&#21033;&#29992;&#20808;&#21069;&#32463;&#39564;&#36827;&#34892;&#20197;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#23398;&#20064;&#65292;&#33719;&#24471;&#26356;&#23494;&#38598;&#22870;&#21169;&#12290;&#21463;&#27492;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#19981;&#21516;&#22870;&#21169;&#36716;&#25442;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#24847;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#20174;&#31232;&#30095;&#21040;&#22522;&#20110;&#28508;&#22312;&#30340;&#23494;&#38598;&#22870;&#21169;&#30340;&#36716;&#25442;&#65292;&#36825;&#20004;&#32773;&#20849;&#20139;&#26080;&#35770;&#22870;&#21169;&#21464;&#21270;&#22343;&#20026;&#26368;&#20339;&#31574;&#30053;&#12290;&#36890;&#36807;&#21253;&#25324;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#23548;&#33322;&#21644;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#22312;&#20869;&#30340;&#21508;&#31181;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22870;&#21169;&#36716;&#25442;&#26174;&#33879;&#24433;&#21709;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#21463;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#36825;&#20123;&#24615;&#33021;&#25351;&#26631;&#22806;&#65292;&#20351;&#29992;&#20132;&#21449;&#23494;&#24230;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36716;&#25442;&#65292;&#29305;&#21035;&#26159;S2D&#65292;&#20351;&#31574;&#30053;&#25439;&#22833;&#26223;&#35266;&#26356;&#21152;&#24179;&#28369;&#65292;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06880v1 Announce Type: cross  Abstract: Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting
&lt;/p&gt;</description></item><item><title>ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.06754</link><description>&lt;p&gt;
ALaRM: &#36890;&#36807;&#20998;&#23618;&#22870;&#21169;&#24314;&#27169;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ALaRM: Align Language Models via Hierarchical Rewards Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06754
&lt;/p&gt;
&lt;p&gt;
ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ALaRM&#65292;&#31532;&#19968;&#20010;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20174;&#20154;&#31867;&#21453;&#39304;&#27169;&#22411;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#40784;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#20154;&#31867;&#30417;&#30563;&#20449;&#21495;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36890;&#36807;&#23558;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#21644;&#19968;&#33268;&#22320;&#25351;&#23548;&#26397;&#30528;&#26399;&#26395;&#30340;&#32467;&#26524;&#21069;&#36827;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#21644;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26426;&#21046;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#20351;&#29992;gpt-3.5-turbo&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Ricci&#27969;&#30340;&#22823;&#33041;&#34920;&#38754;&#21327;&#26041;&#24046;&#25551;&#36848;&#31526;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#29992;&#20110;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;</title><link>https://arxiv.org/abs/2403.06645</link><description>&lt;p&gt;
&#22522;&#20110;Ricci&#27969;&#30340;&#22823;&#33041;&#34920;&#38754;&#21327;&#26041;&#24046;&#25551;&#36848;&#31526;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Ricci flow-based brain surface covariance descriptors for Alzheimer disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Ricci&#27969;&#30340;&#22823;&#33041;&#34920;&#38754;&#21327;&#26041;&#24046;&#25551;&#36848;&#31526;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#29992;&#20110;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;MRI&#22823;&#33041;&#25195;&#25551;&#20013;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#12290;&#38543;&#30528;3D&#25104;&#20687;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;3D&#25968;&#25454;&#37319;&#38598;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#26356;&#20855;&#21487;&#34892;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#27700;&#32447;&#65292;&#20174;&#30382;&#23618;&#34920;&#38754;&#21033;&#29992;Ricci&#33021;&#37327;&#20248;&#21270;&#25552;&#21462;&#26032;&#39062;&#30340;&#22522;&#20110;&#21327;&#26041;&#24046;&#30340;&#25551;&#36848;&#31526;&#65292;&#32780;&#38750;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#21521;&#37327;&#12290;&#36825;&#20123;&#21327;&#26041;&#24046;&#25551;&#36848;&#31526;&#26159;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#38750;&#32447;&#24615;&#27969;&#24418;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#27492;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#39640;&#26031;&#24452;&#21521;&#22522;&#20989;&#25968;&#23558;&#22522;&#20110;&#27969;&#24418;&#30340;&#20998;&#31867;&#24212;&#29992;&#20110;3D&#24418;&#29366;&#38382;&#39064;&#12290;&#23558;&#36825;&#19968;&#26032;&#39062;&#29305;&#24449;&#24212;&#29992;&#20110;&#24322;&#24120;&#30382;&#23618;&#33041;&#24418;&#24577;&#23398;&#20998;&#26512;&#20013;&#65292;&#21487;&#20197;&#29992;&#20110;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#22312;&#22823;&#32422;&#20004;&#30334;&#20010;3D MRI&#22823;&#33041;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36825;&#20123;&#27169;&#22411;&#26469;&#33258;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758; (ADNI) &#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06645v1 Announce Type: cross  Abstract: Automated feature extraction from MRI brain scans and diagnosis of Alzheimer's disease are ongoing challenges. With advances in 3D imaging technology, 3D data acquisition is becoming more viable and efficient than its 2D counterpart. Rather than using feature-based vectors, in this paper, for the first time, we suggest a pipeline to extract novel covariance-based descriptors from the cortical surface using the Ricci energy optimization. The covariance descriptors are components of the nonlinear manifold of symmetric positive-definite matrices, thus we focus on using the Gaussian radial basis function to apply manifold-based classification to the 3D shape problem. Applying this novel signature to the analysis of abnormal cortical brain morphometry allows for diagnosing Alzheimer's disease. Experimental studies performed on about two hundred 3D MRI brain models, gathered from Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset dem
&lt;/p&gt;</description></item><item><title>TrafficGPT &#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#31361;&#30772;&#20196;&#29260;&#38271;&#24230;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#38271;&#26102;&#38388;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#20013;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#21644;&#29983;&#25104;&#31526;&#21512;&#23454;&#38469;&#27169;&#24335;&#30340;&#27969;&#37327;&#26679;&#26412;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05822</link><description>&lt;p&gt;
TrafficGPT&#65306;&#31361;&#30772;&#20196;&#29260;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#38271;&#26102;&#38388;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic Analysis and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05822
&lt;/p&gt;
&lt;p&gt;
TrafficGPT &#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#31361;&#30772;&#20196;&#29260;&#38271;&#24230;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#38271;&#26102;&#38388;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#20013;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#21644;&#29983;&#25104;&#31526;&#21512;&#23454;&#38469;&#27169;&#24335;&#30340;&#27969;&#37327;&#26679;&#26412;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#20174;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#35813;&#39046;&#22495;&#21457;&#23637;&#21040;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#36825;&#31181;&#36827;&#27493;&#25552;&#39640;&#20102;&#26816;&#27979;&#22797;&#26434;&#27169;&#24335;&#21644;&#23433;&#20840;&#23041;&#32961;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#27979;&#35797;&#21644;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#38556;&#30861;&#65292;&#20363;&#22914;&#23545;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#20381;&#36182;&#20197;&#21450;&#29983;&#25104;&#36981;&#24490;&#23454;&#38469;&#27169;&#24335;&#30340;&#27969;&#37327;&#26679;&#26412;&#30340;&#22256;&#38590;&#12290;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#20581;&#22766;&#30340;&#25968;&#25454;&#34920;&#31034;&#26469;&#25552;&#20379;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#23384;&#22312;&#30410;&#22788;&#65292;&#20294;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#38754;&#20020;&#20196;&#29260;&#38271;&#24230;&#38480;&#21046;&#31561;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20840;&#38754;&#27969;&#37327;&#20998;&#26512;&#21644;&#23454;&#38469;&#27969;&#37327;&#29983;&#25104;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TrafficGPT&#65292;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05822v1 Announce Type: new  Abstract: Over the years, network traffic analysis and generation have advanced significantly. From traditional statistical methods, the field has progressed to sophisticated deep learning techniques. This progress has improved the ability to detect complex patterns and security threats, as well as to test and optimize network performance. However, obstacles persist, such as the dependence on labeled data for analysis and the difficulty of generating traffic samples that follow realistic patterns. Pre-trained deep neural networks have emerged as powerful tools to resolve these issues, offering improved performance by learning robust data representations from large unlabeled datasets. Despite their benefits, existing pre-trained models face challenges like token length limitation, which restricts their usefulness in comprehensive traffic analysis and realistic traffic generation. To address these challenges, we introduce TrafficGPT, a deep learning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MG-TSD&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#30446;&#26631;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;-of-the-art&#30340;&#39044;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.05751</link><description>&lt;p&gt;
MG-TSD&#65306;&#20855;&#26377;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#30340;&#22810;&#31890;&#24230;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MG-TSD&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#30446;&#26631;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;-of-the-art&#30340;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30001;&#20110;&#20854;&#29983;&#25104;&#39640;&#20445;&#30495;&#26679;&#26412;&#30340;&#26174;&#33879;&#33021;&#21147;&#32780;&#22312;&#29983;&#25104;&#24335;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24341;&#36215;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#29305;&#24615;&#24102;&#26469;&#30340;&#19981;&#31283;&#23450;&#24615;&#25361;&#25112;&#65292;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#22312;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24378;&#22823;&#24314;&#27169;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#31890;&#24230;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#65288;MG-TSD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#30340;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#20013;&#38388;&#25193;&#25955;&#27493;&#39588;&#30340;&#32473;&#23450;&#30446;&#26631;&#26469;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05751v1 Announce Type: cross  Abstract: Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate diffusion steps to guide the learning process of diffusion models. The way to construct the targets is motivated by the observation that the forward process of the diffusion model, which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.03835</link><description>&lt;p&gt;
Cobweb&#65306;&#19968;&#31181;&#22686;&#37327;&#21644;&#20998;&#23618;&#24335;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03835
&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#19982;&#20854;&#20182;&#22686;&#37327;&#20998;&#31867;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21033;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Cobweb&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#65292;&#22914;&#22522;&#26412;&#27700;&#24179;&#12289;&#20856;&#22411;&#24615;&#21644;&#25159;&#24418;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#23545;Cobweb&#20316;&#20026;&#20154;&#31867;&#20998;&#31867;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#23427;&#30830;&#23450;&#20102;Cobweb&#19982;&#32463;&#20856;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25928;&#24212;&#30340;&#19968;&#33268;&#24615;&#12290;&#36824;&#25506;&#35752;&#20102;Cobweb&#23637;&#29616;&#20986;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#26082;&#26377;&#23454;&#20363;&#21448;&#26377;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23558;&#26469;&#30740;&#31350;Cobweb&#20316;&#20026;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#30340;&#32508;&#21512;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
&lt;/p&gt;</description></item><item><title>DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.03768</link><description>&lt;p&gt;
DeepCRE&#65306;&#21033;&#29992;&#23574;&#31471;&#35745;&#31639;&#27169;&#22411;&#25913;&#38761;&#33647;&#29289;&#30740;&#21457;
&lt;/p&gt;
&lt;p&gt;
DeepCRE: Revolutionizing Drug R&amp;D with Cutting-Edge Computational Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03768
&lt;/p&gt;
&lt;p&gt;
DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#21644;&#27835;&#30103;&#24212;&#29992;&#39046;&#22495;&#37117;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#27835;&#30103;&#39046;&#22495;&#38656;&#35201;&#26356;&#22810;&#30340;&#27835;&#30103;&#36873;&#25321;&#65292;&#21516;&#26102;&#22823;&#37327;&#26377;&#21069;&#26223;&#30340;&#20020;&#24202;&#21069;&#33647;&#29289;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#22833;&#36133;&#12290;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;&#33647;&#29289;&#24320;&#21457;&#30340;&#21518;&#26399;&#38454;&#27573;&#20132;&#21449;&#33647;&#29289;&#21453;&#24212;&#35780;&#20272;&#65288;CRE&#65289;&#30340;&#19981;&#36275;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;CRE&#27169;&#22411;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23398;&#35201;&#20040;&#23616;&#38480;&#20110;&#26089;&#26399;&#24320;&#21457;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#23545;&#20840;&#38754;CRE&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepCRE&#30340;&#26032;&#22411;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;DeepCRE&#22312;&#25512;&#21160;&#27835;&#30103;&#21457;&#29616;&#21644;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;DeepCRE&#36890;&#36807;&#23454;&#29616;&#24739;&#32773;&#32423;&#21035;CRE&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;17.7\%&#65292;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepCRE&#24050;&#32463;&#30830;&#23450;&#20102;&#20845;&#20010;&#26174;&#31034;&#20986;&#26126;&#26174;&#26356;&#22823;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#35770;&#25209;&#37327;&#22823;&#23567;&#22914;&#20309;&#12290;</title><link>https://arxiv.org/abs/2403.02967</link><description>&lt;p&gt;
&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38750;&#20984;&#38543;&#26426;&#22797;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Convex Stochastic Composite Optimization with Polyak Momentum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#35770;&#25209;&#37327;&#22823;&#23567;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#30340;&#19968;&#20010;&#24378;&#22823;&#27867;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24403;&#38543;&#26426;&#22122;&#22768;&#26174;&#33879;&#26102;&#65288;&#21363;&#20165;&#20351;&#29992;&#23567;&#22411;&#25110;&#26377;&#30028;&#25209;&#37327;&#22823;&#23567;&#26102;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#20984;&#29615;&#22659;&#20013;&#26080;&#27861;&#25910;&#25947;&#12290;&#26412;&#25991;&#20851;&#27880;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#25209;&#37327;&#22823;&#23567;&#22823;&#23567;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;Polyak&#21160;&#37327;&#22312;&#22797;&#21512;&#20248;&#21270;&#29615;&#22659;&#20013;&#30340;&#26041;&#24046;&#20943;&#23569;&#25928;&#24212;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36817;&#31471;&#27493;&#39588;&#21482;&#33021;&#36890;&#36807;&#36817;&#20284;&#35299;&#26469;&#27714;&#35299;&#26102;&#65292;&#35813;&#26041;&#27861;&#20063;&#20250;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02967v1 Announce Type: cross  Abstract: The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35895;&#27468;&#24212;&#29992;&#21830;&#24215;&#19978;1200&#19975;&#26465;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#65292;&#30740;&#31350;&#20102;&#21313;&#24180;&#38388;&#38544;&#31169;&#35780;&#35770;&#30340;&#22823;&#35268;&#27169;&#36235;&#21183;&#65292;&#21457;&#29616;&#38544;&#31169;&#35780;&#35770;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#65292;&#25506;&#35752;&#20102;&#28909;&#38376;&#21644;&#36880;&#28176;&#20943;&#23569;&#30340;&#38544;&#31169;&#35805;&#39064;&#65292;&#20197;&#21450;&#19981;&#21516;&#22269;&#23478;&#29992;&#25143;&#23545;&#38544;&#31169;&#38382;&#39064;&#30475;&#27861;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.02292</link><description>&lt;p&gt;
Android&#24212;&#29992;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#30340;&#21313;&#24180;&#22823;&#35268;&#27169;&#36235;&#21183;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02292
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35895;&#27468;&#24212;&#29992;&#21830;&#24215;&#19978;1200&#19975;&#26465;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#65292;&#30740;&#31350;&#20102;&#21313;&#24180;&#38388;&#38544;&#31169;&#35780;&#35770;&#30340;&#22823;&#35268;&#27169;&#36235;&#21183;&#65292;&#21457;&#29616;&#38544;&#31169;&#35780;&#35770;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#65292;&#25506;&#35752;&#20102;&#28909;&#38376;&#21644;&#36880;&#28176;&#20943;&#23569;&#30340;&#38544;&#31169;&#35805;&#39064;&#65292;&#20197;&#21450;&#19981;&#21516;&#22269;&#23478;&#29992;&#25143;&#23545;&#38544;&#31169;&#38382;&#39064;&#30475;&#27861;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#35895;&#27468;&#24212;&#29992;&#21830;&#24215;&#19978;1200&#19975;&#26465;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#36825;&#20123;&#35780;&#35770;&#36328;&#36234;&#20102;10&#24180;&#26102;&#38388;&#12290;&#36890;&#36807;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#26102;&#38388;&#12289;&#22269;&#23478;&#12289;&#24212;&#29992;&#31867;&#22411;&#12289;&#19981;&#21516;&#38544;&#31169;&#20027;&#39064;&#20197;&#21450;&#22810;&#31181;&#24773;&#24863;&#32500;&#24230;&#19978;&#26816;&#35270;&#29992;&#25143;&#23545;&#38544;&#31169;&#38382;&#39064;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#25345;&#32493;&#22686;&#38271;&#65292;&#24182;&#25506;&#31350;&#20102;&#19968;&#20123;&#28909;&#38376;&#35805;&#39064;&#65288;&#22914;&#25968;&#25454;&#21024;&#38500;&#21644;&#25968;&#25454;&#31363;&#21462;&#65289;&#65292;&#20197;&#21450;&#19968;&#20123;&#36880;&#28176;&#20943;&#23569;&#30340;&#35805;&#39064;&#65288;&#22914;&#28041;&#21450;&#25935;&#24863;&#26435;&#38480;&#30340;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#65289;&#12290;&#23613;&#31649;&#38544;&#31169;&#35780;&#35770;&#26469;&#33258;200&#22810;&#20010;&#22269;&#23478;&#65292;&#20294;&#26377;33&#20010;&#22269;&#23478;&#25552;&#20379;&#20102;90%&#30340;&#38544;&#31169;&#35780;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#27599;&#20010;&#22269;&#23478;&#29992;&#25143;&#35780;&#35770;&#30340;&#38544;&#31169;&#20027;&#39064;&#20998;&#24067;&#26469;&#36827;&#34892;&#36328;&#22269;&#23478;&#27604;&#36739;&#65292;&#21457;&#29616;&#22320;&#29702;&#25509;&#36817;&#24182;&#19981;&#24847;&#21619;&#30528;&#38468;&#36817;&#22269;&#23478;&#26377;&#31867;&#20284;&#30340;&#38544;&#31169;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02292v1 Announce Type: new  Abstract: We present an analysis of 12 million instances of privacy-relevant reviews publicly visible on the Google Play Store that span a 10 year period. By leveraging state of the art NLP techniques, we can examine what users have been writing about privacy along multiple dimensions: time, countries, app types, diverse privacy topics, and even across a spectrum of emotions. We find consistent growth of privacy-relevant reviews, and explore topics that are trending (such as Data Deletion and Data Theft), as well as those on the decline (such as privacy-relevant reviews on sensitive permissions). We find that although privacy reviews come from more than 200 countries, 33 countries provide 90% of privacy reviews. We conduct a comparison across countries by examining the distribution of privacy topics a country's users write about, and find that geographic proximity is not a reliable indicator that nearby countries have similar privacy perspectives.
&lt;/p&gt;</description></item><item><title>TPLLM&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26377;&#38480;&#30340;&#22320;&#21306;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#21644;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.02221</link><description>&lt;p&gt;
TPLLM: &#22522;&#20110;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02221
&lt;/p&gt;
&lt;p&gt;
TPLLM&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26377;&#38480;&#30340;&#22320;&#21306;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#21644;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02221v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#20132;&#36890;&#39044;&#27979;&#26500;&#25104;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#33539;&#22260;&#20869;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#39640;&#31934;&#24230;&#39044;&#27979;&#23545;&#20110;&#26377;&#25928;&#20132;&#36890;&#31649;&#29702;&#20855;&#26377;&#28145;&#36828;&#24847;&#20041;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#30340;&#31934;&#24230;&#36890;&#24120;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#32780;&#21576;&#19978;&#21319;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#20840;&#38754;&#30340;&#20132;&#36890;&#26102;&#31354;&#25968;&#25454;&#24448;&#24448;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#20445;&#23384;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#22312;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26377;&#38480;&#30340;&#22320;&#21306;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#21644;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#24320;&#21457;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#36805;&#36895;&#21457;&#23637;&#30340;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36328;&#27169;&#24577;&#30693;&#35782;&#20256;&#36755;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02221v1 Announce Type: new  Abstract: Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management. The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01570</link><description>&lt;p&gt;
SERVAL&#65306;&#22402;&#30452;&#27169;&#22411;&#21644;LLM&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#23454;&#29616;&#38646;-shot&#32423;&#21035;&#30340;&#21307;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20986;&#23545;&#36890;&#29992;&#21644;&#24120;&#35782;&#38382;&#39064;&#21331;&#36234;&#30340;&#38646;-shot&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22402;&#30452;&#30693;&#35782;&#26041;&#38754;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#22402;&#30452;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#19987;&#23478;&#21442;&#19982;&#65292;&#22240;&#27492;&#22686;&#21152;&#20102;&#22686;&#24378;&#27169;&#22411;&#22402;&#30452;&#33021;&#21147;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23545;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#36827;&#34892;&#26080;&#30417;&#30563;&#24320;&#21457;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SERVAL&#21033;&#29992;LLMs&#30340;&#38646;-shot&#36755;&#20986;&#20316;&#20026;&#27880;&#37322;&#65292;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#26469;&#20174;&#22836;&#24320;&#22987;&#25945;&#25480;&#19968;&#20010;&#24378;&#22823;&#30340;&#22402;&#30452;&#27169;&#22411;&#12290;&#21453;&#36807;&#26469;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#22402;&#30452;&#27169;&#22411;&#24341;&#23548;LLM&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#38646;-shot&#33021;&#21147;&#65292;&#36880;&#27493;&#25913;&#36827;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
&lt;/p&gt;</description></item><item><title>Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01232</link><description>&lt;p&gt;
Polynormer: &#22810;&#39033;&#24335;&#34920;&#36798;&#30340;&#32447;&#24615;&#26102;&#38388;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Polynormer: Polynomial-Expressive Graph Transformer in Linear Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01232
&lt;/p&gt;
&lt;p&gt;
Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36716;&#25442;&#22120;&#65288;GTs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26550;&#26500;&#65292;&#29702;&#35770;&#19978;&#23427;&#27604;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26356;&#20855;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;GT&#27169;&#22411;&#33267;&#23569;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#32447;&#24615;GTs&#65292;&#20294;&#23427;&#20204;&#22312;&#20960;&#20010;&#28909;&#38376;&#22270;&#25968;&#25454;&#38598;&#19978;&#20173;&#33853;&#21518;&#20110;GNN&#23545;&#24212;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#21147;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#20102;&#24179;&#34913;GTs&#30340;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Polynormer&#65292;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#12290;Polynormer&#26500;&#24314;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#36755;&#20837;&#29305;&#24449;&#19978;&#23398;&#20064;&#39640;&#27425;&#22810;&#39033;&#24335;&#12290;&#20026;&#20102;&#20351;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#20998;&#24320;&#38598;&#25104;&#65292;&#20174;&#32780;&#20135;&#29983;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#20851;&#27880;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;Polynormer&#37319;&#29992;&#20102;&#32447;&#24615;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#20851;&#27880;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.00781</link><description>&lt;p&gt;
ChatDiet&#65306;&#36890;&#36807;LLM&#22686;&#24378;&#26694;&#26550;&#36171;&#33021;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#23545;&#20581;&#24247;&#30340;&#28145;&#36828;&#24433;&#21709;&#20351;&#24471;&#20808;&#36827;&#30340;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#26381;&#21153;&#25104;&#20026;&#24517;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#31561;&#20851;&#38190;&#20803;&#32032;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#21333;&#29420;&#30340;&#20351;&#29992;&#26410;&#33021;&#23454;&#29616;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#39537;&#21160;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChatDiet&#38598;&#25104;&#20102;&#20010;&#20154;&#21644;&#20154;&#32676;&#27169;&#22411;&#65292;&#36741;&#20197;&#19968;&#20010;&#21327;&#35843;&#22120;&#65292;&#26080;&#32541;&#26816;&#32034;&#21644;&#22788;&#29702;&#30456;&#20851;&#20449;&#24687;&#12290;&#20854;&#32467;&#26524;&#26159;&#21160;&#24577;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#39135;&#21697;&#25512;&#33616;&#65292;&#26681;&#25454;&#20010;&#20154;&#29992;&#25143;&#21916;&#22909;&#23450;&#21046;&#12290;&#25105;&#20204;&#23545;ChatDiet&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20010;&#20154;&#27169;&#22411;&#26469;&#20272;&#35745;&#20010;&#20154;&#33829;&#20859;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00033</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;&#27880;&#24847;&#21147;&#22823;&#33041;&#32593;&#32476;&#20998;&#26512;&#22823;&#40635;&#20351;&#29992;&#32773;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00033
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#40635;&#30340;&#25345;&#32493;&#20351;&#29992;&#26126;&#26174;&#24433;&#21709;&#20154;&#20204;&#30340;&#29983;&#27963;&#21644;&#20581;&#24247;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;HOGAB&#65288;High-Order Attention Graph Attention&#31070;&#32463;&#32593;&#32476;&#65289;&#27169;&#22411;&#65292;&#20197;&#20998;&#26512;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#23616;&#37096;&#24322;&#24120;&#33041;&#27963;&#21160;&#12290;HOGAB&#23558;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#19982;LSTM&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25429;&#25417;&#22823;&#40635;&#29992;&#25143;fMRI&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#23545;&#37051;&#22495;&#33410;&#28857;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#22686;&#24378;&#38271;&#26399;&#22823;&#40635;&#29992;&#25143;&#30340;&#31038;&#21306;&#32858;&#31867;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#22810;&#22270;&#20998;&#31867;&#20013;&#23454;&#29616;&#20102;85.1%&#30340;AUC&#21644;80.7%&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#32447;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;HODAB&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00033v1 Announce Type: cross  Abstract: The sustained use of marijuana significantly impacts the lives and health of people. In this study, we propose an interpretable novel framework called the HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze local abnormal brain activity in chronic marijuana users in two datasets. The HOGAB integrates dynamic intrinsic functional networks with LSTM technology to capture temporal patterns in fMRI time series of marijuana users. Moreover, we use the high-order attention module in neighborhood nodes for information fusion and message passing, enhancing community clustering analysis for long-term marijuana users. Furthermore, we improve the overall learning ability of the model by incorporating attention mechanisms, achieving an AUC of 85.1% and an accuracy of 80.7% in multigraph classification. In addition, we compare linear machine learning methods and evaluate the effectiveness of our proposed HODAB model. Speci
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#39318;&#27425;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.17736</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Algorithms for Graph Searching Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#39318;&#27425;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;Banerjee&#31561;&#20154;&#65288;2022&#24180;&#65289;&#26368;&#36817;&#25552;&#20986;&#30340;&#20855;&#26377;&#39044;&#27979;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20174;&#26576;&#20010;&#39030;&#28857;$r$&#20986;&#21457;&#30340;&#20195;&#29702;&#32773;&#24517;&#39035;&#22312;&#26368;&#23567;&#21270;&#24635;&#34892;&#31243;&#30340;&#21516;&#26102;&#36941;&#21382;&#19968;&#20010;&#65288;&#28508;&#22312;&#26410;&#30693;&#30340;&#65289;&#22270;$G$&#20197;&#25214;&#21040;&#38544;&#34255;&#30340;&#30446;&#26631;&#33410;&#28857;$g$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#22312;&#20219;&#24847;&#33410;&#28857;$v$&#22788;&#65292;&#20195;&#29702;&#32773;&#20250;&#25509;&#25910;&#21040;&#21040;$g$&#30340;&#36317;&#31163;&#30340;&#22122;&#22768;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#36825;&#31181;&#25628;&#32034;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#31532;&#19968;&#27425;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#38500;&#20102;&#23545;&#25239;&#24615;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#22806;&#65292;&#36824;&#22312;&#35823;&#24046;&#26159;&#38543;&#26426;&#30340;&#20856;&#22411;&#23454;&#20363;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;Banerjee&#31561;&#20154;&#31639;&#27861;&#30340;&#26367;&#20195;&#31616;&#21270;&#24615;&#33021;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17736v1 Announce Type: cross  Abstract: We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CDA&#26041;&#27861;W-MPOT&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20256;&#36882;&#35838;&#31243;&#65292;&#20005;&#26684;&#35299;&#20915;&#20102;&#39046;&#22495;&#25490;&#24207;&#21644;&#38169;&#35823;&#32047;&#31215;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23558;&#28304;&#27169;&#22411;&#36890;&#36807;&#22810;&#26465;&#26377;&#25928;&#36335;&#24452;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.16681</link><description>&lt;p&gt;
&#29992;&#22810;&#36335;&#24452;&#20256;&#36882;&#35838;&#31243;&#22686;&#24378;&#36830;&#32493;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Enhancing Continuous Domain Adaptation with Multi-Path Transfer Curriculum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CDA&#26041;&#27861;W-MPOT&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20256;&#36882;&#35838;&#31243;&#65292;&#20005;&#26684;&#35299;&#20915;&#20102;&#39046;&#22495;&#25490;&#24207;&#21644;&#38169;&#35823;&#32047;&#31215;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23558;&#28304;&#27169;&#22411;&#36890;&#36807;&#22810;&#26465;&#26377;&#25928;&#36335;&#24452;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#35299;&#20915;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#24040;&#22823;&#20998;&#24067;&#24046;&#24322;&#65292;&#36825;&#23548;&#33268;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#31561;&#39046;&#22495;&#30340;&#20986;&#29616;&#12290;&#26368;&#36817;&#65292;&#36830;&#32493;&#39046;&#22495;&#36866;&#24212;&#65288;CDA&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#20986;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31995;&#21015;&#20013;&#38388;&#22495;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CDA&#26041;&#27861;W-MPOT&#65292;&#20005;&#26684;&#35299;&#20915;&#20102;&#20808;&#21069;&#30740;&#31350;&#24573;&#35270;&#30340;&#39046;&#22495;&#25490;&#24207;&#21644;&#38169;&#35823;&#32047;&#31215;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;Wasserstein&#36317;&#31163;&#22312;&#28304;&#22495;&#21644;&#20013;&#38388;&#22495;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20256;&#36882;&#35838;&#31243;&#65292;&#36825;&#21463;&#21040;&#20102;&#23545;CDA&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20462;&#25913;&#29256;&#30340;&#36830;&#32493;&#26368;&#20248;&#20256;&#36755;&#65292;&#22312;&#35838;&#31243;&#20013;&#30340;&#22810;&#26465;&#26377;&#25928;&#36335;&#24452;&#19978;&#23558;&#28304;&#27169;&#22411;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#21452;&#21521;&#36335;&#24452;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#20197;&#20943;&#36731;&#36830;&#32493;&#36807;&#31243;&#20013;&#26144;&#23556;&#38169;&#35823;&#32047;&#31215;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16681v2 Announce Type: replace  Abstract: Addressing the large distribution gap between training and testing data has long been a challenge in machine learning, giving rise to fields such as transfer learning and domain adaptation. Recently, Continuous Domain Adaptation (CDA) has emerged as an effective technique, closing this gap by utilizing a series of intermediate domains. This paper contributes a novel CDA method, W-MPOT, which rigorously addresses the domain ordering and error accumulation problems overlooked by previous studies. Specifically, we construct a transfer curriculum over the source and intermediate domains based on Wasserstein distance, motivated by theoretical analysis of CDA. Then we transfer the source model to the target domain through multiple valid paths in the curriculum using a modified version of continuous optimal transport. A bidirectional path consistency constraint is introduced to mitigate the impact of accumulated mapping errors during contin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15776</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;MDP&#20013;&#30340;&#30495;&#27491;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Truly No-Regret Learning in Constrained MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24314;&#27169;&#23433;&#20840;&#32422;&#26463;&#30340;&#24120;&#35265;&#26041;&#24335;&#12290;&#30446;&#21069;&#29992;&#20110;&#39640;&#25928;&#35299;&#20915;CMDPs&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#12290;&#23545;&#20110;&#36825;&#20123;&#31639;&#27861;&#65292;&#25152;&#26377;&#24403;&#21069;&#24050;&#30693;&#30340;&#21518;&#24724;&#30028;&#37117;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#8212;&#8212;&#21487;&#20197;&#36890;&#36807;&#22312;&#19968;&#20010;&#22238;&#21512;&#20013;&#30340;&#32422;&#26463;&#36829;&#21453;&#26469;&#29992;&#20005;&#26684;&#30340;&#32422;&#26463;&#28385;&#36275;&#22312;&#21478;&#19968;&#20010;&#22238;&#21512;&#20013;&#12290;&#36825;&#20351;&#24471;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#19981;&#23433;&#20840;&#65292;&#22240;&#20026;&#23427;&#20165;&#20445;&#35777;&#26368;&#32456;&#65288;&#28151;&#21512;&#65289;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#20445;&#35777;&#23433;&#20840;&#12290;&#27491;&#22914;Efroni&#31561;&#20154;&#65288;2020&#24180;&#65289;&#25351;&#20986;&#30340;&#65292;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#21487;&#35777;&#26126;&#22320;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20851;&#20110;&#27491;&#21017;&#21270;&#21407;&#22987;-&#23545;&#20598;&#26041;&#26696;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;&#36890;&#29992;&#21270;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#19978;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21407;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15776v1 Announce Type: new  Abstract: Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in reinforcement learning. State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for error cancellations -- one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;OpenSUN3D&#30740;&#35752;&#20250;&#19978;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#21253;&#25324;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;</title><link>https://arxiv.org/abs/2402.15321</link><description>&lt;p&gt;
OpenSUN3D: &#24320;&#25918;&#35789;&#27719;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#31532;&#19968;&#27425;&#30740;&#35752;&#20250;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15321
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;OpenSUN3D&#30740;&#35752;&#20250;&#19978;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#21253;&#25324;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#27010;&#36848;&#20102;&#22312;2023&#24180;ICCV&#20250;&#35758;&#19978;&#20030;&#21150;&#30340;OpenSUN3D Workshop&#20851;&#20110;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#35752;&#20250;&#31995;&#21015;&#30340;&#30446;&#26631;&#26159;&#20026;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#25552;&#20379;&#25506;&#32034;&#21644;&#35752;&#35770;&#24179;&#21488;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#20998;&#21106;&#12289;&#26816;&#27979;&#21644;&#26144;&#23556;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30740;&#35752;&#20250;&#19978;&#20030;&#21150;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#23637;&#31034;&#20102;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#20197;&#21450;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;&#12290;&#26356;&#22810;&#35814;&#24773;&#35831;&#21442;&#38405;https://opensun3d.github.io/index_iccv23.html&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15321v1 Announce Type: cross  Abstract: This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;MIONet&#23398;&#20064;&#23450;&#20041;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;PDE&#30340;&#35299;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#35299;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#21253;&#25324;&#21508;&#31181;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#22788;&#29702;&#24230;&#37327;&#31354;&#38388;&#30340;&#36924;&#36817;&#29702;&#35770;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.15097</link><description>&lt;p&gt;
&#36890;&#36807;MIONet&#23398;&#20064;&#23450;&#20041;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;PDE&#30340;&#35299;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning solution operators of PDEs defined on varying domains via MIONet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15097
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;MIONet&#23398;&#20064;&#23450;&#20041;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;PDE&#30340;&#35299;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#35299;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#21253;&#25324;&#21508;&#31181;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#22788;&#29702;&#24230;&#37327;&#31354;&#38388;&#30340;&#36924;&#36817;&#29702;&#35770;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;MIONet&#23398;&#20064;&#23450;&#20041;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;PDE&#30340;&#35299;&#31639;&#23376;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;MIONet&#30340;&#36924;&#36817;&#29702;&#35770;&#25193;&#23637;&#21040;&#36827;&#19968;&#27493;&#22788;&#29702;&#24230;&#37327;&#31354;&#38388;&#65292;&#24314;&#31435;MIONet&#21487;&#20197;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#36924;&#36817;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#26144;&#23556;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#20123;&#36866;&#24403;&#21306;&#22495;&#30340;&#38598;&#21512;&#65292;&#24182;&#20026;&#36825;&#20010;&#38598;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#24230;&#37327;&#65292;&#20174;&#32780;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#28385;&#36275;MIONet&#30340;&#36924;&#36817;&#26465;&#20214;&#12290;&#22522;&#20110;&#29702;&#35770;&#22522;&#30784;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;PDE&#30340;&#35299;&#26144;&#23556;&#65292;&#20854;&#20013;&#21253;&#25324;&#21508;&#31181;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#21253;&#25324;&#24494;&#20998;&#31639;&#23376;&#30340;&#21442;&#25968;&#65292;&#21491;&#25163;&#36793;&#39033;&#65292;&#36793;&#30028;&#26465;&#20214;&#20197;&#21450;&#22495;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;2D&#27850;&#26494;&#26041;&#31243;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20854;&#20013;&#22495;&#21644;&#21491;&#25163;&#36793;&#39033;&#26159;&#21464;&#21270;&#30340;&#12290;&#32467;&#26524;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15097v1 Announce Type: new  Abstract: In this work, we propose a method to learn the solution operators of PDEs defined on varying domains via MIONet, and theoretically justify this method. We first extend the approximation theory of MIONet to further deal with metric spaces, establishing that MIONet can approximate mappings with multiple inputs in metric spaces. Subsequently, we construct a set consisting of some appropriate regions and provide a metric on this set thus make it a metric space, which satisfies the approximation condition of MIONet. Building upon the theoretical foundation, we are able to learn the solution mapping of a PDE with all the parameters varying, including the parameters of the differential operator, the right-hand side term, the boundary condition, as well as the domain. Without loss of generality, we for example perform the experiments for 2-d Poisson equations, where the domains and the right-hand side terms are varying. The results provide insig
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.14899</link><description>&lt;p&gt;
&#20572;&#27490;&#25512;&#29702;&#65281;&#24403;&#22810;&#27169;&#24577;LLMs&#19982;&#20018;&#32852;&#25512;&#29702;&#36935;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#29702;&#35299;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20687;&#20256;&#32479;&#35270;&#35273;&#27169;&#22411;&#19968;&#26679;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;&#30340;&#25915;&#20987;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20018;&#32852;&#25512;&#29702;&#65288;CoT&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;MLLMs&#19978;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#25552;&#20379;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#20851;&#20110;MLLMs&#22312;CoT&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;MLLMs&#29992;&#23545;&#25239;&#24615;&#22270;&#20687;&#25512;&#26029;&#38169;&#35823;&#31572;&#26696;&#26102;&#25512;&#29702;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#37319;&#29992;CoT&#25512;&#29702;&#26102;MLLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;CoT&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25269;&#25239;&#20102;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;CoT&#24341;&#36215;&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoT&#25512;&#29702;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14169</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#20559;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
A Temporal Bias Correction using a Machine Learning Attention model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#27169;&#22411;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#35266;&#27979;&#25968;&#25454;&#30456;&#27604;&#23384;&#22312;&#20559;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#24433;&#21709;&#30740;&#31350;&#20043;&#21069;&#36827;&#34892;&#26657;&#20934;&#12290;&#20351;&#26657;&#20934;&#25104;&#20026;&#21487;&#33021;&#30340;&#32479;&#35745;&#26041;&#27861;&#38598;&#21512;&#34987;&#31216;&#20026;&#20559;&#24046;&#26657;&#27491;&#65288;BC&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;BC&#26041;&#27861;&#22312;&#35843;&#25972;&#26102;&#38388;&#20559;&#24046;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65288;&#22914;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#21644;&#39057;&#29575;&#65289;&#26080;&#27861;&#20934;&#30830;&#26657;&#27491;&#65292;&#36825;&#20351;&#24471;&#22312;&#36825;&#20123;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BC&#26041;&#27861;&#26469;&#26657;&#27491;&#26102;&#38388;&#20559;&#24046;&#12290;&#36825;&#24471;&#30410;&#20110;&#23558;BC&#37325;&#26032;&#26500;&#24819;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#24182;&#23558;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27010;&#29575;&#20851;&#27880;&#27169;&#22411;&#35843;&#25972;&#21040;BC&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#23612;&#26085;&#21033;&#20122;&#38463;&#24067;&#36158;&#30340;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#26696;&#20363;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14169v1 Announce Type: new  Abstract: Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies. The suite of statistical methods that enable such calibrations is called bias correction (BC). However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points. As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics. In this paper, we offer a novel BC methodology to correct for temporal biases. This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task. With a case study of heatwave duration statistics in Abuja, Nigeria, and
&lt;/p&gt;</description></item><item><title>GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.12566</link><description>&lt;p&gt;
GenAudit&#65306;&#21033;&#29992;&#35777;&#25454;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12566
&lt;/p&gt;
&lt;p&gt;
GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#21442;&#32771;&#25991;&#26723;&#65292;&#20063;&#21487;&#33021;&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#38472;&#36848;&#12290;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#22522;&#20110;&#25991;&#26723;&#30340;&#21307;&#30103;&#20445;&#20581;&#25110;&#37329;&#34701;&#38382;&#31572;&#65289;&#65292;&#36825;&#26679;&#30340;&#38169;&#35823;&#21487;&#33021;&#20855;&#26377;&#21361;&#38505;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenAudit -- &#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#26816;&#26597;&#22522;&#20110;&#25991;&#26723;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#30340;&#24037;&#20855;&#12290;GenAudit&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#26723;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#21516;&#26102;&#20026;&#30475;&#20284;&#34987;&#35777;&#25454;&#25903;&#25345;&#30340;&#20107;&#23454;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#26469;&#24314;&#35758;&#20462;&#25913;LLM&#21709;&#24212;&#12290;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#24314;&#35758;&#30340;&#20462;&#25913;&#21644;&#35777;&#25454;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20998;&#21592;&#30340;&#20840;&#38754;&#35780;&#20272;&#26174;&#31034;&#65292;GenAudit&#22312;&#24635;&#32467;&#19981;&#21516;&#39046;&#22495;&#25991;&#26723;&#26102;&#33021;&#22815;&#26816;&#27979;&#20986;8&#31181;&#19981;&#21516;&#30340;LLM&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#20026;&#30830;&#20445;&#31995;&#32479;&#33021;&#22815;&#26631;&#35760;&#22823;&#22810;&#25968;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#38169;&#35823;&#21484;&#22238;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#39044;&#22788;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.11887</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Semi-supervised Graph Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#32771;&#34385;&#20102;&#19968;&#20010;&#23454;&#38469;&#24773;&#22659;&#19979;&#30340;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#65292;&#22312;&#36825;&#20010;&#24773;&#22659;&#20013;&#65292;&#22270;&#20013;&#30340;&#37096;&#20998;&#33410;&#28857;&#34987;&#30693;&#26195;&#26159;&#27491;&#24120;&#30340;&#65292;&#19982;&#22823;&#22810;&#25968;GAD&#30740;&#31350;&#20013;&#20351;&#29992;&#23436;&#20840;&#26410;&#26631;&#35760;&#22270;&#30340;&#26080;&#30417;&#30563;&#24773;&#20917;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#26377;&#21161;&#20110;&#25552;&#21319;&#29616;&#26377;&#26080;&#30417;&#30563;GAD&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#24773;&#22659;&#19979;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#30340;&#21033;&#29992;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#21322;&#30417;&#30563;&#24773;&#22659;&#30340;&#29983;&#25104;&#24335;GAD&#26041;&#27861;&#65288;GGAD&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#23427;&#20204;&#34701;&#21512;&#20102;&#26412;&#22320;&#32467;&#26500;&#21644;&#33410;&#28857;&#34920;&#31034;&#65292;&#20026;&#35757;&#32451;&#21028;&#21035;&#22411;&#21333;&#31867;&#20998;&#31867;&#22120;&#25552;&#20379;&#26377;&#25928;&#30340;&#36127;&#38754;&#33410;&#28857;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11887v1 Announce Type: new  Abstract: This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#20027;&#35201;&#29992;&#20110;&#23637;&#31034;&#27010;&#24565;&#25110;&#25552;&#20379;&#31034;&#20363;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#25165;&#33021;&#23454;&#29616;&#29983;&#20135;&#23601;&#32490;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.11702</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#25903;&#25345;&#24320;&#21457;&#32773;&#65311;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11702
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#20027;&#35201;&#29992;&#20110;&#23637;&#31034;&#27010;&#24565;&#25110;&#25552;&#20379;&#31034;&#20363;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#25165;&#33021;&#23454;&#29616;&#29983;&#20135;&#23601;&#32490;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#23427;&#20204;&#22312;&#21508;&#31181;&#24320;&#21457;&#22330;&#26223;&#20013;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#25552;&#20379;&#20102;&#22312;&#30740;&#31350;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#65292;&#36825;&#22312;&#29702;&#35299;LLM&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#33021;&#26377;&#25928;&#25903;&#25345;&#24320;&#21457;&#32773;&#26041;&#38754;&#30041;&#19979;&#20102;&#26174;&#33879;&#30340;&#31354;&#30333;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;DevGPT&#20013;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36825;&#26159;&#20174;&#24320;&#21457;&#32773;&#19982;ChatGPT&#30340;&#23545;&#35805;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65288;&#36890;&#36807;GitHub&#31561;&#24179;&#21488;&#19978;&#30340;Share Link&#21151;&#33021;&#25429;&#33719;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23454;&#36341;&#36890;&#24120;&#20165;&#38480;&#20110;&#23637;&#31034;&#39640;&#23618;&#27010;&#24565;&#25110;&#25552;&#20379;&#25991;&#26723;&#20013;&#30340;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#21487;&#29992;&#20110;&#29983;&#20135;&#30340;&#20195;&#30721;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;LLM&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#36824;&#38656;&#35201;&#22823;&#37327;&#26410;&#26469;&#24037;&#20316;&#25165;&#33021;&#20351;&#20854;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11702v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts o
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;GraphPrompter&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10359</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#29992;&#36719;&#25552;&#31034;LLMs&#26469;&#36827;&#34892;&#22270;&#23398;&#20064;&#20219;&#21153;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we soft prompt LLMs for graph learning tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10359
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;GraphPrompter&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#34920;&#31034;&#31038;&#20132;&#32593;&#32476;&#12289;&#29983;&#29289;&#25968;&#25454;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36825;&#20351;&#24471;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#34920;&#26684;&#23588;&#20026;&#35825;&#20154;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#34920;&#26684;&#24418;&#24335;&#19982;&#25991;&#26412;&#24418;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#21644;&#19981;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GraphPrompter&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#36719;&#25552;&#31034;&#26469;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GraphPrompter&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32534;&#30721;&#22797;&#26434;&#30340;&#22270;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#30340;LLM&#12290;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10359v1 Announce Type: cross  Abstract: Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tas
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09900</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Recurrent Reinforcement Learning with Memory Monoids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20687;RNN&#21644;transformers&#36825;&#26679;&#30340;&#35760;&#24518;&#27169;&#22411;&#36890;&#36807;&#23558;&#36712;&#36857;&#26144;&#23556;&#21040;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#26469;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38271;&#24207;&#21015;&#30340;&#35268;&#27169;&#21270;&#22788;&#29702;&#33021;&#21147;&#24182;&#19981;&#29305;&#21035;&#22909;&#65292;&#23588;&#20854;&#26159;&#19982;&#19968;&#31867;&#26032;&#20852;&#30340;&#35760;&#24518;&#27169;&#22411;&#65288;&#26377;&#26102;&#31216;&#20026;&#32447;&#24615;&#24490;&#29615;&#27169;&#22411;&#65289;&#30456;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#24490;&#29615;&#26356;&#26032;&#26159;&#19968;&#20010;&#21333;&#23376;&#65292;&#22240;&#27492;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20256;&#32479;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#21033;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#22686;&#21152;&#20102;&#22238;&#25253;&#65292;&#24182;&#31616;&#21270;&#20102;&#24490;&#29615;&#20002;&#22833;&#20989;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09900v1 Announce Type: cross  Abstract: In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.
&lt;/p&gt;</description></item><item><title>EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.09288</link><description>&lt;p&gt;
EcoVal:&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EcoVal: An Efficient Data Valuation Framework for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09288
&lt;/p&gt;
&lt;p&gt;
EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20013;&#37327;&#21270;&#25968;&#25454;&#30340;&#20215;&#20540;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20513;&#35758;&#20013;&#20570;&#20986;&#26356;&#20855;&#25112;&#30053;&#24847;&#20041;&#30340;&#20915;&#31574;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;Shapley&#20540;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#22312;&#35745;&#31639;&#26041;&#38754;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#37325;&#22797;&#35757;&#32451;&#27169;&#22411;&#25165;&#33021;&#33719;&#24471;Shapley&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;EcoVal&#65292;&#20197;&#24555;&#36895;&#23454;&#29992;&#30340;&#26041;&#24335;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#22788;&#29702;&#29420;&#31435;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;&#26159;&#30830;&#23450;&#31867;&#20284;&#30340;&#25968;&#25454;&#28857;&#31751;&#30340;&#20215;&#20540;&#12290;&#36825;&#20010;&#20215;&#20540;&#36827;&#19968;&#27493;&#22312;&#25152;&#26377;&#25104;&#21592;&#31751;&#28857;&#20043;&#38388;&#20256;&#25773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#26469;&#30830;&#23450;&#25972;&#20307;&#25968;&#25454;&#20215;&#20540;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#24314;&#27169;&#20026;&#8220;&#29983;&#20135;&#20989;&#25968;&#8221;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09288v1 Announce Type: new Abstract: Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#38388;&#21183;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#21407;&#23376;&#23494;&#24230;&#23637;&#24320;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20302;&#32500;&#23884;&#20837;&#21644;&#21407;&#23376;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07472</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#22312;&#29289;&#36136;&#31185;&#23398;&#21644;&#21270;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cartesian atomic cluster expansion for machine learning interatomic potentials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#38388;&#21183;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#21407;&#23376;&#23494;&#24230;&#23637;&#24320;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20302;&#32500;&#23884;&#20837;&#21644;&#21407;&#23376;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#38388;&#21183;&#27491;&#22312;&#38761;&#26032;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#20934;&#30830;&#30340;&#21407;&#23376;&#27169;&#25311;&#12290;&#36825;&#20123;&#21183;&#20989;&#25968;&#36890;&#24120;&#20351;&#29992;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#25110;&#21464;&#25442;&#28040;&#24687;&#20256;&#36882;&#19982;&#29699;&#35856;&#22522;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25345;&#26059;&#36716;&#23545;&#31216;&#24615;&#65292;&#20381;&#36182;Clebsch-Gordan&#31995;&#25968;&#20250;&#23548;&#33268;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#21644;&#20887;&#20313;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65306;&#22522;&#20110;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#21407;&#23376;&#23494;&#24230;&#23637;&#24320;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#30456;&#20114;&#20316;&#29992;&#20307;&#31995;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21407;&#23376;&#29615;&#22659;&#30340;&#23436;&#25972;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#21508;&#31181;&#21270;&#23398;&#20803;&#32032;&#30340;&#20302;&#32500;&#23884;&#20837;&#21644;&#21407;&#23376;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;&#25152;&#24471;&#21040;&#30340;&#21183;&#20989;&#25968;&#34987;&#21629;&#21517;&#20026;Cartesian Atomic Cluster Expansion(CACE)&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#31995;&#32479;&#20013;&#36827;&#34892;&#39564;&#35777;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#27700;&#12289;&#23567;&#20998;&#23376;&#21644;25&#31181;&#20803;&#32032;&#39640;&#29109;&#21512;&#37329;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning interatomic potentials are revolutionizing large-scale, accurate atomistic modelling in material science and chemistry. These potentials often use atomic cluster expansion or equivariant message passing with spherical harmonics as basis functions. However, the dependence on Clebsch-Gordan coefficients for maintaining rotational symmetry leads to computational inefficiencies and redundancies. We propose an alternative: a Cartesian-coordinates-based atomic density expansion. This approach provides a complete description of atomic environments while maintaining interaction body orders. Additionally, we integrate low-dimensional embeddings of various chemical elements and inter-atomic message passing. The resulting potential, named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy, stability, and generalizability. We validate its performance in diverse systems, including bulk water, small molecules, and 25-element high-entropy alloys.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#23450;&#29702;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07356</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Novel Gaussian Min-Max Theorem and its Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#23450;&#29702;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gordon&#30340;&#19968;&#20010;&#33879;&#21517;&#32467;&#26524;&#20801;&#35768;&#27604;&#36739;&#20004;&#20010;&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#23567;&#26368;&#22823;&#34892;&#20026;&#65292;&#22914;&#26524;&#28385;&#36275;&#26576;&#20123;&#19981;&#31561;&#24335;&#26465;&#20214;&#12290;&#36825;&#20010;&#32467;&#26524;&#30340;&#32467;&#26524;&#21253;&#25324;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#65288;GMT&#65289;&#21644;&#20984;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#65288;CGMT&#65289;&#23450;&#29702;&#65292;&#36825;&#20123;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#26041;&#38754;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#21457;&#29616;&#28385;&#36275;&#36825;&#20123;&#19981;&#31561;&#24335;&#30340;&#20854;&#20182;&#19968;&#23545;&#39640;&#26031;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#26679;&#19968;&#23545;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#23450;&#29702;&#23558;&#32463;&#20856;&#30340;GMT&#23450;&#29702;&#21644;CGMT&#23450;&#29702;&#20174;&#22522;&#26412;&#36807;&#31243;&#20013;&#30340;&#24213;&#23618;&#39640;&#26031;&#30697;&#38453;&#20855;&#26377;iid&#34892;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#20855;&#26377;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#26032;&#30340;CGMT&#23450;&#29702;&#24212;&#29992;&#20110;&#22810;&#28304;&#39640;&#26031;&#22238;&#24402;&#38382;&#39064;&#65292;&#20197;&#21450;&#23646;&#20110;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A celebrated result by Gordon allows one to compare the min-max behavior of two Gaussian processes if certain inequality conditions are met. The consequences of this result include the Gaussian min-max (GMT) and convex Gaussian min-max (CGMT) theorems which have had far-reaching implications in high-dimensional statistics, machine learning, non-smooth optimization, and signal processing. Both theorems rely on a pair of Gaussian processes, first identified by Slepian, that satisfy Gordon's comparison inequalities. To date, no other pair of Gaussian processes satisfying these inequalities has been discovered. In this paper, we identify such a new pair. The resulting theorems extend the classical GMT and CGMT Theorems from the case where the underlying Gaussian matrix in the primary process has iid rows to where it has independent but non-identically-distributed ones. The new CGMT is applied to the problems of multi-source Gaussian regression, as well as to binary classification of genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.05808</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;R$^3$&#65306;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#25512;&#29702;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#20351;&#29992;&#32467;&#26524;&#30417;&#30563;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#23558;RL&#24212;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#30830;&#23450;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#20197;&#33719;&#24471;&#27491;&#21521;&#22870;&#21169;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#20248;&#21270;&#30417;&#30563;&#12290;&#32467;&#26524;&#30417;&#30563;&#20026;&#26368;&#32456;&#32467;&#26524;&#25552;&#20379;&#20102;&#31232;&#30095;&#22870;&#21169;&#65292;&#32780;&#19981;&#35782;&#21035;&#38169;&#35823;&#20301;&#32622;&#65292;&#32780;&#36807;&#31243;&#30417;&#30563;&#25552;&#20379;&#20102;&#36880;&#27493;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#12290;R$^3$&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;R$^3$&#23558;&#25512;&#29702;&#30340;&#36215;&#22987;&#29366;&#24577;&#20174;&#28436;&#31034;&#30340;&#32467;&#26463;&#28369;&#21160;&#21040;&#24320;&#22987;&#65292;&#20174;&#32780;&#22312;&#25152;&#26377;&#38454;&#27573;&#37117;&#20419;&#36827;&#20102;&#26356;&#23481;&#26131;&#30340;&#27169;&#22411;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;R$^3$&#24314;&#31435;&#20102;&#19968;&#20010;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#20351;&#32467;&#26524;&#30417;&#30563;&#33021;&#22815;&#25552;&#20379;&#38454;&#27573;&#32423;&#20449;&#21495;&#24182;&#31934;&#30830;&#23450;&#20301;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05713</link><description>&lt;p&gt;
&#26126;&#26126;&#23601;&#22312;&#30524;&#21069;&#65306;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#36827;&#34892;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#21095;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#30340;&#20020;&#24202;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#20559;&#35265;&#30340;&#37327;&#21270;&#65292;&#20294;&#38024;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#20197;&#21450;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38024;&#23545;&#20154;&#21475;&#32479;&#35745;&#23398;&#26631;&#31614;&#30340;&#27602;&#21270;&#25915;&#20987;&#21487;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#23545;&#34987;&#20302;&#20272;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#21644;&#20154;&#21475;&#32676;&#20307;&#65288;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#20197;&#21450;&#20854;&#20132;&#21449;&#23376;&#32676;&#65289;&#19978;&#34920;&#26126;&#65292;&#32676;&#20307;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#19982;&#20854;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#24449;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#22810;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#21333;&#27425;GD&#30456;&#27604;&#65292;&#22810;&#27425;GD&#33021;&#22815;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#20165;&#38656;&#20004;&#20010;&#26102;&#38388;&#27493;&#39588;&#23601;&#33021;&#23454;&#29616;&#32593;&#32476;&#19982;&#30446;&#26631;&#23376;&#31354;&#38388;&#30340;&#37325;&#21472;&#65292;&#23637;&#31034;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26377;&#25928;&#23398;&#20064;&#30340;&#24191;&#27867;&#20989;&#25968;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#22522;&#20110;&#21160;&#21147;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.03220</link><description>&lt;p&gt;
&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#22312;&#20004;&#23618;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#22909;&#22788;&#65306;&#25171;&#30772;&#20449;&#24687;&#21644;&#36339;&#36291;&#25351;&#25968;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#22810;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#21333;&#27425;GD&#30456;&#27604;&#65292;&#22810;&#27425;GD&#33021;&#22815;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#20165;&#38656;&#20004;&#20010;&#26102;&#38388;&#27493;&#39588;&#23601;&#33021;&#23454;&#29616;&#32593;&#32476;&#19982;&#30446;&#26631;&#23376;&#31354;&#38388;&#30340;&#37325;&#21472;&#65292;&#23637;&#31034;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26377;&#25928;&#23398;&#20064;&#30340;&#24191;&#27867;&#20989;&#25968;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#22522;&#20110;&#21160;&#21147;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#22810;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#20851;&#27880;&#37325;&#22797;&#22810;&#27425;&#20351;&#29992;&#25209;&#27425;&#30340;&#22810;&#27425;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#65292;&#24182;&#23637;&#31034;&#23427;&#19982;&#21333;&#27425;&#26799;&#24230;&#19979;&#38477;&#30456;&#27604;&#65292;&#26174;&#33879;&#25913;&#21464;&#20102;&#23545;&#20110;&#21738;&#20123;&#20989;&#25968;&#26159;&#21487;&#23398;&#20064;&#30340;&#30340;&#32467;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#26377;&#38480;&#27493;&#38271;&#30340;&#22810;&#27425;GD&#33021;&#22815;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#30340;&#20449;&#24687;&#25351;&#25968;&#65288;Ben Arous&#31561;&#20154;&#65292;2021&#65289;&#21644;&#36339;&#36291;&#25351;&#25968;&#65288;Abbe&#31561;&#20154;&#65292;2023&#65289;&#25152;&#32473;&#20986;&#30340;&#26799;&#24230;&#27969;&#21644;&#21333;&#27425;GD&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#65292;&#32593;&#32476;&#20165;&#38656;&#20004;&#20010;&#26102;&#38388;&#27493;&#39588;&#23601;&#33021;&#19982;&#30446;&#26631;&#23376;&#31354;&#38388;&#36798;&#25104;&#37325;&#21472;&#65292;&#21363;&#20351;&#20989;&#25968;&#19981;&#28385;&#36275;&#38454;&#26799;&#24615;&#36136;&#65288;Abbe&#31561;&#20154;&#65292;2021&#65289;&#12290;&#25105;&#20204;&#23545;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26377;&#25928;&#23398;&#20064;&#30340;&#65288;&#24191;&#27867;&#30340;&#65289;&#20989;&#25968;&#31867;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#22522;&#20110;&#21160;&#21147;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#21160;&#24577;&#30340;&#38381;&#24335;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamica
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#25216;&#26415;&#22312;&#38750;&#20984;&#21644;&#20984;&#35774;&#32622;&#19979;&#37117;&#33021;&#21462;&#24471;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25351;&#20986;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03126</link><description>&lt;p&gt;
&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#33258;&#30001;&#24230;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Free is Parameter-Free Stochastic Optimization?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03126
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#25216;&#26415;&#22312;&#38750;&#20984;&#21644;&#20984;&#35774;&#32622;&#19979;&#37117;&#33021;&#21462;&#24471;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25351;&#20986;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#21487;&#20197;&#23384;&#22312;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65306;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20248;&#35843;&#21442;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#30495;&#23454;&#38382;&#39064;&#21442;&#25968;&#26377;&#24456;&#22810;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#26080;&#21442;&#26041;&#27861;&#21482;&#33021;&#34987;&#35270;&#20026;&#8220;&#37096;&#20998;&#8221;&#26080;&#21442;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23545;&#30495;&#23454;&#38382;&#39064;&#21442;&#25968;&#26377;&#19968;&#20123;&#38750;&#24179;&#20961;&#30340;&#30693;&#35782;&#65292;&#27604;&#22914;&#38543;&#26426;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#12289;&#21040;&#26368;&#23567;&#20540;&#30340;&#36317;&#31163;&#30340;&#19978;&#30028;&#31561;&#12290;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#26356;&#22797;&#26434;&#30340;&#20808;&#36827;&#31639;&#27861;&#12290;&#22312;&#20855;&#26377;&#22122;&#22768;&#20989;&#25968;&#20540;&#30340;&#20984;&#35774;&#32622;&#19979;&#65292;&#22312;&#36739;&#23567;&#30340;&#22122;&#22768;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20063;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#38543;&#26426;&#26799;&#24230;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#20351;&#24471;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#26080;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free s
&lt;/p&gt;</description></item><item><title>LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02544</link><description>&lt;p&gt;
LHRS-Bot&#65306;&#21033;&#29992;VGI&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36965;&#24863;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02544
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38761;&#21629;&#24615;&#33021;&#21147;&#24320;&#21019;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24182;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#39046;&#22495;&#20013;&#65292;&#36817;&#26399;&#30340;MLLM&#21162;&#21147;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#36965;&#24863;&#22270;&#20687;&#20013;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LHRS-Align&#65292;&#20197;&#21450;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;RS&#29305;&#23450;&#25351;&#23548;&#25968;&#25454;&#38598;LHRS-Instruct&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#21644;&#20840;&#29699;&#21487;&#29992;&#30340;RS&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LHRS-Bot&#65292;&#19968;&#31181;&#38024;&#23545;RS&#22270;&#20687;&#29702;&#35299;&#30340;MLLM&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21327;&#21516;&#32534;&#25490;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#25506;&#32034;&#32452;&#21512;&#24211;&#20013;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38477;&#32500;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#25511;&#21046;&#22797;&#26434;&#21487;&#35266;&#27979;&#37327;&#30340;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.02198</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21327;&#21516;&#32534;&#25490;&#36890;&#36807;&#22810;&#20219;&#21153;&#36125;&#21494;&#26031;&#20248;&#21270;&#25506;&#32034;&#32452;&#21512;&#24211;&#20013;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Multimodal Co-orchestration for Exploring Structure-Property Relationships in Combinatorial Libraries via Multi-Task Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21327;&#21516;&#32534;&#25490;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#25506;&#32034;&#32452;&#21512;&#24211;&#20013;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38477;&#32500;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#25511;&#21046;&#22797;&#26434;&#21487;&#35266;&#27979;&#37327;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#21644;&#33258;&#20027;&#20202;&#22120;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#22810;&#27169;&#24577;&#24037;&#20855;&#30340;&#21327;&#21516;&#32534;&#25490;&#26426;&#20250;&#65292;&#36825;&#20123;&#24037;&#20855;&#37197;&#22791;&#20102;&#22810;&#31181;&#39034;&#24207;&#26816;&#27979;&#26041;&#27861;&#65292;&#25110;&#32773;&#22810;&#20010;&#34920;&#24449;&#24037;&#20855;&#26469;&#21516;&#26102;&#25506;&#32034;&#30456;&#21516;&#26679;&#21697;&#12290;&#36825;&#21487;&#20197;&#20197;&#32452;&#21512;&#24211;&#20026;&#20363;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#24037;&#20855;&#21516;&#26102;&#22312;&#22810;&#20010;&#20301;&#32622;&#25506;&#32034;&#65292;&#25110;&#32773;&#22312;&#33258;&#21160;&#21512;&#25104;&#31995;&#32479;&#20013;&#36827;&#34892;&#19979;&#28216;&#34920;&#24449;&#12290;&#22312;&#21327;&#21516;&#32534;&#25490;&#26041;&#27861;&#20013;&#65292;&#20174;&#19968;&#31181;&#27169;&#24577;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#24212;&#35813;&#21152;&#36895;&#20854;&#20182;&#27169;&#24577;&#30340;&#21457;&#29616;&#12290;&#30456;&#24212;&#22320;&#65292;&#21327;&#21516;&#32534;&#25490;&#20195;&#29702;&#24212;&#35813;&#26681;&#25454;&#39044;&#26399;&#30340;&#30693;&#35782;&#22686;&#30410;&#21644;&#27979;&#37327;&#25104;&#26412;&#26469;&#36873;&#25321;&#27979;&#37327;&#27169;&#24577;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#26045;&#20102;&#19968;&#31181;&#29992;&#20110;&#36827;&#34892;&#20855;&#26377;&#22797;&#26434;&#21487;&#35266;&#27979;&#37327;&#65288;&#22914;&#20809;&#35889;&#25110;&#22270;&#20687;&#65289;&#30340;&#27979;&#37327;&#30340;&#21327;&#21516;&#32534;&#25490;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38477;&#32500;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#25511;&#21046;&#28508;&#22312;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of automated and autonomous instrumentations brings forth an opportunity for the co-orchestration of multimodal tools, equipped with multiple sequential detection methods, or several characterization tools to explore identical samples. This can be exemplified by the combinatorial libraries that can be explored in multiple locations by multiple tools simultaneously, or downstream characterization in automated synthesis systems. In the co-orchestration approaches, information gained in one modality should accelerate the discovery of other modalities. Correspondingly, the orchestrating agent should select the measurement modality based on the anticipated knowledge gain and measurement cost. Here, we propose and implement a co-orchestration approach for conducting measurements with complex observables such as spectra or images. The method relies on combining dimensionality reduction by variational autoencoders with representation learning for control over the latent space 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20351;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#25670;&#33073;&#37325;&#22797;&#29305;&#24449;&#20540;&#36755;&#20837;&#30340;&#38480;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#29305;&#24449;&#20540;&#30340;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2401.15603</link><description>&lt;p&gt;
&#29992;&#29305;&#24449;&#20540;&#20462;&#27491;&#25552;&#21319;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20351;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#25670;&#33073;&#37325;&#22797;&#29305;&#24449;&#20540;&#36755;&#20837;&#30340;&#38480;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#29305;&#24449;&#20540;&#30340;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#29305;&#24449;&#20026;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#24444;&#27492;&#19981;&#21516;&#65292;&#22240;&#27492;&#26399;&#26395;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20855;&#26377;&#24456;&#39640;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#22312;&#23454;&#35777;&#19978;&#35266;&#23519;&#21040;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#32463;&#24120;&#20855;&#26377;&#37325;&#22797;&#30340;&#29305;&#24449;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#21487;&#36776;&#35748;&#29305;&#24449;&#20540;&#30340;&#25968;&#37327;&#22312;&#30830;&#23450;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#37492;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#25670;&#33073;&#37325;&#22797;&#29305;&#24449;&#20540;&#36755;&#20837;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#22686;&#24378;&#20102;&#29305;&#24449;&#20540;&#30340;&#22343;&#21248;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15603v2 Announce Type: replace  Abstract: In recent years, spectral graph neural networks, characterized by polynomial filters, have garnered increasing attention and have achieved remarkable performance in tasks such as node classification. These models typically assume that eigenvalues for the normalized Laplacian matrix are distinct from each other, thus expecting a polynomial filter to have a high fitting ability. However, this paper empirically observes that normalized Laplacian matrices frequently possess repeated eigenvalues. Moreover, we theoretically establish that the number of distinguishable eigenvalues plays a pivotal role in determining the expressive power of spectral graph neural networks. In light of this observation, we propose an eigenvalue correction strategy that can free polynomial filters from the constraints of repeated eigenvalue inputs. Concretely, the proposed eigenvalue correction strategy enhances the uniform distribution of eigenvalues, thus mit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21452;SE(d)xSE(d)&#23545;&#31216;&#24615;&#30340;Fourier Transporter&#65288;FourTran&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;3D&#29615;&#22659;&#20013;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;RLbench&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.12046</link><description>&lt;p&gt;
Fourier Transporter&#65306;&#19977;&#32500;&#21452;&#31561;&#21464;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12046
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21452;SE(d)xSE(d)&#23545;&#31216;&#24615;&#30340;Fourier Transporter&#65288;FourTran&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;3D&#29615;&#22659;&#20013;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;RLbench&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#30340;&#25342;&#21462;&#21644;&#25918;&#32622;&#21160;&#20316;&#12290;&#22312;3D&#29615;&#22659;&#20013;&#65292;&#35757;&#32451;&#26426;&#22120;&#20154;&#20195;&#29702;&#23398;&#20064;&#36825;&#20010;&#24207;&#21015;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#36845;&#20195;&#25110;&#28436;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20613;&#31435;&#21494;&#20256;&#36755;&#22120;&#65288;Fourier Transporter&#65292;FourTran&#65289;&#65292;&#21033;&#29992;&#25342;&#25918;&#38382;&#39064;&#20013;&#30340;&#21452;SE(d)xSE(d)&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;FourTran&#26159;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#30340;&#24320;&#29615;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26032;&#29615;&#22659;&#19979;&#30340;&#25342;&#25918;&#21160;&#20316;&#12290;FourTran&#21463;&#38480;&#20110;&#29420;&#31435;&#22320;&#21512;&#24182;&#25342;&#25918;&#21160;&#20316;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#31181;&#32420;&#32500;&#31354;&#38388;&#30340;&#20613;&#31435;&#21494;&#21464;&#25442;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26500;&#36896;&#12290;&#25105;&#20204;&#22312;RLbench&#22522;&#20934;&#27979;&#35797;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12046v2 Announce Type: replace-cross  Abstract: Many complex robotic manipulation tasks can be decomposed as a sequence of pick and place actions. Training a robotic agent to learn this sequence over many different starting conditions typically requires many iterations or demonstrations, especially in 3D environments. In this work, we propose Fourier Transporter (FourTran) which leverages the two-fold SE(d)xSE(d) symmetry in the pick-place problem to achieve much higher sample efficiency. FourTran is an open-loop behavior cloning method trained using expert demonstrations to predict pick-place actions on new environments. FourTran is constrained to incorporate symmetries of the pick and place actions independently. Our method utilizes a fiber space Fourier transformation that allows for memory-efficient construction. We test our proposed network on the RLbench benchmark and achieve state-of-the-art results across various tasks.
&lt;/p&gt;</description></item><item><title>&#22635;&#34917;&#20102;&#29702;&#35770;&#31354;&#30333;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#38750;&#28176;&#36817;&#25910;&#25947;&#30028;&#65292;&#35777;&#26126;&#20102;&#22312;&#21508;&#21521;&#24322;&#24615;&#26799;&#24230;&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#38543;&#26426;&#37325;&#29699;&#26041;&#27861;&#22312;&#20108;&#27425;&#30446;&#26631;&#19978;&#21487;&#20197;&#25552;&#20379;$\tilde{\mathcal{O}}(\sqrt{\kappa})$&#30340;&#21152;&#36895;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2312.14567</link><description>&lt;p&gt;
&#38750;&#21508;&#21521;&#21516;&#24615;&#26799;&#24230;&#22122;&#22768;&#19979;&#38543;&#26426;&#37325;&#29699;&#26041;&#27861;&#30340;&#21152;&#36895;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14567
&lt;/p&gt;
&lt;p&gt;
&#22635;&#34917;&#20102;&#29702;&#35770;&#31354;&#30333;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#38750;&#28176;&#36817;&#25910;&#25947;&#30028;&#65292;&#35777;&#26126;&#20102;&#22312;&#21508;&#21521;&#24322;&#24615;&#26799;&#24230;&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#38543;&#26426;&#37325;&#29699;&#26041;&#27861;&#22312;&#20108;&#27425;&#30446;&#26631;&#19978;&#21487;&#20197;&#25552;&#20379;$\tilde{\mathcal{O}}(\sqrt{\kappa})$&#30340;&#21152;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24102;&#26377;&#36882;&#20943;&#23398;&#20064;&#29575;&#30340;&#37325;&#29699;&#21160;&#37327;&#32463;&#24120;&#19982;SGD&#19968;&#36215;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#27969;&#34892;&#30456;&#27604;&#65292;&#23545;&#20854;&#29702;&#35770;&#24615;&#36136;&#30340;&#20102;&#35299;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#20934;&#30340;&#20108;&#27425;&#22238;&#24402;&#38382;&#39064;&#30340;&#21508;&#21521;&#24322;&#24615;&#26799;&#24230;&#22122;&#22768;&#26465;&#20214;&#19979;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#29702;&#35770;&#31354;&#30333;&#65292;&#36890;&#36807;&#22312;&#20108;&#27425;&#30446;&#26631;&#19979;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#20855;&#26377;&#27493;&#38271;&#34928;&#20943;&#35843;&#24230;&#22120;&#30340;&#38543;&#26426;&#37325;&#29699;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#30028;&#65292;&#32780;&#27492;&#26465;&#20214;&#19979;&#30340;&#26799;&#24230;&#22122;&#22768;&#26159;&#21508;&#21521;&#24322;&#24615;&#30340;&#12290;&#30452;&#25509;&#24433;&#21709;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37325;&#29699;&#21160;&#37327;&#21487;&#20197;&#25552;&#20379;$\tilde{\mathcal{O}}(\sqrt{\kappa})$&#30340;SGD&#20559;&#24046;&#39033;&#21152;&#36895;&#25910;&#25947;&#65292;&#21516;&#26102;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14567v2 Announce Type: replace  Abstract: Heavy-ball momentum with decaying learning rates is widely used with SGD for optimizing deep learning models. In contrast to its empirical popularity, the understanding of its theoretical property is still quite limited, especially under the standard anisotropic gradient noise condition for quadratic regression problems. Although it is widely conjectured that heavy-ball momentum method can provide accelerated convergence and should work well in large batch settings, there is no rigorous theoretical analysis. In this paper, we fill this theoretical gap by establishing a non-asymptotic convergence bound for stochastic heavy-ball methods with step decay scheduler on quadratic objectives, under the anisotropic gradient noise condition. As a direct implication, we show that heavy-ball momentum can provide $\tilde{\mathcal{O}}(\sqrt{\kappa})$ accelerated convergence of the bias term of SGD while still achieving near-optimal convergence rat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;&#65292;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.14134</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Diffusion Reward: Learning Rewards via Conditional Video Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14134
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;&#65292;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19987;&#23478;&#35270;&#39057;&#20013;&#23398;&#20064;&#22870;&#21169;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion Reward&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20174;&#19987;&#23478;&#35270;&#39057;&#20013;&#23398;&#20064;&#22870;&#21169;&#20197;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#19987;&#23478;&#36712;&#36857;&#30340;&#26465;&#20214;&#19979;&#35266;&#23519;&#21040;&#36739;&#20302;&#30340;&#29983;&#25104;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;Diffusion Reward&#34987;&#24418;&#24335;&#21270;&#20026;&#36127;&#30340;&#26465;&#20214;&#29109;&#65292;&#40723;&#21169;&#19987;&#23478;&#24335;&#34892;&#20026;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MetaWorld&#21644;Adroit&#30340;10&#20010;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#20197;&#35270;&#35273;&#36755;&#20837;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;Diffusion Reward&#29978;&#33267;&#33021;&#22815;&#25104;&#21151;&#26377;&#25928;&#22320;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22823;&#22823;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;&#39033;&#30446;&#39029;&#38754;&#21644;&#20195;&#30721;&#65306;https://diffusion-reward.github.io/&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14134v2 Announce Type: replace  Abstract: Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is observed when conditioned on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert-like behaviors. We show the efficacy of our method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual input and sparse reward. Moreover, Diffusion Reward could even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io/.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21452;&#33218;&#39640;&#26031;&#36172;&#33218;&#26426;&#22120;&#20154;&#20013;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#26041;&#24046;&#24773;&#20917;&#19979;&#23616;&#37096;&#26368;&#20248;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.12741</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#26410;&#30693;&#26041;&#24046;&#30340;&#21452;&#33218;&#39640;&#26031;&#36172;&#33218;&#26426;&#22120;&#20154;&#20013;&#23616;&#37096;&#26368;&#20248;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed Gaussian Bandits with Unknown Variances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12741
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21452;&#33218;&#39640;&#26031;&#36172;&#33218;&#26426;&#22120;&#20154;&#20013;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#26041;&#24046;&#24773;&#20917;&#19979;&#23616;&#37096;&#26368;&#20248;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#21452;&#33218;&#39640;&#26031;&#36172;&#33218;&#26426;&#22120;&#20154;&#20013;&#24102;&#26377;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#12290; &#22312;BAI&#20013;&#65292;&#32473;&#23450;&#22810;&#20010;&#33218;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#23454;&#39564;&#26469;&#25214;&#21040;&#20855;&#26377;&#26368;&#39640;&#26399;&#26395;&#22870;&#21169;&#30340;&#26368;&#20339;&#33218;&#12290; Kaufmann&#31561;&#20154;&#65288;2016&#24180;&#65289;&#20026;&#35823;&#35782;&#21035;&#26368;&#20339;&#33218;&#30340;&#27010;&#29575;&#25552;&#20986;&#20102;&#19968;&#20010;&#19979;&#30028;&#12290; &#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#20551;&#35774;&#22870;&#21169;&#30340;&#26041;&#24046;&#24050;&#30693;&#65292;&#24182;&#34920;&#26126;&#38543;&#30528;&#39044;&#31639;&#26080;&#38480;&#25509;&#36817;&#65292;&#36825;&#31181;&#31574;&#30053;&#22312;&#27010;&#24565;&#19978;&#26159;&#26368;&#20248;&#30340;&#65292;&#21363;&#20854;&#35823;&#35782;&#21035;&#27010;&#29575;&#19982;&#19979;&#30028;&#21305;&#37197;&#12290; &#20294;&#26159;&#65292;&#24403;&#26041;&#24046;&#26410;&#30693;&#26102;&#65292;&#19968;&#31181;&#28176;&#36817;&#20248;&#30340;&#31574;&#30053;&#23578;&#26410;&#34987;&#21457;&#29616;&#12290; &#38024;&#23545;&#36825;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#20272;&#35745;&#26041;&#24046;&#24182;&#20197;&#20272;&#35745;&#26631;&#20934;&#24046;&#27604;&#20363;&#25277;&#21462;&#33218;&#30340;&#31574;&#30053;&#12290; &#25105;&#20204;&#31216;&#27492;&#31574;&#30053;&#20026;Neyman&#20998;&#37197;&#65288;NA&#65289;-&#22686;&#24378;&#20498;&#27010;&#29575;&#21152;&#26435;&#65288;AIPW&#65289;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12741v2 Announce Type: replace  Abstract: We address the problem of best arm identification (BAI) with a fixed budget for two-armed Gaussian bandits. In BAI, given multiple arms, we aim to find the best arm, an arm with the highest expected reward, through an adaptive experiment. Kaufmann et al. (2016) develops a lower bound for the probability of misidentifying the best arm. They also propose a strategy, assuming that the variances of rewards are known, and show that it is asymptotically optimal in the sense that its probability of misidentification matches the lower bound as the budget approaches infinity. However, an asymptotically optimal strategy is unknown when the variances are unknown. For this open issue, we propose a strategy that estimates variances during an adaptive experiment and draws arms with a ratio of the estimated standard deviations. We refer to this strategy as the Neyman Allocation (NA)-Augmented Inverse Probability weighting (AIPW) strategy. We then d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;PEARL&#65292;&#36890;&#36807;&#23398;&#20064;&#21333;&#19968;&#31574;&#30053;&#35299;&#20915;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#31616;&#21333;&#23376;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2312.10194</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#21387;&#27700;&#22534;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Reinforcement Learning-based Approach for Pressurized Water Reactor Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10194
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;PEARL&#65292;&#36890;&#36807;&#23398;&#20064;&#21333;&#19968;&#31574;&#30053;&#35299;&#20915;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#31616;&#21333;&#23376;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#34987;&#31216;&#20026;&#24102;&#26377;&#24378;&#21270;&#23398;&#20064;&#30340;Pareto&#21253;&#32476;&#65288;PEARL&#65289;&#65292;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#35299;&#20915;&#22810;&#30446;&#26631;&#38382;&#39064;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#20505;&#36873;&#35299;&#30340;&#35780;&#20272;&#21487;&#33021;&#20250;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;PEARL&#36890;&#36807;&#23398;&#20064;&#21333;&#19968;&#31574;&#30053;&#26469;&#21306;&#21035;&#20110;&#20256;&#32479;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#29420;&#31435;&#35299;&#20915;&#31616;&#21333;&#23376;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;&#21463;&#28145;&#24230;&#23398;&#20064;&#21644;&#36827;&#21270;&#25216;&#26415;&#21551;&#21457;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#29256;&#26412;&#65292;&#20197;&#36866;&#24212;&#26080;&#32422;&#26463;&#21644;&#26377;&#32422;&#26463;&#30340;&#38382;&#39064;&#39046;&#22495;&#12290;&#35838;&#31243;&#23398;&#20064;&#34987;&#21033;&#29992;&#26469;&#26377;&#25928;&#31649;&#29702;&#36825;&#20123;&#29256;&#26412;&#20013;&#30340;&#32422;&#26463;&#12290;&#39318;&#20808;&#65292;PEARL&#30340;&#24615;&#33021;&#22312;&#32463;&#20856;&#22810;&#30446;&#26631;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#22312;&#20004;&#20010;&#23454;&#38469;&#30340;PWR&#22534;&#33455;&#35013;&#36733;&#26041;&#26696;&#20248;&#21270;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10194v3 Announce Type: replace  Abstract: A novel method, the Pareto Envelope Augmented with Reinforcement Learning (PEARL), has been developed to address the challenges posed by multi-objective problems, particularly in the field of engineering where the evaluation of candidate solutions can be time-consuming. PEARL distinguishes itself from traditional policy-based multi-objective Reinforcement Learning methods by learning a single policy, eliminating the need for multiple neural networks to independently solve simpler sub-problems. Several versions inspired from deep learning and evolutionary techniques have been crafted, catering to both unconstrained and constrained problem domains. Curriculum Learning is harnessed to effectively manage constraints in these versions. PEARL's performance is first evaluated on classical multi-objective benchmarks. Additionally, it is tested on two practical PWR core Loading Pattern optimization problems to showcase its real-world applicab
&lt;/p&gt;</description></item><item><title>CaVE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#39044;&#27979;&#28982;&#21518;&#20248;&#21270;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#25104;&#26412;&#21521;&#37327;&#19982;&#30495;&#23454;&#26368;&#20248;&#35299;&#30340;&#27861;&#32447;&#38181;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#24863;&#30693;&#22411;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2312.07718</link><description>&lt;p&gt;
CaVE: &#19968;&#31181;&#38754;&#21521;&#24555;&#36895;&#39044;&#27979;&#28982;&#21518;&#20248;&#21270;&#30340;&#38181;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize with Binary Linear Programs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07718
&lt;/p&gt;
&lt;p&gt;
CaVE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#39044;&#27979;&#28982;&#21518;&#20248;&#21270;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#25104;&#26412;&#21521;&#37327;&#19982;&#30495;&#23454;&#26368;&#20248;&#35299;&#30340;&#27861;&#32447;&#38181;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#24863;&#30693;&#22411;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#28982;&#21518;&#20248;&#21270;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20063;&#31216;&#20026;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65292;&#22240;&#20854;&#33021;&#22815;&#23558;&#20248;&#21270;&#38598;&#25104;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24182;&#20174;&#19978;&#19979;&#25991;&#23454;&#20363;&#20449;&#24687;&#20013;&#39044;&#27979;&#26410;&#30693;&#25104;&#26412;&#65288;&#30446;&#26631;&#20989;&#25968;&#65289;&#31995;&#25968;&#32780;&#22791;&#21463;&#27426;&#36814;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#65292;&#22823;&#22810;&#25968;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#36890;&#24120;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#35268;&#21010;&#65288;BLPs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#39044;&#27979;&#28982;&#21518;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#38181;&#23545;&#40784;&#21521;&#37327;&#20272;&#35745;&#65288;CaVE&#65289;&#65292;&#23558;&#39044;&#27979;&#30340;&#25104;&#26412;&#21521;&#37327;&#19982;&#35757;&#32451;&#23454;&#20363;&#30340;&#30495;&#23454;&#26368;&#20248;&#35299;&#23545;&#24212;&#30340;&#27861;&#32447;&#38181;&#36827;&#34892;&#23545;&#40784;&#12290;&#24403;&#39044;&#27979;&#30340;&#25104;&#26412;&#21521;&#37327;&#20301;&#20110;&#38181;&#20869;&#37096;&#26102;&#65292;&#20108;&#36827;&#21046;&#38382;&#39064;&#32447;&#24615;&#26494;&#24347;&#30340;&#26368;&#20248;&#35299;&#20063;&#26159;&#26368;&#20248;&#30340;&#12290;&#36825;&#31181;&#23545;&#40784;&#19981;&#20165;&#33021;&#20135;&#29983;&#20915;&#31574;&#24863;&#30693;&#22411;&#23398;&#20064;&#27169;&#22411;&#65292;&#36824;&#33021;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07718v2 Announce Type: replace  Abstract: The end-to-end predict-then-optimize framework, also known as decision-focused learning, has gained popularity for its ability to integrate optimization into the training procedure of machine learning models that predict the unknown cost (objective function) coefficients of optimization problems from contextual instance information. Naturally, most of the problems of interest in this space can be cast as integer linear programs. In this work, we focus on binary linear programs (BLPs) and propose a new end-to-end training method to predict-then-optimize. Our method, Cone-aligned Vector Estimation (CaVE), aligns the predicted cost vectors with the normal cone corresponding to the true optimal solution of a training instance. When the predicted cost vector lies inside the cone, the optimal solution to the linear relaxation of the binary problem is optimal. This alignment not only produces decision-aware learning models but also dramatic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20026;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#35757;&#32451;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;&#65292;&#25104;&#21151;&#25429;&#25417;&#25972;&#20010;&#35299;&#20998;&#24067;&#24182;&#20934;&#30830;&#20272;&#35745;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05320</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Surrogate Models for Airfoil Flow Simulations with Denoising Diffusion Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20026;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#35757;&#32451;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;&#65292;&#25104;&#21151;&#25429;&#25417;&#25972;&#20010;&#35299;&#20998;&#24067;&#24182;&#20934;&#30830;&#20272;&#35745;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#28237;&#27969;&#27169;&#25311;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#21516;&#26102;&#65292;&#22312;&#20195;&#29702;&#27169;&#22411;&#30340;&#39044;&#27979;&#20013;&#20307;&#29616;&#27169;&#25311;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#28237;&#27969;&#27169;&#25311;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;&#12290;&#30001;&#20110;&#20854;&#26222;&#36941;&#24615;&#65292;&#36873;&#25321;&#20197;&#21508;&#31181;&#24418;&#29366;&#12289;&#38647;&#35834;&#25968;&#21644;&#25915;&#35282;&#30340;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DDPMs&#33021;&#25104;&#21151;&#25429;&#25417;&#35299;&#30340;&#25972;&#20010;&#20998;&#24067;&#65292;&#20174;&#32780;&#20934;&#30830;&#20272;&#35745;&#27169;&#25311;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;DDPMs&#30340;&#24615;&#33021;&#36824;&#19982;&#20197;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#24322;&#26041;&#24046;&#27169;&#22411;&#24418;&#24335;&#30340;&#19981;&#21516;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DDPMs&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05320v2 Announce Type: replace-cross  Abstract: Leveraging neural networks as surrogate models for turbulence simulation is a topic of growing interest. At the same time, embodying the inherent uncertainty of simulations in the predictions of surrogate models remains very challenging. The present study makes a first attempt to use denoising diffusion probabilistic models (DDPMs) to train an uncertainty-aware surrogate model for turbulence simulations. Due to its prevalence, the simulation of flows around airfoils with various shapes, Reynolds numbers, and angles of attack is chosen as the learning objective. Our results show that DDPMs can successfully capture the whole distribution of solutions and, as a consequence, accurately estimate the uncertainty of the simulations. The performance of DDPMs is also compared with varying baselines in the form of Bayesian neural networks and heteroscedastic models. Experiments demonstrate that DDPMs outperform the other methods regardin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SpanishTinyRoBERTa&#65292;&#19968;&#20010;&#22522;&#20110;RoBERTa&#30340;&#35199;&#29677;&#29273;&#35821;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#35199;&#29677;&#29273;&#35821;&#38382;&#31572;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.04193</link><description>&lt;p&gt;
&#35199;&#29677;&#29273;&#35821;&#38382;&#31572;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Language Model Knowledge Distillation for Efficient Question Answering in Spanish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04193
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SpanishTinyRoBERTa&#65292;&#19968;&#20010;&#22522;&#20110;RoBERTa&#30340;&#35199;&#29677;&#29273;&#35821;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#35199;&#29677;&#29273;&#35821;&#38382;&#31572;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#23637;&#30340;&#39044;&#35757;&#32451;&#35199;&#29677;&#29273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#39640;&#25928;&#27169;&#22411;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#37319;&#29992;&#26500;&#25104;&#20102;&#19968;&#36947;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#35199;&#29677;&#29273;&#35821;&#30340;&#36739;&#23567;&#30340;&#33976;&#39311;&#27169;&#22411;&#21487;&#33021;&#34987;&#35777;&#26126;&#26159;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#20419;&#36827;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#30340;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;RoBERTa&#30340;&#35199;&#29677;&#29273;&#35821;&#39640;&#25928;&#38382;&#31572;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;SpanishTinyRoBERTa&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#22823;&#27169;&#22411;&#21521;&#19968;&#20010;&#26356;&#36731;&#30340;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#36825;&#20351;&#24471;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#25104;&#20026;&#21487;&#33021;&#65292;&#21363;&#20351;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#65292;&#20063;&#33021;&#23454;&#29616;&#21487;&#24573;&#30053;&#30340;&#24615;&#33021;&#29306;&#29298;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#23494;&#38598;&#30340;&#33976;&#39311;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#38480;&#21046;&#30340;&#35745;&#31639;&#24615;&#33021;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04193v2 Announce Type: replace  Abstract: Recent advances in the development of pre-trained Spanish language models has led to significant progress in many Natural Language Processing (NLP) tasks, such as question answering. However, the lack of efficient models imposes a barrier for the adoption of such models in resource-constrained environments. Therefore, smaller distilled models for the Spanish language could be proven to be highly scalable and facilitate their further adoption on a variety of tasks and scenarios. In this work, we take one step in this direction by developing SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient question answering in Spanish. To achieve this, we employ knowledge distillation from a large model onto a lighter model that allows for a wider implementation, even in areas with limited computational resources, whilst attaining negligible performance sacrifice. Our experiments show that the dense distilled model can st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(FL-IDS)&#65292;&#26088;&#22312;&#35299;&#20915;FANETs&#20013;&#38598;&#20013;&#24335;&#31995;&#32479;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#36866;&#21512;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#20154;&#26426;&#12290;</title><link>https://arxiv.org/abs/2312.04135</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#29992;&#20110;&#22686;&#24378;&#26080;&#20154;&#26426;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Novel Federated Learning-Based IDS for Enhancing UAVs Privacy and Security
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(FL-IDS)&#65292;&#26088;&#22312;&#35299;&#20915;FANETs&#20013;&#38598;&#20013;&#24335;&#31995;&#32479;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#36866;&#21512;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#20154;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22312;&#39134;&#34892;&#33258;&#32452;&#32455;&#32593;&#32476;(FANETs)&#20013;&#36816;&#34892;&#26102;&#20250;&#36935;&#21040;&#23433;&#20840;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#32593;&#32476;&#20855;&#26377;&#21160;&#24577;&#21644;&#20998;&#24067;&#24335;&#30340;&#29305;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38598;&#20013;&#24335;&#20837;&#20405;&#26816;&#27979;&#19978;&#65292;&#20551;&#35774;&#19968;&#20010;&#20013;&#22830;&#23454;&#20307;&#36127;&#36131;&#23384;&#20648;&#21644;&#20998;&#26512;&#26469;&#33258;&#25152;&#26377;&#35774;&#22791;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20197;&#21450;&#21333;&#28857;&#25925;&#38556;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#23041;&#32961;&#21040;&#25968;&#25454;&#38544;&#31169;&#21644;&#21487;&#29992;&#24615;&#12290;&#25968;&#25454;&#22312;&#20114;&#36830;&#35774;&#22791;&#20043;&#38388;&#24191;&#27867;&#20998;&#25955;&#30340;&#24773;&#20917;&#31361;&#26174;&#20102;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(FL-IDS)&#65292;&#35299;&#20915;&#20102;FANETs&#20013;&#38598;&#20013;&#24335;&#31995;&#32479;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;FL-IDS&#22312;&#21435;&#20013;&#24515;&#21270;&#26041;&#24335;&#19979;&#36816;&#34892;&#65292;&#38477;&#20302;&#20102;&#23458;&#25143;&#31471;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#20154;&#26426;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04135v2 Announce Type: replace-cross  Abstract: Unmanned aerial vehicles (UAVs) operating within Flying Ad-hoc Networks (FANETs) encounter security challenges due to the dynamic and distributed nature of these networks. Previous studies predominantly focused on centralized intrusion detection, assuming a central entity responsible for storing and analyzing data from all devices.However, these approaches face challenges including computation and storage costs, along with a single point of failure risk, threatening data privacy and availability. The widespread dispersion of data across interconnected devices underscores the necessity for decentralized approaches. This paper introduces the Federated Learning-based Intrusion Detection System (FL-IDS), addressing challenges encountered by centralized systems in FANETs. FL-IDS reduces computation and storage costs for both clients and the central server, crucial for resource-constrained UAVs. Operating in a decentralized manner, F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;1D&#24515;&#38899;&#22270;&#26679;&#26412;&#20013;&#24322;&#24120;&#26816;&#27979;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38899;&#39057;&#22686;&#24378;&#26041;&#27861;&#27604;&#36739;&#35780;&#20272;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2312.00502</link><description>&lt;p&gt;
&#23545;&#31283;&#20581;&#30340;OOD&#33258;&#30417;&#30563;&#23545;&#27604;&#24515;&#38899;&#22270;&#34920;&#31034;&#23398;&#20064;&#22686;&#24378;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Augmentations for Robust OOD Self-Supervised Contrastive Phonocardiogram Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;1D&#24515;&#38899;&#22270;&#26679;&#26412;&#20013;&#24322;&#24120;&#26816;&#27979;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38899;&#39057;&#22686;&#24378;&#26041;&#27861;&#27604;&#36739;&#35780;&#20272;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#27963;&#21160;&#26377;&#25152;&#22686;&#21152;&#65292;&#20294;&#22312;&#21307;&#23398;&#31561;&#22810;&#20010;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#34987;&#24191;&#27867;&#25509;&#21463;&#12290;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#30701;&#32570;&#32463;&#24120;&#38459;&#30861;&#20102;&#24320;&#21457;&#31283;&#20581;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#27169;&#22411;&#65292;&#24403;&#38754;&#20020;&#26032;&#25910;&#38598;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;&#22240;&#25928;&#26524;&#19979;&#38477;&#32780;&#21463;&#25439;&#12290;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20026;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#22686;&#21152;&#27169;&#22411;&#30340;&#25928;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23545;&#27604;SSL&#24212;&#29992;&#20110;&#26816;&#27979;1D&#24515;&#38899;&#22270;&#65288;PCG&#65289;&#26679;&#26412;&#20013;&#30340;&#24322;&#24120;&#65292;&#36890;&#36807;&#23398;&#20064;&#20449;&#21495;&#30340;&#24191;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#28041;&#21450;&#22810;&#31181;&#22522;&#20110;&#38899;&#39057;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#22312;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00502v2 Announce Type: replace  Abstract: Despite the recent increase in research activity, deep-learning models have not yet been widely accepted in several real-world settings, such as medicine. The shortage of high-quality annotated data often hinders the development of robust and generalizable models, which do not suffer from degraded effectiveness when presented with newly-collected, out-of-distribution (OOD) datasets. Contrastive Self-Supervised Learning (SSL) offers a potential solution to labeled data scarcity, as it takes advantage of unlabeled data to increase model effectiveness and robustness. In this research, we propose applying contrastive SSL for detecting abnormalities in 1D phonocardiogram (PCG) samples by learning a generalized representation of the signal. Specifically, we perform an extensive comparative evaluation of a wide range of audio-based augmentations, evaluate trained classifiers on multiple datasets across different downstream tasks, and finall
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Wasserstein&#36317;&#31163;&#21450;&#20854;&#37325;&#24515;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20998;&#24067;&#21305;&#37197;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18531</link><description>&lt;p&gt;
&#36890;&#36807;Wasserstein&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation via the Wasserstein Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18531
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Wasserstein&#36317;&#31163;&#21450;&#20854;&#37325;&#24515;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20998;&#24067;&#21305;&#37197;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#20449;&#24687;&#23553;&#35013;&#20026;&#26126;&#26174;&#26356;&#23567;&#30340;&#21512;&#25104;&#31561;&#20215;&#29289;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#30041;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#36317;&#31163;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#22686;&#24378;DD&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Wasserstein&#37325;&#24515;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#37327;&#21270;&#20998;&#24067;&#24046;&#24322;&#21644;&#39640;&#25928;&#25429;&#33719;&#20998;&#24067;&#38598;&#21512;&#20013;&#24515;&#30340;&#20960;&#20309;&#24847;&#20041;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#20419;&#36827;&#20102;&#26377;&#25928;&#30340;&#20998;&#24067;&#21305;&#37197;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#25345;&#20102;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#25216;&#26415;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#32780;&#19988;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18531v2 Announce Type: replace-cross  Abstract: Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the expansive information of large datasets into significantly smaller, synthetic equivalents, thereby preserving model performance with reduced computational overhead. Pursuing this objective, we introduce the Wasserstein distance, a metric grounded in optimal transport theory, to enhance distribution matching in DD. Our approach employs the Wasserstein barycenter to provide a geometrically meaningful method for quantifying distribution differences and capturing the centroid of distribution sets efficiently. By embedding synthetic data in the feature spaces of pretrained classification models, we facilitate effective distribution matching that leverages prior knowledge inherent in these models. Our method not only maintains the computational advantages of distribution matching-based techniques but also achieves new state-of-the-art performance across a ran
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#22810;&#39033;&#24335;&#35745;&#25968;&#25968;&#25454;&#30340;&#20998;&#26512;&#38656;&#27714;&#65292;&#24182;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23436;&#20840;&#33258;&#21160;&#25552;&#21462;&#20986;&#29983;&#29289;&#24847;&#20041;&#30340;&#20803;&#31614;&#21517;&#12290;</title><link>https://arxiv.org/abs/2311.16909</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#20449;&#24565;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multinomial belief networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#22810;&#39033;&#24335;&#35745;&#25968;&#25968;&#25454;&#30340;&#20998;&#26512;&#38656;&#27714;&#65292;&#24182;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23436;&#20840;&#33258;&#21160;&#25552;&#21462;&#20986;&#29983;&#29289;&#24847;&#20041;&#30340;&#20803;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;&#38656;&#35201;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12289;&#22788;&#29702;&#32570;&#22833;&#35266;&#27979;&#12289;&#26679;&#26412;&#31232;&#32570;&#25110;&#25968;&#25454;&#31232;&#30095;&#26102;&#26159;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#20998;&#26512;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#39033;&#24335;&#35745;&#25968;&#25968;&#25454;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#21333;&#20803;&#22343;&#26381;&#20174;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21033;&#29992;&#19968;&#31995;&#21015;&#22686;&#24191;&#20851;&#31995;&#30340;&#21513;&#24067;&#26031;&#25277;&#26679;&#36807;&#31243;&#65292;&#31867;&#20284;&#20110;&#21608;-&#19995;-&#38472;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#23567;&#35268;&#27169;&#25163;&#20889;&#25968;&#23383;&#21644;&#19968;&#20010;&#22823;&#22411;&#30340;DNA&#31361;&#21464;&#30284;&#30151;&#23454;&#39564;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22914;&#20309;&#33021;&#22815;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#22320;&#25552;&#21462;&#20986;&#29983;&#29289;&#24847;&#20041;&#30340;&#20803;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16909v2 Announce Type: replace-cross  Abstract: A Bayesian approach to machine learning is attractive when we need to quantify uncertainty, deal with missing observations, when samples are scarce, or when the data is sparse. All of these commonly apply when analysing healthcare data. To address these analytical requirements, we propose a deep generative model for multinomial count data where both the weights and hidden units of the network are Dirichlet distributed. A Gibbs sampling procedure is formulated that takes advantage of a series of augmentation relations, analogous to the Zhou--Cong--Chen model. We apply the model on small handwritten digits, and a large experimental dataset of DNA mutations in cancer, and we show how the model is able to extract biologically meaningful meta-signatures in a fully data-driven way.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2311.16834</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#20351;&#29992;&#27880;&#24847;&#21147;&#36827;&#34892;&#35299;&#37322;&#24615;&#21644;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#27668;&#35937;&#23398;&#21644;&#29983;&#21629;&#31185;&#23398;&#31561;&#39046;&#22495;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#34987;&#25209;&#35780;&#20026;&#8220;&#40657;&#30418;&#8221;&#25110;&#26080;&#27861;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#26500;&#36896;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#25233;&#21046;&#22312;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#24615;&#20013;&#20351;&#29992;&#30340;&#20887;&#20313;&#29305;&#24449;&#12290;&#20174;&#36873;&#25321;&#30340;&#29305;&#24449;&#29420;&#31435;&#35757;&#32451;&#27169;&#22359;&#21270;&#28145;&#24230;&#32593;&#32476;&#65292;&#21521;&#29992;&#25143;&#23637;&#31034;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16834v3 Announce Type: replace-cross  Abstract: Multivariate time series have many applications, from healthcare and meteorology to life science. Although deep learning models have shown excellent predictive performance for time series, they have been criticised for being "black-boxes" or non-interpretable. This paper proposes a novel modular neural network model for multivariate time series prediction that is interpretable by construction. A recurrent neural network learns the temporal dependencies in the data while an attention-based feature selection component selects the most relevant features and suppresses redundant features used in the learning of the temporal dependencies. A modular deep network is trained from the selected features independently to show the users how features influence outcomes, making the model interpretable. Experimental results show that this approach can outperform state-of-the-art interpretable Neural Additive Models (NAM) and variations thereo
&lt;/p&gt;</description></item><item><title>Symphony&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$E(3)$-&#31561;&#21464;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#23545;&#31216;&#29699;&#24418;&#35856;&#27874;&#20449;&#21495;&#26469;&#39640;&#25928;&#24314;&#27169;&#20998;&#23376;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2311.16199</link><description>&lt;p&gt;
Symphony: &#23545;&#20998;&#23376;&#29983;&#25104;&#30340;&#28857;&#23545;&#31216;&#29699;&#24418;&#35856;&#27874;&#30340;&#23545;&#31216;&#31561;&#21464;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16199
&lt;/p&gt;
&lt;p&gt;
Symphony&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$E(3)$-&#31561;&#21464;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#23545;&#31216;&#29699;&#24418;&#35856;&#27874;&#20449;&#21495;&#26469;&#39640;&#25928;&#24314;&#27169;&#20998;&#23376;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Symphony&#65292;&#36825;&#26159;&#19968;&#20010;$E(3)$-&#31561;&#21464;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#26500;&#24314;&#65292;&#36890;&#36807;&#20174;&#20998;&#23376;&#30862;&#29255;&#20013;&#36845;&#20195;&#22320;&#26500;&#24314;&#20998;&#23376;&#12290;&#29616;&#26377;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#22914;G-SchNet&#21644;G-SphereNet&#29992;&#20110;&#20998;&#23376;&#30340;&#26059;&#36716;&#19981;&#21464;&#29305;&#24449;&#26469;&#23562;&#37325;&#20998;&#23376;&#30340;3D&#23545;&#31216;&#24615;&#12290;&#30456;&#21453;&#65292;Symphony&#20351;&#29992;&#24102;&#26377;&#26356;&#39640;&#27425;$E(3)$-&#31561;&#21464;&#29305;&#24449;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;&#36825;&#20351;&#24471;&#36890;&#36807;&#29699;&#35856;&#20449;&#21495;&#26377;&#25928;&#22320;&#24314;&#27169;&#20998;&#23376;&#30340;3D&#20960;&#20309;&#30340;&#27010;&#29575;&#20998;&#24067;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Symphony&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;QM9&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#23567;&#20998;&#23376;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#25509;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16199v2 Announce Type: replace  Abstract: We present Symphony, an $E(3)$-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet and G-SphereNet for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree $E(3)$-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22235;&#39033;&#29992;&#20110;&#35774;&#35745;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#25351;&#21335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20855;&#26377;&#36890;&#29992;&#24863;&#30693;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.15599</link><description>&lt;p&gt;
UniRepLKNet&#65306;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#28857;&#20113;&#12289;&#26102;&#38388;&#24207;&#21015;&#21644;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#24863;&#30693;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15599
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22235;&#39033;&#29992;&#20110;&#35774;&#35745;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#25351;&#21335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20855;&#26377;&#36890;&#29992;&#24863;&#30693;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNets)&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20294;&#20173;&#26377;&#20004;&#20010;&#26410;&#35299;&#20915;&#30340;&#37325;&#35201;&#38382;&#39064;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;1) &#29616;&#26377;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#36981;&#24490;&#20256;&#32479;ConvNets&#25110;transformers&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#32780;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#35774;&#35745;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;2) &#38543;&#30528;transformers&#20027;&#23548;&#20102;&#22810;&#31181;&#27169;&#24577;&#65292;&#20173;&#38656;&#25506;&#35752;ConvNets&#22312;&#35270;&#35273;&#20197;&#22806;&#39046;&#22495;&#26159;&#21542;&#20855;&#26377;&#24378;&#22823;&#30340;&#36890;&#29992;&#24863;&#30693;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;1) &#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#39033;&#29992;&#20110;&#35774;&#35745;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#25351;&#21335;&#65292;&#20854;&#20013;&#30340;&#26680;&#24515;&#26159;&#21033;&#29992;&#22823;&#21367;&#31215;&#26680;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#21306;&#21035;&#20110;&#23567;&#21367;&#31215;&#26680;-&#23427;&#20204;&#21487;&#20197;&#24191;&#27867;&#22320;&#35266;&#23519;&#32780;&#26080;&#38656;&#28145;&#20837;&#12290;&#36981;&#24490;&#36825;&#20123;&#25351;&#21335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15599v2 Announce Type: replace-cross  Abstract: Large-kernel convolutional neural networks (ConvNets) have recently received extensive research attention, but two unresolved and critical issues demand further investigation. 1) The architectures of existing large-kernel ConvNets largely follow the design principles of conventional ConvNets or transformers, while the architectural design for large-kernel ConvNets remains under-addressed. 2) As transformers have dominated multiple modalities, it remains to be investigated whether ConvNets also have a strong universal perception ability in domains beyond vision. In this paper, we contribute from two aspects. 1) We propose four architectural guidelines for designing large-kernel ConvNets, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep. Following such guidelines, our proposed large-kernel ConvNet shows leading performanc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.15487</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15487
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#26368;&#23567;&#21270;$\mathcal{L}^2$&#20195;&#20215;&#20989;&#25968;&#30340;&#26799;&#24230;&#19979;&#38477;&#27969;&#65292;&#24182;&#24341;&#20837;&#20004;&#20010;&#25913;&#36827;&#29256;&#26412;&#65307;&#19968;&#20010;&#36866;&#29992;&#20110;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#65292;&#21478;&#19968;&#20010;&#36866;&#29992;&#20110;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#12290;&#36825;&#20004;&#20010;&#29256;&#26412;&#37117;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#65292;&#32771;&#34385;&#21040;&#22312;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25289;&#22238;&#21521;&#37327;&#19995;&#32467;&#26500;&#21644;&#22312;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25512;&#21069;&#21521;&#37327;&#19995;&#32467;&#26500;&#12290;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#28385;&#36275;&#31209;&#26465;&#20214;&#65292;&#25913;&#36827;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#25152;&#26377;&#36712;&#36947;&#23558;&#20197;&#22343;&#21248;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#23558;$\mathcal{L}^2$&#20195;&#20215;&#39537;&#21160;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#65307;&#22240;&#27492;&#65292;&#23545;&#20110;&#20219;&#20309;&#39044;&#20808;&#25351;&#23450;&#30340;&#25509;&#36817;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#20808;&#39564;&#20572;&#27490;&#26102;&#38388;&#12290;&#25105;&#20204;&#25351;&#20986;&#21518;&#32773;&#19982;&#27425;Riemann&#20960;&#20309;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15487v3 Announce Type: replace-cross  Abstract: We consider the gradient descent flow widely used for the minimization of the $\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate; one thereby obtains an a priori stopping time for any prescribed proximity to the global minimum. We point out relations of the latter to sub-Riemannian geometry.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CRISP&#65292;&#19968;&#31181;&#32467;&#21512;&#31934;&#32454;N:M&#32467;&#26500;&#31232;&#30095;&#24615;&#21644;&#31895;&#31890;&#24230;&#22359;&#31232;&#30095;&#24615;&#30340;&#28151;&#21512;&#32467;&#26500;&#31232;&#30095;&#27169;&#24335;&#65292;&#26088;&#22312;&#22686;&#24378;&#22522;&#20110;&#31867;&#21035;&#30340;&#27169;&#22411;&#21098;&#26525;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.14272</link><description>&lt;p&gt;
CRISP&#65306;&#29992;&#20110;&#22522;&#20110;&#31867;&#21035;&#30340;&#27169;&#22411;&#21098;&#26525;&#30340;&#28151;&#21512;&#32467;&#26500;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
CRISP: Hybrid Structured Sparsity for Class-aware Model Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14272
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CRISP&#65292;&#19968;&#31181;&#32467;&#21512;&#31934;&#32454;N:M&#32467;&#26500;&#31232;&#30095;&#24615;&#21644;&#31895;&#31890;&#24230;&#22359;&#31232;&#30095;&#24615;&#30340;&#28151;&#21512;&#32467;&#26500;&#31232;&#30095;&#27169;&#24335;&#65292;&#26088;&#22312;&#22686;&#24378;&#22522;&#20110;&#31867;&#21035;&#30340;&#27169;&#22411;&#21098;&#26525;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#36890;&#24120;&#20250;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22312;&#21508;&#31181;&#31867;&#21035;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#29992;&#25143;&#21482;&#20250;&#23450;&#26399;&#36935;&#21040;&#26377;&#38480;&#30340;&#31867;&#21035;&#36873;&#25321;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#20250;&#65292;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#37325;&#28857;&#20851;&#27880;&#29992;&#25143;&#29305;&#23450;&#31867;&#21035;&#26469;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#12290;&#29616;&#26377;&#20316;&#21697;&#20381;&#36182;&#20110;&#19981;&#32467;&#26500;&#21270;&#30340;&#21098;&#26525;&#65292;&#36825;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#38543;&#26426;&#20998;&#24067;&#30340;&#38750;&#38646;&#20540;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#30828;&#20214;&#21152;&#36895;&#12290;&#21478;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#20351;&#29992;&#32467;&#26500;&#21270;&#21098;&#26525;&#65292;&#22914;&#36890;&#36947;&#21098;&#26525;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#25552;&#20379;&#26368;&#23567;&#30340;&#21387;&#32553;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CRISP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21098;&#26525;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#28151;&#21512;&#32467;&#26500;&#31232;&#30095;&#24615;&#27169;&#24335;&#65292;&#32467;&#21512;&#31934;&#32454;&#30340;N:M&#32467;&#26500;&#31232;&#30095;&#24615;&#21644;&#31895;&#31890;&#24230;&#30340;&#22359;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#30340;&#21098;&#26525;&#31574;&#30053;&#26159;&#22522;&#20110;&#26799;&#24230;&#30340;&#22522;&#20110;&#31867;&#21035;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14272v2 Announce Type: replace-cross  Abstract: Machine learning pipelines for classification tasks often train a universal model to achieve accuracy across a broad range of classes. However, a typical user encounters only a limited selection of classes regularly. This disparity provides an opportunity to enhance computational efficiency by tailoring models to focus on user-specific classes. Existing works rely on unstructured pruning, which introduces randomly distributed non-zero values in the model, making it unsuitable for hardware acceleration. Alternatively, some approaches employ structured pruning, such as channel pruning, but these tend to provide only minimal compression and may lead to reduced model accuracy. In this work, we propose CRISP, a novel pruning framework leveraging a hybrid structured sparsity pattern that combines both fine-grained N:M structured sparsity and coarse-grained block sparsity. Our pruning strategy is guided by a gradient-based class-aware
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;K-M3AID&#65289;&#65292;&#36890;&#36807;&#22312;&#20998;&#23376;&#22270;&#21644;NMR&#20809;&#35889;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#37319;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#20998;&#23376;&#26816;&#32034;&#12289;&#24322;&#26500;&#20307;&#35782;&#21035;&#21644;&#23792;&#24402;&#23646;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.13817</link><description>&lt;p&gt;
&#20998;&#23376;&#37492;&#23450;&#19982;&#23792;&#24402;&#23646;&#65306;&#22312;NMR&#19978;&#21033;&#29992;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Molecular Identification and Peak Assignment: Leveraging Multi-Level Multimodal Alignment on NMR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;K-M3AID&#65289;&#65292;&#36890;&#36807;&#22312;&#20998;&#23376;&#22270;&#21644;NMR&#20809;&#35889;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#37319;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#20998;&#23376;&#26816;&#32034;&#12289;&#24322;&#26500;&#20307;&#35782;&#21035;&#21644;&#23792;&#24402;&#23646;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13817v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#35299;&#35835;&#20998;&#23376;&#32467;&#26500;&#21644;&#21160;&#24577;&#34892;&#20026;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#22522;&#20110;AI&#30340;NMR&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#20998;&#23376;&#26816;&#32034;&#12289;&#24322;&#26500;&#20307;&#35782;&#21035;&#21644;&#23792;&#24402;&#23646;&#31561;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20855;&#26377;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#40784;&#30340;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;K-M3AID&#65289;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;&#20004;&#31181;&#24322;&#36136;&#27169;&#24577;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65306;&#20998;&#23376;&#22270;&#21644;NMR&#20809;&#35889;&#12290;K-M3AID&#37319;&#29992;&#20102;&#19968;&#20010;&#21452;&#21327;&#35843;&#23545;&#27604;&#23398;&#20064;&#26550;&#26500;&#65292;&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#22270;&#32423;&#23545;&#40784;&#27169;&#22359;&#12289;&#33410;&#28857;&#32423;&#23545;&#40784;&#27169;&#22359;&#21644;&#36890;&#20449;&#36890;&#36947;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#33410;&#28857;&#32423;&#23545;&#40784;&#27169;&#22359;&#20013;&#65292;K-M3AID&#24341;&#20837;&#20102;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;K-M3AID&#34920;&#26126;&#22312;&#33410;&#28857;&#32423;&#23545;&#40784;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13817v2 Announce Type: replace  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays an essential role in deciphering molecular structure and dynamic behaviors. While AI-enhanced NMR prediction models hold promise, challenges still persist in tasks such as molecular retrieval, isomer recognition, and peak assignment. In response, this paper introduces a novel solution, Multi-Level Multimodal Alignment with Knowledge-Guided Instance-Wise Discrimination (K-M3AID), which establishes correspondences between two heterogeneous modalities: molecular graphs and NMR spectra. K-M3AID employs a dual-coordinated contrastive learning architecture with three key modules: a graph-level alignment module, a node-level alignment module, and a communication channel. Notably, K-M3AID introduces knowledge-guided instance-wise discrimination into contrastive learning within the node-level alignment module. In addition, K-M3AID demonstrates that skills acquired during node-level alignment
&lt;/p&gt;</description></item><item><title>&#36870;&#38382;&#39064;&#20013;&#20351;&#29992;&#30340;&#20855;&#26377;&#23398;&#20064;&#27491;&#28436;&#31639;&#23376;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#31181;&#65306;&#19968;&#31181;&#23436;&#20840;&#19981;&#32771;&#34385;&#27491;&#28436;&#31639;&#23376;&#65292;&#23398;&#20064;&#20854;&#22312;&#35757;&#32451;&#25968;&#25454;&#23376;&#31354;&#38388;&#19978;&#30340;&#38480;&#21046;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;&#31616;&#21270;&#30340;&#29289;&#29702;&#27169;&#22411;&#24182;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#27169;&#22411;&#20462;&#27491;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24378;&#35843;&#23545;&#27491;&#28436;&#31639;&#23376;&#21450;&#20854;&#20849;&#36717;&#31639;&#23376;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.12528</link><description>&lt;p&gt;
&#20855;&#26377;&#23398;&#20064;&#27491;&#28436;&#31639;&#23376;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Inverse Problems with Learned Forward Operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12528
&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#20013;&#20351;&#29992;&#30340;&#20855;&#26377;&#23398;&#20064;&#27491;&#28436;&#31639;&#23376;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#31181;&#65306;&#19968;&#31181;&#23436;&#20840;&#19981;&#32771;&#34385;&#27491;&#28436;&#31639;&#23376;&#65292;&#23398;&#20064;&#20854;&#22312;&#35757;&#32451;&#25968;&#25454;&#23376;&#31354;&#38388;&#19978;&#30340;&#38480;&#21046;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;&#31616;&#21270;&#30340;&#29289;&#29702;&#27169;&#22411;&#24182;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#27169;&#22411;&#20462;&#27491;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24378;&#35843;&#23545;&#27491;&#28436;&#31639;&#23376;&#21450;&#20854;&#20849;&#36717;&#31639;&#23376;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#36870;&#38382;&#39064;&#38656;&#35201;&#23545;&#27491;&#28436;&#31639;&#23376;&#26377;&#25152;&#20102;&#35299;&#65292;&#20294;&#20934;&#30830;&#30340;&#27169;&#22411;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#24076;&#26395;&#26377;&#19981;&#24433;&#21709;&#37325;&#24314;&#36136;&#37327;&#30340;&#26356;&#20415;&#23452;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#31456;&#22238;&#39038;&#20102;&#37319;&#29992;&#20855;&#26377;&#23398;&#20064;&#27491;&#28436;&#31639;&#23376;&#30340;&#36870;&#38382;&#39064;&#20013;&#30340;&#37325;&#24314;&#26041;&#27861;&#65292;&#36981;&#24490;&#20004;&#31181;&#19981;&#21516;&#30340;&#33539;&#20363;&#12290;&#31532;&#19968;&#31181;&#23545;&#27491;&#28436;&#31639;&#23376;&#23436;&#20840;&#19981;&#21487;&#30693;&#65292;&#24182;&#23398;&#20064;&#20854;&#22312;&#30001;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#23376;&#31354;&#38388;&#19978;&#30340;&#38480;&#21046;&#12290;&#28982;&#21518;&#20351;&#29992;&#25237;&#24433;&#27491;&#21017;&#21270;&#26694;&#26550;&#25214;&#21040;&#37325;&#24314;&#12290;&#31532;&#20108;&#31181;&#20351;&#29992;&#27979;&#37327;&#36807;&#31243;&#29289;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#24182;&#20165;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#27169;&#22411;&#26657;&#27491;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#29702;&#35770;&#24182;&#36827;&#34892;&#20102;&#25968;&#20540;&#27604;&#36739;&#12290;&#19968;&#20010;&#20849;&#21516;&#30340;&#20027;&#39064;&#20986;&#29616;&#65306;&#20004;&#31181;&#26041;&#27861;&#37117;&#38656;&#35201;&#25110;&#33267;&#23569;&#21463;&#30410;&#20110;&#35757;&#32451;&#25968;&#25454;&#19981;&#20165;&#23545;&#20110;&#27491;&#28436;&#31639;&#23376;&#32780;&#19988;&#23545;&#20110;&#20854;&#20849;&#36717;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12528v2 Announce Type: replace-cross  Abstract: Solving inverse problems requires the knowledge of the forward operator, but accurate models can be computationally expensive and hence cheaper variants that do not compromise the reconstruction quality are desired. This chapter reviews reconstruction methods in inverse problems with learned forward operators that follow two different paradigms. The first one is completely agnostic to the forward operator and learns its restriction to the subspace spanned by the training data. The framework of regularisation by projection is then used to find a reconstruction. The second one uses a simplified model of the physics of the measurement process and only relies on the training data to learn a model correction. We present the theory of these two approaches and compare them numerically. A common theme emerges: both methods require, or at least benefit from, training data not only for the forward operator, but also for its adjoint.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24490;&#29615;&#21464;&#21387;&#22120;&#26550;&#26500;&#21450;&#20854;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36845;&#20195;&#29305;&#24449;&#34701;&#20837;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#35299;&#20915;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#26041;&#38754;&#19982;&#26631;&#20934;&#21464;&#21387;&#22120;&#24615;&#33021;&#21487;&#27604;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;10%&#12290;</title><link>https://arxiv.org/abs/2311.12424</link><description>&lt;p&gt;
&#24490;&#29615;&#21464;&#21387;&#22120;&#22312;&#23398;&#20064;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#26356;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Looped Transformers are Better at Learning Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24490;&#29615;&#21464;&#21387;&#22120;&#26550;&#26500;&#21450;&#20854;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36845;&#20195;&#29305;&#24449;&#34701;&#20837;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#35299;&#20915;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#26041;&#38754;&#19982;&#26631;&#20934;&#21464;&#21387;&#22120;&#24615;&#33021;&#21487;&#27604;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#24050;&#32463;&#35777;&#26126;&#22312;&#19978;&#19979;&#25991;&#20013;&#35299;&#20915;&#26469;&#33258;&#21508;&#31181;(&#28508;&#22312;)&#27169;&#22411;&#30340;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26159;&#65292;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#32570;&#20047;&#22266;&#26377;&#30340;&#36845;&#20195;&#32467;&#26500;&#22312;&#27169;&#25311;&#36845;&#20195;&#31639;&#27861;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#32780;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36845;&#20195;&#31639;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#21464;&#21387;&#22120;&#26550;&#26500;&#21450;&#20854;&#30456;&#20851;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#36845;&#20195;&#29305;&#24449;&#34701;&#20837;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24490;&#29615;&#21464;&#21387;&#22120;&#22312;&#35299;&#20915;&#21508;&#31181;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#26041;&#38754;&#30340;&#24615;&#33021;&#19982;&#26631;&#20934;&#21464;&#21387;&#22120;&#21487;&#27604;&#65292;&#21516;&#26102;&#21442;&#25968;&#25968;&#37327;&#19981;&#21040;&#26631;&#20934;&#21464;&#21387;&#22120;&#30340;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12424v3 Announce Type: replace  Abstract: Transformers have demonstrated effectiveness in in-context solving data-fitting problems from various (latent) models, as reported by Garg et al. However, the absence of an inherent iterative structure in the transformer architecture presents a challenge in emulating the iterative algorithms, which are commonly employed in traditional machine learning methods. To address this, we propose the utilization of looped transformer architecture and its associated training methodology, with the aim of incorporating iterative characteristics into the transformer architectures. Experimental results suggest that the looped transformer achieves performance comparable to the standard transformer in solving various data-fitting problems, while utilizing less than 10% of the parameter count.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#26694;&#26550;&#65292;&#35780;&#20272;&#31227;&#21160;&#22240;&#32032;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;&#20135;&#29983;&#20855;&#26377;&#19981;&#21516;&#31227;&#21160;&#34892;&#20026;&#30340;&#20301;&#32622;&#24207;&#21015;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#39044;&#27979;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2311.11749</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#25581;&#31034;&#34892;&#20026;&#23545;&#31227;&#21160;&#39044;&#27979;&#32593;&#32476;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Revealing behavioral impact on mobility prediction networks through causal interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11749
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#26694;&#26550;&#65292;&#35780;&#20272;&#31227;&#21160;&#22240;&#32032;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;&#20135;&#29983;&#20855;&#26377;&#19981;&#21516;&#31227;&#21160;&#34892;&#20026;&#30340;&#20301;&#32622;&#24207;&#21015;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#39044;&#27979;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#31227;&#21160;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#34987;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#36816;&#20316;&#26041;&#24335;&#32473;&#21487;&#35299;&#37322;&#24615;&#24102;&#26469;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#21508;&#31181;&#31227;&#21160;&#34892;&#20026;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#22240;&#26524;&#24178;&#39044;&#26694;&#26550;&#65292;&#35780;&#20272;&#19982;&#19979;&#19968;&#20010;&#20301;&#32622;&#39044;&#27979;&#30456;&#20851;&#30340;&#31227;&#21160;&#22240;&#32032;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709; -- &#36825;&#26159;&#19968;&#39033;&#19987;&#27880;&#20110;&#39044;&#27979;&#20010;&#20307;&#21363;&#23558;&#21040;&#36798;&#20301;&#32622;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#20010;&#20307;&#31227;&#21160;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#20301;&#32622;&#35775;&#38382;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#24178;&#39044;&#23427;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26469;&#25511;&#21046;&#34892;&#20026;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#31227;&#21160;&#25351;&#26631;&#35780;&#20272;&#24178;&#39044;&#20301;&#32622;&#24207;&#21015;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#35757;&#32451;&#33391;&#22909;&#30340;&#32593;&#32476;&#20013;&#20197;&#20998;&#26512;&#24615;&#33021;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20135;&#29983;&#20855;&#26377;&#19981;&#21516;&#31227;&#21160;&#34892;&#20026;&#30340;&#20301;&#32622;&#24207;&#21015;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#39044;&#27979;&#32593;&#32476;&#30340;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11749v2 Announce Type: replace-cross  Abstract: Deep neural networks are increasingly utilized in mobility prediction tasks, yet their intricate internal workings pose challenges for interpretability, especially in comprehending how various aspects of mobility behavior affect predictions. This study introduces a causal intervention framework to assess the impact of mobility-related factors on neural networks designed for next location prediction -- a task focusing on predicting the immediate next location of an individual. To achieve this, we employ individual mobility models to generate synthetic location visit sequences and control behavior dynamics by intervening in their data generation process. We evaluate the interventional location sequences using mobility metrics and input them into well-trained networks to analyze performance variations. The results demonstrate the effectiveness in producing location sequences with distinct mobility behaviors, thereby facilitating t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20363;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;MILLET&#65292;&#20351;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;&#21464;&#24471;&#20869;&#22312;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#29978;&#33267;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2311.10049</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20363;&#23398;&#20064;&#23454;&#29616;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Inherently Interpretable Time Series Classification via Multiple Instance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10049
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20363;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;MILLET&#65292;&#20351;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;&#21464;&#24471;&#20869;&#22312;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#29978;&#33267;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#26041;&#27861;&#36890;&#24120;&#26159;&#40657;&#21283;&#23376;&#65292;&#38590;&#20197;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MILLET&#30340;&#26032;&#26694;&#26550;&#65306;&#29992;&#20110;&#26412;&#22320;&#21487;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#22810;&#20363;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;MILLET&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;TSC&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22914;&#20309;&#21464;&#24471;&#20869;&#22312;&#21487;&#35299;&#37322;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#65288;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#25552;&#39640;&#65289;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;85&#20010;UCR TSC&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;MILLET&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#26032;&#39062;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;MILLET&#33021;&#22815;&#24555;&#36895;&#20135;&#29983;&#27604;&#20854;&#20182;&#20247;&#25152;&#21608;&#30693;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26356;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10049v3 Announce Type: replace-cross  Abstract: Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/J
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#32508;&#21512;&#23457;&#26597;&#20102;LLMs&#20013;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.07914</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#21542;&#21487;&#20197;&#20943;&#23569;LLMs&#20013;&#30340;&#24187;&#35273;&#65311;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07914
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#32508;&#21512;&#23457;&#26597;&#20102;LLMs&#20013;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;LLMs&#24456;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#20027;&#35201;&#28304;&#20110;&#27169;&#22411;&#20869;&#30340;&#30693;&#35782;&#31354;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38480;&#21046;&#65292;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#21508;&#31181;&#31574;&#30053;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#65292;&#26088;&#22312;&#20943;&#23569;&#24187;&#35273;&#24182;&#25552;&#21319;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#20123;&#31574;&#30053;&#20013;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#22806;&#37096;&#20449;&#24687;&#28304;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#26597;&#20102;LLMs&#20013;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#31995;&#32479;&#22320;&#24402;&#31867;&#20026;&#19977;&#20010;&#24635;&#20307;&#32452;&#65292;&#25552;&#20379;&#26041;&#27861;&#27604;&#36739;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#19982;&#36825;&#20123;&#25216;&#26415;&#30456;&#20851;&#30340;&#24403;&#21069;&#36235;&#21183;&#21644;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07914v2 Announce Type: replace  Abstract: The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#26500;&#24314;&#20102;&#26368;&#22823;&#21644;&#38750;&#20887;&#20313;&#30340;3D&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25968;&#25454;&#38598;PPIRef&#65292;&#25552;&#20986;&#20102;&#21487;&#27867;&#21270;&#36328;&#19981;&#21516;&#34507;&#30333;&#36136;&#32467;&#21512;&#21058;&#21464;&#20307;&#30340;PPIformer&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#28909;&#21147;&#23398;&#35843;&#25972;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#31361;&#21464;&#24433;&#21709;&#30340;&#39044;&#27979;&#65292;&#26368;&#32456;&#23637;&#31034;&#20102;&#35813;&#26032;&#26041;&#27861;&#22312;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2310.18515</link><description>&lt;p&gt;
&#23398;&#20064;&#35774;&#35745;&#20855;&#26377;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#30340;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to design protein-protein interactions with enhanced generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#26500;&#24314;&#20102;&#26368;&#22823;&#21644;&#38750;&#20887;&#20313;&#30340;3D&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25968;&#25454;&#38598;PPIRef&#65292;&#25552;&#20986;&#20102;&#21487;&#27867;&#21270;&#36328;&#19981;&#21516;&#34507;&#30333;&#36136;&#32467;&#21512;&#21058;&#21464;&#20307;&#30340;PPIformer&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#28909;&#21147;&#23398;&#35843;&#25972;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#31361;&#21464;&#24433;&#21709;&#30340;&#39044;&#27979;&#65292;&#26368;&#32456;&#23637;&#31034;&#20102;&#35813;&#26032;&#26041;&#27861;&#22312;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#22686;&#24378;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPIs&#65289;&#30340;&#31361;&#21464;&#23545;&#20110;&#25512;&#36827;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#21644;&#24320;&#21457;&#25913;&#36827;&#30340;&#27835;&#30103;&#33647;&#29289;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#23454;&#36136;&#36827;&#23637;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24448;&#24448;&#38590;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#36827;&#34892;&#27867;&#21270;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;PPIRef&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#22823;&#30340;&#38750;&#20887;&#20313;&#30340;3D&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26377;&#25928;&#36827;&#34892;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;PPIRef&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#20102;PPIformer&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;SE(3)-&#31561;&#21464;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#34507;&#30333;&#36136;&#32467;&#21512;&#21058;&#21464;&#20307;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#28909;&#21147;&#23398;&#35843;&#25972;&#65292;&#23545;PPIformer&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#39044;&#27979;&#31361;&#21464;&#23545;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26032;&#30340;&#12289;&#26410;&#30693;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#36229;&#36234;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;PPIformer&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18515v3 Announce Type: replace  Abstract: Discovering mutations enhancing protein-protein interactions (PPIs) is critical for advancing biomedical research and developing improved therapeutics. While machine learning approaches have substantially advanced the field, they often struggle to generalize beyond training data in practical scenarios. The contributions of this work are three-fold. First, we construct PPIRef, the largest and non-redundant dataset of 3D protein-protein interactions, enabling effective large-scale learning. Second, we leverage the PPIRef dataset to pre-train PPIformer, a new SE(3)-equivariant model generalizing across diverse protein-binder variants. We fine-tune PPIformer to predict effects of mutations on protein-protein interactions via a thermodynamically motivated adjustment of the pre-training loss function. Finally, we demonstrate the enhanced generalization of our new PPIformer approach by outperforming other state-of-the-art methods on new, no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#29992;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#25163;&#20381;&#36182;&#20110;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#36801;&#31227;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21338;&#24328;&#35770;&#35270;&#35282;&#30340;&#19987;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#27169;&#22411;&#21644;&#25915;&#20987;&#31639;&#27861;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2310.17645</link><description>&lt;p&gt;
PubDef&#65306;&#38450;&#24481;&#26469;&#33258;&#20844;&#20849;&#27169;&#22411;&#30340;&#36801;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PubDef: Defending Against Transfer Attacks From Public Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.17645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#29992;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#25163;&#20381;&#36182;&#20110;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#36801;&#31227;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21338;&#24328;&#35770;&#35270;&#35282;&#30340;&#19987;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#27169;&#22411;&#21644;&#25915;&#20987;&#31639;&#27861;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#19968;&#30452;&#26159;&#34892;&#19994;&#20013;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#19988;&#26410;&#35299;&#20915;&#30340;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21313;&#24180;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25991;&#29486;&#21382;&#21490;&#65292;&#25105;&#20204;&#20102;&#35299;&#21040;&#21457;&#36215;&#24378;&#22823;&#25110;&#26368;&#20248;&#25915;&#20987;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23427;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36807;&#21435;&#22823;&#22810;&#25968;&#25991;&#29486;&#22362;&#23450;&#20551;&#35774;&#30340;&#30333;&#30418;&#23041;&#32961;&#27169;&#22411;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#29992;&#23041;&#32961;&#27169;&#22411;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#23545;&#25163;&#20381;&#36182;&#20110;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#36801;&#31227;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#35774;&#32622;&#23558;&#25104;&#20026;&#26410;&#26469;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#20013;&#26368;&#26222;&#36941;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#36801;&#31227;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21338;&#24328;&#35770;&#35270;&#35282;&#30340;&#19987;&#38376;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#22312;24&#20010;&#20844;&#20849;&#27169;&#22411;&#21644;11&#31181;&#25915;&#20987;&#31639;&#27861;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#12289;CIFAR-100 &#21644; ImageNet&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.17645v2 Announce Type: replace-cross  Abstract: Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this paper, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. The defenses are evaluated under 24 public models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and ImageNet). Under thi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;softmax&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#29616;&#26377;&#23545;&#27604;&#25439;&#22833;&#22312;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25299;&#23637;&#21040;soft top-k &#25805;&#20316;&#31526;&#12290;</title><link>https://arxiv.org/abs/2310.10636</link><description>&lt;p&gt;
&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#21452;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dual-Encoders for Extreme Multi-Label Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;softmax&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#29616;&#26377;&#23545;&#27604;&#25439;&#22833;&#22312;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25299;&#23637;&#21040;soft top-k &#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#32534;&#30721;&#22120;&#65288;DE&#65289;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#26816;&#32034;&#20219;&#21153;&#65292;&#36890;&#24120;&#22312;&#24320;&#25918;&#24335;QA&#22522;&#20934;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#20197;&#22810;&#31867;&#21644;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#20026;&#29305;&#24449;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#20687;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;XMC&#65289;&#36825;&#26679;&#30340;&#22810;&#26631;&#31614;&#21644;&#25968;&#25454;&#20016;&#23500;&#30340;&#26816;&#32034;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#24403;&#21069;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;DE&#27169;&#22411;&#22312;XMC&#22522;&#20934;&#19978;&#26126;&#26174;&#19981;&#36275;&#65292;&#20854;&#20013;SOTA&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#27599;&#31867;&#20998;&#31867;&#22836;&#23558;&#21487;&#23398;&#20064;&#21442;&#25968;&#25968;&#37327;&#19982;&#24635;&#31867;&#21035;&#25968;&#65288;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26723;&#65289;&#32447;&#24615;&#25193;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#24182;&#24378;&#35843;&#29616;&#26377;&#30340;&#22810;&#26631;&#31614;&#23545;&#27604;&#35757;&#32451;&#25439;&#22833;&#19981;&#36866;&#29992;&#20110;&#22312;XMC&#20219;&#21153;&#19978;&#35757;&#32451;DE&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#32806;&#30340;softmax&#25439;&#22833; - &#36825;&#26159;&#23545;InfoNCE&#25439;&#22833;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#25439;&#22833;&#35774;&#35745;&#25193;&#23637;&#21040;&#19968;&#20010;&#36719;top-k&#25805;&#20316;&#31526;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10636v2 Announce Type: replace  Abstract: Dual-encoder (DE) models are widely used in retrieval tasks, most commonly studied on open QA benchmarks that are often characterized by multi-class and limited training data. In contrast, their performance in multi-label and data-rich retrieval settings like extreme multi-label classification (XMC), remains under-explored. Current empirical evidence indicates that DE models fall significantly short on XMC benchmarks, where SOTA methods linearly scale the number of learnable parameters with the total number of classes (documents in the corpus) by employing per-class classification head. To this end, we first study and highlight that existing multi-label contrastive training losses are not appropriate for training DE models on XMC tasks. We propose decoupled softmax loss - a simple modification to the InfoNCE loss - that overcomes the limitations of existing contrastive losses. We further extend our loss design to a soft top-k operato
&lt;/p&gt;</description></item><item><title>Bongard-OpenWorld&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#23569;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24320;&#25918;&#19990;&#30028;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20004;&#39033;&#26032;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.10207</link><description>&lt;p&gt;
Bongard-OpenWorld: &#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10207
&lt;/p&gt;
&lt;p&gt;
Bongard-OpenWorld&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#23569;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24320;&#25918;&#19990;&#30028;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20004;&#39033;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bongard-OpenWorld&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#30495;&#23454;&#19990;&#30028;&#23569;&#26679;&#26412;&#25512;&#29702;&#30340;&#26032;&#22522;&#20934;&#12290; &#23427;&#28304;&#33258;&#32463;&#20856;&#30340;Bongard&#38382;&#39064;&#65288;BPs&#65289;&#65306;&#32473;&#23450;&#20004;&#32452;&#22270;&#20687;&#65288;&#27491;&#21644;&#36127;&#65289;&#65292;&#27169;&#22411;&#38656;&#35201;&#36890;&#36807;&#35825;&#23548;&#35270;&#35273;&#27010;&#24565;&#26469;&#30830;&#23450;&#26597;&#35810;&#22270;&#20687;&#25152;&#23646;&#30340;&#22270;&#20687;&#38598;&#65292;&#36825;&#20123;&#27010;&#24565;&#20165;&#30001;&#27491;&#38598;&#20013;&#30340;&#22270;&#20687;&#25152;&#25551;&#36848;&#12290; &#25105;&#20204;&#30340;&#22522;&#20934;&#32487;&#25215;&#20102;&#21407;&#22987;BPs&#30340;&#23569;&#26679;&#26412;&#27010;&#24565;&#24402;&#32435;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#20004;&#23618;&#26032;&#25361;&#25112;&#65306;1&#65289;&#24320;&#25918;&#19990;&#30028;&#30340;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#65292;&#22240;&#20026;Bongard-OpenWorld&#20013;&#30340;&#35270;&#35273;&#27010;&#24565;&#26159;&#20174;&#24320;&#25918;&#35789;&#27719;&#34920;&#20013;&#29420;&#29305;&#32452;&#21512;&#30340;&#26415;&#35821;&#65292;&#33539;&#22260;&#20174;&#23545;&#35937;&#31867;&#21035;&#21040;&#25277;&#35937;&#35270;&#35273;&#23646;&#24615;&#21644;&#24120;&#35782;&#20107;&#23454;&#30693;&#35782;&#65307; 2&#65289;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#35768;&#22810;&#31867;&#20284;&#29289;&#20351;&#29992;&#30340;&#21512;&#25104;&#22270;&#34920;&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;Bongard-OpenWorld&#24050;&#32463;&#23545;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36828;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10207v2 Announce Type: replace  Abstract: We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We furt
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33033;&#20914;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20316;&#20026;&#23545;&#20170;&#22825;&#20027;&#23548;&#35270;&#35273;&#39046;&#22495;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#26367;&#20195;&#65292;&#25215;&#35834;&#33021;&#22815;&#23454;&#29616;&#25968;&#37327;&#32423;&#30340;&#33021;&#37327;&#25928;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2310.09692</link><description>&lt;p&gt;
&#38754;&#21521;&#19979;&#19968;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Spike-based Neuromorphic Computing for Next-Generation Computer Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09692
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20316;&#20026;&#23545;&#20170;&#22825;&#20027;&#23548;&#35270;&#35273;&#39046;&#22495;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#26367;&#20195;&#65292;&#25215;&#35834;&#33021;&#22815;&#23454;&#29616;&#25968;&#37327;&#32423;&#30340;&#33021;&#37327;&#25928;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#25215;&#35834;&#22312;&#33021;&#37327;&#25928;&#29575;&#19978;&#27604;&#20256;&#32479;&#30340;&#20911;&#183;&#35834;&#20381;&#26364;&#35745;&#31639;&#33539;&#24335;&#26377;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#12290;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#23398;&#20064;&#21644;&#27169;&#25311;&#22823;&#33041;&#21151;&#33021;&#26469;&#24320;&#21457;&#19968;&#31181;&#33258;&#36866;&#24212;&#12289;&#23481;&#38169;&#12289;&#20302;&#21344;&#22320;&#38754;&#31215;&#12289;&#24555;&#36895;&#12289;&#20302;&#33021;&#32791;&#30340;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#21019;&#26032;&#26469;&#23454;&#29616;&#65292;&#21253;&#25324;&#26448;&#26009;&#12289;&#22120;&#20214;&#12289;&#30005;&#36335;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#12290;&#30001;&#20110;&#22312;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#22240;&#25968;&#25454;&#38598;&#22686;&#22823;&#32780;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#22522;&#20110;&#33033;&#20914;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20170;&#22825;&#20027;&#23548;&#35270;&#35273;&#39046;&#22495;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#26412;&#20070;&#31456;&#33410;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#65292;&#27010;&#36848;&#20102;&#35774;&#35745;&#22534;&#26632;&#30340;&#19981;&#21516;&#23618;&#38754;&#65288;&#22120;&#20214;&#12289;&#30005;&#36335;&#21644;&#31639;&#27861;&#65289;&#20013;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#31034;&#20363;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09692v2 Announce Type: replace-cross  Abstract: Neuromorphic Computing promises orders of magnitude improvement in energy efficiency compared to traditional von Neumann computing paradigm. The goal is to develop an adaptive, fault-tolerant, low-footprint, fast, low-energy intelligent system by learning and emulating brain functionality which can be realized through innovation in different abstraction layers including material, device, circuit, architecture and algorithm. As the energy consumption in complex vision tasks keep increasing exponentially due to larger data set and resource-constrained edge devices become increasingly ubiquitous, spike-based neuromorphic computing approaches can be viable alternative to deep convolutional neural network that is dominating the vision field today. In this book chapter, we introduce neuromorphic computing, outline a few representative examples from different layers of the design stack (devices, circuits and algorithms) and conclude w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;EDMD-DL&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#24494;&#20998;&#21516;&#26102;&#30830;&#23450;&#21487;&#35266;&#27979;&#23383;&#20856;&#21644;Koopman&#31639;&#23376;&#30340;&#36817;&#20284;&#20540;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.06790</link><description>&lt;p&gt;
&#29992;&#33258;&#21160;&#24494;&#20998;&#22686;&#24378;&#25968;&#25454;&#39537;&#21160;&#21160;&#24577;&#24314;&#27169;&#20013;&#30340;&#39044;&#27979;&#33021;&#21147;&#65306;Koopman&#21644;&#31070;&#32463;ODE&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;EDMD-DL&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#24494;&#20998;&#21516;&#26102;&#30830;&#23450;&#21487;&#35266;&#27979;&#23383;&#20856;&#21644;Koopman&#31639;&#23376;&#30340;&#36817;&#20284;&#20540;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;Koopman&#31639;&#23376;&#36924;&#36817;&#22312;&#39044;&#27979;&#20855;&#26377;&#22797;&#26434;&#21160;&#24577;&#29305;&#24449;&#30340;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#34987;&#31216;&#20026;&#25193;&#23637;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;&#19982;&#23383;&#20856;&#23398;&#20064;&#65288;EDMD-DL&#65289;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;EDMD-DL&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#23450;&#21487;&#35266;&#27979;&#23383;&#20856;&#21644;&#23545;&#24212;&#30340;Koopman&#31639;&#23376;&#30340;&#36817;&#20284;&#20540;&#12290;&#35813;&#21019;&#26032;&#21033;&#29992;&#33258;&#21160;&#24494;&#20998;&#26469;&#36890;&#36807;&#20266;&#36870;&#31616;&#21270;&#26799;&#24230;&#19979;&#38477;&#35745;&#31639;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#8220;&#32431;&#8221;Koopman&#26041;&#27861;&#65292;&#23427;&#28041;&#21450;&#22312;&#21487;&#35266;&#27979;&#31354;&#38388;&#20869;&#31649;&#29702;&#21160;&#24577;&#30340;&#32447;&#24615;&#12289;&#39640;&#32500;&#31995;&#32479;&#30340;&#30452;&#25509;&#26102;&#38388;&#31215;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31995;&#32479;&#22312;&#29366;&#24577;&#31354;&#38388;&#21644;&#21487;&#35266;&#27979;&#31354;&#38388;&#20043;&#38388;&#20132;&#26367;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06790v2 Announce Type: replace  Abstract: Data-driven approximations of the Koopman operator are promising for predicting the time evolution of systems characterized by complex dynamics. Among these methods, the approach known as extended dynamic mode decomposition with dictionary learning (EDMD-DL) has garnered significant attention. Here we present a modification of EDMD-DL that concurrently determines both the dictionary of observables and the corresponding approximation of the Koopman operator. This innovation leverages automatic differentiation to facilitate gradient descent computations through the pseudoinverse. We also address the performance of several alternative methodologies. We assess a 'pure' Koopman approach, which involves the direct time-integration of a linear, high-dimensional system governing the dynamics within the space of observables. Additionally, we explore a modified approach where the system alternates between spaces of states and observables at ea
&lt;/p&gt;</description></item><item><title>Robust-GBDT&#32467;&#21512;&#20102;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.05067</link><description>&lt;p&gt;
Robust-GBDT: &#22312;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#29992;&#38750;&#20984;&#25439;&#22833;&#36827;&#34892;&#34920;&#26684;&#20998;&#31867;&#30340;GBDT
&lt;/p&gt;
&lt;p&gt;
Robust-GBDT: GBDT with Nonconvex Loss for Tabular Classification in the Presence of Label Noise and Class Imbalance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05067
&lt;/p&gt;
&lt;p&gt;
Robust-GBDT&#32467;&#21512;&#20102;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#34920;&#26684;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#24378;&#21270;&#25552;&#21319;&#26041;&#27861;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#22810;&#31867;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#20540;&#21644;&#35745;&#31639;&#25928;&#29575;&#31561;&#38382;&#39064;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Robust-GBDT&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#30340;&#24378;&#22823;&#33021;&#21147;&#19982;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;&#30340;&#38887;&#24615;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#29305;&#23450;&#21306;&#22495;&#20869;&#30340;&#23616;&#37096;&#20984;&#24615;&#65292;Robust-GBDT&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31283;&#20581;&#24615;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#26234;&#24935;&#12290;&#36890;&#36807;&#23558;&#20808;&#36827;&#30340;GBDT&#19982;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;Robust Focal Loss&#36827;&#34892;&#26080;&#32541;&#38598;&#25104;&#65292;Robust-GBDT&#26174;&#33879;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#22024;&#26434;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05067v2 Announce Type: replace  Abstract: Dealing with label noise in tabular classification tasks poses a persistent challenge in machine learning. While robust boosting methods have shown promise in binary classification, their effectiveness in complex, multi-class scenarios is often limited. Additionally, issues like imbalanced datasets, missing values, and computational inefficiencies further complicate their practical utility. This study introduces Robust-GBDT, a groundbreaking approach that combines the power of Gradient Boosted Decision Trees (GBDT) with the resilience of nonconvex loss functions against label noise. By leveraging local convexity within specific regions, Robust-GBDT demonstrates unprecedented robustness, challenging conventional wisdom. Through seamless integration of advanced GBDT with a novel Robust Focal Loss tailored for class imbalance, Robust-GBDT significantly enhances generalization capabilities, particularly in noisy and imbalanced datasets. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23545;&#25239;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20004;&#31181;&#26426;&#21046;&#65292;&#21363;&#31867;&#20869;&#23545;&#27604;&#21644;&#31867;&#38388;&#29305;&#24449;&#20849;&#20139;&#65292;&#36825;&#26377;&#21161;&#20110;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#29305;&#24449;&#24178;&#25200;&#26680;&#24515;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2310.04971</link><description>&lt;p&gt;
&#29702;&#35299;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23545;&#25239;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20004;&#31181;&#26426;&#21046;&#65292;&#21363;&#31867;&#20869;&#23545;&#27604;&#21644;&#31867;&#38388;&#29305;&#24449;&#20849;&#20139;&#65292;&#36825;&#26377;&#21161;&#20110;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#29305;&#24449;&#24178;&#25200;&#26680;&#24515;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;CLIP&#20043;&#31867;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65288;MMCL&#65289;&#26041;&#27861;&#22312;&#23398;&#20064;&#23545;&#25239;&#20998;&#24067;&#36716;&#31227;&#19988;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#30340;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#23613;&#31649;&#22312;&#23454;&#35777;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23398;&#20064;&#36825;&#31181;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#34920;&#31034;&#32972;&#21518;&#30340;&#26426;&#29702;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#38382;&#39064;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;MMCL&#40065;&#26834;&#24615;&#32972;&#21518;&#30340;&#20004;&#31181;&#26426;&#21046;&#65306;\emph{&#31867;&#20869;&#23545;&#27604;}&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#39640;&#26041;&#24046;&#30340;&#29305;&#24449;&#65307;\emph{&#31867;&#38388;&#29305;&#24449;&#20849;&#20139;}&#65292;&#20854;&#20013;&#19968;&#20010;&#31867;&#21035;&#20013;&#30340;&#27880;&#37322;&#32454;&#33410;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#23398;&#20064;&#20854;&#20182;&#31867;&#21035;&#12290;&#36825;&#20004;&#31181;&#26426;&#21046;&#38450;&#27490;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#24230;&#34920;&#31034;&#30340;&#34394;&#20551;&#29305;&#24449;&#25513;&#30422;&#27867;&#21270;&#24615;&#26680;&#24515;&#29305;&#24449;&#12290;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#65292;&#36825;&#20135;&#29983;&#20102;&#26356;&#20248;&#31168;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20351;&#29992;&#20016;&#23500;&#23383;&#24149;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04971v2 Announce Type: replace  Abstract: Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and uncover two mechanisms behind MMCL's robustness: \emph{intra-class contrasting}, which allows the model to learn features with a high variance, and \emph{inter-class feature sharing}, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#40784;&#25490;&#21015;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#25972;&#29702;&#29289;&#21697;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04566</link><description>&lt;p&gt;
Knolling Bot: &#20174;&#25972;&#27905;&#30340;&#31034;&#33539;&#20013;&#23398;&#20064;&#26426;&#22120;&#20154;&#23545;&#35937;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#40784;&#25490;&#21015;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#25972;&#29702;&#29289;&#21697;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#22336;&#65306;arXiv:2310.04566v2  &#20844;&#21578;&#31867;&#22411;&#65306;replace-cross  &#25688;&#35201;&#65306;&#35299;&#20915;&#23478;&#24237;&#31354;&#38388;&#20013;&#25955;&#20081;&#29289;&#21697;&#30340;&#25972;&#29702;&#25361;&#25112;&#21463;&#21040;&#25972;&#27905;&#24615;&#30340;&#22810;&#26679;&#24615;&#21644;&#20027;&#35266;&#24615;&#30340;&#22797;&#26434;&#24615;&#24433;&#21709;&#12290;&#27491;&#22914;&#20154;&#31867;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#20801;&#35768;&#21516;&#19968;&#29702;&#24565;&#30340;&#22810;&#31181;&#34920;&#36798;&#19968;&#26679;&#65292;&#23478;&#24237;&#25972;&#27905;&#20559;&#22909;&#21644;&#32452;&#32455;&#27169;&#24335;&#21464;&#21270;&#24191;&#27867;&#65292;&#22240;&#27492;&#39044;&#35774;&#29289;&#20307;&#20301;&#32622;&#23558;&#38480;&#21046;&#23545;&#26032;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#27905;&#24067;&#23616;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#31867;&#20284;&#20110;&#20351;&#29992;&#20250;&#35805;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;Transformer&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#21518;&#32493;&#29289;&#20307;&#30340;&#25670;&#25918;&#20301;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#8220;&#25972;&#29702;&#8221;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#26800;&#33218;&#21644;RGB&#30456;&#26426;&#22312;&#26700;&#23376;&#19978;&#32452;&#32455;&#19981;&#21516;&#22823;&#23567;&#21644;&#25968;&#37327;&#30340;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04566v2 Announce Type: replace-cross  Abstract: Addressing the challenge of organizing scattered items in domestic spaces is complicated by the diversity and subjective nature of tidiness. Just as the complexity of human language allows for multiple expressions of the same idea, household tidiness preferences and organizational patterns vary widely, so presetting object locations would limit the adaptability to new objects and environments. Inspired by advancements in natural language processing (NLP), this paper introduces a self-supervised learning framework that allows robots to understand and replicate the concept of tidiness from demonstrations of well-organized layouts, akin to using conversational datasets to train Large Language Models(LLM). We leverage a transformer neural network to predict the placement of subsequent objects. We demonstrate a ``knolling'' system with a robotic arm and an RGB camera to organize items of varying sizes and quantities on a table. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;LLM&#26041;&#27861;&#23545;&#31264;&#23494;LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21387;&#32553;LLM&#22522;&#20934;&#26469;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2310.01382</link><description>&lt;p&gt;
&#21387;&#32553;LLM&#65306;&#30495;&#30456;&#24456;&#23569;&#32431;&#31929;&#65292;&#20063;&#32477;&#19981;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Compressing LLMs: The Truth is Rarely Pure and Never Simple
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;LLM&#26041;&#27861;&#23545;&#31264;&#23494;LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21387;&#32553;LLM&#22522;&#20934;&#26469;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#20294;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290; &#26368;&#36817;&#65292;&#20960;&#39033;&#24037;&#20316;&#26174;&#31034;&#20986;&#22312;&#26080;&#38656;&#35757;&#32451;&#21644;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;LLMs&#36827;&#34892;&#21387;&#32553;&#65288;&#20462;&#21098;&#21644;&#37327;&#21270;&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#36798;&#21040;&#20102;50-60&#65285;&#30340;&#31232;&#30095;&#24230;&#65292;&#24182;&#23558;&#20301;&#23485;&#20943;&#23567;&#21040;&#27599;&#20010;&#26435;&#37325;3&#25110;4&#20301;&#65292;&#24182;&#19988;&#19982;&#26410;&#21387;&#32553;&#22522;&#32447;&#30456;&#27604;&#65292;&#22256;&#24785;&#24230;&#30340;&#38477;&#20302;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290; &#38543;&#30528;&#26368;&#36817;&#30740;&#31350;&#24037;&#20316;&#38598;&#20013;&#22312;&#24320;&#21457;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#21387;&#32553;&#26041;&#27861;&#19978;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#36864;&#19968;&#27493;&#37325;&#26032;&#35780;&#20272;&#20102;&#29616;&#26377;SoTA&#21387;&#32553;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#30456;&#24403;&#31616;&#21333;&#19988;&#24191;&#21463;&#36136;&#30097;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22256;&#24785;&#24230;&#65288;&#21363;&#20351;&#23545;&#20110;&#31264;&#23494;&#30340;LLMs&#65289;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#30693;&#35782;&#23494;&#38598;&#22411;&#21387;&#32553;&#30340;LLM&#22522;&#20934;&#65288;LLM-KICK&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#20219;&#21153;&#38598;&#21512;&#65292;&#29992;&#20110;&#37325;&#26032;&#23450;&#20041;&#23545;&#21387;&#32553;LLMs&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36825;&#20123;LLMs&#19982;&#20854;&#31264;&#23494;&#23545;&#24212;&#29289;&#26377;&#26174;&#33879;&#30340;&#23545;&#40784;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01382v2 Announce Type: replace  Abstract: Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#35266;&#28857;&#19981;&#21516;&#65292;&#22312;&#38754;&#23545;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#36755;&#20837;&#26102;&#65292;&#39640;&#32500;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24448;&#24448;&#20250;&#36235;&#21521;&#20110;&#19968;&#20010;&#24658;&#23450;&#20540;&#65292;&#19988;&#36825;&#20010;&#20540;&#36890;&#24120;&#33021;&#22815;&#25509;&#36817;&#26368;&#20248;&#24658;&#23450;&#35299;&#12290;</title><link>https://arxiv.org/abs/2310.00873</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#21487;&#39044;&#27979;&#30340;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks Tend To Extrapolate Predictably
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00873
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#35266;&#28857;&#19981;&#21516;&#65292;&#22312;&#38754;&#23545;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#36755;&#20837;&#26102;&#65292;&#39640;&#32500;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24448;&#24448;&#20250;&#36235;&#21521;&#20110;&#19968;&#20010;&#24658;&#23450;&#20540;&#65292;&#19988;&#36825;&#20010;&#20540;&#36890;&#24120;&#33021;&#22815;&#25509;&#36817;&#26368;&#20248;&#24658;&#23450;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#30340;&#36755;&#20837;&#26102;&#65292;&#39044;&#27979;&#24448;&#24448;&#26159;&#19981;&#21487;&#39044;&#27979;&#21644;&#36807;&#20110;&#33258;&#20449;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#26032;&#35780;&#20272;&#20102;&#20855;&#26377;&#39640;&#32500;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#36825;&#19968;&#20551;&#35774;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19982;&#20197;&#24448;&#35748;&#20026;&#30340;&#20219;&#24847;&#22806;&#25512;&#19981;&#21516;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24448;&#24448;&#22312;&#36755;&#20837;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;OOD&#26102;&#36235;&#21521;&#20110;&#19968;&#20010;&#24658;&#23450;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#20540;&#24448;&#24448;&#33021;&#22815;&#32039;&#23494;&#22320;&#36924;&#36817;&#26368;&#20248;&#24658;&#23450;&#35299;&#65288;OCS&#65289;&#65292;&#21363;&#22312;&#19981;&#35266;&#23519;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#24179;&#22343;&#25439;&#22833;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21253;&#25324;CIFAR10-C&#12289;ImageNet-R&#12289;S&#22312;&#20869;&#30340;8&#20010;&#25968;&#25454;&#38598;&#19978;&#20197;&#21450;&#19981;&#21516;&#20998;&#24067;&#36716;&#31227;&#65288;&#21253;&#25324;&#20132;&#21449;&#29109;&#12289;MSE&#21644;&#39640;&#26031;NLL&#65289;&#21644;&#19981;&#21516;&#26550;&#26500;&#65288;CNN&#21644;&#21464;&#21387;&#22120;&#65289;&#19979;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#34892;&#20026;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#65292;&#24182;&#39318;&#27425;&#39564;&#35777;&#20102;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00873v2 Announce Type: replace  Abstract: Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LVLM Hallucination Revisor&#65288;LURE&#65289;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#36739;&#23569;&#20855;&#26377;&#24187;&#35273;&#24615;&#30340;&#25551;&#36848;&#65292;&#26469;&#20107;&#21518;&#32416;&#27491;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.00754</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#20943;&#36731;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Mitigating Object Hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LVLM Hallucination Revisor&#65288;LURE&#65289;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#36739;&#23569;&#20855;&#26377;&#24187;&#35273;&#24615;&#30340;&#25551;&#36848;&#65292;&#26469;&#20107;&#21518;&#32416;&#27491;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#29702;&#35299;&#22270;&#20687;&#20449;&#24687;&#21644;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LVLMs &#20173;&#28982;&#23384;&#22312;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#21253;&#21547;&#22270;&#20687;&#20013;&#23454;&#38469;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#25551;&#36848;&#12290;&#36825;&#21487;&#33021;&#23545;&#35768;&#22810;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#22914;&#35270;&#35273;&#24635;&#32467;&#21644;&#25512;&#29702;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#31639;&#27861;&#65292;LVLM &#24187;&#35273;&#20462;&#27491;&#22120;&#65288;LURE&#65289;&#65292;&#36890;&#36807;&#37325;&#26500;&#36739;&#23569;&#20855;&#26377;&#24187;&#35273;&#24615;&#30340;&#25551;&#36848;&#26469;&#20107;&#21518;&#32416;&#27491;LVLM &#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;LURE&#26681;&#25454;&#23545;&#23548;&#33268;&#29289;&#20307;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#30340;&#20005;&#26684;&#32479;&#35745;&#20998;&#26512;&#65292;&#21253;&#25324;&#20849;&#29616;&#65288;&#22270;&#20687;&#20013;&#26576;&#20123;&#23545;&#35937;&#32463;&#24120;&#19982;&#20854;&#20182;&#23545;&#35937;&#19968;&#36215;&#20986;&#29616;&#65289;&#12289;&#19981;&#30830;&#23450;&#24615;&#65288;&#22312;LVLM&#35299;&#30721;&#36807;&#31243;&#20013;&#19981;&#30830;&#23450;&#24615;&#36739;&#39640;&#30340;&#23545;&#35937;&#65289;&#21644;&#23545;&#35937;&#20301;&#32622;&#65288;&#24187;&#35273;&#36890;&#24120;&#20986;&#29616;&#22312;&#29983;&#25104;&#25551;&#36848;&#30340;&#21518;&#37096;&#20998;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00754v2 Announce Type: replace-cross  Abstract: Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the gen
&lt;/p&gt;</description></item><item><title>LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2309.13788</link><description>&lt;p&gt;
&#33021;&#22815;&#26816;&#27979;&#21040;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLM-Generated Misinformation Be Detected?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13788
&lt;/p&gt;
&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;LLMs&#65288;&#22914;ChatGPT&#65289;&#21487;&#33021;&#34987;&#21033;&#29992;&#26469;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#32473;&#22312;&#32447;&#23433;&#20840;&#21644;&#20844;&#20247;&#20449;&#20219;&#24102;&#26469;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#20250;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#36896;&#25104;&#26356;&#22823;&#21361;&#23475;?&#25105;&#20204;&#25552;&#20986;&#20174;&#26816;&#27979;&#38590;&#24230;&#30340;&#35282;&#24230;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21033;&#29992;LLMs&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#28508;&#22312;&#30495;&#23454;&#19990;&#30028;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#30456;&#27604;&#65292;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#23545;&#20154;&#31867;&#21644;&#26816;&#27979;&#22120;&#26469;&#35828;&#26356;&#38590;&#26816;&#27979;&#65292;&#36825;&#34920;&#26126;&#23427;&#21487;&#33021;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#28508;&#22312;&#22320;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;$\mathcal{L}_\alpha$-GAN&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Jensen-$f_\alpha$-&#25955;&#24230;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#25991;&#29486;&#20013;&#30340;&#22810;&#20010;GAN&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.07233</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#32479;&#19968;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Unifying Generator Loss Function for Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07233
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;$\mathcal{L}_\alpha$-GAN&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Jensen-$f_\alpha$-&#25955;&#24230;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#25991;&#29486;&#20013;&#30340;&#22810;&#20010;GAN&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;$\alpha$&#21442;&#25968;&#21270;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#21452;&#30446;&#26631;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#32463;&#20856;&#37492;&#21035;&#22120;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#21407;&#22987;GAN&#65288;VanillaGAN&#65289;&#31995;&#32479;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#23545;&#31216;&#31867;&#27010;&#29575;&#20272;&#35745;&#31867;&#22411;&#20989;&#25968;$\mathcal{L}_\alpha$&#65292;&#24471;&#21040;&#30340;GAN&#31995;&#32479;&#34987;&#31216;&#20026;$\mathcal{L}_\alpha$-GAN&#12290;&#22312;&#26368;&#20339;&#37492;&#21035;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#26126;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#38382;&#39064;&#21253;&#25324;&#26368;&#23567;&#21270;Jensen-$f_\alpha$-&#25955;&#24230;&#65292;&#36825;&#26159;Jensen-Shannon&#25955;&#24230;&#30340;&#33258;&#28982;&#27867;&#21270;&#65292;&#20854;&#20013;$f_\alpha$&#26159;&#20851;&#20110;&#25439;&#22833;&#20989;&#25968;$\mathcal{L}_\alpha$&#30340;&#20984;&#20989;&#25968;&#12290;&#36824;&#35777;&#26126;&#20102;&#36825;&#20010;$\mathcal{L}_\alpha$-GAN&#38382;&#39064;&#24674;&#22797;&#20102;&#25991;&#29486;&#20013;&#19968;&#20123;GAN&#38382;&#39064;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#21253;&#25324;VanillaGAN&#12289;&#26368;&#23567;&#20108;&#20056;GAN&#65288;LSGAN&#65289;&#12289;&#26368;&#23567;$k$&#38454;GAN&#65288;L
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07233v2 Announce Type: replace  Abstract: A unifying $\alpha$-parametrized generator loss function is introduced for a dual-objective generative adversarial network (GAN), which uses a canonical (or classical) discriminator loss function such as the one in the original GAN (VanillaGAN) system. The generator loss function is based on a symmetric class probability estimation type function, $\mathcal{L}_\alpha$, and the resulting GAN system is termed $\mathcal{L}_\alpha$-GAN. Under an optimal discriminator, it is shown that the generator's optimization problem consists of minimizing a Jensen-$f_\alpha$-divergence, a natural generalization of the Jensen-Shannon divergence, where $f_\alpha$ is a convex function expressed in terms of the loss function $\mathcal{L}_\alpha$. It is also demonstrated that this $\mathcal{L}_\alpha$-GAN problem recovers as special cases a number of GAN problems in the literature, including VanillaGAN, Least Squares GAN (LSGAN), Least $k$th order GAN (L$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20855;&#26377;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27748;&#26222;&#26862;&#37319;&#26679;&#21644;&#21160;&#24577;&#22823;&#23567;&#29255;&#27573;&#30340;&#31639;&#27861;&#36827;&#34892;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2306.02574</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20855;&#26377;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27748;&#26222;&#26862;&#37319;&#26679;&#21644;&#21160;&#24577;&#22823;&#23567;&#29255;&#27573;&#30340;&#31639;&#27861;&#36827;&#34892;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#29616;&#23454;&#24212;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;&#36890;&#20449;&#32593;&#32476;&#25110;&#35745;&#31639;&#31995;&#32479;&#30340;&#25490;&#38431;&#27169;&#22411;&#65292;&#37117;&#20855;&#26377;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#12290;&#30446;&#21069;&#24050;&#32463;&#24320;&#21457;&#30340;&#31639;&#27861;&#21644;&#23398;&#20064;&#36807;&#31243;&#20027;&#35201;&#38024;&#23545;&#26377;&#38480;&#29366;&#24577;&#35774;&#32622;&#65292;&#24182;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#20123;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#26410;&#30693;&#21442;&#25968;&#952;&#8712;&#920;&#25511;&#21046;&#19979;&#30340;&#23478;&#26063;&#31163;&#25955;&#26102;&#38388;&#21487;&#25968;&#29366;&#24577;&#31354;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#35813;&#36807;&#31243;&#23450;&#20041;&#22312;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;X&#965;={Z+}d&#19978;&#65292;&#20855;&#26377;&#26377;&#38480;&#21160;&#20316;&#31354;&#38388;A&#965;&#20197;&#21450;&#26080;&#30028;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#35266;&#28857;&#65292;&#23558;&#38543;&#26426;&#26410;&#30693;&#21442;&#25968;&#952;*&#20316;&#20026;&#32473;&#23450;&#20808;&#39564;&#20998;&#24067;&#22312;&#920;&#19978;&#29983;&#25104;&#12290;&#20026;&#20102;&#26368;&#20248;&#22320;&#25511;&#21046;&#26410;&#30693;&#30340;MDP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27748;&#26222;&#26862;&#37319;&#26679;&#21644;&#21160;&#24577;&#22823;&#23567;&#29255;&#27573;&#30340;&#31639;&#27861;&#65306;&#22312;&#27599;&#20010;&#29255;&#27573;&#30340;&#24320;&#22987;&#65292;&#26681;&#25454;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#36873;&#25321;&#21160;&#20316;&#65292;&#24182;&#22312;&#27599;&#20010;&#29255;&#27573;&#32467;&#26463;&#26102;&#26356;&#26032;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of many real-life applications, such as queuing models of communication networks or computing systems, have a countably infinite state-space. Algorithmic and learning procedures that have been developed to produce optimal policies mainly focus on finite state settings, and do not directly apply to these models. To overcome this lacuna, in this work we study the problem of optimal control of a family of discrete-time countable state-space Markov Decision Processes (MDPs) governed by an unknown parameter $\theta\in\Theta$, and defined on a countably-infinite state space $\mathcal X=\mathbb{Z}_+^d$, with finite action space $\mathcal A$, and an unbounded cost function. We take a Bayesian perspective with the random unknown parameter $\boldsymbol{\theta}^*$ generated via a given fixed prior distribution on $\Theta$. To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes: at the beginning of each episode, the posterior
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Langevin Monte Carlo&#30452;&#25509;&#37319;&#26679;Q&#20989;&#25968;&#21518;&#39564;&#20998;&#24067;&#65292;&#36991;&#20813;&#20102;&#39640;&#26031;&#36924;&#36817;&#65292;&#23454;&#29616;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#39640;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2305.18246</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#19988;&#23454;&#29992;&#65306;&#36890;&#36807;Langevin Monte Carlo&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.18246
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Langevin Monte Carlo&#30452;&#25509;&#37319;&#26679;Q&#20989;&#25968;&#21518;&#39564;&#20998;&#24067;&#65292;&#36991;&#20813;&#20102;&#39640;&#26031;&#36924;&#36817;&#65292;&#23454;&#29616;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#39640;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Thompson&#37319;&#26679;&#30340;&#21487;&#20280;&#32553;&#19988;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#31574;&#30053;&#12290;&#29616;&#26377;Thompson&#37319;&#26679;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#38656;&#35201;&#23545;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#39640;&#26031;&#36924;&#36817;&#65292;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#22330;&#26223;&#20013;&#24182;&#19981;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;Langevin Monte Carlo&#30452;&#25509;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;Q&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#25191;&#34892;&#24102;&#22122;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#23601;&#21487;&#20197;&#23398;&#20064;Q&#20989;&#25968;&#30340;&#31934;&#30830;&#21518;&#39564;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26131;&#20110;&#37096;&#32626;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#22312;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(linear MDP)&#35774;&#32622;&#20013;&#65292;&#23427;&#30340;&#36951;&#25022;&#30028;&#32422;&#20026;$\tilde{O}(d^{3/2}H^{3/2}\sqrt{T})$&#65292;&#20854;&#20013; $d$ &#26159;&#29305;&#24449;&#26144;&#23556;&#30340;&#32500;&#24230;&#65292;$H$ &#26159;&#35745;&#21010;&#30340;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.18246v2 Announce Type: replace  Abstract: We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\tilde{O}(d^{3/2}H^{3/2}\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the plannin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;</title><link>https://arxiv.org/abs/2305.16877</link><description>&lt;p&gt;
&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning with Dual Expectile-Quantile Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16877
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36817;&#20284;&#25972;&#20010;&#22238;&#25253;&#20998;&#24067;&#65292;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#29615;&#22659;&#26679;&#26412;&#12290;&#24120;&#29992;&#30340;&#22522;&#20110;&#19981;&#23545;&#31216;$L_1$&#25439;&#22833;&#30340;&#20998;&#24067;&#24335;RL&#30340;&#20998;&#20301;&#22238;&#24402;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;&#30340;&#26041;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#25928;&#30340;&#28151;&#21512;&#19981;&#23545;&#31216;$L_1$-$L_2$ Huber&#25439;&#22833;&#26469;&#25913;&#36827;&#24448;&#24448;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#20998;&#24067;&#20272;&#35745;&#20445;&#35777;&#28040;&#22833;&#20102;&#65292;&#25105;&#20204;&#23454;&#35777;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#20998;&#24067;&#20250;&#36805;&#36895;&#25910;&#25947;&#21040;&#20854;&#22343;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#26399;&#26395;&#22238;&#24402;&#30456;&#23545;&#24212;&#30340;&#19981;&#23545;&#31216;$L_2$&#25439;&#22833;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#12290;&#21463;&#21040;$L_2$&#20026;&#22522;&#30784;&#23398;&#20064;&#25928;&#29575;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#21644;&#20998;&#20301;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16877v2 Announce Type: replace-cross  Abstract: Distributional reinforcement learning (RL) has proven useful in multiple benchmarks as it enables approximating the full distribution of returns and makes a better use of environment samples. The commonly used quantile regression approach to distributional RL -- based on asymmetric $L_1$ losses -- provides a flexible and effective way of learning arbitrary return distributions. In practice, it is often improved by using a more efficient, hybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by doing so, distributional estimation guarantees vanish, and we empirically observe that the estimated distribution rapidly collapses to its mean. Indeed, asymmetric $L_2$ losses, corresponding to expectile regression, cannot be readily used for distributional temporal difference learning. Motivated by the efficiency of $L_2$-based learning, we propose to jointly learn expectiles and quantiles of the return distribution
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31726;&#29699;&#27963;&#21160;&#35782;&#21035;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35760;&#24405;&#20102;&#26469;&#33258;&#32654;&#22269;&#21644;&#24503;&#22269;&#20004;&#25903;&#31726;&#29699;&#38431;&#30340;24&#21517;&#29699;&#21592;&#22312;&#35757;&#32451;&#21644;&#27604;&#36187;&#20013;&#20329;&#25140;&#33109;&#37096;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#19981;&#21516;&#22269;&#23478;&#25991;&#21270;&#24046;&#24322;&#21644;&#21442;&#19982;&#32773;&#31726;&#29699;&#32463;&#39564;&#24322;&#36136;&#24615;&#31561;&#29305;&#28857;</title><link>https://arxiv.org/abs/2305.13124</link><description>&lt;p&gt;
Hang-Time HAR&#65306;&#19968;&#31181;&#20351;&#29992;&#33109;&#37096;&#24815;&#24615;&#20256;&#24863;&#22120;&#36827;&#34892;&#31726;&#29699;&#27963;&#21160;&#35782;&#21035;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Hang-Time HAR: A Benchmark Dataset for Basketball Activity Recognition using Wrist-Worn Inertial Sensors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13124
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31726;&#29699;&#27963;&#21160;&#35782;&#21035;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35760;&#24405;&#20102;&#26469;&#33258;&#32654;&#22269;&#21644;&#24503;&#22269;&#20004;&#25903;&#31726;&#29699;&#38431;&#30340;24&#21517;&#29699;&#21592;&#22312;&#35757;&#32451;&#21644;&#27604;&#36187;&#20013;&#20329;&#25140;&#33109;&#37096;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#19981;&#21516;&#22269;&#23478;&#25991;&#21270;&#24046;&#24322;&#21644;&#21442;&#19982;&#32773;&#31726;&#29699;&#32463;&#39564;&#24322;&#36136;&#24615;&#31561;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#33109;&#37096;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#20307;&#32946;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#31726;&#29699;&#35757;&#32451;&#12289;&#32451;&#20064;&#21644;&#27604;&#36187;&#12290;&#31726;&#29699;&#27963;&#21160;&#38750;&#24120;&#36866;&#21512;&#30001;&#33109;&#37096;&#24815;&#24615;&#20256;&#24863;&#22120;&#36827;&#34892;&#27979;&#37327;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#36825;&#31181;&#19982;&#36816;&#21160;&#30456;&#20851;&#30340;&#27963;&#21160;&#30340;&#31995;&#32479;&#21487;&#24212;&#29992;&#20110;&#27604;&#36187;&#20998;&#26512;&#12289;&#25351;&#23548;&#35757;&#32451;&#21644;&#20010;&#20154;&#20307;&#32946;&#27963;&#21160;&#36319;&#36394;&#12290;&#35813;&#25968;&#25454;&#38598;&#35760;&#24405;&#20102;&#26469;&#33258;&#20004;&#20010;&#19981;&#21516;&#22269;&#23478;&#65288;&#32654;&#22269;&#21644;&#24503;&#22269;&#65289;&#30340;&#20004;&#25903;&#22242;&#38431;&#20849;24&#21517;&#36816;&#21160;&#21592;&#65292;&#22312;&#37325;&#22797;&#30340;&#31726;&#29699;&#35757;&#32451;&#35838;&#21644;&#23436;&#25972;&#27604;&#36187;&#26399;&#38388;&#20182;&#20204;&#20329;&#25140;&#20102;&#33109;&#37096;&#24815;&#24615;&#20256;&#24863;&#22120;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#29305;&#28857;&#21253;&#25324;&#25968;&#25454;&#22312;&#20004;&#20010;&#22269;&#23478;&#35760;&#24405;&#65292;&#30001;&#20110;&#25991;&#21270;&#24046;&#24322;&#22312;&#28216;&#25103;&#35268;&#21017;&#21644;&#39118;&#26684;&#20013;&#22266;&#26377;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#36816;&#21160;&#25216;&#33021;&#27700;&#24179;&#65292;&#22240;&#20026;&#21442;&#19982;&#32773;&#22312;&#31726;&#29699;&#32463;&#39564;&#26041;&#38754;&#26159;&#24322;&#36136;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13124v2 Announce Type: replace  Abstract: We present a benchmark dataset for evaluating physical human activity recognition methods from wrist-worn sensors, for the specific setting of basketball training, drills, and games. Basketball activities lend themselves well for measurement by wrist-worn inertial sensors, and systems that are able to detect such sport-relevant activities could be used in applications toward game analysis, guided training, and personal physical activity tracking. The dataset was recorded for two teams from separate countries (USA and Germany) with a total of 24 players who wore an inertial sensor on their wrist, during both repetitive basketball training sessions and full games. Particular features of this dataset include an inherent variance through cultural differences in game rules and styles as the data was recorded in two countries, as well as different sport skill levels, since the participants were heterogeneous in terms of prior basketball ex
&lt;/p&gt;</description></item><item><title>&#20998;&#38454;&#27573;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#26032;&#39062;&#25216;&#26415;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#38480;&#25968;&#25454;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#22686;&#24378;&#24378;&#24230;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#65292;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#20445;&#30495;&#24230;&#65292;&#22312;&#25972;&#21512;PixelCNNs&#19982;VQ-VAE-2&#30340;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2305.12681</link><description>&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#35757;&#32451;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#38454;&#27573;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Phased Data Augmentation for Training a Likelihood-Based Generative Model with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12681
&lt;/p&gt;
&lt;p&gt;
&#20998;&#38454;&#27573;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#26032;&#39062;&#25216;&#26415;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#38480;&#25968;&#25454;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#22686;&#24378;&#24378;&#24230;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#65292;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#20445;&#30495;&#24230;&#65292;&#22312;&#25972;&#21512;PixelCNNs&#19982;VQ-VAE-2&#30340;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#21019;&#24314;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#23427;&#20204;&#23545;&#20110;&#35757;&#32451;&#22823;&#37327;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#22312;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#26114;&#25110;&#22256;&#38590;&#30340;&#39046;&#22495;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#20027;&#35201;&#19987;&#27880;&#20110;GAN&#26550;&#26500;&#65292;&#23548;&#33268;&#20854;&#20182;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#23384;&#22312;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;&#20998;&#38454;&#27573;&#25968;&#25454;&#22686;&#24378;&#8221;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#26223;&#19979;&#20248;&#21270;&#35757;&#32451;&#65292;&#32780;&#19981;&#25913;&#21464;&#22266;&#26377;&#25968;&#25454;&#20998;&#24067;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#22312;&#23398;&#20064;&#38454;&#27573;&#38480;&#21046;&#22686;&#24378;&#24378;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#20445;&#30495;&#24230;&#12290;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;PixelCNNs&#19982;VQ-VAE-2&#38598;&#25104;&#30340;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#36825;&#20195;&#34920;&#20102;&#26377;&#25928;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12681v2 Announce Type: replace-cross  Abstract: Generative models excel in creating realistic images, yet their dependency on extensive datasets for training presents significant challenges, especially in domains where data collection is costly or challenging. Current data-efficient methods largely focus on GAN architectures, leaving a gap in training other types of generative models. Our study introduces "phased data augmentation" as a novel technique that addresses this gap by optimizing training in limited data scenarios without altering the inherent data distribution. By limiting the augmentation intensity throughout the learning phases, our method enhances the model's ability to learn from limited data, thus maintaining fidelity. Applied to a model integrating PixelCNNs with VQ-VAE-2, our approach demonstrates superior performance in both quantitative and qualitative evaluations across diverse datasets. This represents an important step forward in the efficient training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; BCQ &#31639;&#27861;&#65292;&#23558; VQC &#20316;&#20026;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31227;&#20301;&#25968;&#25454;&#32534;&#30721;&#23618;&#20013;&#36755;&#20837;&#21464;&#37327;&#30340;&#39034;&#24207;&#26469;&#36827;&#34892;&#20840;&#26032;&#30340;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#25209;&#37327; RL &#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2305.00905</link><description>&lt;p&gt;
BCQQ: &#24490;&#29615;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;&#30340;&#25209;&#37327;&#32422;&#26463;&#37327;&#23376; Q &#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BCQQ: Batch-Constraint Quantum Q-Learning with Cyclic Data Re-uploading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.00905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; BCQ &#31639;&#27861;&#65292;&#23558; VQC &#20316;&#20026;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31227;&#20301;&#25968;&#25454;&#32534;&#30721;&#23618;&#20013;&#36755;&#20837;&#21464;&#37327;&#30340;&#39034;&#24207;&#26469;&#36827;&#34892;&#20840;&#26032;&#30340;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#25209;&#37327; RL &#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#29615;&#22659;&#20132;&#20114;&#65292;&#20351;&#24471;&#35757;&#32451;&#36807;&#31243;&#32791;&#26102;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#25209;&#37327;RL&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#24694;&#21270;&#65292;&#20854;&#20013;&#20195;&#29702;&#21482;&#22312;&#19968;&#20010;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#28041;&#21450;&#29615;&#22659;&#20132;&#20114;&#12290;&#37327;&#23376;&#35745;&#31639;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#27604;&#65292;&#37327;&#23376;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21033;&#29992;VQC&#20316;&#20026;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#25209;&#37327;RL&#31639;&#27861;&#65292;&#22312;&#31163;&#25955;&#25209;&#37327;&#32422;&#26463;&#28145;&#24230; Q &#23398;&#20064;(BCQ)&#31639;&#27861;&#20013;&#25506;&#35752;&#20102;&#36825;&#19968;&#28508;&#22312;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24490;&#29615;&#31227;&#20301;&#25968;&#25454;&#32534;&#30721;&#23618;&#20013;&#36755;&#20837;&#21464;&#37327;&#30340;&#39034;&#24207;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;OpenAI CartPole&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#22522;&#20110;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#25955;BC&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.00905v2 Announce Type: replace-cross  Abstract: Deep reinforcement learning (DRL) often requires a large number of data and environment interactions, making the training process time-consuming. This challenge is further exacerbated in the case of batch RL, where the agent is trained solely on a pre-collected dataset without environment interactions. Recent advancements in quantum computing suggest that quantum models might require less data for training compared to classical methods. In this paper, we investigate this potential advantage by proposing a batch RL algorithm that utilizes VQC as function approximators within the discrete batch-constraint deep Q-learning (BCQ) algorithm. Additionally, we introduce a novel data re-uploading scheme by cyclically shifting the order of input variables in the data encoding layers. We evaluate the efficiency of our algorithm on the OpenAI CartPole environment and compare its performance to the classical neural network-based discrete BC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#30340;&#20999;&#19995;&#19978;&#36827;&#34892;&#21367;&#31215;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#27492;&#23450;&#20041;&#20102;&#20999;&#19995;&#28388;&#27874;&#22120;&#21644;&#20999;&#19995;&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#65292;&#36825;&#20026;&#36830;&#32493;&#26550;&#26500;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#25805;&#20316;&#26041;&#24335;&#65292;&#26368;&#21518;&#35777;&#26126;&#20102;&#31163;&#25955;&#21270;&#21518;&#30340;&#26550;&#26500;&#25910;&#25947;&#20110;&#36830;&#32493;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2303.11323</link><description>&lt;p&gt;
Tangent Bundle Convolutional Learning: &#20174;&#27969;&#24418;&#21040;&#32454;&#32990;&#23618;&#29366;&#32467;&#26500;&#20877;&#22238;&#21435;
&lt;/p&gt;
&lt;p&gt;
Tangent Bundle Convolutional Learning: from Manifolds to Cellular Sheaves and Back
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.11323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#30340;&#20999;&#19995;&#19978;&#36827;&#34892;&#21367;&#31215;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#27492;&#23450;&#20041;&#20102;&#20999;&#19995;&#28388;&#27874;&#22120;&#21644;&#20999;&#19995;&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#65292;&#36825;&#20026;&#36830;&#32493;&#26550;&#26500;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#25805;&#20316;&#26041;&#24335;&#65292;&#26368;&#21518;&#35777;&#26126;&#20102;&#31163;&#25955;&#21270;&#21518;&#30340;&#26550;&#26500;&#25910;&#25947;&#20110;&#36830;&#32493;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#30340;&#20999;&#19995;&#19978;&#36827;&#34892;&#21367;&#31215;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#36830;&#25509;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#25351;&#25968;&#23450;&#20041;&#30340;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#31181;&#21367;&#31215;&#25805;&#20316;&#23450;&#20041;&#20102;&#20999;&#19995;&#28388;&#27874;&#22120;&#21644;&#20999;&#19995;&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#65292;&#36825;&#20123;&#26159;&#22312;&#20999;&#19995;&#20449;&#21495;&#19978;&#25805;&#20316;&#30340;&#26032;&#22411;&#36830;&#32493;&#26550;&#26500;&#65292;&#21363;&#27969;&#24418;&#19978;&#30340;&#30690;&#37327;&#22330;&#12290;&#20999;&#19995;&#28388;&#27874;&#22120;&#20855;&#26377;&#24191;&#20041;&#30340;&#35889;&#34920;&#31034;&#65292;&#25512;&#24191;&#20102;&#26631;&#37327;&#27969;&#24418;&#28388;&#27874;&#22120;&#12289;&#22270;&#28388;&#27874;&#22120;&#21644;&#36830;&#32493;&#26102;&#38388;&#26631;&#20934;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#25955;&#21270;&#36807;&#31243;&#65292;&#28041;&#21450;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#65292;&#20351;&#24471;TNNs&#21487;&#23454;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#31163;&#25955;&#23545;&#24212;&#29289;&#26159;&#26368;&#36817;&#20171;&#32461;&#30340;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#26032;&#30340;&#21512;&#29702;&#21464;&#31181;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#36825;&#31181;&#31163;&#25955;&#21270;&#26550;&#26500;&#25910;&#25947;&#20110;&#22522;&#30784;&#30340;&#36830;&#32493;TNN&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.11323v2 Announce Type: replace-cross  Abstract: In this work we introduce a convolution operation over the tangent bundle of Riemann manifolds in terms of exponentials of the Connection Laplacian operator. We define tangent bundle filters and tangent bundle neural networks (TNNs) based on this convolution operation, which are novel continuous architectures operating on tangent bundle signals, i.e. vector fields over the manifolds. Tangent bundle filters admit a spectral representation that generalizes the ones of scalar manifold filters, graph filters and standard convolutional filters in continuous time. We then introduce a discretization procedure, both in the space and time domains, to make TNNs implementable, showing that their discrete counterpart is a novel principled variant of the very recently introduced sheaf neural networks. We formally prove that this discretized architecture converges to the underlying continuous TNN. Finally, we numerically evaluate the effecti
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25361;&#25112;&#20256;&#32479;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2303.05656</link><description>&lt;p&gt;
EHRDiff: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25506;&#32034;&#30495;&#23454;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.05656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25361;&#25112;&#20256;&#32479;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#21253;&#21547;&#20016;&#23500;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#65292;&#26159;&#21457;&#23637;&#31934;&#20934;&#21307;&#23398;&#31995;&#32479;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#25285;&#24551;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#33719;&#21462;&#39640;&#36136;&#37327;&#21644;&#22823;&#35268;&#27169;EHR&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#26041;&#27861;&#30740;&#21457;&#30340;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#21512;&#25104;&#30495;&#23454;&#30340;EHR&#25968;&#25454;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21450;&#20854;&#21464;&#20307;&#29992;&#20110;EHR&#32508;&#21512;&#12290;&#23613;&#31649;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;EHR&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38590;&#20197;&#35757;&#32451;&#19988;&#23481;&#26131;&#20986;&#29616;&#27169;&#24335;&#22349;&#22604;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#24314;&#27169;&#20013;&#26368;&#26032;&#24341;&#20837;&#30340;&#65292;&#24050;&#32463;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#23574;&#31471;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;EHR&#25968;&#25454;&#32508;&#21512;&#26041;&#38754;&#30340;&#25928;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.05656v2 Announce Type: replace  Abstract: Electronic health records (EHR) contain a wealth of biomedical information, serving as valuable resources for the development of precision medicine systems. However, privacy concerns have resulted in limited access to high-quality and large-scale EHR data for researchers, impeding progress in methodological development. Recent research has delved into synthesizing realistic EHR data through generative modeling techniques, where a majority of proposed methods relied on generative adversarial networks (GAN) and their variants for EHR synthesis. Despite GAN-based methods attaining state-of-the-art performance in generating EHR data, these approaches are difficult to train and prone to mode collapse. Recently introduced in generative modeling, diffusion models have established cutting-edge performance in image generation, but their efficacy in EHR data synthesis remains largely unexplored. In this study, we investigate the potential of d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#27969;&#24335;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#20302;&#24310;&#36831;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2302.13451</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#24335;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#20302;&#24310;&#36831;&#27880;&#24847;&#21147;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
A low latency attention module for streaming self-supervised speech representation learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#27969;&#24335;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#20302;&#24310;&#36831;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
transformer&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;transformer&#30340;&#26680;&#24515;&#32452;&#20214;&#12290;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#26159;transformer&#26550;&#26500;&#30340;&#19968;&#20010;&#27969;&#34892;&#29992;&#20363;&#12290;&#30001;&#20110;transformer&#30340;&#38750;&#22240;&#26524;&#34892;&#20026;&#65292;&#23545;&#20110;SSRL&#30340;transformer&#30340;&#20351;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#38750;&#22240;&#26524;&#24212;&#29992;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#23186;&#20307;&#22788;&#29702;&#38382;&#39064;&#65292;&#22914;&#35821;&#38899;&#22788;&#29702;&#65292;&#38656;&#35201;&#23454;&#26102;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#23454;&#29616;&#65292;&#35813;&#27169;&#22359;&#21487;&#20197;&#36890;&#36807;&#36739;&#20302;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#35757;&#32451;SSRL&#26550;&#26500;&#65292;&#24182;&#20801;&#35768;&#22312;&#20302;&#22266;&#23450;&#24310;&#36831;&#19979;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#27969;&#24335;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#21644;&#20302;&#24310;&#36831;&#27969;&#24335;&#27880;&#24847;&#21147;&#65288;LLSA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.13451v2 Announce Type: replace-cross  Abstract: The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the late
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#20154;&#21475;&#26679;&#26412;&#20197;&#21450;&#30456;&#20284;&#36793;&#38469;&#20381;&#36182;&#24615;&#65292;&#24341;&#20837;&#31354;&#38388;&#32452;&#20214;&#24182;&#32771;&#34385;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20294;&#29616;&#23454;&#30340;&#30446;&#26631;&#20154;&#21475;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2302.09193</link><description>&lt;p&gt;
&#22522;&#20110;Copula&#30340;&#21487;&#36716;&#31227;&#27169;&#22411;&#29992;&#20110;&#21512;&#25104;&#20154;&#21475;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Copula-based transferable models for synthetic population generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.09193
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#20154;&#21475;&#26679;&#26412;&#20197;&#21450;&#30456;&#20284;&#36793;&#38469;&#20381;&#36182;&#24615;&#65292;&#24341;&#20837;&#31354;&#38388;&#32452;&#20214;&#24182;&#32771;&#34385;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20294;&#29616;&#23454;&#30340;&#30446;&#26631;&#20154;&#21475;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32508;&#21512;&#28041;&#21450;&#29983;&#25104;&#24494;&#35266;&#20195;&#29702;&#30446;&#26631;&#20154;&#21475;&#30340;&#21512;&#25104;&#20294;&#29616;&#23454;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#34892;&#20026;&#24314;&#27169;&#21644;&#27169;&#25311;&#12290; &#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#30446;&#26631;&#20154;&#21475;&#26679;&#26412;&#65292;&#22914;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#25110;&#26053;&#34892;&#35843;&#26597;&#65292;&#30001;&#20110;&#39640;&#25104;&#26412;&#21644;&#36739;&#23567;&#30340;&#26679;&#26412;&#37327;&#65292;&#22312;&#36739;&#23567;&#30340;&#22320;&#29702;&#23610;&#24230;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#20165;&#24050;&#30693;&#32463;&#39564;&#36793;&#38469;&#20998;&#24067;&#30340;&#30446;&#26631;&#20154;&#21475;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290; &#35813;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#20855;&#26377;&#30456;&#20284;&#36793;&#38469;&#20381;&#36182;&#24615;&#30340;&#19981;&#21516;&#20154;&#21475;&#30340;&#26679;&#26412;&#65292;&#23558;&#31354;&#38388;&#32452;&#20214;&#24341;&#20837;&#21040;&#20154;&#21475;&#32508;&#21512;&#20013;&#65292;&#24182;&#32771;&#34385;&#21508;&#31181;&#20449;&#24687;&#28304;&#29992;&#20110;&#26356;&#30495;&#23454;&#30340;&#29983;&#25104;&#22120;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#36807;&#31243;&#28041;&#21450;&#23558;&#25968;&#25454;&#26631;&#20934;&#21270;&#24182;&#23558;&#20854;&#35270;&#20026;&#32473;&#23450;Copula&#30340;&#23454;&#29616;&#65292;&#28982;&#21518;&#22312;&#34701;&#20837;&#20851;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.09193v2 Announce Type: replace-cross  Abstract: Population synthesis involves generating synthetic yet realistic representations of a target population of micro-agents for behavioral modeling and simulation. Traditional methods, often reliant on target population samples, such as census data or travel surveys, face limitations due to high costs and small sample sizes, particularly at smaller geographical scales. We propose a novel framework based on copulas to generate synthetic data for target populations where only empirical marginal distributions are known. This method utilizes samples from different populations with similar marginal dependencies, introduces a spatial component into population synthesis, and considers various information sources for more realistic generators. Concretely, the process involves normalizing the data and treat it as realizations of a given copula, and then training a generative model before incorporating the information on the marginals of the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;Hemigrammus rhodostomus&#40060;&#31867;&#30340;&#31038;&#20250;&#20114;&#21160;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20877;&#29616;&#23454;&#39564;&#21487;&#35266;&#27979;&#37327;&#26041;&#38754;&#19982;&#20998;&#26512;&#27169;&#22411;&#30340;&#31454;&#20105;&#21147;&#65292;&#24378;&#35843;&#20102;&#36328;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#19968;&#33268;&#39564;&#35777;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2302.06839</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#40060;&#23545;&#30340;&#38271;&#26399;&#38598;&#20307;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Predicting the long-term collective behaviour of fish pairs with deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06839
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;Hemigrammus rhodostomus&#40060;&#31867;&#30340;&#31038;&#20250;&#20114;&#21160;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20877;&#29616;&#23454;&#39564;&#21487;&#35266;&#27979;&#37327;&#26041;&#38754;&#19982;&#20998;&#26512;&#27169;&#22411;&#30340;&#31454;&#20105;&#21147;&#65292;&#24378;&#35843;&#20102;&#36328;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#19968;&#33268;&#39564;&#35777;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#22686;&#24378;&#20102;&#25105;&#20204;&#23545;&#31038;&#20250;&#30456;&#20114;&#20316;&#29992;&#22914;&#20309;&#22609;&#36896;&#21160;&#29289;&#31038;&#20250;&#38598;&#20307;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#20998;&#26512;&#27169;&#22411;&#22312;&#30740;&#31350;&#38598;&#20307;&#34892;&#20026;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35780;&#20272;Hemigrammus rhodostomus&#40060;&#31867;&#30340;&#31038;&#20250;&#20114;&#21160;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#32467;&#26524;&#19982;&#23454;&#39564;&#32467;&#26524;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#20998;&#26512;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#35780;&#20272;&#38598;&#20307;&#36816;&#21160;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21033;&#29992;&#19968;&#32452;&#20005;&#26684;&#30340;&#20010;&#20307;&#21644;&#38598;&#20307;&#26102;&#31354;&#21487;&#35266;&#27979;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31038;&#20250;&#20114;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#19982;&#20854;&#20998;&#26512;&#23545;&#25163;&#31454;&#20105;&#65292;&#20197;&#37325;&#29616;&#24494;&#22937;&#30340;&#23454;&#39564;&#21487;&#35266;&#27979;&#37327;&#12290;&#27492;&#22806;&#65292;&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#19968;&#33268;&#39564;&#35777;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#20351;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20851;&#38190;&#35774;&#35745;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06839v2 Announce Type: replace  Abstract: Modern computing has enhanced our understanding of how social interactions shape collective behaviour in animal societies. Although analytical models dominate in studying collective behaviour, this study introduces a deep learning model to assess social interactions in the fish species Hemigrammus rhodostomus. We compare the results of our deep learning approach to experiments and to the results of a state-of-the-art analytical model. To that end, we propose a systematic methodology to assess the faithfulness of a collective motion model, exploiting a set of stringent individual and collective spatio-temporal observables. We demonstrate that machine learning models of social interactions can directly compete with their analytical counterparts in reproducing subtle experimental observables. Moreover, this work emphasises the need for consistent validation across different timescales, and identifies key design aspects that enable our d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#20851;&#31995;&#23545;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#20989;&#25968;&#37325;&#26032;&#21152;&#26435;&#65292;D$^3$G&#26041;&#27861;&#33021;&#22815;&#25913;&#36827;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2302.02609</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#20851;&#31995;&#25913;&#36827;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving Domain Generalization with Domain Relations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02609
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#20851;&#31995;&#23545;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#20989;&#25968;&#37325;&#26032;&#21152;&#26435;&#65292;D$^3$G&#26041;&#27861;&#33021;&#22815;&#25913;&#36827;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#36716;&#31227;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#24403;&#27169;&#22411;&#22312;&#27979;&#35797;&#38454;&#27573;&#38754;&#23545;&#19981;&#21516;&#20110;&#35757;&#32451;&#26102;&#30340;&#20998;&#24067;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20851;&#27880;&#39046;&#22495;&#36716;&#31227;&#65292;&#21363;&#24403;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#19982;&#20854;&#35757;&#32451;&#26102;&#19981;&#21516;&#30340;&#26032;&#39046;&#22495;&#26102;&#21457;&#29983;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#31216;&#20026;D$^3$G&#12290;&#19982;&#20808;&#21069;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#39046;&#22495;&#19981;&#21464;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;D$^3$G&#21033;&#29992;&#22522;&#20110;&#39046;&#22495;&#20803;&#25968;&#25454;&#30340;&#39046;&#22495;&#30456;&#20284;&#24615;&#26469;&#23398;&#20064;&#29305;&#23450;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;D$^3$G&#22312;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#19968;&#32452;&#29305;&#23450;&#20110;&#35757;&#32451;&#39046;&#22495;&#30340;&#20989;&#25968;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#22522;&#20110;&#39046;&#22495;&#20851;&#31995;&#23545;&#20854;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#12290;&#36825;&#20123;&#39046;&#22495;&#20851;&#31995;&#21487;&#20197;&#30452;&#25509;&#20174;&#39046;&#22495;&#20803;&#25968;&#25454;&#20013;&#33719;&#24471;&#21644;&#23398;&#20064;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#21033;&#29992;&#39046;&#22495;&#20851;&#31995;&#23545;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#20989;&#25968;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#21487;&#20197;&#21462;&#24471;&#24378;&#12290;&#12290;&#12290;&#65288;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.02609v2 Announce Type: replace  Abstract: Distribution shift presents a significant challenge in machine learning, where models often underperform during the test stage when faced with a different distribution than the one they were trained on. This paper focuses on domain shifts, which occur when the model is applied to new domains that are different from the ones it was trained on, and propose a new approach called D$^3$G. Unlike previous methods that aim to learn a single model that is domain invariant, D$^3$G leverages domain similarities based on domain metadata to learn domain-specific models. Concretely, D$^3$G learns a set of training-domain-specific functions during the training stage and reweights them based on domain relations during the test stage. These domain relations can be directly obtained and learned from domain metadata. Under mild assumptions, we theoretically prove that using domain relations to reweight training-domain-specific functions achieves stron
&lt;/p&gt;</description></item><item><title>&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340; $\texttt{[MASK]} $&#31526;&#21495;&#20250;&#23548;&#33268;&#27169;&#22411;&#32500;&#24230;&#36807;&#24230;&#20998;&#37197;&#65292;&#36896;&#25104;&#30495;&#23454;&#26631;&#35760;&#30340;&#34920;&#31034;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MAE-LM&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;</title><link>https://arxiv.org/abs/2302.02060</link><description>&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#34920;&#31034;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
Representation Deficiency in Masked Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02060
&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340; $\texttt{[MASK]} $&#31526;&#21495;&#20250;&#23548;&#33268;&#27169;&#22411;&#32500;&#24230;&#36807;&#24230;&#20998;&#37197;&#65292;&#36896;&#25104;&#30495;&#23454;&#26631;&#35760;&#30340;&#34920;&#31034;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MAE-LM&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24050;&#32463;&#25104;&#20026;&#21452;&#21521;&#25991;&#26412;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;&#20851;&#20110;MLM&#30340;&#19968;&#20010;&#26174;&#33879;&#38382;&#39064;&#26159;&#29305;&#27530;&#30340; $\texttt{[MASK]}$ &#31526;&#21495;&#20250;&#23548;&#33268;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#19979;&#28216;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#20026;&#23427;&#21482;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#32780;&#19981;&#20986;&#29616;&#22312;&#24494;&#35843;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#24046;&#24322;&#30340;&#21518;&#26524;&#65306;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#35777;&#26126;&#20102;MLM&#39044;&#35757;&#32451;&#19987;&#38376;&#20998;&#37197;&#20102;&#19968;&#20123;&#27169;&#22411;&#32500;&#24230;&#26469;&#34920;&#31034; $\texttt{[MASK]}$ &#26631;&#35760;&#65292;&#23548;&#33268;&#30495;&#23454;&#26631;&#35760;&#30340;&#34920;&#31034;&#19981;&#36275;&#65292;&#24182;&#22312;&#27809;&#26377; $\texttt{[MASK]}$ &#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#65292;&#38480;&#21046;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36866;&#24212;&#19979;&#28216;&#25968;&#25454;&#26102;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#21463;&#21040;&#35782;&#21035;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAE-LM&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;MLM&#23545;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#25490;&#38500;&#20102; $\texttt{[MASK]}$ &#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.02060v2 Announce Type: replace  Abstract: Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically that MLM pretraining allocates some model dimensions exclusively for representing $\texttt{[MASK]}$ tokens, resulting in a representation deficiency for real tokens and limiting the pretrained model's expressiveness when it is adapted to downstream data without $\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM, which pretrains the Masked Autoencoder architecture with MLM where $\texttt{[MASK]}$ tokens are excluded from the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;INVPROP&#31639;&#27861;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#38598;&#30340;&#21069;&#20687;&#19978;&#30340;&#23646;&#24615;&#65292;&#32467;&#21512;&#20998;&#25903;&#30028;&#38480;&#20197;&#22686;&#21152;&#31934;&#24230;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;GPU&#21152;&#36895;&#65292;&#36991;&#20813;&#20102;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2302.01404</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#36793;&#30028;&#31070;&#32463;&#32593;&#32476;&#21069;&#20687;
&lt;/p&gt;
&lt;p&gt;
Provably Bounding Neural Network Preimages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.01404
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;INVPROP&#31639;&#27861;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#38598;&#30340;&#21069;&#20687;&#19978;&#30340;&#23646;&#24615;&#65292;&#32467;&#21512;&#20998;&#25903;&#30028;&#38480;&#20197;&#22686;&#21152;&#31934;&#24230;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;GPU&#21152;&#36895;&#65292;&#36991;&#20813;&#20102;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#39564;&#35777;&#24037;&#20316;&#20391;&#37325;&#20110;&#38480;&#23450;&#32473;&#23450;&#36755;&#20837;&#38598;&#23545;&#24212;&#30340;&#36755;&#20986;&#38598;&#65288;&#20363;&#22914;&#65292;&#26631;&#20934;&#36755;&#20837;&#30340;&#26377;&#30028;&#25200;&#21160;&#65289;&#12290;&#20294;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#35768;&#22810;&#24212;&#29992;&#24773;&#26223;&#38656;&#35201;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#25110;&#32773;&#23545;&#23548;&#33268;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#38598;&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;INVPROP&#31639;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22312;&#32447;&#24615;&#32422;&#26463;&#36755;&#20986;&#38598;&#30340;&#21069;&#20687;&#19978;&#30340;&#23646;&#24615;&#65292;&#21487;&#20197;&#19982;&#20998;&#25903;&#30028;&#38480;&#32467;&#21512;&#20197;&#22686;&#21152;&#31934;&#24230;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#39640;&#25928;&#31639;&#27861;&#26159;GPU&#21152;&#36895;&#30340;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#29992;&#20110;&#36890;&#36807;&#21518;&#21521;&#21487;&#36798;&#24615;&#20998;&#26512;&#35782;&#21035;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#25511;&#21046;&#21306;&#22495;&#65292;&#39564;&#35777;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.01404v4 Announce Type: replace-cross  Abstract: Most work on the formal verification of neural networks has focused on bounding the set of outputs that correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However, many use cases of neural network verification require solving the inverse problem, or over-approximating the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set, which can be combined with branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for identifying safe control regions for a dynamical system via backward reachability analysis, verifying adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show that in certain settings, we find over-a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Curriculum Learning&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;DeepLens&#35774;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#34920;&#38754;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#22797;&#21512;&#36879;&#38236;&#30340;&#20809;&#23398;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#23545;&#33391;&#22909;&#21021;&#22987;&#35774;&#35745;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#32463;&#20856;&#25104;&#20687;&#36879;&#38236;&#21644;&#22823;&#35270;&#22330;&#24310;&#20280;&#26223;&#28145;&#35745;&#31639;&#36879;&#38236;&#12290;</title><link>https://arxiv.org/abs/2302.01089</link><description>&lt;p&gt;
Curriculum Learning&#29992;&#20110;&#20174;&#22836;&#24320;&#22987;&#30340;&#28145;&#24230;&#23398;&#20064;&#25240;&#23556;&#20809;&#23398;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for ab initio Deep Learned Refractive Optics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.01089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Curriculum Learning&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;DeepLens&#35774;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#34920;&#38754;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#22797;&#21512;&#36879;&#38236;&#30340;&#20809;&#23398;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#23545;&#33391;&#22909;&#21021;&#22987;&#35774;&#35745;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#32463;&#20856;&#25104;&#20687;&#36879;&#38236;&#21644;&#22823;&#35270;&#22330;&#24310;&#20280;&#26223;&#28145;&#35745;&#31639;&#36879;&#38236;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20809;&#23398;&#20248;&#21270;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#20351;&#29992;&#36755;&#20986;&#22270;&#20687;&#20316;&#20026;&#30446;&#26631;&#30340;&#35745;&#31639;&#25104;&#20687;&#31995;&#32479;&#35774;&#35745;&#30340;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23427;&#30446;&#21069;&#34987;&#38480;&#21046;&#20110;&#21333;&#20010;&#20803;&#32032;&#65288;&#22914;&#34893;&#23556;&#20809;&#23398;&#20803;&#20214;&#65288;DOE&#65289;&#25110;&#37329;&#23646;&#36879;&#38236;&#65289;&#26500;&#25104;&#30340;&#31616;&#21333;&#20809;&#23398;&#31995;&#32479;&#65292;&#25110;&#32773;&#20174;&#33391;&#22909;&#30340;&#21021;&#22987;&#35774;&#35745;&#24494;&#35843;&#22797;&#21512;&#36879;&#38236;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Curriculum Learning&#30340;DeepLens&#35774;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#34920;&#38754;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#22797;&#21512;&#36879;&#38236;&#30340;&#20809;&#23398;&#35774;&#35745;&#65292;&#32780;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#65292;&#22240;&#27492;&#20811;&#26381;&#20102;&#23545;&#33391;&#22909;&#21021;&#22987;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#23436;&#20840;&#33258;&#21160;&#35774;&#35745;&#32463;&#20856;&#25104;&#20687;&#36879;&#38236;&#21644;&#19968;&#20010;&#31867;&#20284;&#25163;&#26426;&#39118;&#26684;&#30340;&#22823;&#35270;&#22330;&#24310;&#20280;&#26223;&#28145;&#35745;&#31639;&#36879;&#38236;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#39640;&#24230;&#38750;&#29699;&#38754;&#26354;&#38754;&#21644;&#30701;&#21518;&#28966;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.01089v3 Announce Type: replace-cross  Abstract: Deep optical optimization has recently emerged as a new paradigm for designing computational imaging systems using only the output image as the objective. However, it has been limited to either simple optical systems consisting of a single element such as a diffractive optical element (DOE) or metalens, or the fine-tuning of compound lenses from good initial designs. Here we present a DeepLens design method based on curriculum learning, which is able to learn optical designs of compound lenses ab initio from randomly initialized surfaces without human intervention, therefore overcoming the need for a good initial design. We demonstrate the effectiveness of our approach by fully automatically designing both classical imaging lenses and a large field-of-view extended depth-of-field computational lens in a cellphone-style form factor, with highly aspheric surfaces and a short back focal length.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26080;&#20559;&#35265;&#30340;&#35270;&#35273;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#31995;&#32479;&#19981;&#20381;&#36182;&#20154;&#31867;&#35268;&#21017;&#32780;&#33258;&#20027;&#23398;&#20064;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2302.00569</link><description>&lt;p&gt;
&#26080;&#20559;&#35265;&#30340;&#35270;&#35273;&#25512;&#33616;&#31995;&#32479;&#65306;&#24320;&#25918;&#25361;&#25112;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Agnostic Visual Recommendation Systems: Open Challenges and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26080;&#20559;&#35265;&#30340;&#35270;&#35273;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#31995;&#32479;&#19981;&#20381;&#36182;&#20154;&#31867;&#35268;&#21017;&#32780;&#33258;&#20027;&#23398;&#20064;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#65288;VRSs&#65289;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#25968;&#25454;&#20013;&#29983;&#25104;&#27934;&#23519;&#21147;&#22270;&#34920;&#65292;&#24182;&#25903;&#25345;&#38750;&#19987;&#19994;&#29992;&#25143;&#36827;&#34892;&#20449;&#24687;&#21457;&#29616;&#12290;&#22312;&#25552;&#20986;&#30340;&#35768;&#22810;&#36129;&#29486;&#20013;&#65292;&#19968;&#20123;&#31995;&#32479;&#37319;&#29992;&#20102;&#27169;&#20223;&#20154;&#31867;&#20998;&#26512;&#24072;&#30340;&#38596;&#24515;&#21187;&#21187;&#30446;&#26631;&#65292;&#21363;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#20851;&#31995;&#65292;&#24182;&#20570;&#20986;&#36866;&#24403;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#29992;&#20855;&#26377;&#27934;&#23519;&#21147;&#30340;&#22270;&#34920;&#26469;&#34920;&#31034;&#36825;&#20123;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31995;&#32479;&#31216;&#20026;&#8220;&#26080;&#20559;&#35265;&#8221;VRSs&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20381;&#36182;&#20110;&#20154;&#31867;&#25552;&#20379;&#30340;&#32422;&#26463;&#21644;&#35268;&#21017;&#65292;&#32780;&#26159;&#35797;&#22270;&#33258;&#20027;&#23398;&#20064;&#20219;&#21153;&#12290;&#23613;&#31649;&#26080;&#20559;&#35265;&#30340;VRSs&#20855;&#26377;&#24456;&#39640;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#36827;&#23637;&#21463;&#21040;&#22810;&#20010;&#38556;&#30861;&#30340;&#38459;&#30861;&#65292;&#21253;&#25324;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#25512;&#33616;&#31639;&#27861;&#30340;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#12289;&#23398;&#20064;&#35774;&#35745;&#35268;&#21017;&#30340;&#22256;&#38590;&#20197;&#21450;&#23450;&#20041;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#22270;&#34920;&#30340;&#24863;&#30693;&#26377;&#25928;&#24615;&#30340;&#23450;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.00569v2 Announce Type: replace  Abstract: Visualization Recommendation Systems (VRSs) are a novel and challenging field of study aiming to help generate insightful visualizations from data and support non-expert users in information discovery. Among the many contributions proposed in this area, some systems embrace the ambitious objective of imitating human analysts to identify relevant relationships in data and make appropriate design choices to represent these relationships with insightful charts. We denote these systems as "agnostic" VRSs since they do not rely on human-provided constraints and rules but try to learn the task autonomously. Despite the high application potential of agnostic VRSs, their progress is hindered by several obstacles, including the absence of standardized datasets to train recommendation algorithms, the difficulty of learning design rules, and defining quantitative criteria for evaluating the perceptual effectiveness of generated plots. This pape
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36328;&#22495;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21307;&#23398;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#30142;&#30149;&#39044;&#21518;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.08559</link><description>&lt;p&gt;
&#20351;&#29992;&#36328;&#22495;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.08559
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36328;&#22495;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21307;&#23398;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#30142;&#30149;&#39044;&#21518;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23454;&#36341;&#19978;&#21462;&#20915;&#20110;&#40065;&#26834;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#33719;&#21462;&#24182;&#26631;&#35760;&#25968;&#25454;&#21487;&#33021;&#22826;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#20363;&#22914;&#20020;&#24202;&#30142;&#30149;&#27169;&#22411;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#23567;&#25968;&#25454;&#24773;&#20917;&#19979;&#22686;&#21152;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#21307;&#23398;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#30340;&#36328;&#22495;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#30142;&#30149;&#39044;&#21518;&#24314;&#27169;&#20316;&#20026;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.08559v2 Announce Type: cross  Abstract: Developing successful artificial intelligence systems in practice depends on both robust deep learning models and large, high-quality data. However, acquiring and labeling data can be prohibitively expensive and time-consuming in many real-world applications, such as clinical disease models. Self-supervised learning has demonstrated great potential in increasing model accuracy and robustness in small data regimes. In addition, many clinical imaging and disease modeling applications rely heavily on regression of continuous quantities. However, the applicability of self-supervised learning for these medical-imaging regression tasks has not been extensively studied. In this study, we develop a cross-domain self-supervised learning approach for disease prognostic modeling as a regression problem using medical images as input. We demonstrate that self-supervised pretraining can improve the prediction of Alzheimer's Disease progression from 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#35745;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2210.06891</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#35745;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#23454;&#39564;&#35774;&#35745;&#33539;&#24335;&#65292;&#26088;&#22312;&#32553;&#30701;&#37319;&#38598;&#26102;&#38388;&#12289;&#38477;&#20302;&#25104;&#26412;&#12289;&#21152;&#36895;&#25104;&#20687;&#35774;&#22791;&#30340;&#37096;&#32626;&#12290;&#24403;&#21069;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#19978;&#65292;&#24182;&#35201;&#27714;&#23545;&#29305;&#23450;&#27169;&#22411;&#36827;&#34892;&#35268;&#33539;&#65292;&#32780;&#22312;&#25104;&#20687;&#39046;&#22495;&#65292;&#20854;&#20182;&#20219;&#21153;&#21487;&#33021;&#39537;&#21160;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25104;&#20687;&#24212;&#29992;&#20013;&#23548;&#33268;&#38590;&#20197;&#27714;&#35299;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#35774;&#35745;&#33539;&#24335;&#65292;&#21516;&#26102;&#20248;&#21270;&#35774;&#35745;&#65288;&#22270;&#20687;&#36890;&#36947;&#38598;&#65289;&#24182;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#31354;&#38388;&#19978;&#23494;&#38598;&#37319;&#26679;&#25968;&#25454;&#65288;&#35768;&#22810;&#22270;&#20687;&#36890;&#36947;&#65289;&#36827;&#34892;&#20102;&#23569;&#37327;&#37319;&#38598;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#39044;&#20808;&#25351;&#23450;&#23610;&#23544;&#30340;&#26368;&#20339;&#25903;&#25345;&#20219;&#21153;&#30340;&#36890;&#36947;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06891v3 Announce Type: replace-cross  Abstract: This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#30828;&#38408;&#20540;&#65288;SZOHT&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#38543;&#26426;&#25903;&#25345;&#37319;&#26679;&#65292;&#35299;&#20915;&#20102;$\ell_0$&#32422;&#26463;&#19979;&#26799;&#24230;&#35745;&#31639;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.05279</link><description>&lt;p&gt;
&#38646;&#38454;&#30828;&#38408;&#20540;&#65306;&#26799;&#24230;&#35823;&#24046;&#19982;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.05279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#30828;&#38408;&#20540;&#65288;SZOHT&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#38543;&#26426;&#25903;&#25345;&#37319;&#26679;&#65292;&#35299;&#20915;&#20102;$\ell_0$&#32422;&#26463;&#19979;&#26799;&#24230;&#35745;&#31639;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$\ell_0$&#32422;&#26463;&#20248;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24456;&#24120;&#35265;&#65292;&#23588;&#20854;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#65292;&#22240;&#20026;&#23427;&#26159;&#23454;&#29616;&#31232;&#30095;&#23398;&#20064;&#30340;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#12290;&#30828;&#38408;&#20540;&#26799;&#24230;&#19979;&#38477;&#26159;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#20027;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#19968;&#38454;&#26799;&#24230;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#25110;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#65292;&#36825;&#26102;&#38646;&#38454;&#65288;ZO&#65289;&#26799;&#24230;&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#24456;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#38646;&#38454;&#26799;&#24230;&#33021;&#21542;&#19982;&#30828;&#38408;&#20540;&#31639;&#23376;&#37197;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35868;&#39064;&#65292;&#26412;&#25991;&#20851;&#27880;$\ell_0$&#32422;&#26463;&#30340;&#40657;&#30418;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#30828;&#38408;&#20540;&#65288;SZOHT&#65289;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#36890;&#29992;&#30340;ZO&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#38543;&#26426;&#25903;&#25345;&#37319;&#26679;&#24471;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;SZOHT&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.05279v2 Announce Type: replace  Abstract: $\ell_0$ constrained optimization is prevalent in machine learning, particularly for high-dimensional problems, because it is a fundamental approach to achieve sparse learning. Hard-thresholding gradient descent is a dominant technique to solve this problem. However, first-order gradients of the objective function may be either unavailable or expensive to calculate in a lot of real-world problems, where zeroth-order (ZO) gradients could be a good surrogate. Unfortunately, whether ZO gradients can work with the hard-thresholding operator is still an unsolved problem. To solve this puzzle, in this paper, we focus on the $\ell_0$ constrained black-box stochastic optimization problems, and propose a new stochastic zeroth-order gradient hard-thresholding (SZOHT) algorithm with a general ZO gradient estimator powered by a novel random support sampling. We provide the convergence analysis of SZOHT under standard assumptions. Importantly, we
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#26426;&#26800;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#26469;&#33258;90&#22810;&#31687;&#25991;&#31456;&#21644;140&#20010;&#25968;&#25454;&#34920;&#30340;&#22823;&#37327;&#23454;&#39564;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2209.12605</link><description>&lt;p&gt;
MechProNet&#65306;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26426;&#26800;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MechProNet: Machine Learning Prediction of Mechanical Properties in Metal Additive Manufacturing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.12605
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#26426;&#26800;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#26469;&#33258;90&#22810;&#31687;&#25991;&#31456;&#21644;140&#20010;&#25968;&#25454;&#34920;&#30340;&#22823;&#37327;&#23454;&#39564;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#65288;MAM&#65289;&#20013;&#30340;&#26426;&#26800;&#24615;&#33021;&#23545;&#20110;&#30830;&#20445;&#21360;&#21046;&#38646;&#37096;&#20214;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#20197;&#21450;&#20854;&#36866;&#29992;&#20110;&#29305;&#23450;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#27979;&#26426;&#26800;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;90&#22810;&#31687;MAM&#25991;&#31456;&#21644;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#34920;&#20013;&#25910;&#38598;&#20102;&#22823;&#37327;&#23454;&#39564;&#25968;&#25454;&#65292;&#21253;&#25324;140&#20010;&#19981;&#21516;&#30340;MAM&#25968;&#25454;&#34920;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#25324;MAM&#22788;&#29702;&#26465;&#20214;&#12289;&#26426;&#22120;&#12289;&#26448;&#26009;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.12605v2 Announce Type: replace-cross  Abstract: Predicting mechanical properties in metal additive manufacturing (MAM) is essential for ensuring the performance and reliability of printed parts, as well as their suitability for specific applications. However, conducting experiments to estimate mechanical properties in MAM processes can be laborious and expensive, and they are often limited to specific materials and processes. Machine learning (ML) methods offer a more flexible and cost-effective approach to predicting mechanical properties based on processing parameters and material properties. In this study, we introduce a comprehensive framework for benchmarking ML models for predicting mechanical properties. We compiled an extensive experimental dataset from over 90 MAM articles and data sheets from a diverse range of sources, encompassing 140 different MAM data sheets. This dataset includes information on MAM processing conditions, machines, materials, and resulting mech
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#21033;&#29992;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#20027;&#39064;&#65292;&#20026;&#35780;&#20272;&#27010;&#29575;&#39044;&#27979;&#25552;&#20379;&#20102;&#30456;&#20851;&#24230;&#37327;&#65292;&#20174;&#32463;&#20856;&#32479;&#35745;&#26041;&#27861;&#21040;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26803;&#29702;&#12290;</title><link>https://arxiv.org/abs/2209.08307</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A review of predictive uncertainty estimation with machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08307
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#21033;&#29992;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#20027;&#39064;&#65292;&#20026;&#35780;&#20272;&#27010;&#29575;&#39044;&#27979;&#25552;&#20379;&#20102;&#30456;&#20851;&#24230;&#37327;&#65292;&#20174;&#32463;&#20856;&#32479;&#35745;&#26041;&#27861;&#21040;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26803;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#24212;&#24403;&#20197;&#27010;&#29575;&#20998;&#24067;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#26088;&#22312;&#22686;&#21152;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#20449;&#24687;&#37327;&#12290;&#23613;&#31649;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#21644;&#39044;&#27979;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#39057;&#32321;&#65292;&#20294;&#30456;&#20851;&#27010;&#24565;&#21644;&#26041;&#27861;&#23578;&#26410;&#22312;&#25972;&#20010;&#39046;&#22495;&#30340;&#25972;&#20307;&#35270;&#35282;&#19979;&#24471;&#21040;&#24418;&#24335;&#21270;&#21644;&#32467;&#26500;&#21270;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#20027;&#39064;&#65292;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#27010;&#29575;&#39044;&#27979;&#30340;&#30456;&#20851;&#24230;&#37327;&#65288;&#19968;&#33268;&#35780;&#20998;&#20989;&#25968;&#21644;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65289;&#12290;&#35813;&#32508;&#36848;&#28085;&#30422;&#20102;&#20174;&#26089;&#26399;&#32479;&#35745;&#24341;&#20837;&#65288;&#22522;&#20110;&#36125;&#21494;&#26031;&#32479;&#35745;&#25110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#32447;&#24615;&#22238;&#24402;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65289;&#21040;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;&#29992;&#20110;&#20301;&#32622;&#12289;&#35268;&#27169;&#30340;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08307v2 Announce Type: replace-cross  Abstract: Predictions and forecasts of machine learning models should take the form of probability distributions, aiming to increase the quantity of information communicated to end users. Although applications of probabilistic prediction and forecasting with machine learning models in academia and industry are becoming more frequent, related concepts and methods have not been formalized and structured under a holistic view of the entire field. Here, we review the topic of predictive uncertainty estimation with machine learning algorithms, as well as the related metrics (consistent scoring functions and proper scoring rules) for assessing probabilistic predictions. The review covers a time period spanning from the introduction of early statistical (linear regression and time series models, based on Bayesian statistics or quantile regression) to recent machine learning algorithms (including generalized additive models for location, scale a
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;log-cosh&#25439;&#22833;&#20989;&#25968;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#27604;&#36739;&#23427;&#19982;&#26607;&#35199;&#20998;&#24067;&#30340;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;MLE&#30340;&#20559;&#24046;&#12289;&#26041;&#24046;&#21644;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#25439;&#22833;&#20989;&#25968;&#30340;&#40065;&#26834;&#20272;&#35745;&#22120;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2208.04564</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;log-cosh&#25439;&#22833;&#20989;&#25968;&#30340;&#32479;&#35745;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Statistical Properties of the log-cosh Loss Function Used in Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.04564
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;log-cosh&#25439;&#22833;&#20989;&#25968;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#27604;&#36739;&#23427;&#19982;&#26607;&#35199;&#20998;&#24067;&#30340;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;MLE&#30340;&#20559;&#24046;&#12289;&#26041;&#24046;&#21644;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#25439;&#22833;&#20989;&#25968;&#30340;&#40065;&#26834;&#20272;&#35745;&#22120;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#19968;&#31181;&#24120;&#35265;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;log-cosh&#25439;&#22833;&#20989;&#25968;&#12290;&#24050;&#32463;&#21457;&#34920;&#20102;&#35768;&#22810;&#20351;&#29992;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#35770;&#25991;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#25991;&#29486;&#20013;&#23578;&#26410;&#25552;&#20986;&#36807;&#32479;&#35745;&#20998;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;log-cosh&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#30340;&#20998;&#24067;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#31867;&#20284;&#30340;&#26607;&#35199;&#20998;&#24067;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#22810;&#31181;&#34920;&#24449;&#20854;&#29305;&#24615;&#30340;&#32479;&#35745;&#31243;&#24207;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;&#19982;&#20854;&#30456;&#20851;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12289;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#12289;&#20284;&#28982;&#20989;&#25968;&#21644;&#36153;&#33293;&#23572;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#26607;&#35199;&#21644;Cosh&#20998;&#24067;&#20197;&#21450;MLE&#30340;&#20301;&#32622;&#21442;&#25968;&#30340;&#28176;&#36817;&#20559;&#24046;&#12289;&#28176;&#36817;&#26041;&#24046;&#21644;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#24182;&#21015;&#32771;&#34385;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26469;&#33258;&#20854;&#20182;&#20960;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#40065;&#26834;&#20272;&#35745;&#22120;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;Huber&#25439;&#22833;&#20989;&#25968;&#21644;&#31209;&#20998;&#25955;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35813;&#25439;&#22833;&#20989;&#25968;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.04564v4 Announce Type: replace-cross  Abstract: This paper analyzes a popular loss function used in machine learning called the log-cosh loss function. A number of papers have been published using this loss function but, to date, no statistical analysis has been presented in the literature. In this paper, we present the distribution function from which the log-cosh loss arises. We compare it to a similar distribution, called the Cauchy distribution, and carry out various statistical procedures that characterize its properties. In particular, we examine its associated pdf, cdf, likelihood function and Fisher information. Side-by-side we consider the Cauchy and Cosh distributions as well as the MLE of the location parameter with asymptotic bias, asymptotic variance, and confidence intervals. We also provide a comparison of robust estimators from several other loss functions, including the Huber loss function and the rank dispersion function. Further, we examine the use of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;Krylov&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#21644;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#20248;&#21183;&#30340;&#26032;&#31639;&#27861;&#65292;&#26469;&#21019;&#24314;&#39640;&#25928;&#30340;&#27969;&#20307;&#27969;&#21160;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#26426;&#27491;&#20132;&#20998;&#35299;&#31639;&#27861;&#30456;&#23545;&#20110;SVD&#32463;&#39564;&#27491;&#20132;&#20998;&#35299;&#26041;&#27861;&#20855;&#26377;&#25968;&#20010;&#20248;&#21183;</title><link>https://arxiv.org/abs/2206.08659</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#27491;&#20132;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#25968;&#23383;&#23402;&#29983;&#25968;&#25454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Digital Twin Data Modelling by Randomized Orthogonal Decomposition and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;Krylov&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#21644;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#20248;&#21183;&#30340;&#26032;&#31639;&#27861;&#65292;&#26469;&#21019;&#24314;&#39640;&#25928;&#30340;&#27969;&#20307;&#27969;&#21160;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#26426;&#27491;&#20132;&#20998;&#35299;&#31639;&#27861;&#30456;&#23545;&#20110;SVD&#32463;&#39564;&#27491;&#20132;&#20998;&#35299;&#26041;&#27861;&#20855;&#26377;&#25968;&#20010;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#65292;&#20854;&#20027;&#35201;&#29305;&#28857;&#26159;&#21453;&#26144;&#21407;&#22987;&#36807;&#31243;&#34892;&#20026;&#12290;&#23558;&#21160;&#24577;&#36807;&#31243;&#19982;&#38477;&#20302;&#22797;&#26434;&#24230;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#20851;&#32852;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#26174;&#33879;&#21464;&#21270;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#20197;&#39640;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;CPU&#26102;&#38388;&#21644;&#30828;&#20214;&#25104;&#26412;&#30340;&#39640;&#25928;&#26144;&#23556;&#21160;&#24577;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#27969;&#20307;&#27969;&#21160;&#21019;&#24314;&#39640;&#25928;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22522;&#20110;Krylov&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#21644;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#20248;&#21183;&#30340;&#26032;&#31639;&#27861;&#65292;&#20248;&#20110;&#36873;&#25321;&#26368;&#20855;&#24433;&#21709;&#21147;&#27169;&#24577;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#38543;&#26426;&#27491;&#20132;&#20998;&#35299;&#31639;&#27861;&#30456;&#27604;SVD&#32463;&#39564;&#27491;&#20132;&#20998;&#35299;&#26041;&#27861;&#25552;&#20379;&#20102;&#20960;&#20010;&#20248;&#21183;&#65292;&#24182;&#32531;&#35299;&#20102;&#25237;&#24433;&#35823;&#24046;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.08659v2 Announce Type: replace-cross  Abstract: A digital twin is a surrogate model that has the main feature to mirror the original process behavior. Associating the dynamical process with a digital twin model of reduced complexity has the significant advantage to map the dynamics with high accuracy and reduced costs in CPU time and hardware to timescales over which that suffers significantly changes and so it is difficult to explore. This paper introduces a new framework for creating efficient digital twin models of fluid flows. We introduce a novel algorithm that combines the advantages of Krylov based dynamic mode decomposition with proper orthogonal decomposition and outperforms the selection of the most influential modes. We prove that randomized orthogonal decomposition algorithm provides several advantages over SVD empirical orthogonal decomposition methods and mitigates the projection error formulating a multiobjective optimization problem.We involve the state-of-th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#20250;&#24615;&#31227;&#21160;&#20013;&#32487;&#65292;&#25552;&#20986;&#20102;FedMobile&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#20026;$O(\frac{1}{\sqrt{NT}})$&#12290;</title><link>https://arxiv.org/abs/2206.04742</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#20250;&#24615;&#31227;&#21160;&#20013;&#32487;&#21152;&#36895;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Accelerating Asynchronous Federated Learning Convergence via Opportunistic Mobile Relaying
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.04742
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#20250;&#24615;&#31227;&#21160;&#20013;&#32487;&#65292;&#25552;&#20986;&#20102;FedMobile&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#20026;$O(\frac{1}{\sqrt{NT}})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#31227;&#21160;&#32593;&#32476;&#29615;&#22659;&#19979;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#22823;&#22810;&#25968;FL&#31639;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#22987;&#32456;&#21487;&#29992;&#65292;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#31995;&#32479;&#20013;&#24182;&#38750;&#22914;&#27492;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#31227;&#21160;&#24615;&#23545;&#24322;&#27493;FL&#25910;&#25947;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21033;&#29992;&#31227;&#21160;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#21478;&#19968;&#23458;&#25143;&#31471;&#20805;&#24403;&#20013;&#32487;&#19982;&#26381;&#21153;&#22120;&#38388;&#25509;&#36890;&#20449;&#65292;&#21019;&#36896;&#39069;&#22806;&#30340;&#36890;&#20449;&#26426;&#20250;&#12290;&#36825;&#20351;&#23458;&#25143;&#31471;&#33021;&#22815;&#26356;&#26089;&#22320;&#19978;&#20256;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#25110;&#25509;&#25910;&#26356;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FL&#31639;&#27861;&#65292;&#31216;&#20026;FedMobile&#65292;&#35813;&#31639;&#27861;&#34701;&#21512;&#20102;&#26426;&#20250;&#24615;&#20013;&#32487;&#65292;&#24182;&#35299;&#20915;&#20102;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20013;&#32487;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;FedMobile&#23454;&#29616;&#20102;&#25910;&#25947;&#36895;&#29575;&#20026;$O(\frac{1}{\sqrt{NT}})$&#65292;&#20854;&#20013;$N$&#26159;&#23458;&#25143;&#31471;&#25968;&#37327;&#65292;$T$&#26159;&#23458;&#25143;&#31471;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.04742v2 Announce Type: replace-cross  Abstract: This paper presents a study on asynchronous Federated Learning (FL) in a mobile network setting. The majority of FL algorithms assume that communication between clients and the server is always available, however, this is not the case in many real-world systems. To address this issue, the paper explores the impact of mobility on the convergence performance of asynchronous FL. By exploiting mobility, the study shows that clients can indirectly communicate with the server through another client serving as a relay, creating additional communication opportunities. This enables clients to upload local model updates sooner or receive fresher global models. We propose a new FL algorithm, called FedMobile, that incorporates opportunistic relaying and addresses key questions such as when and how to relay. We prove that FedMobile achieves a convergence rate $O(\frac{1}{\sqrt{NT}})$, where $N$ is the number of clients and $T$ is the numbe
&lt;/p&gt;</description></item><item><title>&#20195;&#29702;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#36275;&#22815;&#30340;&#25216;&#33021;&#21407;&#35821;&#65292;&#21487;&#20197;&#22312;&#29615;&#22659;&#20013;&#23454;&#29616;&#25152;&#26377;&#39640;&#23618;&#30446;&#26631;&#65292;&#24182;&#33021;&#22815;&#22312;&#20219;&#20309;&#24120;&#35268;&#35821;&#35328;&#20013;&#36923;&#36753;&#21644;&#26102;&#38388;&#22320;&#32452;&#21512;&#36825;&#20123;&#25216;&#33021;&#65292;&#20197;&#30830;&#20999;&#23454;&#29616;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#12290;</title><link>https://arxiv.org/abs/2205.12532</link><description>&lt;p&gt;
&#25216;&#33021;&#26426;&#22120;&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#36923;&#36753;&#25216;&#33021;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.12532
&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#36275;&#22815;&#30340;&#25216;&#33021;&#21407;&#35821;&#65292;&#21487;&#20197;&#22312;&#29615;&#22659;&#20013;&#23454;&#29616;&#25152;&#26377;&#39640;&#23618;&#30446;&#26631;&#65292;&#24182;&#33021;&#22815;&#22312;&#20219;&#20309;&#24120;&#35268;&#35821;&#35328;&#20013;&#36923;&#36753;&#21644;&#26102;&#38388;&#22320;&#32452;&#21512;&#36825;&#20123;&#25216;&#33021;&#65292;&#20197;&#30830;&#20999;&#23454;&#29616;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#33021;&#22815;&#22312;&#30456;&#21516;&#29615;&#22659;&#20013;&#36890;&#36807;&#35821;&#35328;&#25351;&#23450;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#26159;&#21487;&#21462;&#30340;&#12290;&#33719;&#24471;&#36825;&#26679;&#30340;&#20195;&#29702;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#37325;&#22797;&#20351;&#29992;&#22312;&#20808;&#21069;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#25216;&#33021;&#65292;&#20197;&#23558;&#20854;&#32452;&#21512;&#27010;&#25324;&#21040;&#26032;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#23618;&#30446;&#26631;&#22312;&#35821;&#35328;&#20013;&#36923;&#36753;&#21644;&#26102;&#38388;&#19978;&#21487;&#20197;&#32452;&#21512;&#30340;&#26041;&#24335;&#24456;&#22810;&#19988;&#32452;&#21512;&#25968;&#30446;&#24040;&#22823;&#23548;&#33268;&#32500;&#24230;&#28798;&#38590;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#39318;&#20808;&#23398;&#20064;&#36275;&#22815;&#30340;&#25216;&#33021;&#21407;&#35821;&#26469;&#23454;&#29616;&#20854;&#29615;&#22659;&#20013;&#30340;&#25152;&#26377;&#39640;&#23618;&#30446;&#26631;&#12290;&#28982;&#21518;&#20195;&#29702;&#21487;&#20197;&#28789;&#27963;&#22320;&#36923;&#36753;&#21644;&#26102;&#38388;&#22320;&#32452;&#21512;&#23427;&#20204;&#65292;&#26126;&#30830;&#22320;&#23454;&#29616;&#20219;&#20309;&#27491;&#21017;&#35821;&#35328;&#20013;&#30340;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#65292;&#22914;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#30340;&#27491;&#21017;&#29255;&#27573;&#12290;&#36825;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#22797;&#26434;&#30340;&#26102;&#38388;&#36923;&#36753;&#20219;&#21153;&#35268;&#33539;&#26144;&#23556;&#21040;&#36817;&#20046;&#26368;&#20248;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.12532v2 Announce Type: replace  Abstract: It is desirable for an agent to be able to solve a rich variety of problems that can be specified through language in the same environment. A popular approach towards obtaining such agents is to reuse skills learned in prior tasks to generalise compositionally to new ones. However, this is a challenging problem due to the curse of dimensionality induced by the combinatorially large number of ways high-level goals can be combined both logically and temporally in language. To address this problem, we propose a framework where an agent first learns a sufficient set of skill primitives to achieve all high-level goals in its environment. The agent can then flexibly compose them both logically and temporally to provably achieve temporal logic specifications in any regular language, such as regular fragments of linear temporal logic. This provides the agent with the ability to map from complex temporal logic task specifications to near-opti
&lt;/p&gt;</description></item><item><title>PyGOD&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25903;&#25345;&#22810;&#31181;&#39046;&#20808;&#30340;&#22522;&#20110;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#20016;&#23500;&#30340;&#23454;&#29992;&#31243;&#24207;&#20989;&#25968;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#26368;&#20339;&#30340;&#20195;&#30721;&#21487;&#38752;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2204.12095</link><description>&lt;p&gt;
PyGOD: &#19968;&#20010;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
PyGOD: A Python Library for Graph Outlier Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.12095
&lt;/p&gt;
&lt;p&gt;
PyGOD&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25903;&#25345;&#22810;&#31181;&#39046;&#20808;&#30340;&#22522;&#20110;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#20016;&#23500;&#30340;&#23454;&#29992;&#31243;&#24207;&#20989;&#25968;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#26368;&#20339;&#30340;&#20195;&#30721;&#21487;&#38752;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyGOD&#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22270;&#25968;&#25454;&#20013;&#24322;&#24120;&#20540;&#30340;&#24320;&#28304;Python&#24211;&#12290;&#20316;&#20026;&#36825;&#31867;&#24211;&#20013;&#39318;&#20010;&#32508;&#21512;&#24615;&#24037;&#20855;&#65292;PyGOD&#25903;&#25345;&#22810;&#31181;&#39046;&#20808;&#30340;&#22522;&#20110;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#12289;&#26377;&#35814;&#32454;&#25991;&#26723;&#25903;&#25345;&#30340;API&#65292;&#26088;&#22312;&#20379;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20351;&#29992;&#12290;PyGOD&#25552;&#20379;&#20102;&#19981;&#21516;&#26816;&#27979;&#22120;&#30340;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#27599;&#20010;&#26816;&#27979;&#22120;&#20197;&#36866;&#24212;&#20854;&#29992;&#36884;&#12290;&#20026;&#31616;&#21270;&#26816;&#27979;&#24037;&#20316;&#27969;&#30340;&#26500;&#24314;&#65292;PyGOD&#25552;&#20379;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#23454;&#29992;&#31243;&#24207;&#20989;&#25968;&#12290;&#20026;&#20102;&#23558;&#35745;&#31639;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#24418;&#65292;PyGOD&#25903;&#25345;&#28145;&#24230;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#22914;&#37319;&#26679;&#21644;&#23567;&#25209;&#37327;&#22788;&#29702;&#12290;PyGOD&#37319;&#29992;&#20102;&#20419;&#36827;&#20195;&#30721;&#21487;&#38752;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#21253;&#25324;&#21333;&#20803;&#27979;&#35797;&#12289;&#25345;&#32493;&#38598;&#25104;&#21644;&#20195;&#30721;&#35206;&#30422;&#12290;&#20026;&#20102;&#26041;&#20415;&#35775;&#38382;&#65292;PyGOD&#20197;BSD 2-Clause&#35768;&#21487;&#35777;&#21457;&#24067;&#22312;https://pygod.org &#21644; Python&#21253;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.12095v2 Announce Type: replace  Abstract: PyGOD is an open-source Python library for detecting outliers in graph data. As the first comprehensive library of its kind, PyGOD supports a wide array of leading graph-based methods for outlier detection under an easy-to-use, well-documented API designed for use by both researchers and practitioners. PyGOD provides modularized components of the different detectors implemented so that users can easily customize each detector for their purposes. To ease the construction of detection workflows, PyGOD offers numerous commonly used utility functions. To scale computation to large graphs, PyGOD supports functionalities for deep models such as sampling and mini-batch processing. PyGOD uses best practices in fostering code reliability and maintainability, including unit testing, continuous integration, and code coverage. To facilitate accessibility, PyGOD is released under a BSD 2-Clause license at https://pygod.org and at the Python Packa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#26377;&#29702;&#25968;&#20316;&#20026;&#36866;&#24212;&#24615;&#28608;&#27963;&#20989;&#25968;&#26469;&#25913;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23558;&#31616;&#21333;&#30340;DQN&#25552;&#21319;&#20026;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2102.09407</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26377;&#29702;&#28608;&#27963;&#20197;&#25552;&#21319;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Rational Activations to Boost Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.09407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#26377;&#29702;&#25968;&#20316;&#20026;&#36866;&#24212;&#24615;&#28608;&#27963;&#20989;&#25968;&#26469;&#25913;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23558;&#31616;&#21333;&#30340;DQN&#25552;&#21319;&#20026;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#30340;&#26368;&#26032;&#35265;&#35299;&#26174;&#31034;&#65292;&#26234;&#33021;&#19981;&#20165;&#28304;&#33258;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#32780;&#19988;&#21333;&#20010;&#31070;&#32463;&#20803;&#25215;&#25285;&#30340;&#35745;&#31639;&#36131;&#20219;&#27604;&#20197;&#24448;&#39044;&#26399;&#30340;&#26356;&#22810;&#12290;&#36825;&#31181;&#35266;&#28857;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24403;&#21069;&#26041;&#27861;&#20173;&#28982;&#20027;&#35201;&#20351;&#29992;&#38745;&#24577;&#28608;&#27963;&#20989;&#25968;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#20026;&#20160;&#20040;&#26377;&#29702;&#25968;&#36866;&#21512;&#20316;&#20026;&#21487;&#36866;&#24212;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#23558;&#20854;&#21253;&#21547;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;Residual&#32593;&#32476;&#20013;&#24490;&#29615;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#26377;&#29702;&#21333;&#20301;&#22312;&#27531;&#24046;&#36830;&#25509;&#19979;&#23553;&#38381;&#30340;&#26465;&#20214;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#33258;&#28982;&#27491;&#21017;&#21270;&#30340;&#29256;&#26412;&#65306;&#24490;&#29615;&#26377;&#29702;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20026;&#27969;&#34892;&#31639;&#27861;&#37197;&#22791;&#65288;&#24490;&#29615;&#65289;&#26377;&#29702;&#25968;&#28608;&#27963;&#20250;&#26174;&#33879;&#25552;&#39640;Atari&#28216;&#25103;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23558;&#31616;&#21333;&#30340;DQN&#36716;&#21270;&#20026;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.09407v4 Announce Type: replace  Abstract: Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. This perspective should be critical in the context of constantly changing distinct reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are suitable for adaptable activation functions and why their inclusion into neural networks is crucial. Inspired by recurrence in residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version: the recurrent-rational. We demonstrate that equipping popular algorithms with (recurrent-)rational activations leads to consistent improvements on Atari games, especially turning simple DQN into a solid approach, competiti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#23618;&#20998;&#23618;&#20998;&#35299;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#23398;&#20064;&#22823;&#22411;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#23567;&#22411;&#21270;&#23398;&#29983;&#29289;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/1703.05537</link><description>&lt;p&gt;
&#31227;&#21160;&#32858;&#21512;&#25552;&#21462;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Shift Aggregate Extract Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1703.05537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#23618;&#20998;&#23618;&#20998;&#35299;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#23398;&#20064;&#22823;&#22411;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#23567;&#22411;&#21270;&#23398;&#29983;&#29289;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#23618;&#20998;&#23618;&#20998;&#35299;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#22823;&#22411;&#22270;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25193;&#23637;&#20102;&#22312;&#26680;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#32463;&#20856;R-&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#23884;&#22871;&#30340;&#37096;&#20998;-&#37096;&#20998;&#20851;&#31995;&#12290;&#19982;&#30452;&#25509;&#22312;&#36755;&#20837;&#22270;&#19978;&#23637;&#24320;&#27169;&#26495;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#20998;&#35299;&#23618;&#27425;&#32467;&#26500;&#19978;&#23637;&#24320;&#31070;&#32463;&#32593;&#32476;&#27169;&#26495;&#65292;&#20174;&#32780;&#33021;&#22815;&#22788;&#29702;&#36890;&#24120;&#34920;&#24449;&#31038;&#20132;&#32593;&#32476;&#22270;&#30340;&#39640;&#24230;&#21464;&#21270;&#12290;&#28145;&#23618;&#27425;&#30340;&#20998;&#23618;&#20998;&#35299;&#20063;&#36866;&#29992;&#20110;&#39046;&#22495;&#21387;&#32553;&#65292;&#36825;&#31181;&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#31354;&#38388;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22411;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#32988;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#23567;&#22411;&#21270;&#23398;&#29983;&#29289;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1703.05537v2 Announce Type: replace  Abstract: We introduce an architecture based on deep hierarchical decompositions to learn effective representations of large graphs. Our framework extends classic R-decompositions used in kernel methods, enabling nested part-of-part relations. Unlike recursive neural networks, which unroll a template on input graphs directly, we unroll a neural network template over the decomposition hierarchy, allowing us to deal with the high degree variability that typically characterize social network graphs. Deep hierarchical decompositions are also amenable to domain compression, a technique that reduces both space and time complexity by exploiting symmetries. We show empirically that our approach is able to outperform current state-of-the-art graph classification methods on large social network datasets, while at the same time being competitive on small chemobiological benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.16694</link><description>&lt;p&gt;
EdgeOL: &#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#21407;&#20301;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EdgeOL: Efficient in-situ Online Learning on Edge Devices. (arXiv:2401.16694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#24212;&#29992;&#65292;&#22914;&#26426;&#22120;&#20154;&#36741;&#21161;&#20859;&#32769;&#21644;&#29289;&#20307;&#35782;&#21035;&#65292;&#36890;&#24120;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#33258;&#28982;&#38656;&#35201;&#65306;i) &#22788;&#29702;&#23454;&#26102;&#25512;&#29702;&#35831;&#27714;&#21644;ii) &#36866;&#24212;&#21487;&#33021;&#30340;&#37096;&#32626;&#22330;&#26223;&#21464;&#21270;&#12290;&#22312;&#32447;&#27169;&#22411;&#24494;&#35843;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#20351;&#20854;&#38590;&#20197;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;EdgeOL&#24179;&#22343;&#20943;&#23569;&#20102;82%&#30340;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#65292;74%&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#24179;&#22343;&#25512;&#29702;&#20934;&#30830;&#29575;1.70%&#65292;&#30456;&#23545;&#20110;&#21363;&#26102;&#22312;&#32447;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LDGD&#30340;&#21028;&#21035;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16497</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#21028;&#21035;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Discriminative Bayesian Gaussian Process Latent Variable Model for High-Dimensional Data. (arXiv:2401.16497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LDGD&#30340;&#21028;&#21035;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24314;&#27169;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#34987;&#22122;&#22768;&#24178;&#25200;&#25110;&#20197;&#19981;&#21516;&#30340;&#27169;&#24577;&#34920;&#31034;&#26102;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#28508;&#22312;&#30340;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#36825;&#20010;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;&#28508;&#22312;&#21028;&#21035;&#29983;&#25104;&#35299;&#30721;&#22120;&#65288;LDGD&#65289;&#65292;&#23427;&#22312;&#27969;&#24418;&#21457;&#29616;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;&#25968;&#25454;&#65288;&#25110;&#20854;&#29305;&#24449;&#65289;&#21644;&#30456;&#20851;&#26631;&#31614;&#65288;&#22914;&#31867;&#21035;&#25110;&#21050;&#28608;&#65289;&#12290;&#20026;&#20102;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#35299;&#65292;&#20351;&#24471;LDGD&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;LDGD&#30340;&#24212;&#29992;&#12290;LDGD&#19981;&#20165;&#33021;&#20934;&#30830;&#22320;&#25512;&#26029;&#27969;&#24418;&#65292;&#32780;&#19988;&#22312;&#39044;&#27979;&#26631;&#31614;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting meaningful information from high-dimensional data poses a formidable modeling challenge, particularly when the data is obscured by noise or represented through different modalities. In this research, we propose a novel non-parametric modeling approach, leveraging the Gaussian Process (GP), to characterize high-dimensional data by mapping it to a latent low-dimensional manifold. This model, named the Latent Discriminative Generative Decoder (LDGD), utilizes both the data (or its features) and associated labels (such as category or stimulus) in the manifold discovery process. To infer the latent variables, we derive a Bayesian solution, allowing LDGD to effectively capture inherent uncertainties in the data while enhancing the model's predictive accuracy and robustness. We demonstrate the application of LDGD on both synthetic and benchmark datasets. Not only does LDGD infer the manifold accurately, but its prediction accuracy in anticipating labels surpasses state-of-the-art a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22495;&#27867;&#21270;&#31639;&#27861;&#21644;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#26174;&#31034;&#20986;&#38544;&#24615;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20551;&#20887;&#20313;&#30456;&#20851;&#21644;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#30456;&#27604;ERM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14846</link><description>&lt;p&gt;
&#29702;&#35299;&#22495;&#27867;&#21270;&#65306;&#20174;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Domain Generalization: A Noise Robustness Perspective. (arXiv:2401.14846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22495;&#27867;&#21270;&#31639;&#27861;&#21644;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#26174;&#31034;&#20986;&#38544;&#24615;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20551;&#20887;&#20313;&#30456;&#20851;&#21644;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#30456;&#27604;ERM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24555;&#36895;&#21457;&#23637;&#20102;&#29992;&#20110;&#22495;&#27867;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#27809;&#26377;&#26126;&#30830;&#30340;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#32463;&#20856;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#31639;&#27861;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36890;&#36807;&#26631;&#31614;&#22122;&#22768;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22495;&#27867;&#21270;&#31639;&#27861;&#30456;&#23545;&#20110;ERM&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#25581;&#31034;&#20102;&#26631;&#31614;&#22122;&#22768;&#21152;&#21095;&#20102;ERM&#20013;&#20551;&#20887;&#20313;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#65292;&#21066;&#24369;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#65292;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#23384;&#22312;&#20551;&#20887;&#20313;&#30456;&#20851;&#26102;&#20855;&#26377;&#38544;&#24615;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#26377;&#21033;&#24615;&#26377;&#21161;&#20110;&#20943;&#36731;&#20551;&#20887;&#20313;&#30456;&#20851;&#24615;&#24182;&#25913;&#21892;&#21512;&#25104;&#23454;&#39564;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#39069;&#22806;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#30456;&#27604;ERM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We co
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14591</link><description>&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#21464;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14591
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#20854;&#20013;&#27969;&#24418;&#28508;&#31354;&#38388;&#26681;&#25454;Ricci&#27969;&#21457;&#23637;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#29289;&#29702;&#20449;&#24687;&#35774;&#32622;&#20013;&#27169;&#25311;Ricci&#27969;&#26469;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#21305;&#37197;&#27969;&#24418;&#37327;&#65292;&#20197;&#20415;&#23454;&#29616;Ricci&#27969;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27969;&#24418;&#26159;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#23398;&#20064;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#35782;&#21035;&#20986;&#29702;&#24819;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21516;&#26102;&#28436;&#21464;&#20063;&#33021;&#22312;&#38745;&#24577;&#26041;&#27861;&#19978;&#24341;&#36215;&#26356;&#23485;&#23481;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#31561;&#29702;&#24819;&#29305;&#24449;&#30340;PDE&#65292;&#24182;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#36827;&#34892;&#35823;&#24046;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#32452;&#20214;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14405</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#36335;&#24452;&#65306;&#36890;&#36807;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;Transformer
&lt;/p&gt;
&lt;p&gt;
Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#32452;&#20214;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#38899;&#39057;&#25110;&#28857;&#20113;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;ImageNet&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#30446;&#26631;&#27169;&#24577;&#30340;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#20182;&#27169;&#24577;&#26080;&#20851;&#65292;&#36825;&#19982;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#37197;&#23545;&#25968;&#25454;&#65288;&#22914;CLIP&#65289;&#25110;&#20132;&#38169;&#25968;&#25454;&#30340;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;-&#32473;&#23450;&#30446;&#26631;&#27169;&#24577;&#21644;&#35774;&#35745;&#29992;&#20110;&#35813;&#27169;&#24577;&#30340;Transformer&#65292;&#25105;&#20204;&#20351;&#29992;&#20351;&#29992;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#36741;&#21161;Transformer&#65292;&#24182;&#26500;&#24314;&#36335;&#24452;&#26469;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#30340;&#32452;&#20214;&#65292;&#20197;&#20415;&#30446;&#26631;&#27169;&#24577;&#30340;&#25968;&#25454;&#21487;&#20197;&#34987;&#20004;&#20010;&#27169;&#22411;&#22788;&#29702;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20174;&#20004;&#20010;&#27169;&#24577;&#33719;&#24471;&#30340;Transformer&#30340;&#36890;&#29992;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;&#20316;&#20026;&#20855;&#20307;&#23454;&#29616;&#65292;&#25105;&#20204;&#36890;&#24120;&#20351;&#29992;&#29305;&#23450;&#27169;&#24577;&#30340;tokenizer&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;head&#65292;&#20294;&#26159;&#21033;&#29992;&#36741;&#21161;&#27169;&#22411;&#30340;Transformer block&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3D-MoLM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#35821;&#35328;&#27169;&#22411;&#37197;&#22791;&#19968;&#20010;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#20998;&#23376;-&#25991;&#26412;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#65292;&#27492;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2401.13923</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;3D&#20998;&#23376;-&#25991;&#26412;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards 3D Molecule-Text Interpretation in Language Models. (arXiv:2401.13923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3D-MoLM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#35821;&#35328;&#27169;&#22411;&#37197;&#22791;&#19968;&#20010;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#20998;&#23376;-&#25991;&#26412;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#65292;&#27492;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#30528;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;3D&#20998;&#23376;&#32467;&#26500;&#30340;&#22266;&#26377;&#38480;&#21046;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;3D&#20998;&#23376;-&#25991;&#26412;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;3D-MoLM&#65306;3D&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;3D-MoLM&#36890;&#36807;&#20026;LM&#37197;&#22791;&#19968;&#20010;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;LM&#33021;&#22815;&#35299;&#37322;&#21644;&#20998;&#26512;3D&#20998;&#23376;&#12290;&#36825;&#31181;&#38598;&#25104;&#26159;&#36890;&#36807;&#19968;&#20010;3D&#20998;&#23376;-&#25991;&#26412;&#25237;&#24433;&#22120;&#23454;&#29616;&#30340;&#65292;&#23427;&#36830;&#25509;&#20102;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#21644;LM&#30340;&#36755;&#20837;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;3D-MoLM&#22312;&#36328;&#27169;&#24577;&#20998;&#23376;&#29702;&#35299;&#21644;&#25351;&#20196;&#36319;&#38543;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#20197;3D&#20998;&#23376;&#20026;&#20013;&#24515;&#30340;&#25351;&#24341;&#35843;&#25972;&#25968;&#25454;&#38598;--3D-MoIT&#12290;&#36890;&#36807;3D&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#21644;3D&#20998;&#23376;&#20013;&#24515;&#30340;&#25351;&#24341;&#35843;&#25972;&#65292;3D-MoLM&#24314;&#31435;&#20102;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#21644;LM&#30340;&#38598;&#25104;&#12290;&#23427;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13652</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#30340;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#19981;&#36830;&#32493;&#30028;&#38754;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#35299;&#20915;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#12290;&#35757;&#32451;&#36807;&#30340;GINNs&#22312;&#31232;&#30095;&#32593;&#26684;&#19978;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#28857;&#65292;&#24182;&#21033;&#29992;&#26500;&#24314;&#22312;&#32593;&#26684;&#19978;&#30340;&#22270;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#31639;&#27861;&#29992;&#20110;&#19968;&#33324;&#30340;&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#26131;&#20110;&#24212;&#29992;&#24615;&#12290;&#22312;&#32500;&#24230;n=2&#21644;n=4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;GINNs&#22312;&#26816;&#27979;&#19981;&#36830;&#32493;&#30028;&#38754;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;GINNs&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#21508;&#31181;&#31639;&#27861;&#20013;&#24182;&#20849;&#20139;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#22270;&#24418;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;Multi-G-UCB&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10383</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#22270;&#24418;&#36172;&#21338;&#26426;&#65306;UCB&#31639;&#27861;&#21644;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Agent Graph Bandits: UCB Algorithm and Regret Analysis. (arXiv:2401.10383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#22270;&#24418;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;Multi-G-UCB&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22810;&#26234;&#33021;&#20307;&#22270;&#24418;&#36172;&#21338;&#26426;&#38382;&#39064;&#24314;&#27169;&#20026;Zhang&#12289;Johansson&#21644;Li&#22312;[CISS 57, 1-6 (2023)]&#20013;&#25552;&#20986;&#30340;&#22270;&#24418;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#22810;&#26234;&#33021;&#20307;&#25193;&#23637;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;N&#20010;&#21512;&#20316;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#36830;&#36890;&#30340;&#22270;G&#19978;&#31227;&#21160;&#65292;&#22270;G&#26377;K&#20010;&#33410;&#28857;&#12290;&#25269;&#36798;&#27599;&#20010;&#33410;&#28857;&#26102;&#65292;&#26234;&#33021;&#20307;&#35266;&#23519;&#21040;&#20174;&#19968;&#20010;&#19982;&#33410;&#28857;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#38543;&#26426;&#25277;&#21462;&#30340;&#22870;&#21169;&#12290;&#31995;&#32479;&#22870;&#21169;&#34987;&#24314;&#27169;&#20026;&#26234;&#33021;&#20307;&#35266;&#27979;&#21040;&#30340;&#22870;&#21169;&#30340;&#21152;&#26435;&#21644;&#65292;&#20854;&#20013;&#26435;&#37325;&#34920;&#36798;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#23545;&#21516;&#19968;&#33410;&#28857;&#36827;&#34892;&#37319;&#26679;&#30340;&#36793;&#38469;&#20943;&#23569;&#22870;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;UCB&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Multi-G-UCB&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;T&#27493;&#20869;&#20854;&#26399;&#26395;&#36951;&#25022;&#34987;&#30028;&#23450;&#20026;$O(N\log(T)[\sqrt{KT} + DK])$&#65292;&#20854;&#20013;D&#26159;&#22270;G&#30340;&#30452;&#24452;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we formulate the multi-agent graph bandit problem as a multi-agent extension of the graph bandit problem introduced by Zhang, Johansson, and Li [CISS 57, 1-6 (2023)]. In our formulation, $N$ cooperative agents travel on a connected graph $G$ with $K$ nodes. Upon arrival at each node, agents observe a random reward drawn from a node-dependent probability distribution. The reward of the system is modeled as a weighted sum of the rewards the agents observe, where the weights capture the decreasing marginal reward associated with multiple agents sampling the same node at the same time. We propose an Upper Confidence Bound (UCB)-based learning algorithm, Multi-G-UCB, and prove that its expected regret over $T$ steps is bounded by $O(N\log(T)[\sqrt{KT} + DK])$, where $D$ is the diameter of graph $G$. Lastly, we numerically test our algorithm by comparing it to alternative methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#20219;&#21153;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#21644;&#21487;&#36716;&#31227;&#30340;&#24102;&#23485;&#20998;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;GNN&#21644;HML&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#36890;&#20449;&#22330;&#26223;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10253</link><description>&lt;p&gt;
&#28151;&#21512;&#20219;&#21153;&#20803;&#23398;&#20064;&#65306;&#19968;&#31181;&#29992;&#20110;&#21487;&#25193;&#23637;&#21644;&#21487;&#36716;&#31227;&#24102;&#23485;&#20998;&#37197;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation. (arXiv:2401.10253v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#20219;&#21153;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#21644;&#21487;&#36716;&#31227;&#30340;&#24102;&#23485;&#20998;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;GNN&#21644;HML&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#36890;&#20449;&#22330;&#26223;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24102;&#23485;&#20998;&#37197;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;1&#65289;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65307;2&#65289;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#36890;&#20449;&#22330;&#26223;&#19979;&#36827;&#34892;&#36716;&#31227;&#65292;&#20363;&#22914;&#38750;&#24179;&#31283;&#30340;&#26080;&#32447;&#20449;&#36947;&#12289;&#19981;&#21516;&#30340;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#21644;&#21160;&#24577;&#21487;&#29992;&#36164;&#28304;&#12290;&#20026;&#20102;&#25903;&#25345;&#21487;&#25193;&#23637;&#24615;&#65292;&#24102;&#23485;&#20998;&#37197;&#31574;&#30053;&#37319;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#31034;&#65292;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#38543;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#19981;&#21464;&#12290;&#20026;&#20102;&#23454;&#29616;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#20219;&#21153;&#20803;&#23398;&#20064;&#65288;HML&#65289;&#31639;&#27861;&#65292;&#22312;&#20803;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#36890;&#20449;&#22330;&#26223;&#26469;&#35757;&#32451;GNN&#30340;&#21021;&#22987;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#22312;&#20803;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23545;GNN&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#36890;&#20449;&#22330;&#26223;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;HML&#26041;&#27861;&#21487;&#20197;&#23558;&#21021;&#22987;&#24615;&#33021;&#25552;&#39640;8.79&#65285;&#65292;&#24182;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;73&#65285;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a deep learning-based bandwidth allocation policy that is: 1) scalable with the number of users and 2) transferable to different communication scenarios, such as non-stationary wireless channels, different quality-of-service (QoS) requirements, and dynamically available resources. To support scalability, the bandwidth allocation policy is represented by a graph neural network (GNN), with which the number of training parameters does not change with the number of users. To enable the generalization of the GNN, we develop a hybrid-task meta-learning (HML) algorithm that trains the initial parameters of the GNN with different communication scenarios during meta-training. Next, during meta-testing, a few samples are used to fine-tune the GNN with unseen communication scenarios. Simulation results demonstrate that our HML approach can improve the initial performance by $8.79\%$, and sampling efficiency by $73\%$, compared with existing benchmarks. After fine-tuning,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.09493</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#26377;&#20851;&#30340;&#19977;&#32500;&#36752;&#23556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification. (arXiv:2401.09493v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#36752;&#23556;&#21453;&#39304;&#24433;&#21709;&#20102;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#30340;&#24378;&#21270;&#65292;&#20294;&#29616;&#26377;&#35786;&#26029;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#20351;&#20854;&#26080;&#27861;&#29992;&#26469;&#30740;&#31350;&#19981;&#23545;&#31216;&#25110;&#30636;&#24577;&#30340;&#36752;&#23556;&#21152;&#28909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;VED&#65289;&#26469;&#23398;&#20064;&#36752;&#23556;&#19982;&#23454;&#38469;&#27169;&#25311;&#30340;&#27668;&#26059;&#34920;&#38754;&#24378;&#21270;&#20043;&#38388;&#30340;&#38544;&#34255;&#20851;&#31995;&#12290;&#38480;&#21046;VED&#27169;&#22411;&#30340;&#36755;&#20837;&#21487;&#20197;&#21033;&#29992;&#20854;&#19981;&#30830;&#23450;&#24615;&#26469;&#35782;&#21035;&#36752;&#23556;&#23545;&#24378;&#21270;&#26356;&#37325;&#35201;&#30340;&#26102;&#26399;&#12290;&#23545;&#25552;&#21462;&#30340;&#19977;&#32500;&#36752;&#23556;&#32467;&#26500;&#30340;&#32454;&#33268;&#26816;&#26597;&#34920;&#26126;&#65292;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#22312;&#25972;&#20307;&#19978;&#20855;&#26377;&#26368;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27973;&#20113;&#30340;&#19979;&#39118;&#22788;&#30340;&#28145;&#23545;&#27969;&#23545;&#28023;&#29141;&#30340;&#24378;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21457;&#29616;&#28909;&#21147;-&#21160;&#21147;&#23398;&#20851;&#31995;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#36724;&#23545;&#31216;&#25110;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud radiative feedback impacts early tropical cyclone (TC) intensification, but limitations in existing diagnostic frameworks make them unsuitable for studying asymmetric or transient radiative heating. We propose a linear Variational Encoder-Decoder (VED) to learn the hidden relationship between radiation and the surface intensification of realistic simulated TCs. Limiting VED model inputs enables using its uncertainty to identify periods when radiation has more importance for intensification. A close examination of the extracted 3D radiative structures suggests that longwave radiative forcing from inner core deep convection and shallow clouds both contribute to intensification, with the deep convection having the most impact overall. We find that deep convection downwind of the shallow clouds is critical to the intensification of Haiyan. Our work demonstrates that machine learning can discover thermodynamic-kinematic relationships without relying on axisymmetric or deterministic as
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#27010;&#29575;Lambert&#38382;&#39064;&#19982;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#12289;Schr\"odinger&#26725;&#21644;&#21453;&#24212;-&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#31561;&#39046;&#22495;&#36830;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#27714;&#35299;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07961</link><description>&lt;p&gt;
&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#19982;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#12289;Schr\"odinger&#26725;&#21644;&#21453;&#24212;-&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Solution of the Probabilistic Lambert Problem: Connections with Optimal Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs. (arXiv:2401.07961v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07961
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#27010;&#29575;Lambert&#38382;&#39064;&#19982;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#12289;Schr\"odinger&#26725;&#21644;&#21453;&#24212;-&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#31561;&#39046;&#22495;&#36830;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#27714;&#35299;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lambert&#38382;&#39064;&#28041;&#21450;&#36890;&#36807;&#36895;&#24230;&#25511;&#21046;&#22312;&#35268;&#23450;&#30340;&#39134;&#34892;&#26102;&#38388;&#20869;&#23558;&#33322;&#22825;&#22120;&#20174;&#32473;&#23450;&#30340;&#21021;&#22987;&#20301;&#32622;&#36716;&#31227;&#21040;&#32473;&#23450;&#30340;&#32456;&#31471;&#20301;&#32622;&#65292;&#21463;&#21040;&#37325;&#21147;&#21147;&#22330;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;Lambert&#38382;&#39064;&#30340;&#27010;&#29575;&#21464;&#31181;&#65292;&#20854;&#20013;&#20301;&#32622;&#21521;&#37327;&#30340;&#31471;&#28857;&#32422;&#26463;&#30340;&#30693;&#35782;&#34987;&#23427;&#20204;&#21508;&#33258;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#25152;&#26367;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#31471;&#28857;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#32422;&#26463;&#30340;Lambert&#38382;&#39064;&#26159;&#19968;&#20010;&#24191;&#20041;&#30340;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#65288;OMT&#65289;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#36825;&#20010;&#32463;&#20856;&#30340;&#22825;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#19982;&#29616;&#20195;&#38543;&#26426;&#25511;&#21046;&#21644;&#38543;&#26426;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20010;&#26032;&#21457;&#29616;&#30340;&#36830;&#25509;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#24314;&#31435;&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#21516;&#26679;&#30340;&#36830;&#25509;&#36824;&#24110;&#21161;&#36890;&#36807;&#25193;&#25955;&#27491;&#35268;&#21270;&#25968;&#20540;&#27714;&#35299;&#27010;&#29575;Lambert&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#36830;&#25509;&#26469;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lambert's problem concerns with transferring a spacecraft from a given initial to a given terminal position within prescribed flight time via velocity control subject to a gravitational force field. We consider a probabilistic variant of the Lambert problem where the knowledge of the endpoint constraints in position vectors are replaced by the knowledge of their respective joint probability density functions. We show that the Lambert problem with endpoint joint probability density constraints is a generalized optimal mass transport (OMT) problem, thereby connecting this classical astrodynamics problem with a burgeoning area of research in modern stochastic control and stochastic machine learning. This newfound connection allows us to rigorously establish the existence and uniqueness of solution for the probabilistic Lambert problem. The same connection also helps to numerically solve the probabilistic Lambert problem via diffusion regularization, i.e., by leveraging further connection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#19981;&#21516;&#27169;&#25311;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#21457;&#29616;&#23613;&#31649;&#25968;&#25454;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#65292;&#20294;&#23384;&#22312;&#20302;&#32500;&#19988;&#32531;&#24930;&#21464;&#21270;&#30340;&#26799;&#24230;&#23376;&#31354;&#38388;&#65292;&#36825;&#26377;&#21161;&#20110;&#26410;&#26469;&#26356;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.06604</link><description>&lt;p&gt;
&#35782;&#21035;&#31574;&#30053;&#26799;&#24230;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Identifying Policy Gradient Subspaces. (arXiv:2401.06604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#19981;&#21516;&#27169;&#25311;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#21457;&#29616;&#23613;&#31649;&#25968;&#25454;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#65292;&#20294;&#23384;&#22312;&#20302;&#32500;&#19988;&#32531;&#24930;&#21464;&#21270;&#30340;&#26799;&#24230;&#23376;&#31354;&#38388;&#65292;&#36825;&#26377;&#21161;&#20110;&#26410;&#26469;&#26356;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21033;&#29992;&#20248;&#21270;&#38382;&#39064;&#20869;&#37096;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#25552;&#39640;&#20854;&#35757;&#32451;&#25928;&#29575;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#20301;&#20110;&#20302;&#32500;&#19988;&#32531;&#24930;&#21464;&#21270;&#23376;&#31354;&#38388;&#20013;&#30340;&#20107;&#23454;&#65292;&#21487;&#20197;&#21152;&#36895;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#25311;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22266;&#26377;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#65292;&#20294;&#23384;&#22312;&#36825;&#26679;&#30340;&#26799;&#24230;&#23376;&#31354;&#38388;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#26356;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#65292;&#20363;&#22914;&#25913;&#36827;&#21442;&#25968;&#31354;&#38388;&#25506;&#32034;&#25110;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04536</link><description>&lt;p&gt;
&#36890;&#36807;&#35848;&#21028;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#12289;&#32452;&#32455;&#21644;&#25919;&#24220;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#23637;&#31034;&#31867;&#20284;&#20195;&#29702;&#34892;&#20026;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#38543;&#30528;LM&#34987;&#37319;&#29992;&#26469;&#25191;&#34892;&#36234;&#26469;&#36234;&#20855;&#26377;&#33258;&#20027;&#24615;&#30340;&#20219;&#21153;&#65292;&#36843;&#20999;&#38656;&#35201;&#21487;&#38752;&#19988;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#24403;&#21069;&#20027;&#35201;&#26159;&#38745;&#24577;&#30340;LM&#22522;&#20934;&#26080;&#27861;&#24456;&#22909;&#22320;&#35780;&#20272;&#27492;&#31867;&#21160;&#24577;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#26469;&#20849;&#21516;&#35780;&#20272;LM&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#20849;&#21516;&#20219;&#21153;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#35848;&#21028;&#28216;&#25103;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#35843;&#25972;&#22797;&#26434;&#24615;&#65292;&#24182;&#36991;&#20813;&#35780;&#20272;&#20013;&#30340;&#24847;&#22806;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26469;&#33258;&#20960;&#20010;&#20027;&#35201;&#20379;&#24212;&#21830;&#30340;&#20845;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;LM&#22312;&#21508;&#31181;&#35848;&#21028;&#28216;&#25103;&#19978;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;i&#65289;&#24320;&#28304;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#20943;&#23569;&#39046;&#22495;&#36716;&#31227;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03253</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#20943;&#23569;&#39046;&#22495;&#36716;&#31227;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#26368;&#26032;&#36827;&#23637;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#38754;&#23545;&#39046;&#22495;&#36716;&#31227;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#36328;&#39046;&#22495;&#23398;&#20064;&#26088;&#22312;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#35757;&#32451;&#19982;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#20013;&#65292;&#20256;&#32479;&#26041;&#27861;&#20165;&#20851;&#27880;&#22270;&#20687;&#27169;&#24577;&#65292;&#24573;&#35270;&#20102;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#26469;&#32531;&#35299;&#39046;&#22495;&#36716;&#31227;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#12290;LLaVO&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#35814;&#32454;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#22312;&#32463;&#36807;&#35774;&#35745;&#30340;&#25351;&#23548;&#27169;&#26495;&#29983;&#25104;&#30340;&#28304;&#22495;/&#30446;&#26631;&#22495;&#30340;&#25991;&#26412;&#25551;&#36848;&#19978;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#21508;&#31181;&#36328;&#39046;&#22495;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01851</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#21147;&#37327;&#65306;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#23545;&#33021;&#28304;&#38656;&#27714;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#21464;&#21270;&#23545;&#30456;&#24212;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25552;&#39640;&#21644;&#39640;&#24615;&#33021;&#30828;&#20214;&#30340;&#21019;&#26032;&#25512;&#21160;&#20102;&#22797;&#26434;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20294;&#20063;&#25903;&#25345;&#20102;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#28040;&#38544;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#21152;&#20154;&#20204;&#23545;&#19968;&#33324;&#35757;&#32451;&#21442;&#25968;&#21644;&#36807;&#31243;&#65288;&#20174;&#23398;&#20064;&#29575;&#21040;&#25209;&#37327;&#22823;&#23567;&#20877;&#21040;&#30693;&#35782;&#20256;&#36755;&#65289;&#30340;&#33021;&#28304;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21021;&#22987;&#21270;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#30828;&#20214;&#37197;&#32622;&#19978;&#35780;&#20272;&#22810;&#31181;&#35774;&#32622;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#22312;&#22522;&#20934;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.01519</link><description>&lt;p&gt;
&#25506;&#32034;LLMs&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#12290;&#24515;&#29702;&#23398;&#32463;&#21382;&#20102;&#20960;&#27425;&#29702;&#35770;&#21464;&#38761;&#65292;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;LLMs&#30340;&#20351;&#29992;&#26377;&#26395;&#24320;&#21551;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;LLMs&#22914;ChatGPT&#22312;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#36716;&#21464;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;LLMs&#22312;&#35748;&#30693;&#19982;&#34892;&#20026;&#24515;&#29702;&#23398;&#12289;&#20020;&#24202;&#19982;&#21672;&#35810;&#24515;&#29702;&#23398;&#12289;&#25945;&#32946;&#19982;&#21457;&#23637;&#24515;&#29702;&#23398;&#20197;&#21450;&#31038;&#20250;&#19982;&#25991;&#21270;&#24515;&#29702;&#23398;&#31561;&#24515;&#29702;&#23398;&#20998;&#25903;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#25991;&#26412;&#29983;&#25104;&#30340;&#33021;&#21147;&#65292;&#20026;&#24515;&#29702;&#23398;&#20013;&#30340;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#12289;&#23454;&#39564;&#23545;&#35937;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#23398;&#26415;&#20889;&#20316;&#21644;&#21516;&#34892;&#35780;&#23457;&#31561;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#12290;&#34429;&#28982;LLMs&#22312;&#25512;&#21160;&#30740;&#31350;&#26041;&#27861;&#23398;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2312.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#21644;&#20808;&#39564;&#30693;&#35782;&#22312;&#38544;&#31169;&#25915;&#20987;&#20013;&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24378;&#35843;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#25968;&#25454;&#24182;&#20165;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#65292;&#24378;&#35843;&#29992;&#25143;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26377;&#20851;&#38544;&#31169;&#25915;&#20987;&#30340;&#24037;&#20316;&#36890;&#36807;&#20174;&#32852;&#37030;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#30340;&#35757;&#32451;&#25991;&#26412;&#26469;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#25216;&#26415;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#38556;&#30861;&#65306;&#19968;&#20123;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#26377;&#38480;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026;1&#65289;&#65292;&#32780;&#20854;&#20182;&#25216;&#26415;&#21017;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#38590;&#20197;&#26816;&#27979;&#30340;&#29305;&#28857;&#65292;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#35774;&#32622;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#24674;&#22797;&#29575;&#12290;&#22522;&#20110;&#22522;&#26412;&#30340;&#26799;&#24230;&#21305;&#37197;&#21644;&#39046;&#22495;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#22686;&#24378;&#25915;&#20987;&#33021;&#21147;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#32423;&#21035;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#26799;&#24230;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#20449;&#21495;&#19981;&#20250;&#22312;&#21477;&#23376;&#21644;&#26631;&#35760;&#20043;&#38388;&#36827;&#34892;&#24179;&#22343;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
&lt;/p&gt;</description></item><item><title>Uni-O4&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#32541;&#20256;&#36882;&#65292;&#22686;&#24378;&#20102;&#23398;&#20064;&#33539;&#24335;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;Uni-O4&#21033;&#29992;&#22810;&#26679;&#30340;&#38598;&#21512;&#31574;&#30053;&#35299;&#20915;&#20102;&#20272;&#35745;&#34892;&#20026;&#31574;&#30053;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.03351</link><description>&lt;p&gt;
Uni-O4: &#23558;&#22312;&#32447;&#19982;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#36215;&#26469;&#65292;&#37319;&#29992;&#22810;&#27493;&#22312;&#32447;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization. (arXiv:2311.03351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03351
&lt;/p&gt;
&lt;p&gt;
Uni-O4&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#32541;&#20256;&#36882;&#65292;&#22686;&#24378;&#20102;&#23398;&#20064;&#33539;&#24335;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;Uni-O4&#21033;&#29992;&#22810;&#26679;&#30340;&#38598;&#21512;&#31574;&#30053;&#35299;&#20915;&#20102;&#20272;&#35745;&#34892;&#20026;&#31574;&#30053;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#23545;&#20110;&#39640;&#25928;&#21644;&#23433;&#20840;&#30340;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#23558;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#35270;&#20026;&#29420;&#31435;&#30340;&#36807;&#31243;&#65292;&#23548;&#33268;&#37325;&#22797;&#30340;&#35774;&#35745;&#21644;&#26377;&#38480;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Uni-o4&#65292;&#23427;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#20013;&#37117;&#20351;&#29992;&#20102;&#19968;&#20010;&#22312;&#32447;&#31574;&#30053;&#30446;&#26631;&#12290;&#30001;&#20110;&#20004;&#20010;&#38454;&#27573;&#30340;&#30446;&#26631;&#23545;&#40784;&#65292;RL&#20195;&#29702;&#21487;&#20197;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#20043;&#38388;&#26080;&#32541;&#20256;&#36882;&#12290;&#36825;&#31181;&#24615;&#36136;&#22686;&#24378;&#20102;&#23398;&#20064;&#33539;&#24335;&#30340;&#28789;&#27963;&#24615;&#65292;&#20801;&#35768;&#20219;&#24847;&#32452;&#21512;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#12289;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;Uni-o4&#21033;&#29992;&#22810;&#26679;&#30340;&#38598;&#21512;&#31574;&#30053;&#26469;&#35299;&#20915;&#20272;&#35745;&#34892;&#20026;&#31574;&#30053;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;
&lt;/p&gt;
&lt;p&gt;
Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization? In this study, we propose Uni-o4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-o4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;REBAR&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20449;&#24687;&#21644;&#37325;&#24314;&#23376;&#24207;&#21015;&#26469;&#26500;&#24314;&#27491;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;REBAR&#35823;&#24046;&#21487;&#20197;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#65292;&#24182;&#19988;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#38598;&#25104;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26377;&#29992;&#20449;&#24687;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2311.00519</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Based Reconstruction For Time-series Contrastive Learning. (arXiv:2311.00519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;REBAR&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20449;&#24687;&#21644;&#37325;&#24314;&#23376;&#24207;&#21015;&#26469;&#26500;&#24314;&#27491;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;REBAR&#35823;&#24046;&#21487;&#20197;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#65292;&#24182;&#19988;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#38598;&#25104;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26377;&#29992;&#20449;&#24687;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#37492;&#21035;&#20986;&#30340;&#27491;&#26679;&#26412;&#23545;&#65292;&#24403;&#23427;&#20204;&#34987;&#25512;&#21040;&#23884;&#20837;&#31354;&#38388;&#26102;&#65292;&#21487;&#20197;&#20026;&#21518;&#32493;&#30340;&#19979;&#28216;&#20219;&#21153;&#32534;&#30721;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#21487;&#33021;&#20250;&#30772;&#22351;&#21407;&#22987;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#25105;&#20204;&#20551;&#35774;&#22914;&#26524;&#25105;&#20204;&#33021;&#20174;&#19968;&#20010;&#23376;&#24207;&#21015;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#25104;&#21151;&#37325;&#24314;&#21478;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#37027;&#20040;&#23427;&#20204;&#24212;&#35813;&#26159;&#19968;&#20010;&#27491;&#26679;&#26412;&#23545;&#12290;&#22522;&#20110;&#36825;&#20010;&#30452;&#35273;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;REBAR&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21367;&#31215;&#20132;&#21449;&#27880;&#24847;&#21147;&#26550;&#26500;&#35745;&#31639;&#20004;&#20010;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;REBAR&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;REBAR&#35823;&#24046;&#26159;&#20114;&#30456;&#31867;&#21035;&#25104;&#21592;&#30340;&#39044;&#27979;&#22120;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#23427;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#30340;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#19968;&#26086;&#38598;&#25104;&#21040;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#30340;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
The success of self-supervised contrastive learning hinges on identifying positive data pairs that, when pushed together in embedding space, encode useful information for subsequent downstream tasks. However, in time-series, this is challenging because creating positive pairs via augmentations may break the original semantic meaning. We hypothesize that if we can retrieve information from one subsequence to successfully reconstruct another subsequence, then they should form a positive pair. Harnessing this intuition, we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR) contrastive learning. First, we utilize a convolutional cross-attention architecture to calculate the REBAR error between two different time-series. Then, through validation experiments, we show that the REBAR error is a predictor of mutual class membership, justifying its usage as a positive/negative labeler. Finally, once integrated into a contrastive learning framework, our REBAR method can learn an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30452;&#25509;&#23545;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#24212;&#29992;&#27867;&#21270;&#26041;&#27861;&#38450;&#27490;&#21028;&#21035;&#22120;&#30340;&#25439;&#22833;&#36807;&#20998;&#38477;&#20302;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00318</link><description>&lt;p&gt;
&#29992;&#20110;&#31283;&#23450;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27867;&#21270;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flooding Regularization for Stable Training of Generative Adversarial Networks. (arXiv:2311.00318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30452;&#25509;&#23545;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#24212;&#29992;&#27867;&#21270;&#26041;&#27861;&#38450;&#27490;&#21028;&#21035;&#22120;&#30340;&#25439;&#22833;&#36807;&#20998;&#38477;&#20302;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GAN&#35757;&#32451;&#23384;&#22312;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#26159;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#24120;&#20351;&#29992;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#21464;&#23545;&#25239;&#25439;&#22833;&#30340;&#31867;&#22411;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#30452;&#25509;&#23545;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#27867;&#21270;&#65288;&#36807;&#25311;&#21512;&#25233;&#21046;&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;GANs&#20013;&#65292;&#30452;&#25509;&#38450;&#27490;&#21028;&#21035;&#22120;&#30340;&#25439;&#22833;&#36807;&#20998;&#38477;&#20302;&#12290;&#27867;&#21270;&#38656;&#35201;&#35843;&#25972;&#27867;&#21270;&#27700;&#24179;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;GANs&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#36866;&#24403;&#30340;&#27867;&#21270;&#27700;&#24179;&#35774;&#32622;&#33539;&#22260;&#30001;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#30830;&#23450;&#65292;&#35813;&#35770;&#25454;&#24471;&#21040;&#20102;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#23545;GANs&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#27867;&#21270;&#31283;&#23450;&#20102;GAN&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#31283;&#23450;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#36890;&#36807;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have shown remarkable performance in image generation. However, GAN training suffers from the problem of instability. One of the main approaches to address this problem is to modify the loss function, often using regularization terms in addition to changing the type of adversarial losses. This paper focuses on directly regularizing the adversarial loss function. We propose a method that applies flooding, an overfitting suppression method in supervised learning, to GANs to directly prevent the discriminator's loss from becoming excessively low. Flooding requires tuning the flood level, but when applied to GANs, we propose that the appropriate range of flood level settings is determined by the adversarial loss function, supported by theoretical analysis of GANs using the binary cross entropy loss. We experimentally verify that flooding stabilizes GAN training and can be combined with other stabilization techniques. We also reveal that by restricting
&lt;/p&gt;</description></item><item><title>Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#20013;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21518;&#33021;&#20934;&#30830;&#39044;&#27979;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#24182;&#25512;&#26029;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#33021;&#29992;&#20110;&#39044;&#27979;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.00136</link><description>&lt;p&gt;
Neuroformer&#65306;&#29992;&#20110;&#33041;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00136
&lt;/p&gt;
&lt;p&gt;
Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#20013;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21518;&#33021;&#20934;&#30830;&#39044;&#27979;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#24182;&#25512;&#26029;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#33021;&#29992;&#20110;&#39044;&#27979;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#20135;&#29983;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#38656;&#35201;&#26032;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;&#21463;&#21040;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22823;&#35268;&#27169;&#30340;&#32454;&#32990;&#20998;&#36776;&#29575;&#31070;&#32463;&#20803;&#23574;&#23792;&#25968;&#25454;&#30340;&#20998;&#26512;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#26102;&#31354;&#29983;&#25104;&#38382;&#39064;&#12290;Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;transformer&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#19987;&#20026;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35774;&#35745;&#12290;&#23427;&#19982;&#29305;&#24449;&#22823;&#23567;&#21576;&#32447;&#24615;&#25193;&#23637;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#27169;&#24577;&#65292;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#39044;&#27979;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;Neuroformer&#65292;&#24182;&#21457;&#29616;&#23427;&#26082;&#33021;&#20934;&#30830;&#39044;&#27979;&#27169;&#25311;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#65292;&#20063;&#33021;&#20869;&#22312;&#22320;&#25512;&#26029;&#20986;&#24213;&#23618;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21253;&#25324;&#26041;&#21521;&#12290;&#24403;&#39044;&#35757;&#32451;&#29992;&#20110;&#35299;&#30721;&#31070;&#32463;&#21709;&#24212;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#39044;&#27979;&#23567;&#40736;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#29992;&#20110;&#25552;&#39640;&#28436;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18247</link><description>&lt;p&gt;
&#20026;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#25552;&#20379;&#25351;&#23548;&#24615;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning. (arXiv:2310.18247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#29992;&#20110;&#25552;&#39640;&#28436;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#19987;&#23478;&#32423;&#28436;&#31034;&#30340;&#38590;&#24230;&#38480;&#21046;&#20102;&#28436;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65306;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#24456;&#26114;&#36149;&#65292;&#24182;&#19988;&#28436;&#31034;&#30340;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#28436;&#31034;&#32773;&#30340;&#33021;&#21147;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#19968;&#20123;&#24037;&#20316;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#24265;&#20215;&#29983;&#25104;&#39069;&#22806;&#30340;&#28436;&#31034;&#25968;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20197;&#38543;&#26426;&#26041;&#24335;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#26368;&#32456;&#20135;&#29983;&#39640;&#24230;&#27425;&#20248;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22686;&#24378;&#25968;&#25454;&#12290;GuDA&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#65292;&#34429;&#28982;&#28436;&#31034;&#21160;&#20316;&#24207;&#21015;&#21487;&#33021;&#24456;&#38590;&#23637;&#31034;&#20135;&#29983;&#19987;&#23478;&#25968;&#25454;&#25152;&#38656;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#20294;&#29992;&#25143;&#32463;&#24120;&#21487;&#20197;&#36731;&#26494;&#22320;&#36776;&#21035;&#20986;&#22686;&#24378;&#36712;&#36857;&#27573;&#34920;&#31034;&#30340;&#20219;&#21153;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#21487;&#20197;&#26045;&#21152;&#19968;&#31995;&#21015;s
&lt;/p&gt;
&lt;p&gt;
Learning from demonstration (LfD) is a popular technique that uses expert demonstrations to learn robot control policies. However, the difficulty in acquiring expert-quality demonstrations limits the applicability of LfD methods: real-world data collection is often costly, and the quality of the demonstrations depends greatly on the demonstrator's abilities and safety concerns. A number of works have leveraged data augmentation (DA) to inexpensively generate additional demonstration data, but most DA works generate augmented data in a random fashion and ultimately produce highly suboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a human-guided DA framework that generates expert-quality augmented data. The key insight of GuDA is that while it may be difficult to demonstrate the sequence of actions required to produce expert data, a user can often easily identify when an augmented trajectory segment represents task progress. Thus, the user can impose a series of s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#65292;&#21160;&#21147;&#23398;&#19981;&#21464;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#23545;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#26032;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#29366;&#24577;-&#21160;&#20316;&#35206;&#30422;&#29575;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17786</link><description>&lt;p&gt;
&#29702;&#35299;&#20309;&#26102;&#21160;&#21147;&#23398;&#19981;&#21464;&#30340;&#25968;&#25454;&#22686;&#24378;&#23545;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#26032;&#26377;&#30410;
&lt;/p&gt;
&lt;p&gt;
Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates. (arXiv:2310.17786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#65292;&#21160;&#21147;&#23398;&#19981;&#21464;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#23545;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#26032;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#29366;&#24577;-&#21160;&#20316;&#35206;&#30422;&#29575;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#20197;&#20302;&#25104;&#26412;&#20135;&#29983;&#39069;&#22806;&#25968;&#25454;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24448;&#24448;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#23558;&#22686;&#24378;&#25968;&#25454;&#30452;&#25509;&#32435;&#20837;&#27169;&#22411;&#26080;&#20851;&#30340;RL&#26356;&#26032;&#20013;&#30340;&#25928;&#29992;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#22826;&#28165;&#26970;&#29305;&#23450;&#30340;DA&#31574;&#30053;&#20309;&#26102;&#20250;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#20986;DA&#30340;&#19968;&#33324;&#26041;&#38754;&#65292;&#20197;&#30830;&#23450;&#23548;&#33268;&#35266;&#23519;&#21040;&#30340;&#23398;&#20064;&#25913;&#36827;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20855;&#26377;&#21160;&#21147;&#23398;&#19981;&#21464;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#30340;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#19978;&#65292;&#36825;&#26159;&#29702;&#35299;DA&#21450;&#20854;&#19982;RL&#35757;&#32451;&#25972;&#21512;&#30340;&#26356;&#19968;&#33324;&#30340;&#29702;&#35299;&#30340;&#19968;&#20010;&#21021;&#22987;&#27493;&#39588;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#20998;&#31163;&#20102;&#19977;&#20010;&#19982;DA&#30456;&#20851;&#30340;&#26041;&#38754;&#65306;&#29366;&#24577;-&#21160;&#20316;&#35206;&#30422;&#29575;&#65292;&#22870;&#21169;&#23494;&#24230;&#21644;&#27599;&#27425;&#26356;&#26032;&#29983;&#25104;&#30340;&#22686;&#24378;&#36716;&#25442;&#30340;&#25968;&#37327;&#65288;&#22686;&#24378;&#22238;&#25918;&#29575;&#65289;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#24471;&#20986;&#20004;&#20010;&#32467;&#35770;&#65306;(1) &#22686;&#21152;&#29366;&#24577;-&#21160;&#20316;&#35206;&#30422;&#29575;&#21487;&#25913;&#36827;&#23398;&#20064;&#25928;&#26524;&#65307;
&lt;/p&gt;
&lt;p&gt;
Recently, data augmentation (DA) has emerged as a method for leveraging domain knowledge to inexpensively generate additional data in reinforcement learning (RL) tasks, often yielding substantial improvements in data efficiency. While prior work has demonstrated the utility of incorporating augmented data directly into model-free RL updates, it is not well-understood when a particular DA strategy will improve data efficiency. In this paper, we seek to identify general aspects of DA responsible for observed learning improvements. Our study focuses on sparse-reward tasks with dynamics-invariant data augmentation functions, serving as an initial step towards a more general understanding of DA and its integration into RL training. Experimentally, we isolate three relevant aspects of DA: state-action coverage, reward density, and the number of augmented transitions generated per update (the augmented replay ratio). From our experiments, we draw two conclusions: (1) increasing state-action c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.17513</link><description>&lt;p&gt;
&#12298;&#20302;&#31209;&#36866;&#24212;&#30340;&#34920;&#36798;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#30697;&#38453;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;LoRA&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#29702;&#35770;&#35282;&#24230;&#20998;&#26512;LoRA&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#39318;&#27425;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#65292;&#21017;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#24403;LoRA-rank&#20302;&#20110;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#31995;&#32479;&#34892;&#20026;&#65292;&#19981;&#38656;&#35201;&#22240;&#26524;&#22270;&#30340;&#24418;&#24335;&#21270;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#24178;&#39044;&#30340;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.17405</link><description>&lt;p&gt;
&#24102;&#26377;&#24179;&#31283;&#25193;&#25955;&#30340;&#22240;&#26524;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Causal Modeling with Stationary Diffusions. (arXiv:2310.17405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#31995;&#32479;&#34892;&#20026;&#65292;&#19981;&#38656;&#35201;&#22240;&#26524;&#22270;&#30340;&#24418;&#24335;&#21270;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#24178;&#39044;&#30340;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;&#19982;&#20351;&#29992;&#22240;&#26524;&#22270;&#30340;&#32467;&#26500;&#26041;&#31243;&#19981;&#21516;&#65292;&#25105;&#20204;&#23398;&#20064;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#65292;&#20854;&#24179;&#31283;&#23494;&#24230;&#21487;&#20197;&#27169;&#25311;&#31995;&#32479;&#22312;&#24178;&#39044;&#19979;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#24179;&#31283;&#25193;&#25955;&#27169;&#22411;&#19981;&#38656;&#35201;&#22240;&#26524;&#22270;&#30340;&#24418;&#24335;&#21270;&#65292;&#26356;&#19981;&#38656;&#35201;&#24120;&#35265;&#30340;&#26080;&#29615;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#21464;&#37327;&#19978;&#30340;&#26410;&#35265;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#25512;&#26029;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#34920;&#36798;&#20102;&#25193;&#25955;&#30340;&#29983;&#25104;&#22120;&#30340;&#31283;&#23450;&#26465;&#20214;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26680;&#20174;&#31283;&#24577;&#20559;&#31163;(KDS)&#26159;&#19968;&#20010;&#20540;&#24471;&#29420;&#31435;&#20851;&#27880;&#30340;&#23458;&#35266;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel approach towards causal inference. Rather than structural equations over a causal graph, we learn stochastic differential equations (SDEs) whose stationary densities model a system's behavior under interventions. These stationary diffusion models do not require the formalism of causal graphs, let alone the common assumption of acyclicity. We show that in several cases, they generalize to unseen interventions on their variables, often better than classical approaches. Our inference method is based on a new theoretical result that expresses a stationarity condition on the diffusion's generator in a reproducing kernel Hilbert space. The resulting kernel deviation from stationarity (KDS) is an objective function of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#23558;&#35777;&#26126;&#30340;&#33539;&#22260;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#26435;&#37325;&#25193;&#23637;&#21040;&#20102;&#26356;&#22823;&#30340;&#26435;&#37325;&#20998;&#24067;&#31867;&#21035;(PSEUDO-IID)&#65292;&#21253;&#25324;&#20302;&#31209;&#21644;&#31232;&#30095;&#35774;&#32622;&#12290;&#20316;&#32773;&#21457;&#29616;&#20351;&#29992;PSEUDO-IID&#20998;&#24067;&#21021;&#22987;&#21270;&#30340;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#22312;&#26041;&#24046;&#19978;&#37117;&#26159;&#31561;&#25928;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#30028;&#28151;&#27788;&#29366;&#24577;&#65292;&#24182;&#36827;&#34892;&#24615;&#33021;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2310.16597</link><description>&lt;p&gt;
&#36229;&#36234;&#29420;&#31435;&#21516;&#20998;&#24067;&#26435;&#37325;&#65306;&#31232;&#30095;&#21644;&#20302;&#31209;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20063;&#26159;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes. (arXiv:2310.16597v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#23558;&#35777;&#26126;&#30340;&#33539;&#22260;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#26435;&#37325;&#25193;&#23637;&#21040;&#20102;&#26356;&#22823;&#30340;&#26435;&#37325;&#20998;&#24067;&#31867;&#21035;(PSEUDO-IID)&#65292;&#21253;&#25324;&#20302;&#31209;&#21644;&#31232;&#30095;&#35774;&#32622;&#12290;&#20316;&#32773;&#21457;&#29616;&#20351;&#29992;PSEUDO-IID&#20998;&#24067;&#21021;&#22987;&#21270;&#30340;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#22312;&#26041;&#24046;&#19978;&#37117;&#26159;&#31561;&#25928;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#30028;&#28151;&#27788;&#29366;&#24577;&#65292;&#24182;&#36827;&#34892;&#24615;&#33021;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#23485;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#26377;&#29992;&#19988;&#21487;&#31649;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#35768;&#22810;&#29616;&#35937;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#38543;&#26426;&#28145;&#23618;&#32593;&#32476;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#65292;&#20174;&#32780;&#33021;&#22815;&#23545;&#28608;&#27963;&#20989;&#25968;&#21644;&#32593;&#32476;&#26435;&#37325;&#36873;&#25321;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#24433;&#21709;&#36827;&#34892;&#20005;&#26684;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Matthews&#31561;&#20154;(2018)&#30340;&#24320;&#21019;&#24615;&#35777;&#26126;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#21021;&#22987;&#26435;&#37325;&#20998;&#24067;&#31867;&#21035;(&#25105;&#20204;&#31216;&#20043;&#20026;PSEUDO-IID)&#65292;&#20854;&#20013;&#21253;&#25324;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#27491;&#20132;&#26435;&#37325;&#30340;&#24050;&#26377;&#24773;&#20917;&#65292;&#20197;&#21450;&#22240;&#20854;&#35745;&#31639;&#21152;&#36895;&#20248;&#21183;&#32780;&#21463;&#21040;&#36190;&#35465;&#30340;&#26032;&#20852;&#20302;&#31209;&#21644;&#32467;&#26500;&#31232;&#30095;&#35774;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;PSEUDO-IID&#20998;&#24067;&#21021;&#22987;&#21270;&#30340;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#22312;&#26041;&#24046;&#19978;&#37117;&#26159;&#31561;&#25928;&#30340;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#35782;&#21035;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#30028;&#28151;&#27788;&#29366;&#24577;&#65292;&#24182;&#35843;&#25972;&#23427;&#20204;&#30340;&#20020;&#30028;&#24615;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that allows a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics. In this paper, we extend the seminal proof of Matthews et al. (2018) to a larger class of initial weight distributions (which we call PSEUDO-IID), including the established cases of IID and orthogonal weights, as well as the emerging low-rank and structured sparse settings celebrated for their computational speed-up benefits. We show that fully-connected and convolutional networks initialized with PSEUDO-IID distributions are all effectively equivalent up to their variance. Using our results, one can identify the Edge-of-Chaos for a broader class of neural networks and tune them at criticality in order to enhance the
&lt;/p&gt;</description></item><item><title>MARVEL&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#21033;&#29992;&#24120;&#35265;&#25968;&#25454;&#22312;&#39640;&#36895;&#20844;&#36335;&#36208;&#24266;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#21487;&#21464;&#36895;&#38480;&#25511;&#21046;&#12290;&#23427;&#36890;&#36807;&#22870;&#21169;&#32467;&#26500;&#21644;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#35843;&#65292;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;&#21644;&#27969;&#21160;&#24615;&#12290;&#19982;&#26080;&#25511;&#21046;&#24773;&#20917;&#30456;&#27604;&#65292;MARVEL&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;63.4%&#65292;&#25552;&#39640;&#20132;&#36890;&#27969;&#21160;&#24615;14.6%&#12290;</title><link>http://arxiv.org/abs/2310.12359</link><description>&lt;p&gt;
MARVEL: &#29992;&#20110;&#22823;&#35268;&#27169;&#21487;&#21464;&#36895;&#38480;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits. (arXiv:2310.12359v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12359
&lt;/p&gt;
&lt;p&gt;
MARVEL&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#21033;&#29992;&#24120;&#35265;&#25968;&#25454;&#22312;&#39640;&#36895;&#20844;&#36335;&#36208;&#24266;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#21487;&#21464;&#36895;&#38480;&#25511;&#21046;&#12290;&#23427;&#36890;&#36807;&#22870;&#21169;&#32467;&#26500;&#21644;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#35843;&#65292;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;&#21644;&#27969;&#21160;&#24615;&#12290;&#19982;&#26080;&#25511;&#21046;&#24773;&#20917;&#30456;&#27604;&#65292;MARVEL&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;63.4%&#65292;&#25552;&#39640;&#20132;&#36890;&#27969;&#21160;&#24615;14.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#21464;&#36895;&#38480;&#65288;VSL&#65289;&#25511;&#21046;&#26159;&#19968;&#31181;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#27969;&#21160;&#24615;&#30340;&#26377;&#21069;&#36884;&#30340;&#20132;&#36890;&#31649;&#29702;&#31574;&#30053;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;MARVEL&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#20165;&#26377;&#24120;&#35265;&#21487;&#29992;&#25968;&#25454;&#23454;&#29616;&#39640;&#36895;&#20844;&#36335;&#36208;&#24266;&#22823;&#35268;&#27169;VSL&#25511;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#21253;&#25324;&#23545;&#20132;&#36890;&#29366;&#20917;&#30340;&#36866;&#24212;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#27969;&#21160;&#24615;&#22312;&#20869;&#30340;&#22870;&#21169;&#32467;&#26500;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#35843;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#25152;&#26377;VSL&#26234;&#33021;&#20307;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#35768;&#22810;&#31435;&#26609;&#30340;&#36208;&#24266;&#12290;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#22522;&#20110;&#19968;&#20010;&#30701;&#30340;&#39640;&#36895;&#20844;&#36335;&#36335;&#27573;&#30340;&#24494;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#36335;&#27573;&#26377;8&#20010;&#31435;&#26609;&#65292;&#36328;&#36234;7&#33521;&#37324;&#65292;&#24182;&#22312;&#32435;&#20160;&#32500;&#23572;&#38468;&#36817;&#30340;I-24&#19978;&#26377;34&#20010;&#31435;&#26609;&#65292;&#36328;&#36234;17&#33521;&#37324;&#36827;&#34892;&#27979;&#35797;&#12290;&#19982;&#26080;&#25511;&#21046;&#24773;&#20917;&#30456;&#27604;&#65292;MARVEL&#23558;&#20132;&#36890;&#23433;&#20840;&#24615;&#25552;&#39640;&#20102;63.4%&#65292;&#24182;&#23558;&#20132;&#36890;&#27969;&#21160;&#24615;&#25552;&#39640;&#20102;14.6%&#65292;&#19982;&#24050;&#22312;I-24&#19978;&#37096;&#32626;&#30340;&#26368;&#26032;&#31639;&#27861;&#30456;&#27604;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variable speed limit (VSL) control is a promising traffic management strategy for enhancing safety and mobility. This work introduces MARVEL, a multi-agent reinforcement learning (MARL) framework for implementing large-scale VSL control on freeway corridors using only commonly available data. The agents learn through a reward structure that incorporates adaptability to traffic conditions, safety, and mobility; enabling coordination among the agents. The proposed framework scales to cover corridors with many gantries thanks to a parameter sharing among all VSL agents. The agents are trained in a microsimulation environment based on a short freeway stretch with 8 gantries spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no control scenario and enhances traffic mobility by 14.6% compared to a state-of-the-practice algorithm that has been deployed on I-24. An explainability analysis is underta
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#21457;&#20986;&#22768;&#38899;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#22522;&#20110;&#20381;&#24651;&#29702;&#35770;&#30340;&#33258;&#25105;&#20381;&#24651;&#25216;&#26415;&#12290;&#36890;&#36807;&#20351;&#29992;&#35268;&#21017;&#21644;&#20998;&#31867;&#27169;&#22359;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#36755;&#20837;&#24182;&#25512;&#33616;&#36866;&#24403;&#30340;&#33258;&#25105;&#20381;&#24651;&#32451;&#20064;&#12290;&#35813;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20934;&#30830;&#29575;&#36229;&#36807;92%&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22359;&#65292;&#20197;&#35782;&#21035;&#29992;&#25143;&#24773;&#24863;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#22312;&#21518;&#30123;&#24773;&#26102;&#20195;&#25552;&#20379;&#25968;&#23383;&#24515;&#29702;&#30103;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.09362</link><description>&lt;p&gt;
&#20174;&#35789;&#35821;&#21644;&#32451;&#20064;&#21040;&#20581;&#24247;&#65306;&#29992;&#20110;&#33258;&#25105;&#20381;&#24651;&#25216;&#26415;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique. (arXiv:2310.09362v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#21457;&#20986;&#22768;&#38899;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#22522;&#20110;&#20381;&#24651;&#29702;&#35770;&#30340;&#33258;&#25105;&#20381;&#24651;&#25216;&#26415;&#12290;&#36890;&#36807;&#20351;&#29992;&#35268;&#21017;&#21644;&#20998;&#31867;&#27169;&#22359;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#36755;&#20837;&#24182;&#25512;&#33616;&#36866;&#24403;&#30340;&#33258;&#25105;&#20381;&#24651;&#32451;&#20064;&#12290;&#35813;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20934;&#30830;&#29575;&#36229;&#36807;92%&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22359;&#65292;&#20197;&#35782;&#21035;&#29992;&#25143;&#24773;&#24863;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#22312;&#21518;&#30123;&#24773;&#26102;&#20195;&#25552;&#20379;&#25968;&#23383;&#24515;&#29702;&#30103;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21518;&#30123;&#24773;&#26102;&#20195;&#65292;&#31038;&#20132;&#23396;&#31435;&#21644;&#25233;&#37057;&#28966;&#34385;&#30151;&#30340;&#24739;&#30149;&#29575;&#25856;&#21319;&#30340;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#25968;&#23383;&#24515;&#29702;&#30103;&#27861;&#30340;&#23545;&#35805;&#20195;&#29702;&#30456;&#23545;&#20110;&#20256;&#32479;&#30103;&#27861;&#20250;&#21457;&#25381;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#21457;&#20986;&#22768;&#38899;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#33258;&#25105;&#20381;&#24651;(Self-Attachment, SAT)&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20381;&#24651;&#29702;&#35770;&#30340;&#26032;&#22411;&#12289;&#33258;&#25105;&#31649;&#29702;&#12289;&#20840;&#38754;&#30340;&#24515;&#29702;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;&#19968;&#31995;&#21015;&#22522;&#20110;&#35268;&#21017;&#21644;&#20998;&#31867;&#30340;&#27169;&#22359;&#26469;&#29702;&#35299;&#29992;&#25143;&#22312;&#23545;&#35805;&#20013;&#30340;&#36755;&#20837;&#65292;&#24182;&#30456;&#24212;&#22320;&#23548;&#33322;&#23545;&#35805;&#27969;&#31243;&#22270;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#24773;&#24863;&#21644;&#24515;&#29702;&#29366;&#24577;&#25512;&#33616;&#36866;&#24403;&#30340;SAT&#32451;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#36229;&#36807;6,000&#27425;&#35805;&#35821;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22359;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#24773;&#24863;&#20998;&#20026;12&#20010;&#31867;&#21035;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;92%&#12290;&#20026;&#20102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26032;&#39062;&#21644;&#21560;&#24341;&#21147;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#26159;&#20174;&#22823;&#37327;&#35805;&#35821;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of the post-pandemic era, marked by social isolation and surging rates of depression and anxiety, conversational agents based on digital psychotherapy can play an influential role compared to traditional therapy sessions. In this work, we develop a voice-capable chatbot in Farsi to guide users through Self-Attachment (SAT), a novel, self-administered, holistic psychological technique based on attachment theory. Our chatbot uses a dynamic array of rule-based and classification-based modules to comprehend user input throughout the conversation and navigates a dialogue flowchart accordingly, recommending appropriate SAT exercises that depend on the user's emotional and mental state. In particular, we collect a dataset of over 6,000 utterances and develop a novel sentiment-analysis module that classifies user sentiment into 12 classes, with accuracy above 92%. To keep the conversation novel and engaging, the chatbot's responses are retrieved from a large dataset of utterances c
&lt;/p&gt;</description></item><item><title>&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#20998;&#35789;&#22120;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#19981;&#19968;&#23450;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08754</link><description>&lt;p&gt;
LLM&#35757;&#32451;&#20013;&#30340;&#20998;&#35789;&#36873;&#25321;&#65306;&#24494;&#19981;&#36275;&#36947;&#36824;&#26159;&#33267;&#20851;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Tokenizer Choice For LLM Training: Negligible or Crucial?. (arXiv:2310.08754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08754
&lt;/p&gt;
&lt;p&gt;
&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#20998;&#35789;&#22120;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#19981;&#19968;&#23450;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;LLM&#30340;&#25104;&#21151;&#20027;&#35201;&#26159;&#30001;&#20110;&#31574;&#21010;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#25193;&#23637;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36827;&#27493;&#65292;&#32780;&#20998;&#35789;&#22120;&#30340;&#24433;&#21709;&#21017;&#26159;&#19968;&#20010;&#30450;&#28857;&#12290;&#36890;&#36807;&#23545;24&#20010;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#31639;&#27861;&#21644;&#21442;&#25968;&#36827;&#34892;&#22823;&#33539;&#22260;&#23454;&#39564;&#65292;&#25105;&#20204;&#23545;&#20998;&#35789;&#22120;&#36873;&#25321;&#23545;LLM&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20998;&#35789;&#22120;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;&#20016;&#23500;&#24230;&#21644;&#24179;&#31561;&#24615;&#65289;&#24182;&#19981;&#24635;&#26159;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#25351;&#26631;&#25104;&#20026;&#23545;&#20998;&#35789;&#22120;&#35780;&#20272;&#30340;&#21487;&#30097;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38024;&#23545;&#20116;&#31181;&#26368;&#24120;&#35265;&#30340;&#27431;&#27954;&#35821;&#35328;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#38656;&#35201;&#35789;&#27719;&#34920;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable choice for tokenizer evaluation. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#26426;&#32452;&#21551;&#20572;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#39044;&#27979;&#24120;&#35268;&#26426;&#32452;&#30340;&#21551;&#20572;&#20915;&#31574;&#65292;&#31995;&#32479;&#36816;&#33829;&#21830;&#21487;&#20197;&#22312;&#27714;&#35299;&#22120;&#20013;&#20351;&#29992;&#39044;&#28909;&#21551;&#21160;&#24182;&#26174;&#33879;&#21152;&#36895;&#35745;&#31639;&#12290;&#23545;&#20110;&#39044;&#27979;&#65292;&#20351;&#29992;&#20102;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#26680;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#65292;&#33021;&#23558;&#35745;&#31639;&#26102;&#38388;&#20943;&#23569;1.7&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.08601</link><description>&lt;p&gt;
&#24102;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#26426;&#32452;&#21551;&#20572;&#39044;&#27979;&#22120;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unit Commitment Predictor With a Performance Guarantee: A Support Vector Machine Classifier. (arXiv:2310.08601v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#26426;&#32452;&#21551;&#20572;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#39044;&#27979;&#24120;&#35268;&#26426;&#32452;&#30340;&#21551;&#20572;&#20915;&#31574;&#65292;&#31995;&#32479;&#36816;&#33829;&#21830;&#21487;&#20197;&#22312;&#27714;&#35299;&#22120;&#20013;&#20351;&#29992;&#39044;&#28909;&#21551;&#21160;&#24182;&#26174;&#33879;&#21152;&#36895;&#35745;&#31639;&#12290;&#23545;&#20110;&#39044;&#27979;&#65292;&#20351;&#29992;&#20102;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#26680;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#65292;&#33021;&#23558;&#35745;&#31639;&#26102;&#38388;&#20943;&#23569;1.7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#36816;&#33829;&#21830;&#36890;&#24120;&#38656;&#35201;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#26426;&#32452;&#21551;&#20572;&#38382;&#39064;&#36827;&#34892;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#36890;&#36807;&#23398;&#20064;&#21644;&#39044;&#27979;&#24120;&#35268;&#26426;&#32452;&#30340;&#21551;&#20572;&#20915;&#31574;&#65292;&#31995;&#32479;&#36816;&#33829;&#21830;&#26377;&#21487;&#33021;&#22312;&#27714;&#35299;&#22120;&#20013;&#20351;&#29992;&#39044;&#28909;&#21551;&#21160;&#24182;&#26174;&#33879;&#21152;&#36895;&#35745;&#31639;&#12290;&#23545;&#20110;&#39044;&#27979;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#32447;&#24615;&#21644;&#26680;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65288;&#22914;&#26524;&#36866;&#24403;&#27491;&#21017;&#21270;&#65289;&#65292;&#36716;&#21270;&#20026;&#20998;&#24067;&#40065;&#26834;&#20998;&#31867;&#22120;&#12290;&#23545;&#20110;&#26426;&#32452;&#21551;&#20572;&#38382;&#39064;&#65292;&#25105;&#20204;&#27714;&#35299;&#20102;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#20108;&#38454;&#38181;&#38382;&#39064;&#12290;&#22522;&#20110;IEEE 6&#33410;&#28857;&#21644;118&#33410;&#28857;&#27979;&#35797;&#31995;&#32479;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#26680;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#20248;&#20110;&#20854;&#20182;&#20998;&#31867;&#22120;&#65292;&#23558;&#35745;&#31639;&#26102;&#38388;&#20943;&#23569;&#20102;1.7&#20493;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23384;&#22312;&#20005;&#26684;&#30340;&#35745;&#31639;&#38480;&#21046;&#65292;&#27809;&#26377;&#39044;&#28909;&#21551;&#21160;&#30340;&#26426;&#32452;&#21551;&#20572;&#38382;&#39064;&#19982;&#26368;&#20248;&#35299;&#30340;&#36317;&#31163;&#36739;&#36828;&#12290;
&lt;/p&gt;
&lt;p&gt;
The system operators usually need to solve large-scale unit commitment problems within limited time frame for computation. This paper provides a pragmatic solution, showing how by learning and predicting the on/off commitment decisions of conventional units, there is a potential for system operators to warm start their solver and speed up their computation significantly. For the prediction, we train linear and kernelized support vector machine classifiers, providing an out-of-sample performance guarantee if properly regularized, converting to distributionally robust classifiers. For the unit commitment problem, we solve a mixed-integer second-order cone problem. Our results based on the IEEE 6-bus and 118-bus test systems show that the kernelized SVM with proper regularization outperforms other classifiers, reducing the computational time by a factor of 1.7. In addition, if there is a tight computational limit, while the unit commitment problem without warm start is far away from the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#36755;&#20986;&#32467;&#26524;&#36827;&#34892;&#30828;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#38169;&#35823;&#30340;&#32047;&#31215;&#29616;&#35937;&#65292;&#20445;&#35777;&#20102;&#31574;&#30053;&#25928;&#26524;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.06171</link><description>&lt;p&gt;
&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Memory-Consistent Neural Networks for Imitation Learning. (arXiv:2310.06171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#36755;&#20986;&#32467;&#26524;&#36827;&#34892;&#30828;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#38169;&#35823;&#30340;&#32047;&#31215;&#29616;&#35937;&#65292;&#20445;&#35777;&#20102;&#31574;&#30053;&#25928;&#26524;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#19987;&#23478;&#28436;&#31034;&#22823;&#22823;&#31616;&#21270;&#20102;&#31574;&#30053;&#21512;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#27169;&#20223;&#31574;&#30053;&#26469;&#35828;&#65292;&#36828;&#31163;&#35757;&#32451;&#26679;&#26412;&#30340;&#38169;&#35823;&#23588;&#20026;&#20851;&#38190;&#12290;&#21363;&#20351;&#22312;&#31574;&#30053;&#30340;&#34892;&#21160;&#36755;&#20986;&#20013;&#20986;&#29616;&#32597;&#35265;&#30340;&#38169;&#35823;&#65292;&#30001;&#20110;&#36825;&#20123;&#38169;&#35823;&#20250;&#23548;&#33268;&#19981;&#29087;&#24713;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#31574;&#30053;&#22312;&#36825;&#20123;&#29366;&#24577;&#19979;&#20173;&#26356;&#23481;&#26131;&#20986;&#38169;&#65292;&#26368;&#32456;&#23548;&#33268;&#20219;&#21153;&#22833;&#36133;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#31616;&#21333;&#30340;&#30417;&#30563;&#24335;&#8220;&#34892;&#20026;&#20811;&#38534;&#8221;&#26041;&#27861;&#65292;&#33021;&#22815;&#26041;&#20415;&#22320;&#20165;&#36890;&#36807;&#39044;&#20808;&#35760;&#24405;&#30340;&#28436;&#31034;&#26469;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33021;&#22815;&#25269;&#28040;&#38169;&#35823;&#32047;&#31215;&#29616;&#35937;&#30340;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#30340;&#8220;&#20869;&#23384;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;&#8221;(MCNN)&#36755;&#20986;&#34987;&#24378;&#21046;&#32422;&#26463;&#22312;&#19982;&#20856;&#22411;&#30340;&#8220;&#20869;&#23384;&#8221;&#35757;&#32451;&#26679;&#26412;&#30456;&#20851;&#30340;&#26126;&#30830;&#25351;&#23450;&#30340;&#20801;&#35768;&#21306;&#22495;&#20869;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;MCNN&#31574;&#30053;&#23548;&#33268;&#30340;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#20445;&#35777;&#19978;&#30028;&#12290;&#36890;&#36807;&#22312;9&#20010;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#20351;&#29992;MCNNs&#65292;&#37319;&#29992;MLP&#12289;Transformer&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#22522;&#20110;&#20998;&#25968;&#30340;&#21453;&#21521;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#65292;&#20294;&#20026;&#20102;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#39640;&#32500;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#38598;&#30340;&#38750;&#37325;&#21472;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#30456;&#21516;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;DNN&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02557</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#27867;&#21270;&#24615;&#36136;&#28304;&#20110;&#20960;&#20309;&#33258;&#36866;&#24212;&#30340;&#35856;&#27874;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generalization in diffusion models arises from geometry-adaptive harmonic representation. (arXiv:2310.02557v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02557
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#22522;&#20110;&#20998;&#25968;&#30340;&#21453;&#21521;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#65292;&#20294;&#20026;&#20102;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#39640;&#32500;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#38598;&#30340;&#38750;&#37325;&#21472;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#30456;&#21516;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;DNN&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#21453;&#21521;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#23613;&#31649;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#65292;&#20026;&#20102;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21487;&#20197;&#23398;&#20064;&#39640;&#32500;&#23494;&#24230;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35757;&#32451;&#38598;&#35760;&#24518;&#21270;&#30340;&#26368;&#26032;&#25253;&#21578;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#32593;&#32476;&#26159;&#21542;&#23398;&#20064;&#20102;&#25968;&#25454;&#30340;&#8220;&#30495;&#23454;&#8221;&#36830;&#32493;&#23494;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#65292;&#35757;&#32451;&#22312;&#25968;&#25454;&#38598;&#30340;&#38750;&#37325;&#21472;&#23376;&#38598;&#19978;&#30340;&#20004;&#20010;&#38477;&#22122;DNN&#23398;&#20064;&#30340;&#20960;&#20046;&#26159;&#30456;&#21516;&#30340;&#20998;&#25968;&#20989;&#25968;&#65292;&#20174;&#32780;&#23398;&#20064;&#20102;&#30456;&#21516;&#30340;&#23494;&#24230;&#65292;&#19988;&#20165;&#38656;&#24456;&#23569;&#30340;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#31181;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#35777;&#26126;&#20102;DNN&#26550;&#26500;&#21644;/&#25110;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#26377;&#21147;&#24402;&#32435;&#20559;&#24046;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#24615;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#38477;&#22122;&#22120;&#22312;&#36866;&#24212;&#20110;&#24213;&#23618;&#22270;&#20687;&#30340;&#22522;&#30784;&#19978;&#25191;&#34892;&#25910;&#32553;&#25805;&#20316;&#12290;&#23545;&#36825;&#20123;&#22522;&#30690;&#30340;&#26816;&#26597;&#25581;&#31034;&#20102;&#27839;&#36718;&#24275;&#21644;&#22343;&#21248;&#22270;&#20687;&#21306;&#22495;&#30340;&#25391;&#33633;&#35856;&#27874;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality samples generated with score-based reverse diffusion algorithms provide evidence that deep neural networks (DNN) trained for denoising can learn high-dimensional densities, despite the curse of dimensionality. However, recent reports of memorization of the training set raise the question of whether these networks are learning the "true" continuous density of the data. Here, we show that two denoising DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, with a surprisingly small number of training images. This strong generalization demonstrates an alignment of powerful inductive biases in the DNN architecture and/or training algorithm with properties of the data distribution. We analyze these, demonstrating that the denoiser performs a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous image region
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01820</link><description>&lt;p&gt;
&#36827;&#21521;&#40065;&#26834;&#24230;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks. (arXiv:2310.01820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#25968;&#25454;&#20013;&#30340;&#20381;&#36182;&#32467;&#26500;&#36890;&#36807;&#33410;&#28857;&#20043;&#38388;&#30340;&#28040;&#24687;&#20256;&#36882;&#36827;&#34892;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GNN&#24050;&#32463;&#25104;&#20026;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20851;&#38190;&#26550;&#26500;&#65292;&#22312;&#25935;&#24863;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#35201;&#27714;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#26377;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#36825;&#23601;&#38656;&#35201;&#19968;&#20010;GNN&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;GNN&#35299;&#37322;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#38656;&#35201;&#25552;&#20379;&#21487;&#38752;&#30340;&#20445;&#30495;&#24230;&#24230;&#37327;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#19968;&#22522;&#30784;&#24615;&#25361;&#25112;&#65292;&#37325;&#28857;&#22312;&#29616;&#26377;&#30340;&#20445;&#30495;&#24230;&#24230;&#37327;&#26041;&#27861;&#20013;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;Fid_+&#65292;Fid_-&#21644;Fid_&#916;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#20449;&#24687;&#35770;&#35299;&#37322;&#24615;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes -- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#24555;&#36895;&#27880;&#24847;&#21147;&#26426;&#21046;PolySketchFormer&#65292;&#20197;&#31361;&#30772;Transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38590;&#39064;&#65292;&#26080;&#38656;&#20551;&#35774;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#22522;&#20110;&#22359;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01655</link><description>&lt;p&gt;
PolySketchFormer:&#22522;&#20110;&#33609;&#22270;&#30340;&#22810;&#39033;&#24335;&#26680;&#21464;&#25442;&#22120;&#21152;&#36895;Transformer
&lt;/p&gt;
&lt;p&gt;
PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels. (arXiv:2310.01655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#24555;&#36895;&#27880;&#24847;&#21147;&#26426;&#21046;PolySketchFormer&#65292;&#20197;&#31361;&#30772;Transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38590;&#39064;&#65292;&#26080;&#38656;&#20551;&#35774;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#22522;&#20110;&#22359;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#19968;&#30452;&#26159;&#25193;&#23637;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#30340;&#29942;&#39048;&#12290;&#23454;&#38469;&#19978;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20551;&#35774;&#24378;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#20284;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36755;&#20986;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#26367;&#20195;softmax&#26469;&#31361;&#30772;&#36825;&#20010;&#29702;&#35770;&#38556;&#30861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25991;&#29486;&#20013;&#30340;&#22810;&#39033;&#24335;&#26680;&#30340;&#33609;&#22270;&#21487;&#20197;&#29992;&#20110;&#36817;&#20284;&#22810;&#39033;&#24335;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#24555;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32780;&#19981;&#38656;&#35201;&#20551;&#35774;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#32467;&#26500;&#65292;&#36825;&#22312;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#24050;&#32463;&#23436;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#22359;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22240;&#26524;&#25513;&#30721;&#24212;&#29992;&#20110;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#22320;&#35745;&#31639;$n \times n$&#27880;&#24847;&#21147;&#30697;&#38453;&#24182;&#35745;&#31639;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quadratic complexity of attention in transformer architectures remains a big bottleneck in scaling up large foundation models for long context. In fact, recent theoretical results show the hardness of approximating the output of softmax attention mechanism in sub-quadratic time assuming Strong Exponential Time Hypothesis. In this paper, we show how to break this theoretical barrier by replacing softmax with a polynomial function and polynomial sketching. In particular we show that sketches for Polynomial Kernel from the randomized numerical linear algebra literature can be used to approximate the polynomial attention which leads to a significantly faster attention mechanism without assuming any sparse structure for the attention matrix that has been done in many previous works.  In addition, we propose an efficient block-based algorithm that lets us apply the causal mask to the attention matrix without explicitly realizing the $n \times n$ attention matrix and compute the output of
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2310.01012</link><description>&lt;p&gt;
CCA&#23478;&#26063;&#30340;&#39640;&#25928;&#31639;&#27861;&#65306;&#26080;&#32422;&#26463;&#30446;&#26631;&#19982;&#26080;&#20559;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#23398;&#20064;&#20013;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#27491;&#21017;&#21270;&#32447;&#24615;CCA&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#30340;&#25512;&#24191;&#65292;&#24182;&#19982;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65288;GEP&#65289;&#26694;&#26550;&#32479;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#24615;&#26041;&#27861;&#30340;&#20256;&#32479;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;CCA&#30340;&#25193;&#23637;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#35757;&#32451;&#36807;&#31243;&#32531;&#24930;&#19988;&#22797;&#26434;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#25551;&#36848;GEPs&#30340;&#39030;&#32423;&#23376;&#31354;&#38388;&#30340;&#26032;&#39062;&#26080;&#32422;&#26463;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#26159;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24212;&#29992;&#20110;&#30456;&#24212;&#30340;CCA&#30446;&#26631;&#65292;&#20174;&#32780;&#33719;&#24471;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25152;&#26377;&#26631;&#20934;CCA&#21644;&#28145;&#24230;CCA&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;&#36825;&#26679;&#30340;&#36895;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39318;&#27425;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#29289;&#25968;&#25454;&#30340;PLS&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20154;&#30340;&#20381;&#20174;&#31243;&#24230;&#21644;&#26426;&#22120;&#25552;&#20379;&#24314;&#35758;&#30340;&#26102;&#26426;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#26368;&#20339;&#30340;&#24314;&#35758;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.00817</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#25552;&#20379;&#27880;&#37325;&#20381;&#20174;&#24615;&#30340;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Learning to Make Adherence-Aware Advice. (arXiv:2310.00817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20154;&#30340;&#20381;&#20174;&#31243;&#24230;&#21644;&#26426;&#22120;&#25552;&#20379;&#24314;&#35758;&#30340;&#26102;&#26426;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#26368;&#20339;&#30340;&#24314;&#35758;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#20154;&#31867;&#20915;&#31574;&#20013;&#25198;&#28436;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#23384;&#22312;&#25361;&#25112;&#12290;&#30001;&#20110;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21040;&#20154;&#31867;&#24573;&#35270;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#21644;&#20154;&#24037;&#26234;&#33021;&#36873;&#25321;&#24615;&#25552;&#20379;&#24314;&#35758;&#30340;&#38656;&#27714;&#65292;&#19968;&#20010;&#25361;&#25112;&#23601;&#26469;&#33258;&#20110;&#24213;&#23618;&#20154;&#24037;&#26234;&#33021;&#31574;&#30053;&#30340;&#19981;&#20339;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20154;&#31867;&#30340;&#20381;&#20174;&#31243;&#24230;&#65288;&#21363;&#20154;&#31867;&#36981;&#24490;/&#25298;&#32477;&#26426;&#22120;&#24314;&#35758;&#30340;&#27010;&#29575;&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#36831;&#36873;&#39033;&#65292;&#20351;&#24471;&#26426;&#22120;&#22312;&#26368;&#21512;&#36866;&#30340;&#26102;&#20505;&#21487;&#20197;&#26242;&#26102;&#19981;&#25552;&#20379;&#24314;&#35758;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#26368;&#20339;&#30340;&#24314;&#35758;&#31574;&#30053;&#65292;&#24182;&#20165;&#22312;&#20851;&#38190;&#26102;&#21051;&#25552;&#20379;&#24314;&#35758;&#12290;&#19982;&#38382;&#39064;&#19981;&#21487;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#19987;&#38376;&#21270;&#23398;&#20064;&#31639;&#27861;&#19981;&#20165;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#23454;&#35777;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.
&lt;/p&gt;</description></item><item><title>Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00229</link><description>&lt;p&gt;
&#22312;&#35268;&#21010;&#20013;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00229
&lt;/p&gt;
&lt;p&gt;
Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#26377;&#24847;&#35782;&#35268;&#21010;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Skipper&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#25512;&#24191;&#22312;&#26032;&#24773;&#22659;&#20013;&#23398;&#21040;&#30340;&#25216;&#33021;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23427;&#33258;&#21160;&#23558;&#32473;&#23450;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#21487;&#31649;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#36825;&#20381;&#36182;&#20110;&#20174;&#22238;&#28335;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#20026;&#26377;&#21521;&#22270;&#30340;&#25277;&#35937;&#20195;&#29702;&#38382;&#39064;&#30340;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21738;&#20123;&#26041;&#38754;&#26377;&#26395;&#25552;&#20379;&#24110;&#21161;&#12290;&#38024;&#23545;&#27867;&#21270;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.16883</link><description>&lt;p&gt;
&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;Lipschitz-&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#22122;&#22768;&#36755;&#20837;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#20854;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35748;&#35777;&#21322;&#24452;&#26159;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#36275;&#22815;&#35748;&#35777;&#21322;&#24452;&#30340;&#39640;&#25928;&#20998;&#31867;&#22120;&#21602;&#65311;&#38543;&#26426;&#24179;&#28369;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#27880;&#20837;&#22122;&#22768;&#26469;&#33719;&#24471;&#24179;&#28369;&#19988;&#26356;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;&#21478;&#22806;&#20004;&#20010;&#37325;&#35201;&#23646;&#24615;&#65292;&#21363;&#20854;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22522;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#23545;&#24179;&#28369;&#20998;&#31867;&#22120;&#21644;&#32463;&#39564;&#26041;&#24046;&#30340;&#21452;&#37325;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#65292;&#20197;&#20415;&#36890;&#36807;Bernst&#30340;&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;&#26469;&#21033;&#29992;&#22522;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16512</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#20998;&#26512;&#34920;&#36798;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#21270;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#30001;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#38382;&#39064;&#21487;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#22312;&#20854;&#20013;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#20197;&#25968;&#25454;&#21521;&#37327;&#29983;&#25104;&#30340;&#19977;&#35282;&#24418;&#21644;&#24179;&#34892;&#20307;&#30340;&#26377;&#31526;&#21495;&#20307;&#31215;&#34920;&#31034;&#12290;&#20984;&#38382;&#39064;&#36890;&#36807;$\ell_1$&#27491;&#21017;&#21270;&#25214;&#21040;&#26679;&#26412;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#20197;&#21457;&#29616;&#20165;&#30456;&#20851;&#30340;&#26964;&#31215;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#36830;&#25509;&#29702;&#35770;&#21644;&#23454;&#36341;&#65292;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#23545;&#20110;&#20004;&#23618;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#36870;&#20808;&#39564;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#35813;&#32593;&#32476;&#23558;&#20174;&#25105;&#20204;&#30340;&#20445;&#35777;&#20013;&#21463;&#30410;&#12290;</title><link>http://arxiv.org/abs/2309.12128</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems. (arXiv:2309.12128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#36830;&#25509;&#29702;&#35770;&#21644;&#23454;&#36341;&#65292;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#23545;&#20110;&#20004;&#23618;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#36870;&#20808;&#39564;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#35813;&#32593;&#32476;&#23558;&#20174;&#25105;&#20204;&#30340;&#20445;&#35777;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#36825;&#26679;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#32463;&#39564;&#24615;&#22320;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#26126;&#30830;&#29702;&#35770;&#20445;&#35777;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#26469;&#25511;&#21046;&#31070;&#32463;&#20999;&#21521;&#26680;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#19979;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#22914;&#20309;&#36830;&#25509;&#36825;&#20004;&#20010;&#39046;&#22495;&#65292;&#24182;&#20026;&#26080;&#30417;&#30563;&#21069;&#39304;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#35757;&#32451;&#36807;&#31243;&#25552;&#20379;&#30830;&#23450;&#24615;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#36229;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#22312;&#36825;&#20123;&#30028;&#38480;&#19979;&#65292;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#28145;&#24230;&#36870;&#20808;&#39564;&#32593;&#32476;&#23558;&#21463;&#30410;&#20110;&#25105;&#20204;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have become a prominent approach to solve inverse problems in recent years. While a plethora of such methods was developed to solve inverse problems empirically, we are still lacking clear theoretical guarantees for these methods. On the other hand, many works proved convergence to optimal solutions of neural networks in a more general setting using overparametrization as a way to control the Neural Tangent Kernel. In this work we investigate how to bridge these two worlds and we provide deterministic convergence and recovery guarantees for the class of unsupervised feedforward multilayer neural networks trained to solve inverse problems. We also derive overparametrization bounds under which a two-layers Deep Inverse Prior network with smooth activation function will benefit from our guarantees.
&lt;/p&gt;</description></item><item><title>DreamLLM&#26159;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;DreamLLM&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11499</link><description>&lt;p&gt;
DreamLLM&#65306;&#21327;&#21516;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11499
&lt;/p&gt;
&lt;p&gt;
DreamLLM&#26159;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;DreamLLM&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DreamLLM&#65292;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#39318;&#27425;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#21033;&#29992;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#20043;&#38388;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;DreamLLM&#36981;&#24490;&#20004;&#20010;&#22522;&#26412;&#21407;&#21017;&#12290;&#31532;&#19968;&#20010;&#21407;&#21017;&#19987;&#27880;&#20110;&#36890;&#36807;&#22312;&#21407;&#22987;&#22810;&#27169;&#24577;&#31354;&#38388;&#20013;&#36827;&#34892;&#30452;&#25509;&#37319;&#26679;&#26469;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#21518;&#39564;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#20687;CLIP&#36825;&#26679;&#30340;&#22806;&#37096;&#29305;&#24449;&#25552;&#21462;&#22120;&#25152;&#22266;&#26377;&#30340;&#38480;&#21046;&#21644;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;DreamLLM&#20419;&#36827;&#20102;&#21407;&#22987;&#30340;&#12289;&#20132;&#32455;&#30340;&#25991;&#20214;&#29983;&#25104;&#65292;&#23545;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#20197;&#21450;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;DreamLLM&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#26377;&#26465;&#20214;&#12289;&#36793;&#32536;&#21644;&#32852;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;DreamLLM&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#30340;MLLM&#12290;&#32508;&#21512;&#23454;&#39564;&#31361;&#26174;&#20102;DreamLLM&#20316;&#20026;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.10370</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#19968;&#20010;&#38544;&#34255;&#23618;&#12289;&#19968;&#20010;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#12289;&#19968;&#20010;${\mathcal L}^2$&#35889;&#33539;&#31867;&#65288;&#25110;&#32773;Hilbert-Schmidt&#65289;&#30340;&#20195;&#20215;&#20989;&#25968;&#12289;&#36755;&#20837;&#31354;&#38388;${\mathbb R}^M$&#12289;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#65288;&#20854;&#20013;$Q\leq M$&#65289;&#65292;&#20197;&#21450;&#35757;&#32451;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;$N&gt;QM$&#25152;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;$O(\delta_P)$&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$\delta_P$&#34913;&#37327;&#20102;&#35757;&#32451;&#36755;&#20837;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24212;&#20110;&#23646;&#20110;&#21516;&#19968;&#36755;&#20986;&#21521;&#37327;$y_j$&#30340;&#35757;&#32451;&#36755;&#20837;&#21521;&#37327;$\overline{x_{0,j}}$&#30340;&#25237;&#24433;&#26469;&#33719;&#24471;&#36817;&#20284;&#30340;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;$j=1,\dots,Q$&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;$M=Q$&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#30830;&#23450;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#19968;&#20010;&#30830;&#20999;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#65307;&#36825;&#20010;&#23574;&#38160;&#30340;&#20540;&#19982;&#23545;&#20110;$Q\leq M$&#25152;&#33719;&#24471;&#30340;&#19978;&#30028;&#20043;&#38388;&#26377;&#19968;&#20010;&#30456;&#23545;&#35823;&#24046;$O(\delta_P^2)$&#12290;&#19978;&#30028;&#35777;&#26126;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26500;&#36896;&#24615;&#35757;&#32451;&#30340;&#32593;&#32476;&#65307;&#25105;&#20204;&#35777;&#26126;&#23427;&#27979;&#24230;&#20102;$Q$&#32500;&#31354;&#38388;&#20013;&#30340;&#32473;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N&gt;QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#20351;&#29992;&#20102;$\ell_0$-&#33539;&#25968;&#32422;&#26463;&#65292;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.10152</link><description>&lt;p&gt;
Primal-Dual $\ell_0$-&#32422;&#26463;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual $\ell_0$-Constrained Sparse Index Tracking. (arXiv:2309.10152v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#20351;&#29992;&#20102;$\ell_0$-&#33539;&#25968;&#32422;&#26463;&#65292;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#34987;&#21160;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#26469;&#36319;&#36394;&#37329;&#34701;&#25351;&#25968;&#12290;&#30456;&#27604;&#20110;&#20840;&#20179;&#25237;&#36164;&#32452;&#21512;&#65292;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#22312;&#38477;&#20302;&#20132;&#26131;&#25104;&#26412;&#21644;&#36991;&#20813;&#19981;&#27969;&#21160;&#36164;&#20135;&#26041;&#38754;&#26356;&#20855;&#20248;&#21183;&#12290;&#20026;&#20102;&#24378;&#21046;&#25237;&#36164;&#32452;&#21512;&#30340;&#31232;&#30095;&#24615;&#65292;&#20256;&#32479;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;$\ell_p$-&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#20844;&#24335;&#65292;&#20316;&#20026;$\ell_0$-&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#26367;&#20195;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#20844;&#24335;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#65292;&#20294;&#22312;&#23454;&#38469;&#25237;&#36164;&#20013;&#21364;&#19981;&#26131;&#20351;&#29992;&#65292;&#22240;&#20026;&#32454;&#33268;&#30340;&#21442;&#25968;&#35843;&#25972;&#26469;&#25351;&#23450;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#26159;&#33392;&#38590;&#19988;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#20351;&#29992;&#20102;$\ell_0$-&#33539;&#25968;&#32422;&#26463;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#20801;&#35768;&#22312;&#25237;&#36164;&#32452;&#21512;&#31232;&#30095;&#24615;&#21644;&#25442;&#25163;&#29575;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse index tracking is one of the prominent passive portfolio management strategies that construct a sparse portfolio to track a financial index. A sparse portfolio is desirable over a full portfolio in terms of transaction cost reduction and avoiding illiquid assets. To enforce the sparsity of the portfolio, conventional studies have proposed formulations based on $\ell_p$-norm regularizations as a continuous surrogate of the $\ell_0$-norm regularization. Although such formulations can be used to construct sparse portfolios, they are not easy to use in actual investments because parameter tuning to specify the exact upper bound on the number of assets in the portfolio is delicate and time-consuming. In this paper, we propose a new problem formulation of sparse index tracking using an $\ell_0$-norm constraint that enables easy control of the upper bound on the number of assets in the portfolio. In addition, our formulation allows the choice between portfolio sparsity and turnover spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#20026;2&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#36923;&#36753;&#22238;&#24402;&#20195;&#20215;&#20989;&#25968;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#33021;&#22815;&#25910;&#25947;&#21040;&#20840;&#23616;&#26497;&#23567;&#20540;&#65292;&#36825;&#36866;&#29992;&#20110;&#20219;&#24847;&#25968;&#25454;&#21644;&#20855;&#26377;&#20805;&#20998;&#24179;&#28369;&#19988;&#26377;&#30028;&#28608;&#27963;&#20989;&#25968;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36830;&#32493;&#26102;&#38388;SGD&#30340;&#25351;&#25968;&#32423;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#35813;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;&#20809;&#28369;&#26080;&#30028;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.09258</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#36923;&#36753;&#22238;&#24402;&#20195;&#20215;&#20989;&#25968;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets. (arXiv:2309.09258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#20026;2&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#36923;&#36753;&#22238;&#24402;&#20195;&#20215;&#20989;&#25968;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#33021;&#22815;&#25910;&#25947;&#21040;&#20840;&#23616;&#26497;&#23567;&#20540;&#65292;&#36825;&#36866;&#29992;&#20110;&#20219;&#24847;&#25968;&#25454;&#21644;&#20855;&#26377;&#20805;&#20998;&#24179;&#28369;&#19988;&#26377;&#30028;&#28608;&#27963;&#20989;&#25968;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36830;&#32493;&#26102;&#38388;SGD&#30340;&#25351;&#25968;&#32423;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#35813;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;&#20809;&#28369;&#26080;&#30028;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23545;&#20110;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20026;2&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36923;&#36753;&#22238;&#24402;&#20195;&#20215;&#20989;&#25968;&#33021;&#22815;&#25910;&#25947;&#21040;&#20840;&#23616;&#26497;&#23567;&#20540;&#65292;&#23545;&#20110;&#20219;&#24847;&#25968;&#25454;&#21644;&#20855;&#26377;&#20805;&#20998;&#24179;&#28369;&#19988;&#26377;&#30028;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;sigmoid&#21644;tanh&#65289;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36830;&#32493;&#26102;&#38388;SGD&#30340;&#25351;&#25968;&#32423;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#35813;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;&#20809;&#28369;&#26080;&#30028;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;SoftPlus&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#35777;&#26126;&#20102;&#22312;&#24658;&#23450;&#22823;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#23384;&#22312;Frobenius&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#36923;&#36753;&#22238;&#24402;&#20195;&#20215;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#26159;"Villani&#20989;&#25968;"&#65292;&#20174;&#32780;&#33021;&#22815;&#26500;&#24314;&#22312;&#26368;&#36817;&#23545;&#20110;&#27492;&#31867;&#30446;&#26631;&#20989;&#25968;&#19978;&#20998;&#26512;SGD&#30340;&#30740;&#31350;&#36827;&#23637;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates with adequately smooth and bounded activations like sigmoid and tanh. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence of Frobenius norm regularized logistic loss functions on constant-sized neural nets which are "Villani functions" and thus be able to build on recent progress with analyzing SGD on such objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#20248;&#21270;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#25928;&#26524;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#22320;&#22270;&#23450;&#20301;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#22312;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08731</link><description>&lt;p&gt;
&#25351;&#24341;&#30340;&#26041;&#27861;&#65306;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#25913;&#36827;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP Weights. (arXiv:2309.08731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#20248;&#21270;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#25928;&#26524;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#22320;&#22270;&#23450;&#20301;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#22312;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#12290;&#34429;&#28982;&#30446;&#21069;&#23450;&#20301;&#30340;&#25216;&#26415;&#27700;&#24179;&#26159;&#23558;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#19982;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#36827;&#34892;&#21305;&#37197;&#65292;&#20294;&#26159;&#38647;&#36798;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#23545;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#20855;&#26377;&#26356;&#24378;&#30340;&#38887;&#24615;&#12290;&#20026;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;&#39640;&#36136;&#37327;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#65292;&#21516;&#26102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#20445;&#25345;&#24615;&#33021;&#65292;&#23558;&#38647;&#36798;&#25968;&#25454;&#19982;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#36827;&#34892;&#21305;&#37197;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38647;&#36798;&#27979;&#37327;&#20013;&#23384;&#22312;&#30340;&#29420;&#29305;&#20266;&#24433;&#65292;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;&#19968;&#30452;&#38590;&#20197;&#36798;&#21040;&#19982;&#28608;&#20809;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#26080;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#26412;&#24037;&#20316;&#22312;&#22522;&#20110;ICP&#30340;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;&#31995;&#32479;&#22522;&#30784;&#19978;&#65292;&#21253;&#25324;&#19968;&#20010;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#26681;&#25454;&#39640;&#23618;&#27425;&#30340;&#25195;&#25551;&#20449;&#24687;&#23545;&#38647;&#36798;&#28857;&#36827;&#34892;&#21152;&#26435;&#12290;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20998;&#26512;&#26041;&#27861;&#19982;&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#30456;&#32467;&#21512;&#65292;&#20943;&#23567;&#20102;&#38647;&#36798;&#23450;&#20301;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel deep-learning-based approach to improve localizing radar measurements against lidar maps. Although the state of the art for localization is matching lidar data to lidar maps, radar has been considered as a promising alternative, as it is potentially more resilient against adverse weather such as precipitation and heavy fog. To make use of existing high-quality lidar maps, while maintaining performance in adverse weather, matching radar data to lidar maps is of interest. However, owing in part to the unique artefacts present in radar measurements, radar-lidar localization has struggled to achieve comparable performance to lidar-lidar systems, preventing it from being viable for autonomous driving. This work builds on an ICP-based radar-lidar localization system by including a learned preprocessing step that weights radar points based on high-level scan information. Combining a proven analytical approach with a learned weight reduces localization errors in rad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#20027;&#39064;&#35782;&#21035;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#26448;&#26009;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.08249</link><description>&lt;p&gt;
&#24102;&#26377;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Nonnegative Matrix Factorization with Beta Divergences. (arXiv:2309.08249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#20027;&#39064;&#35782;&#21035;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#26448;&#26009;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;deep NMF&#65289;&#26368;&#36817;&#25104;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25552;&#21462;&#22810;&#23618;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#28145;&#24230;NMF&#27169;&#22411;&#21644;&#31639;&#27861;&#20027;&#35201;&#37117;&#20197;&#26368;&#23567;&#20108;&#20056;&#35823;&#24046;&#20026;&#35780;&#20272;&#26631;&#20934;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#35780;&#20272;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#36817;&#20284;&#36136;&#37327;&#30340;&#26368;&#21512;&#36866;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#24403;&#22788;&#29702;&#38899;&#39057;&#20449;&#21495;&#21644;&#25991;&#26723;&#31561;&#25968;&#25454;&#31867;&#22411;&#26102;&#65292;&#24191;&#27867;&#35748;&#21487;&#30340;&#26159;$\beta$-divergences&#25552;&#20379;&#20102;&#26356;&#36866;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#22522;&#20110;$\beta$-divergences&#24320;&#21457;&#20102;&#26032;&#30340;&#28145;&#24230;NMF&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#38598;&#21512;&#20013;&#30340;&#20027;&#39064;&#35782;&#21035;&#20197;&#21450;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#26448;&#26009;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Nonnegative Matrix Factorization (deep NMF) has recently emerged as a valuable technique for extracting multiple layers of features across different scales. However, all existing deep NMF models and algorithms have primarily centered their evaluation on the least squares error, which may not be the most appropriate metric for assessing the quality of approximations on diverse datasets. For instance, when dealing with data types such as audio signals and documents, it is widely acknowledged that $\beta$-divergences offer a more suitable alternative. In this paper, we develop new models and algorithms for deep NMF using $\beta$-divergences. Subsequently, we apply these techniques to the extraction of facial features, the identification of topics within document collections, and the identification of materials within hyperspectral images.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22768;&#38899;&#25968;&#25454;&#35757;&#32451;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#21628;&#21560;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#21644;&#39057;&#35889;&#20998;&#26512;&#26469;&#25552;&#21462;&#19982;&#24515;&#34880;&#31649;&#21644;&#21628;&#21560;&#27169;&#24335;&#30456;&#20851;&#30340;&#29983;&#29702;&#20449;&#21495;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#21644;&#39044;&#27979;&#24314;&#27169;&#23454;&#29616;&#24555;&#36895;&#31579;&#26597;&#21644;&#35786;&#26029;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.07183</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#32423;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22768;&#38899;&#30456;&#20851;&#21628;&#21560;&#30142;&#30149;&#20998;&#31867;&#29992;&#20110;&#36741;&#21161;&#35786;&#26029;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support. (arXiv:2309.07183v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22768;&#38899;&#25968;&#25454;&#35757;&#32451;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#21628;&#21560;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#21644;&#39057;&#35889;&#20998;&#26512;&#26469;&#25552;&#21462;&#19982;&#24515;&#34880;&#31649;&#21644;&#21628;&#21560;&#27169;&#24335;&#30456;&#20851;&#30340;&#29983;&#29702;&#20449;&#21495;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#21644;&#39044;&#27979;&#24314;&#27169;&#23454;&#29616;&#24555;&#36895;&#31579;&#26597;&#21644;&#35786;&#26029;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#36825;&#31361;&#26174;&#20102;&#36805;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#25512;&#36827;&#21548;&#35786;&#30340;&#24555;&#36895;&#31579;&#26597;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#21033;&#29992;&#20854;&#20013;&#19968;&#20010;&#26368;&#22823;&#30340;&#20844;&#24320;&#30340;&#21628;&#21560;&#38899;&#21307;&#23398;&#25968;&#25454;&#24211;&#26469;&#35757;&#32451;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23545;&#19981;&#21516;&#30340;&#20581;&#24247;&#29366;&#20917;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#65288;EMD&#65289;&#21644;&#39057;&#35889;&#20998;&#26512;&#65292;&#20174;&#22768;&#23398;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#24515;&#34880;&#31649;&#21644;&#21628;&#21560;&#27169;&#24335;&#23494;&#20999;&#30456;&#20851;&#30340;&#29983;&#29702;&#30456;&#20851;&#29983;&#29289;&#20449;&#21495;&#65292;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;&#21151;&#29575;&#35889;&#23494;&#24230;&#20998;&#26512;&#21644;&#28388;&#27874;&#25216;&#26415;&#26469;&#36873;&#25321;&#19982;&#28508;&#22312;&#29983;&#29702;&#29616;&#35937;&#24378;&#30456;&#20851;&#30340;&#22266;&#26377;&#27169;&#24577;&#20989;&#25968;&#65288;IMFs&#65289;&#12290;&#36825;&#20123;&#29983;&#29289;&#20449;&#21495;&#32463;&#36807;&#20840;&#38754;&#30340;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37096;&#32626;&#19968;&#20010;&#20108;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In global healthcare, respiratory diseases are a leading cause of mortality, underscoring the need for rapid and accurate diagnostics. To advance rapid screening techniques via auscultation, our research focuses on employing one of the largest publicly available medical database of respiratory sounds to train multiple machine learning models able to classify different health conditions. Our method combines Empirical Mode Decomposition (EMD) and spectral analysis to extract physiologically relevant biosignals from acoustic data, closely tied to cardiovascular and respiratory patterns, making our approach apart in its departure from conventional audio feature extraction practices. We use Power Spectral Density analysis and filtering techniques to select Intrinsic Mode Functions (IMFs) strongly correlated with underlying physiological phenomena. These biosignals undergo a comprehensive feature extraction process for predictive modeling. Initially, we deploy a binary classification model t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>Compact&#25552;&#20986;&#20102;&#19968;&#31181;&#36924;&#36817;&#22797;&#26434;&#28608;&#27963;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;MPC&#25216;&#26415;&#39640;&#25928;&#37197;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04664</link><description>&lt;p&gt;
Compact&#65306;&#36924;&#36817;&#22797;&#26434;&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#23433;&#20840;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Compact: Approximating Complex Activation Functions for Secure Computation. (arXiv:2309.04664v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04664
&lt;/p&gt;
&lt;p&gt;
Compact&#25552;&#20986;&#20102;&#19968;&#31181;&#36924;&#36817;&#22797;&#26434;&#28608;&#27963;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;MPC&#25216;&#26415;&#39640;&#25928;&#37197;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;Secure multi-party computation&#65292;MPC&#65289;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#22312;&#29992;&#25143;&#26597;&#35810;&#37096;&#32626;&#22312;&#20844;&#20849;&#20113;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#26102;&#25552;&#20379;&#25968;&#25454;&#38544;&#31169;&#12290;&#26368;&#20808;&#36827;&#30340;MPC&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#20351;&#29992;&#31616;&#21333;&#28608;&#27963;&#20989;&#25968;&#65288;AFs&#65289;&#30340;DNN&#27169;&#22411;&#65292;&#22914;ReLU&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26368;&#26032;&#30340;&#24212;&#29992;&#35774;&#35745;&#30340;DNN&#27169;&#22411;&#26550;&#26500;&#36890;&#24120;&#20351;&#29992;&#22797;&#26434;&#19988;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;AFs&#12290;&#20026;&#36825;&#20123;&#22797;&#26434;AFs&#35774;&#35745;&#39640;&#25928;&#30340;MPC&#25216;&#26415;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Compact&#65292;&#23427;&#20135;&#29983;&#22797;&#26434;AFs&#30340;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#20197;&#20415;&#19982;&#26368;&#20808;&#36827;&#30340;MPC&#25216;&#26415;&#39640;&#25928;&#37197;&#21512;&#20351;&#29992;&#12290;Compact&#26082;&#19981;&#38656;&#35201;&#20063;&#19981;&#24378;&#21152;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#23548;&#33268;&#20960;&#20046;&#30456;&#21516;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#27969;&#34892;&#30340;&#22797;&#26434;AFs SiLU&#12289;GeLU&#21644;Mish&#30340;DNN&#26550;&#26500;&#30340;&#22235;&#20010;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#23545;Compact&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;Compact&#20960;&#20046;&#19981;&#20250;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Secure multi-party computation (MPC) techniques can be used to provide data privacy when users query deep neural network (DNN) models hosted on a public cloud. State-of-the-art MPC techniques can be directly leveraged for DNN models that use simple activation functions (AFs) such as ReLU. However, DNN model architectures designed for cutting-edge applications often use complex and highly non-linear AFs. Designing efficient MPC techniques for such complex AFs is an open problem.  Towards this, we propose Compact, which produces piece-wise polynomial approximations of complex AFs to enable their efficient use with state-of-the-art MPC techniques. Compact neither requires nor imposes any restriction on model training and results in near-identical model accuracy. We extensively evaluate Compact on four different machine-learning tasks with DNN architectures that use popular complex AFs SiLU, GeLU, and Mish. Our experimental results show that Compact incurs negligible accuracy loss compared
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#39640;&#25928;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#35270;&#35273;&#25552;&#31034;/&#37325;&#26032;&#32534;&#31243;&#65288;VP&#65289;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#27169;&#22411;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24425;&#31080;&#24335;&#36716;&#31227;&#24182;&#38750;&#36890;&#29992;&#30340;&#37325;&#26032;&#32534;&#31243;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.14969</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;&#19979;&#30340;&#37325;&#26032;&#32534;&#31243;: &#37325;&#26032;&#23457;&#35270;&#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#39640;&#25928;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets. (arXiv:2308.14969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14969
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#39640;&#25928;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#35270;&#35273;&#25552;&#31034;/&#37325;&#26032;&#32534;&#31243;&#65288;VP&#65289;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#27169;&#22411;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24425;&#31080;&#24335;&#36716;&#31227;&#24182;&#38750;&#36890;&#29992;&#30340;&#37325;&#26032;&#32534;&#31243;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#39044;&#31639;&#24040;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#65292;&#19979;&#28216;&#20219;&#21153;&#24050;&#32463;&#36716;&#21521;&#20102;&#39640;&#25928;&#24555;&#36895;&#36866;&#24212;&#30340;&#21465;&#36848;&#12290;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#22522;&#20110;&#20998;&#31867;&#30340;&#20219;&#21153;&#65292;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#35270;&#35273;&#25552;&#31034;/&#37325;&#26032;&#32534;&#31243;&#65288;VP&#65289;; &#21069;&#32773;&#26088;&#22312;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#30340;&#29305;&#24449;&#19978;&#23398;&#20064;&#32447;&#24615;&#22836;&#37096;&#20998;&#31867;&#22120;&#65292;&#32780;&#21518;&#32773;&#23558;&#36755;&#20837;&#25968;&#25454;&#26144;&#23556;&#21040;&#26368;&#21021;&#22312;&#20854;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#28304;&#25968;&#25454;&#39046;&#22495;&#12290;&#23613;&#31649;&#24191;&#27867;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;LP&#21644;VP&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;&#24230;&#36724;&#26469;&#25506;&#32034;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#33021;&#21147;: (a) &#25968;&#25454;&#31232;&#30095;&#24615;: &#23569;&#26679;&#26412;&#33258;&#36866;&#24212;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450; (b) &#27169;&#22411;&#31232;&#30095;&#24615;: &#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24425;&#31080;&#24335;&#36716;&#31227;&#19981;&#26159;&#36890;&#29992;&#30340;&#37325;&#26032;&#32534;&#31243;&#22120;&#65292;&#21363;&#23545;&#20110;&#26576;&#20123;&#30446;&#26631;&#25968;&#25454;&#38598;&#65292;&#37325;&#26032;&#32534;&#31243;&#24425;&#31080;&#24335;&#36716;&#31227;&#20250;&#20135;&#29983;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of foundation models with huge pre-training budgets, the downstream tasks have been shifted to the narrative of efficient and fast adaptation. For classification-based tasks in the domain of computer vision, the two most efficient approaches have been linear probing (LP) and visual prompting/reprogramming (VP); the former aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model, while the latter maps the input data to the domain of the source data on which the model was originally pre-trained on. Although extensive studies have demonstrated the differences between LP and VP in terms of downstream performance, we explore the capabilities of the two aforementioned methods via the sparsity axis: (a) Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the impact of lottery tickets (LT). We demonstrate that LT are not universal reprogrammers, i.e., for certain target datasets, reprogramming an LT yields signif
&lt;/p&gt;</description></item><item><title>OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13137</link><description>&lt;p&gt;
OmniQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13137
&lt;/p&gt;
&lt;p&gt;
OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#20102;&#20854;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25552;&#39640;LLM&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#25163;&#24037;&#21046;&#23450;&#37327;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#24182;&#19988;&#19981;&#33021;&#22788;&#29702;&#26497;&#20302;&#20301;&#37327;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#65288;OmniQuant&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;LLMs&#65292;&#23427;&#22312;&#22810;&#31181;&#37327;&#21270;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#26469;&#20445;&#25345;PTQ&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;OmniQuant&#21253;&#21547;&#20004;&#20010;&#21019;&#26032;&#32452;&#20214;&#65292;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#21098;&#35009;&#65288;LWC&#65289;&#21644;&#21487;&#23398;&#20064;&#30340;&#31561;&#25928;&#21464;&#25442;&#65288;LET&#65289;&#12290;LWC&#36890;&#36807;&#20248;&#21270;&#21098;&#35009;&#38408;&#20540;&#26469;&#35843;&#33410;&#26435;&#37325;&#30340;&#26497;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LET&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#38024;&#23545;&#29983;&#29289;&#28151;&#21512;&#31995;&#32479;&#20013;&#30340;&#20223;&#29983;&#24046;&#36317;&#36827;&#34892;&#20102;&#37327;&#21270;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#40060;&#31867;&#21644;&#20223;&#29983;&#35825;&#39285;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#23545;&#40060;&#31867;&#23545;&#30340;&#27169;&#25311;&#65292;&#30740;&#31350;&#20154;&#21592;&#35777;&#26126;&#20102;&#20182;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#19982;&#30495;&#23454;&#40060;&#31867;&#23436;&#20840;&#30456;&#20284;&#30340;&#31038;&#20250;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.08978</link><description>&lt;p&gt;
&#37327;&#21270;&#29983;&#29289;&#28151;&#21512;&#31995;&#32479;&#20013;&#30340;&#20223;&#29983;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Quantifying the biomimicry gap in biohybrid systems. (arXiv:2308.08978v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08978
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#38024;&#23545;&#29983;&#29289;&#28151;&#21512;&#31995;&#32479;&#20013;&#30340;&#20223;&#29983;&#24046;&#36317;&#36827;&#34892;&#20102;&#37327;&#21270;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#40060;&#31867;&#21644;&#20223;&#29983;&#35825;&#39285;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#23545;&#40060;&#31867;&#23545;&#30340;&#27169;&#25311;&#65292;&#30740;&#31350;&#20154;&#21592;&#35777;&#26126;&#20102;&#20182;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#19982;&#30495;&#23454;&#40060;&#31867;&#23436;&#20840;&#30456;&#20284;&#30340;&#31038;&#20250;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21160;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#20223;&#29983;&#28151;&#21512;&#31995;&#32479;&#24050;&#25104;&#20026;&#25506;&#32034;&#21644;&#35782;&#21035;&#32676;&#20307;&#21160;&#29289;&#34892;&#20026;&#26426;&#21046;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#23558;&#31038;&#20250;&#20114;&#21160;&#27169;&#22411;&#20174;&#27169;&#25311;&#36716;&#31227;&#21040;&#29616;&#23454;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#39564;&#35777;&#24314;&#27169;&#20551;&#35774;&#12290;&#36825;&#19968;&#25361;&#25112;&#28304;&#20110;&#25152;&#35859;&#30340;&#8220;&#20223;&#29983;&#24046;&#36317;&#8221;&#30340;&#36328;&#36234;&#65292;&#35813;&#24046;&#36317;&#20351;&#29992;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#20154;&#22797;&#21046;&#21697;&#12289;&#36890;&#20449;&#32447;&#32034;&#21644;&#29289;&#29702;&#32422;&#26463;&#65292;&#36825;&#20123;&#22240;&#32032;&#26410;&#34987;&#32435;&#20837;&#27169;&#25311;&#20013;&#65292;&#21487;&#33021;&#22312;&#21160;&#29289;&#36523;&#19978;&#24341;&#21457;&#19981;&#30495;&#23454;&#30340;&#34892;&#20026;&#21453;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#32418;&#22068;&#26080;&#39035;&#39790;&#40060;&#65288;Hemigrammus rhodostomus&#65289;&#30340;&#20223;&#29983;&#35825;&#39285;&#21644;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#26469;&#29983;&#25104;&#20223;&#29983;&#31038;&#20250;&#20114;&#21160;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#30001;&#40060;&#21644;&#26426;&#22120;&#20154;&#35825;&#39285;&#32452;&#25104;&#30340;&#29983;&#29289;&#28151;&#21512;&#20307;&#12289;&#19968;&#23545;&#30495;&#23454;&#40060;&#21644;&#19968;&#23545;&#40060;&#30340;&#27169;&#25311;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29983;&#29289;&#28151;&#21512;&#31995;&#32479;&#20135;&#29983;&#20102;&#19982;&#30495;&#23454;&#40060;&#31867;&#23436;&#20840;&#30456;&#20284;&#30340;&#20223;&#30495;&#31038;&#20250;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biohybrid systems in which robotic lures interact with animals have become compelling tools for probing and identifying the mechanisms underlying collective animal behavior. One key challenge lies in the transfer of social interaction models from simulations to reality, using robotics to validate the modeling hypotheses. This challenge arises in bridging what we term the "biomimicry gap", which is caused by imperfect robotic replicas, communication cues and physics constrains not incorporated in the simulations that may elicit unrealistic behavioral responses in animals. In this work, we used a biomimetic lure of a rummy-nose tetra fish (Hemigrammus rhodostomus) and a neural network (NN) model for generating biomimetic social interactions. Through experiments with a biohybrid pair comprising a fish and the robotic lure, a pair of real fish, and simulations of pairs of fish, we demonstrate that our biohybrid system generates high-fidelity social interactions mirroring those of genuine f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;</title><link>http://arxiv.org/abs/2308.08487</link><description>&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Temporal Interest Network for Click-Through Rate Prediction. (arXiv:2308.08487v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#30340;&#21382;&#21490;&#26159;&#39044;&#27979;&#28857;&#20987;&#29575;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#30446;&#26631;&#39033;&#30446;&#20855;&#26377;&#24378;&#28872;&#30340;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#24050;&#26377;&#25991;&#29486;&#20998;&#21035;&#30740;&#31350;&#20102;&#36825;&#20123;&#30456;&#20851;&#24615;&#65292;&#20294;&#23578;&#26410;&#20998;&#26512;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#21363;&#34892;&#20026;&#35821;&#20041;&#12289;&#30446;&#26631;&#35821;&#20041;&#12289;&#34892;&#20026;&#26102;&#38388;&#21644;&#30446;&#26631;&#26102;&#38388;&#30340;&#22235;&#37325;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#27979;&#37327;&#20102;&#22235;&#37325;&#30456;&#20851;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#30452;&#35266;&#32780;&#24378;&#22823;&#30340;&#22235;&#37325;&#27169;&#24335;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#29992;&#25143;&#34892;&#20026;&#26041;&#27861;&#30340;&#23398;&#20064;&#30456;&#20851;&#24615;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#37117;&#27809;&#26377;&#23398;&#20064;&#21040;&#36825;&#26679;&#30340;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#26469;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.  In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28857;&#23545;&#28857;&#35748;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#21644;&#37319;&#26679;&#39640;&#26031;&#26426;&#21046;&#65292;&#33021;&#22815;&#30830;&#20445;&#23545;&#26377;&#38480;&#25968;&#37327;&#20013;&#27602;&#26679;&#26412;&#30340;&#39044;&#27979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.07553</link><description>&lt;p&gt;
&#21152;&#24378;&#35299;&#33647;&#65306;&#38024;&#23545;&#20013;&#27602;&#25915;&#20987;&#30340;&#25913;&#36827;&#28857;&#23545;&#28857;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks. (arXiv:2308.07553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28857;&#23545;&#28857;&#35748;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#21644;&#37319;&#26679;&#39640;&#26031;&#26426;&#21046;&#65292;&#33021;&#22815;&#30830;&#20445;&#23545;&#26377;&#38480;&#25968;&#37327;&#20013;&#27602;&#26679;&#26412;&#30340;&#39044;&#27979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#27602;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#23545;&#35757;&#32451;&#35821;&#26009;&#36827;&#34892;&#24494;&#23567;&#25913;&#21160;&#26469;&#23545;&#27169;&#22411;&#34892;&#20026;&#20135;&#29983;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23384;&#22312;&#23545;&#29305;&#23450;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#25552;&#20379;&#20219;&#20309;&#20445;&#35777;&#65292;&#21487;&#33021;&#34987;&#26032;&#22411;&#25915;&#20987;&#25152;&#23545;&#25239;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#36807;&#32771;&#23519;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#35748;&#35777;&#38450;&#24481;&#21487;&#20197;&#25552;&#20379;&#38024;&#23545;&#26377;&#38480;&#25968;&#37327;&#35757;&#32451;&#26679;&#26412;&#34987;&#25932;&#23545;&#25915;&#20987;&#20462;&#25913;&#30340;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#31216;&#20026;&#28857;&#23545;&#28857;&#35748;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#21644;&#37319;&#26679;&#39640;&#26031;&#26426;&#21046;&#65292;&#30830;&#20445;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#23545;&#26377;&#38480;&#25968;&#37327;&#20013;&#27602;&#26679;&#26412;&#30340;&#39044;&#27979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20445;&#35777;&#27604;&#20043;&#21069;&#30340;&#35748;&#35777;&#26041;&#27861;&#25552;&#20379;&#30340;&#20445;&#35777;&#26356;&#22823;&#20004;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poisoning attacks can disproportionately influence model behaviour by making small changes to the training corpus. While defences against specific poisoning attacks do exist, they in general do not provide any guarantees, leaving them potentially countered by novel attacks. In contrast, by examining worst-case behaviours Certified Defences make it possible to provide guarantees of the robustness of a sample against adversarial attacks modifying a finite number of training samples, known as pointwise certification. We achieve this by exploiting both Differential Privacy and the Sampled Gaussian Mechanism to ensure the invariance of prediction for each testing instance against finite numbers of poisoned examples. In doing so, our model provides guarantees of adversarial robustness that are more than twice as large as those provided by prior certifications.
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.06668</link><description>&lt;p&gt;
&#26234;&#33021;&#20892;&#19994;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#65306;&#22522;&#30784;&#30693;&#35782;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges. (arXiv:2308.06668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06668
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#38388;&#65292;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#20892;&#19994;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#12289;&#38590;&#20197;&#33719;&#21462;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#24320;&#21457;&#21644;&#32500;&#25252;&#65292;&#32780;&#19988;&#22823;&#22810;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#65292;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36328;&#36234;&#20102;&#21508;&#20010;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#23569;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#21508;&#31181;&#22810;&#26679;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#20892;&#19994;&#39046;&#22495;&#20013;&#24212;&#29992;&#23578;&#26410;&#26377;&#22826;&#22810;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agricultu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06378</link><description>&lt;p&gt;
DCNFIS&#65306;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System. (arXiv:2308.06378v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#36879;&#26126;&#24230;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#33879;&#21517;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#20294;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#24182;&#22312;&#22235;&#20010;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#19982;&#19977;&#20010;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;DCNFIS&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#36879;&#26126;&#24230;&#65292;&#20174;DCNFIS&#20013;&#32534;&#30721;&#30340;&#27169;&#31946;&#35268;&#21017;&#20013;&#25552;&#21462;&#35299;&#37322;&#65292;&#20197;&#28176;&#21464;&#26144;&#23556;&#30340;&#24418;&#24335;&#23637;&#31034;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;Fashion-MNIST&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#35299;&#37322;&#30340;&#29305;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22312;&#32447;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#21644;&#36882;&#24402;&#26680;&#26041;&#27861;&#12290;&#19982;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#25110;&#19981;&#31934;&#30830;&#36924;&#36817;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#23454;&#20363;&#30340;&#20195;&#20215;&#19978;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#39118;&#21147;&#30701;&#26399;&#39044;&#27979;&#25361;&#25112;&#65292;&#24182;&#19982;&#20854;&#20182;&#31454;&#20105;&#32773;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.01938</link><description>&lt;p&gt;
&#22312;&#32447;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#22522;&#20110;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#21644;&#36882;&#24402;&#26680;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods. (arXiv:2308.01938v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22312;&#32447;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#21644;&#36882;&#24402;&#26680;&#26041;&#27861;&#12290;&#19982;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#25110;&#19981;&#31934;&#30830;&#36924;&#36817;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#23454;&#20363;&#30340;&#20195;&#20215;&#19978;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#39118;&#21147;&#30701;&#26399;&#39044;&#27979;&#25361;&#25112;&#65292;&#24182;&#19982;&#20854;&#20182;&#31454;&#20105;&#32773;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#39640;&#24615;&#33021;&#22522;&#20110;&#22270;&#30340;MTL&#20844;&#24335;&#65292;&#22522;&#20110;&#21152;&#26435;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#65288;WRLS&#65289;&#21644;&#22312;&#32447;&#31232;&#30095;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;OSLSSVR&#65289;&#24320;&#21457;&#20854;&#36882;&#24402;&#29256;&#26412;&#12290;&#37319;&#29992;&#20219;&#21153;&#22534;&#21472;&#36716;&#25442;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#20010;&#21333;&#30697;&#38453;&#65292;&#23427;&#34701;&#21512;&#20102;&#22810;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20026;MT-WRLS&#26041;&#27861;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#21644;MT-OSLSSVR&#30340;&#22810;&#20219;&#21153;&#26680;&#20989;&#25968;&#25552;&#20379;&#32467;&#26500;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#22823;&#37096;&#20998;&#22522;&#20110;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#25110;&#19981;&#31934;&#30830;&#31435;&#26041;&#36924;&#36817;&#26041;&#27861;&#30340;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36817;&#20284;&#36882;&#24402;&#65292;&#20854;&#27599;&#20010;&#23454;&#20363;&#30340;&#20195;&#20215;&#22312;&#36755;&#20837;&#31354;&#38388;&#30340;&#32500;&#24230;&#65288;MT-WRLS&#65289;&#25110;&#23454;&#20363;&#23383;&#20856;&#30340;&#22823;&#23567;&#19978;&#26159;&#20108;&#27425;&#30340;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#22312;&#32447;MTL&#26041;&#27861;&#19982;&#20854;&#20182;&#31454;&#20105;&#32773;&#22312;&#23454;&#38469;&#39118;&#30701;&#26399;&#39044;&#27979;&#25361;&#25112;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces two novel approaches for Online Multi-Task Learning (MTL) Regression Problems. We employ a high performance graph-based MTL formulation and develop its recursive versions based on the Weighted Recursive Least Squares (WRLS) and the Online Sparse Least Squares Support Vector Regression (OSLSSVR). Adopting task-stacking transformations, we demonstrate the existence of a single matrix incorporating the relationship of multiple tasks and providing structural information to be embodied by the MT-WRLS method in its initialization procedure and by the MT-OSLSSVR in its multi-task kernel function. Contrasting the existing literature, which is mostly based on Online Gradient Descent (OGD) or cubic inexact approaches, we achieve exact and approximate recursions with quadratic per-instance cost on the dimension of the input space (MT-WRLS) or on the size of the dictionary of instances (MT-OSLSSVR). We compare our online MTL methods to other contenders in a real-world wind sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01313</link><description>&lt;p&gt;
&#26356;&#22810;&#19978;&#19979;&#25991;&#65292;&#26356;&#23569;&#24178;&#25200;&#65306;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;CLIP&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20154;&#31867;&#33324;&#29702;&#35299;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#20013;&#24471;&#21040;&#21551;&#21457;&#65306;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#35266;&#28857;&#35748;&#20026;&#65292;&#22312;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#20154;&#31867;&#39318;&#20808;&#25512;&#26029;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#32972;&#26223;&#21644;&#26041;&#21521;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#21306;&#20998;&#24320;&#26469;&#65292;&#28982;&#21518;&#20197;&#27492;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20915;&#31574;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20026;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#21487;&#20197;&#25913;&#21892;&#38646;&#26679;&#26412;&#20998;&#31867;&#24182;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;CLIP&#26412;&#36523;&#21487;&#20197;&#21512;&#29702;&#22320;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#35757;&#32451;&#12289;&#20004;&#27493;&#39588;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#32479;&#35745;&#23398;&#20013;&#30340;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#25512;&#24191;&#21040;&#22312;&#32447;&#20915;&#31574;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13916</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online learning in bandits with predicted context. (arXiv:2307.13916v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#32479;&#35745;&#23398;&#20013;&#30340;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#25512;&#24191;&#21040;&#22312;&#32447;&#20915;&#31574;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#27599;&#20010;&#26102;&#21051;&#65292;&#20195;&#29702;&#21482;&#33021;&#35775;&#38382;&#21040;&#19978;&#19979;&#25991;&#30340;&#19968;&#20010;&#24102;&#22122;&#22768;&#30340;&#29256;&#26412;&#20197;&#21450;&#35823;&#24046;&#26041;&#24046;&#65288;&#25110;&#32773;&#36825;&#20010;&#26041;&#24046;&#30340;&#19968;&#20010;&#20272;&#35745;&#65289;&#12290;&#36825;&#19968;&#35774;&#32622;&#21463;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#20915;&#31574;&#30340;&#30495;&#23454;&#19978;&#19979;&#25991;&#26159;&#19981;&#21487;&#35266;&#27979;&#30340;&#65292;&#32780;&#21482;&#26377;&#19968;&#20010;&#30001;&#21487;&#33021;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#20986;&#30340;&#19978;&#19979;&#25991;&#12290;&#24403;&#19978;&#19979;&#25991;&#35823;&#24046;&#26159;&#38750;&#34928;&#20943;&#30340;&#26102;&#20505;&#65292;&#32463;&#20856;&#30340;bandit&#31639;&#27861;&#26080;&#27861;&#36798;&#21040;&#27425;&#32447;&#24615;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36825;&#19968;&#35774;&#32622;&#19979;&#65292;&#31532;&#19968;&#20010;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24182;&#19982;&#36866;&#24403;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20851;&#38190;&#30340;&#24605;&#24819;&#26159;&#23558;&#32463;&#20856;&#32479;&#35745;&#23398;&#20013;&#30340;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#25512;&#24191;&#21040;&#22312;&#32447;&#20915;&#31574;&#35774;&#32622;&#20013;&#65292;&#36825;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#22240;&#20026;&#31574;&#30053;&#20381;&#36182;&#20110;&#26377;&#22122;&#22768;&#30340;&#19978;&#19979;&#25991;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20999;&#29255;Wasserstein&#25439;&#22833;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;SGD&#36712;&#36857;&#36924;&#36817;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.11714</link><description>&lt;p&gt;
&#20351;&#29992;&#20999;&#29255;Wasserstein&#25439;&#22833;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;SGD&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses. (arXiv:2307.11714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20999;&#29255;Wasserstein&#25439;&#22833;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;SGD&#36712;&#36857;&#36924;&#36817;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#36817;&#24180;&#26469;&#24341;&#21457;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;Wasserstein&#36317;&#31163;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#20960;&#20309;&#19978;&#21512;&#29702;&#21644;&#30452;&#35266;&#30340;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#30340;&#26041;&#27861;&#12290;&#20986;&#20110;&#35745;&#31639;&#21407;&#22240;&#65292;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#20316;&#20026;Wasserstein&#36317;&#31163;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#12290;&#34429;&#28982;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#20013;&#23454;&#38469;&#35266;&#23519;&#21040;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23545;&#20110;&#36825;&#19968;&#35266;&#23519;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#20511;&#37492;Bianchi&#31561;&#20154;&#65288;2022&#65289;&#20851;&#20110;SGD&#22312;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#20989;&#25968;&#19978;&#25910;&#25947;&#24615;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#19978;&#19979;&#25991;&#65292;&#20351;&#24471;SW&#25439;&#22833;&#23545;NN&#21442;&#25968;&#30340;&#22266;&#23450;&#27493;&#38271;SGD&#36712;&#36857;&#25910;&#25947;&#21040;&#65288;&#27425;&#65289;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#30528;&#27493;&#38271;&#20943;&#23567;&#65292;&#36825;&#20123;&#36712;&#36857;&#36924;&#36817;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10352</link><description>&lt;p&gt;
&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#21106;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#24050;&#25104;&#20026;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#24191;&#27867;&#24212;&#29992;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#24314;&#27169;&#65292;&#24120;&#24120;&#38656;&#35201;&#20248;&#21270;&#19968;&#20123;&#21442;&#25968;&#20197;&#26368;&#23567;&#21270;SW&#65292;&#35813;&#21442;&#25968;&#20805;&#24403;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#22240;&#20026;&#20855;&#26377;&#23494;&#24230;&#30340;&#27979;&#24230;&#22312;&#25968;&#20540;&#19978;&#26159;&#26080;&#27861;&#23454;&#29616;&#30340;&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#37117;&#23384;&#22312;&#30456;&#21516;&#30340;&#23376;&#38382;&#39064;&#65292;&#21363;&#26368;&#23567;&#21270;&#20999;&#21106;Wasserstein&#33021;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$&#30340;&#23646;&#24615;&#65292;&#21363;&#20004;&#20010;&#20855;&#26377;&#19982;&#19968;&#20010;&#27979;&#24230;&#30340;&#25903;&#25745;&#30456;&#21516;&#25968;&#37327;&#30340;&#31163;&#25955;&#22343;&#21248;&#27979;&#24230;&#20043;&#38388;&#30340;SW&#36317;&#31163;&#20316;&#20026;&#25903;&#25745;$Y \in \mathbb{R}^{n \times d}$&#20989;&#25968;&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#33021;&#37327;&#30340;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#65292;&#20197;&#21450;&#20854;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;$\mathcal{E}_p$&#65288;&#20351;&#29992;SW&#20013;&#30340;&#26399;&#26395;&#20272;&#35745;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06104</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#65306;&#27169;&#22411;&#19982;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#28145;&#24230;&#22270;&#32593;&#32476;&#65288;DGNs&#65289;&#30340;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#25512;&#21160;&#20102;&#22270;&#19978;&#23398;&#20064;&#30340;&#39046;&#22495;&#25104;&#29087;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#27491;&#22312;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#23578;&#26410;&#35299;&#20915;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#24613;&#38656;&#20351;DGNs&#36866;&#29992;&#20110;&#23454;&#26102;&#31995;&#32479;&#20013;&#38543;&#26102;&#38388;&#25512;&#31227;&#19981;&#26029;&#28436;&#21270;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#20026;&#20419;&#36827;&#21160;&#24577;&#22270;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#24403;&#21069;&#26368;&#26032;&#27010;&#35272;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#35780;&#20272;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in research on Deep Graph Networks (DGNs) has led to a maturation of the domain of learning on graphs. Despite the growth of this research field, there are still important challenges that are yet unsolved. Specifically, there is an urge of making DGNs suitable for predictive tasks on realworld systems of interconnected entities, which evolve over time. With the aim of fostering research in the domain of dynamic graphs, at first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs. Secondly, we conduct a fair performance comparison among the most popular proposed approaches, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches
&lt;/p&gt;</description></item><item><title>ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16922</link><description>&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#65306;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16922
&lt;/p&gt;
&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#31185;&#23398;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#31616;&#21270;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#20381;&#38752;&#38598;&#20307;&#27963;&#21160;&#21644;&#36866;&#24403;&#35843;&#25972;&#30340;&#36830;&#25509;&#26469;&#25191;&#34892;&#22797;&#26434;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#29983;&#29289;&#30382;&#23618;&#31070;&#32463;&#20803;&#26412;&#36136;&#19978;&#37117;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#35774;&#22791;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#30740;&#31350;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#30340;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#22797;&#21046;&#35814;&#32454;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#22810;&#20010;&#21442;&#25968;&#30340;&#24517;&#35201;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#24341;&#20837;&#20102;&#34920;&#36798;&#21147;&#24378;&#30340;&#27844;&#28431;&#23384;&#20648;&#22120;&#65288;ELM&#65289;&#31070;&#32463;&#20803;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#35745;&#31639;&#34920;&#36798;&#21147;&#65292;&#21516;&#26102;&#20063;&#38750;&#24120;&#39640;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ELM&#31070;&#32463;&#20803;&#20165;&#38656;&#35201;8,000&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#21305;&#37197;&#21069;&#36848;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20934;&#30830;&#30340;&#27169;&#22411;&#38656;&#35201;&#22810;&#20010;&#31867;&#20284;&#20110;&#23384;&#20648;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#31361;&#35302;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#27169;&#22411;D3FG&#65292;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;&#12290;&#22522;&#20110;&#21018;&#24615;&#20307;&#30340;&#21151;&#33021;&#22242;&#21644;&#36136;&#28857;&#30340;&#36830;&#25509;&#22120;&#21487;&#20197;&#20849;&#21516;&#24418;&#25104;&#22686;&#24378;&#37197;&#20307;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#29255;&#27573;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.13769</link><description>&lt;p&gt;
&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration. (arXiv:2306.13769v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#27169;&#22411;D3FG&#65292;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;&#12290;&#22522;&#20110;&#21018;&#24615;&#20307;&#30340;&#21151;&#33021;&#22242;&#21644;&#36136;&#28857;&#30340;&#36830;&#25509;&#22120;&#21487;&#20197;&#20849;&#21516;&#24418;&#25104;&#22686;&#24378;&#37197;&#20307;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#29255;&#27573;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;AI&#36741;&#21161;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#32473;&#23450;&#30446;&#26631;&#34507;&#30333;&#36136;&#30340;&#21475;&#34955;&#32467;&#26500;&#30340;&#20998;&#23376;&#12290;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#21407;&#23376;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#23558;&#21407;&#23376;&#35270;&#20026;&#22522;&#26412;&#32452;&#20214;&#65292;&#24182;&#29983;&#25104;&#21407;&#23376;&#20301;&#32622;&#21644;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#30340;&#29616;&#23454;&#29255;&#27573;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D3FG&#65292;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;&#12290;D3FG&#23558;&#20998;&#23376;&#20998;&#35299;&#20026;&#20004;&#31867;&#32452;&#20214;&#65306;&#23450;&#20041;&#20026;&#21018;&#24615;&#20307;&#30340;&#21151;&#33021;&#22242;&#21644;&#36136;&#28857;&#30340;&#36830;&#25509;&#22120;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;&#32452;&#20214;&#21487;&#20197;&#20849;&#21516;&#24418;&#25104;&#22686;&#24378;&#37197;&#20307;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#29255;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;D3FG&#23558;&#32452;&#20214;&#30340;&#20301;&#32622;&#12289;&#26041;&#21521;&#21644;&#31867;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#25193;&#25955;&#21040;&#19968;&#20010;&#20808;&#39564;&#20998;&#24067;&#65307;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#21435;&#22122;&#22120;&#36880;&#28176;&#21435;&#38500;&#19977;&#20010;&#21464;&#37327;&#30340;&#22122;&#22768;&#65292;&#20197;&#33719;&#24471;&#29616;&#23454;&#20998;&#23376;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#38024;&#23545;&#20843;&#20010;&#30446;&#26631;&#36827;&#34892;&#24211;&#29983;&#25104;&#65292;&#38024;&#23545;&#20116;&#31181;&#29616;&#26377;&#33647;&#29289;&#36827;&#34892;&#29255;&#27573;&#23436;&#21892;&#65292;&#20197;&#21450;&#38024;&#23545;&#20004;&#20010;&#26410;&#30693;&#21442;&#32771;&#20998;&#23376;&#30340;&#30446;&#26631;&#36827;&#34892;&#21435;&#26032;&#35774;&#35745;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#21697;&#36136;&#30340;&#20998;&#23376;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, AI-assisted drug design methods have been proposed to generate molecules given the pockets' structures of target proteins. Most of them are atom-level-based methods, which consider atoms as basic components and generate atom positions and types. In this way, however, it is hard to generate realistic fragments with complicated structures. To solve this, we propose D3FG, a functional-group-based diffusion model for pocket-specific molecule generation and elaboration. D3FG decomposes molecules into two categories of components: functional groups defined as rigid bodies and linkers as mass points. And the two kinds of components can together form complicated fragments that enhance ligand-protein interactions.  To be specific, in the diffusion process, D3FG diffuses the data distribution of the positions, orientations, and types of the components into a prior distribution; In the generative process, the noise is gradually removed from the three variables by denoisers parame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#34892;&#36208;&#36339;&#36291;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24179;&#28369;&#33021;&#37327;&#20989;&#25968;&#12289;&#20351;&#29992; Langevin Markov &#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#31639;&#27861;&#21644;&#19968;&#27493;&#21435;&#22122;&#30340;&#25237;&#23556;&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#20915;&#31163;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#37319;&#26679;&#38590;&#39064;&#12290;&#21516;&#26102;&#22312;&#25239;&#20307;&#34507;&#30333;&#36136;&#30340;&#29983;&#25104;&#24314;&#27169;&#19978;&#36827;&#34892;&#20102;&#24212;&#29992;&#21644;&#27979;&#35797;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#24471;&#20998;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.12360</link><description>&lt;p&gt;
&#31163;&#25955;&#34892;&#36208;&#36339;&#36291;&#37319;&#26679;&#29992;&#20110;&#34507;&#30333;&#36136;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Protein Discovery with Discrete Walk-Jump Sampling. (arXiv:2306.12360v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#34892;&#36208;&#36339;&#36291;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24179;&#28369;&#33021;&#37327;&#20989;&#25968;&#12289;&#20351;&#29992; Langevin Markov &#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#31639;&#27861;&#21644;&#19968;&#27493;&#21435;&#22122;&#30340;&#25237;&#23556;&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#20915;&#31163;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#37319;&#26679;&#38590;&#39064;&#12290;&#21516;&#26102;&#22312;&#25239;&#20307;&#34507;&#30333;&#36136;&#30340;&#29983;&#25104;&#24314;&#27169;&#19978;&#36827;&#34892;&#20102;&#24212;&#29992;&#21644;&#27979;&#35797;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#24471;&#20998;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#24179;&#28369;&#33021;&#37327;&#20989;&#25968;&#12289;&#20351;&#29992; Langevin Markov &#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#20174;&#24179;&#28369;&#25968;&#25454;&#27969;&#24418;&#20013;&#37319;&#26679;&#65292;&#24182;&#20351;&#29992;&#19968;&#27493;&#21435;&#22122;&#25237;&#23556;&#22238;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#22256;&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31163;&#25955;&#34892;&#36208;&#36339;&#36291;&#37319;&#26679;&#24418;&#24335;&#21270;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#30340;&#25913;&#36827;&#26679;&#26412;&#36136;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#20165;&#38656;&#35201;&#19968;&#20010;&#22122;&#22768;&#27700;&#24179;&#26469;&#31616;&#21270;&#35757;&#32451;&#21644;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#25239;&#20307;&#34507;&#30333;&#36136;&#30340;&#29983;&#25104;&#24314;&#27169;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#24471;&#20998;&#20197;&#23545;&#34507;&#30333;&#36136;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#20248;&#21270;&#21644;&#37319;&#26679;&#25105;&#20204;&#30340;&#27169;&#22411;&#29992;&#20110;&#25552;&#35758;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#24471;&#20998;&#65292;97-100%&#30340;&#29983;&#25104;&#26679;&#21697;&#21487;&#20197;&#25104;&#21151;&#34920;&#36798;&#21644;&#32431;&#21270;&#65292;&#24182;&#19988;35%&#30340;&#21151;&#33021;&#35774;&#35745;&#22312;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#26174;&#31034;&#20986;&#19982;&#24050;&#30693;&#21151;&#33021;&#25239;&#20307;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our Discrete Walk-Jump Sampling formalism combines the maximum likelihood training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the distributional conformity score to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100% of generated samples are successfully expressed and purified and 35% of functional designs show equal or improved binding affinity compared to known functional antibodies on the first attempt in a sing
&lt;/p&gt;</description></item><item><title>MUBen&#35780;&#20272;&#19981;&#21516;&#39592;&#24178;&#21644;UQ&#27169;&#22411;&#32452;&#21512;&#23545;&#20998;&#23376;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#36807;&#25311;&#21512;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10060</link><description>&lt;p&gt;
MUBen&#65306;&#35780;&#20272;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction. (arXiv:2306.10060v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10060
&lt;/p&gt;
&lt;p&gt;
MUBen&#35780;&#20272;&#19981;&#21516;&#39592;&#24178;&#21644;UQ&#27169;&#22411;&#32452;&#21512;&#23545;&#20998;&#23376;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#36807;&#25311;&#21512;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#20110;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20998;&#23376;&#25968;&#25454;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#26399;&#38388;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#36807;&#24230;&#33258;&#20449;&#39044;&#27979;&#33853;&#22312;&#20102;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#26657;&#20934;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;UQ&#26041;&#27861;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#26041;&#27861;&#37117;&#20250;&#23548;&#33268;&#24615;&#33021;&#25913;&#21892;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20351;&#29992;UQ&#26469;&#25913;&#21892;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#36873;&#25321;&#36866;&#21512;&#30340;&#39592;&#24178;&#21644;UQ&#26041;&#27861;&#20197;&#21487;&#38752;&#22320;&#20272;&#35745;&#20998;&#23376;&#19981;&#30830;&#23450;&#24615;&#30340;&#36807;&#31243;&#20173;&#28982;&#26159;&#26410;&#32463;&#25506;&#32034;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUBen&#65292;&#35780;&#20272;&#19981;&#21516;&#30340;&#39592;&#24178;&#21644;UQ&#27169;&#22411;&#32452;&#21512;&#65292;&#20197;&#37327;&#21270;&#23427;&#20204;&#22312;&#23646;&#24615;&#39044;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24494;&#35843;&#20351;&#29992;&#19981;&#21516;&#20998;&#23376;&#25551;&#36848;&#31526;&#30340;&#21508;&#31181;&#39592;&#24178;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Transformer models pre-trained on massive unlabeled molecular data have shown great success in predicting molecular properties. However, these models can be prone to overfitting during fine-tuning, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have used UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different combinations of backbone and UQ models to quantify their performance for both property prediction and uncertainty estimation. By fine-tuning various backbone molecular representation models using different molecular descrip
&lt;/p&gt;</description></item><item><title>Jumanji&#26159;JAX&#20013;&#19968;&#22871;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;</title><link>http://arxiv.org/abs/2306.09884</link><description>&lt;p&gt;
Jumanji: JAX&#20013;&#19968;&#22871;&#22810;&#26679;&#21270;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX. (arXiv:2306.09884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09884
&lt;/p&gt;
&lt;p&gt;
Jumanji&#26159;JAX&#20013;&#19968;&#22871;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22312;&#25512;&#21160;AI&#31639;&#27861;&#30340;&#21457;&#23637;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#38656;&#35201;&#27169;&#25311;&#29615;&#22659;&#20855;&#22791;&#24615;&#33021;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jumanji&#65292;&#36825;&#26159;&#19968;&#22871;&#35774;&#35745;&#29992;&#20110;&#24555;&#36895;&#12289;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#19981;&#21516;RL&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;Jumanji&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#30340;&#29615;&#22659;&#65292;&#19987;&#27880;&#20110;&#24037;&#19994;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#20197;&#21450;&#25361;&#25112;&#24615;&#30340;&#19968;&#33324;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;JAX&#21644;GPU&#12289;TPU&#31561;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#29575;&#65292;Jumanji&#33021;&#22815;&#36805;&#36895;&#36845;&#20195;&#30740;&#31350;&#24605;&#36335;&#21644;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#26368;&#32456;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;&#19982;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22871;&#20214;&#19981;&#21516;&#65292;Jumanji&#20855;&#26377;&#39640;&#24230;&#21487;&#23450;&#21046;&#24615;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#20854;&#38656;&#27714;&#35843;&#25972;&#21021;&#22987;&#29366;&#24577;&#20998;&#24067;&#21644;&#38382;&#39064;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms. In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications. Therefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable. Jumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks. By leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents. Unlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs. Further
&lt;/p&gt;</description></item><item><title>MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07952</link><description>&lt;p&gt;
MOFI: &#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07952
&lt;/p&gt;
&lt;p&gt;
MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411; MOFI&#65292;&#26088;&#22312;&#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;MOFI &#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#26377;&#20004;&#28857;&#19981;&#21516;&#65306;&#65288;i&#65289;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#65288;ii&#65289;&#35757;&#32451;&#37197;&#26041;&#12290;&#22312;&#25968;&#25454;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#20174; alt-text &#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992; CLIP &#27169;&#22411;&#36873;&#25321;&#27491;&#30830;&#30340;&#23454;&#20307;&#20316;&#20026;&#22270;&#20687;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20174; web &#19978;&#25366;&#25496;&#30340;&#25968;&#21313;&#20159;&#20010;&#22270;&#20687;&#25991;&#26412;&#23545;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102; Image-to-Entities&#65288;I2E&#65289;&#36825;&#19968;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 10 &#20159;&#24352;&#22270;&#20687;&#21644; 200 &#19975;&#20010;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#28085;&#30422;&#20102;&#37326;&#22806;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#22522;&#20110; I2E &#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12289;&#23545;&#27604;&#24230;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#36807;&#31243;&#20013;&#20302;&#25509;&#36817;&#24230;&#25968;&#25454;&#21644;&#39640;&#25509;&#36817;&#24230;&#25968;&#25454;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04590</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#36807;&#31243;&#20013;&#20302;&#25509;&#36817;&#24230;&#25968;&#25454;&#21644;&#39640;&#25509;&#36817;&#24230;&#25968;&#25454;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#26657;&#20934;&#23545;&#20110;&#25552;&#20379;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#12290;&#24050;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#25509;&#36817;&#24230;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#22312;&#20302;&#25509;&#36817;&#24615;&#25968;&#25454;&#65288;&#21363;&#20998;&#24067;&#30340;&#31232;&#30095;&#21306;&#22495;&#65289;&#20013;&#20542;&#21521;&#20110;&#26356;&#33258;&#20449;&#65292;&#32780;&#22312;&#39640;&#25509;&#36817;&#24615;&#26679;&#26412;&#20013;&#34920;&#29616;&#20986;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30740;&#31350;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#35266;&#23519;&#21040;&#65306;1&#65289;&#25509;&#36817;&#24230;&#20559;&#24046;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#22823;&#23567;&#20043;&#38388;&#65307;2&#65289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#27604;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25509;&#36817;&#24230;&#20559;&#24046;&#30340;&#24433;&#21709;&#65307;3&#65289;&#21363;&#20351;&#37319;&#29992;&#27969;&#34892;&#30340;&#26657;&#20934;&#31639;&#27861;&#22914;&#28201;&#24230;&#32553;&#25918;&#65292;&#25509;&#36817;&#24230;&#20559;&#24046;&#20063;&#20250;&#25345;&#32493;&#23384;&#22312;&#65307;4&#65289;&#27169;&#22411;&#22312;&#20302;&#25509;&#36817;&#24615;&#26679;&#26412;&#19978;&#30340;&#36807;&#25311;&#21512;&#31243;&#24230;&#27604;&#39640;&#25509;&#36817;&#24615;&#26679;&#26412;&#26356;&#20005;&#37325;&#12290;&#22312;&#36825;&#20123;&#23454;&#35777;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProCal&#12290;
&lt;/p&gt;
&lt;p&gt;
Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, 
&lt;/p&gt;</description></item><item><title>HUBL&#26159;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#30340;&#24191;&#27867;&#31867;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#33021;&#25552;&#21319;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;HUBL&#21487;&#36890;&#36807;&#35843;&#25972;&#22870;&#21169;&#21644;&#25240;&#25187;&#22240;&#23376;&#26469;&#31616;&#21333;&#23454;&#29616;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HUBL&#33021;&#22815;&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.00321</link><description>&lt;p&gt;
&#36890;&#36807;&#21551;&#21457;&#24335;&#31574;&#30053;&#28151;&#21512;&#25913;&#36827;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Offline RL by Blending Heuristics. (arXiv:2306.00321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00321
&lt;/p&gt;
&lt;p&gt;
HUBL&#26159;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#30340;&#24191;&#27867;&#31867;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#33021;&#25552;&#21319;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;HUBL&#21487;&#36890;&#36807;&#35843;&#25972;&#22870;&#21169;&#21644;&#25240;&#25187;&#22240;&#23376;&#26469;&#31616;&#21333;&#23454;&#29616;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HUBL&#33021;&#22815;&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21551;&#21457;&#24335;&#31574;&#30053;&#28151;&#21512;&#65288;HUBL&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#30340;&#24191;&#27867;&#31867;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#33021;&#25552;&#21319;&#25216;&#26415;&#12290;HUBL&#20462;&#25913;&#20102;&#36825;&#20123;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;Bellman&#25805;&#20316;&#31526;&#65292;&#37096;&#20998;&#29992;&#21551;&#21457;&#24335;&#30340;&#33945;&#29305;&#21345;&#32599;&#22238;&#25253;&#26367;&#25442;&#20102;&#20540;&#20989;&#25968;&#22238;&#28335;&#12290;&#23545;&#20110;&#22238;&#25253;&#26356;&#39640;&#30340;&#36712;&#36857;&#65292;HUBL&#26356;&#22810;&#22320;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#65292;&#36739;&#23569;&#20381;&#36182;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#12290;&#21542;&#21017;&#65292;&#23427;&#20250;&#26356;&#21152;&#20506;&#37325;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24819;&#27861;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22870;&#21169;&#21644;&#25240;&#25187;&#22240;&#23376;&#26469;&#31616;&#21333;&#23454;&#29616;&#65292;&#20351;HUBL&#21487;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#35768;&#22810;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;HUBL&#38477;&#20302;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20854;&#26377;&#38480;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#23454;&#35777;&#26041;&#38754;&#34920;&#26126;&#20102;HUBL&#23545;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22238;&#28335;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31574;&#30053;&#36136;&#37327;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;9%&#30340;&#25928;&#26524;&#65292;&#22312;D4RL&#21644;Meta-Wo&#30340;27&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Heuristic Blending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies Bellman operators used in these algorithms, partially replacing the bootstrapped values with Monte-Carlo returns as heuristics. For trajectories with higher returns, HUBL relies more on heuristics and less on bootstrapping; otherwise, it leans more heavily on bootstrapping. We show that this idea can be easily implemented by relabeling the offline datasets with adjusted rewards and discount factors, making HUBL readily usable by many existing offline RL implementations. We theoretically prove that HUBL reduces offline RL's complexity and thus improves its finite-sample performance. Furthermore, we empirically demonstrate that HUBL consistently improves the policy quality of four state-of-the-art bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9% on average over 27 datasets of the D4RL and Meta-Wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.15852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#65306;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#24819;&#30340;&#25991;&#26412;&#12290;&#33258;&#30456;&#30683;&#30462;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24187;&#35273;&#24418;&#24335;&#65292;&#25351;&#30340;&#26159;&#35821;&#35328;&#27169;&#22411;&#22312;&#21516;&#19968;&#35821;&#22659;&#20013;&#29983;&#25104;&#20004;&#20010;&#30683;&#30462;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#20808;&#36827;&#12289;&#32463;&#36807;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#33258;&#30456;&#30683;&#30462;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12289;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#33879;&#21517;&#30340;&#36824;&#26159;&#19981;&#22826;&#20986;&#21517;&#30340;&#35805;&#39064;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#30456;&#30683;&#30462;&#37117;&#32463;&#24120;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.15759</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30452;&#25509;&#22312;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;DMs&#30340;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#32452;&#21512;&#24615;&#23646;&#24615;&#65292;&#22823;&#37327;&#22122;&#38899;&#27880;&#20837;&#21040;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#12290;LDMs&#20351;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20943;&#23569;&#21040;&#26356;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#35757;&#32451;DMs&#26356;&#21152;&#39640;&#25928;&#21644;&#24555;&#36895;&#12290;&#19982;[Ghalebikesabi&#31561;&#20154;&#65292;2023]&#39044;&#20808;&#29992;&#20844;&#20849;&#25968;&#25454;&#39044;&#35757;&#32451;DMs&#65292;&#28982;&#21518;&#20877;&#29992;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#20165;&#24494;&#35843;LDMs&#20013;&#19981;&#21516;&#23618;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#20197;&#33719;&#24471;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#25972;&#20010;DM&#24494;&#35843;&#65292;&#21487;&#20943;&#23569;&#22823;&#32422;96%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
&lt;/p&gt;</description></item><item><title>FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;</title><link>http://arxiv.org/abs/2305.14392</link><description>&lt;p&gt;
FEDORA&#65306;&#29992;&#20110;&#21453;&#24212;&#34892;&#20026;&#30340;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FEDORA: Flying Event Dataset fOr Reactive behAvior. (arXiv:2305.14392v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14392
&lt;/p&gt;
&lt;p&gt;
FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20307;&#22312;&#39134;&#34892;&#20013;&#20351;&#29992;&#26497;&#23569;&#25968;&#30340;&#31070;&#32463;&#20803;&#21644;&#26497;&#20302;&#30340;&#22833;&#35823;&#29575;&#25191;&#34892;&#22797;&#26434;&#30340;&#39640;&#36895;&#26426;&#21160;&#65292;&#31361;&#26174;&#20102;&#36825;&#20123;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#20107;&#20214;&#39537;&#21160;&#30828;&#20214;&#36880;&#28176;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#21253;&#25324;&#20960;&#20010;&#29420;&#31435;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#22914;&#20809;&#27969;&#20272;&#35745;&#12289;&#28145;&#24230;&#20272;&#35745;&#12289;&#21516;&#26102;&#23450;&#20301;&#19982;&#24314;&#22270;&#65288;SLAM&#65289;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20182;&#20204;&#24517;&#39035;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#25968;&#25454;&#38598;&#21482;&#25552;&#20379;&#25152;&#38656;&#25968;&#25454;&#30340;&#36873;&#23450;&#23376;&#38598;&#65292;&#36825;&#20351;&#24471;&#32593;&#32476;&#38388;&#30340;&#19968;&#33268;&#24615;&#38590;&#20197;&#23454;&#29616;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#21478;&#19968;&#20010;&#38480;&#21046;&#26159;&#25552;&#20379;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FEDORA&#65292;
&lt;/p&gt;
&lt;p&gt;
The ability of living organisms to perform complex high speed manoeuvers in flight with a very small number of neurons and an incredibly low failure rate highlights the efficacy of these resource-constrained biological systems. Event-driven hardware has emerged, in recent years, as a promising avenue for implementing complex vision tasks in resource-constrained environments. Vision-based autonomous navigation and obstacle avoidance consists of several independent but related tasks such as optical flow estimation, depth estimation, Simultaneous Localization and Mapping (SLAM), object detection, and recognition. To ensure coherence between these tasks, it is imperative that they be trained on a single dataset. However, most existing datasets provide only a selected subset of the required data. This makes inter-network coherence difficult to achieve. Another limitation of existing datasets is the limited temporal resolution they provide. To address these limitations, we present FEDORA, a 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#20984;&#32452;&#21512;&#30340;&#34920;&#36798;&#24615;&#25439;&#22833;&#65292;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#26368;&#26032;&#30340;&#31639;&#27861;&#21487;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;IBP&#36793;&#30028;&#20043;&#38388;&#30340;&#31616;&#21333;&#20984;&#32452;&#21512;&#36827;&#34892;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13991</link><description>&lt;p&gt;
&#22522;&#20110;&#20984;&#32452;&#21512;&#30340;&#34920;&#36798;&#24615;&#25439;&#22833;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Expressive Losses for Verified Robustness via Convex Combinations. (arXiv:2305.13991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20984;&#32452;&#21512;&#30340;&#34920;&#36798;&#24615;&#25439;&#22833;&#65292;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#26368;&#26032;&#30340;&#31639;&#27861;&#21487;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;IBP&#36793;&#30028;&#20043;&#38388;&#30340;&#31616;&#21333;&#20984;&#32452;&#21512;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#65288;&#25200;&#21160;&#21306;&#22495;&#30340;&#23376;&#38598;&#65289;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#38480;&#65292;&#25110;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#19978;&#24341;&#20837;&#21487;&#39564;&#35777;&#24615;&#26469;&#35757;&#32451;&#20855;&#26377;&#24050;&#39564;&#35777;&#40065;&#26834;&#24615;&#30340;&#32593;&#32476;&#12290;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20851;&#38190;&#22312;&#20110;&#25152;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#21305;&#37197;&#35757;&#32451;&#21518;&#35201;&#20351;&#29992;&#30340;&#39564;&#35777;&#22120;&#30340;&#32039;&#23494;&#24230;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#23450;&#20041;&#20102;&#34920;&#36798;&#21147;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;IBP&#36793;&#30028;&#20043;&#38388;&#30340;&#31616;&#21333;&#20984;&#32452;&#21512;&#26469;&#28385;&#36275;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;CC-IBP&#21644;MTL-IBP&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#22343;&#21487;&#20197;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#20854;&#27010;&#24565;&#19978;&#26159;&#31616;&#21333;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;TinyImageNet&#21644;&#32553;&#23567;&#30340;ImageNet&#19978;&#65292;&#23545;&#20110;&#21322;&#24452;&#20026;$ \frac{1} {255} $&#30340;$ \ell_ \infty $&#25200;&#21160;&#65292;MTL-IBP&#21487;&#20197;&#23558;&#25991;&#29486;&#20013;&#26368;&#20339;&#26631;&#20934;&#21644;&#39564;&#35777;&#20934;&#30830;&#24615;&#20174;$1.98\%$&#25552;&#39640;&#21040;$3.92\%$&#65292;&#21516;&#26102;&#20165;&#20381;&#36182;&#20110;&#21333;&#27493;&#33258;&#36866;&#24212;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to train networks for verified adversarial robustness, previous work typically over-approximates the worst-case loss over (subsets of) perturbation regions or induces verifiability on top of adversarial training. The key to state-of-the-art performance lies in the expressivity of the employed loss function, which should be able to match the tightness of the verifiers to be employed post-training. We formalize a definition of expressivity, and show that it can be satisfied via simple convex combinations between adversarial attacks and IBP bounds. We then show that the resulting algorithms, named CC-IBP and MTL-IBP, yield state-of-the-art results across a variety of settings in spite of their conceptual simplicity. In particular, for $\ell_\infty$ perturbations of radius $\frac{1}{255}$ on TinyImageNet and downscaled ImageNet, MTL-IBP improves on the best standard and verified accuracies from the literature by from $1.98\%$ to $3.92\%$ points while only relying on single-step ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11997</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#20559;&#31227;&#65292;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#21487;&#33021;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#21453;&#20107;&#23454;&#35299;&#37322;&#20248;&#21270;&#20013;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#23558;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#38752;&#36817;&#25968;&#25454;&#27969;&#24418;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#39640;&#27010;&#29575;&#40065;&#26834;&#24615;&#12290;&#26032;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{&lt;}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#22312;&#21019;&#24314;&#35745;&#31639;&#26426;&#35270;&#35273;&#20998;&#31867;&#21644;&#26816;&#27979;&#27169;&#22411;&#26102;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#21644;&#25913;&#36827;&#27169;&#22411;&#19978;&#30340;&#38382;&#39064;&#65292;&#26377;&#25928;&#20943;&#23569;&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11927</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#21738;&#20123;&#22320;&#26041;&#20250;&#20986;&#38169;&#65311;&#20351;&#29992;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#25214;&#20986;&#24182;&#25913;&#36827;CV&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Where does a computer vision model make mistakes? Using interactive visualizations to find where and how CV models can improve. (arXiv:2305.11927v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11927
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#22312;&#21019;&#24314;&#35745;&#31639;&#26426;&#35270;&#35273;&#20998;&#31867;&#21644;&#26816;&#27979;&#27169;&#22411;&#26102;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#21644;&#25913;&#36827;&#27169;&#22411;&#19978;&#30340;&#38382;&#39064;&#65292;&#26377;&#25928;&#20943;&#23569;&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#36807;&#31243;&#65292;&#32780;&#20351;&#32456;&#31471;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#12289;&#26816;&#26597;&#21644;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#35282;&#24050;&#32463;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#20855;&#26377;&#19981;&#21516;&#32423;&#21035;&#26426;&#22120;&#23398;&#20064;&#19987;&#19994;&#25216;&#33021;&#30340;&#32456;&#31471;&#29992;&#25143;&#30340;&#20307;&#39564;&#65292;&#25105;&#20204;&#22312;Sprite&#30340;&#19978;&#19979;&#25991;&#20013;&#35774;&#35745;&#21644;&#35780;&#20272;&#20102;&#20004;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20026;&#20174;&#35270;&#39057;&#20013;&#25277;&#21462;&#30340;&#22270;&#20687;&#21019;&#24314;CV&#20998;&#31867;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#21487;&#35270;&#21270;&#24037;&#20855;&#22914;&#20309;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#24490;&#29615;&#30340;&#19968;&#37096;&#20998;&#65292;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#65288;&#35780;&#20272;&#65289;&#21644;&#36873;&#25321;&#65288;&#35268;&#21010;&#65289;&#27169;&#22411;&#23384;&#22312;&#38382;&#39064;&#30340;&#22270;&#20687;&#65292;&#24182;&#25913;&#21892;&#27491;&#22312;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36825;&#20123;&#21487;&#35270;&#21270;&#24037;&#20855;&#30340;&#29992;&#25143;&#22312;&#26356;&#24191;&#27867;&#30340;&#27169;&#22411;&#38169;&#35823;&#31867;&#22411;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#34892;&#20026;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#26041;&#38754;&#21457;&#29616;&#20102;&#26356;&#22810;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35774;&#35745;&#24072;&#21019;&#24314;&#21644;&#25913;&#36827;CV&#27169;&#22411;&#25152;&#38656;&#30340;&#28508;&#22312;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating Computer Vision (CV) models remains a complex and taxing practice for end-users to build, inspect, and improve these models. Interactive ML perspectives have helped address some of these issues by considering a teacher-in-the-loop where planning, teaching, and evaluating tasks take place. To improve the experience of end-users with various levels of ML expertise, we designed and evaluated two interactive visualizations in the context of Sprite, a system for creating CV classification and detection models for images originating from videos. We study how these visualizations, as part of the machine teaching loop, help users identify (evaluate) and select (plan) images where a model is struggling and improve the model being trained. We found that users who had used the visualizations found more images across a wider set of potential types of model errors, as well as in assessing and contrasting the prediction behavior of one or more models, thus reducing the potential effort requ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13138</link><description>&lt;p&gt;
&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#30340;&#26356;&#26032;&#31561;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26827;&#31867;&#28216;&#25103;&#31561;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#20013;&#65292;&#21363;&#26102;&#20462;&#27491;&#65288;&#25110;&#26500;&#24314;&#65289;&#31574;&#30053;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26159;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#30340;&#20851;&#38190;&#12290;&#19968;&#20123;&#30740;&#31350;&#23558;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#25193;&#23637;&#21040;&#26356;&#26222;&#36941;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25169;&#20811;&#20013;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#38543;&#30528;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#30340;&#22686;&#21152;&#32780;&#24555;&#36895;&#22686;&#38271;&#30340;&#23376;&#28216;&#25103;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#36739;&#22823;&#26102;&#19981;&#36215;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#32780;&#19981;&#26159;&#23376;&#28216;&#25103;&#27010;&#24565;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#27169;&#25311;&#21516;&#27493;&#23398;&#20064;&#31639;&#27861;&#30340;&#26356;&#26032;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#24341;&#20837;&#19968;&#31995;&#21015;&#21407;&#21017;&#19978;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#20026;&#26032;&#30340;&#19968;&#20010;&#31995;&#21015;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20854;&#22312;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.09097</link><description>&lt;p&gt;
&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sheaf Neural Networks for Graph-based Recommender Systems. (arXiv:2304.09097v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09097
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20854;&#22312;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Graph&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;Graph&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#22312;&#20110;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#21487;&#20197;&#33258;&#28982;&#22320;&#24314;&#27169;&#20026;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21487;&#20197;&#26159;&#29992;&#25143;&#25110;&#39033;&#30446;&#65292;&#36793;&#20195;&#34920;&#20559;&#22909;&#20851;&#31995;&#12290; &#22312;&#24403;&#21069;&#30340;Graph&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20013;&#65292;&#33410;&#28857;&#29992;&#22312;&#35757;&#32451;&#26102;&#23398;&#20064;&#21040;&#30340;&#38745;&#24577;&#21521;&#37327;&#34920;&#31034;&#12290;&#36825;&#31181;&#38745;&#24577;&#21521;&#37327;&#21487;&#33021;&#21482;&#36866;&#29992;&#20110;&#25429;&#25417;&#23450;&#20041;&#23427;&#20204;&#30340;&#19968;&#20123;&#29992;&#25143;&#25110;&#39033;&#30446;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#21551;&#21457;&#33539;&#30068;&#35770;&#30340;&#27169;&#22411;&#65306;Sheaf&#31070;&#32463;&#32593;&#32476;&#12290;Sheaf&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#36830;&#25509;&#30340;&#25289;&#26222;&#25289;&#26031;&#21487;&#20197;&#36890;&#36807;&#23558;&#27599;&#20010;&#33410;&#28857;&#65288;&#20197;&#21450;&#36793;&#65289;&#19982;&#21521;&#37327;&#31354;&#38388;&#32780;&#19981;&#26159;&#21333;&#20010;&#21521;&#37327;&#30456;&#20851;&#32852;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26356;&#20016;&#23500;&#65292;&#24182;&#20801;&#35768;&#22312;&#25512;&#29702;&#26102;&#36873;&#25321;&#27491;&#30830;&#30340;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Graph Neural Networks has resulted in wide adoption by many applications, including recommendation systems. The reason for Graph Neural Networks' superiority over other approaches is that many problems in recommendation systems can be naturally modeled as graphs, where nodes can be either users or items and edges represent preference relationships. In current Graph Neural Network approaches, nodes are represented with a static vector learned at training time. This static vector might only be suitable to capture some of the nuances of users or items they define. To overcome this limitation, we propose using a recently proposed model inspired by category theory: Sheaf Neural Networks. Sheaf Neural Networks, and its connected Laplacian, can address the previous problem by associating every node (and edge) with a vector space instead than a single vector. The vector space representation is richer and allows picking the proper representation at inference time. This approa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#21644;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06094</link><description>&lt;p&gt;
&#33021;&#37327;&#24341;&#23548;&#30340;&#29109;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#21644;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#65288;EBMs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24050;&#32463;&#26377;&#25968;&#21313;&#24180;&#30340;&#21382;&#21490;&#12290;&#33258;&#20004;&#21315;&#24180;&#20195;&#36215;&#65292;&#19968;&#30452;&#26377;&#24456;&#22810;&#39640;&#25928;&#30340;&#26041;&#27861;&#36890;&#36807;&#33021;&#37327;&#21183;&#65288;&#38750;&#24402;&#19968;&#21270;&#30340;&#20284;&#28982;&#20989;&#25968;&#65289;&#26469;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;OT&#27714;&#35299;&#22120;&#65292;&#21463;&#21040;&#30340;&#25506;&#32034;&#35201;&#23569;&#24471;&#22810;&#65292;&#20165;&#26377;&#19968;&#20123;&#36817;&#26399;&#30340;&#30740;&#31350;&#65288;&#19981;&#21253;&#25324;&#21033;&#29992;OT&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#38382;&#39064;&#30340;WGAN&#26041;&#27861;&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;EBMs&#21644;&#29109;&#27491;&#21017;&#21270;OT&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#21033;&#29992;&#21069;&#32773;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#25216;&#26415;&#25913;&#36827;&#26469;&#20016;&#23500;&#21518;&#32773;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#26631;&#20934;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#31616;&#21333;&#36215;&#35265;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#31616;&#30701;&#21644;&#38271;&#36305;&#30340;EBMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) are known in the Machine Learning community for the decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. We validate the applicability of our method on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For the sake of simplicity, we choose simple short- and long- run EBMs as a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#20998;&#24067;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23884;&#20837;&#65292;&#22312;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03376</link><description>&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#21160;&#24577;&#21644;&#20960;&#20309;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpretable statistical representations of neural population dynamics and geometry. (arXiv:2304.03376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#20998;&#24067;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23884;&#20837;&#65292;&#22312;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#20803;&#32676;&#20307;&#30340;&#21160;&#24577;&#36890;&#24120;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#20960;&#20309;&#21644;&#21160;&#24577;&#23545;&#32534;&#30721;&#30456;&#20851;&#34892;&#20026;&#21464;&#37327;&#30340;&#36129;&#29486;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#30456;&#36712;&#29305;&#24449;&#30340;&#32479;&#35745;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32479;&#35745;&#34920;&#31034;&#21487;&#20197;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#23454;&#20363;&#36827;&#34892;&#25512;&#24191;&#65292;&#20197;&#21306;&#20998;&#35745;&#31639;&#26426;&#21046;&#65292;&#22312;&#20855;&#26377;&#20960;&#20309;&#23545;&#24212;&#30340;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#35299;&#37322;&#23884;&#20837;&#31070;&#32463;&#21160;&#21147;&#23398;&#65292;&#24182;&#24320;&#21457;&#20855;&#26377;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20351;&#29992;&#20869;&#22312;&#27969;&#24418;&#32467;&#26500;&#20248;&#20110;&#26102;&#38388;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of neuron populations during diverse tasks often evolve on low-dimensional manifolds. However, it remains challenging to discern the contributions of geometry and dynamics for encoding relevant behavioural variables. Here, we introduce an unsupervised geometric deep learning framework for representing non-linear dynamical systems based on statistical distributions of local phase portrait features. Our method provides robust geometry-aware or geometry-agnostic representations for the unbiased comparison of dynamics based on measured trajectories. We demonstrate that our statistical representation can generalise across neural network instances to discriminate computational mechanisms, obtain interpretable embeddings of neural dynamics in a primate reaching task with geometric correspondence to hand kinematics, and develop a decoding algorithm with state-of-the-art accuracy. Our results highlight the importance of using the intrinsic manifold structure over temporal informati
&lt;/p&gt;</description></item><item><title>DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00133</link><description>&lt;p&gt;
DeforestVis&#65306;&#20351;&#29992;&#20195;&#29702;&#20915;&#31574;&#26641;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00133
&lt;/p&gt;
&lt;p&gt;
DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#65288;&#21644;&#20851;&#38190;&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#22686;&#21152;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#26356;&#26131;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;ML&#12290;&#35299;&#37322;&#22797;&#26434;ML&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#19988;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65288;&#20363;&#22914;&#35268;&#21017;&#38598;&#21644;&#20915;&#31574;&#26641;&#65289;&#65292;&#20197;&#36275;&#22815;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#65292;&#20294;&#26356;&#31616;&#21333;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#35268;&#21017;&#38598;&#21487;&#20197;&#21464;&#24471;&#38750;&#24120;&#20887;&#38271;&#65292;&#21253;&#21547;&#35768;&#22810;if-else&#35821;&#21477;&#65292;&#32780;&#20915;&#31574;&#26641;&#30340;&#28145;&#24230;&#20250;&#38543;&#30528;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;ML&#27169;&#22411;&#32780;&#36805;&#36895;&#22686;&#21152;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#20854;&#26680;&#24515;&#30446;&#26631;&#65292;&#25552;&#20379;&#29992;&#25143;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;DeforestVis&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20351;&#29992;&#33258;&#36866;&#24212;&#22686;&#24378;&#65288;AdaBoost&#65289;&#25216;&#26415;&#29983;&#25104;&#30340;&#20195;&#29702;&#20915;&#31574;&#26641;&#65288;&#19968;&#32423;&#20915;&#31574;&#26641;&#65289;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#23545;&#22797;&#26434;ML&#27169;&#22411;&#34892;&#20026;&#30340;&#21451;&#22909;&#24635;&#32467;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity of machine learning (ML) models increases and the applications in different (and critical) domains grow, there is a strong demand for more interpretable and trustworthy ML. One straightforward and model-agnostic way to interpret complex ML models is to train surrogate models, such as rule sets and decision trees, that sufficiently approximate the original ones while being simpler and easier-to-explain. Yet, rule sets can become very lengthy, with many if-else statements, and decision tree depth grows rapidly when accurately emulating complex ML models. In such cases, both approaches can fail to meet their core goal, providing users with model interpretability. We tackle this by proposing DeforestVis, a visual analytics tool that offers user-friendly summarization of the behavior of complex ML models by providing surrogate decision stumps (one-level decision trees) generated with the adaptive boosting (AdaBoost) technique. Our solution helps users to explore the comple
&lt;/p&gt;</description></item><item><title>Magnushammer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#22823;&#24133;&#24230;&#36229;&#36234;&#20256;&#32479;&#31526;&#21495;&#31995;&#32479;&#65292;&#24182;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#25552;&#39640;&#21040;71.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.04488</link><description>&lt;p&gt;
Magnushammer: &#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04488
&lt;/p&gt;
&lt;p&gt;
Magnushammer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#22823;&#24133;&#24230;&#36229;&#36234;&#20256;&#32479;&#31526;&#21495;&#31995;&#32479;&#65292;&#24182;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#25552;&#39640;&#21040;71.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#25552;&#36873;&#25321;&#26159;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#24120;&#24120;&#20351;&#29992;&#22797;&#26434;&#30340;&#31526;&#21495;&#26041;&#27861;&#65292;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#24037;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#31070;&#32463;&#36716;&#25442;&#22120;&#30340;Magnushammer&#26041;&#27861;&#21487;&#20197;&#22823;&#24133;&#24230;&#22320;&#36229;&#36234;&#20256;&#32479;&#30340;&#31526;&#21495;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#65292;Magnushammer&#30340;&#35777;&#26126;&#29575;&#36798;&#21040;&#20102;59.5&#65285;&#65292;&#32780;&#26368;&#25104;&#29087;&#21644;&#27969;&#34892;&#30340;&#22522;&#20110;&#31526;&#21495;&#30340;&#27714;&#35299;&#22120;Sledgehammer&#30340;&#35777;&#26126;&#29575;&#21482;&#26377;38.3&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;Magnushammer&#19982;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#24418;&#24335;&#35777;&#26126;&#22120;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#22823;&#24133;&#25552;&#39640;&#21040;71.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Premise selection is a fundamental problem of automated theorem proving. Previous works often use intricate symbolic methods, rely on domain knowledge, and require significant engineering effort to solve this task. In this work, we show that Magnushammer, a neural transformer-based approach, can outperform traditional symbolic systems by a large margin. Tested on the PISA benchmark, Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of Sledgehammer, the most mature and popular symbolic-based solver. Furthermore, by combining Magnushammer with a neural formal prover based on a language model, we significantly improve the previous state-of-the-art proof rate from $57.0\%$ to $71.0\%$.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Koopman&#31639;&#23376;&#23545;&#20840;&#31209;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#27867;&#21270;&#30340;&#26032;&#30028;&#38480;&#65292;&#24403;&#26435;&#37325;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#36739;&#23567;&#26102;&#65292;&#35813;&#30028;&#38480;&#27604;&#29616;&#26377;&#22522;&#20110;&#33539;&#25968;&#30340;&#30028;&#38480;&#26356;&#32039;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#19981;&#19982;&#29616;&#26377;&#30028;&#38480;&#30456;&#30683;&#30462;&#65292;&#32780;&#26159;&#23545;&#29616;&#26377;&#30028;&#38480;&#36827;&#34892;&#30340;&#34917;&#20805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#21487;&#20197;&#19982;&#29616;&#26377;&#30028;&#38480;&#32467;&#21512;&#20197;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#20026;&#29702;&#35299;&#20840;&#31209;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#20063;&#20026;&#31639;&#23376;&#29702;&#35770;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#20043;&#38388;&#25552;&#20379;&#20102;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2302.05825</link><description>&lt;p&gt;
&#22522;&#20110;Koopman&#31639;&#23376;&#30340;&#20840;&#31209;&#26435;&#37325;&#30340;&#27867;&#21270;&#30028;&#38480;&#65306;&#26032;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Koopman-based generalization bound: New aspect for full-rank weights. (arXiv:2302.05825v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05825
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Koopman&#31639;&#23376;&#23545;&#20840;&#31209;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#27867;&#21270;&#30340;&#26032;&#30028;&#38480;&#65292;&#24403;&#26435;&#37325;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#36739;&#23567;&#26102;&#65292;&#35813;&#30028;&#38480;&#27604;&#29616;&#26377;&#22522;&#20110;&#33539;&#25968;&#30340;&#30028;&#38480;&#26356;&#32039;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#19981;&#19982;&#29616;&#26377;&#30028;&#38480;&#30456;&#30683;&#30462;&#65292;&#32780;&#26159;&#23545;&#29616;&#26377;&#30028;&#38480;&#36827;&#34892;&#30340;&#34917;&#20805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#21487;&#20197;&#19982;&#29616;&#26377;&#30028;&#38480;&#32467;&#21512;&#20197;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#20026;&#29702;&#35299;&#20840;&#31209;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#20063;&#20026;&#31639;&#23376;&#29702;&#35770;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#20043;&#38388;&#25552;&#20379;&#20102;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Koopman&#31639;&#23376;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27867;&#21270;&#30340;&#26032;&#30028;&#38480;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#19978;&#65292;&#32780;&#25105;&#20204;&#19987;&#27880;&#20110;&#20840;&#31209;&#26435;&#37325;&#30697;&#38453;&#12290;&#24403;&#26435;&#37325;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#36739;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#27604;&#29616;&#26377;&#22522;&#20110;&#33539;&#25968;&#30340;&#30028;&#38480;&#26356;&#32039;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#26435;&#37325;&#30697;&#38453;&#26159;&#27491;&#20132;&#30340;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#19982;&#32593;&#32476;&#30340;&#23485;&#24230;&#23436;&#20840;&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#19981;&#19982;&#29616;&#26377;&#30028;&#38480;&#30456;&#30683;&#30462;&#65292;&#32780;&#26159;&#23545;&#29616;&#26377;&#30028;&#38480;&#36827;&#34892;&#30340;&#34917;&#20805;&#12290;&#30001;&#20960;&#20010;&#24050;&#26377;&#23454;&#39564;&#35777;&#26126;&#65292;&#20302;&#31209;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#30340;&#21807;&#19968;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#21487;&#20197;&#19982;&#29616;&#26377;&#30028;&#38480;&#32467;&#21512;&#20197;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#29702;&#35299;&#20855;&#26377;&#20840;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#36824;&#20026;&#31639;&#23376;&#29702;&#35770;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#20043;&#38388;&#25552;&#20379;&#20102;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new bound for generalization of neural networks using Koopman operators. Whereas most of existing works focus on low-rank weight matrices, we focus on full-rank weight matrices. Our bound is tighter than existing norm-based bounds when the condition numbers of weight matrices are small. Especially, it is completely independent of the width of the network if the weight matrices are orthogonal. Our bound does not contradict to the existing bounds but is a complement to the existing bounds. As supported by several existing empirical results, low-rankness is not the only reason for generalization. Furthermore, our bound can be combined with the existing bounds to obtain a tighter bound. Our result sheds new light on understanding generalization of neural networks with full-rank weight matrices, and it provides a connection between operator-theoretic analysis and generalization of neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20048;&#35266;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22312;Stochastically Extended Adversarial (SEA)&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#23545;&#20110;&#20984;&#21644;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20854;&#36951;&#25022;&#30028;&#38480;&#20026;O(sqrt(&#963;_{1:T}^2) + sqrt(&#931;_{1:T}^2))&#65292;&#23545;&#20110;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20854;&#30028;&#38480;&#20026;O(sqrt(&#963;_{\max}^2) + sqrt(&#931;_{\max}^2))&#12290;</title><link>http://arxiv.org/abs/2302.04552</link><description>&lt;p&gt;
&#20048;&#35266;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#36830;&#25509;&#38543;&#26426;&#24615;&#21644;&#23545;&#25239;&#24615;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization. (arXiv:2302.04552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20048;&#35266;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22312;Stochastically Extended Adversarial (SEA)&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#23545;&#20110;&#20984;&#21644;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20854;&#36951;&#25022;&#30028;&#38480;&#20026;O(sqrt(&#963;_{1:T}^2) + sqrt(&#931;_{1:T}^2))&#65292;&#23545;&#20110;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20854;&#30028;&#38480;&#20026;O(sqrt(&#963;_{\max}^2) + sqrt(&#931;_{\max}^2))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sachs&#31561;&#20154;&#20171;&#32461;&#20102;Stochastically Extended Adversarial (SEA)&#27169;&#22411;&#65292;&#20316;&#20026;&#38543;&#26426;&#24615;&#21644;&#23545;&#25239;&#24615;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#25554;&#20540;&#26041;&#27861;&#12290;&#22312;&#20809;&#28369;&#26465;&#20214;&#19979;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#20048;&#35266;&#30340;Follow-the-Regularized-Leader (FTRL)&#31639;&#27861;&#30340;&#26399;&#26395;&#36951;&#25022;&#20381;&#36182;&#20110;&#20984;&#20989;&#25968;&#30340;&#32047;&#31215;&#38543;&#26426;&#26041;&#24046;&#21644;&#32047;&#31215;&#23545;&#25239;&#21464;&#21270;&#12290;&#23545;&#20110;&#24378;&#20984;&#20989;&#25968;&#65292;&#20182;&#20204;&#20063;&#32473;&#20986;&#20102;&#22522;&#20110;&#26368;&#22823;&#38543;&#26426;&#26041;&#24046;&#21644;&#26368;&#22823;&#23545;&#25239;&#21464;&#21270;&#30340;&#31245;&#24369;&#30028;&#38480;&#12290;&#21463;&#21040;&#20182;&#20204;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20048;&#35266;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22312;SEA&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#23545;&#20110;&#20984;&#19988;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21363;O(sqrt(&#963;_{1:T}^2) + sqrt(&#931;_{1:T}^2))&#65292;&#32780;&#19981;&#38656;&#35201;&#20010;&#21035;&#20989;&#25968;&#30340;&#20984;&#24615;&#35201;&#27714;&#12290;&#23545;&#20110;&#24378;&#20984;&#19988;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;O(sqrt(&#963;_{\max}^2) + sqrt(&#931;_{\max}^2))&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastically Extended Adversarial (SEA) model is introduced by Sachs et al. [2022] as an interpolation between stochastic and adversarial online convex optimization. Under the smoothness condition, they demonstrate that the expected regret of optimistic follow-the-regularized-leader (FTRL) depends on the cumulative stochastic variance $\sigma_{1:T}^2$ and the cumulative adversarial variation $\Sigma_{1:T}^2$ for convex functions. They also provide a slightly weaker bound based on the maximal stochastic variance $\sigma_{\max}^2$ and the maximal adversarial variation $\Sigma_{\max}^2$ for strongly convex functions. Inspired by their work, we investigate the theoretical guarantees of optimistic online mirror descent (OMD) for the SEA model. For convex and smooth functions, we obtain the same $\mathcal{O}(\sqrt{\sigma_{1:T}^2}+\sqrt{\Sigma_{1:T}^2})$ regret bound, without the convexity requirement of individual functions. For strongly convex and smooth functions, we establish an $\mathc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#38899;&#20048;&#21512;&#25104;&#21644;&#28304;&#20998;&#31163;&#65292;&#24182;&#20110;&#37096;&#20998;&#29983;&#25104;&#21644;&#20998;&#31163;&#20219;&#21153;&#19978;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#23450;&#37327;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.02257</link><description>&lt;p&gt;
&#22810;&#28304;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#38899;&#20048;&#29983;&#25104;&#21644;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Diffusion Models for Simultaneous Music Generation and Separation. (arXiv:2302.02257v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#38899;&#20048;&#21512;&#25104;&#21644;&#28304;&#20998;&#31163;&#65292;&#24182;&#20110;&#37096;&#20998;&#29983;&#25104;&#21644;&#20998;&#31163;&#20219;&#21153;&#19978;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#23450;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#25193;&#25955;&#22522;&#20110;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#19978;&#19979;&#25991;&#28304;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#30340;&#24471;&#20998;&#26469;&#36827;&#34892;&#38899;&#20048;&#21512;&#25104;&#21644;&#28304;&#20998;&#31163;&#12290;&#38500;&#20102;&#32463;&#20856;&#30340;&#24635;&#25512;&#29702;&#20219;&#21153;&#65288;&#21363;&#29983;&#25104;&#28151;&#21512;&#29289;&#20307;&#65292;&#20998;&#31163;&#28304;&#65289;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#24182;&#22312;&#28304;&#22635;&#20805;&#30340;&#37096;&#20998;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20123;&#28304;&#32473;&#20854;&#20182;&#20154;&#65288;&#20363;&#22914;&#65292;&#28436;&#22863;&#19968;&#26465;&#19982;&#40723;&#30456;&#37197;&#30340;&#38050;&#29748;&#26354;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dirac&#20284;&#28982;&#20989;&#25968;&#30340;&#20998;&#31163;&#20219;&#21153;&#30340;&#26032;&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Slakh2100&#36825;&#20010;&#26631;&#20934;&#30340;&#38899;&#20048;&#28304;&#20998;&#31163;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#35774;&#32622;&#20013;&#25552;&#20379;&#23450;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#22312;&#28304;&#20998;&#31163;&#35774;&#32622;&#20013;&#30340;&#26377;&#31454;&#20105;&#21147;&#30340;&#23450;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#29983;&#25104;&#21644;&#20998;&#31163;&#20219;&#21153;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#20363;&#23376;&#65292;&#22240;&#27492;&#20195;&#34920;&#20102;&#36890;&#29992;&#38899;&#39057;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we define a diffusion-based generative model capable of both music synthesis and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we also introduce and experiment on the partial generation task of source imputation, where we generate a subset of the sources given the others (e.g., play a piano track that goes well with the drums). Additionally, we introduce a novel inference method for the separation task based on Dirac likelihood functions. We train our model on Slakh2100, a standard dataset for musical source separation, provide qualitative results in the generation settings, and showcase competitive quantitative results in the source separation setting. Our method is the first example of a single model that can handle both generation and separation tasks, thus representing a step toward general audio models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2302.01622</link><description>&lt;p&gt;
&#31169;&#23494;&#12289;&#20844;&#24179;&#19988;&#31934;&#30830;&#65306;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#35757;&#32451;&#22823;&#35268;&#27169;&#38544;&#31169;&#20445;&#25252;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#39640;&#24230;&#25935;&#24863;&#24615;&#65292;&#38656;&#35201;&#37319;&#21462;&#29305;&#27530;&#25514;&#26045;&#30830;&#20445;&#20854;&#20445;&#25252;&#12290;&#20445;&#25252;&#38544;&#31169;&#30340;&#40644;&#37329;&#26631;&#20934;&#26159;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;DP&#23545;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#22312;&#21307;&#23398;&#20013;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#24182;&#19988;&#26159;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23545;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;N=193,311&#65289;&#30340;&#39640;&#36136;&#37327;&#20020;&#24202;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65292;&#21644;&#65288;2&#65289;&#19968;&#20010;&#25968;&#25454;&#38598;&#65288;N=1,625&#65289;&#30340;3D&#33145;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#65292;&#29992;&#20110;&#20998;&#31867;&#33008;&#33146;&#23548;&#31649;&#33146;&#30284;&#65288;PDAC&#65289;&#30340;&#23384;&#22312;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20026;&#22238;&#39038;&#24615;&#37319;&#38598;&#65292;&#24182;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#23398;&#24433;&#20687;&#19987;&#23478;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11476</link><description>&lt;p&gt;
&#20351;&#29992;Tsallis KL&#25955;&#24230;&#30340;&#24191;&#20041;Munchausen&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11476
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#37117;&#37319;&#29992;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#21040;&#19978;&#19968;&#20010;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#31574;&#30053;&#21464;&#21270;&#36807;&#24555;&#12290;&#36825;&#20010;&#24819;&#27861;&#26368;&#21021;&#26159;&#22312;Conservative Policy Iteration&#30340;&#19968;&#31687;&#37325;&#35201;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#65292;&#36817;&#20284;&#31639;&#27861;&#22914;TRPO&#21644;Munchausen Value Iteration&#65288;MVI&#65289;&#32473;&#20986;&#20102;&#26377;&#38480;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19968;&#31181;&#24191;&#20041;&#30340;KL&#25955;&#24230; - &#31216;&#20026;Tsallis KL&#25955;&#24230; - &#26469;&#32487;&#32493;&#36825;&#19968;&#24037;&#20316;&#65292;&#23427;&#22312;&#23450;&#20041;&#20013;&#20351;&#29992;&#20102;$q$-&#23545;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#25512;&#24191;&#65292;&#22240;&#20026;$q = 1$&#23545;&#24212;&#20110;&#26631;&#20934;&#30340;KL&#25955;&#24230;&#65307;$q &gt; 1$&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#23545;&#22312;Tsallis KL&#19979;&#23398;&#20064;&#30340;&#31574;&#30053;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#38416;&#36848;&#20102;&#20309;&#26102;$ q &gt; 1 $&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#23558;Tsallis KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;MVI&#65292;&#23427;&#26159;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#21253;&#21547;KL&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24191;&#20041;MVI&#65288;$q$&#65289;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q &gt; 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q &gt;1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#22810;&#37325;&#26816;&#39564;&#26041;&#27861;&#65306;&#27604;&#20363;&#21305;&#37197;&#21644;&#36138;&#23146;&#32858;&#21512;&#12290;&#36138;&#24515;&#32858;&#21512;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20248;&#25298;&#32477;&#21306;&#22495;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2211.16059</link><description>&lt;p&gt;
&#20851;&#20110;&#32593;&#32476;&#35268;&#27169;&#19979;&#30340;&#22823;&#35268;&#27169;&#22810;&#37325;&#26816;&#39564;&#65306;&#19968;&#31181;&#28176;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Large-Scale Multiple Testing Over Networks: An Asymptotic Approach. (arXiv:2211.16059v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#22810;&#37325;&#26816;&#39564;&#26041;&#27861;&#65306;&#27604;&#20363;&#21305;&#37197;&#21644;&#36138;&#23146;&#32858;&#21512;&#12290;&#36138;&#24515;&#32858;&#21512;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20248;&#25298;&#32477;&#21306;&#22495;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#22810;&#37325;&#26816;&#39564;&#32593;&#32476;&#26041;&#27861;&#65292;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#28176;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#26041;&#27861;&#65306;&#27604;&#20363;&#21305;&#37197;&#21644;&#36138;&#23146;&#32858;&#21512;&#12290;&#27604;&#20363;&#21305;&#37197;&#26041;&#27861;&#23454;&#29616;&#20102;&#20840;&#23616; BH&#65288;Benjamini&#8211;Hochberg&#65289;&#24615;&#33021;&#65292;&#20165;&#38656;&#35201;&#36890;&#20449;&#19968;&#27425;&#65288;&#20272;&#35745;&#65289;&#30495;&#38646;&#20551;&#35774;&#27604;&#20363;&#20197;&#21450;&#27599;&#20010;&#33410;&#28857;&#30340; p &#20540;&#25968;&#37327;&#12290;&#36890;&#36807;&#20851;&#27880;&#28176;&#36827;&#26368;&#20248;&#21151;&#29575;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102; BH &#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#28176;&#36817;&#26368;&#20248;&#35299;&#30340;&#26174;&#24335;&#29305;&#24449;&#12290;&#36825;&#23548;&#33268;&#20102;&#36138;&#24515;&#32858;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20248;&#25298;&#32477;&#21306;&#22495;&#65292;&#32780;&#35745;&#31639;&#25928;&#29575;&#33258;&#28982;&#26469;&#33258;&#36138;&#24515;&#31867;&#22411;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102; FDR &#21644;&#21151;&#29575;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#30340;&#24191;&#27867;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#20102;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work concerns developing communication- and computation-efficient methods for large-scale multiple testing over networks, which is of interest to many practical applications. We take an asymptotic approach and propose two methods, proportion-matching and greedy aggregation, tailored to distributed settings. The proportion-matching method achieves the global BH performance yet only requires a one-shot communication of the (estimated) proportion of true null hypotheses as well as the number of p-values at each node. By focusing on the asymptotic optimal power, we go beyond the BH procedure by providing an explicit characterization of the asymptotic optimal solution. This leads to the greedy aggregation method that effectively approximates the optimal rejection regions at each node, while computation efficiency comes from the greedy-type approach naturally. Moreover, for both methods, we provide the rate of convergence for both the FDR and power. Extensive numerical results over a va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65288;IFU&#65289;&#30340;&#26032;&#39062;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;&#26377;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;&#35831;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#29575;&#36739;&#22522;&#26412;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2211.11656</link><description>&lt;p&gt;
&#39034;&#24207;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65306;&#32852;&#37030;&#20248;&#21270;&#20013;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65288;IFU&#65289;&#30340;&#26032;&#39062;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;&#26377;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;&#35831;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#29575;&#36739;&#22522;&#26412;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#65288;MU&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;&#20174;&#35757;&#32451;&#36807;&#31243;&#20013;&#21024;&#38500;&#32473;&#23450;&#25968;&#25454;&#28857;&#30340;&#36129;&#29486;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#32852;&#37030;&#28040;&#38500;&#65288;FU&#65289;&#26159;&#23558;MU&#25193;&#23637;&#21040;&#20174;&#32852;&#21512;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#32473;&#23450;&#23458;&#25143;&#31471;&#30340;&#36129;&#29486;&#12290;&#24403;&#21069;&#30340;FU&#26041;&#27861;&#36890;&#24120;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#27809;&#26377;&#23545;&#28040;&#38500;&#25928;&#26524;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#21512;&#29702;&#30340;&#29702;&#35770;&#37327;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;(IFU)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;FU&#26041;&#27861;&#12290;&#22312;&#25509;&#25910;&#21040;&#32473;&#23450;&#23458;&#25143;&#31471;&#30340;&#28040;&#38500;&#35831;&#27714;&#21518;&#65292;IFU&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#26426;&#21046;&#30830;&#23450;&#20102;&#37325;&#26032;&#21021;&#22987;&#21270;FL&#25152;&#38656;&#30340;&#26368;&#20339;FL&#36845;&#20195;&#65292;&#21487;&#20197;&#33719;&#24471;&#28040;&#38500;&#20445;&#35777;&#12290;IFU&#30340;&#29702;&#35770;&#20063;&#21487;&#20197;&#25193;&#23637;&#20197;&#35299;&#20915;&#39034;&#24207;&#28040;&#38500;&#35831;&#27714;&#12290;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#26412;&#37325;&#26032;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#30456;&#27604;&#65292;IFU&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#28040;&#38500;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26694;&#26550;&#65292;&#20026;&#27599;&#20010;&#35266;&#27979;&#20540;&#25552;&#20379;&#31232;&#30095;&#30340;&#23616;&#37096;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#24182;&#23558;&#36825;&#20123;&#35268;&#21017;&#32858;&#21512;&#25104;&#21306;&#22495;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#20197;&#36866;&#24212;&#19981;&#31283;&#23450;&#30340;&#23454;&#29616;&#29615;&#22659;&#65292;&#24182;&#20135;&#29983;&#31283;&#20581;&#30340;&#25937;&#27982;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2209.14568</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#20316;&#20026;&#23616;&#37096;&#21644;&#21306;&#22495;&#21453;&#20107;&#23454;&#25919;&#31574;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Rethinking Counterfactual Explanations as Local and Regional Counterfactual Policies. (arXiv:2209.14568v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26694;&#26550;&#65292;&#20026;&#27599;&#20010;&#35266;&#27979;&#20540;&#25552;&#20379;&#31232;&#30095;&#30340;&#23616;&#37096;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#24182;&#23558;&#36825;&#20123;&#35268;&#21017;&#32858;&#21512;&#25104;&#21306;&#22495;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#20197;&#36866;&#24212;&#19981;&#31283;&#23450;&#30340;&#23454;&#29616;&#29615;&#22659;&#65292;&#24182;&#20135;&#29983;&#31283;&#20581;&#30340;&#25937;&#27982;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#38754;&#20020;&#30528;&#35768;&#22810;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#22914;&#30830;&#20445;&#31283;&#23450;&#24615;&#12289;&#32508;&#21512;&#22810;&#20010;CE&#20197;&#21450;&#25552;&#20379;&#21512;&#29702;&#24615;&#21644;&#31232;&#30095;&#24615;&#20445;&#35777;&#12290;&#20174;&#26356;&#23454;&#38469;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#35268;&#23450;&#30340;&#21453;&#20107;&#23454;&#25937;&#27982;&#25514;&#26045;&#36890;&#24120;&#19981;&#20250;&#34987;&#20010;&#20307;&#23436;&#20840;&#23454;&#26045;&#65292;&#24182;&#35777;&#26126;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;CE&#31639;&#27861;&#22312;&#36825;&#31181;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#24456;&#21487;&#33021;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#26694;&#26550;&#65292;&#20026;&#27599;&#20010;&#35266;&#27979;&#20540;&#25552;&#20379;&#31232;&#30095;&#30340;&#23616;&#37096;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#25552;&#20379;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#25913;&#21464;&#20915;&#31574;&#30340;&#20540;&#33539;&#22260;&#30340;&#35268;&#21017;&#12290;&#36825;&#20123;&#35268;&#21017;&#20316;&#20026;&#22810;&#26679;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24635;&#32467;&#65292;&#24182;&#20135;&#29983;&#31283;&#20581;&#30340;&#25937;&#27982;&#25514;&#26045;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#36825;&#20123;&#23616;&#37096;&#35268;&#21017;&#32858;&#21512;&#25104;&#21306;&#22495;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#35782;&#21035;&#25968;&#25454;&#23376;&#32452;&#30340;&#20849;&#20139;&#25937;&#27982;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#23616;&#37096;&#21644;&#21306;&#22495;&#35268;&#21017;&#26469;&#33258;&#20110;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CE) face several unresolved challenges, such as ensuring stability, synthesizing multiple CEs, and providing plausibility and sparsity guarantees. From a more practical point of view, recent studies [Pawelczyk et al., 2022] show that the prescribed counterfactual recourses are often not implemented exactly by individuals and demonstrate that most state-of-the-art CE algorithms are very likely to fail in this noisy environment. To address these issues, we propose a probabilistic framework that gives a sparse local counterfactual rule for each observation, providing rules that give a range of values capable of changing decisions with high probability. These rules serve as a summary of diverse counterfactual explanations and yield robust recourses. We further aggregate these local rules into a regional counterfactual rule, identifying shared recourses for subgroups of the data. Our local and regional rules are derived from the Random Forest algorithm, which of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW&#65292;&#30456;&#36739;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05208</link><description>&lt;p&gt;
&#32593;&#32476;&#27969;&#30340;&#22270;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Modeling of Network Flows. (arXiv:2209.05208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW&#65292;&#30456;&#36739;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#38382;&#39064;&#28041;&#21450;&#23558;&#27969;&#37327;&#20998;&#24067;&#22312;&#32593;&#32476;&#20013;&#65292;&#20197;&#20351;&#22522;&#30784;&#35774;&#26045;&#24471;&#21040;&#26377;&#25928;&#21033;&#29992;&#65292;&#36825;&#22312;&#20132;&#36890;&#36816;&#36755;&#21644;&#29289;&#27969;&#20013;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#20854;&#20013;&#65292;&#22810;&#21830;&#21697;&#32593;&#32476;&#27969; (MCNF) &#38382;&#39064;&#26159;&#26222;&#36941;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#22312;&#22810;&#20010;&#28304;&#21644;&#27719;&#20043;&#38388;&#20998;&#37197;&#19981;&#21516;&#22823;&#23567;&#30340;&#22810;&#20010;&#27969;&#65292;&#21516;&#26102;&#23454;&#29616;&#38142;&#36335;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;&#30001;&#20110;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#30340;&#21560;&#24341;&#21147;&#65292;&#36825;&#20123;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW (Per-Edge Weights)&#12290;&#27492;&#26041;&#27861;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#24182;&#27839;&#30528;&#27599;&#20010;&#38142;&#25509;&#20351;&#29992;&#19981;&#21516;&#21442;&#25968;&#21270;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992; $17$ &#20010;&#26381;&#21153;&#25552;&#20379;&#21830;&#25299;&#25169;&#21644; $2$ &#20010;&#36335;&#30001;&#26041;&#26696;&#36827;&#34892;&#20114;&#32852;&#32593;&#27969;&#37327;&#36335;&#30001;&#26696;&#20363;&#30740;&#31350;&#65292;&#23545;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; PEW &#30456;&#23545;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network flow problems, which involve distributing traffic over a network such that the underlying infrastructure is used effectively, are ubiquitous in transportation and logistics. Among them, the Multi-Commodity Network Flow (MCNF) problem is of general interest, as it concerns the distribution of multiple flows of different sizes between several sources and sinks, while achieving effective utilization of the links. Due to the appeal of data-driven optimization, these problems have increasingly been approached using graph learning methods. In this paper, we propose a novel graph learning architecture for network flow problems called Per-Edge Weights (PEW). This method builds on a Graph Attention Network and uses distinctly parametrized message functions along each link. We extensively evaluate the proposed solution through an Internet flow routing case study using $17$ Service Provider topologies and $2$ routing schemes. We show that PEW yields substantial gains over architectures wh
&lt;/p&gt;</description></item></channel></rss>