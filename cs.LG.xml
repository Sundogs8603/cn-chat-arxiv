<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#21644;&#25968;&#25454;&#37325;&#22797;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;D4&#31639;&#27861;&#21487;&#20197;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.12284</link><description>&lt;p&gt;
D4&#65306;&#36890;&#36807;&#25991;&#26723;&#21435;&#37325;&#19982;&#22810;&#26679;&#21270;&#25913;&#36827;LLM&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
D4: Improving LLM Pretraining via Document De-Duplication and Diversification. (arXiv:2308.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#21644;&#25968;&#25454;&#37325;&#22797;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;D4&#31639;&#27861;&#21487;&#20197;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#34987;&#29992;&#20110;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#23545;&#26469;&#33258;&#22823;&#35268;&#27169;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#38543;&#26426;&#36873;&#25321;&#30340;&#23613;&#21487;&#33021;&#22810;&#30340;&#26631;&#35760;&#36827;&#34892;&#19968;&#27425;&#23398;&#20064;&#12290;&#34429;&#28982;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#20114;&#32852;&#32593;&#23616;&#37096;&#19978;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#19981;&#26029;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25913;&#36827;&#30340;&#35268;&#27169;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#25968;&#25454;&#36873;&#25321;&#23545;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#38500;&#20102;MinHash&#31561;&#31616;&#21333;&#30340;&#21435;&#37325;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#23637;&#31034;&#20102;&#36890;&#36807;&#35880;&#24910;&#30340;&#25968;&#25454;&#36873;&#25321;(&#22312;&#21435;&#37325;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;)&#21487;&#20197;&#21152;&#24555;&#35757;&#32451;(&#25552;&#39640;&#20102;20%&#30340;&#25928;&#29575;)&#24182;&#19988;&#22312;16&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24179;&#22343;&#19979;&#28216;&#20934;&#30830;&#29575;&#19978;&#26377;&#25152;&#25552;&#21319;(&#39640;&#36798;2%)&#65292;&#22312;6.7B&#27169;&#22411;&#35268;&#27169;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26234;&#33021;&#37325;&#22797;&#25968;&#25454;&#30340;&#34920;&#29616;&#24635;&#26159;&#20248;&#20110;&#22522;&#32447;&#35757;&#32451;(&#32780;&#37325;&#22797;&#38543;&#26426;&#25968;&#25454;&#30340;&#34920;&#29616;&#27604;&#22522;&#32447;&#35757;&#32451;&#26356;&#24046;)&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32874;&#26126;&#30340;&#25968;&#25454;&#22788;&#29702;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;LLM&#30340;&#35757;&#32451;&#25928;&#26524;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#20998;&#26512;&#26354;&#32447;&#38754;&#31215;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25439;&#22833;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#24320;&#21457;&#26368;&#20248;&#30340;&#32447;&#24615;&#22238;&#24402;&#26041;&#31243;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#24120;&#37327;&#26435;&#37325;&#26356;&#26032;&#21644;&#22788;&#29702;&#37096;&#20998;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.12280</link><description>&lt;p&gt;
&#25193;&#23637;&#32447;&#24615;&#22238;&#24402;&#65306;&#19968;&#31181;&#36890;&#36807;&#26354;&#32447;&#19979;&#38754;&#31215;&#26469;&#20943;&#23567;&#25439;&#22833;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extended Linear Regression: A Kalman Filter Approach for Minimizing Loss via Area Under the Curve. (arXiv:2308.12280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#20998;&#26512;&#26354;&#32447;&#38754;&#31215;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25439;&#22833;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#24320;&#21457;&#26368;&#20248;&#30340;&#32447;&#24615;&#22238;&#24402;&#26041;&#31243;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#24120;&#37327;&#26435;&#37325;&#26356;&#26032;&#21644;&#22788;&#29702;&#37096;&#20998;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#20998;&#26512;&#26354;&#32447;&#38754;&#31215;&#26469;&#22686;&#24378;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25439;&#22833;&#12290;&#30446;&#26631;&#26159;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26469;&#24320;&#21457;&#26368;&#20248;&#30340;&#32447;&#24615;&#22238;&#24402;&#26041;&#31243;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#20010;&#36880;&#27493;&#36807;&#31243;&#65292;&#20174;&#29992;&#25143;&#23450;&#20041;&#30340;&#21442;&#25968;&#24320;&#22987;&#12290;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20351;&#29992;SGD&#36827;&#34892;&#35757;&#32451;&#65292;&#20998;&#21035;&#36319;&#36394;&#26435;&#37325;&#21644;&#25439;&#22833;&#65292;&#24182;&#26368;&#32456;&#36827;&#34892;&#21387;&#32553;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#26435;&#37325;&#21644;&#25439;&#22833;&#25968;&#32452;&#35757;&#32451;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#21512;&#24182;&#21518;&#30340;&#26435;&#37325;&#12290;&#39044;&#27979;&#32467;&#26524;&#26159;&#23558;&#36755;&#20837;&#22343;&#20540;&#19982;&#26435;&#37325;&#30456;&#20056;&#65292;&#32463;&#36807;&#25439;&#22833;&#35780;&#20272;&#21518;&#24418;&#25104;&#26435;&#37325;&#19982;&#25439;&#22833;&#30340;&#26354;&#32447;&#12290;&#20351;&#29992;&#20004;&#28857;&#20844;&#24335;&#25512;&#23548;&#20986;&#26354;&#32447;&#30340;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#31215;&#20998;&#35745;&#31639;&#26354;&#32447;&#19979;&#38754;&#31215;&#12290;&#20855;&#26377;&#26368;&#23567;&#38754;&#31215;&#30340;&#32447;&#24615;&#22238;&#24402;&#26041;&#31243;&#25104;&#20026;&#26368;&#20339;&#39044;&#27979;&#26354;&#32447;&#12290;&#20248;&#28857;&#21253;&#25324;&#36991;&#20813;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#24120;&#37327;&#26435;&#37325;&#26356;&#26032;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#37096;&#20998;&#25968;&#25454;&#38598;&#65292;&#19981;&#20687;&#20854;&#20182;&#38656;&#35201;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research enhances linear regression models by integrating a Kalman filter and analysing curve areas to minimize loss. The goal is to develop an optimal linear regression equation using stochastic gradient descent (SGD) for weight updating. Our approach involves a stepwise process, starting with user-defined parameters. The linear regression model is trained using SGD, tracking weights and loss separately and zipping them finally. A Kalman filter is then trained based on weight and loss arrays to predict the next consolidated weights. Predictions result from multiplying input averages with weights, evaluated for loss to form a weight-versus-loss curve. The curve's equation is derived using the two-point formula, and area under the curve is calculated via integration. The linear regression equation with minimum area becomes the optimal curve for prediction. Benefits include avoiding constant weight updates via gradient descent and working with partial datasets, unlike methods needin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#27969;&#24418;&#25237;&#24433;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#25968;&#25454;&#20013;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#24182;&#29983;&#25104;&#20102;&#26032;&#39062;&#30340;&#27969;&#24418;&#25968;&#25454;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12279</link><description>&lt;p&gt;
&#24352;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#27969;&#24418;&#25237;&#24433;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On-Manifold Projected Gradient Descent. (arXiv:2308.12279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#27969;&#24418;&#25237;&#24433;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#25968;&#25454;&#20013;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#24182;&#29983;&#25104;&#20102;&#26032;&#39062;&#30340;&#27969;&#24418;&#25968;&#25454;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35745;&#31639;&#12289;&#30452;&#25509;&#19988;&#25968;&#23398;&#20005;&#35880;&#30340;&#36924;&#36817;&#39640;&#32500;&#25968;&#25454;&#31867;&#27969;&#24418;&#30340;&#24494;&#20998;&#20960;&#20309;&#26041;&#27861;&#65292;&#20197;&#21450;&#20174;&#36755;&#20837;&#31354;&#38388;&#26144;&#23556;&#21040;&#36825;&#20123;&#31867;&#27969;&#24418;&#19978;&#30340;&#38750;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#12290;&#36825;&#20123;&#24037;&#20855;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#26032;&#39062;&#30340;&#27969;&#24418;&#25968;&#25454;&#26679;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#29992;&#20110;&#27969;&#24418;&#23545;&#25239;&#35757;&#32451;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;(NN)&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#31361;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#36793;&#30028;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#23545;&#25239;&#26679;&#26412;&#24050;&#34987;&#35777;&#26126;&#33021;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65307;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#26679;&#26412;&#23545;&#20110;&#35813;&#31867;&#21035;&#26080;&#25928;&#65292;&#20063;&#20250;&#38477;&#20302;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#21069;&#24050;&#32463;&#29983;&#25104;&#20102;&#30495;&#23454;&#30340;&#8220;&#27969;&#24418;&#19978;&#8221;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22320;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, along with nonlinear projections from input space onto these class manifolds. The tools are applied to the setting of neural network image classifiers, where we generate novel, on-manifold data samples, and implement a projected gradient descent algorithm for on-manifold adversarial training. The susceptibility of neural networks (NNs) to adversarial attack highlights the brittle nature of NN decision boundaries in input space. Introducing adversarial examples during training has been shown to reduce the susceptibility of NNs to adversarial attack; however, it has also been shown to reduce the accuracy of the classifier if the examples are not valid examples for that class. Realistic "on-manifold" examples have been previously generated from class manifolds in the latent of an autoencoder. Our work explores these phenomena in a ge
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAMP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#22870;&#21169;&#20989;&#25968;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#20449;&#21495;&#65292;&#20195;&#26367;&#20256;&#32479;&#30340;&#20219;&#21153;&#22870;&#21169;&#65292;&#20197;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.12270</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#22870;&#21169;&#35843;&#21046;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language Reward Modulation for Pretraining Reinforcement Learning. (arXiv:2308.12270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAMP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#22870;&#21169;&#20989;&#25968;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#20449;&#21495;&#65292;&#20195;&#26367;&#20256;&#32479;&#30340;&#20219;&#21153;&#22870;&#21169;&#65292;&#20197;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#65288;LRF&#65289;&#26469;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#36817;&#24180;&#26469;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#24403;&#20170;&#30340;LRF&#26159;&#21542;&#26368;&#36866;&#21512;&#20316;&#20026;&#20219;&#21153;&#22870;&#21169;&#30340;&#30452;&#25509;&#26367;&#20195;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LRF&#20316;&#20026;RL&#30340;&#39044;&#35757;&#32451;&#20449;&#21495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Language Reward Modulated Pretraining&#65288;LAMP&#65289;&#65292;&#23427;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#38646;&#23556;&#33021;&#21147;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#25928;&#29992;&#26469;&#20195;&#26367;&#19979;&#28216;&#20219;&#21153;&#22870;&#21169;&#12290;LAMP&#20351;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#12289;&#39044;&#35757;&#32451;&#30340;VLM&#26469;&#21487;&#25193;&#23637;&#22320;&#29983;&#25104;&#22122;&#22768;&#30340;&#25506;&#32034;&#22870;&#21169;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#25351;&#20196;&#38598;&#21644;&#19968;&#20010;&#20195;&#29702;&#22312;&#39044;&#35757;&#32451;&#29615;&#22659;&#20013;&#30340;&#22270;&#20687;&#35266;&#27979;&#20043;&#38388;&#30340;&#23545;&#27604;&#23545;&#40784;&#12290;LAMP&#19982;&#26631;&#20934;&#30340;&#25506;&#32034;&#22870;&#21169;&#19968;&#36215;&#20248;&#21270;&#36825;&#20123;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\textbf{LA}$nguage Reward $\textbf{M}$odulated $\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with rei
&lt;/p&gt;</description></item><item><title>FECoM&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#22240;&#32032;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.12264</link><description>&lt;p&gt;
FECoM: &#26397;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning. (arXiv:2308.12264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12264
&lt;/p&gt;
&lt;p&gt;
FECoM&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#22240;&#32032;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20351;&#29992;&#12289;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20854;&#33021;&#28304;&#28040;&#32791;&#36805;&#36895;&#22686;&#38271;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20419;&#36827;&#32511;&#33394;&#21457;&#23637;&#21644;&#19981;&#21516;&#31890;&#24230;&#30340;&#33021;&#28304;&#24847;&#35782;&#65292;&#20197;&#38480;&#21046;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#30899;&#25490;&#25918;&#26159;&#24403;&#21153;&#20043;&#24613;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20934;&#30830;&#27979;&#37327;&#21644;&#20248;&#21270;&#32454;&#31890;&#24230;&#65288;&#20363;&#22914;&#26041;&#27861;&#32423;&#21035;&#65289;&#33021;&#32791;&#30340;&#26631;&#20934;&#21644;&#21487;&#37325;&#22797;&#24037;&#20855;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FECoM&#65288;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#20202;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FECoM&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;FECoM&#36890;&#36807;&#20351;&#29992;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#21508;&#31181;&#22240;&#32032;&#26469;&#35299;&#20915;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;FECoM&#22312;&#26368;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#19978;&#27979;&#37327;&#32454;&#31890;&#24230;&#33021;&#32791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#22914;&#20309;&#20174;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#20197;&#21450;&#22914;&#20309;&#24230;&#37327;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#37319;&#29992;&#8220;&#19981;&#25512;&#33616;&#8221;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#21319;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12256</link><description>&lt;p&gt;
&#20174;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#21644;&#24230;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#21709;&#24212;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders. (arXiv:2308.12256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#22914;&#20309;&#20174;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#20197;&#21450;&#22914;&#20309;&#24230;&#37327;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#37319;&#29992;&#8220;&#19981;&#25512;&#33616;&#8221;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#21319;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#22312;&#24037;&#19994;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#25797;&#38271;&#23398;&#20064;&#29992;&#25143;&#30340;&#27491;&#38754;&#20852;&#36259;&#65292;&#20294;&#23545;&#20110;&#20174;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#21364;&#20184;&#20986;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#26159;&#29992;&#25143;&#25511;&#21046;&#30340;&#37325;&#35201;&#25163;&#27573;&#65292;&#24182;&#20276;&#38543;&#30528;&#23545;&#25512;&#33616;&#31995;&#32479;&#24212;&#35813;&#24555;&#36895;&#21709;&#24212;&#21644;&#20943;&#23569;&#31867;&#20284;&#25512;&#33616;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#36127;&#38754;&#21453;&#39304;&#20449;&#21495;&#22312;&#39034;&#24207;&#26816;&#32034;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#26088;&#22312;&#39044;&#27979;&#29992;&#25143;&#30340;&#27491;&#38754;&#20132;&#20114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#19981;&#25512;&#33616;&#8221;&#25439;&#22833;&#20989;&#25968;&#23558;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#22312;&#26816;&#32034;&#38454;&#27573;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20248;&#21270;&#19981;&#25512;&#33616;&#24102;&#26377;&#36127;&#38754;&#21453;&#39304;&#30340;&#39033;&#30446;&#30340;&#23545;&#25968;&#20284;&#28982;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#23454;&#26102;&#23454;&#39564;&#26469;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommenders have been widely used in industry due to their strength in modeling user preferences. While these models excel at learning a user's positive interests, less attention has been paid to learning from negative user feedback. Negative user feedback is an important lever of user control, and comes with an expectation that recommenders should respond quickly and reduce similar recommendations to the user. However, negative feedback signals are often ignored in the training objective of sequential retrieval models, which primarily aim at predicting positive user interactions. In this work, we incorporate explicit and implicit negative user feedback into the training objective of sequential recommenders in the retrieval stage using a "not-to-recommend" loss function that optimizes for the log-likelihood of not recommending items with negative feedback. We demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#36890;&#36807;&#20811;&#26381;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#34920;&#31034;&#21644;&#20998;&#24067;&#28418;&#31227;&#19979;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#20855;&#26377;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#30340;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12252</link><description>&lt;p&gt;
&#25105;&#30475;&#21040;&#30340;&#19996;&#35199;&#26377;&#22810;&#23433;&#20840;&#65311;&#22522;&#20110;&#22270;&#20687;&#25511;&#21046;&#30340;&#33258;&#27835;&#23433;&#20840;&#24615;&#39044;&#27979;&#30340;&#26657;&#20934;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy. (arXiv:2308.12252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#36890;&#36807;&#20811;&#26381;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#34920;&#31034;&#21644;&#20998;&#24067;&#28418;&#31227;&#19979;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#20855;&#26377;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#30340;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#24320;&#21457;&#33258;&#27835;&#31995;&#32479;&#30340;&#20027;&#35201;&#33539; paradigm&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#38543;&#30528;&#20854;&#24615;&#33021;&#21644;&#20415;&#21033;&#24615;&#65292;&#23433;&#20840;&#20445;&#35777;&#38754;&#20020;&#30528;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#25361;&#25112;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#32570;&#20047;&#20302;&#32500;&#21487;&#35299;&#37322;&#21160;&#24577;&#29366;&#24577;&#30340;&#27010;&#24565;&#65292;&#20256;&#32479;&#30340;&#20445;&#35777;&#26041;&#27861;&#37117;&#22260;&#32469;&#36825;&#19968;&#27010;&#24565;&#23637;&#24320;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#21487;&#37197;&#32622;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#19981;&#38656;&#35201;&#20302;&#32500;&#29366;&#24577;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#28508;&#22312;&#34920;&#31034;&#21644;&#39044;&#27979;&#24341;&#36215;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#23545;&#20854;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#25552;&#20379;&#20102;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#22312;&#20004;&#20010;&#22270;&#20687;&#25511;&#21046;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65306;&#36187;&#36710;&#21644;&#27773;&#36710;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a car
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#36991;&#20813;&#29983;&#25104;&#29256;&#26435;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#20248;&#21270;&#35270;&#20026;softmax&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#19968;&#31181;&#39640;&#25928;&#36827;&#34892;softmax&#22238;&#24402;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12247</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20445;&#25252;&#29256;&#26435;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Protect Copyright Data in Optimization of Large Language Models?. (arXiv:2308.12247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#36991;&#20813;&#29983;&#25104;&#29256;&#26435;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#20248;&#21270;&#35270;&#20026;softmax&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#19968;&#31181;&#39640;&#25928;&#36827;&#34892;softmax&#22238;&#24402;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35745;&#31639;&#26426;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#21457;&#25381;&#20102;&#21464;&#38761;&#24615;&#30340;&#20316;&#29992;&#12290;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36755;&#20986;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#25968;&#25454;&#24341;&#21457;&#20102;&#20105;&#35758;&#65292;&#36825;&#21487;&#33021;&#21457;&#29983;&#22312;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#26412;&#36523;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#12290;LLM&#26159;&#24314;&#31435;&#22312;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#65292;&#32780;Transformer&#20381;&#36182;&#19968;&#31181;&#31216;&#20026;Attention&#30340;&#25968;&#23398;&#35745;&#31639;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;softmax&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#20248;&#21270;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;softmax&#22238;&#24402;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#36827;&#34892;softmax&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#20197;&#38450;&#27490;&#22238;&#24402;&#20989;&#25968;&#29983;&#25104;&#29256;&#26435;&#25968;&#25454;&#12290;&#36825;&#20026;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#36991;&#20813;&#29983;&#25104;&#29256;&#26435;&#25968;&#25454;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.  In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12243</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#19981;&#21516;&#30340;&#20914;&#31361;&#20248;&#21270;&#20934;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#30340;&#20027;&#20219;&#21153;&#65288;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65289;&#65292;&#20063;&#21487;&#20197;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#21644;&#27425;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#25439;&#22833;&#26368;&#23567;&#21270;&#19982;&#31232;&#30095;&#24615;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#21152;&#26435;&#20934;&#21017;&#65292;&#20294;&#22312;&#20984;&#35774;&#32622;&#20013;&#25165;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#23545;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26631;&#37327;&#21270;&#25216;&#26415;&#65292;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#21407;&#22987;&#38382;&#39064;&#30340;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#23558;&#20854;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#35299;&#20915;&#31616;&#21270;&#21518;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;Adam&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#32463;&#27982;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
&lt;/p&gt;</description></item><item><title>&#21363;&#20351;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#20851;&#38190;&#23398;&#20064;&#26399;&#65292;&#36825;&#20123;&#20851;&#38190;&#23398;&#20064;&#26399;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.12221</link><description>&lt;p&gt;
&#21363;&#20351;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#20851;&#38190;&#23398;&#20064;&#26399;
&lt;/p&gt;
&lt;p&gt;
Critical Learning Periods Emerge Even in Deep Linear Networks. (arXiv:2308.12221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12221
&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#20851;&#38190;&#23398;&#20064;&#26399;&#65292;&#36825;&#20123;&#20851;&#38190;&#23398;&#20064;&#26399;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#23398;&#20064;&#26399;&#26159;&#25351;&#22312;&#21457;&#32946;&#26089;&#26399;&#65292;&#26242;&#26102;&#30340;&#24863;&#30693;&#32570;&#38519;&#20250;&#23545;&#34892;&#20026;&#21644;&#23398;&#20064;&#34920;&#31034;&#20135;&#29983;&#27704;&#20037;&#24433;&#21709;&#30340;&#26102;&#38388;&#27573;&#12290;&#23613;&#31649;&#29983;&#29289;&#32593;&#32476;&#21644;&#20154;&#24037;&#32593;&#32476;&#20043;&#38388;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#24046;&#24322;&#65292;&#20294;&#20851;&#38190;&#23398;&#20064;&#26399;&#22312;&#20004;&#20010;&#31995;&#32479;&#20013;&#37117;&#26377;&#32463;&#39564;&#35266;&#23519;&#21040;&#12290;&#36825;&#34920;&#26126;&#20851;&#38190;&#23398;&#20064;&#26399;&#21487;&#33021;&#26159;&#23398;&#20064;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#32780;&#19981;&#26159;&#29983;&#29289;&#23398;&#19978;&#30340;&#20598;&#28982;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#20851;&#38190;&#23398;&#20064;&#26399;&#20250;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#65292;&#23588;&#20854;&#26159;&#19981;&#28165;&#26970;&#22312;&#20004;&#20010;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#23398;&#20064;&#26399;&#26159;&#21542;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#26550;&#26500;&#25110;&#20248;&#21270;&#32454;&#33410;&#12290;&#20026;&#20102;&#30830;&#23450;&#20851;&#38190;&#30340;&#22522;&#26412;&#22240;&#32032;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#32593;&#32476;&#20063;&#26174;&#31034;&#20986;&#29983;&#29289;&#23398;&#21644;&#20154;&#24037;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#35768;&#22810;&#34892;&#20026;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#36827;&#34892;&#20998;&#26512;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#23398;&#20064;&#26399;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology. Yet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution. We also show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12219</link><description>&lt;p&gt;
&#25193;&#23637;&#24615;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#22810;&#31181;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#24471;&#30410;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#26041;&#38754;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#20808;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20877;&#36890;&#36807;&#25193;&#25955;&#36866;&#24212;&#23558;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#21644;&#25351;&#23548;&#35843;&#20248;&#26469;&#21457;&#25496;&#20854;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12215</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#19968;&#20010;&#38024;&#23545;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#19978;&#23398;&#26415;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#20013;270&#31687;&#24191;&#21463;&#24341;&#29992;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#33258;&#21160;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#30340;&#25991;&#29486;&#31995;&#32479;&#21270;&#65292;&#24182;&#23545;&#23376;&#38598;&#20013;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#12289;&#35774;&#35745;&#22833;&#35823;&#12289;&#21487;&#22797;&#29616;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#36825;&#23545;&#25152;&#22768;&#31216;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#19982;&#22312;&#32447;&#26381;&#21153;&#30495;&#27491;&#38754;&#20020;&#30340;&#25361;&#25112;&#26377;&#26412;&#36136;&#19978;&#30340;&#21306;&#21035;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#36890;&#24120;&#19981;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#32780;&#19988;&#35780;&#20272;&#24448;&#24448;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#24456;&#24046;&#12290;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;L2GMOM&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#37329;&#34701;&#32593;&#32476;&#21644;&#20248;&#21270;&#32593;&#32476;&#21160;&#37327;&#31574;&#30053;&#30340;&#20132;&#26131;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#26114;&#36149;&#25968;&#25454;&#24211;&#21644;&#37329;&#34701;&#19987;&#19994;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#21069;&#21521;&#20256;&#25773;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.12212</link><description>&lt;p&gt;
&#23398;&#20064;&#23398;&#20064;&#37329;&#34701;&#32593;&#32476;&#20197;&#20248;&#21270;&#21160;&#21147;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn Financial Networks for Optimising Momentum Strategies. (arXiv:2308.12212v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;L2GMOM&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#37329;&#34701;&#32593;&#32476;&#21644;&#20248;&#21270;&#32593;&#32476;&#21160;&#37327;&#31574;&#30053;&#30340;&#20132;&#26131;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#26114;&#36149;&#25968;&#25454;&#24211;&#21644;&#37329;&#34701;&#19987;&#19994;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#21069;&#21521;&#20256;&#25773;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#21160;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#39118;&#38505;&#28322;&#20215;&#65292;&#23427;&#21033;&#29992;&#37329;&#34701;&#32593;&#32476;&#20013;&#36164;&#20135;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#22238;&#25253;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26500;&#24314;&#37329;&#34701;&#32593;&#32476;&#30340;&#36807;&#31243;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#25968;&#25454;&#24211;&#21644;&#37329;&#34701;&#19987;&#19994;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23567;&#22411;&#21644;&#23398;&#26415;&#26426;&#26500;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#26041;&#27861;&#23558;&#32593;&#32476;&#26500;&#24314;&#21644;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#35270;&#20026;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#25237;&#36164;&#32452;&#21512;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2GMOM&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#21516;&#26102;&#23398;&#20064;&#37329;&#34701;&#32593;&#32476;&#21644;&#20248;&#21270;&#32593;&#32476;&#21160;&#37327;&#31574;&#30053;&#30340;&#20132;&#26131;&#20449;&#21495;&#12290;L2GMOM&#27169;&#22411;&#26159;&#19968;&#20010;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#21069;&#21521;&#20256;&#25773;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26159;&#20174;&#31639;&#27861;&#23637;&#24320;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;L2GMOM&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#25237;&#36164;&#32452;&#21512;&#32489;&#25928;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#20363;&#22914;&#36127;&#22799;&#26222;&#27604;&#29575;&#12290;&#22312;&#22238;&#27979;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 
&lt;/p&gt;</description></item><item><title>ULDP-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#29992;&#20110;&#36328;&#36793;&#30028;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#12290;&#31639;&#27861;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;&#30340;&#21152;&#26435;&#21098;&#35009;&#30452;&#25509;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#23494;&#30721;&#23398;&#26500;&#20214;&#22686;&#24378;&#20102;&#20854;&#25928;&#29992;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12210</link><description>&lt;p&gt;
ULDP-FL:&#20855;&#26377;&#36328;&#36793;&#30028;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy. (arXiv:2308.12210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12210
&lt;/p&gt;
&lt;p&gt;
ULDP-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#29992;&#20110;&#36328;&#36793;&#30028;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#12290;&#31639;&#27861;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;&#30340;&#21152;&#26435;&#21098;&#35009;&#30452;&#25509;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#23494;&#30721;&#23398;&#26500;&#20214;&#22686;&#24378;&#20102;&#20854;&#25928;&#29992;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65288;DP-FL&#65289;&#20316;&#20026;&#19968;&#31181;&#30830;&#20445;&#24418;&#24335;&#38544;&#31169;&#30340;&#21327;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;DP-FL&#26041;&#27861;&#30830;&#20445;&#22312;&#27599;&#20010;&#36793;&#30028;&#20869;&#20197;&#35760;&#24405;&#32423;&#21035;&#30340;DP&#36827;&#34892;&#36328;&#36793;&#30028;FL&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#29992;&#25143;&#30340;&#25968;&#25454;&#21487;&#33021;&#24310;&#20280;&#21040;&#22810;&#20010;&#36793;&#30028;&#65292;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#29992;&#25143;&#32423;DP&#20445;&#35777;&#20173;&#28982;&#26410;&#30693;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ULDP-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21333;&#20010;&#29992;&#25143;&#30340;&#25968;&#25454;&#21487;&#33021;&#23646;&#20110;&#22810;&#20010;&#36793;&#30028;&#30340;&#36328;&#36793;&#30028;FL&#20013;&#20445;&#35777;&#29992;&#25143;&#32423;DP&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;&#30340;&#21152;&#26435;&#21098;&#35009;&#30452;&#25509;&#30830;&#20445;&#29992;&#25143;&#32423;DP&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#32452;&#38544;&#31169;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#31639;&#27861;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#23494;&#30721;&#23398;&#26500;&#20214;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#25928;&#29992;&#24182;&#23637;&#31034;&#20102;&#20854;&#31169;&#23494;&#23454;&#29616;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm's privacy and utility. Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20309;&#35838;&#31243;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23558;&#35838;&#31243;&#19982;Adam&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#23398;&#20064;&#36866;&#24212;&#20102;&#27425;&#20248;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#20854;&#33030;&#24369;&#24615;&#22686;&#21152;</title><link>http://arxiv.org/abs/2308.12202</link><description>&lt;p&gt;
&#20351;&#29992;Adam&#36827;&#34892;&#35838;&#31243;&#23398;&#20064;&#65306;&#39764;&#39740;&#22312;&#20110;&#38169;&#35823;&#30340;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning with Adam: The Devil Is in the Wrong Details. (arXiv:2308.12202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20309;&#35838;&#31243;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23558;&#35838;&#31243;&#19982;Adam&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#23398;&#20064;&#36866;&#24212;&#20102;&#27425;&#20248;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#20854;&#33030;&#24369;&#24615;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#65288;CL&#65289;&#35748;&#20026;&#65292;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26356;&#26377;&#25928;&#22320;&#20174;&#19982;&#20854;&#24403;&#21069;&#23398;&#20064;&#36827;&#23637;&#30456;&#21305;&#37197;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;CL&#26041;&#27861;&#20173;&#28982;&#34987;&#24456;&#23569;&#20102;&#35299;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#20854;&#21462;&#24471;&#30340;&#25104;&#26524;&#20063;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20854;&#20013;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#23581;&#35797;&#22797;&#29616;&#21644;&#25193;&#23637;&#19968;&#20123;&#26368;&#36817;&#30340;&#35838;&#31243;&#26041;&#27861;&#65292;&#20294;&#21457;&#29616;&#24403;&#24212;&#29992;&#20110;NLP&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#32467;&#26524;&#20986;&#22855;&#22320;&#33030;&#24369;&#12290;&#23545;&#26576;&#20123;&#24773;&#20917;&#19979;&#35838;&#31243;&#25928;&#26524;&#30340;&#28145;&#20837;&#30740;&#31350;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#21407;&#22240;&#65306;&#24403;&#23558;&#35838;&#31243;&#19982;&#24191;&#21463;&#27426;&#36814;&#30340;Adam&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#23398;&#20064;&#36866;&#24212;&#27492;&#31639;&#27861;&#30340;&#27425;&#20248;&#21270;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#19981;&#21516;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#28041;&#21450;&#24120;&#35265;&#30340;&#25163;&#24037;&#21046;&#20316;&#21644;&#33258;&#21160;&#21270;CL&#26041;&#27861;&#65292;&#20197;&#35828;&#26126;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#32988;&#36807;&#20165;&#20351;&#29992;&#31934;&#24515;&#36873;&#25321;&#30340;Adam&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chose
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;3D&#30913;&#24615;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#38381;&#29615;&#23398;&#20064;&#30446;&#26631;&#22330;&#22320;&#25968;&#25454;&#30340;&#21453;&#28436;&#21644;&#27491;&#28436;&#27169;&#22411;&#26469;&#23454;&#29616;&#30913;&#24615;&#21453;&#28436;&#12290;&#22312;&#39044;&#35774;&#30340;&#27491;&#28436;&#27169;&#22411;&#21442;&#25968;&#30340;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#21453;&#28436;&#27169;&#22411;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#35266;&#27979;&#21644;&#37325;&#26032;&#20272;&#35745;&#30340;&#22320;&#34920;&#30913;&#24322;&#24120;&#20043;&#38388;&#30340;&#32477;&#23545;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.12193</link><description>&lt;p&gt;
3D&#30913;&#24615;&#21453;&#28436;&#30340;&#33258;&#30417;&#30563;&#30693;&#35782;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion. (arXiv:2308.12193v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12193
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;3D&#30913;&#24615;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#38381;&#29615;&#23398;&#20064;&#30446;&#26631;&#22330;&#22320;&#25968;&#25454;&#30340;&#21453;&#28436;&#21644;&#27491;&#28436;&#27169;&#22411;&#26469;&#23454;&#29616;&#30913;&#24615;&#21453;&#28436;&#12290;&#22312;&#39044;&#35774;&#30340;&#27491;&#28436;&#27169;&#22411;&#21442;&#25968;&#30340;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#21453;&#28436;&#27169;&#22411;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#35266;&#27979;&#21644;&#37325;&#26032;&#20272;&#35745;&#30340;&#22320;&#34920;&#30913;&#24322;&#24120;&#20043;&#38388;&#30340;&#32477;&#23545;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#24615;&#21453;&#28436;&#26041;&#27861;&#26159;&#19968;&#31181;&#38750;&#30772;&#22351;&#24615;&#22320;&#29699;&#29289;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22320;&#34920;&#30913;&#24322;&#24120;&#25968;&#25454;&#20272;&#35745;&#22320;&#19979;&#30913;&#21270;&#29575;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21253;&#25324;&#30913;&#24615;&#21453;&#28436;&#22312;&#20869;&#30340;&#22320;&#29699;&#29289;&#29702;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#24615;&#33021;&#21463;&#38480;&#20110;&#21512;&#25104;&#25968;&#25454;&#19982;&#23454;&#38469;&#22330;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#29420;&#31435;&#19988;&#19981;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#30913;&#24615;&#21453;&#28436;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21453;&#28436;&#21644;&#27491;&#28436;&#27169;&#22411;&#30340;&#38381;&#29615;&#23398;&#20064;&#30446;&#26631;&#22330;&#22320;&#25968;&#25454;&#65292;&#23454;&#29616;&#30913;&#24615;&#21453;&#28436;&#12290;&#22312;&#39044;&#35774;&#30340;&#27491;&#28436;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#30417;&#30563;&#30693;&#35782;&#39537;&#21160;3D&#30913;&#24615;&#21453;&#28436;&#26041;&#27861;(SSKMI)&#36890;&#36807;&#26368;&#23567;&#21270;&#35266;&#27979;&#21644;&#37325;&#26032;&#20272;&#35745;&#30340;&#22320;&#34920;&#30913;&#24322;&#24120;&#20043;&#38388;&#30340;&#32477;&#23545;&#35823;&#24046;&#26469;&#20248;&#21270;&#21453;&#28436;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22312;&#35813;&#26041;&#27861;&#20013;&#36824;&#26377;&#19968;&#20010;&#30693;&#35782;&#39537;&#21160;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The magnetic inversion method is one of the non-destructive geophysical methods, which aims to estimate the subsurface susceptibility distribution from surface magnetic anomaly data. Recently, supervised deep learning methods have been widely utilized in lots of geophysical fields including magnetic inversion. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magnetic inversion by self-supervised deep learning. The proposed self-supervised knowledge-driven 3D magnetic inversion method (SSKMI) learns on the target field data by a closed loop of the inversion and forward models. Given that the parameters of the forward model are preset, SSKMI can optimize the inversion model by minimizing the mean absolute error between observed and re-estimated surface magnetic anomalies. Besides, there is a knowledge-driven module in the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#22320;&#37327;&#21270;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#34892;&#20026;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22522;&#20110;Lagrangian&#25216;&#26415;&#30340;&#31639;&#27861;&#65292;&#26500;&#36896;&#20102;&#32039;&#33268;&#30340;ReachTube&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#20445;&#35777;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#20013;&#30340;&#21464;&#20998;&#26041;&#31243;&#12289;&#22343;&#20540;&#23450;&#29702;&#21644;Lipschitz&#24120;&#25968;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.12192</link><description>&lt;p&gt;
&#22522;&#20110;Lagrangian&#25216;&#26415;&#30340;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques. (arXiv:2308.12192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#22320;&#37327;&#21270;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#34892;&#20026;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22522;&#20110;Lagrangian&#25216;&#26415;&#30340;&#31639;&#27861;&#65292;&#26500;&#36896;&#20102;&#32039;&#33268;&#30340;ReachTube&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#20445;&#35777;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#20013;&#30340;&#21464;&#20998;&#26041;&#31243;&#12289;&#22343;&#20540;&#23450;&#29702;&#21644;Lipschitz&#24120;&#25968;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#20171;&#32461;&#20102;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;Lagrangian&#39564;&#35777;&#25216;&#26415;&#12290;&#23427;&#20204;&#24418;&#24335;&#21270;&#22320;&#37327;&#21270;&#20102;&#20219;&#20309;&#20197;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#24418;&#24335;&#21576;&#29616;&#30340;&#26102;&#38388;&#36830;&#32493;&#36807;&#31243;&#30340;&#34892;&#20026;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;LRT-NG&#65292;SLR&#21644;GoTube&#31639;&#27861;&#65292;&#29992;&#20110;&#26500;&#36896;&#32039;&#33268;ReachTube&#65292;&#21363;&#22312;&#32473;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#21040;&#36798;&#30340;&#29366;&#24577;&#38598;&#30340;&#36807;&#24230;&#20272;&#35745;&#65292;&#24182;&#20026;ReachTube&#36793;&#30028;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#19982;&#31995;&#32479;&#26041;&#31243;&#30456;&#20851;&#30340;&#21464;&#20998;&#26041;&#31243;&#12289;&#22343;&#20540;&#23450;&#29702;&#21644;Lipschitz&#24120;&#25968;&#22312;&#23454;&#29616;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#20445;&#35777;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#22312;LRT-NG&#20013;&#65292;Lipschitz&#24120;&#25968;&#34987;&#29992;&#20316;&#21021;&#22987;&#25200;&#21160;&#30340;&#33192;&#32960;&#22240;&#23376;&#65292;&#20197;&#35745;&#31639;&#26925;&#22278;&#20307;&#20013;&#30340;&#21322;&#24452;&#65292;&#35813;&#26925;&#22278;&#20307;&#20197;&#26368;&#20339;&#24230;&#37327;&#26041;&#24335;&#36807;&#24230;&#20272;&#35745;&#20102;&#21487;&#36798;&#29366;&#24577;&#38598;&#12290;&#22312;SLR&#21644;GoTube&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;Lipschitz&#24120;&#25968;&#22312;&#26679;&#26412;&#21608;&#22260;&#35745;&#31639;&#23616;&#37096;&#29699;&#26469;&#33719;&#24471;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#30340;&#32954;&#30284;&#39118;&#38505;&#20272;&#35745;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#12290;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#20272;&#35745;&#20116;&#24180;&#20869;&#21457;&#29983;&#32954;&#30284;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#38750;&#21560;&#28895;&#32773;&#21644;&#19982;&#32954;&#30284;&#26080;&#20851;&#30340;&#27515;&#22240;&#30340;&#24739;&#32773;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#20197;&#20943;&#36731;&#20559;&#24046;&#12290;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#27169;&#22411;&#26657;&#20934;&#65292;&#35813;&#24037;&#20855;&#21033;&#29992;XGBoost&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.12188</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#30340;&#32954;&#30284;&#39118;&#38505;&#20272;&#35745;&#24037;&#20855;&#30340;&#24320;&#21457;&#21644;&#22806;&#37096;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Development and external validation of a lung cancer risk estimation tool using gradient-boosting. (arXiv:2308.12188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#30340;&#32954;&#30284;&#39118;&#38505;&#20272;&#35745;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#12290;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#20272;&#35745;&#20116;&#24180;&#20869;&#21457;&#29983;&#32954;&#30284;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#38750;&#21560;&#28895;&#32773;&#21644;&#19982;&#32954;&#30284;&#26080;&#20851;&#30340;&#27515;&#22240;&#30340;&#24739;&#32773;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#20197;&#20943;&#36731;&#20559;&#24046;&#12290;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#27169;&#22411;&#26657;&#20934;&#65292;&#35813;&#24037;&#20855;&#21033;&#29992;XGBoost&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#20840;&#29699;&#33268;&#27515;&#30340;&#37325;&#35201;&#21407;&#22240;&#65292;&#24378;&#35843;&#26089;&#26399;&#21457;&#29616;&#23545;&#25552;&#39640;&#29983;&#23384;&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;(ML)&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;PLCO&#30284;&#30151;&#31579;&#26597;&#35797;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;NLST&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#20197;&#20272;&#35745;&#20116;&#24180;&#20869;&#21457;&#29983;&#32954;&#30284;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#30740;&#31350;&#21033;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;PLCO (n=55,161)&#21644;NLST (n=48,595)&#65292;&#20854;&#20013;&#21253;&#21547;&#26377;&#20851;&#32954;&#30284;&#39118;&#38505;&#22240;&#32032;&#12289;&#20020;&#24202;&#27979;&#37327;&#21644;&#32467;&#23616;&#30340;&#20840;&#38754;&#20449;&#24687;&#12290;&#25968;&#25454;&#39044;&#22788;&#29702;&#21253;&#25324;&#21024;&#38500;&#19981;&#26159;&#30446;&#21069;&#25110;&#26366;&#32463;&#21560;&#28895;&#32773;&#20197;&#21450;&#27515;&#20110;&#19982;&#32954;&#30284;&#26080;&#20851;&#21407;&#22240;&#30340;&#24739;&#32773;&#12290;&#27492;&#22806;&#65292;&#37325;&#28857;&#25918;&#22312;&#20943;&#36731;&#34987;&#25130;&#26029;&#25968;&#25454;&#24341;&#36215;&#30340;&#20559;&#24046;&#19978;&#12290;&#29305;&#24449;&#36873;&#25321;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#27169;&#22411;&#26657;&#20934;&#20351;&#29992;XGBoost&#36827;&#34892;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26799;&#24230;&#25552;&#21319;&#21644;&#20915;&#31574;&#26641;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#12290;ML&#27169;&#22411;&#22312;&#39044;&#22788;&#29702;&#30340;PLCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is a significant cause of mortality worldwide, emphasizing the importance of early detection for improved survival rates. In this study, we propose a machine learning (ML) tool trained on data from the PLCO Cancer Screening Trial and validated on the NLST to estimate the likelihood of lung cancer occurrence within five years. The study utilized two datasets, the PLCO (n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. Data preprocessing involved removing patients who were not current or former smokers and those who had died of causes unrelated to lung cancer. Additionally, a focus was placed on mitigating bias caused by censored data. Feature selection, hyper-parameter optimization, and model calibration were performed using XGBoost, an ensemble learning algorithm that combines gradient boosting and decision trees. The ML model was trained on the pre-processed PLCO dataset and t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#36793;&#32536;&#35774;&#22791;&#32593;&#32476;&#20013;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#25910;&#38598;&#25968;&#25454;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12175</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#36793;&#32536;&#35774;&#22791;&#32593;&#32476;&#20013;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomalies detection in IIoT edge devices networks using federated learning. (arXiv:2308.12175v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#36793;&#32536;&#35774;&#22791;&#32593;&#32476;&#20013;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#25910;&#38598;&#25968;&#25454;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#25509;&#35768;&#22810;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#36825;&#35201;&#27714;&#20005;&#26684;&#30340;&#38544;&#31169;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#23433;&#20840;&#38382;&#39064;&#65292;&#19968;&#20123;&#19994;&#20027;&#19981;&#24895;&#23558;&#20854;&#25968;&#25454;&#25552;&#20379;&#32473;&#20844;&#21496;&#20043;&#22806;&#30340;&#20154;&#12290;&#32852;&#21512;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25910;&#38598;&#25968;&#25454;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#19981;&#20250;&#22312;&#32593;&#32476;&#19978;&#20849;&#20139;&#29992;&#20110;&#35757;&#32451;&#12290;Fedavg&#20316;&#20026;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#20043;&#19968;&#65292;&#20801;&#35768;&#22312;&#35757;&#32451;&#20250;&#35805;&#26399;&#38388;&#23558;&#27169;&#22411;&#22797;&#21046;&#21040;&#21442;&#19982;&#35774;&#22791;&#19978;&#12290;&#35774;&#22791;&#21487;&#20197;&#38543;&#26426;&#36873;&#25321;&#65292;&#20063;&#21487;&#20197;&#20013;&#26029;&#12290;&#29983;&#25104;&#30340;&#27169;&#22411;&#23558;&#21457;&#36865;&#21040;&#21327;&#35843;&#26381;&#21153;&#22120;&#65292;&#28982;&#21518;&#35745;&#31639;&#26469;&#33258;&#23436;&#25104;&#35757;&#32451;&#30340;&#35774;&#22791;&#30340;&#24179;&#22343;&#27169;&#22411;&#12290;&#36825;&#20010;&#36807;&#31243;&#24490;&#29615;&#37325;&#22797;&#65292;&#30452;&#21040;&#36798;&#21040;&#25152;&#38656;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;/&#24037;&#19994;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitiv
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#31616;&#21270;&#30340;&#20984;&#20248;&#21270;&#27169;&#22411;&#26469;&#26368;&#23567;&#21270;&#21407;&#22987;&#21644;&#20195;&#29702;&#20248;&#21270;&#27169;&#22411;&#20043;&#38388;&#30340;&#20915;&#31574;&#39044;&#27979;&#35823;&#24046;&#65292;&#24182;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12161</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-driven decision-focused surrogate modeling. (arXiv:2308.12161v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#31616;&#21270;&#30340;&#20984;&#20248;&#21270;&#27169;&#22411;&#26469;&#26368;&#23567;&#21270;&#21407;&#22987;&#21644;&#20195;&#29702;&#20248;&#21270;&#27169;&#22411;&#20043;&#38388;&#30340;&#20915;&#31574;&#39044;&#27979;&#35823;&#24046;&#65292;&#24182;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#12289;&#20363;&#22914;&#20984;&#20248;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#20197;&#26368;&#23567;&#21270;&#20915;&#31574;&#39044;&#27979;&#35823;&#24046;&#65292;&#20915;&#31574;&#39044;&#27979;&#35823;&#24046;&#23450;&#20041;&#20026;&#21407;&#22987;&#21644;&#20195;&#29702;&#20248;&#21270;&#27169;&#22411;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23398;&#20064;&#38382;&#39064;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#36870;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#22522;&#20110;&#20998;&#35299;&#30340;&#27714;&#35299;&#31639;&#27861;&#12290;&#36890;&#36807;&#28041;&#21450;&#24120;&#35265;&#38750;&#32447;&#24615;&#21270;&#23398;&#36807;&#31243;&#65288;&#22914;&#21270;&#23398;&#21453;&#24212;&#22120;&#12289;&#25442;&#28909;&#22120;&#32593;&#32476;&#21644;&#26448;&#26009;&#28151;&#21512;&#31995;&#32479;&#65289;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#23558;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;&#19982;&#26631;&#20934;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings. The proposed data-driven framework seeks to learn a simpler, e.g. convex, surrogate optimization model that is trained to minimize the decision prediction error, which is defined as the difference between the optimal solutions of the original and the surrogate optimization models. The learning problem, formulated as a bilevel program, can be viewed as a data-driven inverse optimization problem to which we apply a decomposition-based solution algorithm from previous work. We validate our framework through numerical experiments involving the optimization of common nonlinear chemical processes such as chemical reactors, heat exchanger networks, and material blending systems. We also present a detailed comparison of decision-focused surrogate modeling with standard data-driven surrogate modeling methods and demonstrate that our appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2308.12143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIA)&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35760;&#24405;&#12290;&#23545;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;MIA&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#23558;MIA&#24212;&#29992;&#21040;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#29983;&#25104;&#27169;&#22411;&#30340;MIA&#20027;&#35201;&#20381;&#36182;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#36991;&#20813;&#65292;&#32780;&#29616;&#26377;&#30340;MIA&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#36807;&#25311;&#21512;&#19981;&#21516;&#65292;&#35760;&#24518;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20026;&#26222;&#36941;&#30340;&#29616;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#23548;&#33268;&#29983;&#25104;&#35760;&#24405;&#30340;&#27010;&#29575;&#20998;&#24067;&#21576;&#29616;&#20986;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#23427;&#26159;&#19968;&#31181;&#40657;&#30418;MIA&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#27874;&#21160;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32972;&#26223;&#24341;&#36215;&#30340;&#20559;&#24046;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#25513;&#34109;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#35813;&#20559;&#24046;&#12290;&#36890;&#36807;&#35780;&#20272;&#26631;&#20934;&#39592;&#24178;&#27169;&#22411;&#65288;CNN&#21644;ViT&#65289;&#22312;&#19981;&#21516;&#25513;&#34109;&#31574;&#30053;&#19979;&#30340;&#34892;&#20026;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#23545;&#38750;&#20998;&#24067;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12127</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#21435;&#38500;&#32972;&#26223;&#20559;&#24046;&#30340;&#25513;&#34109;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Masking Strategies for Background Bias Removal in Computer Vision Models. (arXiv:2308.12127v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32972;&#26223;&#24341;&#36215;&#30340;&#20559;&#24046;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#25513;&#34109;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#35813;&#20559;&#24046;&#12290;&#36890;&#36807;&#35780;&#20272;&#26631;&#20934;&#39592;&#24178;&#27169;&#22411;&#65288;CNN&#21644;ViT&#65289;&#22312;&#19981;&#21516;&#25513;&#34109;&#31574;&#30053;&#19979;&#30340;&#34892;&#20026;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#23545;&#38750;&#20998;&#24067;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#30001;&#20110;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#24322;&#26497;&#20026;&#24494;&#22937;&#19988;&#27599;&#31867;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#65292;&#24456;&#23481;&#26131;&#21463;&#21040;&#32972;&#26223;&#30456;&#20851;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#20351;&#29992;&#31283;&#20581;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#38750;&#20998;&#24067;&#32972;&#26223;&#30340;&#28508;&#22312;&#26679;&#26412;&#12290;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#32972;&#26223;&#24341;&#36215;&#30340;&#20559;&#24046;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#20102;&#35832;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViT&#65289;&#31561;&#26631;&#20934;&#39592;&#24178;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#25513;&#34109;&#31574;&#30053;&#26469;&#20943;&#36731;&#32972;&#26223;&#24341;&#36215;&#30340;&#20559;&#24046;&#65306;&#26089;&#26399;&#25513;&#34109;&#65292;&#21363;&#22312;&#65288;&#36755;&#20837;&#65289;&#22270;&#20687;&#32423;&#21035;&#19978;&#21435;&#38500;&#32972;&#26223;&#20449;&#24687;&#65292;&#20197;&#21450;&#26202;&#26399;&#25513;&#34109;&#65292;&#21363;&#36873;&#25321;&#24615;&#22320;&#25513;&#34109;&#19982;&#32972;&#26223;&#30456;&#23545;&#24212;&#30340;&#39640;&#23618;&#31354;&#38388;&#29305;&#24449;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#20102;CNN&#21644;ViT&#27169;&#22411;&#22312;&#19981;&#21516;&#25513;&#34109;&#31574;&#30053;&#19979;&#30340;&#34892;&#20026;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#23545;&#38750;&#20998;&#24067;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#30340;&#21152;&#36895;&#20998;&#22359;&#36817;&#31471;&#26694;&#26550;(ABPL+)&#26469;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#27604;&#36739;&#36807;&#31243;&#35299;&#20915;&#22806;&#25512;&#27493;&#39588;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25193;&#23637;&#31639;&#27861;&#36866;&#29992;&#20110;&#26356;&#26032;&#22359;&#21464;&#37327;&#30340;&#20219;&#20309;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23637;&#31034;&#24207;&#21015;&#30340;&#23548;&#25968;&#38598;&#20026;&#20851;&#38190;&#28857;&#30340;&#24615;&#36136;&#65292;&#26356;&#26126;&#26174;&#22320;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.12126</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#30340;&#21152;&#36895;&#20998;&#22359;&#36817;&#31471;&#26694;&#26550;&#29992;&#20110;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization. (arXiv:2308.12126v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#30340;&#21152;&#36895;&#20998;&#22359;&#36817;&#31471;&#26694;&#26550;(ABPL+)&#26469;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#27604;&#36739;&#36807;&#31243;&#35299;&#20915;&#22806;&#25512;&#27493;&#39588;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25193;&#23637;&#31639;&#27861;&#36866;&#29992;&#20110;&#26356;&#26032;&#22359;&#21464;&#37327;&#30340;&#20219;&#20309;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23637;&#31034;&#24207;&#21015;&#30340;&#23548;&#25968;&#38598;&#20026;&#20851;&#38190;&#28857;&#30340;&#24615;&#36136;&#65292;&#26356;&#26126;&#26174;&#22320;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#30340;&#21152;&#36895;&#20998;&#22359;&#36817;&#31471;&#32447;&#24615;&#26694;&#26550;&#65288;ABPL+&#65289;&#29992;&#20110;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#20248;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20123;&#31639;&#27861;&#20013;&#30340;&#22806;&#25512;&#27493;&#39588;&#22833;&#36133;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#27604;&#36739;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#36807;&#31243;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#20013;&#36817;&#31471;&#26799;&#24230;&#27493;&#39588;&#19982;&#32447;&#24615;&#22806;&#25512;&#27493;&#39588;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#28041;&#21450;&#20351;&#29992;&#27491;&#25972;&#25968;&#26356;&#26032;&#22359;&#21464;&#37327;&#30340;&#20219;&#20309;&#24773;&#20917;&#65292;&#20801;&#35768;&#27599;&#20010;&#21608;&#26399;&#38543;&#26426;&#37325;&#26032;&#25490;&#21015;&#21464;&#37327;&#22359;&#30340;&#26356;&#26032;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#20123;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ABPL+&#21487;&#20197;&#22312;&#20005;&#26684;&#38480;&#23450;&#22806;&#25512;&#21442;&#25968;&#21644;&#27493;&#38271;&#30340;&#24773;&#20917;&#19979;&#21333;&#35843;&#22320;&#20943;&#23567;&#20989;&#25968;&#20540;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#38543;&#26426;&#39034;&#24207;&#26356;&#26032;&#36825;&#20123;&#22359;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26356;&#26126;&#26174;&#30452;&#35266;&#22320;&#23637;&#31034;&#30001;&#25105;&#20204;&#31639;&#27861;&#29983;&#25104;&#30340;&#24207;&#21015;&#30340;&#23548;&#25968;&#38598;&#26159;&#20851;&#38190;&#28857;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an accelerated block proximal linear framework with adaptive momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the potential causes of the extrapolation step failing in some algorithms, and resolve this issue by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm. Furthermore, we extends our algorithm to any scenario involving updating block variables with positive integers, allowing each cycle to randomly shuffle the update order of the variable blocks. Additionally, under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the function value without strictly restricting the extrapolation parameters and step size, demonstrates the viability and effectiveness of updating these blocks in a random order, and we also more obviously and intuitively demonstrate that the derivative set of the sequence generated by our algorithm is a critical point 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#20840;&#26632;&#20248;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#21518;&#31471;&#21644;&#21069;&#31471;&#27169;&#25311;&#65292;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#31995;&#32479;&#25351;&#26631;&#20272;&#35745;&#21644;&#26550;&#26500;&#21442;&#25968;&#30340;&#33258;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.12120</link><description>&lt;p&gt;
&#19968;&#20010;&#24320;&#28304;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#26632;&#20248;&#21270;&#26694;&#26550;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators. (arXiv:2308.12120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#20840;&#26632;&#20248;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#21518;&#31471;&#21644;&#21069;&#31471;&#27169;&#25311;&#65292;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#31995;&#32479;&#25351;&#26631;&#20272;&#35745;&#21644;&#26550;&#26500;&#21442;&#25968;&#30340;&#33258;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#21442;&#25968;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21152;&#36895;&#22120;&#26159;&#36817;&#26399;ML&#39046;&#22495;&#30340;&#31361;&#30772;&#25104;&#26524;&#12290;&#20026;&#20102;&#23436;&#20840;&#23454;&#29616;&#20854;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65288;DSE&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#35774;&#35745;&#39537;&#21160;&#30340;&#12289;&#22522;&#20110;&#23398;&#20064;&#30340;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#38750;DNN ML&#31639;&#27861;&#30340;&#30828;&#20214;&#21152;&#36895;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#23558;&#21518;&#31471;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;&#65288;PPA&#65289;&#20998;&#26512;&#19982;&#21069;&#31471;&#24615;&#33021;&#27169;&#25311;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21518;&#31471;PPA&#21644;&#31995;&#32479;&#25351;&#26631;&#65288;&#22914;&#36816;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#65289;&#30340;&#30495;&#23454;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#21253;&#25324;&#20102;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;DSE&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#26550;&#26500;&#21644;&#21518;&#31471;&#21442;&#25968;&#30340;&#33258;&#21160;&#25628;&#32034;&#26469;&#20248;&#21270;&#21518;&#31471;&#21644;&#31995;&#32479;&#25351;&#26631;&#12290;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20197;&#24179;&#22343;7%&#25110;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#20934;&#30830;&#39044;&#27979;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#24179;&#21488;&#65288;VTA&#21644;VeriGOOD-ML&#65289;&#22312;&#21830;&#19994;12&#32435;&#31859;&#24037;&#33402;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML. To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms. It adopts a unified approach that combines backend power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy. In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters. Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#32467;&#26500;&#31232;&#30095;&#24615;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#65292;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12289;&#35745;&#31639;&#38656;&#27714;&#21644;&#39044;&#27979;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20849;&#20139;&#23618;&#20013;&#20351;&#29992;&#36890;&#36947;&#32423;l1/l2&#32452;&#31232;&#30095;&#65292;&#21516;&#26102;&#28040;&#38500;&#22810;&#20313;&#30340;&#32452;&#65288;&#36890;&#36947;&#65289;&#24182;&#23545;&#26435;&#37325;&#26045;&#21152;&#24809;&#32602;&#65292;&#25552;&#39640;&#20102;&#25152;&#26377;&#20219;&#21153;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12114</link><description>&lt;p&gt;
&#31616;&#32422;&#22810;&#20219;&#21153;&#27169;&#22411;-&#20351;&#29992;&#32467;&#26500;&#31232;&#30095;&#24615;&#23454;&#29616;&#31616;&#27905;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Less is More -- Towards parsimonious multi-task models using structured sparsity. (arXiv:2308.12114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#32467;&#26500;&#31232;&#30095;&#24615;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#65292;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12289;&#35745;&#31639;&#38656;&#27714;&#21644;&#39044;&#27979;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20849;&#20139;&#23618;&#20013;&#20351;&#29992;&#36890;&#36947;&#32423;l1/l2&#32452;&#31232;&#30095;&#65292;&#21516;&#26102;&#28040;&#38500;&#22810;&#20313;&#30340;&#32452;&#65288;&#36890;&#36947;&#65289;&#24182;&#23545;&#26435;&#37325;&#26045;&#21152;&#24809;&#32602;&#65292;&#25552;&#39640;&#20102;&#25152;&#26377;&#20219;&#21153;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32452;&#31232;&#30095;&#24615;&#40723;&#21169;&#26356;&#31616;&#21333;&#12289;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#36739;&#23569;&#30340;&#27963;&#36291;&#21442;&#25968;&#32452;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#32467;&#26500;&#21270;&#32452;&#31232;&#30095;&#24615;&#32435;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#30340;&#20849;&#20139;&#21442;&#25968;&#65292;&#24320;&#21457;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#65292;&#32780;&#20445;&#25345;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#26377;&#21161;&#20110;&#20943;&#23569;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12289;&#35745;&#31639;&#38656;&#27714;&#21644;&#39044;&#27979;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20849;&#20139;&#23618;&#20013;&#20351;&#29992;&#36890;&#36947;&#32423;l1/l2&#32452;&#31232;&#30095;&#12290;&#27492;&#26041;&#27861;&#19981;&#20165;&#26377;&#21161;&#20110;&#28040;&#38500;&#22810;&#20313;&#30340;&#32452;&#65288;&#36890;&#36947;&#65289;&#65292;&#36824;&#23545;&#26435;&#37325;&#26045;&#21152;&#24809;&#32602;&#65292;&#20174;&#32780;&#22686;&#24378;&#25152;&#26377;&#20219;&#21153;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25968;&#25454;&#38598;NYU-v2&#21644;CelebAMask-HQ&#19978;&#27604;&#36739;&#20102;&#32452;&#31232;&#30095;&#24615;&#19979;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12112</link><description>&lt;p&gt;
&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Continual Category Discovery. (arXiv:2308.12112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#25512;&#21160;&#30528;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#26399;&#26395;&#23398;&#20064;&#26032;&#30340;&#26631;&#35760;&#20219;&#21153;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#32622;&#19982;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#19981;&#22826;&#21563;&#21512;&#65292;&#20854;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#21253;&#25324;&#20840;&#26032;&#65288;&#23436;&#20840;&#26080;&#26631;&#35760;&#65289;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#30340;&#31034;&#20363;&#12290;&#21463;&#21040;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;&#22312;&#20219;&#20309;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#23384;&#22312;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#24517;&#39035;&#20351;&#29992;&#25345;&#32493;&#29256;&#26412;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#35774;&#32622;&#20026;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#12290;&#23427;&#32479;&#19968;&#20102;CL&#21644;GCD&#65292;&#24357;&#21512;&#20102;&#21512;&#25104;&#22522;&#20934;&#21644;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#20174;&#21518;&#32493;&#20219;&#21153;&#20013;&#31215;&#32047;&#30693;&#35782;&#65292;&#20854;&#20013;&#21253;&#21547;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.12108</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31995;&#25968;&#37327;&#21270;&#22855;&#24322;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Quantifying degeneracy in singular models via the learning coefficient. (arXiv:2308.12108v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26159;&#20855;&#26377;&#22797;&#26434;&#36864;&#21270;&#30340;&#22855;&#24322;&#32479;&#35745;&#27169;&#22411;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#23427;&#22312;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#20013;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36864;&#21270;&#31243;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#35777;&#26126;DNN&#20013;&#30340;&#36864;&#21270;&#19981;&#33021;&#20165;&#36890;&#36807;&#35745;&#31639;&#8220;&#24179;&#22374;&#8221;&#26041;&#21521;&#30340;&#25968;&#37327;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#36817;&#20284;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;&#29702;&#35770;&#20540;&#30340;&#20302;&#32500;&#27169;&#22411;&#19978;&#28436;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#33021;&#22815;&#27491;&#30830;&#24674;&#22797;&#24863;&#20852;&#36259;&#21442;&#25968;&#21306;&#22495;&#20043;&#38388;&#36864;&#21270;&#30340;&#39034;&#24207;&#12290;&#23545;MNIST&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#21487;&#20197;&#25581;&#31034;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#26356;&#36864;&#21270;&#25110;&#19981;&#22826;&#36864;&#21270;&#30340;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32479;&#19968;&#35270;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#31232;&#30095;&#24615;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32531;&#23384;&#21644;&#33258;&#36866;&#24212;&#25805;&#20316;&#37325;&#25490;&#23454;&#29616;&#20102;GCN&#21644;GAT&#30340;&#39640;&#36895;&#36816;&#34892;&#65292;&#26377;&#25928;&#33410;&#30465;&#20869;&#23384;&#24182;&#32531;&#35299;&#24615;&#33021;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2308.12093</link><description>&lt;p&gt;
&#32531;&#23384;&#25805;&#20316;&#37325;&#25490;&#65306;&#29992;&#20110;&#24555;&#36895;GNN&#35757;&#32451;&#30340;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Cached Operator Reordering: A Unified View for Fast GNN Training. (arXiv:2308.12093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32479;&#19968;&#35270;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#31232;&#30095;&#24615;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32531;&#23384;&#21644;&#33258;&#36866;&#24212;&#25805;&#20316;&#37325;&#25490;&#23454;&#29616;&#20102;GCN&#21644;GAT&#30340;&#39640;&#36895;&#36816;&#34892;&#65292;&#26377;&#25928;&#33410;&#30465;&#20869;&#23384;&#24182;&#32531;&#35299;&#24615;&#33021;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#22788;&#29702;&#32467;&#26500;&#21270;&#22270;&#25968;&#25454;&#21644;&#35299;&#20915;&#33410;&#28857;&#20998;&#31867;&#12289;&#22270;&#20998;&#31867;&#21644;&#32858;&#31867;&#31561;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;GNN&#35745;&#31639;&#30340;&#31232;&#30095;&#24615;&#25552;&#20986;&#20102;&#24615;&#33021;&#20248;&#21270;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;GNN&#35745;&#31639;&#12289;&#36755;&#20837;/&#36755;&#20986;&#21644;&#20869;&#23384;&#36827;&#34892;&#32479;&#19968;&#35270;&#22270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#20998;&#26512;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#22270;&#27880;&#24847;&#21147;&#65288;GAT&#65289;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GNN&#23618;&#30340;&#35745;&#31639;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26367;&#20195;&#30340;&#35745;&#31639;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25805;&#20316;&#37325;&#25490;&#21644;&#32531;&#23384;&#65292;&#20351;GCN&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;2.43&#20493;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#23545;GAT&#30340;&#19981;&#21516;&#32531;&#23384;&#26041;&#26696;&#30340;&#25506;&#32034;&#20351;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;1.94&#20493;&#12290;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26041;&#27861;&#33410;&#30465;&#20102;&#20869;&#23384;&#65292;&#26131;&#20110;&#22312;&#21508;&#31181;&#30828;&#20214;&#24179;&#21488;&#19978;&#23454;&#29616;&#65292;&#24182;&#26377;&#26395;&#32531;&#35299;&#24615;&#33021;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I/O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#25299;&#23637;&#20102;&#24050;&#26377;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#32467;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2308.12075</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#31283;&#23450;RNN&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stabilizing RNN Gradients through Pre-training. (arXiv:2308.12075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#25299;&#23637;&#20102;&#24050;&#26377;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#32467;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#20247;&#22810;&#29702;&#35770;&#37117;&#24314;&#35758;&#36890;&#36807;&#38450;&#27490;&#26799;&#24230;&#30340;&#26041;&#24046;&#20197;&#25351;&#25968;&#24418;&#24335;&#38543;&#28145;&#24230;&#25110;&#26102;&#38388;&#22686;&#38271;&#26469;&#31283;&#23450;&#21644;&#25913;&#21892;&#35757;&#32451;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20998;&#26512;&#26159;&#22312;&#21069;&#39304;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25110;&#21333;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#23398;&#30340;&#21487;&#35299;&#24615;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20307;&#31995;&#32467;&#26500;&#36807;&#20110;&#22797;&#26434;&#20197;&#33267;&#20110;&#26080;&#27861;&#36827;&#34892;&#35299;&#26512;&#21021;&#22987;&#21270;&#26102;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24050;&#30693;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#28085;&#30422;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#28145;&#23618;&#24490;&#29615;&#32593;&#32476;&#23478;&#26063;&#65292;&#23545;&#25968;&#25454;&#21644;&#21442;&#25968;&#20998;&#24067;&#30340;&#35201;&#27714;&#36739;&#23569;&#65292;&#36825;&#20010;&#29702;&#35770;&#34987;&#31216;&#20026;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65288;LSC&#65289;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#32463;&#20856;&#30340;Glorot&#12289;He&#21644;&#27491;&#20132;&#21021;&#22987;&#21270;&#26041;&#26696;&#22312;&#24212;&#29992;&#20110;&#21069;&#39304;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#28385;&#36275;LSC&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#28145;&#23618;&#24490;&#29615;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#25968;&#22686;&#38271;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#20102;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36710;&#36742;&#30340;&#21453;&#24212;&#24863;&#30693;&#39550;&#39542;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2308.12069</link><description>&lt;p&gt;
&#38752;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#35782;&#21035;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36710;&#36742;&#30340;&#21453;&#24212;&#24863;&#30693;&#39550;&#39542;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning. (arXiv:2308.12069v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#20102;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36710;&#36742;&#30340;&#21453;&#24212;&#24863;&#30693;&#39550;&#39542;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#39550;&#39542;&#39118;&#26684;&#25351;&#30340;&#26159;&#20854;&#19982;&#20854;&#20182;AV&#30340;&#34892;&#20026;&#21644;&#30456;&#20114;&#20316;&#29992;&#26041;&#24335;&#12290;&#22312;&#22810;&#36710;&#36742;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#33021;&#22815;&#35782;&#21035;&#38468;&#36817;AV&#30340;&#39550;&#39542;&#39118;&#26684;&#30340;AV&#21487;&#20197;&#21487;&#38752;&#35780;&#20272;&#30896;&#25758;&#39118;&#38505;&#24182;&#20570;&#20986;&#26356;&#21512;&#29702;&#30340;&#39550;&#39542;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#23545;&#20110;AV&#39550;&#39542;&#39118;&#26684;&#30340;&#23450;&#20041;&#24182;&#19981;&#19968;&#33268;&#65292;&#23613;&#31649;&#36890;&#24120;&#35748;&#20026;&#39550;&#39542;&#39118;&#26684;&#36890;&#36807;AV&#30340;&#36712;&#36857;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#26368;&#22823;&#29109;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;ME-IRL&#65289;&#26041;&#27861;&#20316;&#20026;&#25104;&#26412;&#20989;&#25968;&#26469;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#39550;&#39542;&#39118;&#26684;&#30340;&#19968;&#20010;&#37325;&#35201;&#25351;&#26631;&#65292;&#21363;AV&#23545;&#38468;&#36817;AV&#30340;&#21453;&#24212;&#26041;&#24335;&#65292;&#22312;&#20808;&#21069;&#30340;ME-IRL&#26041;&#27861;&#30340;&#29305;&#24449;&#35774;&#35745;&#20013;&#24182;&#26410;&#20805;&#20998;&#32771;&#34385;&#12290;&#26412;&#25991;&#23558;&#39550;&#39542;&#39118;&#26684;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#21152;&#26435;&#29305;&#24449;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#39069;&#22806;&#30340;&#26032;&#29305;&#24449;&#26469;&#25429;&#25417;AV&#30340;&#21453;&#24212;&#24863;&#30693;&#29305;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#39550;&#39542;&#39118;&#26684;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV's trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV's reaction-aware characteristics. Then, we identify the driving styles 
&lt;/p&gt;</description></item><item><title>InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;</title><link>http://arxiv.org/abs/2308.12067</link><description>&lt;p&gt;
InstructionGPT-4: &#19968;&#20010;200&#25351;&#20196;&#33539;&#24335;&#29992;&#20110;&#24494;&#35843;MiniGPT-4
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12067
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#33719;&#21462;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65306;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30417;&#30563;&#24335;&#35270;&#35273;-&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#37327;&#30340;&#39640;&#36136;&#37327;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InstructionGPT-4&#65292;&#23427;&#32463;&#36807;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;200&#20010;&#20363;&#23376;&#65292;&#32422;&#21344;MiniGPT-4&#23545;&#40784;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#30340;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#30340;6%&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20960;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#22522;&#20110;&#36825;&#20123;&#24230;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#22120;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#30340;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;InstructionGPT-4&#22312;&#21508;&#31181;&#35780;&#20272;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#12289;GPT-4&#20559;&#22909;&#65289;&#19978;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
&lt;/p&gt;</description></item><item><title>&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12066</link><description>&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#65306;&#24555;&#36895;&#19988;&#21487;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#25512;&#29702;&#30340;&#31639;&#27861;&#21644;&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference. (arXiv:2308.12066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12066
&lt;/p&gt;
&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;transformers&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20854;&#25104;&#21151;&#28304;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#23613;&#31649;&#31639;&#27861;&#24615;&#33021;&#24456;&#39640;&#65292;&#20294;LLMs&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#25104;&#27604;&#20363;&#22320;&#25193;&#22823;&#35745;&#31639;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#39640;&#23384;&#20648;&#38656;&#27714;&#21644;&#31232;&#30095;&#19987;&#23478;&#30340;&#21160;&#24577;&#28608;&#27963;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;MoE&#30340;&#20869;&#23384;&#21344;&#29992;&#39640;&#30340;&#19987;&#23478;&#21442;&#25968;&#36716;&#31227;&#21040;CPU&#20869;&#23384;&#19978;&#65292;&#20294;&#26159;&#20174;CPU&#36801;&#31227;&#24050;&#28608;&#27963;&#30340;&#19987;&#23478;&#21040;GPU&#30340;&#24310;&#36831;&#23548;&#33268;&#20102;&#39640;&#24615;&#33021;&#24320;&#38144;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SPROUT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SPROUT&#21487;&#20197;&#35782;&#21035;&#20986;&#32477;&#22823;&#37096;&#20998;&#30340;&#38169;&#20998;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.12065</link><description>&lt;p&gt;
&#23558;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#38598;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#40657;&#30418;&#20998;&#31867;&#22120;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers. (arXiv:2308.12065v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SPROUT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SPROUT&#21487;&#20197;&#35782;&#21035;&#20986;&#32477;&#22823;&#37096;&#20998;&#30340;&#38169;&#20998;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20998;&#31867;&#22120;&#21487;&#33021;&#20250;&#39044;&#27979;&#38169;&#35823;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#20986;&#29616;&#38169;&#20998;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#38169;&#20998;&#21487;&#33021;&#23545;&#31995;&#32479;&#20135;&#29983;&#36830;&#38145;&#25928;&#24212;&#65292;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#25925;&#38556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SPROUT&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26469;&#24576;&#30097;&#38169;&#20998;&#30340;&#23433;&#20840;&#24615;&#23553;&#35013;&#22120;&#12290;&#22914;&#26524;&#26816;&#27979;&#21040;&#38169;&#20998;&#65292;SPROUT&#20250;&#38459;&#27490;&#23558;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20256;&#25773;&#21040;&#31995;&#32479;&#20013;&#12290;&#23545;&#23433;&#20840;&#24615;&#30340;&#24433;&#21709;&#26159;&#65292;SPROUT&#23558;&#19981;&#31283;&#23450;&#30340;&#36755;&#20986;&#65288;&#38169;&#20998;&#65289;&#36716;&#21270;&#20026;&#25968;&#25454;&#36951;&#28431;&#25925;&#38556;&#65292;&#22312;&#31995;&#32479;&#23618;&#38754;&#19978;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36827;&#34892;&#31649;&#29702;&#12290;SPROUT&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36866;&#29992;&#20110;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#38382;&#39064;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SPROUT&#24635;&#26159;&#33021;&#22815;&#35782;&#21035;&#20986;&#22823;&#37096;&#20998;&#36229;&#32423;&#38169;&#20998;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of super
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#20840;&#29699;&#23567;&#20892;&#25143;&#31995;&#32479;&#20013;&#24120;&#35265;&#30340;&#25910;&#33719;&#22534;&#26469;&#26144;&#23556;&#20892;&#30000;&#30340;&#23384;&#22312;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;HarvestNet&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#19987;&#23478;&#30693;&#35782;&#21644;&#21355;&#26143;&#22270;&#20687;&#25910;&#38598;&#32780;&#26469;&#65292;&#21487;&#29992;&#20110;&#22312;&#22467;&#22622;&#20420;&#27604;&#20122;&#30340;&#29305;&#23450;&#22320;&#21306;&#36827;&#34892;&#20892;&#30000;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20316;&#32773;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#19978;&#20855;&#26377;&#32422;80%&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12061</link><description>&lt;p&gt;
HarvestNet&#65306;&#21033;&#29992;&#25910;&#33719;&#22534;&#21644;&#36965;&#24863;&#25216;&#26415;&#26816;&#27979;&#23567;&#20892;&#25143;&#20892;&#19994;&#27963;&#21160;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing. (arXiv:2308.12061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12061
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#20840;&#29699;&#23567;&#20892;&#25143;&#31995;&#32479;&#20013;&#24120;&#35265;&#30340;&#25910;&#33719;&#22534;&#26469;&#26144;&#23556;&#20892;&#30000;&#30340;&#23384;&#22312;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;HarvestNet&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#19987;&#23478;&#30693;&#35782;&#21644;&#21355;&#26143;&#22270;&#20687;&#25910;&#38598;&#32780;&#26469;&#65292;&#21487;&#29992;&#20110;&#22312;&#22467;&#22622;&#20420;&#27604;&#20122;&#30340;&#29305;&#23450;&#22320;&#21306;&#36827;&#34892;&#20892;&#30000;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20316;&#32773;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#19978;&#20855;&#26377;&#32422;80%&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#20892;&#22330;&#22312;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#29983;&#20135;&#22303;&#22320;&#20013;&#25152;&#21344;&#27604;&#20363;&#24456;&#22823;&#12290;&#22312;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#31561;&#22320;&#21306;&#65292;80%&#30340;&#20892;&#22330;&#37117;&#24456;&#23567;&#65288;&#38754;&#31215;&#23567;&#20110;2&#20844;&#39031;&#65289;&#65292;&#26144;&#23556;&#23567;&#20892;&#25143;&#30340;&#20892;&#30000;&#26159;&#36861;&#36394;&#20316;&#29289;&#29983;&#20135;&#21147;&#31561;&#21487;&#25345;&#32493;&#21457;&#23637;&#25514;&#26045;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23567;&#20892;&#22330;&#30340;&#22806;&#35266;&#22810;&#26679;&#19988;&#24494;&#22937;&#65292;&#20256;&#32479;&#30340;&#20892;&#30000;&#26144;&#23556;&#26041;&#27861;&#30340;&#25928;&#26524;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#27979;&#36941;&#24067;&#20840;&#29699;&#35768;&#22810;&#23567;&#20892;&#25143;&#31995;&#32479;&#30340;&#25910;&#33719;&#22534;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;HarvestNet&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#22467;&#22622;&#20420;&#27604;&#20122;&#30340;&#25552;&#26684;&#38647;&#21644;&#38463;&#22982;&#21704;&#25289;&#22320;&#21306;&#30340;2020-2023&#24180;&#38388;&#26144;&#23556;&#20892;&#30000;&#30340;&#23384;&#22312;&#65292;&#35813;&#25968;&#25454;&#38598;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#21644;&#21355;&#26143;&#22270;&#20687;&#25910;&#38598;&#65292;&#20849;&#26377;7k&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#22270;&#20687;&#21644;2k&#20010;&#22320;&#38754;&#25910;&#38598;&#26631;&#31614;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#22312;&#36965;&#24863;&#39046;&#22495;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#19978;&#20855;&#26377;&#32422;80%&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#25913;&#21464;&#25552;&#31034;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#32454;&#31890;&#24230;&#21644;&#30446;&#26631;&#21270;&#30340;&#25511;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#29983;&#25104;&#27169;&#22411;&#35270;&#20026;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#21644;&#25552;&#31034;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#20256;&#36882;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12059</link><description>&lt;p&gt;
&#25805;&#20316;&#31283;&#23450;&#25193;&#25955;&#25552;&#31034;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Manipulating Embeddings of Stable Diffusion Prompts. (arXiv:2308.12059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#25913;&#21464;&#25552;&#31034;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#32454;&#31890;&#24230;&#21644;&#30446;&#26631;&#21270;&#30340;&#25511;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#29983;&#25104;&#27169;&#22411;&#35270;&#20026;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#21644;&#25552;&#31034;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#20256;&#36882;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#25913;&#21464;&#25552;&#31034;&#20173;&#28982;&#26159;&#29992;&#25143;&#24819;&#35201;&#25913;&#21464;&#29983;&#25104;&#22270;&#20687;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24605;&#25552;&#31034;&#26469;&#25913;&#21464;&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#35797;&#38169;&#36807;&#31243;&#65292;&#36825;&#23548;&#33268;&#20102;&#25552;&#31034;&#24037;&#31243;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#30452;&#25509;&#25913;&#21464;&#25552;&#31034;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#25552;&#31034;&#25991;&#26412;&#12290;&#23427;&#20801;&#35768;&#26356;&#31934;&#32454;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25511;&#21046;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#35270;&#20026;&#19968;&#20010;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#21644;&#25552;&#31034;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#20256;&#36882;&#26799;&#24230;&#12290;&#36890;&#36807;&#35299;&#20915;&#19981;&#21516;&#30340;&#29992;&#25143;&#20132;&#20114;&#38382;&#39064;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19977;&#20010;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20010;&#24819;&#27861;&#65306;&#65288;1&#65289;&#20248;&#21270;&#22270;&#20687;&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#24230;&#37327;&#65292;&#21487;&#20197;&#27979;&#37327;&#22270;&#20687;&#39118;&#26684;&#31561;&#12290;&#65288;2&#65289;&#24110;&#21161;&#29992;&#25143;&#36827;&#34892;&#21019;&#36896;&#24615;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#23398;&#20064;&#22312;&#38754;&#23545;&#35268;&#36991;&#25915;&#20987;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#20551;&#35774;&#22312;&#21482;&#26377;&#38543;&#26426;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28385;&#36275;Lipschitz&#26465;&#20214;&#30340;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#40065;&#26834;&#23398;&#20064;&#21333;&#35843;&#24182;&#32852;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#33267;&#23569;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;&#22914;&#26524;&#25915;&#20987;&#32773;&#21463;&#38480;&#20110;&#25200;&#21160;$O(\log n)$&#20301;&#65292;&#21017;&#26679;&#26412;&#22797;&#26434;&#24230;&#20250;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2308.12054</link><description>&lt;p&gt;
&#40065;&#26834;&#23398;&#20064;&#25269;&#24481;&#35268;&#36991;&#25915;&#20987;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Robust Learning against Evasion Attacks. (arXiv:2308.12054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#23398;&#20064;&#22312;&#38754;&#23545;&#35268;&#36991;&#25915;&#20987;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#20551;&#35774;&#22312;&#21482;&#26377;&#38543;&#26426;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28385;&#36275;Lipschitz&#26465;&#20214;&#30340;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#40065;&#26834;&#23398;&#20064;&#21333;&#35843;&#24182;&#32852;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#33267;&#23569;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;&#22914;&#26524;&#25915;&#20987;&#32773;&#21463;&#38480;&#20110;&#25200;&#21160;$O(\log n)$&#20301;&#65292;&#21017;&#26679;&#26412;&#22797;&#26434;&#24230;&#20250;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22312;&#35268;&#36991;&#25915;&#20987;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#22810;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#26469;&#37327;&#21270;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#20934;&#30830;&#22312;&#29699;&#20869;&#30340;&#40065;&#26834;&#24615;&#20026;&#22522;&#30784;&#65292;&#20174;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23545;&#25239;&#25915;&#20987;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#23398;&#20064;&#21487;&#34892;&#24615;&#65292;&#32771;&#34385;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20165;&#26377;&#38543;&#26426;&#31034;&#20363;&#30340;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#20998;&#24067;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20855;&#26377;&#28385;&#36275;Lipschitz&#26465;&#20214;&#30340;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#40065;&#26834;&#23398;&#20064;&#21333;&#35843;&#24182;&#32852;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#23545;&#24212;&#20110;&#25915;&#20987;&#32773;&#30340;&#39044;&#31639;&#65288;&#27599;&#20010;&#36755;&#20837;&#21487;&#20197;&#25200;&#21160;&#30340;&#26368;&#22823;&#20301;&#25968;&#65289;&#30340;&#22686;&#38271;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#25915;&#20987;&#32773;&#34987;&#38480;&#21046;&#22312;&#25200;&#21160;$O(\log n)$&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. One of the fundamental problems in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks, where data is corrupted at test time. In this thesis, we work with the exact-in-the-ball notion of robustness and study the feasibility of adversarially robust learning from the perspective of learning theory, considering sample complexity.  We first explore the setting where the learner has access to random examples only, and show that distributional assumptions are essential. We then focus on learning problems with distributions on the input data that satisfy a Lipschitz condition and show that robustly learning monotone conjunctions has sample complexity at least exponential in the adversary's budget (the maximum number of bits it can perturb on each input). However, if the adversary is restricted to perturbing $O(\log
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;&#65288;LFP&#65289;&#8221;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#32454;&#21270;&#19982;&#23618;&#32423;&#30456;&#20851;&#24615;&#20256;&#25773;&#65288;LRP&#65289;&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#27599;&#20010;&#36830;&#25509;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#20998;&#37197;&#22870;&#21169;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;LFP&#21462;&#24471;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12053</link><description>&lt;p&gt;
&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Layer-wise Feedback Propagation. (arXiv:2308.12053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;&#65288;LFP&#65289;&#8221;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#32454;&#21270;&#19982;&#23618;&#32423;&#30456;&#20851;&#24615;&#20256;&#25773;&#65288;LRP&#65289;&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#27599;&#20010;&#36830;&#25509;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#20998;&#37197;&#22870;&#21169;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;LFP&#21462;&#24471;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;&#65288;LFP&#65289;&#8221;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#23618;&#32423;&#30456;&#20851;&#24615;&#20256;&#25773;&#65288;LRP&#65289;&#65292;&#26681;&#25454;&#27599;&#20010;&#36830;&#25509;&#23545;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#36129;&#29486;&#29420;&#31435;&#20998;&#37197;&#22870;&#21169;&#12290;&#36825;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26159;&#26397;&#21521;&#20272;&#35745;&#30340;&#25439;&#22833;&#26368;&#23567;&#20540;&#26356;&#26032;&#21442;&#25968;&#12290;LFP&#22312;&#27169;&#22411;&#20013;&#20256;&#25773;&#22870;&#21169;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#26799;&#24230;&#35745;&#31639;&#12290;&#23427;&#22686;&#24378;&#25509;&#25910;&#21040;&#27491;&#21453;&#39304;&#30340;&#32467;&#26500;&#65292;&#21516;&#26102;&#38477;&#20302;&#25509;&#25910;&#21040;&#36127;&#21453;&#39304;&#30340;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#35777;&#26126;&#20102;LFP&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LFP&#20811;&#26381;&#20102;&#26799;&#24230;&#26041;&#27861;&#30340;&#26576;&#20123;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#23545;&#26377;&#24847;&#20041;&#30340;&#23548;&#25968;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;LFP&#22914;&#20309;&#35299;&#20915;&#26799;&#24230;&#26041;&#27861;&#30456;&#20851;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>IncreLoRA&#26159;&#19968;&#31181;&#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2308.12043</link><description>&lt;p&gt;
IncreLoRA: &#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning. (arXiv:2308.12043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12043
&lt;/p&gt;
&lt;p&gt;
IncreLoRA&#26159;&#19968;&#31181;&#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#24182;&#19981;&#39640;&#25928;&#65292;&#29305;&#21035;&#26159;&#24403;&#23384;&#22312;&#22823;&#37327;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#35757;&#32451;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#24050;&#26377;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#65288;PEFT&#65289;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#20854;&#20013;&#65292;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#31209;&#20998;&#35299;&#30697;&#38453;&#27880;&#20837;&#27599;&#20010;&#30446;&#26631;&#27169;&#22359;&#30340;&#20856;&#22411;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LoRA&#24573;&#35270;&#20102;&#19981;&#21516;&#27169;&#22359;&#20013;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21098;&#26525;LoRA&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26465;&#20214;&#19979;&#65292;&#20462;&#21098;&#21442;&#25968;&#30697;&#38453;&#30340;&#31209;&#30340;&#19978;&#30028;&#20173;&#28982;&#21463;&#21040;&#39044;&#35774;&#20540;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IncreLoRA&#65292;&#19968;&#31181;&#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#27599;&#20010;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20462;&#21098;&#26041;&#27861;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#20174;&#22686;&#21152;&#21442;&#25968;&#30340;&#35282;&#24230;&#26469;&#20248;&#21270;&#35843;&#20248;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it i
&lt;/p&gt;</description></item><item><title>CACTUS&#26159;&#19968;&#20010;&#29992;&#20110;&#21457;&#29616;&#32467;&#26500;&#30340;&#20840;&#38754;&#30340;&#25277;&#35937;&#21644;&#20998;&#31867;&#24037;&#20855;&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#23433;&#20840;&#20998;&#26512;&#65292;&#25552;&#20379;&#23545;&#20998;&#31867;&#23646;&#24615;&#30340;&#39069;&#22806;&#25903;&#25345;&#65292;&#24182;&#36890;&#36807;&#24182;&#34892;&#21270;&#20248;&#21270;&#20869;&#23384;&#20351;&#29992;&#21644;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12031</link><description>&lt;p&gt;
CACTUS: &#19968;&#20010;&#20840;&#38754;&#30340;&#25277;&#35937;&#21644;&#20998;&#31867;&#24037;&#20855;&#65292;&#29992;&#20110;&#21457;&#29616;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures. (arXiv:2308.12031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12031
&lt;/p&gt;
&lt;p&gt;
CACTUS&#26159;&#19968;&#20010;&#29992;&#20110;&#21457;&#29616;&#32467;&#26500;&#30340;&#20840;&#38754;&#30340;&#25277;&#35937;&#21644;&#20998;&#31867;&#24037;&#20855;&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#23433;&#20840;&#20998;&#26512;&#65292;&#25552;&#20379;&#23545;&#20998;&#31867;&#23646;&#24615;&#30340;&#39069;&#22806;&#25903;&#25345;&#65292;&#24182;&#36890;&#36807;&#24182;&#34892;&#21270;&#20248;&#21270;&#20869;&#23384;&#20351;&#29992;&#21644;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#25512;&#21160;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#37096;&#32626;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#23545;&#20110;&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#24320;&#21457;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#25361;&#25112;&#12290;CACTUS&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#25277;&#35937;&#21644;&#20998;&#31867;&#24037;&#20855;&#65292;&#29992;&#20110;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26377;&#25928;&#22320;&#25903;&#25345;&#23433;&#20840;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#23545;&#20998;&#31867;&#23646;&#24615;&#30340;&#39069;&#22806;&#25903;&#25345;&#65292;&#20445;&#25345;&#20854;&#21407;&#22987;&#21547;&#20041;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#20248;&#21270;&#20869;&#23384;&#20351;&#29992;&#21644;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;&#23427;&#21521;&#29992;&#25143;&#26174;&#31034;&#27599;&#20010;&#31867;&#21035;&#30340;&#23646;&#24615;&#39057;&#29575;&#24182;&#26681;&#25454;&#20854;&#21306;&#20998;&#33021;&#21147;&#23545;&#20854;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#23041;&#26031;&#24247;&#26143;&#35786;&#26029;&#20083;&#33146;&#30284;&#21644;&#30002;&#29366;&#33146;0387&#25968;&#25454;&#38598;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12030</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#22240;&#20854;&#24778;&#20154;&#30340;&#25913;&#36827;&#21644;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#25104;&#20026;LLM&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#35805;&#39064;&#65292;&#23427;&#36824;&#20351;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#22312;&#26356;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#29983;&#25104;&#25152;&#38656;&#38271;&#24230;&#30340;&#21512;&#36866;&#31572;&#26696;&#25110;&#25991;&#31456;&#12290;&#27492;&#22806;&#65292;LLM&#20013;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#38750;&#24120;&#32791;&#26102;&#65292;&#32780;&#25511;&#21046;&#29983;&#25104;&#38271;&#24230;&#30340;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#38271;&#24230;&#20219;&#24847;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#20174;&#32780;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#26469;&#23454;&#29616;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#31867;&#20284;GPT&#30340;LLM&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22870;&#21169;&#27169;&#22411;&#25552;&#20379;&#22870;&#21169;&#20449;&#21495;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#23545;&#39044;&#23450;&#20041;&#30446;&#26631;&#38271;&#24230;&#36827;&#34892;&#22870;&#21169;&#26469;&#24433;&#21709;LLM&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;SI-MTL&#65289;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#23545;&#25968;&#21464;&#25442;&#21644;&#23545;&#20219;&#21153;&#26799;&#24230;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12029</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#23610;&#24230;&#19981;&#21464;&#20219;&#21153;&#24179;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Scale-Invariant Task Balancing Approach for Multi-Task Learning. (arXiv:2308.12029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12029
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;SI-MTL&#65289;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#23545;&#25968;&#21464;&#25442;&#21644;&#23545;&#20219;&#21153;&#26799;&#24230;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#24179;&#34913;&#20173;&#28982;&#26159;MTL&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25439;&#22833;/&#26799;&#24230;&#23610;&#24230;&#30340;&#19981;&#24179;&#34913;&#32463;&#24120;&#23548;&#33268;&#24615;&#33021;&#25240;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;SI-MTL&#65289;&#26041;&#27861;&#65292;&#20174;&#25439;&#22833;&#21644;&#26799;&#24230;&#35282;&#24230;&#32531;&#35299;&#20102;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SI-MTL&#21253;&#21547;&#23545;&#25152;&#26377;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#30340;&#23545;&#25968;&#21464;&#25442;&#65292;&#20197;&#30830;&#20445;&#22312;&#25439;&#22833;&#27700;&#24179;&#19978;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#26799;&#24230;&#24179;&#34913;&#26041;&#27861;SI-G&#65292;&#23427;&#23558;&#25152;&#26377;&#20219;&#21153;&#30340;&#26799;&#24230;&#24402;&#19968;&#21270;&#20026;&#19982;&#26368;&#22823;&#26799;&#24230;&#33539;&#25968;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#19968;&#33268;&#35777;&#26126;&#20102;SI-G&#30340;&#26377;&#25928;&#24615;&#21644;SI-MTL&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#20559;&#24046;&#24863;&#30693;&#30340;&#26368;&#23567;&#21270;&#65288;BAM&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#38477;&#20302;&#31169;&#26377;&#26799;&#24230;&#20272;&#35745;&#20559;&#24046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;BAM&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#20559;&#24046;&#65292;&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25913;&#21892;&#38544;&#31169;-&#25928;&#29992;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.12018</link><description>&lt;p&gt;
&#26377;&#20559;&#24046;&#24863;&#30693;&#30340;&#26368;&#23567;&#21270;&#65306;&#29702;&#35299;&#21644;&#20943;&#36731;&#31169;&#26377;SGD&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD. (arXiv:2308.12018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#20559;&#24046;&#24863;&#30693;&#30340;&#26368;&#23567;&#21270;&#65288;BAM&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#38477;&#20302;&#31169;&#26377;&#26799;&#24230;&#20272;&#35745;&#20559;&#24046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;BAM&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#20559;&#24046;&#65292;&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25913;&#21892;&#38544;&#31169;-&#25928;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;SGD&#65288;DP-SGD&#65289;&#25215;&#35834;&#33021;&#22815;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#22320;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#25935;&#24863;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;DP-SGD&#20165;&#25552;&#20379;&#26377;&#20559;&#24046;&#12289;&#22122;&#22768;&#36739;&#22823;&#30340;&#23567;&#25209;&#37327;&#26799;&#24230;&#20272;&#35745;&#12290;&#36825;&#20351;&#24471;&#20248;&#21270;&#27493;&#39588;&#21464;&#24471;&#19981;&#22826;&#26377;&#25928;&#65292;&#24182;&#22240;&#27492;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#27599;&#20010;&#26679;&#26412;&#26799;&#24230;&#33539;&#25968;&#19982;DP-SGD&#20013;&#20351;&#29992;&#30340;&#31169;&#26377;&#26799;&#24230;&#39044;&#27979;&#30340;&#20272;&#35745;&#20559;&#24046;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#26377;&#20559;&#24046;&#24863;&#30693;&#30340;&#26368;&#23567;&#21270;&#65288;BAM&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#38477;&#20302;&#31169;&#26377;&#26799;&#24230;&#20272;&#35745;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39640;&#25928;&#35745;&#31639;BAM&#25152;&#38656;&#30340;&#37327;&#65292;&#20197;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#31361;&#20986;&#20102;&#19982;&#30456;&#20851;&#26041;&#27861;&#65288;&#22914;Sharpness-Aware Minimisation&#65289;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;BAM&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#20559;&#24046;&#65292;&#36824;&#21487;&#20197;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-32&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#38544;&#31169;-&#25928;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private SGD (DP-SGD) holds the promise of enabling the safe and responsible application of machine learning to sensitive datasets. However, DP-SGD only provides a biased, noisy estimate of a mini-batch gradient. This renders optimisation steps less effective and limits model utility as a result. With this work, we show a connection between per-sample gradient norms and the estimation bias of the private gradient oracle used in DP-SGD. Here, we propose Bias-Aware Minimisation (BAM) that allows for the provable reduction of private gradient estimator bias. We show how to efficiently compute quantities needed for BAM to scale to large neural networks and highlight similarities to closely related methods such as Sharpness-Aware Minimisation. Finally, we provide empirical evidence that BAM not only reduces bias but also substantially improves privacy-utility trade-offs on the CIFAR-10, CIFAR-100, and ImageNet-32 datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26680;&#23398;&#20064;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;(MKL-$L_{0/1}$-SVM)&#65292;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#22788;&#29702;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#39046;&#20808;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12016</link><description>&lt;p&gt;
MKL-$L_{0/1}$-SVM: &#19968;&#31181;&#22810;&#26680;&#23398;&#20064;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MKL-$L_{0/1}$-SVM. (arXiv:2308.12016v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26680;&#23398;&#20064;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;(MKL-$L_{0/1}$-SVM)&#65292;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#22788;&#29702;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#39046;&#20808;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;$(0, 1)$&#25439;&#22833;&#20989;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#22810;&#26680;&#23398;&#20064;&#65288;MKL&#65289;&#26694;&#26550;&#12290;&#39318;&#20808;&#32473;&#20986;&#20102;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#28982;&#21518;&#21033;&#29992;&#23427;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#26469;&#22788;&#29702;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35814;&#32454;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MKL-$L_{0/1}$-SVM&#30340;&#24615;&#33021;&#19982;&#19968;&#31181;&#21517;&#20026;SimpleMKL&#30340;&#39046;&#20808;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12013</link><description>&lt;p&gt;
&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Quantum-Noise-driven Generative Diffusion Models. (arXiv:2308.12013v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#26029;&#20986;&#22797;&#26434;&#21644;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#24182;&#20135;&#29983;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26694;&#26550;&#65292;&#26368;&#36817;&#22312;&#21019;&#24314;&#21512;&#25104;&#25991;&#26412;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#24050;&#32463;&#36229;&#36234;&#20102;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#23376;&#25512;generalization&#65292;&#21363;&#19977;&#31181;&#21487;&#33021;&#22312;&#23454;&#38469;&#37327;&#23376;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#30340;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#29420;&#29305;&#30340;&#37327;&#23376;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#30446;&#21069;&#21487;&#29992;&#30340;&#26377;&#22122;&#22768;&#37327;&#23376;&#22788;&#29702;&#22120;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#30340;&#30456;&#24178;&#24615;&#12289;&#32416;&#32544;&#24615;&#21644;&#22122;&#22768;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#19981;&#20316;&#20026;&#38656;&#35201;&#26816;&#27979;&#21644;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#32780;&#26159;&#20316;&#20026;&#19968;&#31181;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a ver
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;HystRNN&#30340;&#31070;&#32463;&#25391;&#33633;&#22120;&#65292;&#36890;&#36807;&#26356;&#26032;&#38544;&#34255;&#29366;&#24577;&#26469;&#24314;&#27169;&#21644;&#37327;&#21270;&#30913;&#28382;&#29616;&#35937;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#24191;&#20041;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#26174;&#20102;&#31070;&#32463;&#25391;&#33633;&#22120;&#22312;&#25429;&#25417;&#30913;&#24615;&#26448;&#26009;&#30340;&#22797;&#26434;&#30913;&#28382;&#27169;&#24335;&#26041;&#38754;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.12002</link><description>&lt;p&gt;
&#30913;&#28382;&#24314;&#27169;&#30340;&#31070;&#32463;&#25391;&#33633;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural oscillators for magnetic hysteresis modeling. (arXiv:2308.12002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;HystRNN&#30340;&#31070;&#32463;&#25391;&#33633;&#22120;&#65292;&#36890;&#36807;&#26356;&#26032;&#38544;&#34255;&#29366;&#24577;&#26469;&#24314;&#27169;&#21644;&#37327;&#21270;&#30913;&#28382;&#29616;&#35937;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#24191;&#20041;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#26174;&#20102;&#31070;&#32463;&#25391;&#33633;&#22120;&#22312;&#25429;&#25417;&#30913;&#24615;&#26448;&#26009;&#30340;&#22797;&#26434;&#30913;&#28382;&#27169;&#24335;&#26041;&#38754;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#28382;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65307;&#20854;&#24314;&#27169;&#21644;&#35782;&#21035;&#23545;&#20110;&#29702;&#35299;&#21644;&#20248;&#21270;&#21508;&#31181;&#31995;&#32479;&#30340;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#21644;&#37327;&#21270;&#30913;&#28382;&#65292;&#23427;&#34920;&#29616;&#20026;&#26102;&#24207;&#24615;&#21644;&#21382;&#21490;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#25391;&#33633;&#22120;&#65292;HystRNN&#65292;&#21463;&#32806;&#21512;&#25391;&#33633;RNN&#21644;&#29616;&#35937;&#23398;&#30913;&#28382;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#26356;&#26032;&#38544;&#34255;&#29366;&#24577;&#26469;&#25551;&#36848;&#30913;&#28382;&#12290;&#35780;&#20272;&#20102;HystRNN&#22312;&#39044;&#27979;&#24191;&#20041;&#22330;&#26223;&#65288;&#21253;&#25324;&#19968;&#38454;&#21453;&#36716;&#26354;&#32447;&#21644;&#23567;&#29615;&#65289;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;HystRNN&#20855;&#26377;&#23558;&#20854;&#34892;&#20026;&#25512;&#24191;&#21040;&#20197;&#21069;&#26410;&#32463;&#35757;&#32451;&#21306;&#22495;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#30913;&#28382;&#27169;&#22411;&#24517;&#39035;&#20855;&#22791;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#26174;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#65292;&#22312;&#25429;&#25417;&#30913;&#24615;&#26448;&#26009;&#20013;&#22797;&#26434;&#30913;&#28382;&#27169;&#24335;&#26041;&#38754;&#65292;&#31070;&#32463;&#25391;&#33633;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hysteresis is a ubiquitous phenomenon in science and engineering; its modeling and identification are crucial for understanding and optimizing the behavior of various systems. We develop an ordinary differential equation-based recurrent neural network (RNN) approach to model and quantify the hysteresis, which manifests itself in sequentiality and history-dependence. Our neural oscillator, HystRNN, draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states. The performance of HystRNN is evaluated to predict generalized scenarios, involving first-order reversal curves and minor loops. The findings show the ability of HystRNN to generalize its behavior to previously untrained regions, an essential feature that hysteresis models must have. This research highlights the advantage of neural oscillators over the traditional RNN-based methods in capturing complex hysteresis patterns in magnetic materials, where traditional rate-dependent me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39044;&#31639;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#19981;&#23384;&#22312;&#27604;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#33268;&#31283;&#23450;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#30340;&#31639;&#27861;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36825;&#19968;&#32467;&#26524;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;</title><link>http://arxiv.org/abs/2308.12000</link><description>&lt;p&gt;
&#26377;&#20851;&#22312;&#26377;&#38480;&#39044;&#31639;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#32479;&#19968;&#26368;&#20248;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget. (arXiv:2308.12000v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39044;&#31639;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#19981;&#23384;&#22312;&#27604;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#33268;&#31283;&#23450;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#30340;&#31639;&#27861;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36825;&#19968;&#32467;&#26524;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20271;&#21162;&#21033;&#22870;&#21169;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#20351;&#29992;&#26377;&#38480;&#39044;&#31639;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19981;&#23384;&#22312;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#65288;&#35813;&#31639;&#27861;&#34987;&#31216;&#20026;&#8220;&#22343;&#21248;&#37319;&#26679;&#8221;&#31639;&#27861;&#65289;&#65292;&#24182;&#19988;&#22312;&#33267;&#23569;&#19968;&#20010;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#35813;&#31639;&#27861;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#19981;&#23384;&#22312;&#27604;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#19968;&#33268;&#8221;&#21644;&#8220;&#31283;&#23450;&#8221;&#31639;&#27861;&#30340;&#33258;&#28982;&#31867;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20219;&#20309;&#31639;&#27861;&#35201;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#65292;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36890;&#36807;&#23548;&#20986;&#28385;&#36275;&#20219;&#20309;&#19968;&#33268;&#19988;&#31283;&#23450;&#31639;&#27861;&#30340;&#38169;&#35823;&#29575;&#30340;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#19982;&#27492;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#25105;&#20204;&#23436;&#25104;&#20102;&#35777;&#26126;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35299;&#20915;&#20102;\cite{qin2022open}&#20013;&#25552;&#20986;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
&lt;/p&gt;</description></item><item><title>&#20851;&#31995;&#27010;&#24565;&#27169;&#22411;&#26159;&#19968;&#31181;&#20851;&#31995;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23478;&#26063;&#65292;&#29992;&#20110;&#22312;&#20851;&#31995;&#39046;&#22495;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#39044;&#27979;&#65292;&#30456;&#27604;&#38750;&#20851;&#31995;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#19982;&#29616;&#26377;&#30340;&#20851;&#31995;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#24182;&#25903;&#25345;&#29983;&#25104;&#37327;&#21270;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#26102;&#24178;&#39044;&#12289;&#36229;&#20986;&#20998;&#24067;&#24773;&#26223;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#21644;&#31232;&#32570;&#30340;&#27010;&#24565;&#30417;&#30563;&#31561;&#33499;&#21051;&#26465;&#20214;&#19979;&#20063;&#33021;&#26377;&#25928;&#24212;&#23545;&#12290;</title><link>http://arxiv.org/abs/2308.11991</link><description>&lt;p&gt;
&#20851;&#20110;&#20851;&#31995;&#27010;&#24565;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Relational Concept Based Models. (arXiv:2308.11991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11991
&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#27010;&#24565;&#27169;&#22411;&#26159;&#19968;&#31181;&#20851;&#31995;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23478;&#26063;&#65292;&#29992;&#20110;&#22312;&#20851;&#31995;&#39046;&#22495;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#39044;&#27979;&#65292;&#30456;&#27604;&#38750;&#20851;&#31995;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#19982;&#29616;&#26377;&#30340;&#20851;&#31995;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#24182;&#25903;&#25345;&#29983;&#25104;&#37327;&#21270;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#26102;&#24178;&#39044;&#12289;&#36229;&#20986;&#20998;&#24067;&#24773;&#26223;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#21644;&#31232;&#32570;&#30340;&#27010;&#24565;&#30417;&#30563;&#31561;&#33499;&#21051;&#26465;&#20214;&#19979;&#20063;&#33021;&#26377;&#25928;&#24212;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#31995;&#39046;&#22495;&#20013;&#35774;&#35745;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#65306;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#27010;&#24565;&#30340;&#27169;&#22411;&#65288;CBMs&#65289;&#65292;&#24182;&#27809;&#26377;&#35774;&#35745;&#26469;&#35299;&#20915;&#20851;&#31995;&#38382;&#39064;&#65292;&#32780;&#20851;&#31995;&#27169;&#22411;&#20063;&#27809;&#26377;&#20687;CBMs&#37027;&#26679;&#21487;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#27010;&#24565;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#20379;&#21487;&#35299;&#37322;&#20219;&#21153;&#39044;&#27979;&#30340;&#20851;&#31995;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23478;&#26063;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20174;&#22270;&#20687;&#20998;&#31867;&#21040;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65292;&#34920;&#26126;&#20851;&#31995;CBMs&#65306;&#65288;i&#65289;&#19982;&#29616;&#26377;&#30340;&#20851;&#31995;&#40657;&#30418;&#30340;&#27867;&#21270;&#24615;&#33021;&#30456;&#21305;&#37197;&#65288;&#19981;&#21516;&#20110;&#38750;&#20851;&#31995;&#30340;CBMs&#65289;&#65292;&#65288;ii&#65289;&#25903;&#25345;&#29983;&#25104;&#37327;&#21270;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65292;&#65288;iii&#65289;&#26377;&#25928;&#24212;&#23545;&#27979;&#35797;&#26102;&#30340;&#24178;&#39044;&#65292;&#20197;&#21450;&#65288;iv&#65289;&#32463;&#21463;&#20303;&#21253;&#25324;&#36229;&#20986;&#20998;&#24067;&#24773;&#26223;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#21644;&#31232;&#32570;&#30340;&#27010;&#24565;&#30417;&#30563;&#31561;&#33499;&#21051;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11978</link><description>&lt;p&gt;
&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#31614;&#39044;&#27979;&#19968;&#20010;&#23436;&#25972;&#30340;&#20855;&#26377;&#22810;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#33647;&#29289;&#21644;&#20998;&#23376;&#35774;&#35745;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#20986;&#29616;&#20102;&#20960;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;(1) &#36825;&#20123;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#24448;&#24448;&#26410;&#32463;&#28145;&#20837;&#25506;&#32034;&#65307;(2) &#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#22312;&#26377;&#38480;&#30340;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26367;&#25442;&#20026;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#65292;&#30740;&#31350;&#20102;GNN&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#21516;&#29983;&#25104;&#26694;&#26550;&#65288;GCPN&#21644;GraphAF&#65289;&#20013;&#20845;&#31181;GNN&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20998;&#23376;&#29983;&#25104;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#22238;&#24402;&#27169;&#22411;&#26469;&#36817;&#20284;&#35780;&#20998;&#35299;&#37322;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#24402;&#32435;&#24335;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#25552;&#20379;&#36817;&#20284;&#20540;&#30340;&#26377;&#25928;&#24615;&#20445;&#35777;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#25191;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11975</link><description>&lt;p&gt;
&#20351;&#29992;&#31526;&#21512;&#22238;&#24402;&#26041;&#27861;&#36817;&#20284;&#35780;&#20998;&#35299;&#37322;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Approximating Score-based Explanation Techniques Using Conformal Regression. (arXiv:2308.11975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#22238;&#24402;&#27169;&#22411;&#26469;&#36817;&#20284;&#35780;&#20998;&#35299;&#37322;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#24402;&#32435;&#24335;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#25552;&#20379;&#36817;&#20284;&#20540;&#30340;&#26377;&#25928;&#24615;&#20445;&#35777;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32463;&#24120;&#34987;&#29992;&#26469;&#29702;&#35299;&#40657;&#30418;&#27169;&#22411;&#32972;&#21518;&#30340;&#36923;&#36753;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#37322;&#25216;&#26415;&#36890;&#24120;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26102;&#38388;&#20851;&#38190;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#22238;&#24402;&#27169;&#22411;&#26469;&#36817;&#20284;&#35780;&#20998;&#35299;&#37322;&#25216;&#26415;&#65288;&#22914;SHAP&#65289;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#24402;&#32435;&#24335;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#25552;&#20379;&#20102;&#36817;&#20284;&#20540;&#30340;&#26377;&#25928;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#38750;&#31526;&#21512;&#24230;&#24230;&#37327;&#65292;&#26088;&#22312;&#21516;&#26102;&#32771;&#34385;&#36817;&#20284;&#35299;&#37322;&#30340;&#38590;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#20302;&#24265;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#36817;&#20284;&#35299;&#37322;&#30340;&#25928;&#29575;&#65288;&#21306;&#38388;&#22823;&#23567;&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;Transformer&#32593;&#32476;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36827;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11971</link><description>&lt;p&gt;
EVE: &#20351;&#29992;&#36974;&#34109;&#39044;&#27979;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;Transformer&#32593;&#32476;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36827;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#26159;&#30001;&#19968;&#31181;&#32479;&#19968;&#30340;Transformer&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;EVE&#36890;&#36807;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#26469;&#32479;&#19968;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#37325;&#24314;&#21487;&#35265;&#20449;&#21495;&#65292;&#21363;&#22270;&#20687;&#20687;&#32032;&#21644;&#25991;&#26412;&#26631;&#35760;&#12290;&#36890;&#36807;&#38598;&#25104;&#27169;&#24577;&#24863;&#30693;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22359;&#65292;EVE&#22312;&#19968;&#20010;&#20849;&#20139;&#30340;Transformer&#32593;&#32476;&#20013;&#32534;&#30721;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20999;&#25442;&#21040;&#19981;&#21516;&#30340;&#19987;&#23478;&#26469;&#25429;&#25417;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#22522;&#20110;&#19981;&#22343;&#21248;&#27169;&#22411;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#32925;&#33039;&#32959;&#30244;&#20998;&#21106;&#12290;&#31532;&#19968;&#20010;&#27969;&#31243;&#20351;&#29992;&#22810;&#31867;&#27169;&#22411;&#21516;&#26102;&#20998;&#21106;&#32925;&#33039;&#21644;&#32959;&#30244;&#65292;&#32780;&#31532;&#20108;&#20010;&#27969;&#31243;&#20351;&#29992;&#20004;&#20010;&#20108;&#36827;&#21046;&#27169;&#22411;&#20998;&#21035;&#20998;&#21106;&#32925;&#33039;&#21644;&#32959;&#30244;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20004;&#31181;&#27969;&#31243;&#22312;&#20998;&#21106;&#32925;&#33039;&#21644;&#32959;&#30244;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11969</link><description>&lt;p&gt;
&#19981;&#22343;&#21248;&#28151;&#21512;&#32593;&#32476;&#29992;&#20110;&#24102;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#32925;&#33039;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification. (arXiv:2308.11969v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#22522;&#20110;&#19981;&#22343;&#21248;&#27169;&#22411;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#32925;&#33039;&#32959;&#30244;&#20998;&#21106;&#12290;&#31532;&#19968;&#20010;&#27969;&#31243;&#20351;&#29992;&#22810;&#31867;&#27169;&#22411;&#21516;&#26102;&#20998;&#21106;&#32925;&#33039;&#21644;&#32959;&#30244;&#65292;&#32780;&#31532;&#20108;&#20010;&#27969;&#31243;&#20351;&#29992;&#20004;&#20010;&#20108;&#36827;&#21046;&#27169;&#22411;&#20998;&#21035;&#20998;&#21106;&#32925;&#33039;&#21644;&#32959;&#30244;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20004;&#31181;&#27969;&#31243;&#22312;&#20998;&#21106;&#32925;&#33039;&#21644;&#32959;&#30244;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32925;&#33039;&#32959;&#30244;&#30340;&#36127;&#25285;&#24456;&#37325;&#35201;&#65292;&#26159;&#30284;&#30151;&#27515;&#20129;&#30340;&#31532;&#22235;&#22823;&#21407;&#22240;&#12290;&#23545;&#20110;&#32925;&#32454;&#32990;&#30284;&#65288;HCC&#65289;&#65292;&#22312;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;CE-MRI&#65289;&#19978;&#30340;&#32925;&#33039;&#21644;&#32959;&#30244;&#36827;&#34892;&#30028;&#23450;&#65292;&#20197;&#25351;&#23548;&#27835;&#30103;&#31574;&#30053;&#12290;&#30001;&#20110;&#36825;&#20010;&#20219;&#21153;&#32791;&#26102;&#65292;&#38656;&#35201;&#39640;&#24230;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#19988;&#21487;&#33021;&#23384;&#22312;&#35266;&#23519;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22240;&#27492;&#36843;&#20999;&#38656;&#35201;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;MRI&#24207;&#21015;&#30340;&#39640;&#24230;&#21464;&#24322;&#24615;&#37117;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#19981;&#22343;&#21248;&#27169;&#22411;&#30340;&#19981;&#21516;&#27969;&#31243;&#65292;&#29992;&#20110;&#33719;&#21462;&#32925;&#33039;&#21644;&#32959;&#30244;&#30340;&#20998;&#21106;&#12290;&#31532;&#19968;&#20010;&#27969;&#31243;&#26159;&#22522;&#20110;&#22810;&#31867;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#20998;&#21106;&#32925;&#33039;&#21644;&#32959;&#30244;&#31867;&#21035;&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20108;&#36827;&#21046;&#27169;&#22411;&#65292;&#19968;&#20010;&#29992;&#20110;&#20998;&#21106;&#32925;&#33039;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#20998;&#21106;&#32959;&#30244;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#27969;&#31243;&#37117;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burden of liver tumors is important, ranking as the fourth leading cause of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is performed to guide the treatment strategy. As this task is time-consuming, needs high expertise and could be subject to inter-observer variability there is a strong need for automatic tools. However, challenges arise from the lack of available training data, as well as the high variability in terms of image resolution and MRI sequence. In this work we propose to compare two different pipelines based on anisotropic models to obtain the segmentation of the liver and tumors. The first pipeline corresponds to a baseline multi-class model that performs the simultaneous segmentation of the liver and tumor classes. In the second approach, we train two distinct binary models, one segmenting the liver only and the other the tumors. Our results show that both pipe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#30340;&#21487;&#22609;&#24615;&#19988;&#26131;&#20110;&#23454;&#26045;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#24182;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;</title><link>http://arxiv.org/abs/2308.11958</link><description>&lt;p&gt;
&#36890;&#36807;&#20877;&#29983;&#24615;&#27491;&#21017;&#21270;&#32500;&#25345;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Maintaining Plasticity via Regenerative Regularization. (arXiv:2308.11958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#30340;&#21487;&#22609;&#24615;&#19988;&#26131;&#20110;&#23454;&#26045;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#24182;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#21487;&#22609;&#24615;&#25351;&#30340;&#26159;&#20195;&#29702;&#24555;&#36895;&#36866;&#24212;&#26032;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#24050;&#30693;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#20250;&#22833;&#21435;&#21487;&#22609;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#21487;&#22609;&#24615;&#12290;&#36825;&#19982;&#26631;&#20934;&#30340;L2&#27491;&#21017;&#21270;&#38750;&#24120;&#30456;&#20284;&#65292;&#21807;&#19968;&#30340;&#21306;&#21035;&#22312;&#20110;L2 Init&#27491;&#21017;&#21270;&#26397;&#21521;&#21407;&#28857;&#12290;L2 Init&#26131;&#20110;&#23454;&#26045;&#65292;&#21482;&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#36229;&#21442;&#25968;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#21160;&#26426;&#19982;&#37325;&#32622;&#31070;&#32463;&#20803;&#25110;&#21442;&#25968;&#20540;&#30340;&#26041;&#27861;&#30456;&#21516;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#24403;&#26368;&#36817;&#30340;&#25439;&#22833;&#23545;&#29305;&#23450;&#21442;&#25968;&#19981;&#25935;&#24863;&#26102;&#65292;&#36825;&#20123;&#21442;&#25968;&#20250;&#21521;&#23427;&#20204;&#30340;&#21021;&#22987;&#20540;&#28418;&#31227;&#12290;&#36825;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#22312;&#20195;&#34920;&#36830;&#32493;&#23398;&#20064;&#20013;&#19981;&#21516;&#31867;&#22411;&#38750;&#24179;&#31283;&#24615;&#30340;&#31616;&#21333;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;L2 Init&#33021;&#22815;&#19968;&#33268;&#22320;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates 
&lt;/p&gt;</description></item><item><title>&#24403;MiniBatch SGD&#36935;&#19978;SplitFed Learning&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;MiniBatch-SFL&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;MiniBatch SGD&#21040;SplitFed Learning&#20013;&#65292;&#35299;&#20915;&#20102;&#38750;&#22343;&#34913;&#25968;&#25454;&#23548;&#33268;&#30340;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11953</link><description>&lt;p&gt;
&#24403;MiniBatch SGD&#36935;&#19978;SplitFed Learning: &#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation. (arXiv:2308.11953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11953
&lt;/p&gt;
&lt;p&gt;
&#24403;MiniBatch SGD&#36935;&#19978;SplitFed Learning&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;MiniBatch-SFL&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;MiniBatch SGD&#21040;SplitFed Learning&#20013;&#65292;&#35299;&#20915;&#20102;&#38750;&#22343;&#34913;&#25968;&#25454;&#23548;&#33268;&#30340;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#65288;&#20363;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#33021;&#22815;&#21327;&#21516;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;FL&#22312;&#35745;&#31639;&#19978;&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#38656;&#35201;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#22810;&#27425;&#12290;SplitFed&#23398;&#20064;&#65288;SFL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#19968;&#20010;&#20999;&#21106;&#23618;&#23558;&#27169;&#22411;&#20998;&#25104;&#20004;&#37096;&#20998;&#26469;&#20943;&#36731;&#23458;&#25143;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#36127;&#36733;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#21482;&#38656;&#35201;&#35757;&#32451;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#39640;&#24230;&#19981;&#22343;&#34913;&#26102;&#65292;SFL&#20173;&#28982;&#38754;&#20020;&#30528;&#8220;&#23458;&#25143;&#31471;&#28418;&#31227;&#8221;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MiniBatch-SFL&#12290;&#35813;&#31639;&#27861;&#23558;MiniBatch SGD&#24341;&#20837;SFL&#20013;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20197;FL&#26041;&#24335;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#32780;&#26381;&#21153;&#22120;&#21017;&#31867;&#20284;&#20110;MiniBatch SGD&#35757;&#32451;&#26381;&#21153;&#22120;&#31471;&#27169;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;MiniBatch-SFL&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#39044;&#26399;&#30340;&#26381;&#21153;&#22120;&#31471;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#26356;&#26032;&#26469;&#24471;&#21040;&#26399;&#26395;&#25439;&#22833;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;Transformer&#37329;&#23383;&#22612;&#32593;&#32476;(MTPNet)&#65292;&#36890;&#36807;&#24341;&#20837;&#32500;&#24230;&#19981;&#21464;&#30340;&#23884;&#20837;&#25216;&#26415;&#21644;&#25429;&#25417;&#19981;&#21463;&#38480;&#21046;&#23610;&#24230;&#19978;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25429;&#25417;&#22810;&#26679;&#30340;&#23395;&#33410;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11946</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;Transformer&#37329;&#23383;&#22612;&#32593;&#32476;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting. (arXiv:2308.11946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;Transformer&#37329;&#23383;&#22612;&#32593;&#32476;(MTPNet)&#65292;&#36890;&#36807;&#24341;&#20837;&#32500;&#24230;&#19981;&#21464;&#30340;&#23884;&#20837;&#25216;&#26415;&#21644;&#25429;&#25417;&#19981;&#21463;&#38480;&#21046;&#23610;&#24230;&#19978;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25429;&#25417;&#22810;&#26679;&#30340;&#23395;&#33410;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#28041;&#21450;&#23545;&#21382;&#21490;&#35760;&#24405;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#30001;&#20110;&#33021;&#22815;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;Transformer&#22312;MTS&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20165;&#23616;&#38480;&#20110;&#22312;&#22266;&#23450;&#23610;&#24230;&#25110;&#25351;&#25968;&#32423;&#22686;&#21152;&#30340;&#22810;&#20010;&#23610;&#24230;&#19978;&#24314;&#27169;&#26102;&#38388;&#20381;&#36182;&#24615;&#65288;&#22823;&#37096;&#20998;&#20197;2&#20026;&#22522;&#25968;&#65289;&#12290;&#36825;&#31181;&#38480;&#21046;&#38459;&#30861;&#20102;&#20854;&#22312;&#25429;&#25417;&#22810;&#26679;&#30340;&#23395;&#33410;&#24615;&#65288;&#20363;&#22914;&#23567;&#26102;&#21644;&#26085;&#24120;&#27169;&#24335;&#65289;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32500;&#24230;&#19981;&#21464;&#30340;&#23884;&#20837;&#25216;&#26415;&#65292;&#29992;&#20110;&#25429;&#25417;&#30701;&#26399;&#26102;&#38388;&#20381;&#36182;&#24615;&#24182;&#23558;MTS&#25968;&#25454;&#25237;&#24433;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;MTS&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#27493;&#39588;&#21644;&#21464;&#37327;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;Transformer&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;MTPNet&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#26377;&#25928;&#22320;&#25429;&#25417;&#22810;&#20010;&#19981;&#21463;&#38480;&#21046;&#23610;&#24230;&#19978;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#39044;&#27979;&#26159;&#20174;&#22810;&#23610;&#24230;&#28508;&#21464;&#37327;&#20013;&#25512;&#26029;&#24471;&#20986;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series (MTS) forecasting involves modeling temporal dependencies within historical records. Transformers have demonstrated remarkable performance in MTS forecasting due to their capability to capture long-term dependencies. However, prior work has been confined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially increase (most with base 2). This limitation hinders their effectiveness in capturing diverse seasonalities, such as hourly and daily patterns. In this paper, we introduce a dimension invariant embedding technique that captures short-term temporal dependencies and projects MTS data into a higher-dimensional space, while preserving the dimensions of time steps and variables in MTS data. Furthermore, we present a novel Multi-scale Transformer Pyramid Network (MTPNet), specifically designed to effectively capture temporal dependencies at multiple unconstrained scales. The predictions are inferred from multi-scale latent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#21270;&#30340;Ramsey&#25968;&#21453;&#20363;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21521;&#37327;&#21270;&#21644;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#65292;&#20197;&#23547;&#25214;&#29305;&#23450;Ramsey&#25968;&#30340;&#21453;&#20363;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#20248;&#21270;&#20197;&#38480;&#21046;&#22810;&#39033;&#24335;&#25628;&#32034;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11943</link><description>&lt;p&gt;
RamseyRL:&#19968;&#31181;&#26234;&#33021;&#21270;&#30340;Ramsey&#25968;&#21453;&#20363;&#25628;&#32034;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching. (arXiv:2308.11943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#21270;&#30340;Ramsey&#25968;&#21453;&#20363;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21521;&#37327;&#21270;&#21644;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#65292;&#20197;&#23547;&#25214;&#29305;&#23450;Ramsey&#25968;&#30340;&#21453;&#20363;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#20248;&#21270;&#20197;&#38480;&#21046;&#22810;&#39033;&#24335;&#25628;&#32034;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Ramsey&#25968;&#26159;&#20351;&#24471;&#25152;&#26377;&#33410;&#28857;&#25968;&#20026;$n$&#30340;&#31616;&#21333;&#26080;&#21521;&#22270;&#21253;&#21547;&#19968;&#20010;&#39034;&#24207;&#20026;$s$&#30340;&#22242;&#25110;&#32773;&#19968;&#20010;&#39034;&#24207;&#20026;$t$&#30340;&#29420;&#31435;&#38598;&#30340;&#26368;&#23567;&#25968;$n = R(s, t)$&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#20197;&#23547;&#25214;&#29305;&#23450;Ramsey&#25968;&#30340;&#21453;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22270;&#21521;&#37327;&#21270;&#21644;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36880;&#27493;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#25628;&#32034;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#25628;&#32034;&#65289;&#65292;&#36890;&#36807;&#34913;&#37327;&#22270;&#24418;&#25104;&#20026;&#21453;&#20363;&#30340;&#21487;&#33021;&#24615;&#26469;&#27979;&#31639;&#12290;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#31639;&#27861;&#20248;&#21270;&#20197;&#38480;&#21046;&#22810;&#39033;&#24335;&#25628;&#32034;&#36816;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#19981;&#26159;&#25552;&#20379;&#26032;&#30340;&#21453;&#20363;&#65292;&#32780;&#26159;&#20171;&#32461;&#21644;&#35780;&#20272;&#25903;&#25345;Ramsey&#21453;&#20363;&#25506;&#32034;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20854;&#20182;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#20195;&#30721;&#21644;&#26041;&#27861;&#36890;&#36807;PyPI&#36719;&#20214;&#21253;&#21644;GitHub&#23384;&#20648;&#24211;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Ramsey number is the minimum number of nodes, $n = R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11940</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21253;&#21547;&#38899;&#39057;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20165;&#20381;&#38752;&#25991;&#26412;&#20250;&#23548;&#33268;&#21463;&#25511;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#65288;&#21253;&#25324;&#20869;&#23481;&#65288;&#26102;&#38388;&#25139;&#65289;&#21644;&#39118;&#26684;&#65288;&#38899;&#39640;&#26354;&#32447;&#21644;&#33021;&#37327;&#26354;&#32447;&#65289;&#65289;&#20316;&#20026;&#25991;&#26412;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#20026;&#20102;&#20445;&#25345;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#25972;&#21512;&#20026;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#38899;&#39057;&#21644;&#30456;&#24212;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#23458;&#25143;&#38656;&#27714;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19982;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#22238;&#24402;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#38646;&#21806;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.11939</link><description>&lt;p&gt;
&#38646;&#21806;&#38656;&#27714;&#39044;&#27979;&#65306;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Retail Demand Forecasting: A Comparative Study for Multivariate Time Series. (arXiv:2308.11939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#23458;&#25143;&#38656;&#27714;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19982;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#22238;&#24402;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#38646;&#21806;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#21806;&#34892;&#19994;&#20013;&#65292;&#20934;&#30830;&#30340;&#38656;&#27714;&#39044;&#27979;&#26159;&#36130;&#21153;&#32489;&#25928;&#21644;&#20379;&#24212;&#38142;&#25928;&#29575;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;&#38543;&#30528;&#20840;&#29699;&#24066;&#22330;&#26085;&#30410;&#20114;&#32852;&#20114;&#36890;&#65292;&#20225;&#19994;&#24320;&#22987;&#37319;&#29992;&#20808;&#36827;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#33719;&#21462;&#31454;&#20105;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#21382;&#21490;&#38144;&#21806;&#25968;&#25454;&#65292;&#24573;&#30053;&#20102;&#23439;&#35266;&#32463;&#27982;&#26465;&#20214;&#23545;&#28040;&#36153;&#32773;&#25903;&#20986;&#34892;&#20026;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#23458;&#25143;&#38656;&#27714;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19982;&#28040;&#36153;&#32773;&#29289;&#20215;&#25351;&#25968;&#65288;CPI&#65289;&#12289;&#28040;&#36153;&#32773;&#20449;&#24515;&#25351;&#25968;&#65288;ICS&#65289;&#21644;&#22833;&#19994;&#29575;&#31561;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#30456;&#32467;&#21512;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#21033;&#29992;&#36825;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#22238;&#24402;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#38646;&#21806;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#35266;&#27979;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;Kalman&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#26102;&#38388;Ito&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25512;&#24191;Kalman&#28388;&#27874;&#22120;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#28388;&#27874;&#22120;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27966;&#29983;&#33719;&#24471;&#30340;&#35299;&#26512;&#24418;&#24335;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;SDE&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.11933</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
System Identification for Continuous-time Linear Dynamical Systems. (arXiv:2308.11933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#35266;&#27979;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;Kalman&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#26102;&#38388;Ito&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25512;&#24191;Kalman&#28388;&#27874;&#22120;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#28388;&#27874;&#22120;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27966;&#29983;&#33719;&#24471;&#30340;&#35299;&#26512;&#24418;&#24335;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;SDE&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kalman&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#22312;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#21442;&#25968;&#26102;&#65292;&#36890;&#24120;&#20551;&#35774;&#35266;&#27979;&#20540;&#22312;&#31561;&#38388;&#38548;&#30340;&#26102;&#38388;&#28857;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#26159;&#26377;&#38480;&#21046;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#36830;&#32493;&#31163;&#25955;&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#27714;&#35299;&#36830;&#32493;&#26102;&#38388;Ito&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#25512;&#24191;Kalman&#28388;&#27874;&#22120;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#28388;&#27874;&#22120;&#65292;&#20855;&#26377;&#36125;&#21494;&#26031;&#27966;&#29983;&#30340;&#35299;&#26512;&#24418;&#24335;&#30340;&#21518;&#39564;&#65292;&#36825;&#26679;&#21487;&#20197;&#24471;&#21040;&#19981;&#38656;&#35201;&#39044;&#20808;&#35745;&#31639;&#30340;&#27491;&#21521;&#20256;&#36882;&#30340;&#35299;&#26512;&#26356;&#26032;&#12290;&#21033;&#29992;&#36825;&#31181;&#35299;&#26512;&#30340;&#39640;&#25928;&#35745;&#31639;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;EM&#36807;&#31243;&#65292;&#29992;&#20110;&#20272;&#35745;SDE&#30340;&#21442;&#25968;&#65292;&#33258;&#28982;&#22320;&#32435;&#20837;&#20102;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurement
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#28369;&#22369;&#26131;&#21457;&#24615;&#21046;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#24180;&#24230;&#35780;&#20272;&#65292;&#24182;&#35299;&#20915;&#20102;&#23567;&#26679;&#26412;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11929</link><description>&lt;p&gt;
&#36817;30&#24180;&#26469;&#21160;&#24577;&#23665;&#22320;&#22478;&#24066;&#22320;&#21306;&#28369;&#22369;&#26131;&#21457;&#24615;&#21046;&#22270;&#25581;&#31034;&#28369;&#22369;&#25104;&#22240;&#30340;&#21464;&#21270;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas. (arXiv:2308.11929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#28369;&#22369;&#26131;&#21457;&#24615;&#21046;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#24180;&#24230;&#35780;&#20272;&#65292;&#24182;&#35299;&#20915;&#20102;&#23567;&#26679;&#26412;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28369;&#22369;&#26131;&#21457;&#24615;&#35780;&#20272;&#23545;&#20110;&#20943;&#36731;&#28369;&#22369;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#31354;&#20013;&#21644;&#21355;&#26143;&#25968;&#25454;&#30340;&#26085;&#30410;&#20016;&#23500;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#39044;&#27979;&#28369;&#22369;&#26131;&#21457;&#24615;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#28369;&#22369;&#35825;&#21457;&#29615;&#22659;&#65288;LIE&#65289;&#20869;&#30340;&#24555;&#36895;&#27874;&#21160;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38477;&#38632;&#31561;&#22806;&#37096;&#35302;&#21457;&#22240;&#32032;&#30340;&#26174;&#33879;&#21464;&#21270;&#65292;&#32473;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;LSA&#26041;&#27861;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#20869;&#36866;&#24212;LIE&#24102;&#26469;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21160;&#24577;&#28369;&#22369;&#26131;&#21457;&#24615;&#21046;&#22270;&#65292;&#31616;&#21333;&#22320;&#21033;&#29992;&#22810;&#31181;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#24180;&#24230;LSA&#12290;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#26576;&#20123;&#24180;&#20221;&#28369;&#22369;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#36935;&#21040;&#23567;&#26679;&#26412;&#38382;&#39064;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#29616;&#26377;&#30340;LSA&#26041;&#27861;&#22823;&#22810;&#37319;&#29992;&#35757;&#32451;&#40657;&#30418;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20294;&#24448;&#24448;&#22312;&#27867;&#21270;&#21644;&#25552;&#20379;&#20840;&#38754;&#35299;&#37322;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Landslide susceptibility assessment (LSA) is of paramount importance in mitigating landslide risks. Recently, there has been a surge in the utilization of data-driven methods for predicting landslide susceptibility due to the growing availability of aerial and satellite data. Nonetheless, the rapid oscillations within the landslide-inducing environment (LIE), primarily due to significant changes in external triggers such as rainfall, pose difficulties for contemporary data-driven LSA methodologies to accommodate LIEs over diverse timespans. This study presents dynamic landslide susceptibility mapping that simply employs multiple predictive models for annual LSA. In practice, this will inevitably encounter small sample problems due to the limited number of landslide samples in certain years. Another concern arises owing to the majority of the existing LSA approaches train black-box models to fit distinct datasets, yet often failing in generalization and providing comprehensive explanati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#27714;&#35299;&#26925;&#22278;&#22411;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#35823;&#24046;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11925</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26925;&#22278;&#22411;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks. (arXiv:2308.11925v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#27714;&#35299;&#26925;&#22278;&#22411;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#35823;&#24046;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#29992;&#20110;&#32447;&#24615;&#21644;&#21322;&#32447;&#24615;&#20108;&#38454;&#26925;&#22278;&#38382;&#39064;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65288;&#26377;/&#26080;&#30418;&#32422;&#26463;&#65289;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20174;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#19968;&#38454;&#26368;&#20248;&#24615;&#31995;&#32479;&#25512;&#23548;&#20986;&#30340;&#32806;&#21512;&#31995;&#32479;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26469;&#35299;&#20915;&#32806;&#21512;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25968;&#20540;&#26041;&#26696;&#36827;&#34892;&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65288;&#22914;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#21442;&#25968;&#33539;&#22260;&#65289;&#20197;&#21450;&#22495;&#20869;&#21644;&#36793;&#30028;&#19978;&#30340;&#37319;&#26679;&#28857;&#25968;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#29366;&#24577;&#12289;&#25511;&#21046;&#21644;&#20276;&#38543;&#29366;&#24577;&#30340;$L^2(\Omega)$&#35823;&#24046;&#30028;&#12290;&#20998;&#26512;&#20013;&#30340;&#20027;&#35201;&#24037;&#20855;&#21253;&#25324;&#20559;&#31227;Rademacher&#22797;&#26434;&#24230;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#26377;&#30028;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#25968;&#20540;&#31034;&#20363;&#26469;&#35828;&#26126;&#35813;&#26041;&#27861;&#65292;&#24182;&#19982;&#19977;&#31181;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present and analyze a numerical solver for optimal control problems (without / with box constraint) for linear and semilinear second-order elliptic problems. The approach is based on a coupled system derived from the first-order optimality system of the optimal control problem, and applies physics informed neural networks (PINNs) to solve the coupled system. We present an error analysis of the numerical scheme, and provide $L^2(\Omega)$ error bounds on the state, control and adjoint state in terms of deep neural network parameters (e.g., depth, width, and parameter bounds) and the number of sampling points in the domain and on the boundary. The main tools in the analysis include offset Rademacher complexity and boundedness and Lipschitz continuity of neural network functions. We present several numerical examples to illustrate the approach and compare it with three existing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#22810;&#26679;&#21270;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#39640;&#25928;&#24615;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11924</link><description>&lt;p&gt;
&#19981;&#21516;&#25919;&#31574;&#22312;&#26080;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diverse Policies Converge in Reward-free Markov Decision Processe. (arXiv:2308.11924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#22810;&#26679;&#21270;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#39640;&#25928;&#24615;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#20915;&#31574;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#26159;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#21333;&#19968;&#30340;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#21457;&#23637;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#22810;&#31181;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#20013;&#27809;&#26377;&#19968;&#20010;&#22312;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#31639;&#27861;&#22914;&#20309;&#25910;&#25947;&#20197;&#21450;&#31639;&#27861;&#30340;&#25928;&#29575;&#22914;&#20309;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#22810;&#26679;&#21270;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#36825;&#26679;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#39640;&#25928;&#24615;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38899;&#39057;&#24046;&#24322;&#23383;&#24149;&#29983;&#25104;&#65288;ADC&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#25193;&#23637;&#20219;&#21153;&#65292;&#29992;&#20110;&#25551;&#36848;&#31867;&#20284;&#20294;&#30053;&#26377;&#24046;&#24322;&#30340;&#38899;&#39057;&#29255;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#38598;&#20013;&#30340;Transformer&#32534;&#30721;&#22120;&#21644;&#30456;&#20284;&#24615;-&#24046;&#24322;&#35299;&#32544;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#24046;&#24322;&#25551;&#36848;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21487;&#35270;&#21270;&#26469;&#25913;&#21892;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#21462;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.11923</link><description>&lt;p&gt;
&#21033;&#29992;&#30456;&#20284;&#24615;-&#24046;&#24322;&#35299;&#32544;&#26469;&#36827;&#34892;&#38899;&#39057;&#24046;&#24322;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement. (arXiv:2308.11923v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38899;&#39057;&#24046;&#24322;&#23383;&#24149;&#29983;&#25104;&#65288;ADC&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#25193;&#23637;&#20219;&#21153;&#65292;&#29992;&#20110;&#25551;&#36848;&#31867;&#20284;&#20294;&#30053;&#26377;&#24046;&#24322;&#30340;&#38899;&#39057;&#29255;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#38598;&#20013;&#30340;Transformer&#32534;&#30721;&#22120;&#21644;&#30456;&#20284;&#24615;-&#24046;&#24322;&#35299;&#32544;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#24046;&#24322;&#25551;&#36848;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21487;&#35270;&#21270;&#26469;&#25913;&#21892;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#21462;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#38899;&#39057;&#24046;&#24322;&#23383;&#24149;&#29983;&#25104;&#65288;ADC&#65289;&#20316;&#20026;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#26032;&#25193;&#23637;&#20219;&#21153;&#65292;&#29992;&#20110;&#25551;&#36848;&#31867;&#20284;&#20294;&#30053;&#26377;&#24046;&#24322;&#30340;&#38899;&#39057;&#29255;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;ADC&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#65292;&#23545;&#20110;&#30456;&#20284;&#38899;&#39057;&#29255;&#27573;&#29983;&#25104;&#31867;&#20284;&#23383;&#24149;&#30340;&#38382;&#39064;&#65292;&#26080;&#27861;&#25551;&#36848;&#20869;&#23481;&#24046;&#24322;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#27880;&#24847;&#21147;&#38598;&#20013;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#27604;&#36739;&#19968;&#23545;&#38899;&#39057;&#29255;&#27573;&#21644;&#19968;&#31181;&#30456;&#20284;&#24615;-&#24046;&#24322;&#35299;&#32544;&#26469;&#24378;&#35843;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;AudioDiffCaps&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#31867;&#20284;&#20294;&#30053;&#26377;&#24046;&#24322;&#30340;&#38899;&#39057;&#29255;&#27573;&#23545;&#20197;&#21450;&#20154;&#24037;&#26631;&#27880;&#30340;&#23427;&#20204;&#20043;&#38388;&#24046;&#24322;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;ADC&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22312;Transformer&#32534;&#30721;&#22120;&#20013;&#23545;&#20854;&#36827;&#34892;&#21487;&#35270;&#21270;&#26469;&#25913;&#21892;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#21462;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#20013;&#23384;&#22312;&#30340;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#30340;&#32858;&#21512;&#24433;&#21709;&#20989;&#25968;&#26041;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11912</link><description>&lt;p&gt;
&#35299;&#20915;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65306;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#30340;&#32858;&#21512;&#24433;&#21709;&#20989;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach. (arXiv:2308.11912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#20013;&#23384;&#22312;&#30340;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#30340;&#32858;&#21512;&#24433;&#21709;&#20989;&#25968;&#26041;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#39640;&#25928;&#27979;&#35797;&#27169;&#24335;&#65292;&#21487;&#20197;&#26681;&#25454;&#21463;&#35797;&#32773;&#22312;&#27979;&#35797;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#36827;&#34892;&#36866;&#24212;&#12290;CAT&#38656;&#35201;&#39044;&#20808;&#35757;&#32451;&#30340;&#39033;&#30446;&#31616;&#20171;&#65292;&#22240;&#20026;CAT&#26681;&#25454;&#24050;&#27880;&#20876;&#39033;&#30446;&#30340;&#31616;&#20171;&#23454;&#26102;&#35780;&#20272;&#23398;&#29983;&#65292;&#24182;&#20351;&#29992;&#20505;&#36873;&#39033;&#30446;&#30340;&#31616;&#20171;&#36873;&#25321;&#19979;&#19968;&#20010;&#35201;&#25351;&#23548;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36825;&#26679;&#30340;&#39033;&#30446;&#31616;&#20171;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#28041;&#21450;&#25910;&#38598;&#22823;&#37327;&#23494;&#38598;&#30340;&#39033;&#30446;&#21709;&#24212;&#25968;&#25454;&#65292;&#28982;&#21518;&#22312;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#35786;&#26029;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;CAT&#26381;&#21153;&#20013;&#25910;&#38598;&#30340;&#21709;&#24212;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36825;&#24102;&#26469;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;CAT&#24341;&#20837;&#20102;&#22266;&#26377;&#30340;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#29087;&#32451;&#31243;&#24230;&#26356;&#39640;&#30340;&#23398;&#29983;&#20250;&#25910;&#21040;&#26356;&#38590;&#30340;&#38382;&#39064;&#12290;&#23454;&#38469;&#19978;&#65292;&#24403;&#20351;&#29992;CAT&#21709;&#24212;&#25968;&#25454;&#36827;&#34892;&#31616;&#21333;&#35757;&#32451;&#35786;&#26029;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39033;&#30446;&#31616;&#20171;&#19982;&#23454;&#38469;&#24773;&#20917;&#26174;&#33879;&#20559;&#31163;&#12290;&#20026;&#20102;&#35299;&#20915;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29992;&#25143;&#30340;&#32858;&#21512;&#24433;&#21709;&#20989;&#25968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computerized Adaptive Testing (CAT) is a widely used, efficient test mode that adapts to the examinee's proficiency level in the test domain. CAT requires pre-trained item profiles, for CAT iteratively assesses the student real-time based on the registered items' profiles, and selects the next item to administer using candidate items' profiles. However, obtaining such item profiles is a costly process that involves gathering a large, dense item-response data, then training a diagnostic model on the collected data. In this paper, we explore the possibility of leveraging response data collected in the CAT service. We first show that this poses a unique challenge due to the inherent selection bias introduced by CAT, i.e., more proficient students will receive harder questions. Indeed, when naively training the diagnostic model using CAT response data, we observe that item profiles deviate significantly from the ground-truth. To tackle the selection bias issue, we propose the user-wise agg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11905</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#25509;&#21463;&#36793;&#30028;&#36827;&#34892;&#21551;&#21457;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Utilizing Admissible Bounds for Heuristic Learning. (arXiv:2308.11905v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21033;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#21069;&#21521;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#24212;&#35813;&#23398;&#20064;&#30340;&#20869;&#23481;&#12289;&#22914;&#20309;&#35757;&#32451;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#26679;&#20570;&#30340;&#29702;&#35770;&#35748;&#35782;&#36824;&#24456;&#23569;&#12290;&#36825;&#31181;&#29702;&#35299;&#30340;&#19981;&#36275;&#23548;&#33268;&#25991;&#29486;&#20013;&#36827;&#34892;&#25968;&#25454;&#38598;&#36873;&#25321;&#65288;&#27425;&#20248;&#25104;&#26412;&#23545;&#26368;&#20248;&#25104;&#26412;&#25110;&#21487;&#25509;&#21463;&#23545;&#19981;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#65289;&#21644;&#20248;&#21270;&#25351;&#26631;&#65288;&#20363;&#22914;&#24179;&#26041;&#35823;&#24046;&#21644;&#32477;&#23545;&#35823;&#24046;&#65289;&#26102;&#36827;&#34892;&#20102;&#20020;&#26102;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25152;&#24471;&#21040;&#30340;&#35757;&#32451;&#21551;&#21457;&#24335;&#20989;&#25968;&#32570;&#20047;&#21487;&#25509;&#21463;&#24615;&#65292;&#23545;&#20110;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25509;&#21463;&#24615;&#30340;&#37325;&#35201;&#24615;&#20063;&#32570;&#20047;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#30456;&#27604;&#26222;&#36890;&#39640;&#26031;&#20998;&#24067;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#25968;&#23398;&#27169;&#22411;&#24544;&#23454;&#22320;&#36981;&#24490;&#20102;&#26368;&#22823;&#29109;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and em
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#25200;&#21160;&#21644;&#27169;&#22411;&#31283;&#23450;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;DPMS&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#21322;&#30417;&#30563;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11903</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#25200;&#21160;&#21644;&#27169;&#22411;&#31283;&#23450;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation. (arXiv:2308.11903v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#25200;&#21160;&#21644;&#27169;&#22411;&#31283;&#23450;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;DPMS&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#21322;&#30417;&#30563;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65288;SSMIS&#65289;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#30001;&#20110;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;SSMIS&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20248;&#20808;&#32771;&#34385;&#38598;&#25104;&#22797;&#26434;&#25216;&#26415;&#21644;&#25439;&#22833;&#39033;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35299;&#20915;&#21322;&#30417;&#30563;&#22330;&#26223;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;SSMIS&#30340;&#20851;&#38190;&#22312;&#20110;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#29983;&#25104;&#20805;&#20998;&#19988;&#21512;&#36866;&#30340;&#39044;&#27979;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#21322;&#30417;&#30563;&#20998;&#21106;&#20013;&#25968;&#25454;&#25200;&#21160;&#21644;&#27169;&#22411;&#31283;&#23450;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26174;&#33879;&#25552;&#39640;SSMIS&#24615;&#33021;&#65292;&#31216;&#20026;DPMS&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#25439;&#22833;&#19977;&#20010;&#19981;&#21516;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;SSMIS&#65292;&#24182;&#23545;&#30456;&#24212;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#20197;&#26816;&#39564;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies on semi-supervised medical image segmentation (SSMIS) have seen fast progress recently. Due to the limited labelled data, SSMIS methods mainly focus on effectively leveraging unlabeled data to enhance the segmentation performance. However, despite their promising performance, current state-of-the-art methods often prioritize integrating complex techniques and loss terms rather than addressing the core challenges of semi-supervised scenarios directly. We argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. To this end, we emphasize the crutiality of data perturbation and model stabilization in semi-supervised segmentation, and propose a simple yet effective approach to boost SSMIS performance significantly, dubbed DPMS. Specifically, we first revisit SSMIS from three distinct perspectives: the data, the model, and the loss, and conduct a comprehensive study of corresponding strategies to examine their effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#21464;&#24418;&#29366;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;ShapeMol&#25104;&#21151;&#29983;&#25104;&#20102;&#26032;&#39062;&#12289;&#22810;&#26679;&#19988;&#31867;&#20284;&#32473;&#23450;&#24418;&#29366;&#26465;&#20214;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2308.11890</link><description>&lt;p&gt;
&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#24418;&#29366;&#30340;3D&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models. (arXiv:2308.11890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#21464;&#24418;&#29366;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;ShapeMol&#25104;&#21151;&#29983;&#25104;&#20102;&#26032;&#39062;&#12289;&#22810;&#26679;&#19988;&#31867;&#20284;&#32473;&#23450;&#24418;&#29366;&#26465;&#20214;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#20307;&#22522;&#33647;&#29289;&#35774;&#35745;&#26088;&#22312;&#35782;&#21035;&#19982;&#24050;&#30693;&#27963;&#24615;&#20998;&#23376;&#24418;&#29366;&#30456;&#20284;&#30340;&#26032;&#22411;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#20998;&#23376;&#30340;&#24418;&#29366;&#26465;&#20214;&#19979;&#29983;&#25104;3D&#20998;&#23376;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31561;&#21464;&#24418;&#29366;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;ShapeMol&#12290;ShapeMol&#30001;&#19968;&#20010;&#31561;&#21464;&#24418;&#29366;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#36825;&#20123;&#32534;&#30721;&#29983;&#25104;3D&#20998;&#23376;&#30340;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#32452;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ShapeMol&#33021;&#22815;&#29983;&#25104;&#26032;&#39062;&#12289;&#22810;&#26679;&#19988;&#31867;&#20284;&#32473;&#23450;&#24418;&#29366;&#26465;&#20214;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;ShapeMol&#22312;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;3D&#24418;&#29366;&#24182;&#19982;&#34507;&#30333;&#38774;&#28857;&#32467;&#21512;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#25511;&#21046;&#30340;&#26032;&#22411;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21453;&#39304;&#25511;&#21046;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22686;&#24378;DNN&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11881</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#39304;&#24490;&#29615;&#30340;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training Using Feedback Loops. (arXiv:2308.11881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#25511;&#21046;&#30340;&#26032;&#22411;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21453;&#39304;&#25511;&#21046;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22686;&#24378;DNN&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30001;&#20110;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#38750;&#24120;&#22797;&#26434;&#30340;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#32780;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;DNN&#20934;&#30830;&#24615;&#39640;&#19988;&#20351;&#29992;&#24191;&#27867;&#65292;&#20294;&#30001;&#20110;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#23427;&#20204;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#65292;&#26500;&#24314;&#23545;&#20219;&#20309;&#25968;&#25454;&#28857;&#30340;&#25200;&#21160;&#37117;&#20855;&#26377;&#25269;&#25239;&#33021;&#21147;&#30340;DNN&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36807;&#21435;&#25552;&#20986;&#20102;&#35768;&#22810;&#20351;&#29992;&#32593;&#32476;&#30340;&#19968;&#38454;&#23548;&#25968;&#20449;&#24687;&#26469;&#22686;&#24378;DNN&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#35770;&#30340;&#26032;&#22411;&#22686;&#24378;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21453;&#39304;&#25511;&#21046;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#21453;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#25511;&#21046;&#22120;&#26412;&#36523;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#24120;&#35268;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#31283;&#23450;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#22522;&#20110;&#21453;&#39304;&#25511;&#21046;&#26550;&#26500;&#30340;&#26032;&#22411;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#31216;&#20026;&#21453;&#39304;&#24490;&#29615;&#23545;&#25239;&#35757;&#32451;(FLAT)&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have found wide applicability in numerous fields due to their ability to accurately learn very complex input-output relations. Despite their accuracy and extensive use, DNNs are highly susceptible to adversarial attacks due to limited generalizability. For future progress in the field, it is essential to build DNNs that are robust to any kind of perturbations to the data points. In the past, many techniques have been proposed to robustify DNNs using first-order derivative information of the network.  This paper proposes a new robustification approach based on control theory. A neural network architecture that incorporates feedback control, named Feedback Neural Networks, is proposed. The controller is itself a neural network, which is trained using regular and adversarial data such as to stabilize the system outputs. The novel adversarial training approach based on the feedback control architecture is called Feedback Looped Adversarial Training (FLAT). Numeri
&lt;/p&gt;</description></item><item><title>SUMMIT&#26041;&#27861;&#25918;&#23485;&#20102;&#20256;&#32479;&#27169;&#22411;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#20004;&#20010;&#20551;&#35774;&#65292;&#36890;&#36807;&#35299;&#20915;&#26080;&#28304;&#25968;&#25454;&#12289;&#26080;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#23558;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36866;&#24212;&#21040;&#22810;&#27169;&#24577;&#30446;&#26631;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11880</link><description>&lt;p&gt;
SUMMIT: &#26080;&#28304;&#33258;&#36866;&#24212;&#21333;&#27169;&#22411;&#21040;&#22810;&#27169;&#24577;&#30446;&#26631;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets. (arXiv:2308.11880v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11880
&lt;/p&gt;
&lt;p&gt;
SUMMIT&#26041;&#27861;&#25918;&#23485;&#20102;&#20256;&#32479;&#27169;&#22411;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#20004;&#20010;&#20551;&#35774;&#65292;&#36890;&#36807;&#35299;&#20915;&#26080;&#28304;&#25968;&#25454;&#12289;&#26080;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#23558;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36866;&#24212;&#21040;&#22810;&#27169;&#24577;&#30446;&#26631;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#22330;&#26223;&#29702;&#35299;&#26159;&#24517;&#35201;&#30340;&#65292;&#20363;&#22914;&#33258;&#20027;&#23548;&#33322;&#12290;&#20026;&#20102;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#32321;&#29712;&#30340;&#25968;&#25454;&#26631;&#27880;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20551;&#35774;&#22312;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#23384;&#22312;&#28304;&#25968;&#25454;&#65292;&#24182;&#19988;&#28304;&#25968;&#25454;&#26159;&#25104;&#23545;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#35768;&#22810;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;&#28304;&#25968;&#25454;&#21487;&#33021;&#30001;&#20110;&#38544;&#31169;&#12289;&#23433;&#20840;&#25110;&#32463;&#27982;&#26041;&#38754;&#30340;&#32771;&#34385;&#32780;&#19981;&#21487;&#29992;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20551;&#35774;&#23384;&#22312;&#25104;&#23545;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36824;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#65292;&#24182;&#19988;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24191;&#27867;&#21487;&#29992;&#30340;&#20813;&#36153;&#20998;&#21457;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#20197;&#19979;&#38382;&#39064;&#26469;&#25918;&#23485;&#36825;&#20004;&#20010;&#20551;&#35774;&#65306;&#22914;&#20309;&#23558;&#29420;&#31435;&#35757;&#32451;&#22312;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#30340;&#19968;&#32452;&#27169;&#22411;&#33258;&#36866;&#24212;&#21040;&#30001;&#26080;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#32452;&#25104;&#30340;&#30446;&#26631;&#22495;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#28304;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. O
&lt;/p&gt;</description></item><item><title>Cabrita&#26159;&#19968;&#31181;&#35299;&#20915;&#24615;&#33021;&#21644;&#39640;&#25928;&#26631;&#35760;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21487;&#25215;&#21463;&#30340;&#25104;&#26412;&#35299;&#20915;&#20102;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.11878</link><description>&lt;p&gt;
Cabrita: &#24357;&#21512;&#22806;&#35821;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Cabrita: closing the gap for foreign languages. (arXiv:2308.11878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11878
&lt;/p&gt;
&lt;p&gt;
Cabrita&#26159;&#19968;&#31181;&#35299;&#20915;&#24615;&#33021;&#21644;&#39640;&#25928;&#26631;&#35760;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21487;&#25215;&#21463;&#30340;&#25104;&#26412;&#35299;&#20915;&#20102;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#35821;&#35328;&#25110;&#39046;&#22495;&#20013;&#26377;&#20004;&#20010;&#37325;&#35201;&#30446;&#30340;&#65306;i)&#22686;&#24378;&#22312;&#29305;&#23450;&#35821;&#35328;&#25110;&#39046;&#22495;&#32972;&#26223;&#19979;&#30340;&#24615;&#33021;&#65292;ii)&#30830;&#20445;&#26377;&#25928;&#30340;&#26631;&#35760;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#30456;&#20851;&#25104;&#26412;&#65292;&#36825;&#20123;&#25104;&#26412;&#21487;&#33021;&#36798;&#21040;&#20845;&#20301;&#25968;&#29978;&#33267;&#19971;&#20301;&#25968;&#30340;&#32654;&#20803;&#37329;&#39069;&#65292;&#36825;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#21644;&#28041;&#21450;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25104;&#26412;&#25361;&#25112;&#65292;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#26159;&#20381;&#36182;&#21487;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23613;&#31649;&#26368;&#36817;&#20986;&#29616;&#20102;&#20687;LLaMA&#21644;LLaMA-2&#27169;&#22411;&#36825;&#26679;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#26576;&#20123;&#29305;&#23450;&#39046;&#22495;&#38382;&#39064;&#20173;&#28982;&#34920;&#29616;&#20302;&#25928;&#65292;&#25110;&#32773;&#22312;&#28041;&#21450;&#23545;&#35805;&#24335;&#35760;&#24518;&#36164;&#28304;&#30340;&#22330;&#26223;&#20013;&#26080;&#25928;&#65292;&#22240;&#20026;&#34920;&#31034;&#25991;&#26412;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#24040;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cabrita&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#23427;&#25104;&#21151;&#35299;&#20915;&#20102;&#24615;&#33021;&#21644;&#39640;&#25928;&#26631;&#35760;&#21270;&#30340;&#38382;&#39064;&#65292;&#32780;&#19988;&#25104;&#26412;&#21487;&#25215;&#21463;&#12290;
&lt;/p&gt;
&lt;p&gt;
The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.  The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.  To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#35843;&#35797;C&#32534;&#35793;&#22120;&#30340;&#38169;&#35823;&#35299;&#37322;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#35780;&#20272;&#35777;&#26126;&#20854;&#22312;&#32534;&#35793;&#26102;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11873</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#35843;&#35797;C&#32534;&#35793;&#22120;&#20013;&#20197;&#29983;&#25104;&#19978;&#19979;&#25991;&#38169;&#35823;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations. (arXiv:2308.11873v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#35843;&#35797;C&#32534;&#35793;&#22120;&#30340;&#38169;&#35823;&#35299;&#37322;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#35780;&#20272;&#35777;&#26126;&#20854;&#22312;&#32534;&#35793;&#26102;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25105;&#20204;&#30340;&#35843;&#35797;C&#32534;&#35793;&#22120;&#65288;DCC&#65289;&#20013;&#29983;&#25104;&#22686;&#24378;&#22411;&#32534;&#35793;&#22120;&#38169;&#35823;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#32534;&#35793;&#22120;&#38169;&#35823;&#28040;&#24687;&#23545;&#20110;&#21021;&#23398;&#32773;&#23398;&#20064;&#22914;&#20309;&#32534;&#31243;&#26159;&#19968;&#20010;&#38556;&#30861;&#12290;&#34429;&#28982;&#25105;&#20204;&#26368;&#21021;&#22312;&#20837;&#38376;&#32534;&#31243;&#65288;CS1&#65289;&#20013;&#20351;&#29992;DCC&#24050;&#32463;&#36890;&#36807;&#25552;&#20379;&#24120;&#35265;&#38169;&#35823;&#30340;&#20445;&#25252;&#26426;&#21046;&#21644;&#32763;&#35793;&#36890;&#24120;&#21547;&#20041;&#38544;&#26214;&#30340;&#32534;&#35793;&#22120;&#38169;&#35823;&#28040;&#24687;&#65292;&#26377;&#21161;&#20110;&#25945;&#25480;C&#35821;&#35328;&#32473;&#21021;&#23398;&#32773;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#23558;LLM&#29983;&#25104;&#30340;&#35299;&#37322;&#32435;&#20837;&#36827;&#26469;&#20250;&#36827;&#19968;&#27493;&#22686;&#24378;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#36890;&#36807;&#19987;&#23478;&#35780;&#20272;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#29983;&#25104;&#30340;&#32534;&#35793;&#22120;&#38169;&#35823;&#35299;&#37322;&#22312;90%&#30340;&#32534;&#35793;&#26102;&#38169;&#35823;&#21644;75%&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#20013;&#22312;&#27010;&#24565;&#19978;&#26159;&#20934;&#30830;&#30340;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;DCC&#24110;&#21161;&#24037;&#20855;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#34987;&#23398;&#29983;&#20351;&#29992;&#65292;&#27599;&#21608;&#24179;&#22343;&#26377;1047&#27425;&#29420;&#29305;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a method for Large Language Models (LLM) to produce enhanced compiler error explanations, in simple language, within our Debugging C Compiler (DCC). It is well documented that compiler error messages have been known to present a barrier for novices learning how to program. Although our initial use of DCC in introductory programming (CS1) has been instrumental in teaching C to novice programmers by providing safeguards to commonly occurring errors and translating the usually cryptic compiler error messages at both compile- and run-time, we proposed that incorporating LLM-generated explanations would further enhance the learning experience for novice programmers. Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors, and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, dem
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12289;&#35838;&#31243;&#36827;&#24230;&#24494;&#35843;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#25968;&#25454;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11863</link><description>&lt;p&gt;
KinSPEAK: &#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods. (arXiv:2308.11863v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11863
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12289;&#35838;&#31243;&#36827;&#24230;&#24494;&#35843;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#25968;&#25454;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#20855;&#22791;&#20102;&#22823;&#35268;&#27169;&#35760;&#24405;&#30340;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#35821;&#38899;&#25968;&#25454;&#65292;&#20294;&#26159;&#23454;&#29616;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#30340;&#24378;&#22823;&#35821;&#38899;&#35782;&#21035;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36981;&#24490;&#31616;&#21333;&#30340;&#35838;&#31243;&#36827;&#24230;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21450;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26469;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#35821;&#38899;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20851;&#27880;&#20351;&#29992;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;&#20844;&#20849;&#32593;&#31449;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#21046;&#20316;&#23460;&#32423;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#24178;&#20928;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35813;&#24178;&#20928;&#30340;&#22522;&#20934;&#27169;&#22411;&#23545;&#26469;&#33258;&#26356;&#22810;&#22810;&#26679;&#21644;&#22024;&#26434;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#65292;&#23450;&#20041;&#19968;&#20010;&#31616;&#21333;&#30340;&#35838;&#31243;&#35757;&#32451;&#36827;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#36830;&#32493;&#22235;&#20195;&#23545;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#21644;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;3.2&#65285;&#30340;&#23383;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#22312;Mozilla Common Voice&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;15.9&#65285;&#30340;WER&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;ClimateBench&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#27668;&#20505;&#27169;&#25311;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#27604;&#36739;&#20102;&#19977;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22120;&#22312;&#27169;&#25311;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.11854</link><description>&lt;p&gt;
&#23547;&#25214;&#23436;&#32654;&#25311;&#21512;: &#24212;&#29992;&#22238;&#24402;&#27169;&#22411;&#21040;ClimateBench v1.0
&lt;/p&gt;
&lt;p&gt;
Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0. (arXiv:2308.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;ClimateBench&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#27668;&#20505;&#27169;&#25311;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#27604;&#36739;&#20102;&#19977;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22120;&#22312;&#27169;&#25311;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#27169;&#25311;&#22120;&#30340;&#27668;&#20505;&#39044;&#27979;&#26159;&#30740;&#31350;&#30340;&#37325;&#28857;&#20043;&#19968;&#65292;&#20197;&#20351;&#20915;&#31574;&#32773;&#33021;&#22815;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;&#22120;&#20316;&#20026;&#35745;&#31639;&#22797;&#26434;&#30340;GCM&#27169;&#25311;&#22120;&#30340;&#26367;&#20195;&#65292;&#21487;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;ClimateBench&#26159;&#19968;&#20010;&#26368;&#36817;&#20026;&#35780;&#20272;&#27668;&#20505;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23613;&#31649;&#34987;&#35748;&#20026;&#26159;&#22522;&#30784;&#65292;&#22238;&#24402;&#27169;&#22411;&#22312;&#27668;&#20505;&#27169;&#25311;&#20013;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#36890;&#36807;&#21033;&#29992;&#26680;&#25216;&#24039;&#65292;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#20851;&#31995;&#24182;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#35780;&#20272;&#22312;&#19978;&#36848;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#25311;&#33021;&#21147;&#12290;&#20854;&#20013;&#65292;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22120;&#35777;&#26126;&#20102;&#23427;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#23454;&#29616;&#23454;&#26102;&#35843;&#24230;&#65292;&#24182;&#30528;&#37325;&#32771;&#34385;&#39640;&#38656;&#27714;&#36710;&#31449;&#30340;&#37325;&#26032;&#23433;&#25490;&#12290;</title><link>http://arxiv.org/abs/2308.11849</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data. (arXiv:2308.11849v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#23454;&#29616;&#23454;&#26102;&#35843;&#24230;&#65292;&#24182;&#30528;&#37325;&#32771;&#34385;&#39640;&#38656;&#27714;&#36710;&#31449;&#30340;&#37325;&#26032;&#23433;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#26159;&#19968;&#31181;&#21450;&#26102;&#28789;&#27963;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26681;&#25454;&#26102;&#21464;&#26465;&#20214;&#33258;&#21160;&#25913;&#21464;&#36816;&#33829;&#35745;&#21010;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#38081;&#36335;&#20013;&#20056;&#23458;&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#30340;&#23454;&#26102;&#27969;&#21160;&#24615;&#65292;&#20027;&#35201;&#20381;&#36182;&#22522;&#20110;OD&#30340;&#25968;&#25454;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21015;&#36710;&#30340;&#38656;&#27714;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#38271;&#26399;&#32039;&#24613;&#24773;&#20917;&#19979;&#30340;&#35843;&#24230;&#26356;&#26032;&#21407;&#21017;&#24573;&#35270;&#20102;&#38656;&#27714;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38656;&#27714;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#20419;&#36827;&#23454;&#26102;&#35843;&#24230;&#12290;&#19982;&#32593;&#32476;&#23618;&#38754;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#30528;&#37325;&#20851;&#27880;&#32039;&#24613;&#21306;&#22495;&#19978;&#28216;&#30340;&#39640;&#38656;&#27714;&#36710;&#31449;&#12290;&#30446;&#26631;&#26159;&#37325;&#26032;&#23433;&#25490;&#36890;&#36807;&#35813;&#30446;&#26631;&#31449;&#30340;&#22810;&#26465;&#32447;&#36335;&#19978;&#21463;&#21040;&#20005;&#37325;&#31361;&#21457;&#20107;&#20214;&#65288;&#22914;&#33258;&#28982;&#28798;&#23475;&#65289;&#24433;&#21709;&#30340;&#25152;&#26377;&#21015;&#36710;&#12290;&#38656;&#35201;&#29305;&#21035;&#27880;&#24847;&#36991;&#20813;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accum
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SEA&#65292;&#19968;&#31181;&#29992;&#20110;&#24402;&#22240;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#30340;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26694;&#26550;&#26469;&#29702;&#35299;&#25915;&#20987;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#24182;&#26377;&#25928;&#24402;&#22240;&#25915;&#20987;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#31532;&#20108;&#27425;&#20986;&#29616;&#30340;&#25915;&#20987;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#26088;&#22312;&#23454;&#29616;&#21462;&#35777;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#24773;&#25253;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2308.11845</link><description>&lt;p&gt;
SEA&#65306;&#21487;&#20849;&#20139;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks. (arXiv:2308.11845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SEA&#65292;&#19968;&#31181;&#29992;&#20110;&#24402;&#22240;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#30340;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26694;&#26550;&#26469;&#29702;&#35299;&#25915;&#20987;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#24182;&#26377;&#25928;&#24402;&#22240;&#25915;&#20987;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#31532;&#20108;&#27425;&#20986;&#29616;&#30340;&#25915;&#20987;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#26088;&#22312;&#23454;&#29616;&#21462;&#35777;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#24773;&#25253;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#26469;&#33258;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#30340;&#25932;&#23545;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#21162;&#21147;&#26469;&#26816;&#27979;&#21644;&#38450;&#27490;&#36825;&#20123;&#25915;&#20987;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#19968;&#31181;&#26356;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35760;&#24405;&#12289;&#20998;&#26512;&#21644;&#20998;&#20139;&#25915;&#20987;&#35777;&#25454;&#12290;&#34429;&#28982;&#32463;&#20856;&#23433;&#20840;&#39046;&#22495;&#21463;&#30410;&#20110;&#25104;&#29087;&#30340;&#21462;&#35777;&#21644;&#24773;&#25253;&#20849;&#20139;&#25216;&#26415;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#23578;&#26410;&#25214;&#21040;&#19968;&#31181;&#26041;&#24335;&#26469;&#23545;&#25915;&#20987;&#32773;&#36827;&#34892;&#30011;&#20687;&#65292;&#24182;&#20998;&#20139;&#20851;&#20110;&#20182;&#20204;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;SEA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#31995;&#32479;&#65292;&#29992;&#20110;&#20026;&#21462;&#35777;&#30446;&#30340;&#34920;&#24449;&#23545;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#24182;&#20419;&#36827;&#21487;&#35299;&#37322;&#30340;&#24773;&#25253;&#20849;&#20139;&#12290;SEA&#21033;&#29992;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26694;&#26550;&#23558;&#35266;&#23519;&#21040;&#30340;&#26597;&#35810;&#24207;&#21015;&#24402;&#22240;&#20110;&#24050;&#30693;&#30340;&#25915;&#20987;&#65292;&#22240;&#27492;&#23427;&#33021;&#22815;&#29702;&#35299;&#25915;&#20987;&#30340;&#28436;&#21464;&#36807;&#31243;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#30340;&#25932;&#23545;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;SEA&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#25915;&#20987;&#24402;&#22240;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#31532;&#20108;&#27425;&#20986;&#29616;&#30340;&#25915;&#20987;&#65292;&#20063;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, there is a need for a more comprehensive approach to logging, analyzing, and sharing evidence of attacks. While classic security benefits from well-established forensics and intelligence sharing, Machine Learning is yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages the Hidden Markov Models framework to attribute the observed query sequence to known attacks. It thus understands the attack's progression rather than just focusing on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on their second occurrence, and is robus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11842</link><description>&lt;p&gt;
&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#30028;&#20013;&#35782;&#21035;&#21644;&#20998;&#26512;&#23545;&#31216;&#27169;&#24335;&#24050;&#32463;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#20363;&#22914;&#29289;&#29702;&#23398;&#20013;&#30340;&#24341;&#21147;&#23450;&#24459;&#30340;&#21046;&#23450;&#21644;&#21270;&#23398;&#32467;&#26500;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#21033;&#29992;&#22312;&#26576;&#20123;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#20197;&#21450;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24418;&#24335;&#21270;&#22320;&#25551;&#36848;&#19968;&#31867;&#20855;&#26377;&#19968;&#33324;&#23545;&#31216;&#24615;&#27010;&#24565;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#35813;&#27010;&#24565;&#20801;&#35768;&#23384;&#22312;&#23545;&#31216;&#30340;&#26368;&#20248;&#20540;&#21644;&#31574;&#30053;&#12290;&#21463;&#21040;&#36825;&#20123;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20316;&#20026;&#22810;&#26234;&#20307;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#22312;&#20855;&#26377;&#37325;&#22797;&#23545;&#31216;&#27169;&#24335;&#30340;&#26410;&#35265;&#22330;&#26223;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#31561;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#35780;&#20272;&#30446;&#26631;&#21644;&#25351;&#26631;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;FedEval&#65292;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.11841</link><description>&lt;p&gt;
&#12298;&#32852;&#37030;&#23398;&#20064;&#35780;&#20272;&#65306;&#30446;&#26631;&#21644;&#25351;&#26631;&#30340;&#35843;&#26597;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey for Federated Learning Evaluations: Goals and Measures. (arXiv:2308.11841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#35780;&#20272;&#30446;&#26631;&#21644;&#25351;&#26631;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;FedEval&#65292;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26159;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;&#19968;&#20010;&#31995;&#32479;&#22914;&#20309;&#23454;&#29616;&#20854;&#39044;&#26399;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26032;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#19981;&#20849;&#20139;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;FL&#20855;&#26377;&#36328;&#23398;&#31185;&#24615;&#21644;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#65288;&#22914;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#65289;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#20027;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#28982;&#21518;&#25506;&#35752;&#20102;&#27599;&#20010;&#30446;&#26631;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;FedEval&#65292;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#21644;&#20840;&#38754;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#35780;&#20272;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#20854;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#35780;&#20272;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#31354;&#24179;&#22343;&#26041;&#27861;&#26469;&#30740;&#31350;&#22260;&#20135;&#26399;&#20154;&#31867;&#22823;&#33041;&#32467;&#26500;&#36830;&#25509;&#30340;&#27491;&#24120;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.11836</link><description>&lt;p&gt;
&#25551;&#36848;&#20154;&#31867;&#33041;&#32467;&#26500;&#36830;&#25509;&#30340;&#27491;&#24120;&#22260;&#20135;&#26399;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Characterizing normal perinatal development of the human brain structural connectivity. (arXiv:2308.11836v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#31354;&#24179;&#22343;&#26041;&#27861;&#26469;&#30740;&#31350;&#22260;&#20135;&#26399;&#20154;&#31867;&#22823;&#33041;&#32467;&#26500;&#36830;&#25509;&#30340;&#27491;&#24120;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#22823;&#33041;&#21457;&#32946;&#29305;&#28857;&#26159;&#39640;&#24230;&#26377;&#24207;&#30340;&#32467;&#26500;&#36830;&#25509;&#24418;&#25104;&#12290;&#36825;&#31181;&#36830;&#25509;&#30340;&#20114;&#30456;&#20851;&#24615;&#26159;&#22823;&#33041;&#35748;&#30693;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24433;&#21709;&#20854;&#23545;&#30142;&#30149;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#22312;&#22260;&#20135;&#26399;&#38454;&#27573;&#37327;&#21270;&#35780;&#20272;&#32467;&#26500;&#36830;&#25509;&#23545;&#30740;&#31350;&#27491;&#24120;&#21644;&#24322;&#24120;&#31070;&#32463;&#21457;&#32946;&#24456;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#20013;&#20272;&#35745;&#36830;&#25509;&#32452;&#38656;&#35201;&#22797;&#26434;&#30340;&#35745;&#31639;&#12290;&#23545;&#20110;&#22260;&#20135;&#26399;&#65292;&#36825;&#20123;&#35745;&#31639;&#38754;&#20020;&#24555;&#36895;&#33041;&#21457;&#23637;&#21644;&#25104;&#20687;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#21152;&#19978;&#39640;&#20010;&#20307;&#38388;&#21464;&#24322;&#24615;&#65292;&#36825;&#20123;&#22240;&#32032;&#20351;&#24471;&#25551;&#36848;&#32467;&#26500;&#36830;&#25509;&#30340;&#27491;&#24120;&#21457;&#23637;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#32570;&#20047;&#22312;&#36825;&#19968;&#20851;&#38190;&#33041;&#21457;&#32946;&#38454;&#27573;&#30340;&#32467;&#26500;&#36830;&#25509;&#24230;&#37327;&#30340;&#21487;&#38752;&#22522;&#32447;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#31354;&#24179;&#22343;&#30340;&#35745;&#31639;&#26694;&#26550;&#26469;&#30830;&#23450;&#32467;&#26500;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#29616;&#24182;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#21508;&#20010;&#21464;&#20307;&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20551;&#35774;&#26159;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.11834</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#31639;&#27861;&#21464;&#31181;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#27604;&#36739;&#19982;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection. (arXiv:2308.11834v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#29616;&#24182;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#21508;&#20010;&#21464;&#20307;&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20551;&#35774;&#26159;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#22312;&#27599;&#20010;&#29305;&#24449;&#23436;&#20840;&#29420;&#31435;&#20110;&#20854;&#20182;&#29305;&#24449;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36825;&#31181;&#24773;&#20917;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#24182;&#27604;&#36739;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#21508;&#20010;&#21464;&#20307;&#65288;&#22810;&#39033;&#24335;&#12289;&#20271;&#21162;&#21033;&#21644;&#39640;&#26031;&#65289;&#22312;&#32593;&#32476;&#20837;&#20405;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#27599;&#20010;&#21464;&#20307;&#30340;&#20551;&#35774;&#19982;&#24615;&#33021;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#27599;&#20010;&#21464;&#20307;&#37117;&#30450;&#30446;&#22320;&#36981;&#24490;&#20854;&#20551;&#35774;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#24449;&#30340;&#23646;&#24615;&#65292;&#20551;&#35774;&#26159;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20271;&#21162;&#21033;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;69.9%&#65288;&#35757;&#32451;&#38598;&#20026;71%&#65289;&#65292;&#22810;&#39033;&#24335;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;31.2%&#65288;&#35757;&#32451;&#38598;&#20026;31.2%&#65289;&#65292;&#32780;&#39640;&#26031;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;81.69%&#65288;&#35757;&#32451;&#38598;&#20026;82.84%&#65289;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#27599;&#20010;&#26420;&#32032;&#36125;&#21494;&#26031;&#21464;&#20307;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#20027;&#35201;&#21462;&#20915;&#20110;&#27599;&#20010;&#20998;&#31867;&#22120;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian classifiers perform well when each of the features is completely independent of the other which is not always valid in real world application. The aim of this study is to implement and compare the performances of each variant of Bayesian classifier (Multinomial, Bernoulli, and Gaussian) on anomaly detection in network intrusion, and to investigate whether there is any association between each variant assumption and their performance. Our investigation showed that each variant of Bayesian algorithm blindly follows its assumption regardless of feature property, and that the assumption is the single most important factor that influences their accuracy. Experimental results show that Bernoulli has accuracy of 69.9% test (71% train), Multinomial has accuracy of 31.2% test (31.2% train), while Gaussian has accuracy of 81.69% test (82.84% train). Going deeper, we investigated and found that each Naive Bayes variants performances and accuracy is largely due to each classifier assumpti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#20449;&#24687;&#28304;&#30340;&#19978;&#19979;&#25991;&#65292;&#35753;GPT&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#32771;&#35797;&#39064;&#30446;&#12290;&#22312;&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#39550;&#39542;&#25163;&#20876;&#20316;&#20026;&#20449;&#24687;&#28304;&#30340;&#27979;&#35797;&#20013;&#65292;GPT-3&#27169;&#22411;&#21462;&#24471;&#20102;96%&#30340;&#21450;&#26684;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.11827</link><description>&lt;p&gt;
&#25506;&#32034;GPT&#27169;&#22411;&#22312;&#32771;&#35797;&#20013;&#30340;&#25928;&#26524;&#65306;&#39550;&#39542;&#25191;&#29031;&#30693;&#35782;&#27979;&#35797;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test. (arXiv:2308.11827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#20449;&#24687;&#28304;&#30340;&#19978;&#19979;&#25991;&#65292;&#35753;GPT&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#32771;&#35797;&#39064;&#30446;&#12290;&#22312;&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#39550;&#39542;&#25163;&#20876;&#20316;&#20026;&#20449;&#24687;&#28304;&#30340;&#27979;&#35797;&#20013;&#65292;GPT-3&#27169;&#22411;&#21462;&#24471;&#20102;96%&#30340;&#21450;&#26684;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;Open AI&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#25797;&#38271;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#20854;&#30693;&#35782;&#20165;&#38480;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#38480;&#21046;&#20351;&#24471;&#24403;&#38754;&#20020;&#26377;&#20851;&#26368;&#26032;&#21457;&#23637;&#25110;&#38750;&#20844;&#24320;&#25991;&#20214;&#30340;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26080;&#25928;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20043;&#21069;&#26410;&#21253;&#21547;&#22312;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#28304;&#30340;&#19978;&#19979;&#25991;&#26469;&#20351;GPT&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#39044;&#22788;&#29702;&#12289;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#30340;&#23884;&#20837;&#12289;&#36890;&#36807;&#25972;&#21512;&#19978;&#19979;&#25991;&#23884;&#20837;&#26500;&#24314;&#25552;&#31034;&#20197;&#21450;&#20351;&#29992;GPT&#27169;&#22411;&#29983;&#25104;&#31572;&#26696;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#21463;&#25511;&#27979;&#35797;&#22330;&#26223;&#65292;&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#39550;&#39542;&#25163;&#20876;&#20316;&#20026;&#20449;&#24687;&#28304;&#12290;GPT-3&#27169;&#22411;&#22312;&#19968;&#22871;50&#36947;&#26679;&#26412;&#39550;&#39542;&#30693;&#35782;&#27979;&#35797;&#39064;&#19978;&#21462;&#24471;&#20102;96%&#30340;&#21450;&#26684;&#20998;&#25968;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#21450;&#26684;&#20998;&#25968;&#19979;&#38477;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to
&lt;/p&gt;</description></item><item><title>Accel-GCN&#20351;&#29992;&#36731;&#37327;&#32423;&#24230;&#25490;&#24207;&#12289;&#22359;&#32423;&#20998;&#21306;&#21644;&#32508;&#21512;warp&#31574;&#30053;&#30340;GPU&#21152;&#36895;&#22120;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#20027;&#27969;GPU&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11825</link><description>&lt;p&gt;
Accel-GCN: &#39640;&#24615;&#33021;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;GPU&#21152;&#36895;&#22120;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks. (arXiv:2308.11825v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11825
&lt;/p&gt;
&lt;p&gt;
Accel-GCN&#20351;&#29992;&#36731;&#37327;&#32423;&#24230;&#25490;&#24207;&#12289;&#22359;&#32423;&#20998;&#21306;&#21644;&#32508;&#21512;warp&#31574;&#30053;&#30340;GPU&#21152;&#36895;&#22120;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#20027;&#27969;GPU&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#20174;&#21508;&#20010;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#20027;&#27969;GPU&#19978;&#30340;&#21152;&#36895;&#21463;&#21040;&#20102;&#24037;&#20316;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20869;&#23384;&#35775;&#38382;&#19981;&#35268;&#21017;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Accel-GCN&#65292;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;GPU&#21152;&#36895;&#22120;&#26550;&#26500;&#12290;Accel-GCN&#30340;&#35774;&#35745;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24230;&#25490;&#24207;&#38454;&#27573;&#65292;&#20197;&#32452;&#21512;&#20855;&#26377;&#30456;&#20284;&#24230;&#30340;&#33410;&#28857;&#65307;&#65288;ii&#65289;&#19968;&#20010;&#22359;&#32423;&#20998;&#21306;&#31574;&#30053;&#65292;&#21160;&#24577;&#35843;&#25972;warp&#24037;&#20316;&#36127;&#36733;&#22823;&#23567;&#65292;&#22686;&#24378;&#20849;&#20139;&#20869;&#23384;&#26412;&#22320;&#24615;&#21644;&#24037;&#20316;&#36127;&#36733;&#24179;&#34913;&#65292;&#24182;&#20943;&#23569;&#19982;GNNAdvisor&#31561;&#35774;&#35745;&#30456;&#27604;&#30340;&#20803;&#25968;&#25454;&#24320;&#38144;&#65307;&#65288;iii&#65289;&#19968;&#31181;&#32508;&#21512;warp&#31574;&#30053;&#65292;&#25552;&#39640;&#22312;&#31264;&#23494;&#30697;&#38453;&#30340;&#21015;&#32500;&#24230;&#19978;&#30340;&#20869;&#23384;&#21512;&#24182;&#21644;&#35745;&#31639;&#24182;&#34892;&#24615;&#12290;&#21033;&#29992;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#38024;&#23545;GCNs&#22312;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65288;SpMM&#65289;&#20013;&#30340;&#26680;&#24515;&#36827;&#34892;&#20102;&#35774;&#35745;&#65292;&#37319;&#29992;&#22359;&#32423;&#20998;&#21306;&#21644;&#32508;&#21512;warp&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#22810;&#23618;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs) are pivotal in extracting latent information from graph data across various domains, yet their acceleration on mainstream GPUs is challenged by workload imbalance and memory access irregularity. To address these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. The design of Accel-GCN encompasses: (i) a lightweight degree sorting stage to group nodes with similar degree; (ii) a block-level partition strategy that dynamically adjusts warp workload sizes, enhancing shared memory locality and workload balance, and reducing metadata overhead compared to designs like GNNAdvisor; (iii) a combined warp strategy that improves memory coalescing and computational parallelism in the column dimension of dense matrices.  Utilizing these principles, we formulated a kernel for sparse matrix multiplication (SpMM) in GCNs that employs block-level partitioning and combined warp strategy. This approach augments performance and multi-level memor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25668;&#20687;&#22836;&#21069;&#25918;&#32622;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#34917;&#19969;&#65292;&#21363;&#21487;&#35302;&#21457;&#27169;&#22411;&#30340;&#38169;&#35823;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11822</link><description>&lt;p&gt;
PatchBackdoor: &#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification. (arXiv:2308.11822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25668;&#20687;&#22836;&#21069;&#25918;&#32622;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#34917;&#19969;&#65292;&#21363;&#21487;&#35302;&#21457;&#27169;&#22411;&#30340;&#38169;&#35823;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#26159;&#23545;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#20027;&#35201;&#23041;&#32961;&#65292;&#26088;&#22312;&#22312;&#21463;&#25915;&#20987;&#32773;&#25511;&#21046;&#30340;&#26465;&#20214;&#19979;&#35302;&#21457;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#38169;&#35823;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21518;&#38376;&#25915;&#20987;&#38656;&#35201;&#36890;&#36807;&#35757;&#32451;&#24102;&#26377;&#26377;&#27602;&#25968;&#25454;&#21644;/&#25110;&#30452;&#25509;&#20462;&#25913;&#27169;&#22411;&#26469;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26222;&#36941;&#20294;&#38169;&#35823;&#30340;&#20449;&#24565;&#65292;&#21363;&#36890;&#36807;&#27491;&#30830;&#20445;&#25252;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#36991;&#20813;&#21518;&#38376;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#38656;&#20219;&#20309;&#27169;&#22411;&#20462;&#25913;&#21363;&#21487;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#34917;&#19969;&#65288;&#31216;&#20026;&#21518;&#38376;&#34917;&#19969;&#65289;&#25918;&#32622;&#22312;&#25668;&#20687;&#22836;&#21069;&#38754;&#65292;&#19982;&#36755;&#20837;&#22270;&#20687;&#19968;&#36215;&#36755;&#20837;&#27169;&#22411;&#12290;&#35813;&#34917;&#19969;&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#26102;&#20505;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#24403;&#36755;&#20837;&#22270;&#20687;&#21253;&#21547;&#21463;&#25915;&#20987;&#32773;&#25511;&#21046;&#30340;&#35302;&#21457;&#29289;&#20307;&#26102;&#20250;&#20135;&#29983;&#38169;&#35823;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#21253;&#25324;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#29983;&#25104;&#34917;&#19969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and/or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21435;&#28151;&#28102;&#22120;&#27169;&#22411;FLMD&#65292;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20102;&#26377;&#20559;&#35265;&#30340;EHR&#20013;&#30340;&#20581;&#24247;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11819</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#28151;&#28102;&#22120;&#35299;&#20915;&#26377;&#20559;&#35265;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20581;&#24247;&#24046;&#24322;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder. (arXiv:2308.11819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11819
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21435;&#28151;&#28102;&#22120;&#27169;&#22411;FLMD&#65292;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20102;&#26377;&#20559;&#35265;&#30340;EHR&#20013;&#30340;&#20581;&#24247;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25968;&#25454;&#24314;&#27169;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#19978;&#65292;&#30001;&#20110;EHR&#30340;&#22797;&#26434;&#28508;&#22312;&#32467;&#26500;&#21644;&#28508;&#22312;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#32780;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#20445;&#25345;&#27169;&#22411;&#25972;&#20307;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#20581;&#24247;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#35266;&#23519;&#25968;&#25454;&#20043;&#22806;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20844;&#24179;&#32437;&#21521;&#21307;&#30103;&#21435;&#28151;&#28102;&#22120;&#65288;FLMD&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#24314;&#27169;&#12290;&#21463;&#21040;&#21435;&#28151;&#28102;&#22120;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;FLMD&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;FLMD&#25429;&#25417;&#20102;&#27599;&#27425;&#25509;&#35302;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#23376;&#65292;&#36825;&#26377;&#25928;&#22320;&#34920;&#31034;&#20102;&#36229;&#20986;&#35266;&#23519;&#21040;&#30340;EHR&#20043;&#22806;&#30340;&#28508;&#22312;&#21307;&#30103;&#22240;&#32032;&#65292;&#22914;&#24739;&#32773;&#22522;&#22240;&#22411;&#21644;&#29983;&#27963;&#20064;&#24815;&#12290;&#36825;&#20010;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#23376;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#29289;&#29702;&#20449;&#24687;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#24341;&#20837;&#38750;&#23616;&#37096;&#20132;&#36890;&#27969;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11818</link><description>&lt;p&gt;
&#23558;&#38750;&#23616;&#37096;&#20132;&#36890;&#27969;&#27169;&#22411;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks. (arXiv:2308.11818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29289;&#29702;&#20449;&#24687;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#24341;&#20837;&#38750;&#23616;&#37096;&#20132;&#36890;&#27969;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#38750;&#23616;&#37096;LWR&#27169;&#22411;&#22312;&#29289;&#29702;&#20449;&#24687;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20248;&#21183;&#65292;&#20026;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#32463;&#20856;&#30340;LWR&#27169;&#22411;&#22312;&#20934;&#30830;&#34920;&#31034;&#23454;&#38469;&#20132;&#36890;&#27969;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#38750;&#23616;&#37096;LWR&#27169;&#22411;&#36890;&#36807;&#23558;&#36895;&#24230;&#35270;&#20026;&#19979;&#28216;&#20132;&#36890;&#23494;&#24230;&#30340;&#21152;&#26435;&#22343;&#20540;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PIDL&#26694;&#26550;&#65292;&#23558;&#38750;&#23616;&#37096;LWR&#27169;&#22411;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22266;&#23450;&#38271;&#24230;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#21367;&#31215;&#26680;&#65292;&#24182;&#24320;&#21457;&#20102;&#25152;&#38656;&#30340;&#25968;&#23398;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;NGSIM&#21644;CitySim&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#65292;&#23545;&#25152;&#25552;&#20986;&#30340;PIDL&#26694;&#26550;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#21508;&#31181;&#21367;&#31215;&#26680;&#21644;&#21069;&#30651;&#31383;&#21475;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;&#20351;&#29992;&#23616;&#37096;LWR&#27169;&#22411;&#30340;&#22522;&#32447;PIDL&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#25552;&#39640;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research contributes to the advancement of traffic state estimation methods by leveraging the benefits of the nonlocal LWR model within a physics-informed deep learning framework. The classical LWR model, while useful, falls short of accurately representing real-world traffic flows. The nonlocal LWR model addresses this limitation by considering the speed as a weighted mean of the downstream traffic density. In this paper, we propose a novel PIDL framework that incorporates the nonlocal LWR model. We introduce both fixed-length and variable-length kernels and develop the required mathematics. The proposed PIDL framework undergoes a comprehensive evaluation, including various convolutional kernels and look-ahead windows, using data from the NGSIM and CitySim datasets. The results demonstrate improvements over the baseline PIDL approach using the local LWR model. The findings highlight the potential of the proposed approach to enhance the accuracy and reliability of traffic state es
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#27169;&#22411;&#22312;&#28023;&#27915;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#23545;&#32463;&#20856;&#27969;&#20307;&#27969;&#21160;&#21644;&#29616;&#23454;&#28023;&#27915;&#21160;&#21147;&#23398;&#36827;&#34892;&#26377;&#25928;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11814</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#27169;&#22411;&#22312;&#28023;&#27915;&#39044;&#27979;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Deep Neural Operator Models toward Ocean Forecasting. (arXiv:2308.11814v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11814
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#27169;&#22411;&#22312;&#28023;&#27915;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#23545;&#32463;&#20856;&#27969;&#20307;&#27969;&#21160;&#21644;&#29616;&#23454;&#28023;&#27915;&#21160;&#21147;&#23398;&#36827;&#34892;&#26377;&#25928;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#26694;&#26550;&#24050;&#32463;&#29992;&#20110;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#37117;&#21487;&#33021;&#26377;&#29992;&#65292;&#21253;&#25324;&#22823;&#27668;&#21644;&#28023;&#27915;&#20197;&#21450;&#27969;&#20307;&#23398;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36825;&#31181;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#27169;&#22411;&#22312;&#37325;&#29616;&#21644;&#39044;&#27979;&#32463;&#20856;&#27969;&#20307;&#27969;&#21160;&#21644;&#29616;&#23454;&#28023;&#27915;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#21487;&#33021;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#31616;&#35201;&#35780;&#20272;&#20102;&#36825;&#31181;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#27169;&#22411;&#22312;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#25311;&#30340;&#20108;&#32500;&#27969;&#20307;&#27969;&#32463;&#22278;&#26609;&#20307;&#30340;&#24773;&#20917;&#19979;&#30340;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#22823;&#35199;&#27915;&#20013;&#37096;&#21644;&#39532;&#33832;&#35832;&#22622;&#28286;&#28023;&#27915;&#34920;&#23618;&#29615;&#27969;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#20102;&#29992;&#20110;&#23454;&#38469;&#28023;&#27915;&#23454;&#39564;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#21516;&#21270;&#27169;&#25311;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30830;&#35748;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#29702;&#24819;&#21270;&#30340;&#21608;&#26399;&#24615;&#28065;&#33073;&#33853;&#12290;&#23545;&#20110;&#29616;&#23454;&#28023;&#27915;&#34920;&#23618;&#27969;&#21160;&#21644;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;
&lt;/p&gt;
&lt;p&gt;
Data-driven, deep-learning modeling frameworks have been recently developed for forecasting time series data. Such machine learning models may be useful in multiple domains including the atmospheric and oceanic ones, and in general, the larger fluids community. The present work investigates the possible effectiveness of such deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics. We first briefly evaluate the capabilities of such deep neural operator models when trained on a simulated two-dimensional fluid flow past a cylinder. We then investigate their application to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay, learning from high-resolution data-assimilative simulations employed for real sea experiments. We confirm that trained deep neural operator models are capable of predicting idealized periodic eddy shedding. For realistic ocean surface flows and our preliminary study,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.11804</link><description>&lt;p&gt;
&#36825;&#19981;&#26159;&#19968;&#20010;&#33529;&#26524;&#65306;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#22270;&#20687;&#12289;&#22768;&#38899;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#31561;&#26144;&#23556;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#23558;&#19968;&#24352;&#29399;&#30340;&#22270;&#20687;&#19982;&#19968;&#31181;&#21483;&#22768;&#30456;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#21487;&#20197;&#21463;&#21040;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#23545;&#25239;&#24187;&#35273;&#8221;&#30340;&#25915;&#20987;&#12290;&#32473;&#23450;&#20219;&#24847;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#23427;&#65292;&#20351;&#20854;&#23884;&#20837;&#25509;&#36817;&#20110;&#21478;&#19968;&#27169;&#24577;&#20013;&#20219;&#24847;&#23545;&#25163;&#36873;&#25321;&#30340;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#24187;&#35273;&#20351;&#23545;&#25163;&#33021;&#22815;&#23558;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#31561;&#36827;&#34892;&#23545;&#40784;&#12290;&#23545;&#25239;&#24187;&#35273;&#21033;&#29992;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#25509;&#36817;&#24615;&#65292;&#22240;&#27492;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#12290;&#20351;&#29992;ImageBind&#23884;&#20837;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#27809;&#26377;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#40784;&#30340;&#36755;&#20837;&#22914;&#20309;&#35823;&#23548;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#38646;&#26679;&#20363;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#36890;&#36807;&#20256;&#25773;&#20998;&#24067;&#30340;&#21069;&#20004;&#20010;&#30697;&#26469;&#20248;&#21270;&#38381;&#24335;ELBO&#30446;&#26631;&#65292;&#20174;&#32780;&#36817;&#20284;&#39044;&#27979;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#28040;&#38500;&#20102;Monte Carlo&#37319;&#26679;&#30340;&#38656;&#35201;&#65292;&#26377;&#25928;&#24809;&#32602;&#27169;&#22411;&#20284;&#28982;&#24615;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.11801</link><description>&lt;p&gt;
&#21464;&#20998;&#23494;&#24230;&#20256;&#25773;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variational Density Propagation Continual Learning. (arXiv:2308.11801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#36890;&#36807;&#20256;&#25773;&#20998;&#24067;&#30340;&#21069;&#20004;&#20010;&#30697;&#26469;&#20248;&#21270;&#38381;&#24335;ELBO&#30446;&#26631;&#65292;&#20174;&#32780;&#36817;&#20284;&#39044;&#27979;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#28040;&#38500;&#20102;Monte Carlo&#37319;&#26679;&#30340;&#38656;&#35201;&#65292;&#26377;&#25928;&#24809;&#32602;&#27169;&#22411;&#20284;&#28982;&#24615;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#32463;&#24120;&#38754;&#20020;&#26469;&#33258;&#20998;&#24067;&#22806;&#65288;OoD&#65289;&#25968;&#25454;&#12289;&#21508;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#21644;&#27010;&#24565;&#30446;&#26631;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#24212;&#22522;&#20934;&#36830;&#32493;&#23398;&#20064;&#25968;&#25454;&#38598;&#24314;&#27169;&#30340;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#28040;&#38500;&#27169;&#22411;&#26435;&#37325;&#30340;Monte Carlo&#37319;&#26679;&#26469;&#37319;&#26679;&#39044;&#27979;&#20998;&#24067;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25152;&#26377;&#32593;&#32476;&#23618;&#20013;&#20256;&#25773;&#20998;&#24067;&#30340;&#21069;&#20004;&#20010;&#30697;&#65288;&#21363;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65289;&#26469;&#20248;&#21270;&#38381;&#24335;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#30446;&#26631;&#65292;&#20174;&#32780;&#36817;&#20284;&#39044;&#27979;&#20998;&#24067;&#12290;&#36890;&#36807;&#20351;&#29992;&#38381;&#24335;ELBO&#26469;&#36817;&#20284;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#65288;MDL&#65289;&#21407;&#21017;&#26469;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20174;&#32780;&#24809;&#32602;&#27169;&#22411;&#20284;&#28982;&#24615;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) deployed to the real world are regularly subject to out-of-distribution (OoD) data, various types of noise, and shifting conceptual objectives. This paper proposes a framework for adapting to data distribution drift modeled by benchmark Continual Learning datasets. We develop and evaluate a method of Continual Learning that leverages uncertainty quantification from Bayesian Inference to mitigate catastrophic forgetting. We expand on previous approaches by removing the need for Monte Carlo sampling of the model weights to sample the predictive distribution. We optimize a closed-form Evidence Lower Bound (ELBO) objective approximating the predictive distribution by propagating the first two moments of a distribution, i.e. mean and covariance, through all network layers. Catastrophic forgetting is mitigated by using the closed-form ELBO to approximate the Minimum Description Length (MDL) Principle, inherently penalizing changes in the model likelihood by minimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22797;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#38899;&#39057;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#24133;&#24230;&#35889;&#22270;&#21644;&#21407;&#22987;&#29305;&#24449;&#30340;&#20248;&#28857;&#65292;&#20445;&#30041;&#20102;&#30456;&#20301;&#20449;&#24687;&#65292;&#24182;&#19988;&#20801;&#35768;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;AI&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21453;&#27450;&#35784;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#35299;&#37322;&#24615;AI&#35299;&#37322;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11800</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22768;&#38899;&#21453;&#27450;&#35784;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Complex-valued neural networks for voice anti-spoofing. (arXiv:2308.11800v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22797;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#38899;&#39057;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#24133;&#24230;&#35889;&#22270;&#21644;&#21407;&#22987;&#29305;&#24449;&#30340;&#20248;&#28857;&#65292;&#20445;&#30041;&#20102;&#30456;&#20301;&#20449;&#24687;&#65292;&#24182;&#19988;&#20801;&#35768;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;AI&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21453;&#27450;&#35784;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#35299;&#37322;&#24615;AI&#35299;&#37322;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#21453;&#27450;&#35784;&#21644;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31995;&#32479;&#20351;&#29992;&#22522;&#20110;&#24133;&#24230;&#35889;&#22270;&#30340;&#29305;&#24449;&#65288;&#22914;CQT&#25110;Melspectrograms&#65289;&#25110;&#32463;&#36807;&#21367;&#31215;&#25110;sinc&#23618;&#22788;&#29702;&#30340;&#21407;&#22987;&#38899;&#39057;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#32570;&#28857;&#65306;&#24133;&#24230;&#35889;&#22270;&#20002;&#22833;&#30456;&#20301;&#20449;&#24687;&#65292;&#24433;&#21709;&#38899;&#39057;&#30340;&#33258;&#28982;&#24615;&#65292;&#32780;&#22522;&#20110;&#21407;&#22987;&#29305;&#24449;&#30340;&#27169;&#22411;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;AI&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22797;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#36755;&#20837;&#38899;&#39057;&#30340;&#22797;&#25968;&#20540;CQT&#39057;&#22495;&#34920;&#31034;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#30041;&#20102;&#30456;&#20301;&#20449;&#24687;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;AI&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#8220;In-the-Wild&#8221;&#21453;&#27450;&#35784;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;AI&#21487;&#20197;&#35299;&#37322;&#32467;&#26524;&#12290;&#28040;&#34701;&#30740;&#31350;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#24050;&#32463;&#23398;&#20250;&#20102;&#20351;&#29992;&#30456;&#20301;&#20449;&#24687;&#26469;&#26816;&#27979;&#22768;&#38899;&#20266;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the "In-the-Wild" anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.
&lt;/p&gt;</description></item><item><title>Karasu&#26159;&#19968;&#31181;&#36890;&#36807;&#20419;&#36827;&#31867;&#20284;&#22522;&#30784;&#35774;&#26045;&#12289;&#26694;&#26550;&#12289;&#31639;&#27861;&#25110;&#25968;&#25454;&#38598;&#19978;&#24037;&#20316;&#30340;&#29992;&#25143;&#20043;&#38388;&#30340;&#25968;&#25454;&#20849;&#20139;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36164;&#28304;&#37197;&#32622;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11792</link><description>&lt;p&gt;
Karasu:&#19968;&#31181;&#29992;&#20110;&#22823;&#25968;&#25454;&#20998;&#26512;&#30340;&#39640;&#25928;&#38598;&#32676;&#37197;&#32622;&#30340;&#21327;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics. (arXiv:2308.11792v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11792
&lt;/p&gt;
&lt;p&gt;
Karasu&#26159;&#19968;&#31181;&#36890;&#36807;&#20419;&#36827;&#31867;&#20284;&#22522;&#30784;&#35774;&#26045;&#12289;&#26694;&#26550;&#12289;&#31639;&#27861;&#25110;&#25968;&#25454;&#38598;&#19978;&#24037;&#20316;&#30340;&#29992;&#25143;&#20043;&#38388;&#30340;&#25968;&#25454;&#20849;&#20139;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36164;&#28304;&#37197;&#32622;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#31867;&#22411;&#21644;&#38598;&#32676;&#35268;&#27169;&#31561;&#37197;&#32622;&#36873;&#39033;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#65292;&#36873;&#25321;&#36866;&#21512;&#22823;&#25968;&#25454;&#20998;&#26512;&#20316;&#19994;&#30340;&#27491;&#30830;&#36164;&#28304;&#26159;&#22256;&#38590;&#30340;&#12290;&#30001;&#20110;&#31967;&#31957;&#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#36164;&#28304;&#25928;&#29575;&#12289;&#25104;&#26412;&#21644;&#33021;&#28304;&#20351;&#29992;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#37325;&#22797;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#65292;&#20197;&#23547;&#25214;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#20110;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#36153;&#26102;&#19988;&#26114;&#36149;&#30340;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#29992;&#25143;&#30340;&#22823;&#25968;&#25454;&#20998;&#26512;&#20316;&#19994;&#21487;&#20197;&#20849;&#20139;&#35768;&#22810;&#20849;&#21516;&#29305;&#24615;&#65306;&#23427;&#20204;&#36890;&#24120;&#22312;&#31867;&#20284;&#30340;&#22522;&#30784;&#35774;&#26045;&#19978;&#25805;&#20316;&#65292;&#20351;&#29992;&#31867;&#20284;&#30340;&#31639;&#27861;&#22312;&#31867;&#20284;&#30340;&#26694;&#26550;&#20013;&#23454;&#29616;&#12290;&#20849;&#20139;&#32858;&#21512;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#36816;&#34892;&#20197;&#21327;&#20316;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#28508;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Karasu&#65292;&#19968;&#31181;&#20419;&#36827;&#22312;&#31867;&#20284;&#22522;&#30784;&#35774;&#26045;&#12289;&#26694;&#26550;&#12289;&#31639;&#27861;&#25110;&#25968;&#25454;&#38598;&#19978;&#24037;&#20316;&#30340;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#30340;&#26356;&#39640;&#25928;&#30340;&#36164;&#28304;&#37197;&#32622;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the right resources for big data analytics jobs is hard because of the wide variety of configuration options like machine type and cluster size. As poor choices can have a significant impact on resource efficiency, cost, and energy usage, automated approaches are gaining popularity. Most existing methods rely on profiling recurring workloads to find near-optimal solutions over time. Due to the cold-start problem, this often leads to lengthy and costly profiling phases. However, big data analytics jobs across users can share many common properties: they often operate on similar infrastructure, using similar algorithms implemented in similar frameworks. The potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored.  We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu tr
&lt;/p&gt;</description></item><item><title>HypBO&#26159;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#36125;&#21494;&#26031;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26679;&#26412;&#31181;&#23376;&#26469;&#26356;&#24555;&#22320;&#25214;&#21040;&#26377;&#24076;&#26395;&#30340;&#21270;&#23398;&#31354;&#38388;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11787</link><description>&lt;p&gt;
HypBO: &#19987;&#23478;&#24341;&#23548;&#19979;&#30340;&#21270;&#23398;&#23478;&#21442;&#19982;&#30340;&#36125;&#21494;&#26031;&#25628;&#32034;&#26032;&#26448;&#26009;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials. (arXiv:2308.11787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11787
&lt;/p&gt;
&lt;p&gt;
HypBO&#26159;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#36125;&#21494;&#26031;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26679;&#26412;&#31181;&#23376;&#26469;&#26356;&#24555;&#22320;&#25214;&#21040;&#26377;&#24076;&#26395;&#30340;&#21270;&#23398;&#31354;&#38388;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#21270;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#35299;&#20915;&#26448;&#26009;&#21457;&#29616;&#31561;&#38590;&#20197;&#35299;&#20915;&#30340;&#22810;&#21464;&#37327;&#31185;&#23398;&#38382;&#39064;&#65292;&#20294;&#21487;&#29992;&#30340;&#25628;&#32034;&#31354;&#38388;&#21487;&#33021;&#38750;&#24120;&#24222;&#22823;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#26679;&#26412;&#39640;&#25928;&#20248;&#21270;&#24341;&#25806;&#65292;&#22312;&#27809;&#26377;&#30446;&#26631;&#20989;&#25968;&#25110;&#23646;&#24615;&#30340;&#35299;&#26512;&#24418;&#24335;&#34987;&#30693;&#36947;&#30340;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#30693;&#35782;&#20197;&#20551;&#35774;&#30340;&#24418;&#24335;&#65292;&#26356;&#24555;&#22320;&#23558;&#36125;&#21494;&#26031;&#25628;&#32034;&#24341;&#23548;&#21040;&#26377;&#24076;&#26395;&#30340;&#21270;&#23398;&#31354;&#38388;&#21306;&#22495;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#29616;&#26377;&#23454;&#39564;&#27979;&#37327;&#24471;&#21040;&#30340;&#28508;&#22312;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#26032;&#30340;&#26410;&#24320;&#21457;&#30340;&#31185;&#23398;&#20219;&#21153;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#20998;&#24067;&#26080;&#27861;&#25429;&#25417;&#31934;&#32454;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;HypBO&#65292;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#20551;&#35774;&#29983;&#25104;&#25913;&#36827;&#30340;&#26679;&#26412;&#31181;&#23376;&#12290;&#19981;&#22826;&#26377;&#24076;&#26395;&#30340;&#31181;&#23376;&#33258;&#21160;&#25240;&#25187;&#65292;&#32780;&#26377;&#24076;&#26395;&#30340;&#31181;&#23376;&#29992;&#20110;&#22686;&#21152;&#20195;&#29702;&#27169;&#22411;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20855;&#20449;&#24687;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformers&#36827;&#34892;&#22810;&#22330;&#26223;&#31934;&#30830;&#30456;&#26426;&#23039;&#24577;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#32858;&#21512;&#28608;&#27963;&#22270;&#21644;&#35299;&#30721;&#28508;&#22312;&#29305;&#24449;&#26469;&#23454;&#29616;&#23545;&#36890;&#29992;&#29305;&#24449;&#30340;&#23450;&#20301;&#65292;&#24182;&#22312;&#22810;&#20010;&#22330;&#26223;&#20013;&#24182;&#34892;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.11783</link><description>&lt;p&gt;
&#29992;Transformers&#36827;&#34892;&#31895;&#21040;&#31934;&#22810;&#22330;&#26223;&#23039;&#24577;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Coarse-to-Fine Multi-Scene Pose Regression with Transformers. (arXiv:2308.11783v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformers&#36827;&#34892;&#22810;&#22330;&#26223;&#31934;&#30830;&#30456;&#26426;&#23039;&#24577;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#32858;&#21512;&#28608;&#27963;&#22270;&#21644;&#35299;&#30721;&#28508;&#22312;&#29305;&#24449;&#26469;&#23454;&#29616;&#23545;&#36890;&#29992;&#29305;&#24449;&#30340;&#23450;&#20301;&#65292;&#24182;&#22312;&#22810;&#20010;&#22330;&#26223;&#20013;&#24182;&#34892;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#23545;&#30456;&#26426;&#23039;&#24577;&#22238;&#24402;&#22120;&#26681;&#25454;&#20165;&#38752;&#22270;&#20687;&#36827;&#34892;&#20272;&#35745;&#65292;&#32473;&#20986;&#30456;&#26426;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#21367;&#31215;&#39592;&#24178;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#22836;&#37096;&#65292;&#36890;&#36807;&#22270;&#20687;&#21644;&#23039;&#24577;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#19968;&#27425;&#20165;&#23884;&#20837;&#19968;&#20010;&#21442;&#32771;&#22330;&#26223;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#29992;&#19968;&#32452;&#20840;&#36830;&#25509;&#23618;&#26367;&#25442;MLP&#22836;&#37096;&#65292;&#23558;&#27492;&#26041;&#26696;&#25193;&#23637;&#21040;&#23398;&#20064;&#22810;&#20010;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;Transformers&#23398;&#20064;&#22810;&#22330;&#26223;&#32477;&#23545;&#30456;&#26426;&#23039;&#24577;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#29992;&#20110;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#32858;&#21512;&#28608;&#27963;&#22270;&#65292;&#24182;&#19988;&#35299;&#30721;&#22120;&#23558;&#28508;&#22312;&#29305;&#24449;&#21644;&#22330;&#26223;&#32534;&#30721;&#36716;&#21270;&#20026;&#23039;&#24577;&#39044;&#27979;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#19987;&#27880;&#20110;&#23545;&#23450;&#20301;&#26377;&#20449;&#24687;&#37327;&#30340;&#36890;&#29992;&#29305;&#24449;&#65292;&#21516;&#26102;&#24182;&#34892;&#23884;&#20837;&#22810;&#20010;&#22330;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#28151;&#21512;&#20998;&#31867;-&#22238;&#24402;&#26550;&#26500;&#25913;&#36827;&#20102;&#25105;&#20204;&#20808;&#21069;&#30340;MS-Transformer&#26041;&#27861;\cite{shavit2021learning}&#65292;&#25552;&#39640;&#20102;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly b
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#23450;&#24615;&#25968;&#25454;&#34701;&#20837;&#21040;&#22240;&#26524;&#20272;&#35745;&#30340;&#23450;&#37327;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#23558;&#35266;&#23519;&#21040;&#30340;&#33539;&#30068;&#23884;&#20837;&#21040;&#28508;&#22312;&#30340;Baire&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#36830;&#32493;&#30340;&#32447;&#24615;&#26144;&#23556;&#65292;&#36716;&#21270;&#23450;&#24615;&#21464;&#37327;&#30340;&#22788;&#29702;&#20026;RKHS&#20013;&#30340;&#35782;&#21035;&#32467;&#26500;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20272;&#35745;&#30340;&#36807;&#31243;</title><link>http://arxiv.org/abs/2308.11781</link><description>&lt;p&gt;
&#22788;&#29702;&#21160;&#24577;&#21644;&#31232;&#30095;&#30340;&#23450;&#24615;&#25968;&#25454;&#65306;&#33539;&#30068;&#21464;&#37327;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables. (arXiv:2308.11781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11781
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#23450;&#24615;&#25968;&#25454;&#34701;&#20837;&#21040;&#22240;&#26524;&#20272;&#35745;&#30340;&#23450;&#37327;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#23558;&#35266;&#23519;&#21040;&#30340;&#33539;&#30068;&#23884;&#20837;&#21040;&#28508;&#22312;&#30340;Baire&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#36830;&#32493;&#30340;&#32447;&#24615;&#26144;&#23556;&#65292;&#36716;&#21270;&#23450;&#24615;&#21464;&#37327;&#30340;&#22788;&#29702;&#20026;RKHS&#20013;&#30340;&#35782;&#21035;&#32467;&#26500;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20272;&#35745;&#30340;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#23450;&#24615;&#25968;&#25454;&#34701;&#20837;&#21040;&#22240;&#26524;&#20272;&#35745;&#30340;&#23450;&#37327;&#27169;&#22411;&#20013;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#23450;&#24615;&#25968;&#25454;&#23548;&#20986;&#30340;&#33539;&#30068;&#21464;&#37327;&#26469;&#26500;&#24314;&#23450;&#37327;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23450;&#24615;&#20449;&#24687;&#26159;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#30340;&#33539;&#30068;&#65292;&#24182;&#20135;&#29983;&#19981;&#19968;&#33268;&#65288;&#28176;&#36817;&#20559;&#32622;&#65289;&#21644;&#19981;&#31934;&#30830;&#65288;&#26377;&#38480;&#26679;&#26412;&#20559;&#32622;&#65289;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#20351;&#29992;&#20989;&#25968;&#20998;&#26512;&#21019;&#24314;&#19968;&#20010;&#26356;&#32454;&#33268;&#21644;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#35266;&#23519;&#21040;&#30340;&#33539;&#30068;&#23884;&#20837;&#21040;&#28508;&#22312;&#30340;Baire&#31354;&#38388;&#20013;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#36830;&#32493;&#30340;&#32447;&#24615;&#26144;&#23556;&#8212;&#8212;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23884;&#20837;&#8212;&#8212;&#20174;&#33539;&#30068;&#30340;Baire&#31354;&#38388;&#21040;&#34920;&#31034;&#20989;&#25968;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#12290;&#36890;&#36807;Riesz&#34920;&#31034;&#23450;&#29702;&#65292;&#25105;&#20204;&#35777;&#26126;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#33539;&#30068;&#21464;&#37327;&#30340;&#32463;&#20856;&#22788;&#29702;&#21487;&#20197;&#36716;&#21270;&#20026;RKHS&#20013;&#30340;&#19968;&#20010;&#24050;&#35782;&#21035;&#32467;&#26500;&#12290;&#36716;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#20010;&#20652;&#21270;&#21058;&#65292;&#31616;&#21270;&#20102;&#20272;&#35745;&#8212;&#8212;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for incorporating qualitative data into quantitative models for causal estimation. Previous methods use categorical variables derived from qualitative data to build quantitative models. However, this approach can lead to data-sparse categories and yield inconsistent (asymptotically biased) and imprecise (finite sample biased) estimates if the qualitative information is dynamic and intricate. We use functional analysis to create a more nuanced and flexible framework. We embed the observed categories into a latent Baire space and introduce a continuous linear map -- a Hilbert space embedding -- from the Baire space of categories to a Reproducing Kernel Hilbert Space (RKHS) of representation functions. Through the Riesz representation theorem, we establish that the canonical treatment of categorical variables in causal models can be transformed into an identified structure in the RKHS. Transfer learning acts as a catalyst to streamline estimation -- embeddings
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26694;&#26550;FATE&#65292;&#23427;&#36890;&#36807;&#31163;&#32676;&#23398;&#20064;&#26126;&#30830;&#22320;&#23398;&#20064;&#25991;&#26412;&#20013;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#24182;&#21033;&#29992;&#20808;&#21069;&#24050;&#30693;&#30340;&#23569;&#37327;&#24322;&#24120;&#31034;&#20363;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#20248;&#21270;&#20102;&#24322;&#24120;&#24471;&#20998;&#30340;&#31934;&#30830;&#24230;&#21644;&#25968;&#25454;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11780</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#20351;&#29992;&#31163;&#32676;&#23398;&#20064;&#36827;&#34892;&#23569;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Anomaly Detection in Text with Deviation Learning. (arXiv:2308.11780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26694;&#26550;FATE&#65292;&#23427;&#36890;&#36807;&#31163;&#32676;&#23398;&#20064;&#26126;&#30830;&#22320;&#23398;&#20064;&#25991;&#26412;&#20013;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#24182;&#21033;&#29992;&#20808;&#21069;&#24050;&#30693;&#30340;&#23569;&#37327;&#24322;&#24120;&#31034;&#20363;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#20248;&#21270;&#20102;&#24322;&#24120;&#24471;&#20998;&#30340;&#31934;&#30830;&#24230;&#21644;&#25968;&#25454;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#25991;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#26500;&#24314;&#20165;&#20381;&#36182;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27169;&#22411;&#19978;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#27809;&#26377;&#21487;&#29992;&#30340;&#26631;&#35760;&#24322;&#24120;&#31034;&#20363;&#30340;&#20551;&#35774;&#36816;&#34892;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#24322;&#24120;&#36890;&#24120;&#20197;&#23567;&#25968;&#37327;&#23384;&#22312;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#21033;&#29992;&#20808;&#21069;&#24050;&#30693;&#30340;&#24322;&#24120;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#26356;&#27880;&#37325;&#23398;&#20064;&#29305;&#24449;&#23884;&#20837;&#32780;&#19981;&#26159;&#30452;&#25509;&#20248;&#21270;&#24322;&#24120;&#24471;&#20998;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24322;&#24120;&#24471;&#20998;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#25968;&#25454;&#30340;&#20302;&#25928;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FATE&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26377;&#38480;&#30340;&#24322;&#24120;&#31034;&#20363;&#65292;&#24182;&#20351;&#29992;&#31163;&#32676;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26126;&#30830;&#22320;&#23398;&#20064;&#24322;&#24120;&#24471;&#20998;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#23558;&#27491;&#24120;&#31034;&#20363;&#30340;&#24322;&#24120;&#24471;&#20998;&#35843;&#25972;&#20026;&#19982;&#20808;&#21069;&#20998;&#24067;&#33719;&#24471;&#30340;&#21442;&#32771;&#24471;&#20998;&#30456;&#20284;&#12290;&#30456;&#21453;&#65292;&#24322;&#24120;&#26679;&#26412;&#34987;&#36843;&#20855;&#26377;&#26126;&#26174;&#20559;&#31163;&#30340;&#24322;&#24120;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably devi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#22120;&#30340;Hessian&#30697;&#38453;&#21644;&#26799;&#24230;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#20102;&#36328;&#39046;&#22495;&#30340;Hessian&#30697;&#38453;&#20043;&#38388;&#30340;&#35889;&#33539;&#25968;&#26159;&#20256;&#36755;&#24230;&#37327;&#30340;&#19978;&#30028;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#40723;&#21169;Hessian&#21644;&#26799;&#24230;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26102;&#30340;&#25152;&#26377;&#23545;&#40784;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11778</link><description>&lt;p&gt;
&#29702;&#35299;Hessian&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Understanding Hessian Alignment for Domain Generalization. (arXiv:2308.11778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#22120;&#30340;Hessian&#30697;&#38453;&#21644;&#26799;&#24230;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#20102;&#36328;&#39046;&#22495;&#30340;Hessian&#30697;&#38453;&#20043;&#38388;&#30340;&#35889;&#33539;&#25968;&#26159;&#20256;&#36755;&#24230;&#37327;&#30340;&#19978;&#30028;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#40723;&#21169;Hessian&#21644;&#26799;&#24230;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26102;&#30340;&#25152;&#26377;&#23545;&#40784;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#21644;&#33258;&#21160;&#39550;&#39542;&#65292;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25913;&#36827;OOD&#27867;&#21270;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#27491;&#21017;&#21270;&#22120;&#19982;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#26679;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#23545;Hessian&#21644;&#26799;&#24230;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#20316;&#29992;&#30340;&#35748;&#35782;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#30340;OOD&#36716;&#31227;&#24615;&#29702;&#35770;&#20998;&#26512;&#20102;&#20998;&#31867;&#22120;&#22836;&#37096;Hessian&#30697;&#38453;&#21644;&#26799;&#24230;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36328;&#39046;&#22495;&#30340;&#20998;&#31867;&#22120;&#22836;&#37096;Hessian&#30697;&#38453;&#20043;&#38388;&#30340;&#35889;&#33539;&#25968;&#26159;&#20256;&#36755;&#24230;&#37327;&#30340;&#19978;&#30028;&#65292;&#20256;&#36755;&#24230;&#37327;&#26159;&#30446;&#26631;&#39046;&#22495;&#21644;&#28304;&#39046;&#22495;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#40723;&#21169;Hessian&#21644;&#26799;&#24230;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26102;&#25152;&#26377;&#30340;&#23545;&#40784;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;3ET&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22522;&#20110;&#21464;&#21270;&#30340;ConvLSTM&#32593;&#32476;&#30340;&#39640;&#25928;&#20107;&#20214;&#39537;&#21160;&#30524;&#29699;&#36861;&#36394;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20107;&#20214;&#30456;&#26426;&#30340;&#20248;&#21183;&#65292;&#22312;&#26631;&#35760;&#30340;&#30643;&#23380;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26102;&#31354;&#29305;&#24449;&#25552;&#21462;&#21644;&#20934;&#30830;&#30340;&#30643;&#23380;&#36861;&#36394;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2308.11771</link><description>&lt;p&gt;
3ET: &#20351;&#29992;&#22522;&#20110;&#21464;&#21270;&#30340;ConvLSTM&#32593;&#32476;&#30340;&#39640;&#25928;&#20107;&#20214;&#39537;&#21160;&#30524;&#29699;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network. (arXiv:2308.11771v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;3ET&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22522;&#20110;&#21464;&#21270;&#30340;ConvLSTM&#32593;&#32476;&#30340;&#39640;&#25928;&#20107;&#20214;&#39537;&#21160;&#30524;&#29699;&#36861;&#36394;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20107;&#20214;&#30456;&#26426;&#30340;&#20248;&#21183;&#65292;&#22312;&#26631;&#35760;&#30340;&#30643;&#23380;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26102;&#31354;&#29305;&#24449;&#25552;&#21462;&#21644;&#20934;&#30830;&#30340;&#30643;&#23380;&#36861;&#36394;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#22522;&#20110;&#21464;&#21270;&#30340;Convolutional Long Short-Term Memory (CB-ConvLSTM) &#27169;&#22411;&#65292;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#30524;&#29699;&#36861;&#36394;&#65292;&#36825;&#26159;&#19979;&#19968;&#20195;&#21487;&#31359;&#25140;&#21307;&#30103;&#25216;&#26415;&#65288;&#22914;AR/VR&#22836;&#30420;&#65289;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#32593;&#33180;&#28789;&#24863;&#30340;&#20107;&#20214;&#30456;&#26426;&#30340;&#20302;&#24310;&#36831;&#21709;&#24212;&#21644;&#31232;&#30095;&#36755;&#20986;&#20107;&#20214;&#27969;&#30340;&#20248;&#21183;&#65292;&#36229;&#36807;&#20256;&#32479;&#30340;&#22522;&#20110;&#24103;&#30340;&#30456;&#26426;&#12290;&#25105;&#20204;&#30340;CB-ConvLSTM&#26550;&#26500;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#20107;&#20214;&#27969;&#20013;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#65292;&#29992;&#20110;&#30643;&#23380;&#36861;&#36394;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;CNN&#32467;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#22686;&#24378;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#22686;&#37327;&#32534;&#30721;&#24490;&#29615;&#36335;&#24452;&#65292;CB-ConvLSTM&#22312;&#32463;&#36807;&#26631;&#35760;&#30340;&#30643;&#23380;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#31639;&#26415;&#25805;&#20316;&#20943;&#23569;&#20102;&#32422;4.7&#20493;&#12290;&#36825;&#31181;&#25552;&#39640;&#25928;&#29575;&#30340;&#22686;&#21152;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#30524;&#29699;&#36861;&#36394;&#12290;&#39033;&#30446;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;\url{https://github.com/qinche106/cb-convlstm-eyetracking}&#20013;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{https://github.com/qinche106/cb-convlstm-eyetracking}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#20154;&#37197;&#32622;&#30340;&#24739;&#32773;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#21644;&#25968;&#23383;&#20132;&#20114;&#25968;&#25454;&#26500;&#24314;&#24739;&#32773;&#37197;&#32622;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#32858;&#31867;&#24615;&#33021;&#21644;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11748</link><description>&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#20020;&#24202;&#21644;&#25968;&#23383;&#25968;&#25454;&#36827;&#34892;&#24739;&#32773;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Patient Clustering via Integrated Profiling of Clinical and Digital Data. (arXiv:2308.11748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#20154;&#37197;&#32622;&#30340;&#24739;&#32773;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#21644;&#25968;&#23383;&#20132;&#20114;&#25968;&#25454;&#26500;&#24314;&#24739;&#32773;&#37197;&#32622;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#32858;&#31867;&#24615;&#33021;&#21644;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#20010;&#20154;&#37197;&#32622;&#30340;&#24739;&#32773;&#32858;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#20020;&#24202;&#25968;&#25454;&#35774;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#32422;&#26463;&#30340;&#20302;&#31209;&#36817;&#20284;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#24739;&#32773;&#30340;&#20020;&#24202;&#25968;&#25454;&#21644;&#25968;&#23383;&#20132;&#20114;&#25968;&#25454;&#65288;&#21253;&#25324;&#27983;&#35272;&#21644;&#25628;&#32034;&#65289;&#26469;&#26500;&#24314;&#24739;&#32773;&#37197;&#32622;&#12290;&#20316;&#20026;&#35813;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#29983;&#25104;&#20102;&#38750;&#36127;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#20316;&#20026;&#24739;&#32773;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21307;&#30103;&#20445;&#20581;&#32593;&#31449;&#30340;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#32858;&#31867;&#21644;&#25512;&#33616;&#33021;&#21147;&#12290;&#19982;&#20854;&#20182;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#19968;&#33268;&#24615;&#21644;&#25512;&#33616;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients' clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.
&lt;/p&gt;</description></item><item><title>Animal3D&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21754;&#20083;&#21160;&#29289;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25163;&#24037;&#26631;&#27880;&#21644;&#26816;&#26597;&#30830;&#20445;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;Animal3D&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#24418;&#29366;&#21644;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20351;&#29992;&#21482;&#26377;Animal3D&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#12289;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#21644;&#24494;&#35843;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11737</link><description>&lt;p&gt;
Animal3D:&#19968;&#20221;&#20840;&#38754;&#30340;3D&#21160;&#29289;&#23039;&#24577;&#21644;&#24418;&#29366;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape. (arXiv:2308.11737v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11737
&lt;/p&gt;
&lt;p&gt;
Animal3D&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21754;&#20083;&#21160;&#29289;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25163;&#24037;&#26631;&#27880;&#21644;&#26816;&#26597;&#30830;&#20445;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;Animal3D&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#24418;&#29366;&#21644;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20351;&#29992;&#21482;&#26377;Animal3D&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#12289;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#21644;&#24494;&#35843;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#26159;&#29702;&#35299;&#21160;&#29289;&#34892;&#20026;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#23545;&#20110;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#31561;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#26377;&#28508;&#22312;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#32570;&#20047;&#21253;&#21547;&#39640;&#36136;&#37327;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#27880;&#37322;&#30340;&#20840;&#38754;&#22810;&#26679;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Animal3D&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21754;&#20083;&#21160;&#29289;&#21160;&#29289;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#20272;&#35745;&#30340;&#39318;&#20010;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;Animal3D&#30001;&#26469;&#33258;40&#20010;&#21754;&#20083;&#21160;&#29289;&#29289;&#31181;&#30340;3379&#20010;&#22270;&#20687;&#32452;&#25104;&#65292;&#21253;&#21547;26&#20010;&#20851;&#38190;&#28857;&#30340;&#39640;&#36136;&#37327;&#27880;&#37322;&#65292;&#20197;&#21450;SMAL&#27169;&#22411;&#30340;&#23039;&#24577;&#21644;&#24418;&#29366;&#21442;&#25968;&#12290;&#25152;&#26377;&#27880;&#37322;&#37117;&#32463;&#36807;&#22810;&#38454;&#27573;&#25163;&#24037;&#26631;&#27880;&#21644;&#26816;&#26597;&#65292;&#20197;&#30830;&#20445;&#26368;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#22522;&#20110;Animal3D&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20195;&#34920;&#24615;&#30340;&#24418;&#29366;&#21644;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#20102;&#20197;&#19979;&#22522;&#20934;&#27979;&#35797;&#65306;&#65288;1&#65289;&#21482;&#20351;&#29992;Animal3D&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#65288;2&#65289;&#20174;&#21512;&#25104;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65292;&#65288;3&#65289;&#24494;&#35843;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20154;&#24037;&#21644;&#31639;&#27861;&#21512;&#20316;&#65292;&#23545;&#20110;&#22810;&#20010;&#22122;&#38899;&#27169;&#22411;&#26469;&#35828;&#65292;&#23558;&#36873;&#25321;&#39033;&#30446;&#30340;&#23376;&#38598;&#22823;&#23567;$k$&#35774;&#32622;&#22312;$[2, n-1]$&#33539;&#22260;&#20869;&#33021;&#22815;&#26368;&#22823;&#21270;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#39033;&#30446;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11721</link><description>&lt;p&gt;
&#20004;&#20010;&#21015;&#34920;&#20160;&#20040;&#26102;&#20505;&#27604;&#19968;&#20010;&#21015;&#34920;&#26356;&#22909;&#65311;&#21512;&#20316;&#20915;&#31574;&#20013;&#30340;&#30410;&#22788;&#21644;&#20260;&#23475;
&lt;/p&gt;
&lt;p&gt;
When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11721
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20154;&#24037;&#21644;&#31639;&#27861;&#21512;&#20316;&#65292;&#23545;&#20110;&#22810;&#20010;&#22122;&#38899;&#27169;&#22411;&#26469;&#35828;&#65292;&#23558;&#36873;&#25321;&#39033;&#30446;&#30340;&#23376;&#38598;&#22823;&#23567;$k$&#35774;&#32622;&#22312;$[2, n-1]$&#33539;&#22260;&#20869;&#33021;&#22815;&#26368;&#22823;&#21270;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#39033;&#30446;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#24456;&#22823;&#19968;&#37096;&#20998;&#20851;&#27880;&#30340;&#26159;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#26356;&#22810;&#22320;&#20851;&#27880;&#20110;&#20248;&#21270;&#20154;&#24037;&#21644;&#31639;&#27861;&#30340;&#32852;&#21512;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20154;&#24037;&#21644;&#31639;&#27861;&#21512;&#20316;&#65292;&#22312;&#36825;&#31181;&#21512;&#20316;&#20013;&#65292;&#31639;&#27861;&#21487;&#20197;&#35775;&#38382;&#19968;&#32452;n&#20010;&#39033;&#30446;&#65292;&#24182;&#23558;&#22823;&#23567;&#20026;k&#30340;&#19968;&#20010;&#23376;&#38598;&#21576;&#29616;&#32473;&#20154;&#31867;&#65292;&#28982;&#21518;&#20154;&#31867;&#20174;&#36825;&#20123;k&#20010;&#39033;&#30446;&#20013;&#36873;&#25321;&#19968;&#20010;&#26368;&#32456;&#39033;&#30446;&#12290;&#36825;&#31181;&#24773;&#20917;&#21487;&#20197;&#27169;&#25311;&#20869;&#23481;&#25512;&#33616;&#12289;&#36335;&#24452;&#35268;&#21010;&#25110;&#20219;&#20309;&#31867;&#22411;&#30340;&#26631;&#27880;&#20219;&#21153;&#12290;&#30001;&#20110;&#20154;&#31867;&#21644;&#31639;&#27861;&#37117;&#23545;&#39033;&#30446;&#30340;&#30495;&#23454;&#25490;&#24207;&#26377;&#30528;&#19981;&#23436;&#32654;&#12289;&#26377;&#22122;&#38899;&#30340;&#20449;&#24687;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;&#65306;&#21738;&#20010;$k$&#20540;&#33021;&#26368;&#22823;&#21270;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#39033;&#30446;&#30340;&#27010;&#29575;&#65311;&#23545;&#20110;$k=1$&#65292;&#31639;&#27861;&#21333;&#29420;&#34892;&#21160;&#26102;&#24615;&#33021;&#26368;&#20248;&#65292;&#32780;&#23545;&#20110;$k=n$&#65292;&#20154;&#31867;&#21333;&#29420;&#34892;&#21160;&#26102;&#24615;&#33021;&#26368;&#20248;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#22810;&#20010;&#22122;&#38899;&#27169;&#22411;&#65292;&#23558;$k$&#35774;&#32622;&#22312;$[2, n-1]$&#33539;&#22260;&#20869;&#26159;&#26368;&#20248;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#21512;&#20316;&#26377;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#21644;&#20195;&#34920;&#24615;&#31034;&#20363;&#30340;&#35821;&#35328;&#25506;&#27979;&#26041;&#27861;&#26469;&#25512;&#36827;&#20851;&#31995;&#25552;&#21462;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#31867;&#21035;&#25490;&#24207;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#23545;&#27604;&#31867;&#20043;&#38388;&#30340;&#28151;&#28102;&#12290;&#32463;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11720</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#38598;&#21512;&#25193;&#23637;&#30340;&#31034;&#20363;&#36827;&#34892;&#35821;&#35328;&#25506;&#27979;&#26469;&#25512;&#36827;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion. (arXiv:2308.11720v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#21644;&#20195;&#34920;&#24615;&#31034;&#20363;&#30340;&#35821;&#35328;&#25506;&#27979;&#26041;&#27861;&#26469;&#25512;&#36827;&#20851;&#31995;&#25552;&#21462;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#31867;&#21035;&#25490;&#24207;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#23545;&#27604;&#31867;&#20043;&#38388;&#30340;&#28151;&#28102;&#12290;&#32463;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26159;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20195;&#34920;&#24615;&#31034;&#20363;&#21644;&#38598;&#21512;&#25193;&#23637;&#26469;&#25552;&#39640;&#20851;&#31995;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#23545;&#27604;&#31867;&#20043;&#38388;&#30340;&#28151;&#28102;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#20195;&#34920;&#24615;&#31034;&#20363;&#20026;&#27599;&#20010;&#20851;&#31995;&#31867;&#25552;&#20379;&#31181;&#23376;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#30340;&#38598;&#21512;&#25193;&#23637;&#31639;&#27861;&#36890;&#36807;&#23558;&#30446;&#26631;&#23545;&#21644;&#30446;&#26631;&#31867;&#30340;&#20195;&#34920;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#32435;&#20837;&#35757;&#32451;&#30446;&#26631;&#26469;&#20016;&#23500;&#35757;&#32451;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#38598;&#21512;&#25193;&#23637;&#36807;&#31243;&#36824;&#28041;&#21450;&#19968;&#20010;&#32771;&#34385;&#23545;&#27604;&#31867;&#31034;&#20363;&#30340;&#31867;&#21035;&#25490;&#24207;&#36807;&#31243;&#12290;&#21033;&#29992;&#26080;&#19978;&#19979;&#25991;&#30340;Hearst&#27169;&#24335;&#21033;&#29992;&#20851;&#31995;&#25552;&#21450;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#26469;&#30830;&#23450;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#12290;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#38598;&#21512;&#25193;&#23637;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction (RE) is a pivotal task in automatically extracting structured information from unstructured text. In this paper, we present a multi-faceted approach that integrates representative examples and through co-set expansion. The primary goal of our method is to enhance relation classification accuracy and mitigating confusion between contrastive classes.  Our approach begins by seeding each relationship class with representative examples. Subsequently, our co-set expansion algorithm enriches training objectives by incorporating similarity measures between target pairs and representative pairs from the target class. Moreover, the co-set expansion process involves a class ranking procedure that takes into account exemplars from contrastive classes. Contextual details encompassing relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.  Empirical evaluation demonstrates the efficacy of our co-set expansion approach, resulting in a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperCalo&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#19978;&#37319;&#26679;&#39640;&#32500;&#32454;&#31890;&#24230;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#29983;&#25104;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11700</link><description>&lt;p&gt;
SuperCalo: &#33021;&#37327;&#27785;&#31215;&#37327;&#27169;&#25311;&#30340;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SuperCalo: Calorimeter shower super-resolution. (arXiv:2308.11700v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperCalo&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#19978;&#37319;&#26679;&#39640;&#32500;&#32454;&#31890;&#24230;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#29983;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#26159;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#35745;&#31639;&#27969;&#31243;&#20013;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#24037;&#20316;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#35768;&#22810;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#29983;&#25104;&#26102;&#38388;&#19978;&#26080;&#27861;&#24456;&#22909;&#22320;&#36866;&#24212;&#39640;&#32500;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperCalo&#30340;&#22522;&#20110;&#27969;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#39640;&#32500;&#32454;&#31890;&#24230;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#21487;&#20197;&#20174;&#31895;&#31890;&#24230;&#27169;&#25311;&#20013;&#24555;&#36895;&#19978;&#37319;&#26679;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#19982;&#24555;&#36895;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#20869;&#23384;&#38656;&#27714;&#21644;&#29983;&#25104;&#26102;&#38388;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#30001;SuperCalo&#19978;&#37319;&#26679;&#24471;&#21040;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#20855;&#26377;&#39640;&#24230;&#21464;&#21270;&#30340;&#29305;&#28857;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20174;&#36739;&#23569;&#30340;&#31895;&#31890;&#24230;&#27169;&#25311;&#20013;&#19978;&#37319;&#26679;&#20986;&#22810;&#20010;&#39640;&#32500;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#65292;&#20197;&#39640;&#20445;&#30495;&#24230;&#30340;&#26041;&#24335;&#36827;&#19968;&#27493;&#20943;&#23569;&#29983;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#23384;&#20648;&#38480;&#21046;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#35745;&#31639;&#33021;&#21147;&#20197;&#21450;&#23398;&#20064;&#31867;&#21035;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#36890;&#36807;&#31227;&#21160;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#23398;&#20064;&#20154;&#20307;&#27963;&#21160;&#65292;&#20174;&#32780;&#20026;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.11691</link><description>&lt;p&gt;
&#36793;&#32536;&#19978;&#23545;&#26032;&#30340;&#20154;&#20307;&#27963;&#21160;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#30340;&#23454;&#29992;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Practical Insights on Incremental Learning of New Human Physical Activity on the Edge. (arXiv:2308.11691v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#23384;&#20648;&#38480;&#21046;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#35745;&#31639;&#33021;&#21147;&#20197;&#21450;&#23398;&#20064;&#31867;&#21035;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#36890;&#36807;&#31227;&#21160;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#23398;&#20064;&#20154;&#20307;&#27963;&#21160;&#65292;&#20174;&#32780;&#20026;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#65288;Edge ML&#65289;&#23558;&#35745;&#31639;&#26234;&#33021;&#20174;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#36716;&#31227;&#21040;&#36793;&#32536;&#35774;&#22791;&#65292;&#30001;&#20110;&#20854;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#22914;&#38477;&#20302;&#24310;&#36831;&#12289;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#21644;&#20943;&#23569;&#36830;&#25509;&#20381;&#36182;&#65292;&#21560;&#24341;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#36825;&#20123;&#20248;&#21183;&#20855;&#26377;&#35828;&#26381;&#21147;&#65292;&#20294;&#23427;&#20204;&#24341;&#20837;&#20102;&#20256;&#32479;&#22522;&#20110;&#20113;&#30340;&#26041;&#27861;&#20013;&#32570;&#22833;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20102;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#21463;&#38480;&#25968;&#25454;&#23384;&#20648;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#35745;&#31639;&#33021;&#21147;&#20197;&#21450;&#23398;&#20064;&#31867;&#21035;&#25968;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;MAGNETO&#31995;&#32479;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#31227;&#21160;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#23398;&#20064;&#20154;&#20307;&#27963;&#21160;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#21021;&#22987;&#35757;&#32451;&#31574;&#30053;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21021;&#22987;&#23398;&#20064;&#31574;&#30053;&#30340;&#36873;&#25321;&#20250;&#26174;&#33879;&#24433;&#21709;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.11677</link><description>&lt;p&gt;
&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#21021;&#22987;&#35757;&#32451;&#31574;&#30053;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning. (arXiv:2308.11677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#21021;&#22987;&#35757;&#32451;&#31574;&#30053;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21021;&#22987;&#23398;&#20064;&#31574;&#30053;&#30340;&#36873;&#25321;&#20250;&#26174;&#33879;&#24433;&#21709;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#12290;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#30340;&#27599;&#19968;&#27493;&#20013;&#65292;&#26032;&#30340;&#31867;&#21035;&#24517;&#39035;&#34987;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24403;&#26080;&#27861;&#23384;&#20648;&#36807;&#21435;&#31867;&#21035;&#30340;&#26679;&#26412;&#26102;&#65292;&#31867;&#22686;&#37327;&#23398;&#20064;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#27491;&#26159;&#25105;&#20204;&#22312;&#27492;&#30740;&#31350;&#30340;&#23545;&#35937;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#22522;&#20110;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20351;&#29992;&#24050;&#32463;&#36880;&#28176;&#22686;&#22810;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#30340;&#21021;&#22987;&#27169;&#22411;&#21487;&#33021;&#20165;&#20351;&#29992;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#31532;&#19968;&#25209;&#25968;&#25454;&#65292;&#25110;&#32773;&#36824;&#21487;&#20197;&#20351;&#29992;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#12290;&#36825;&#20004;&#31181;&#21021;&#22987;&#23398;&#20064;&#31574;&#30053;&#30340;&#36873;&#25321;&#21487;&#20197;&#26497;&#22823;&#22320;&#24433;&#21709;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#24615;&#33021;&#36824;&#21463;&#21040;&#31867;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#12289;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12289;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#36136;&#12289;&#31867;&#21035;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of clas
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#26469;&#22686;&#24378;&#23545;&#36825;&#20123;&#27169;&#22411;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#65292;&#20026;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#28508;&#22312;&#20215;&#20540;&#12290;"</title><link>http://arxiv.org/abs/2308.11676</link><description>&lt;p&gt;
"&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#30740;&#31350;"
&lt;/p&gt;
&lt;p&gt;
A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework. (arXiv:2308.11676v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11676
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#26469;&#22686;&#24378;&#23545;&#36825;&#20123;&#27169;&#22411;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#65292;&#20026;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#28508;&#22312;&#20215;&#20540;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#65288;POF&#65289;&#22312;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;POF&#30340;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65288;CIMs-B-POF&#65289;&#26088;&#22312;&#28040;&#38500;&#28151;&#28102;&#20559;&#24046;&#65292;&#24182;&#40664;&#35748;&#23384;&#22312;&#28151;&#28102;&#21327;&#21464;&#37327;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#36825;&#19968;&#20551;&#35774;&#35748;&#20026;&#21327;&#21464;&#37327;&#20165;&#30001;&#28151;&#28102;&#21464;&#37327;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20445;&#25345;&#28151;&#28102;&#21327;&#21464;&#37327;&#30340;&#20551;&#35774;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#39640;&#32500;&#21327;&#21464;&#37327;&#26102;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#22312;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#20043;&#21069;&#21306;&#20998;&#21327;&#21464;&#37327;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23558;&#38750;&#28151;&#28102;&#30340;&#21327;&#21464;&#37327;&#35270;&#20026;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#26524;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;CIMs-B-POF&#26102;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;CIMs-B-POF&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20010;&#22270;&#24418;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;CIMs-B-POF&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#37327;&#21270;&#20998;&#26512;&#12290;"
&lt;/p&gt;
&lt;p&gt;
The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models' underlying principles. Using this graphical framework, we quant
&lt;/p&gt;</description></item><item><title>WEARS&#26159;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#34920;&#20256;&#24863;&#22120;&#30340;&#24773;&#32490;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#30495;&#23454;&#25968;&#25454;&#21644;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#24773;&#32490;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#20998;&#26512;&#20102;&#24515;&#29575;&#12289;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20256;&#24863;&#22120;&#25968;&#25454;&#23545;&#24773;&#32490;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.11673</link><description>&lt;p&gt;
WEARS: &#22522;&#20110;&#23454;&#26102;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21487;&#31359;&#25140;&#24773;&#32490;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
WEARS: Wearable Emotion AI with Real-time Sensor data. (arXiv:2308.11673v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11673
&lt;/p&gt;
&lt;p&gt;
WEARS&#26159;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#34920;&#20256;&#24863;&#22120;&#30340;&#24773;&#32490;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#30495;&#23454;&#25968;&#25454;&#21644;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#24773;&#32490;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#20998;&#26512;&#20102;&#24515;&#29575;&#12289;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20256;&#24863;&#22120;&#25968;&#25454;&#23545;&#24773;&#32490;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#39044;&#27979;&#26159;&#30740;&#31350;&#29702;&#35299;&#20154;&#31867;&#24773;&#32490;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#38754;&#37096;&#34920;&#24773;&#31561;&#29305;&#24449;&#19978;&#65292;&#20294;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#23545;&#29992;&#25143;&#26469;&#35828;&#26159;&#31169;&#23494;&#30340;&#12290;&#24773;&#32490;&#20063;&#21487;&#20197;&#20174;&#20027;&#39064;&#30340;&#24515;&#29702;&#25968;&#25454;&#20013;&#25512;&#23548;&#24471;&#21040;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#29702;&#20256;&#24863;&#22120;&#32452;&#21512;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#20256;&#24863;&#22120;&#23545;&#29992;&#25143;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#20351;&#29992;&#37117;&#26041;&#20415;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26234;&#33021;&#25163;&#34920;&#20256;&#24863;&#22120;&#26469;&#39044;&#27979;&#29992;&#25143;&#24773;&#32490;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;&#33521;&#35821;&#21644;&#21306;&#22495;&#35821;&#35328;&#30340;&#35270;&#39057;&#26469;&#24341;&#36215;&#21442;&#19982;&#32773;&#30340;&#24773;&#32490;&#24182;&#25910;&#38598;&#25968;&#25454;&#65292;&#20197;&#23454;&#26102;&#37319;&#38598;&#30495;&#23454;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#26377;&#38480;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#20108;&#20998;&#31867;&#65292;&#24182;&#23581;&#35797;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#24515;&#29575;&#12289;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20256;&#24863;&#22120;&#25968;&#25454;&#23545;&#24515;&#24773;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion prediction is the field of study to understand human emotions. Existing methods focus on modalities like text, audio, facial expressions, etc., which could be private to the user. Emotion can be derived from the subject's psychological data as well. Various approaches that employ combinations of physiological sensors for emotion recognition have been proposed. Yet, not all sensors are simple to use and handy for individuals in their daily lives. Thus, we propose a system to predict user emotion using smartwatch sensors. We design a framework to collect ground truth in real-time utilizing a mix of English and regional language-based videos to invoke emotions in participants and collect the data. Further, we modeled the problem as binary classification due to the limited dataset size and experimented with multiple machine-learning models. We also did an ablation study to understand the impact of features including Heart Rate, Accelerometer, and Gyroscope sensor data on mood. From
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32452;&#32455;&#21644;&#23454;&#39564;&#23884;&#20837;&#21040;&#19978;&#19979;&#25991;&#21270;&#30340;&#22522;&#22240;&#32452;&#32593;&#32476;&#65288;CGN&#65289;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#22686;&#24378;&#38271;&#31243;&#24207;&#21015;&#23884;&#20837;&#26469;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#26465;&#20214;&#19979;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11671</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#32455;&#21644;&#23454;&#39564;&#23884;&#20837;&#25512;&#24191;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#34920;&#35266;&#22522;&#22240;&#32452;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generalising sequence models for epigenome predictions with tissue and assay embeddings. (arXiv:2308.11671v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32452;&#32455;&#21644;&#23454;&#39564;&#23884;&#20837;&#21040;&#19978;&#19979;&#25991;&#21270;&#30340;&#22522;&#22240;&#32452;&#32593;&#32476;&#65288;CGN&#65289;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#22686;&#24378;&#38271;&#31243;&#24207;&#21015;&#23884;&#20837;&#26469;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#26465;&#20214;&#19979;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29992;&#20110;&#34920;&#35266;&#36951;&#20256;&#22270;&#35889;&#39044;&#27979;&#30340;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#22312;&#24207;&#21015;&#38271;&#24230;&#12289;&#27169;&#22411;&#35268;&#27169;&#21644;&#22270;&#35889;&#22810;&#26679;&#24615;&#26041;&#38754;&#26377;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#65292;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#23545;&#35768;&#22810;&#23454;&#39564;&#21487;&#34892;&#30340;&#32452;&#32455;&#21644;&#23454;&#39564;&#32452;&#21512;&#36827;&#34892;&#25512;&#29702;&#65292;&#38480;&#21046;&#20102;&#23545;&#35843;&#25511;&#22522;&#22240;&#32452;&#30340;$\textit{in silico}$&#29702;&#35299;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36890;&#36807;&#23558;&#32452;&#32455;&#21644;&#23454;&#39564;&#23884;&#20837;&#32435;&#20837;&#21040;&#19978;&#19979;&#25991;&#21270;&#30340;&#22522;&#22240;&#32452;&#32593;&#32476;&#65288;CGN&#65289;&#20013;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#26465;&#20214;&#19979;&#23454;&#29616;&#24378;&#30456;&#20851;&#24615;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#22686;&#24378;&#38271;&#31243;&#24207;&#21015;&#23884;&#20837;&#26469;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#25193;&#23637;&#36755;&#20986;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#34920;&#35266;&#36951;&#20256;&#22270;&#35889;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#39318;&#27425;&#25581;&#31034;&#20102;&#36951;&#20256;&#21464;&#24322;&#23545;&#34920;&#35266;&#24207;&#21015;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#38598;&#25104;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence modelling approaches for epigenetic profile prediction have recently expanded in terms of sequence length, model size, and profile diversity. However, current models cannot infer on many experimentally feasible tissue and assay pairs due to poor usage of contextual information, limiting $\textit{in silico}$ understanding of regulatory genomics. We demonstrate that strong correlation can be achieved across a large range of experimental conditions by integrating tissue and assay embeddings into a Contextualised Genomic Network (CGN). In contrast to previous approaches, we enhance long-range sequence embeddings with contextual information in the input space, rather than expanding the output space. We exhibit the efficacy of our approach across a broad set of epigenetic profiles and provide the first insights into the effect of genetic variants on epigenetic sequence model training. Our general approach to context integration exceeds state of the art in multiple settings while emp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#65292;&#20351;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#36890;&#36807;IoT&#35774;&#22791;&#30340;&#20869;&#37096;&#20256;&#24863;&#22120;&#25968;&#25454;&#23454;&#29616;&#22312;&#24037;&#21378;&#29615;&#22659;&#20013;&#23450;&#20301;&#31227;&#21160;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2308.11670</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#21378;&#29615;&#22659;&#23450;&#20301;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-based Positioning using Multivariate Time Series Classification for Factory Environments. (arXiv:2308.11670v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#65292;&#20351;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#36890;&#36807;IoT&#35774;&#22791;&#30340;&#20869;&#37096;&#20256;&#24863;&#22120;&#25968;&#25454;&#23454;&#29616;&#22312;&#24037;&#21378;&#29615;&#22659;&#20013;&#23450;&#20301;&#31227;&#21160;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20005;&#37325;&#20381;&#36182;&#20110;&#22806;&#37096;&#22522;&#30784;&#35774;&#26045;&#65292;&#21487;&#33021;&#28041;&#21450;&#28508;&#22312;&#30340;&#38544;&#31169;&#22949;&#21327;&#12289;&#22806;&#37096;&#20449;&#24687;&#35201;&#27714;&#21644;&#20551;&#35774;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#38656;&#35201;&#20445;&#25252;&#38544;&#31169;&#21644;&#24310;&#38271;&#21151;&#33021;&#30340;&#29615;&#22659;&#19981;&#21033;&#12290;&#22312;&#26576;&#20123;&#29615;&#22659;&#20013;&#65292;&#37096;&#32626;&#36741;&#21161;&#22522;&#30784;&#35774;&#26045;&#36827;&#34892;&#23460;&#20869;&#23450;&#20301;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#21644;&#26114;&#36149;&#30340;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#30340;&#26041;&#26696;&#65292;&#20165;&#20381;&#38752;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#20869;&#37096;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#21738;&#31181;&#27169;&#22411;&#26368;&#21512;&#36866;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#65292;&#20351;&#29992;&#36816;&#21160;&#21644;&#29615;&#22659;&#20256;&#24863;&#22120;&#65292;&#22312;&#20851;&#27880;&#38544;&#31169;&#30340;&#24037;&#21378;&#29615;&#22659;&#20013;&#23450;&#20301;&#31227;&#21160;&#23454;&#20307;&#12290;&#35813;&#38382;&#39064;&#34987;&#26500;&#24314;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867; (MTSC)&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indoor Positioning Systems (IPS) gained importance in many industrial applications. State-of-the-art solutions heavily rely on external infrastructures and are subject to potential privacy compromises, external information requirements, and assumptions, that make it unfavorable for environments demanding privacy and prolonged functionality. In certain environments deploying supplementary infrastructures for indoor positioning could be infeasible and expensive. Recent developments in machine learning (ML) offer solutions to address these limitations relying only on the data from onboard sensors of IoT devices. However, it is unclear which model fits best considering the resource constraints of IoT devices. This paper presents a machine learning-based indoor positioning system, using motion and ambient sensors, to localize a moving entity in privacy concerned factory environments. The problem is formulated as a multivariate time series classification (MTSC) and a comparative analysis of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#26631;&#31614;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65288;CLAD&#65289;&#65292;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#33410;&#28857;&#26469;&#22686;&#24378;&#26080;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#22312;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11669</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#26631;&#31614;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Class Label-aware Graph Anomaly Detection. (arXiv:2308.11669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#26631;&#31614;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65288;CLAD&#65289;&#65292;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#33410;&#28857;&#26469;&#22686;&#24378;&#26080;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#22312;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20551;&#35774;&#32570;&#20047;&#24322;&#24120;&#26631;&#31614;&#65292;&#21363;&#33410;&#28857;&#26159;&#21542;&#24322;&#24120;&#12290;&#25105;&#20204;&#20174;&#20197;&#24448;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#20013;&#35266;&#23519;&#21040;&#65292;&#23427;&#20204;&#19981;&#20165;&#20551;&#35774;&#32570;&#20047;&#24322;&#24120;&#26631;&#31614;&#65292;&#36824;&#20551;&#35774;&#32570;&#20047;&#31867;&#26631;&#31614;&#65288;&#33410;&#28857;&#22312;&#19968;&#33324;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#25152;&#23646;&#30340;&#31867;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31867;&#26631;&#31614;&#23545;&#26080;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#22914;&#20309;&#22686;&#24378;&#23545;&#32467;&#26500;&#24322;&#24120;&#30340;&#26816;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#26631;&#31614;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65288;CLAD&#65289;&#65292;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#33410;&#28857;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#22312;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;CLAD&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#22320;&#38754;&#30495;&#23454;&#31867;&#26631;&#31614;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#12290;CLAD&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/jhkim611/CLAD}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised GAD methods assume the lack of anomaly labels, i.e., whether a node is anomalous or not. One common observation we made from previous unsupervised methods is that they not only assume the absence of such anomaly labels, but also the absence of class labels (the class a node belongs to used in a general node classification task). In this work, we study the utility of class labels for unsupervised GAD; in particular, how they enhance the detection of structural anomalies. To this end, we propose a Class Label-aware Graph Anomaly Detection framework (CLAD) that utilizes a limited amount of labeled nodes to enhance the performance of unsupervised GAD. Extensive experiments on ten datasets demonstrate the superior performance of CLAD in comparison to existing unsupervised GAD methods, even in the absence of ground-truth class label information. The source code for CLAD is available at \url{https://github.com/jhkim611/CLAD}.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#25311;&#20445;&#38505;&#27450;&#35784;&#32593;&#32476;&#25968;&#25454;&#30340;&#24341;&#25806;&#65292;&#21033;&#29992;&#32034;&#36180;&#28041;&#21450;&#26041;&#30340;&#31038;&#20132;&#32593;&#32476;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#24320;&#21457;&#39640;&#25928;&#20934;&#30830;&#30340;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#12290;&#20294;&#38754;&#20020;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#32570;&#20047;&#20844;&#24320;&#25968;&#25454;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11659</link><description>&lt;p&gt;
&#19968;&#31181;&#27169;&#25311;&#20445;&#38505;&#27450;&#35784;&#32593;&#32476;&#25968;&#25454;&#30340;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
An engine to simulate insurance fraud network data. (arXiv:2308.11659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#25311;&#20445;&#38505;&#27450;&#35784;&#32593;&#32476;&#25968;&#25454;&#30340;&#24341;&#25806;&#65292;&#21033;&#29992;&#32034;&#36180;&#28041;&#21450;&#26041;&#30340;&#31038;&#20132;&#32593;&#32476;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#24320;&#21457;&#39640;&#25928;&#20934;&#30830;&#30340;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#12290;&#20294;&#38754;&#20020;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#32570;&#20047;&#20844;&#24320;&#25968;&#25454;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26816;&#27979;&#20445;&#38505;&#27450;&#35784;&#32034;&#36180;&#20381;&#36182;&#20110;&#19994;&#21153;&#35268;&#21017;&#21644;&#19987;&#23478;&#21028;&#26029;&#65292;&#36825;&#20351;&#24471;&#36825;&#19968;&#36807;&#31243;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#25506;&#32034;&#24320;&#21457;&#39640;&#25928;&#20934;&#30830;&#30340;&#20998;&#26512;&#31574;&#30053;&#26469;&#26631;&#35760;&#21487;&#30097;&#32034;&#36180;&#12290;&#20174;&#32034;&#36180;&#28041;&#21450;&#26041;&#30340;&#31038;&#20132;&#32593;&#32476;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#20854;&#39304;&#36865;&#32473;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#31181;&#29305;&#21035;&#26377;&#28508;&#21147;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#21457;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#27450;&#35784;&#30340;&#38750;&#24120;&#35268;&#24615;&#36136;&#23548;&#33268;&#20102;&#39640;&#24230;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#22686;&#21152;&#20102;&#24320;&#21457;&#24615;&#33021;&#33391;&#22909;&#30340;&#20998;&#26512;&#20998;&#31867;&#27169;&#22411;&#30340;&#38590;&#24230;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#23569;&#25968;&#32034;&#36180;&#24471;&#21040;&#35843;&#26597;&#21644;&#26631;&#31614;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#65292;&#36825;&#22952;&#30861;&#20102;&#30740;&#31350;&#21644;&#27169;&#22411;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, the detection of fraudulent insurance claims relies on business rules and expert judgement which makes it a time-consuming and expensive process (\'Oskarsd\'ottir et al., 2022). Consequently, researchers have been examining ways to develop efficient and accurate analytic strategies to flag suspicious claims. Feeding learning methods with features engineered from the social network of parties involved in a claim is a particularly promising strategy (see for example Van Vlasselaer et al. (2016); Tumminello et al. (2023)). When developing a fraud detection model, however, we are confronted with several challenges. The uncommon nature of fraud, for example, creates a high class imbalance which complicates the development of well performing analytic classification models. In addition, only a small number of claims are investigated and get a label, which results in a large corpus of unlabeled data. Yet another challenge is the lack of publicly available data. This hinders not 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#22312;&#32447;&#27169;&#24335;&#30340;BCI&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#22312;&#32447;&#22788;&#29702;&#30340;&#29305;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#27835;&#30103;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.11656</link><description>&lt;p&gt;
BCI&#35780;&#20272;&#30340;&#20266;&#22312;&#32447;&#26694;&#26550;&#65306;MOABB&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Pseudo-online framework for BCI evaluation: A MOABB perspective. (arXiv:2308.11656v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11656
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#22312;&#32447;&#27169;&#24335;&#30340;BCI&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#22312;&#32447;&#22788;&#29702;&#30340;&#29305;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#27835;&#30103;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BCI&#65288;&#33041;&#26426;&#25509;&#21475;&#65289;&#25216;&#26415;&#26377;&#19977;&#31181;&#25805;&#20316;&#27169;&#24335;&#65306;&#22312;&#32447;&#12289;&#31163;&#32447;&#21644;&#20266;&#22312;&#32447;&#12290;&#22312;&#32447;&#27169;&#24335;&#19979;&#65292;&#23454;&#26102;&#30340;&#33041;&#30005;&#25968;&#25454;&#34987;&#25345;&#32493;&#20998;&#26512;&#12290;&#31163;&#32447;&#27169;&#24335;&#19979;&#65292;&#20449;&#21495;&#22312;&#37319;&#38598;&#21518;&#36827;&#34892;&#22788;&#29702;&#12290;&#20266;&#22312;&#32447;&#27169;&#24335;&#19979;&#65292;&#37319;&#38598;&#21040;&#30340;&#25968;&#25454;&#34987;&#22788;&#29702;&#25104;&#20223;&#30495;&#23454;&#26102;&#25509;&#25910;&#30340;&#24418;&#24335;&#12290;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#31163;&#32447;&#27169;&#24335;&#32463;&#24120;&#20998;&#26512;&#25972;&#20010;&#25968;&#25454;&#65292;&#32780;&#22312;&#32447;&#21644;&#20266;&#22312;&#32447;&#27169;&#24335;&#21482;&#20998;&#26512;&#30701;&#26102;&#38388;&#31383;&#21475;&#30340;&#25968;&#25454;&#12290;&#31163;&#32447;&#20998;&#26512;&#36890;&#24120;&#19982;&#24322;&#27493;BCI&#19968;&#36215;&#20351;&#29992;&#65292;&#36825;&#38480;&#21046;&#20102;&#20998;&#26512;&#21040;&#39044;&#23450;&#20041;&#30340;&#26102;&#38388;&#31383;&#21475;&#12290;&#19982;&#22312;&#32447;&#21644;&#20266;&#22312;&#32447;&#27169;&#24335;&#20860;&#23481;&#30340;&#24322;&#27493;BCI&#20801;&#35768;&#28789;&#27963;&#30340;&#24605;&#32500;&#27963;&#21160;&#25345;&#32493;&#26102;&#38388;&#12290;&#31163;&#32447;&#22788;&#29702;&#24448;&#24448;&#26356;&#20934;&#30830;&#65292;&#32780;&#22312;&#32447;&#20998;&#26512;&#21017;&#26356;&#36866;&#29992;&#20110;&#27835;&#30103;&#24212;&#29992;&#12290;&#20266;&#22312;&#32447;&#23454;&#29616;&#36817;&#20284;&#20110;&#26080;&#23454;&#26102;&#32422;&#26463;&#30340;&#22312;&#32447;&#22788;&#29702;&#12290;&#35768;&#22810;&#31163;&#32447;&#30340;BCI&#30740;&#31350;&#30456;&#23545;&#20110;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#24773;&#26223;&#24341;&#20837;&#20102;&#20559;&#24046;&#65292;&#24433;&#21709;&#20102;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: BCI (Brain-Computer Interface) technology operates in three modes: online, offline, and pseudo-online. In the online mode, real-time EEG data is constantly analyzed. In offline mode, the signal is acquired and processed afterwards. The pseudo-online mode processes collected data as if they were received in real-time. The main difference is that the offline mode often analyzes the whole data, while the online and pseudo-online modes only analyze data in short time windows. Offline analysis is usually done with asynchronous BCIs, which restricts analysis to predefined time windows. Asynchronous BCI, compatible with online and pseudo-online modes, allows flexible mental activity duration. Offline processing tends to be more accurate, while online analysis is better for therapeutic applications. Pseudo-online implementation approximates online processing without real-time constraints. Many BCI studies being offline introduce biases compared to real-life scenarios, impacting clas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#35774;&#35745;AdaCE&#27169;&#22359;&#22312;&#22810;&#20010;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11654</link><description>&lt;p&gt;
&#22823;&#22411;&#21464;&#21387;&#22120;&#26159;&#26356;&#22909;&#30340;&#33041;&#30005;&#22270;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Transformers are Better EEG Learners. (arXiv:2308.11654v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#35774;&#35745;AdaCE&#27169;&#22359;&#22312;&#22810;&#20010;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#21487;&#29992;&#30340;&#26631;&#35760;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#30340;&#35268;&#27169;&#36828;&#36828;&#20302;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#65292;&#22240;&#27492;&#24456;&#38590;&#23558;&#20174;EEG&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#24320;&#21457;&#21040;&#20687;GPT-4 100T&#36825;&#26679;&#30340;&#35268;&#27169;&#65292;&#20174;&#32780;&#23436;&#20840;&#21457;&#25381;&#35813;&#26550;&#26500;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;AdaCE&#65292;&#21363;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#24418;&#24335;&#30340;&#25554;&#25300;&#24335;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21464;&#21387;&#22120;&#12290;&#25552;&#20986;&#30340;AdaCE&#27169;&#22359;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#21516;&#26102;&#22312;&#22810;&#31181;&#22522;&#20110;EEG&#30340;&#39044;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#39044;&#35757;&#32451;&#30340;Swin-Transformer&#19978;&#30340;AdaCE&#36798;&#21040;&#20102;99.6&#65285;&#30340;&#31934;&#24230;&#65292;&#32477;&#23545;&#25913;&#21892;&#20102;9.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large transformer models have achieved remarkable performance in the fields of natural language processing and computer vision. Since the magnitude of available labeled electroencephalogram (EEG) data is much lower than that of text and image data, it is difficult for transformer models pre-trained from EEG to be developed as large as GPT-4 100T to fully unleash the potential of this architecture. In this paper, we show that transformers pre-trained from images as well as text can be directly fine-tuned for EEG-based prediction tasks. We design AdaCE, plug-and-play Adapters for Converting EEG data into image as well as text forms, to fine-tune pre-trained vision and language transformers. The proposed AdaCE module is highly effective for fine-tuning pre-trained transformers while achieving state-of-the-art performance on diverse EEG-based prediction tasks. For example, AdaCE on the pre-trained Swin-Transformer achieves 99.6%, an absolute improvement of 9.2%, on the EEG-deco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21021;&#22987;&#21270;&#21152;&#36895;&#31934;&#30830;&#32452;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#35843;&#24230;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#21551;&#21457;&#24335;&#26041;&#27861;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24615;&#21644;&#30830;&#23450;&#24615;&#30340;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#38469;&#30340;EdgeTPU&#24179;&#21488;&#19978;&#23454;&#29616;&#20102;128&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.11652</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21021;&#22987;&#21270;&#21152;&#36895;&#31934;&#30830;&#32452;&#21512;&#20248;&#21270;--&#35843;&#24230;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Accelerating Exact Combinatorial Optimization via RL-based Initialization -- A Case Study in Scheduling. (arXiv:2308.11652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21021;&#22987;&#21270;&#21152;&#36895;&#31934;&#30830;&#32452;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#35843;&#24230;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#21551;&#21457;&#24335;&#26041;&#27861;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24615;&#21644;&#30830;&#23450;&#24615;&#30340;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#38469;&#30340;EdgeTPU&#24179;&#21488;&#19978;&#23454;&#29616;&#20102;128&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#22270;&#65288;&#20063;&#31216;&#20026;&#35745;&#31639;&#22270;&#65289;&#19978;&#30340;&#35843;&#24230;&#26159;&#19968;&#20010;NP&#38590;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#31934;&#30830;&#26041;&#27861;&#21463;&#21040;&#36816;&#34892;&#26102;&#22797;&#26434;&#24230;&#30340;&#38480;&#21046;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#30830;&#23450;&#24615;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#35843;&#24230;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#21551;&#21457;&#24335;&#26041;&#27861;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20248;&#21270;&#24615;&#21644;&#30830;&#23450;&#24615;&#30340;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;RL-ILP&#35843;&#24230;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;1&#65289;RL&#27714;&#35299;&#22120;&#20316;&#20026;&#31895;&#31890;&#24230;&#35843;&#24230;&#22120;&#65292;2&#65289;&#35299;&#20915;&#26041;&#26696;&#25918;&#26494;&#21644;3&#65289;&#36890;&#36807;ILP&#36827;&#34892;&#31934;&#30830;&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#30340;EdgeTPU&#24179;&#21488;&#19978;&#65292;&#21033;&#29992;ImageNet DNN&#35745;&#31639;&#65292;&#19982;&#20351;&#29992;&#31934;&#30830;&#35843;&#24230;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#30456;&#21516;&#30340;&#35843;&#24230;&#24615;&#33021;&#24182;&#23454;&#29616;&#39640;&#36798;128&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scheduling on dataflow graphs (also known as computation graphs) is an NP-hard problem. The traditional exact methods are limited by runtime complexity, while reinforcement learning (RL) and heuristic-based approaches struggle with determinism and solution quality. This research aims to develop an innovative approach that employs machine learning (ML) for addressing combinatorial optimization problems, using scheduling as a case study. The goal is to provide guarantees in optimality and determinism while maintaining the runtime cost of heuristic methods. Specifically, we introduce a novel two-phase RL-to-ILP scheduling framework, which includes three steps: 1) RL solver acts as coarse-grain scheduler, 2) solution relaxation and 3) exact solving via ILP. Our framework demonstrates the same scheduling performance compared with using exact scheduling methods while achieving up to 128 $\times$ speed improvements. This was conducted on actual EdgeTPU platforms, utilizing ImageNet DNN comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21160;&#24577;&#28436;&#21270;&#26469;&#25552;&#39640;&#33041;&#30005;&#35299;&#30721;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11651</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#36328;&#20027;&#39064;&#33041;&#30005;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Cross Subject EEG Decoding. (arXiv:2308.11651v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21160;&#24577;&#28436;&#21270;&#26469;&#25552;&#39640;&#33041;&#30005;&#35299;&#30721;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#20110;&#33041;&#30005;&#35299;&#30721;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#28040;&#26497;&#24433;&#21709;&#65306;1&#65289;&#20449;&#21495;&#20013;&#22266;&#26377;&#30340;&#39640;&#26041;&#24046;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#27745;&#26579;&#65292;2&#65289;&#33041;&#30005;&#25968;&#25454;&#38598;&#36890;&#24120;&#30456;&#23545;&#36739;&#23567;&#65292;&#32473;&#23450;&#20102;&#37319;&#38598;&#25104;&#26412;&#12289;&#27880;&#37322;&#25104;&#26412;&#21644;&#25152;&#38656;&#30340;&#21162;&#21147;&#37327;&#12290;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#32463;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#22312;&#31354;&#38388;&#22495;&#12289;&#26102;&#38388;&#22495;&#25110;&#39057;&#29575;&#22495;&#19978;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#65292;&#25163;&#24037;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#19987;&#23478;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21160;&#24577;&#28436;&#21270;&#26469;&#25552;&#39640;&#35299;&#30721;&#30340;&#31283;&#20581;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#19968;&#26063;&#28436;&#21270;&#25968;&#25454;&#20998;&#24067;&#26469;&#23454;&#29616;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#22522;&#20110;Wasserstein&#26799;&#24230;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25968;&#25454;&#28436;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has shown to be effective for Electroencephalography (EEG) decoding tasks. Yet, its performance can be negatively influenced by two key factors: 1) the high variance and different types of corruption that are inherent in the signal, 2) the EEG datasets are usually relatively small given the acquisition cost, annotation cost and amount of effort needed. Data augmentation approaches for alleviation of this problem have been empirically studied, with augmentation operations on spatial domain, time domain or frequency domain handcrafted based on expertise of domain knowledge. In this work, we propose a principled approach to perform dynamic evolution on the data for improvement of decoding robustness. The approach is based on distributionally robust optimization and achieves robustness by optimizing on a family of evolved data distributions instead of the single training data distribution. We derived a general data evolution framework based on Wasserstein gradient f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;FedRANE&#26041;&#27861;&#32467;&#21512;&#20102;&#23616;&#37096;&#20851;&#31995;&#22686;&#24378;&#21644;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65292;&#21487;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11646</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#21512;&#23616;&#37096;&#20851;&#31995;&#22686;&#24378;&#21644;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data. (arXiv:2308.11646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;FedRANE&#26041;&#27861;&#32467;&#21512;&#20102;&#23616;&#37096;&#20851;&#31995;&#22686;&#24378;&#21644;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65292;&#21487;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38656;&#35201;&#26381;&#21153;&#22120;&#21644;&#19968;&#31995;&#21015;&#20855;&#26377;&#20998;&#25955;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#20043;&#38388;&#21512;&#20316;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#12290;&#20026;&#20102;&#20351;&#32852;&#37030;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26377;&#25928;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#38750;&#29420;&#31435;&#30456;&#21516;&#20998;&#24067;(non-IID)&#30340;&#20998;&#25955;&#25968;&#25454;&#30340;&#24314;&#27169;&#12290;&#22312;&#38750;IID&#29615;&#22659;&#20013;&#65292;&#22312;&#25968;&#25454;&#24314;&#27169;&#20013;&#23384;&#22312;&#26469;&#33258;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#20869;&#19968;&#33268;&#24615;&#21644;&#24322;&#26500;&#23458;&#25143;&#31471;&#20998;&#24067;&#20043;&#38388;&#30340;&#23458;&#25143;&#31471;&#38388;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#19981;&#20165;&#38459;&#30861;&#20102;&#23569;&#25968;&#25968;&#25454;&#30340;&#20805;&#20998;&#34920;&#31034;&#65292;&#36824;&#24102;&#26469;&#20102;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#21516;&#26102;&#22788;&#29702;&#19978;&#36848;&#20004;&#31181;&#32806;&#21512;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedRANE&#65292;&#23427;&#30001;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65292;&#21363;&#23616;&#37096;&#20851;&#31995;&#22686;&#24378;(LRA)&#21644;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;(GNE)&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#23458;&#25143;&#31471;&#20869;&#21644;&#23458;&#25143;&#31471;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning paradigm that needs collaboration between a server and a series of clients with decentralized data. To make FL effective in real-world applications, existing work devotes to improving the modeling of decentralized data with non-independent and identical distributions (non-IID). In non-IID settings, there are intra-client inconsistency that comes from the imbalanced data modeling, and inter-client inconsistency among heterogeneous client distributions, which not only hinders sufficient representation of the minority data, but also brings discrepant model deviations. However, previous work overlooks to tackle the above two coupling inconsistencies together. In this work, we propose FedRANE, which consists of two main modules, i.e., local relational augmentation (LRA) and global Nash equilibrium (GNE), to resolve intra- and inter-client inconsistency simultaneously. Specifically, in each client, LRA mines the similarity relations a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#25615;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#23398;&#39044;&#21518;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#26102;&#28857;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#33487;&#37266;&#25110;&#27515;&#20129;&#30340;&#27010;&#29575;&#12290;&#36825;&#26159;&#30446;&#21069;&#24050;&#30693;&#30340;&#31532;&#19968;&#20010;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#21160;&#24577;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.11645</link><description>&lt;p&gt;
&#20351;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#24515;&#25615;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#23398;&#39044;&#21518;:&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#39118;&#38505;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Neurological Prognostication of Post-Cardiac-Arrest Coma Patients Using EEG Data: A Dynamic Survival Analysis Framework with Competing Risks. (arXiv:2308.11645v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#25615;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#23398;&#39044;&#21518;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#26102;&#28857;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#33487;&#37266;&#25110;&#27515;&#20129;&#30340;&#27010;&#29575;&#12290;&#36825;&#26159;&#30446;&#21069;&#24050;&#30693;&#30340;&#31532;&#19968;&#20010;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#21160;&#24577;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24515;&#25615;&#39588;&#20572;&#22797;&#33487;&#36827;&#20837;&#26127;&#36855;&#29366;&#24577;&#30340;&#24739;&#32773;&#38754;&#20020;&#30528;&#36739;&#39640;&#30340;&#27515;&#20129;&#39118;&#38505;&#12290;&#39044;&#27979;&#36825;&#20123;&#24739;&#32773;&#30340;&#31070;&#32463;&#23398;&#32467;&#23616;&#65288;&#31070;&#32463;&#23398;&#39044;&#21518;&#20219;&#21153;&#65289;&#21487;&#20197;&#24110;&#21161;&#20915;&#31574;&#27835;&#30103;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#21160;&#24577;&#26694;&#26550;&#65292;&#20351;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#23545;&#24515;&#25615;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#36827;&#34892;&#31070;&#32463;&#23398;&#39044;&#21518;&#65306;&#25105;&#20204;&#30340;&#26694;&#26550;&#26681;&#25454;&#38543;&#30528;&#26356;&#22810;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#33719;&#24471;&#65292;&#38543;&#26102;&#38388;&#20026;&#24739;&#32773;&#20570;&#20986;&#39044;&#27979;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#35757;&#32451;&#24739;&#32773;&#21487;&#29992;&#30340;&#33041;&#30005;&#22270;&#26102;&#38388;&#24207;&#21015;&#30340;&#38271;&#24230;&#21487;&#33021;&#23384;&#22312;&#24046;&#24322;&#12290;&#39044;&#27979;&#21487;&#36890;&#36807;&#26102;&#38388;-&#20107;&#20214;&#32467;&#26524;&#65288;&#33487;&#37266;&#26102;&#38388;&#25110;&#27515;&#20129;&#26102;&#38388;&#65289;&#25110;&#24739;&#32773;&#22312;&#22810;&#20010;&#26102;&#38388;&#27573;&#33487;&#37266;&#25110;&#27515;&#20129;&#30340;&#27010;&#29575;&#26469;&#34920;&#36848;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20219;&#20309;&#25903;&#25345;&#31454;&#20105;&#39118;&#38505;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#24739;&#32773;&#27700;&#24179;&#30340;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#24739;&#32773;&#39318;&#20808;&#20986;&#29616;&#30340;&#19977;&#31181;&#31454;&#20105;&#39118;&#38505;&#65306;&#33487;&#37266;&#12289;&#27515;&#20129;&#25110;&#21457;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients resuscitated from cardiac arrest who enter a coma are at high risk of death. Forecasting neurological outcomes of these patients (the task of neurological prognostication) could help with treatment decisions. In this paper, we propose, to the best of our knowledge, the first dynamic framework for neurological prognostication of post-cardiac-arrest comatose patients using EEG data: our framework makes predictions for a patient over time as more EEG data become available, and different training patients' available EEG time series could vary in length. Predictions are phrased in terms of either time-to-event outcomes (time-to-awakening or time-to-death) or as the patient's probability of awakening or of dying across multiple time horizons. Our framework uses any dynamic survival analysis model that supports competing risks in the form of estimating patient-level cumulative incidence functions. We consider three competing risks as to what happens first to a patient: awakening, bei
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#21367;&#31215;&#21644;&#24490;&#29615;&#32467;&#26500;&#20197;&#21450;&#20851;&#38190;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21327;&#21516;&#21435;&#22122;&#22810;&#27169;&#24577;&#25391;&#21160;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20013;&#25552;&#21319;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#26089;&#26399;&#25439;&#20260;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11644</link><description>&lt;p&gt;
&#32467;&#26500;&#25391;&#21160;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#30340;&#21327;&#21516;&#20449;&#21495;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Synergistic Signal Denoising for Multimodal Time Series of Structure Vibration. (arXiv:2308.11644v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#21367;&#31215;&#21644;&#24490;&#29615;&#32467;&#26500;&#20197;&#21450;&#20851;&#38190;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21327;&#21516;&#21435;&#22122;&#22810;&#27169;&#24577;&#25391;&#21160;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20013;&#25552;&#21319;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#26089;&#26399;&#25439;&#20260;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#22312;&#30830;&#20445;&#22522;&#30784;&#35774;&#26045;&#30340;&#38271;&#20037;&#24615;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#25198;&#28436;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#35282;&#33394;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20174;&#21508;&#31181;&#32467;&#26500;&#20013;&#20135;&#29983;&#30340;&#25968;&#25454;&#37327;&#21576;&#29616;&#20986;&#21069;&#25152;&#26410;&#26377;&#30340;&#28608;&#22686;&#65292;&#32473;&#26377;&#25928;&#20998;&#26512;&#21644;&#35299;&#37322;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;SHM&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#25391;&#21160;&#20449;&#21495;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#36890;&#36807;&#34701;&#21512;&#21367;&#31215;&#21644;&#24490;&#29615;&#30340;&#32467;&#26500;&#65292;&#35813;&#31639;&#27861;&#28789;&#27963;&#22320;&#25429;&#25417;&#21040;&#20102;&#23616;&#37096;&#21644;&#25345;&#20037;&#30340;&#32467;&#26500;&#34892;&#20026;&#12290;&#20851;&#38190;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25972;&#21512;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#21306;&#20998;&#24182;&#20248;&#20808;&#22788;&#29702;&#26174;&#33879;&#30340;&#32467;&#26500;&#21709;&#24212;&#21644;&#22806;&#37096;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#26089;&#26399;&#25439;&#20260;&#26816;&#27979;&#21644;&#36866;&#24212;&#22810;&#20010;SHM&#22330;&#26223;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#37492;&#20110;SHM&#30340;&#37325;&#35201;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Structural Health Monitoring (SHM) plays an indispensable role in ensuring the longevity and safety of infrastructure. With the rapid growth of sensor technology, the volume of data generated from various structures has seen an unprecedented surge, bringing forth challenges in efficient analysis and interpretation. This paper introduces a novel deep learning algorithm tailored for the complexities inherent in multimodal vibration signals prevalent in SHM. By amalgamating convolutional and recurrent architectures, the algorithm adeptly captures both localized and prolonged structural behaviors. The pivotal integration of attention mechanisms further enhances the model's capability, allowing it to discern and prioritize salient structural responses from extraneous noise. Our results showcase significant improvements in predictive accuracy, early damage detection, and adaptability across multiple SHM scenarios. In light of the critical nature of SHM, the proposed approach not only offers 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#23545;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.11639</link><description>&lt;p&gt;
&#36890;&#36807;&#22788;&#29702;S&#21442;&#25968;&#27169;&#24335;&#23545;&#27687;&#21270;&#38111;&#38177;&#30005;&#26497;&#36827;&#34892;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns. (arXiv:2308.11639v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#23545;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#30005;&#23376;&#39046;&#22495;&#65292;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#22312;&#26174;&#31034;&#22120;&#12289;&#20256;&#24863;&#22120;&#21644;&#22826;&#38451;&#33021;&#30005;&#27744;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#25928;&#30340;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26159;&#30830;&#20445;&#35774;&#22791;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35270;&#35273;&#26816;&#26597;&#23545;&#20110;&#36879;&#26126;&#30340;ITO&#30005;&#26497;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#29616;&#26377;&#30340;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#22312;&#30830;&#23450;&#32570;&#38519;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#30772;&#22351;&#24615;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26681;&#25454;&#32570;&#38519;&#29366;&#24577;&#33719;&#21462;&#20102;&#20840;&#38754;&#30340;S&#21442;&#25968;&#27169;&#24335;&#25968;&#25454;&#24211;&#12290;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#65292;&#21516;&#26102;&#20998;&#26512;&#25925;&#38556;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of optoelectronics, indium tin oxide (ITO) electrodes play a crucial role in various applications, such as displays, sensors, and solar cells. Effective fault detection and diagnosis of the ITO electrodes are essential to ensure the performance and reliability of the devices. However, traditional visual inspection is challenging with transparent ITO electrodes, and existing fault detection methods have limitations in determining the root causes of the defects, often requiring destructive evaluations. In this study, an in situ fault diagnosis method is proposed using scattering parameter (S-parameter) signal processing, offering early detection, high diagnostic accuracy, noise robustness, and root cause analysis. A comprehensive S-parameter pattern database is obtained according to defect states. Deep learning (DL) approaches, including multilayer perceptron (MLP), convolutional neural network (CNN), and transformer, are then used to simultaneously analyze the cause and sev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#28216;&#36208;&#34917;&#20840;&#65288;RWI&#65289;&#26041;&#27861;&#21512;&#25104;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#30340;&#29289;&#32852;&#32593;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#29992;&#20110;&#24320;&#21457;&#21644;&#39564;&#35777;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;&#35780;&#20272;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11638</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
IoT Data Trust Evaluation via Machine Learning. (arXiv:2308.11638v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11638
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#28216;&#36208;&#34917;&#20840;&#65288;RWI&#65289;&#26041;&#27861;&#21512;&#25104;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#30340;&#29289;&#32852;&#32593;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#29992;&#20110;&#24320;&#21457;&#21644;&#39564;&#35777;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;&#35780;&#20272;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35780;&#20272;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#24456;&#38590;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#28216;&#36208;&#34917;&#20840;&#65288;RWI&#65289;&#65292;&#36890;&#36807;&#20174;&#29616;&#26377;&#30340;&#21487;&#20449;&#25968;&#25454;&#21512;&#25104;&#19981;&#21487;&#20449;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#29289;&#32852;&#32593;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#20174;&#29289;&#32852;&#32593;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#26032;&#29305;&#24449;&#65292;&#26377;&#25928;&#25429;&#25417;&#20854;&#33258;&#30456;&#20851;&#24615;&#20197;&#21450;&#19982;&#37051;&#36817;&#65288;&#23545;&#31561;&#65289;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20132;&#21449;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#21512;&#25104;&#30340;&#22522;&#20934;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#24320;&#21457;&#21644;&#39564;&#35777;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;&#35780;&#20272;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various approaches based on supervised or unsupervised machine learning (ML) have been proposed for evaluating IoT data trust. However, assessing their real-world efficacy is hard mainly due to the lack of related publicly-available datasets that can be used for benchmarking. Since obtaining such datasets is challenging, we propose a data synthesis method, called random walk infilling (RWI), to augment IoT time-series datasets by synthesizing untrustworthy data from existing trustworthy data. Thus, RWI enables us to create labeled datasets that can be used to develop and validate ML models for IoT data trust evaluation. We also extract new features from IoT time-series sensor data that effectively capture its auto-correlation as well as its cross-correlation with the data of the neighboring (peer) sensors. These features can be used to learn ML models for recognizing the trustworthiness of IoT sensor data. Equipped with our synthesized ground-truth-labeled datasets and informative corr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#33041;&#30005;&#22270;&#35299;&#30721;&#26694;&#26550;&#65288;FLEEG&#65289;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#20316;&#65292;&#20811;&#26381;&#20102;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#36275;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11636</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#20869;&#22312;&#20449;&#24687;&#25552;&#21319;BCI&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning. (arXiv:2308.11636v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#33041;&#30005;&#22270;&#35299;&#30721;&#26694;&#26550;&#65288;FLEEG&#65289;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#20316;&#65292;&#20811;&#26381;&#20102;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#36275;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26500;&#24314;&#39640;&#24615;&#33021;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25968;&#25454;&#19981;&#36275;&#26159;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#38754;&#20020;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#22242;&#38431;&#21644;&#26426;&#26500;&#20026;&#21516;&#19968;&#20010;BCI&#20219;&#21153;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20294;&#30001;&#20110;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#65292;&#20849;&#20139;&#26469;&#33258;&#22810;&#20010;&#31449;&#28857;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25968;&#25454;&#22810;&#26679;&#24615;&#22312;&#20419;&#36827;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#36825;&#20010;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#19981;&#21487;&#20302;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24456;&#23569;&#35752;&#35770;&#36825;&#20010;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#25968;&#25454;&#38598;&#20869;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#24120;&#26159;&#22312;&#19981;&#21516;&#21463;&#35797;&#32773;&#25110;&#19981;&#21516;&#20250;&#35805;&#35774;&#32622;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#33041;&#30005;&#22270;&#35299;&#30721;&#26694;&#26550;&#65288;FLEEG&#65289;&#65292;&#20197;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#20026;BCI&#24102;&#26469;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21512;&#20316;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#34987;&#20998;&#37197;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;tr
&lt;/p&gt;
&lt;p&gt;
Insufficient data is a long-standing challenge for Brain-Computer Interface (BCI) to build a high-performance deep learning model. Though numerous research groups and institutes collect a multitude of EEG datasets for the same BCI task, sharing EEG data from multiple sites is still challenging due to the heterogeneity of devices. The significance of this challenge cannot be overstated, given the critical role of data diversity in fostering model robustness. However, existing works rarely discuss this issue, predominantly centering their attention on model training within a single dataset, often in the context of inter-subject or inter-session settings. In this work, we propose a hierarchical personalized Federated Learning EEG decoding (FLEEG) framework to surmount this challenge. This innovative framework heralds a new learning paradigm for BCI, enabling datasets with disparate data formats to collaborate in the model training process. Each client is assigned a specific dataset and tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; DS-AGC&#65292;&#29992;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#24182;&#34892;&#27969;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#33041;&#30005;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#35299;&#20915;&#20998;&#24067;&#24046;&#24322;&#21644;&#25552;&#21462;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.11635</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#22312;&#22522;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition. (arXiv:2308.11635v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; DS-AGC&#65292;&#29992;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#24182;&#34892;&#27969;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#33041;&#30005;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#35299;&#20915;&#20998;&#24067;&#24046;&#24322;&#21644;&#25552;&#21462;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270; (EEG) &#26159;&#19968;&#31181;&#26377;&#30528;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#30340;&#23458;&#35266;&#24773;&#32490;&#35782;&#21035;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#22522;&#20110;&#33041;&#30005;&#30340;&#24773;&#32490;&#35782;&#21035;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; (&#31616;&#31216;&#20026; DS-AGC)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;DS-AGC &#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#24182;&#34892;&#27969;&#65292;&#29992;&#20110;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#33041;&#30005;&#29305;&#24449;&#12290;&#38750;&#32467;&#26500;&#21270;&#27969;&#37319;&#29992;&#21322;&#30417;&#30563;&#22810;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#28304;&#22495;&#12289;&#26410;&#26631;&#35760;&#28304;&#22495;&#21644;&#26410;&#30693;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#32467;&#26500;&#21270;&#27969;&#21017;&#24320;&#21457;&#20102;&#19968;&#31181;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21322;&#30417;&#30563;&#26041;&#24335;&#20174;&#22810;&#20010;&#33041;&#30005;&#36890;&#36947;&#20013;&#25552;&#21462;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#27880;&#24847;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoCo-SAS&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36827;&#34892;&#21512;&#25104;&#23380;&#24452;&#22768;&#32435;&#65288;SAS&#65289;&#25968;&#25454;&#22788;&#29702;&#12289;&#20998;&#31867;&#21644;&#27169;&#24335;&#35782;&#21035;&#12290;&#23454;&#39564;&#35777;&#26126;MoCo-SAS&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#22686;&#24378;&#27700;&#19979;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#31867;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.11633</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21512;&#25104;&#23380;&#24452;&#22768;&#32435;&#25968;&#25454;&#22788;&#29702;&#12289;&#20998;&#31867;&#21644;&#27169;&#24335;&#35782;&#21035;&#20013;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition. (arXiv:2308.11633v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoCo-SAS&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36827;&#34892;&#21512;&#25104;&#23380;&#24452;&#22768;&#32435;&#65288;SAS&#65289;&#25968;&#25454;&#22788;&#29702;&#12289;&#20998;&#31867;&#21644;&#27169;&#24335;&#35782;&#21035;&#12290;&#23454;&#39564;&#35777;&#26126;MoCo-SAS&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#22686;&#24378;&#27700;&#19979;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#31867;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23380;&#24452;&#22768;&#32435;&#65288;SAS&#65289;&#25104;&#20687;&#22240;&#20854;&#22312;&#22686;&#21152;&#36317;&#31163;&#26102;&#20445;&#25345;&#20998;&#36776;&#29575;&#30340;&#29420;&#29305;&#33021;&#21147;&#32780;&#25104;&#20026;&#27700;&#19979;&#25506;&#27979;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#36825;&#26159;&#20256;&#32479;&#22768;&#32435;&#25216;&#26415;&#25152;&#27809;&#26377;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;SAS&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MoCo-SAS&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36827;&#34892;SAS&#25968;&#25454;&#22788;&#29702;&#12289;&#20998;&#31867;&#21644;&#27169;&#24335;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoCo-SAS&#22312;F1-score&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;SSL&#22312;&#25512;&#21160;SAS&#25968;&#25454;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20026;&#22686;&#24378;&#27700;&#19979;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#31867;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Aperture Sonar (SAS) imaging has become a crucial technology for underwater exploration because of its unique ability to maintain resolution at increasing ranges, a characteristic absent in conventional sonar techniques. However, the effective application of deep learning to SAS data processing is often limited due to the scarcity of labeled data. To address this challenge, this paper proposes MoCo-SAS that leverages self-supervised learning (SSL) for SAS data processing, classification, and pattern recognition. The experimental results demonstrate that MoCo-SAS significantly outperforms traditional supervised learning methods, as evidenced by significant improvements observed in terms of the F1-score. These findings highlight the potential of SSL in advancing the state-of-the-art in SAS data processing, offering promising avenues for enhanced underwater object detection and classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#27599;&#26085;&#27969;&#37327;&#20998;&#35299;&#20026;&#27599;&#23567;&#26102;&#27969;&#37327;&#65292;&#24182;&#22312;&#25386;&#23041;&#26576;&#27969;&#37327;&#27979;&#31449;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2308.11631</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27700;&#30005;&#31449;&#31649;&#29702;&#27969;&#37327;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based flow disaggregation for hydropower plant management. (arXiv:2308.11631v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#27599;&#26085;&#27969;&#37327;&#20998;&#35299;&#20026;&#27599;&#23567;&#26102;&#27969;&#37327;&#65292;&#24182;&#22312;&#25386;&#23041;&#26576;&#27969;&#37327;&#27979;&#31449;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#25968;&#25454;&#23545;&#20110;&#27700;&#30005;&#31449;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;&#25386;&#23041;&#27700;&#30005;&#31449;&#21482;&#26377;&#27599;&#26085;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#65292;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#31649;&#29702;&#65292;&#36890;&#24120;&#38656;&#35201;&#20122;&#26085;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#20122;&#26085;&#25968;&#25454;&#30340;&#26222;&#36941;&#32570;&#22833;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#25386;&#23041;&#26576;&#27969;&#37327;&#27979;&#31449;&#30340;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#23558;&#27599;&#26085;&#27969;&#37327;&#20998;&#35299;&#20026;&#27599;&#23567;&#26102;&#27969;&#37327;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
High temporal resolution data is a vital resource for hydropower plant management. Currently, only daily resolution data are available for most of Norwegian hydropower plant, however, to achieve more accurate management, sub-daily resolution data are often required. To deal with the wide absence of sub-daily data, time series disaggregation is a potential tool. In this study, we proposed a time series disaggregation model based on deep learning, the model is tested using flow data from a Norwegian flow station, to disaggregate the daily flow into hourly flow. Preliminary results show some promising aspects for the proposed model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20102;&#20809;&#23398;&#30697;&#38453;&#20056;&#27861;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#24314;&#27169;&#35823;&#24046;&#65292;&#22312;&#20165;&#20351;&#29992;25%&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23545;&#30697;&#38453;&#26435;&#37325;&#30340;&#23567;&#20110;1 dB&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.11630</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20809;&#23398;&#30697;&#38453;&#20056;&#27861;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning. (arXiv:2308.11630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20102;&#20809;&#23398;&#30697;&#38453;&#20056;&#27861;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#24314;&#27169;&#35823;&#24046;&#65292;&#22312;&#20165;&#20351;&#29992;25%&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23545;&#30697;&#38453;&#26435;&#37325;&#30340;&#23567;&#20110;1 dB&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#35757;&#32451;Mach-Zehnder&#24178;&#28041;&#20202;&#32593;&#29366;&#20809;&#23398;&#30697;&#38453;&#20056;&#27861;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#23454;&#39564;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#20174;&#19981;&#22826;&#20934;&#30830;&#30340;&#35299;&#26512;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20351;&#29992;&#35299;&#26512;&#27169;&#22411;&#25110;&#29420;&#31435;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24314;&#27169;&#35823;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#21644;&#38598;&#25104;&#24179;&#22343;&#65292;&#25105;&#20204;&#22312;&#20165;&#20351;&#29992;25%&#21487;&#29992;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#30001;&#20809;&#23376;&#33455;&#29255;&#23454;&#29616;&#30340;&#30697;&#38453;&#26435;&#37325;&#30340;&#23567;&#20110;1 dB&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve &lt; 1 dB root-mean-square error on the matrix weights implemented by a photonic chip while using only 25% of the available data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#35774;&#22791;&#32423;&#19978;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11624</link><description>&lt;p&gt;
&#29992;&#36890;&#29992;&#35774;&#22791;&#32534;&#30721;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#38761;&#26032;TCAD&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks. (arXiv:2308.11624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#35774;&#22791;&#32423;&#19978;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#26469;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#19981;&#20165;&#32771;&#34385;&#20102;&#26448;&#26009;&#32423;&#21644;&#22120;&#20214;&#32423;&#23884;&#20837;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31354;&#38388;&#20851;&#31995;&#30340;&#23884;&#20837;&#65292;&#21463;&#26377;&#38480;&#20803;&#32593;&#26684;&#20013;&#24120;&#29992;&#30340;&#25554;&#20540;&#25805;&#20316;&#21551;&#21457;&#32780;&#26469;&#12290;&#21033;&#29992;&#22120;&#20214;&#27169;&#25311;&#30340;&#36890;&#29992;&#29289;&#29702;&#23450;&#24459;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#21253;&#25324;&#22522;&#20110;&#27850;&#26494;&#20223;&#30495;&#30340;&#26367;&#20195;&#21644;&#22522;&#20110;&#28418;&#31227;&#25193;&#25955;&#27169;&#22411;&#30340;&#30005;&#27969;-&#30005;&#21387;&#65288;IV&#65289;&#39044;&#27979;&#12290;&#36825;&#20004;&#32773;&#37117;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;&#31216;&#20026;RelGAT&#65289;&#23454;&#29616;&#30340;&#12290;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;Sentaurus TCAD&#22120;&#20214;&#27169;&#25311;&#22120;&#30340;&#35814;&#32454;&#25216;&#26415;&#32454;&#33410;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#22312;&#35774;&#22791;&#32423;&#19978;&#37319;&#29992;&#25552;&#20986;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
An innovative methodology that leverages artificial intelligence (AI) and graph representation for semiconductor device encoding in TCAD device simulation is proposed. A graph-based universal encoding scheme is presented that not only considers material-level and device-level embeddings, but also introduces a novel spatial relationship embedding inspired by interpolation operations typically used in finite element meshing. Universal physical laws from device simulations are leveraged for comprehensive data-driven modeling, which encompasses surrogate Poisson emulation and current-voltage (IV) prediction based on drift-diffusion model. Both are achieved using a novel graph attention network, referred to as RelGAT. Comprehensive technical details based on the device simulator Sentaurus TCAD are presented, empowering researchers to adopt the proposed AI-driven Electronic Design Automation (EDA) solution at the device level.
&lt;/p&gt;</description></item><item><title>Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2308.11601</link><description>&lt;p&gt;
Tryage: &#23454;&#26102;&#26234;&#33021;&#36335;&#30001;&#29992;&#25143;&#25552;&#31034;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11601
&lt;/p&gt;
&lt;p&gt;
Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26550;&#26500;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#22312;Hugging Face&#29983;&#24577;&#31995;&#32479;&#20013;&#26377;&#36229;&#36807;200,000&#20010;&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#36873;&#25321;&#21644;&#20248;&#21270;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#26041;&#38754;&#30340;&#24037;&#20316;&#27969;&#31243;&#21644;&#25968;&#25454;&#39046;&#22495;&#30340;&#21516;&#26102;&#65292;&#36824;&#35201;&#35299;&#20915;&#35745;&#31639;&#12289;&#23433;&#20840;&#21644;&#26102;&#25928;&#24615;&#31561;&#38382;&#39064;&#12290;&#36843;&#20999;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#24182;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;Tryage&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36335;&#30001;&#22120;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#21463;&#22823;&#33041;&#20013;&#30340;&#19992;&#33041;&#36335;&#30001;&#22120;&#21551;&#21457;&#65292;Tryage&#37319;&#29992;&#24863;&#30693;&#36335;&#30001;&#22120;&#26469;&#39044;&#27979;&#19979;&#28216;&#27169;&#22411;&#22312;&#25552;&#31034;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20570;&#20986;&#36335;&#30001;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#19968;&#27454;&#25915;&#38450;&#28216;&#25103;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24207;&#21015;&#37329;&#34701;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#29616;&#29366;&#21644;&#21160;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27604;&#36187;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#38544;&#34255;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#29992;&#25143;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#30772;&#35299;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11406</link><description>&lt;p&gt;
&#35774;&#35745;&#19968;&#27454;&#25915;&#38450;&#28216;&#25103;&#65306;&#36890;&#36807;&#31454;&#20105;&#26469;&#22686;&#21152;&#37329;&#34701;&#20132;&#26131;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Designing an attack-defense game: how to increase robustness of financial transaction models via a competition. (arXiv:2308.11406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11406
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#19968;&#27454;&#25915;&#38450;&#28216;&#25103;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24207;&#21015;&#37329;&#34701;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#29616;&#29366;&#21644;&#21160;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27604;&#36187;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#38544;&#34255;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#29992;&#25143;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#30772;&#35299;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#37329;&#34701;&#39046;&#22495;&#24694;&#24847;&#25915;&#20987;&#39118;&#38505;&#19981;&#26029;&#21319;&#32423;&#21644;&#30001;&#27492;&#24341;&#21457;&#30340;&#20005;&#37325;&#25439;&#23475;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#31574;&#30053;&#21644;&#40065;&#26834;&#30340;&#38450;&#24481;&#26426;&#21046;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#38134;&#34892;&#26085;&#30410;&#24191;&#27867;&#37319;&#29992;&#26356;&#31934;&#30830;&#20294;&#28508;&#22312;&#33030;&#24369;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#19968;&#23041;&#32961;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#20351;&#29992;&#24207;&#21015;&#37329;&#34701;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#21160;&#24577;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27604;&#36187;&#65292;&#20801;&#35768;&#23545;&#29616;&#20195;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#36827;&#34892;&#36924;&#30495;&#32780;&#35814;&#32454;&#30340;&#30740;&#31350;&#12290;&#21442;&#19982;&#32773;&#30452;&#25509;&#31454;&#20105;&#65292;&#22240;&#27492;&#21487;&#33021;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#25509;&#36817;&#30495;&#23454;&#26465;&#20214;&#19979;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20998;&#26512;&#27604;&#36187;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#38544;&#34255;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#29992;&#25143;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#30772;&#35299;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical. The threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks. We aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.  To achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data. The participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions. Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11241</link><description>&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification. (arXiv:2308.11241v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wav2vec2&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;Transformer&#26550;&#26500;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#65292;&#36824;&#29992;&#20110;&#25972;&#20010;&#35821;&#38899;&#22788;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#26377;&#25928;&#31471;&#21040;&#31471;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21442;&#25968;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#26377;&#25928;&#27169;&#22411;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#26102;&#38388;&#38376;&#27744;&#21270;(Temporal Gate Pooling)&#65292;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;Conformer&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;BEST-RQ&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;VoxCeleb1&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#20165;&#26377;28.5M&#20010;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;85.9%&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;&#20855;&#26377;317.7M&#20010;&#21442;&#25968;&#30340;wav2vec2&#30456;&#24403;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/HarunoriKawano/speaker-identification-with-tgp&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11217</link><description>&lt;p&gt;
&#22823;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#33021;&#22815;&#20840;&#38754;&#24863;&#30693;&#21644;&#35782;&#21035;&#29289;&#29702;&#19990;&#30028;&#65292;&#24050;&#25104;&#20026;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;&#22312;&#29305;&#23450;&#24037;&#19994;&#39046;&#22495;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#22810;&#20010;&#20225;&#19994;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#20316;&#32773;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#27169;&#22411;&#26102;&#20195;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#22522;&#30784;&#21644;&#30446;&#26631;&#30340;&#25112;&#30053;&#36716;&#21464;&#65292;&#20197;&#21450;&#22312;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#28608;&#21169;&#26426;&#21046;&#26041;&#38754;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#39046;&#20808;&#20225;&#19994;&#22312;&#22478;&#24066;&#23433;&#20840;&#36816;&#33829;&#31649;&#29702;&#26041;&#38754;&#36129;&#29486;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#30693;&#35782;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#39640;&#25928;&#24615;&#33021;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31283;&#20581;&#24615;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#30452;&#25509;&#27979;&#37327;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25193;&#23637;&#22270;&#20687;&#25968;&#25454;&#38598;&#20197;&#23436;&#25104;&#36229;&#20986;&#22522;&#20934;&#33539;&#22260;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.10632</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#23548;&#21521;&#30340;&#31283;&#20581;&#24615;: &#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31283;&#20581;&#24615;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models. (arXiv:2308.10632v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31283;&#20581;&#24615;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#30452;&#25509;&#27979;&#37327;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25193;&#23637;&#22270;&#20687;&#25968;&#25454;&#38598;&#20197;&#23436;&#25104;&#36229;&#20986;&#22522;&#20934;&#33539;&#22260;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#22266;&#23450;&#22522;&#20934;&#19978;&#30340;&#20998;&#25968;&#26159;&#21542;&#36275;&#20197;&#20805;&#20998;&#20307;&#29616;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#20173;&#26377;&#35752;&#35770;&#12290;&#23454;&#38469;&#19978;&#65292;&#29702;&#24819;&#30340;&#31283;&#20581;&#27169;&#22411;&#21487;&#33021;&#19982;&#31070;&#35861;&#65288;&#20363;&#22914;&#65292;&#20154;&#31867;&#29992;&#25143;&#65289;&#34920;&#29616;&#31867;&#20284;&#65292;&#22240;&#27492;&#19968;&#20010;&#22909;&#30340;&#35780;&#20272;&#21327;&#35758;&#21487;&#33021;&#26159;&#35780;&#20272;&#27169;&#22411;&#30456;&#23545;&#20110;&#31070;&#35861;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31283;&#20581;&#24615;&#27979;&#37327;&#26041;&#27861;&#65292;&#30452;&#25509;&#27979;&#37327;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30456;&#23545;&#20110;&#26367;&#20195;&#31070;&#35861;&#65288;&#21363;&#22522;&#30784;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#33539;&#22260;&#20043;&#22806;&#23436;&#25104;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#20855;&#26377;&#36275;&#22815;&#25200;&#21160;&#30340;&#26032;&#26679;&#26412;&#26469;&#25193;&#23637;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#26679;&#26412;&#19982;&#21407;&#22987;&#38598;&#21512;&#20013;&#30340;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#20173;&#38480;&#21046;&#22312;&#21407;&#22987;&#27979;&#35797;&#22270;&#20687;&#25152;&#20195;&#34920;&#30340;&#30456;&#21516;&#22270;&#20687;-&#26631;&#31614;&#32467;&#26500;&#20869;&#65292;&#30001;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#38480;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has demonstrated remarkable performance over finite datasets, yet whether the scores over the fixed benchmarks can sufficiently indicate the model's performance in the real world is still in discussion. In reality, an ideal robust model will probably behave similarly to the oracle (e.g., the human users), thus a good evaluation protocol is probably to evaluate the models' behaviors in comparison to the oracle. In this paper, we introduce a new robustness measurement that directly measures the image classification model's performance compared with a surrogate oracle (i.e., a foundation model). Besides, we design a simple method that can accomplish the evaluation beyond the scope of the benchmarks. Our method extends the image datasets with new samples that are sufficiently perturbed to be distinct from the ones in the original sets, but are still bounded within the same image-label structure the original test image represents, constrained by a foundation model pretraine
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22522;&#20110;&#27492;&#26694;&#26550;&#26500;&#24314;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#22810;&#35270;&#22270;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.10522</link><description>&lt;p&gt;
&#20449;&#24687;&#29702;&#35770;&#24341;&#23548;&#30340;&#21551;&#21457;&#24335;&#28176;&#36827;&#24335;&#22810;&#35270;&#22270;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2308.10522v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10522
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22522;&#20110;&#27492;&#26694;&#26550;&#26500;&#24314;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#22810;&#35270;&#22270;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20174;&#22810;&#20010;&#35270;&#22270;&#20013;&#25429;&#33719;&#20849;&#20139;&#19978;&#19979;&#25991;&#30340;&#20840;&#38754;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30452;&#35266;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#19981;&#21516;&#35270;&#22270;&#30340;&#25104;&#23545;&#26041;&#24335;&#65292;&#36825;&#20173;&#28982;&#21487;&#25193;&#23637;&#65306;&#23398;&#20064;&#35270;&#22270;&#20849;&#20139;&#34920;&#31034;&#26102;&#26410;&#36807;&#28388;&#35270;&#22270;&#29305;&#23450;&#30340;&#22122;&#22768;&#65307;&#34394;&#20551;&#30340;&#36127;&#23545;&#20013;&#65292;&#36127;&#39033;&#23454;&#38469;&#19978;&#22312;&#21516;&#19968;&#31867;&#21035;&#20013;&#19982;&#27491;&#39033;&#30456;&#21516;&#65292;&#24182;&#19988;&#30495;&#27491;&#30340;&#36127;&#23545;&#34987;&#21516;&#31561;&#23545;&#24453;&#65307;&#22343;&#21248;&#22320;&#34913;&#37327;&#26415;&#35821;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21487;&#33021;&#24178;&#25200;&#20248;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#30740;&#31350;&#24191;&#20041;&#33258;&#30417;&#30563;&#22810;&#35270;&#22270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22810;&#20110;&#20004;&#20010;&#35270;&#22270;&#30340;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#20102;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#33539;&#24335;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#22810;&#35270;&#22270;&#23398;&#20064;&#12290;&#22312;&#20854;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#19977;&#23618;&#28176;&#36827;&#24335;&#32467;&#26500;&#30340;&#22810;&#35270;&#22270;&#32534;&#30721;&#26041;&#27861;&#65292;&#21363;In
&lt;/p&gt;
&lt;p&gt;
Multi-view representation learning aims to capture comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning to different views in a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; evenly measuring the similarities between terms might interfere with optimization. Importantly, few works study the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the perspective of information theory and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive architecture, namely In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#31616;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;STLinear&#65292;&#36890;&#36807;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#12289;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#21644;&#21608;&#26399;&#24615;&#23398;&#20064;&#35299;&#20915;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#36164;&#28304;&#23494;&#38598;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21482;&#20351;&#29992;&#32447;&#24615;&#23618;&#65292;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#39046;&#20808;&#30340;STGNN&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#65292;&#35745;&#31639;&#38656;&#27714;&#26174;&#33879;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2308.10276</link><description>&lt;p&gt;
&#26497;&#31616;&#20132;&#36890;&#39044;&#27979;&#65306;&#21482;&#38656;&#32447;&#24615;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimalist Traffic Prediction: Linear Layer Is All You Need. (arXiv:2308.10276v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#31616;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;STLinear&#65292;&#36890;&#36807;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#12289;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#21644;&#21608;&#26399;&#24615;&#23398;&#20064;&#35299;&#20915;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#36164;&#28304;&#23494;&#38598;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21482;&#20351;&#29992;&#32447;&#24615;&#23618;&#65292;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#39046;&#20808;&#30340;STGNN&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#65292;&#35745;&#31639;&#38656;&#27714;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#23545;&#20110;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#21644;&#26234;&#33021;&#22478;&#24066;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#36890;&#36807;&#34701;&#21512;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#25110;Transformer&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23637;&#31034;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#20294;&#38754;&#20020;&#30528;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#26799;&#24230;&#38382;&#39064;&#21644;&#36164;&#28304;&#23494;&#38598;&#24615;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#12289;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#21644;&#21608;&#26399;&#24615;&#23398;&#20064;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;STLinear&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26497;&#31616;&#27169;&#22411;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;STGNN&#19981;&#21516;&#65292;STLinear&#23436;&#20840;&#22312;&#26412;&#22320;&#36816;&#34892;&#65292;&#36991;&#20813;&#20102;&#33410;&#28857;&#38388;&#30340;&#25968;&#25454;&#20132;&#25442;&#65292;&#20165;&#20381;&#36182;&#32447;&#24615;&#23618;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#23454;&#20102;STLinear&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#39046;&#20808;&#30340;STGNN&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#35745;&#31639;&#38656;&#27714;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is essential for the progression of Intelligent Transportation Systems (ITS) and the vision of smart cities. While Spatial-Temporal Graph Neural Networks (STGNNs) have shown promise in this domain by leveraging Graph Neural Networks (GNNs) integrated with either RNNs or Transformers, they present challenges such as computational complexity, gradient issues, and resource-intensiveness. This paper addresses these challenges, advocating for three main solutions: a node-embedding approach, time series decomposition, and periodicity learning. We introduce STLinear, a minimalist model architecture designed for optimized efficiency and performance. Unlike traditional STGNNs, STlinear operates fully locally, avoiding inter-node data exchanges, and relies exclusively on linear layers, drastically cutting computational demands. Our empirical studies on real-world datasets confirm STLinear's prowess, matching or exceeding the accuracy of leading STGNNs, but with significantly r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07221</link><description>&lt;p&gt;
AudioFormer: &#36890;&#36807;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#38899;&#39057;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07221
&lt;/p&gt;
&lt;p&gt;
AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioFormer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#26469;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#38543;&#21518;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#30340;&#24418;&#24335;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411; (MLM)&#65292;&#20174;&#32780;&#33719;&#24471;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#21019;&#20102;&#19968;&#31181;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604; (MPC) &#23398;&#20064;&#26041;&#27861;&#30340;&#25972;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21516;&#19968;&#38899;&#39057;&#36755;&#20837;&#20013;&#22810;&#20010;&#31163;&#25955;&#22768;&#23398;&#20195;&#30721;&#38388;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#35270;&#20026;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#22635;&#31354;&#39064;&#30340;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#26368;&#32456;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MPC&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;16&#20493;&#30340;&#25512;&#29702;&#36895;&#24230;&#25552;&#21319;&#65292;&#20165;&#26377;0.019%&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;</title><link>http://arxiv.org/abs/2308.06644</link><description>&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#33976;&#39311;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation. (arXiv:2308.06644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;16&#20493;&#30340;&#25512;&#29702;&#36895;&#24230;&#25552;&#21319;&#65292;&#20165;&#26377;0.019%&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#30340;NP&#23436;&#20840;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#36845;&#20195;&#35780;&#20272;&#29305;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25512;&#29702;&#19978;&#24120;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#20165;&#22312;&#21333;&#27493;&#20869;&#39044;&#27979;&#20004;&#20010;&#27493;&#39588;&#20043;&#21069;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#36880;&#27493;&#33976;&#39311;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#36895;&#24230;&#24555;&#20102;16&#20493;&#65292;&#24615;&#33021;&#20165;&#26377;0.019%&#30340;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06857</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#26041;&#27861;&#29992;&#20110;&#26080;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#33258;&#27965;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#31572;&#26696;&#30340;&#25552;&#31034;&#65292;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;&#30340;&#33258;&#27965;&#24615;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#36229;&#36234;&#20102;&#22266;&#23450;&#31572;&#26696;&#38382;&#39064;&#30340;&#33539;&#22260;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21363;&#20351;&#27809;&#26377;&#35775;&#38382;&#21040;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#20063;&#33021;&#22312;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#21644;&#19968;&#33268;&#22320;&#25913;&#36827;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20877;&#25490;&#24207;&#27169;&#22411;&#25110;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#36817;&#26399;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00252</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An ML approach to resolution of singularities. (arXiv:2307.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#36817;&#26399;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#26041;&#31243;&#32452;&#30340;&#35299;&#38598;&#36890;&#24120;&#21253;&#21547;&#19981;&#20809;&#28369;&#12289;&#22855;&#24322;&#30340;&#28857;&#12290;&#35299;&#20915;&#22855;&#28857;&#26159;&#20960;&#20309;&#20013;&#30340;&#22522;&#26412;&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#22855;&#28857;&#26367;&#25442;&#20026;&#20809;&#28369;&#28857;&#65292;&#21516;&#26102;&#20445;&#25345;&#35299;&#38598;&#30340;&#21097;&#20313;&#37096;&#20998;&#19981;&#21464;&#12290;&#35299;&#20915;&#22855;&#28857;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#65306;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#21453;&#22797;&#36827;&#34892;&#34987;&#31216;&#20026;&#8220;blowing-up&#8221;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#35299;&#20915;&#30340;&#22797;&#26434;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#26576;&#20123;&#36873;&#25321;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#36716;&#21270;&#25104;&#19981;&#21516;&#29256;&#26412;&#30340;&#20004;&#20154;&#21338;&#24328;&#65292;&#21363;&#25152;&#35859;&#30340;Hironaka&#28216;&#25103;&#65292;&#32780;&#31532;&#19968;&#20301;&#29609;&#23478;&#30340;&#33719;&#32988;&#31574;&#30053;&#25552;&#20379;&#20102;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#30340;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Hironaka&#28216;&#25103;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23547;&#25214;&#22855;&#28857;&#30340;&#26368;&#20248;&#35299;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36825;&#35777;&#26126;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution set of a system of polynomial equations typically contains ill-behaved, singular points. Resolution is a fundamental process in geometry in which we replace singular points with smooth points, while keeping the rest of the solution set unchanged. Resolutions are not unique: the usual way to describe them involves repeatedly performing a fundamental operation known as "blowing-up", and the complexity of the resolution highly depends on certain choices. The process can be translated into various versions of a 2-player game, the so-called Hironaka game, and a winning strategy for the first player provides a solution to the resolution problem. In this paper we introduce a new approach to the Hironaka game that uses reinforcement learning agents to find optimal resolutions of singularities. In certain domains, the trained model outperforms state-of-the-art selection heuristics in total number of polynomial additions performed, which provides a proof-of-concept that recent devel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#37197;&#21644;&#20998;&#31867;&#36719;&#20214;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#30456;&#20851;&#30340;&#22242;&#38431;&#25104;&#21592;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20197;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00009</link><description>&lt;p&gt;
&#33258;&#21160;&#20998;&#37197;&#21644;&#20998;&#31867;&#36719;&#20214;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Automated Assignment and Classification of Software Issues. (arXiv:2307.00009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#37197;&#21644;&#20998;&#31867;&#36719;&#20214;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#30456;&#20851;&#30340;&#22242;&#38431;&#25104;&#21592;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20197;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#38382;&#39064;&#21253;&#21547;&#20462;&#22797;&#12289;&#25913;&#36827;&#25110;&#21019;&#24314;&#26032;&#32447;&#31243;&#30340;&#24037;&#20316;&#21333;&#20803;&#65292;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#20419;&#36827;&#22242;&#38431;&#25104;&#21592;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#30456;&#20851;&#30340;&#22242;&#38431;&#25104;&#21592;&#24182;&#30830;&#23450;&#38382;&#39064;&#30340;&#31867;&#21035;&#26159;&#19968;&#39033;&#32321;&#29712;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38169;&#35823;&#30340;&#20998;&#31867;&#20250;&#23548;&#33268;&#39033;&#30446;&#24310;&#36831;&#21644;&#37325;&#26032;&#24037;&#20316;&#65292;&#32473;&#22242;&#38431;&#25104;&#21592;&#24102;&#26469;&#40635;&#28902;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#29992;&#20110;&#27973;&#23618;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#23558;&#27973;&#23618;&#26041;&#27861;&#21644;&#38598;&#25104;&#26041;&#27861;&#19982;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#22235;&#31181;&#35282;&#33394;&#65288;&#35774;&#35745;&#24072;&#12289;&#24320;&#21457;&#20154;&#21592;&#12289;&#27979;&#35797;&#20154;&#21592;&#21644;&#39046;&#23548;&#32773;&#65289;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#20010;&#20154;&#25110;&#22242;&#38431;&#65292;&#20197;&#20419;&#36827;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#24320;&#21457;&#20154;&#21592;&#30340;&#32463;&#39564;&#27700;&#24179;&#65292;&#20197;&#21453;&#26144;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#24037;&#19994;&#23454;&#36341;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#31867;&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#21253;&#25324;&#38169;&#35823;&#12289;&#26032;&#21151;&#33021;&#12289;&#25913;&#36827;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software issues contain units of work to fix, improve or create new threads during the development and facilitate communication among the team members. Assigning an issue to the most relevant team member and determining a category of an issue is a tedious and challenging task. Wrong classifications cause delays and rework in the project and trouble among the team members. This thesis proposes a set of carefully curated linguistic features for shallow machine learning methods and compares the performance of shallow and ensemble methods with deep language models. Unlike the state-of-the-art, we assign issues to four roles (designer, developer, tester, and leader) rather than to specific individuals or teams to contribute to the generality of our solution. We also consider the level of experience of the developers to reflect the industrial practices in our solution formulation. We employ a classification approach to categorize issues into distinct classes, namely bug, new feature, improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.15782</link><description>&lt;p&gt;
UTRNet: &#21360;&#21047;&#25991;&#26723;&#20013;&#39640;&#20998;&#36776;&#29575;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#23610;&#24230;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;UTRNet&#26550;&#26500;&#65292;&#19968;&#20010;&#28151;&#21512;CNN-RNN&#27169;&#22411;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#24037;&#20316;&#24456;&#38590;&#25512;&#24191;&#21040;&#20044;&#23572;&#37117;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#36275;&#22815;&#30340;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UTRSet-Real&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;11,000&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;UTRSet-Synth&#65292;&#19968;&#20010;&#19982;&#23454;&#38469;&#19990;&#30028;&#38750;&#24120;&#30456;&#20284;&#30340;&#21547;&#26377;20,000&#34892;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;IIITH&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#30495;&#23454;&#24615;&#36827;&#34892;&#20102;&#20462;&#27491;&#65292;&#20351;&#20854;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#26356;&#21487;&#38752;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;UrduDoc&#65292;&#19968;&#31181;&#29992;&#20110;&#25195;&#25551;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#24037;&#20855;&#65292;&#36890;&#36807;&#23558;UTRNet&#19982;&#25991;&#26412;&#30340;&#31471;&#21040;&#31471;&#20044;&#23572;&#37117;OCR&#38598;&#25104;&#22312;&#21360;&#21047;&#25991;&#26723;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multi-scale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a tex
&lt;/p&gt;</description></item><item><title>U-TOE&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#20302;&#21151;&#32791;&#29289;&#32852;&#32593;TinyML&#23616;&#37096;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#32467;&#21512;&#20102;&#20302;&#21151;&#32791;&#23884;&#20837;&#24335;&#25805;&#20316;&#31995;&#32479;&#12289;&#27169;&#22411;&#36716;&#25442;&#22120;&#21644;&#32534;&#35793;&#22120;&#12289;&#24615;&#33021;&#27979;&#37327;&#27169;&#22359;&#21644;&#36828;&#31243;&#29289;&#32852;&#32593;&#27979;&#35797;&#24179;&#21488;&#30340;&#21151;&#33021;&#12290;&#33021;&#22815;&#24110;&#21161;&#29289;&#32852;&#32593;&#35774;&#35745;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#22312;&#20302;&#21151;&#32791;&#29289;&#32852;&#32593;&#30828;&#20214;&#19978;&#25191;&#34892;&#21508;&#31181;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14574</link><description>&lt;p&gt;
U-TOE: &#20302;&#21151;&#32791;&#29289;&#32852;&#32593;&#36890;&#29992;TinyML&#23616;&#37096;&#35780;&#20272;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
U-TOE: Universal TinyML On-board Evaluation Toolkit for Low-Power IoT. (arXiv:2306.14574v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14574
&lt;/p&gt;
&lt;p&gt;
U-TOE&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#20302;&#21151;&#32791;&#29289;&#32852;&#32593;TinyML&#23616;&#37096;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#32467;&#21512;&#20102;&#20302;&#21151;&#32791;&#23884;&#20837;&#24335;&#25805;&#20316;&#31995;&#32479;&#12289;&#27169;&#22411;&#36716;&#25442;&#22120;&#21644;&#32534;&#35793;&#22120;&#12289;&#24615;&#33021;&#27979;&#37327;&#27169;&#22359;&#21644;&#36828;&#31243;&#29289;&#32852;&#32593;&#27979;&#35797;&#24179;&#21488;&#30340;&#21151;&#33021;&#12290;&#33021;&#22815;&#24110;&#21161;&#29289;&#32852;&#32593;&#35774;&#35745;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#22312;&#20302;&#21151;&#32791;&#29289;&#32852;&#32593;&#30828;&#20214;&#19978;&#25191;&#34892;&#21508;&#31181;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TinyML&#31038;&#21306;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#23567;&#22411;&#22522;&#20110;&#24494;&#25511;&#21046;&#22120;&#30340;&#35774;&#22791;&#65292;&#20063;&#21487;&#20197;&#30452;&#25509;&#22312;&#32456;&#31471;&#19978;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35813;&#39046;&#22495;&#30340;&#20174;&#19994;&#32773;&#32570;&#20047;&#20415;&#25463;&#30340;&#19968;&#20307;&#21270;&#24037;&#20855;&#21253;&#65292;&#24110;&#21161;&#20182;&#20204;&#35780;&#20272;&#22312;&#20219;&#24847;&#20302;&#21151;&#32791;&#29289;&#32852;&#32593;&#30828;&#20214;&#19978;&#25191;&#34892;&#20219;&#24847;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;U-TOE&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#24037;&#20855;&#21253;&#65292;&#25105;&#20204;&#35774;&#35745;&#23427;&#26469;&#31616;&#21270;&#29289;&#32852;&#32593;&#35774;&#35745;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#24037;&#20316;&#65292;&#23427;&#32467;&#21512;&#20102;&#20302;&#21151;&#32791;&#23884;&#20837;&#24335;&#25805;&#20316;&#31995;&#32479;&#12289;&#36890;&#29992;&#27169;&#22411;&#36716;&#25442;&#22120;&#21644;&#32534;&#35793;&#22120;&#12289;&#38598;&#25104;&#24615;&#33021;&#27979;&#37327;&#27169;&#22359;&#21644;&#24320;&#25918;&#24335;&#36828;&#31243;&#29289;&#32852;&#32593;&#27979;&#35797;&#24179;&#21488;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;U-TOE&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#24182;&#28436;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22522;&#20110;&#27969;&#34892;&#24494;&#25511;&#21046;&#22120;&#26550;&#26500;&#30340;&#20302;&#21151;&#32791;&#29289;&#32852;&#32593;&#26495;&#19978;&#23454;&#39564;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#24615;&#33021;&#30340;&#29992;&#36884;&#12290;U-TOE&#21487;&#23454;&#29616;&#26131;&#20110;&#22797;&#29616;&#21644;&#21487;&#23450;&#21046;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Results from the TinyML community demonstrate that, it is possible to execute machine learning models directly on the terminals themselves, even if these are small microcontroller-based devices. However, to date, practitioners in the domain lack convenient all-in-one toolkits to help them evaluate the feasibility of executing arbitrary models on arbitrary low-power IoT hardware. To this effect, we present in this paper U-TOE, a universal toolkit we designed to facilitate the task of IoT designers and researchers, by combining functionalities from a low-power embedded OS, a generic model transpiler and compiler, an integrated performance measurement module, and an open-access remote IoT testbed. We provide an open source implementation of U-TOE and we demonstrate its use to experimentally evaluate the performance of various models, on a wide variety of low-power IoT boards, based on popular microcontroller architectures. U-TOE allows easily reproducible and customizable comparative eval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#38181;&#26494;&#24347;&#19979;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.04647</link><description>&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;&#65306;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compressed Sensing: A Discrete Optimization Approach. (arXiv:2306.04647v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#38181;&#26494;&#24347;&#19979;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#21363;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#35813;&#21521;&#37327;&#28385;&#36275;&#19968;&#32452;&#32447;&#24615;&#27979;&#37327;&#65292;&#21516;&#26102;&#36798;&#21040;&#19968;&#23450;&#30340;&#25968;&#20540;&#23481;&#38480;&#12290;&#21387;&#32553;&#24863;&#30693;&#26159;&#32479;&#35745;&#23398;&#12289;&#36816;&#31609;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#20449;&#21495;&#22788;&#29702;&#12289;&#25968;&#25454;&#21387;&#32553;&#21644;&#22270;&#20687;&#37325;&#24314;&#31561;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;$\ell_2$&#27491;&#21017;&#21270;&#30340;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#38181;&#35268;&#21010;&#26469;&#37325;&#26032;&#23450;&#20041;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#27492;&#38382;&#39064;&#30340;&#20108;&#27425;&#38181;&#26494;&#24347;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#28201;&#21644;&#38480;&#21046;&#19979;&#65292;&#24471;&#21040;&#30340;&#26494;&#24347;&#31561;&#20215;&#20110;&#28145;&#20837;&#30740;&#31350;&#30340;&#22522;&#30784;&#36861;&#36394;&#21435;&#22122;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#23450;&#26494;&#24347;&#26469;&#21152;&#24378;&#20108;&#27425;&#38181;&#26494;&#24347;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#20108;&#27425;&#38181;&#26494;&#24347;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#20197;&#30830;&#35777;&#30340;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#31934;&#30830;&#30340;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Compressed Sensing (CS) problem, which is the problem of finding the most sparse vector that satisfies a set of linear measurements up to some numerical tolerance. CS is a central problem in Statistics, Operations Research and Machine Learning which arises in applications such as signal processing, data compression and image reconstruction. We introduce an $\ell_2$ regularized formulation of CS which we reformulate as a mixed integer second order cone program. We derive a second order cone relaxation of this problem and show that under mild conditions on the regularization parameter, the resulting relaxation is equivalent to the well studied basis pursuit denoising problem. We present a semidefinite relaxation that strengthens the second order cone relaxation and develop a custom branch-and-bound algorithm that leverages our second order cone relaxation to solve instances of CS to certifiable optimality. Our numerical results show that our approach produces solutions that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01792</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#31995;&#24863;&#30693;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26159;&#22522;&#20110;&#20854;&#36807;&#21435;&#34892;&#20026;&#23398;&#20064;&#23558;&#29992;&#25143;&#34920;&#31034;&#20026;&#20302;&#32500;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23427;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#24314;&#27169;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20026;&#21333;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#21363;&#19982;&#22810;&#31181;&#20219;&#21153;&#30456;&#20851;&#30340;&#26356;&#24191;&#20041;&#29992;&#25143;&#34920;&#31034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38656;&#27714;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#20197;&#21450;&#20026;&#25345;&#32493;&#28155;&#21152;&#30340;&#20219;&#21153;&#25552;&#20379;&#26377;&#38480;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#19981;&#21463;&#20219;&#21153;&#25968;&#37327;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23398;&#20064;&#35270;&#39057;&#21387;&#32553;&#26102;&#65292;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#20110;&#37325;&#24314;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36873;&#25321; PLF-JD &#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#36328;&#24103;&#26102;&#24207;&#30456;&#20851;&#24615;&#65292;&#20294;&#21516;&#26102;&#20250;&#24102;&#26469;&#26356;&#22823;&#30340;&#22833;&#30495;&#24809;&#32602;&#21644;&#26356;&#38590;&#20197;&#32416;&#27491;&#26089;&#26399;&#36755;&#20986;&#24103;&#20013;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.19301</link><description>&lt;p&gt;
&#20851;&#20110;&#23398;&#20064;&#35270;&#39057;&#21387;&#32553;&#20013;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
On the Choice of Perception Loss Function for Learned Video Compression. (arXiv:2305.19301v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23398;&#20064;&#35270;&#39057;&#21387;&#32553;&#26102;&#65292;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#20110;&#37325;&#24314;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36873;&#25321; PLF-JD &#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#36328;&#24103;&#26102;&#24207;&#30456;&#20851;&#24615;&#65292;&#20294;&#21516;&#26102;&#20250;&#24102;&#26469;&#26356;&#22823;&#30340;&#22833;&#30495;&#24809;&#32602;&#21644;&#26356;&#38590;&#20197;&#32416;&#27491;&#26089;&#26399;&#36755;&#20986;&#24103;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20302;&#24310;&#36831;&#12289;&#39034;&#24207;&#35270;&#39057;&#21387;&#32553;&#19979;&#21463;&#21040;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#22833;&#30495;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65288;&#20197;&#23454;&#29616;&#30495;&#23454;&#24863;&#20026;&#30446;&#26631;&#65289;&#30340;&#36755;&#20986;&#35774;&#35745;&#20102;&#30740;&#31350;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65288;PLFs&#65289;&#65292;&#20998;&#21035;&#26159;PLF-JD&#21644;PLF-FMD&#12290;&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#26512;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102; PLF &#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#37325;&#24314;&#25928;&#26524;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#27604;&#29305;&#29575;&#30340;&#24773;&#20917;&#19979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#20445;&#30041;&#36328;&#24103;&#26102;&#24207;&#30456;&#20851;&#24615;&#26041;&#38754;&#65292;&#22522;&#20110;PLF-JD &#30340;&#37325;&#24314;&#25928;&#26524;&#20250;&#26356;&#22909;&#65292;&#20294;&#19982; PLF-FMD &#30456;&#27604;&#65292;&#20063;&#20250;&#24102;&#26469;&#26356;&#22823;&#30340;&#22833;&#30495;&#24809;&#32602;&#65292;&#24182;&#20351;&#20854;&#26356;&#38590;&#20197;&#24674;&#22797;&#26089;&#26399;&#36755;&#20986;&#24103;&#20013;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study causal, low-latency, sequential video compression when the output is subjected to both a mean squared-error (MSE) distortion loss as well as a perception loss to target realism. Motivated by prior approaches, we consider two different perception loss functions (PLFs). The first, PLF-JD, considers the joint distribution (JD) of all the video frames up to the current one, while the second metric, PLF-FMD, considers the framewise marginal distributions (FMD) between the source and reconstruction. Using information theoretic analysis and deep-learning based experiments, we demonstrate that the choice of PLF can have a significant effect on the reconstruction, especially at low-bit rates. In particular, while the reconstruction based on PLF-JD can better preserve the temporal correlation across frames, it also imposes a significant penalty in distortion compared to PLF-FMD and further makes it more difficult to recover from errors made in the earlier output frames. Although the cho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SCPT&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#31354;&#38388;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#26410;&#35265;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#26032;&#36947;&#36335;&#30340;&#20132;&#36890;&#39044;&#27979;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.05237</link><description>&lt;p&gt;
&#21033;&#29992;&#31354;&#38388;&#23545;&#27604;&#39044;&#35757;&#32451;&#36827;&#34892;&#26410;&#35265;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#36947;&#36335;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training. (arXiv:2305.05237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SCPT&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#31354;&#38388;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#26410;&#35265;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#26032;&#36947;&#36335;&#30340;&#20132;&#36890;&#39044;&#27979;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#20250;&#19981;&#26029;&#24314;&#35774;&#26032;&#30340;&#36947;&#36335;&#65292;&#20294;&#26159;&#20043;&#21069;&#30340;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#26032;&#36947;&#36335;&#65288;&#26410;&#35265;&#25968;&#25454;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#26102;&#31354;&#65288;ST&#65289;&#20998;&#21106;&#30340;&#26032;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#19968;&#37096;&#20998;&#30340;&#36947;&#36335;&#25968;&#25454;&#65292;&#20294;&#27979;&#35797;&#26102;&#20351;&#29992;&#26410;&#35265;&#25968;&#25454;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#31354;&#38388;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;SCPT&#65289;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#26469;&#25552;&#21462;&#25512;&#29702;&#26102;&#26410;&#35265;&#36947;&#36335;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#36825;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#26159;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#31354;&#38388;&#32534;&#30721;&#22120;&#20165;&#38656;&#35201;&#26032;&#36947;&#36335;&#30340;&#20004;&#22825;&#20132;&#36890;&#25968;&#25454;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31354;&#38388;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#25512;&#26029;&#26410;&#35265;&#36947;&#36335;&#19978;&#30340;&#28508;&#22312;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
New roads are being constructed all the time. However, the capabilities of previous deep forecasting models to generalize to new roads not seen in the training data (unseen roads) are rarely explored. In this paper, we introduce a novel setup called a spatio-temporal (ST) split to evaluate the models' capabilities to generalize to unseen roads. In this setup, the models are trained on data from a sample of roads, but tested on roads not seen in the training data. Moreover, we also present a novel framework called Spatial Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to extract latent features from unseen roads during inference time. This spatial encoder is pre-trained using contrastive learning. During inference, the spatial encoder only requires two days of traffic data on the new roads and does not require any re-training. We also show that the output from the spatial encoder can be used effectively to infer latent node embeddings on unseen roads during 
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#38598;&#33976;&#39311;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#39640;&#20449;&#24687;&#23494;&#24230;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35843;&#26597;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#24182;&#31995;&#32479;&#22238;&#39038;&#20102;&#25968;&#25454;&#27169;&#24577;&#21644;&#30456;&#20851;&#24212;&#29992;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25361;&#25112;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.01975</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#30740;&#31350;&#32508;&#36848;: &#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01975
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#39640;&#20449;&#24687;&#23494;&#24230;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35843;&#26597;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#24182;&#31995;&#32479;&#22238;&#39038;&#20102;&#25968;&#25454;&#27169;&#24577;&#21644;&#30456;&#20851;&#24212;&#29992;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25361;&#25112;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#38598;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25104;&#26412;&#36234;&#26469;&#36234;&#39640;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36890;&#36807;&#21512;&#25104;&#39640;&#20449;&#24687;&#23494;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#24212;&#29992;&#65292;&#21253;&#25324;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#23545;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#28982;&#21518;&#31995;&#32479;&#22320;&#22238;&#39038;&#25968;&#25454;&#27169;&#24577;&#21644;&#30456;&#20851;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25361;&#25112;&#24182;&#35752;&#35770;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#30340;&#26234;&#33021;&#19988;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#31995;&#32479;&#25277;&#35937;&#25216;&#26415;&#65292;&#24182;&#19988;&#23450;&#20041;&#20102;&#19968;&#31181;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#24230;&#37327;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17618</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#30340;&#25968;&#25454;&#39537;&#21160;&#25277;&#35937;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version]. (arXiv:2303.17618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17618
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#30340;&#26234;&#33021;&#19988;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#31995;&#32479;&#25277;&#35937;&#25216;&#26415;&#65292;&#24182;&#19988;&#23450;&#20041;&#20102;&#19968;&#31181;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#24230;&#37327;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#19988;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#31995;&#32479;&#25277;&#35937;&#33258;&#36866;&#24212;&#32454;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#26681;&#25454;&#26410;&#26469;&#36755;&#20986;&#30340;&#35266;&#23519;&#23558;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#26159;&#21160;&#24577;&#22320;&#20197;&#19981;&#23545;&#31216;&#30340;&#26041;&#24335;&#26500;&#24314;&#30340;&#12290;&#20026;&#20102;&#23398;&#20064;&#26368;&#20248;&#32467;&#26500;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36866;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#20294;&#19981;&#21463;&#38480;&#20110;&#27492;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#19978;&#36848;&#24230;&#37327;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#21487;&#33021;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#35813;&#24230;&#37327;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#20256;&#32479;&#30340;&#32447;&#24615;&#35268;&#21010;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an adaptive refinement procedure for smart, and scalable abstraction of dynamical systems. Our technique relies on partitioning the state space depending on the observation of future outputs. However, this knowledge is dynamically constructed in an adaptive, asymmetric way. In order to learn the optimal structure, we define a Kantorovich-inspired metric between Markov chains, and we use it as a loss function. Our technique is prone to data-driven frameworks, but not restricted to.  We also study properties of the above mentioned metric between Markov chains, which we believe could be of application for wider purpose. We propose an algorithm to approximate it, and we show that our method yields a much better computational complexity than using classical linear programming techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;</title><link>http://arxiv.org/abs/2303.11702</link><description>&lt;p&gt;
&#36830;&#25509;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25506;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#20197;&#21069;&#27809;&#26377;&#27491;&#24335;&#23558;SSL&#21644;OSR&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#21508;&#33258;&#30340;&#26041;&#27861;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSL-GAN&#21644;OSR-GAN&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;SSL&#21644;OSR&#20998;&#31867;&#22120;&#37117;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;SSL&#21644;OSR&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;SSL-GAN&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;OSR-GAN&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#29486;&#22522;&#30784;&#26356;&#21152;&#29282;&#22266;&#30340;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;&#26576;&#20123;&#19968;&#33324;&#30340;OSR&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;OSR&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#20114;&#24800;&#28857;&#65288;ARP&#65289;-GAN&#22312;&#19968;&#20123;OSR&#20219;&#21153;&#20013;&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#23558;&#24515;&#34880;&#31649;&#23454;&#39564;&#23460;&#25351;&#26631;&#30340;&#24739;&#32773;&#36827;&#23637;&#36235;&#21183;&#20174;&#24120;&#35265;&#24773;&#20917;&#36716;&#31227;&#21040;&#32597;&#35265;&#25110;&#29305;&#23450;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#20013;&#65292;&#20197;&#21327;&#21161;&#26816;&#27979;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#27835;&#30103;&#24739;&#32773;&#30340;&#38774;&#34880;&#31649;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.06980</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#36890;&#29992;&#23454;&#39564;&#23460;&#36827;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-supervised based general laboratory progress pretrained model for cardiovascular event detection. (arXiv:2303.06980v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06980
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#23558;&#24515;&#34880;&#31649;&#23454;&#39564;&#23460;&#25351;&#26631;&#30340;&#24739;&#32773;&#36827;&#23637;&#36235;&#21183;&#20174;&#24120;&#35265;&#24773;&#20917;&#36716;&#31227;&#21040;&#32597;&#35265;&#25110;&#29305;&#23450;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#20013;&#65292;&#20197;&#21327;&#21161;&#26816;&#27979;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#27835;&#30103;&#24739;&#32773;&#30340;&#38774;&#34880;&#31649;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#26399;&#30417;&#27979;&#26159;&#31649;&#29702;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#24517;&#35201;&#26041;&#38754;&#12290;&#30001;&#20110;&#32597;&#35265;&#25110;&#29305;&#23450;&#30142;&#30149;&#30340;&#24739;&#32773;&#35268;&#27169;&#36739;&#23567;&#65292;&#35266;&#23519;&#20063;&#26159;&#38388;&#27463;&#24615;&#30340;&#65292;&#22240;&#27492;&#20854;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#65292;&#32780;&#24120;&#35265;&#24773;&#20917;&#30001;&#20110;&#23450;&#26399;&#38543;&#35775;&#32780;&#26356;&#23481;&#26131;&#32047;&#31215;&#32437;&#21521;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#20197;&#20854;&#26080;&#35268;&#24459;&#24615;&#12289;&#26102;&#38388;&#24615;&#12289;&#32570;&#24109;&#24615;&#21644;&#31232;&#30095;&#24615;&#32780;&#38395;&#21517;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20811;&#26381;&#19978;&#36848;&#38556;&#30861;&#65292;&#23558;&#24515;&#34880;&#31649;&#23454;&#39564;&#23460;&#25351;&#26631;&#30340;&#24739;&#32773;&#36827;&#23637;&#36235;&#21183;&#20174;&#24120;&#35265;&#24773;&#20917;&#36716;&#31227;&#21040;&#32597;&#35265;&#25110;&#29305;&#23450;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#34880;&#21387;&#24739;&#32773;&#65288;&#23578;&#26410;&#24739;&#31958;&#23615;&#30149;&#65289;&#36827;&#34892;&#20102;&#19968;&#33324;&#23454;&#39564;&#23460;&#36827;&#23637;&#65288;GLP&#65289;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#23454;&#39564;&#23460;&#36827;&#23637;&#36235;&#21183;&#36716;&#31227;&#65292;&#20197;&#21327;&#21161;&#26816;&#27979;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#27835;&#30103;&#24739;&#32773;&#30340;&#38774;&#34880;&#31649;&#37325;&#24314;&#65288;TVR&#65289;&#12290;GLP&#37319;&#29992;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular surveillance is an indispensable aspect of managing cardiovascular disorders. Patient recruitment for rare or specific diseases is often limited due to their small patient size and episodic observations, whereas prevalent cases accumulate longitudinal data easily due to regular follow-ups. These data, however, are notorious for their irregularity, temporality, absenteeism, and sparsity. In this study, we leveraged self-supervised learning (SSL) and transfer learning to overcome the above-mentioned barriers, transferring patient progress trends in cardiovascular laboratory parameters from prevalent cases to rare or specific cardiovascular events detection. We pretrained a general laboratory progress (GLP) pretrain model using hypertension patients (who were yet to be diabetic), and transferred their laboratory progress trend to assist in detecting target vessel revascularization (TVR) in percutaneous coronary intervention patients. GLP adopted a two-stage training process that u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#65292;&#23454;&#29616;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26469;&#35299;&#20915;KGQA&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#35782;&#21035;&#25152;&#26377;&#27979;&#35797;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#25928;&#26524;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#35299;&#20915;&#39046;&#22495;&#29305;&#23450;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02206</link><description>&lt;p&gt;
&#20351;&#29992;&#36923;&#36753;&#32534;&#31243;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#12290; (arXiv:2303.02206v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models. (arXiv:2303.02206v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#65292;&#23454;&#29616;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26469;&#35299;&#20915;KGQA&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#35782;&#21035;&#25152;&#26377;&#27979;&#35797;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#25928;&#26524;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#35299;&#20915;&#39046;&#22495;&#29305;&#23450;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#35889;&#19978;&#22238;&#31572;&#38382;&#39064;&#38656;&#35201;&#19968;&#31181;&#23450;&#21046;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20851;&#31995;&#30340;&#25968;&#37327;&#26377;&#38480;&#65292;&#24182;&#19988;&#39046;&#22495;&#30340;&#29305;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#32463;&#20856;&#30340;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26469;&#35299;&#20915;KGQA&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#34920;&#31034;&#20026;Prolog&#26597;&#35810;&#65292;&#25105;&#20204;&#21487;&#20197;&#23481;&#26131;&#22320;&#29983;&#25104;&#31243;&#24207;&#21270;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#33879;&#21517;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;MetaQA&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#20102;&#19968;&#23567;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33021;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#25152;&#26377;&#27979;&#35797;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#23454;&#20307;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39046;&#22495;&#29305;&#23450;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#65292;&#36890;&#36807;&#25972;&#21512;&#36923;&#36753;&#32534;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#20581;&#22766;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering questions over domain-specific graphs requires a tailored approach due to the limited number of relations and the specific nature of the domain. Our approach integrates classic logical programming languages into large language models (LLMs), enabling the utilization of logical reasoning capabilities to tackle the KGQA task. By representing the questions as Prolog queries, which are readable and near close to natural language in representation, we facilitate the generation of programmatically derived answers. To validate the effectiveness of our approach, we evaluate it using a well-known benchmark dataset, MetaQA. Our experimental results demonstrate that our method achieves accurate identification of correct answer entities for all test questions, even when trained on a small fraction of annotated data. Overall, our work presents a promising approach to addressing question answering over domain-specific graphs, offering an explainable and robust solution by incorporating log
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13991</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#23398;&#20064;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#65306;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#30340;&#32763;&#35793;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#30001;&#20110;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65292;&#22810;&#39046;&#22495;&#28151;&#21512;&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#35268;&#33539;&#25552;&#21462;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#23545;&#39118;&#26684;&#65288;&#20363;&#22914;&#65292;&#26080;&#20449;&#24687;&#30340;&#32441;&#29702;&#65289;&#26377;&#24456;&#24378;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#23545;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#24418;&#29366;&#65289;&#30340;&#20559;&#22909;&#65292;&#36825;&#19982;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25918;&#23556;&#31185;&#21307;&#24072;&#20542;&#21521;&#20110;&#20174;CXR&#22270;&#20687;&#20013;&#23398;&#20064;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#22240;&#27492;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#20174;CXR&#22270;&#20687;&#36827;&#34892;&#30149;&#29702;&#35786;&#26029;&#30340;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#27169;&#22411;&#24212;&#35813;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#65288;SRMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#39550;&#39542;&#34892;&#20026;&#25968;&#25454;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#29305;&#24449;&#65292;&#20026;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#25552;&#20379;&#36866;&#24212;&#24615;&#21453;&#39304;&#21644;&#39044;&#38450;&#20132;&#36890;&#20107;&#25925;&#12290;</title><link>http://arxiv.org/abs/2302.10898</link><description>&lt;p&gt;
&#20174;&#36947;&#36335;&#34892;&#39542;&#25968;&#25454;&#20013;&#20272;&#35745;&#39550;&#39542;&#21592;&#20010;&#24615;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Estimating Driver Personality Traits from On-Road Driving Data. (arXiv:2302.10898v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#39550;&#39542;&#34892;&#20026;&#25968;&#25454;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#29305;&#24449;&#65292;&#20026;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#25552;&#20379;&#36866;&#24212;&#24615;&#21453;&#39304;&#21644;&#39044;&#38450;&#20132;&#36890;&#20107;&#25925;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#20110;&#21033;&#29992;&#39550;&#39542;&#25968;&#25454;&#26469;&#20272;&#35745;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#29305;&#24449;&#65292;&#20197;&#29992;&#20110;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#12290;&#36866;&#24212;&#20010;&#20307;&#24515;&#29702;&#29305;&#24449;&#30340;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#36866;&#24403;&#30340;&#21453;&#39304;&#24182;&#39044;&#38450;&#20132;&#36890;&#20107;&#25925;&#12290;&#20316;&#20026;&#23454;&#29616;&#36825;&#26679;&#33258;&#36866;&#24212;&#36741;&#21161;&#31995;&#32479;&#30340;&#31532;&#19968;&#27493;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#22312;&#36335;&#34892;&#39542;&#30340;&#34892;&#20026;&#25968;&#25454;&#20013;&#24320;&#21457;&#19968;&#20010;&#20272;&#35745;&#39550;&#39542;&#21592;&#24515;&#29702;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#35748;&#30693;&#21151;&#33021;&#12289;&#24515;&#29702;&#39550;&#39542;&#39118;&#26684;&#21644;&#24037;&#20316;&#36127;&#33655;&#25935;&#24863;&#24230;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22238;&#24402;&#24314;&#27169;&#30740;&#31350;&#20102;&#39550;&#39542;&#34892;&#20026;&#19982;&#21508;&#31181;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#36861;&#36394;&#21046;&#22270;&#27979;&#35797;&#65288;TMT&#65289;&#21644;&#26377;&#29992;&#35270;&#37326;&#65288;UFOV&#65289;&#27979;&#35797;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20851;&#27880;&#36947;&#36335;&#31867;&#22411;&#20449;&#24687;&#65292;&#24182;&#25429;&#25417;&#20174;&#39550;&#39542;&#34892;&#20026;&#20013;&#35266;&#23519;&#21040;&#30340;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25345;&#32493;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#39550;&#39542;&#26102;&#38388;&#36827;&#34892;&#20998;&#27573;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the estimation of a driver's psychological characteristics using driving data for driving assistance systems. Driving assistance systems that support drivers by adapting individual psychological characteristics can provide appropriate feedback and prevent traffic accidents. As a first step toward implementing such adaptive assistance systems, this research aims to develop a model to estimate drivers' psychological characteristics, such as cognitive function, psychological driving style, and workload sensitivity, from on-road driving behavioral data using machine learning and deep learning techniques. We also investigated the relationship between driving behavior and various cognitive functions, including the Trail Making Test (TMT) and Useful Field of View (UFOV) test, through regression modeling. The proposed method focuses on road type information and captures various durations of time-series data observed from driving behaviors. First, we segment the driving ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#20844;&#24179;&#24615;&#22312;&#35813;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.08973</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#20013;&#30340;&#24179;&#31561;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Equality in Machine Learning Security Defenses. (arXiv:2302.08973v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#20844;&#24179;&#24615;&#22312;&#35813;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#31038;&#21306;&#24050;&#32463;&#21457;&#23637;&#20102;&#35768;&#22810;&#23545;&#25239;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#20294;&#36825;&#20010;&#31038;&#21306;&#20013;&#40092;&#26377;&#30740;&#31350;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#20026;&#35841;&#25552;&#20379;&#20445;&#25252;&#21602;&#65311;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#34987;&#19981;&#21516;&#30340;&#23376;&#32676;&#20307;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#23454;&#35777;&#32467;&#26524;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35768;&#22810;&#26041;&#27861;&#21487;&#33021;&#20250;&#30452;&#25509;&#36896;&#25104;&#20260;&#23475;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26377;&#20559;&#28431;&#27934;&#21644;&#26377;&#20559;&#25490;&#26021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24378;&#21270;&#35757;&#32451;&#27169;&#22411;&#12289;&#22522;&#20110;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#21644;&#25298;&#32477;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#22312;&#23433;&#20840;&#39044;&#31639;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#36866;&#21512;&#20110;&#27979;&#37327;&#38450;&#24481;&#30340;&#24179;&#31561;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#20195;&#38450;&#24481;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#24179;&#31561;&#24615;&#24615;&#33021;&#30340;&#34913;&#37327;&#20215;&#20540;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#21644;&#26041;&#27861;&#33021;&#22815;&#40723;&#21169;&#21644;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#21644;&#38450;&#24481;&#39046;&#22495;&#30340;&#20844;&#24179;&#24615;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning security community has developed myriad defenses for evasion attacks over the past decade. An understudied question in that community is: for whom do these defenses defend? In this work, we consider some common approaches to defending learned systems and whether those approaches may offer unexpected performance inequities when used by different sub-populations. We outline simple parity metrics and a framework for analysis that can begin to answer this question through empirical results of the fairness implications of machine learning security methods. Many methods have been proposed that can cause direct harm, which we describe as biased vulnerability and biased rejection. Our framework and metric can be applied to robustly trained models, preprocessing-based methods, and rejection methods to capture behavior over security budgets. We identify a realistic dataset with a reasonable computational cost suitable for measuring the equality of defenses. Through a case st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#35266;&#27979;&#20013;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2302.06912</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#24724;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regret-Based Optimization for Robust Reinforcement Learning. (arXiv:2302.06912v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#35266;&#27979;&#20013;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23545;&#35266;&#27979;&#20013;&#30340;&#24494;&#23567;&#23545;&#25239;&#24615;&#22122;&#22768;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#31181;&#23545;&#25239;&#24615;&#22122;&#22768;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#21487;&#33021;&#36896;&#25104;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#29616;&#26377;&#30340;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#35266;&#27979;&#25200;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#20027;&#35201;&#38598;&#20013;&#22312;&#36845;&#20195;&#25913;&#36827;&#27599;&#20010;&#36845;&#20195;&#20013;&#29983;&#25104;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#23545;&#26222;&#36890;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#26159;&#34987;&#21160;&#24615;&#30340;&#65292;&#22914;&#26524;&#26576;&#20123;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#20135;&#29983;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36861;&#27714;&#19968;&#31181;&#26356;&#31215;&#26497;&#30340;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#30452;&#25509;&#20248;&#21270;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#40065;&#26834;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable to small adversarial noise in observations. Such adversarial noise can have disastrous consequences in safety-critical environments. For instance, a self-driving car receiving adversarially perturbed sensory observations about nearby signs (e.g., a stop sign physically altered to be perceived as a speed limit sign) or objects (e.g., cars altered to be recognized as trees) can be fatal. Existing approaches for making RL algorithms robust to an observation-perturbing adversary have focused on reactive approaches that iteratively improve against adversarial examples generated at each iteration. While such approaches have been shown to provide improvements over regular RL methods, they are reactive and can fare significantly worse if certain categories of adversarial examples are not generated during training. To that end, we pursue a more proactive approach that relies on directly optimizing a well-studied robustn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#33258;&#36866;&#24212;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;PQ&#25351;&#25968;&#26469;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#21387;&#32553;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#27169;&#22411;&#30340;&#21098;&#26525;&#31243;&#24230;&#65292;&#30830;&#20445;&#19981;&#20250;&#36807;&#24230;&#25110;&#27424;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2302.05601</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#24615;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Pruning Deep Neural Networks from a Sparsity Perspective. (arXiv:2302.05601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#33258;&#36866;&#24212;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;PQ&#25351;&#25968;&#26469;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#21387;&#32553;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#27169;&#22411;&#30340;&#21098;&#26525;&#31243;&#24230;&#65292;&#30830;&#20445;&#19981;&#20250;&#36807;&#24230;&#25110;&#27424;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#32593;&#32476;&#21098;&#26525;&#25216;&#26415;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#26088;&#22312;&#23558;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#37096;&#32626;&#21040;&#35745;&#31639;&#21644;&#20869;&#23384;&#21463;&#38480;&#30340;&#23567;&#22411;&#35774;&#22791;&#19978;&#12290;&#36825;&#31181;&#21098;&#26525;&#36890;&#24120;&#36890;&#36807;&#20002;&#24323;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#20887;&#20313;&#26435;&#37325;&#12289;&#31070;&#32463;&#20803;&#25110;&#23618;&#26469;&#23454;&#29616;&#65292;&#21516;&#26102;&#21162;&#21147;&#20445;&#25345;&#21487;&#27604;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#23618;&#21098;&#26525;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#23454;&#35777;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#21487;&#37327;&#21270;&#30340;&#25514;&#26045;&#26469;&#20272;&#31639;&#27599;&#20010;&#21098;&#26525;&#36845;&#20195;&#20013;&#23376;&#32593;&#32476;&#30340;&#21487;&#21387;&#32553;&#24615;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#36827;&#34892;&#36807;&#21098;&#26525;&#25110;&#27424;&#21098;&#26525;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PQ&#25351;&#25968;&#65288;PQI&#65289;&#26469;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#21387;&#32553;&#24615;&#65292;&#24182;&#21033;&#29992;&#27492;&#25351;&#25968;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#33258;&#36866;&#24212;&#21098;&#26525;&#65288;SAP&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#25903;&#25345;&#20551;&#35774;&#65292;&#23545;&#20110;&#36890;&#29992;&#21098;&#26525;&#31243;&#24207;&#65292;&#24403;&#22823;&#22411;&#27169;&#22411;&#34987;&#26377;&#25928;&#22320;&#27491;&#21017;&#21270;&#26102;&#65292;PQI&#39318;&#20808;&#20943;&#23567;&#65292;&#28982;&#21518;&#22312;&#20854;&#21487;&#21387;&#32553;&#24615;&#36798;&#21040;&#26368;&#23567;&#20540;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29380;&#25289;&#20811;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#39640;&#38454;&#25299;&#25169;&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#65292;&#24182;&#33021;&#22815;&#32852;&#21512;&#22788;&#29702;&#33410;&#28857;&#21644;&#38142;&#25509;&#19978;&#30340;&#25299;&#25169;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2301.10137</link><description>&lt;p&gt;
&#39640;&#38454;&#25299;&#25169;&#20449;&#21495;&#30340;&#29380;&#25289;&#20811;&#20449;&#21495;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Dirac signal processing of higher-order topological signals. (arXiv:2301.10137v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29380;&#25289;&#20811;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#39640;&#38454;&#25299;&#25169;&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#65292;&#24182;&#33021;&#22815;&#32852;&#21512;&#22788;&#29702;&#33410;&#28857;&#21644;&#38142;&#25509;&#19978;&#30340;&#25299;&#25169;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#38454;&#32593;&#32476;&#21487;&#20197;&#32500;&#25345;&#19982;&#33410;&#28857;&#12289;&#38142;&#36335;&#12289;&#19977;&#35282;&#24418;&#21644;&#26356;&#39640;&#32500;&#30340;&#21333;&#32431;&#22797;&#21512;&#20307;&#30456;&#20851;&#30340;&#25299;&#25169;&#20449;&#21495;&#65292;&#36825;&#20123;&#25299;&#25169;&#20449;&#21495;&#21487;&#20197;&#25551;&#36848;&#21253;&#25324;&#28023;&#27915;&#30005;&#27969;&#12289;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#31361;&#35302;&#30005;&#27969;&#21644;&#29983;&#29289;&#36816;&#36755;&#32593;&#32476;&#22312;&#20869;&#30340;&#21508;&#31181;&#30495;&#23454;&#31995;&#32479;&#12290;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#25299;&#25169;&#20449;&#21495;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#26159;&#36890;&#36807;&#25552;&#39640;&#20449;&#22122;&#27604;&#26469;&#22788;&#29702;&#36825;&#20123;&#20449;&#21495;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#25299;&#25169;&#20449;&#21495;&#36890;&#24120;&#26159;&#29420;&#31435;&#22788;&#29702;&#30340;&#12290;&#20363;&#22914;&#65292;&#33410;&#28857;&#20449;&#21495;&#29420;&#31435;&#20110;&#38142;&#36335;&#20449;&#21495;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#21487;&#20197;&#22312;&#19981;&#21516;&#32500;&#24230;&#19978;&#24378;&#21046;&#19968;&#33268;&#22788;&#29702;&#25299;&#25169;&#20449;&#21495;&#30340;&#31639;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32570;&#20047;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29380;&#25289;&#20811;&#20449;&#21495;&#22788;&#29702;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#26080;&#30417;&#30563;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#65292;&#23427;&#23398;&#20064;&#32852;&#21512;&#36807;&#28388;&#25903;&#25345;&#22312;&#33410;&#28857;&#12289;&#38142;&#25509;&#19978;&#30340;&#25299;&#25169;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Higher-order networks can sustain topological signals which are variables associated not only to the nodes, but also to the links, to the triangles and in general to the higher dimensional simplices of simplicial complexes. These topological signals can describe a large variety of real systems including currents in the ocean, synaptic currents between neurons and biological transportation networks. In real scenarios topological signal data might be noisy and an important task is to process these signals by improving their signal to noise ratio. So far topological signals are typically processed independently of each other. For instance, node signals are processed independently of link signals, and algorithms that can enforce a consistent processing of topological signals across different dimensions are largely lacking. Here we propose Dirac signal processing, an adaptive, unsupervised signal processing algorithm that learns to jointly filter topological signals supported on nodes, link
&lt;/p&gt;</description></item><item><title>BallGAN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;3D&#24863;&#30693;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#32422;&#26463;&#26469;&#20943;&#23569;&#32972;&#26223;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#12290;</title><link>http://arxiv.org/abs/2301.09091</link><description>&lt;p&gt;
BallGAN: &#24102;&#26377;&#29699;&#24418;&#32972;&#26223;&#30340;3D&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
BallGAN: 3D-aware Image Synthesis with a Spherical Background. (arXiv:2301.09091v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09091
&lt;/p&gt;
&lt;p&gt;
BallGAN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;3D&#24863;&#30693;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#32422;&#26463;&#26469;&#20943;&#23569;&#32972;&#26223;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26088;&#22312;&#21512;&#25104;&#36924;&#30495;&#30340;3D&#22330;&#26223;&#65292;&#20197;&#20415;&#21487;&#20197;&#20197;&#20219;&#24847;&#35282;&#24230;&#36827;&#34892;&#28210;&#26579;&#20197;&#20135;&#29983;&#22270;&#20687;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#23427;&#20204;&#22312;&#35757;&#32451;&#19981;&#31283;&#23450;&#25110;&#23384;&#22312;&#19981;&#33258;&#28982;&#30340;3D&#20960;&#20309;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;3D&#20960;&#20309;&#22312;&#32422;&#26463;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#21363;&#20165;&#23558;&#20854;&#20998;&#31867;&#20026;&#30495;&#23454;&#22270;&#20687;&#23545;&#20110;&#37492;&#21035;&#22120;&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#23558;&#22330;&#26223;&#34920;&#31034;&#20026;&#25918;&#32622;&#22312;&#29699;&#20307;&#20013;&#30340;&#21069;&#26223;&#21644;&#34180;&#29699;&#24418;&#32972;&#26223;&#30340;&#32852;&#21512;&#12290;&#36825;&#26679;&#21487;&#20197;&#20943;&#23569;&#32972;&#26223;&#22330;&#30340;&#33258;&#30001;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#20307;&#28210;&#26579;&#26041;&#31243;&#65292;&#24182;&#21152;&#20837;&#20102;&#19987;&#29992;&#30340;&#32422;&#26463;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;BallGAN&#30340;&#26032;&#22411;3D&#24863;&#30693;GAN&#26694;&#26550;&#12290; BallGAN&#20855;&#26377;&#20197;&#19979;&#22810;&#20010;&#20248;&#28857;&#12290;1&#65289;&#23427;&#20135;&#29983;&#20102;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#65307;&#22330;&#26223;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22270;&#20687;&#20855;&#26377;&#26356;&#22909;&#30340;&#20809;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D-aware GANs aim to synthesize realistic 3D scenes such that they can be rendered in arbitrary perspectives to produce images. Although previous methods produce realistic images, they suffer from unstable training or degenerate solutions where the 3D geometry is unnatural. We hypothesize that the 3D geometry is underdetermined due to the insufficient constraint, i.e., being classified as real image to the discriminator is not enough. To solve this problem, we propose to approximate the background as a spherical surface and represent a scene as a union of the foreground placed in the sphere and the thin spherical background. It reduces the degree of freedom in the background field. Accordingly, we modify the volume rendering equation and incorporate dedicated constraints to design a novel 3D-aware GAN framework named BallGAN. BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D geometry; the images of a scene across different viewpoints have better photometric 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#33539;&#24335;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#24191;&#27867;&#32780;&#20005;&#26684;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.05763</link><description>&lt;p&gt;
&#19968;&#31181;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#23545;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20043;&#21487;&#37325;&#22797;&#20877;&#29616;&#24615;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
A Rigorous Uncertainty-Aware Quantification Framework Is Essential for Reproducible and Replicable Machine Learning Workflows. (arXiv:2301.05763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05763
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#33539;&#24335;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#24191;&#27867;&#32780;&#20005;&#26684;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#21253;&#21547;&#30340;&#32467;&#26524;&#22797;&#29616;&#33021;&#21147;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#12290;&#19968;&#31181;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#24863;&#20852;&#36259;&#37327;&#65288;QoI&#65289;&#22797;&#21046;&#24615;&#30340;&#19981;&#30830;&#23450;&#24230;&#24863;&#30693;&#24230;&#37327;&#26631;&#20934;&#65292;&#23558;&#26377;&#21161;&#20110;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#33719;&#24471;&#30340;&#32467;&#26524;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36125;&#21494;&#26031;&#33539;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22914;&#20309;&#25552;&#20379;&#19968;&#31181;&#24191;&#27867;&#21644;&#20005;&#26684;&#30340;&#26694;&#26550;&#65292;&#23450;&#37327;&#22320;&#35780;&#20272;&#22797;&#26434;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#23558;&#22635;&#34917;&#26426;&#22120;&#23398;&#20064;&#25110;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#22240;&#20026;&#23427;&#23558;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#25110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#39044;&#27979;&#21464;&#24322;&#23545;&#24037;&#20316;&#27969;&#31243;&#20013;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#20010;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#20855;&#37325;&#22797;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to replicate predictions by machine learning (ML) or artificial intelligence (AI) models and results in scientific workflows that incorporate such ML/AI predictions is driven by numerous factors. An uncertainty-aware metric that can quantitatively assess the reproducibility of quantities of interest (QoI) would contribute to the trustworthiness of results obtained from scientific workflows involving ML/AI models. In this article, we discuss how uncertainty quantification (UQ) in a Bayesian paradigm can provide a general and rigorous framework for quantifying reproducibility for complex scientific workflows. Such as framework has the potential to fill a critical gap that currently exists in ML/AI for scientific workflows, as it will enable researchers to determine the impact of ML/AI model prediction variability on the predictive outcomes of ML/AI-powered workflows. We expect that the envisioned framework will contribute to the design of more reproducible and trustworthy wor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04785</link><description>&lt;p&gt;
&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Phase-shifted Adversarial Training. (arXiv:2301.04785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#34987;&#35748;&#20026;&#26159;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#23433;&#20840;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#26356;&#26032;&#27493;&#39588;&#30340;&#25968;&#37327;&#12289;&#20351;&#29992;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#20197;&#21450;&#23558;&#38543;&#26426;&#24615;&#27880;&#20837;&#21040;&#25915;&#20987;&#20013;&#26469;&#29983;&#25104;&#24378;&#26377;&#21147;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#36807;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#39057;&#20449;&#24687;&#30340;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#20010;&#39057;&#29575;&#21407;&#29702;&#30340;&#26222;&#36941;&#29616;&#35937;&#65292;&#21363;\textit{&#36739;&#20302;&#30340;&#39057;&#29575;&#20808;&#23398;&#20064;}&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20173;&#28982;&#25104;&#31435;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#65292;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#26469;&#25913;&#21892;&#23545;&#25239;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been considered an imperative component for safely deploying neural network-based applications to the real world. To achieve stronger robustness, existing methods primarily focus on how to generate strong attacks by increasing the number of update steps, regularizing the models with the smoothed loss function, and injecting the randomness into the attack. Instead, we analyze the behavior of adversarial training through the lens of response frequency. We empirically discover that adversarial training causes neural networks to have low convergence to high-frequency information, resulting in highly oscillated predictions near each data. To learn high-frequency contents efficiently and effectively, we first prove that a universal phenomenon of frequency principle, i.e., \textit{lower frequencies are learned first}, still holds in adversarial training. Based on that, we propose phase-shifted adversarial training (PhaseAT) in which the model learns high-frequency com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#36827;&#34892;&#21367;&#31215;&#20449;&#24687;&#22788;&#29702;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#22270;&#35937;&#29702;&#35770;&#21644;&#23494;&#38598;&#22270;&#24207;&#21015;&#30340;&#26497;&#38480;&#26469;&#23454;&#29616;&#65292;&#24471;&#21040;&#21367;&#31215;&#36816;&#31639;&#31526;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#23616;&#37096;&#25554;&#20540;&#20989;&#25968;&#23454;&#29616;&#20449;&#21495;&#30340;&#38477;&#32500;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.08171</link><description>&lt;p&gt;
&#22270;&#24418;&#27744;&#21270;&#20943;&#23567;&#20449;&#21495;&#21644;&#22270;&#19978;&#21367;&#31215;&#36816;&#31639;&#30340;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Graphon Pooling for Reducing Dimensionality of Signals and Convolutional Operators on Graphs. (arXiv:2212.08171v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#36827;&#34892;&#21367;&#31215;&#20449;&#24687;&#22788;&#29702;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#22270;&#35937;&#29702;&#35770;&#21644;&#23494;&#38598;&#22270;&#24207;&#21015;&#30340;&#26497;&#38480;&#26469;&#23454;&#29616;&#65292;&#24471;&#21040;&#21367;&#31215;&#36816;&#31639;&#31526;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#23616;&#37096;&#25554;&#20540;&#20989;&#25968;&#23454;&#29616;&#20449;&#21495;&#30340;&#38477;&#32500;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#36827;&#34892;&#21367;&#31215;&#20449;&#24687;&#22788;&#29702;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#30340;&#22270;&#35937;&#29702;&#35770;&#21644;&#23494;&#38598;&#22270;&#24207;&#21015;&#30340;&#26497;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#30340;&#22270;&#25569;&#34920;&#31034;&#21644;&#22270;&#35937;&#22312;[0, 1]2&#19978;&#30340;&#20998;&#21306;&#36827;&#34892;&#35745;&#31639;&#12290;&#32467;&#26524;&#25105;&#20204;&#24471;&#21040;&#21367;&#31215;&#36816;&#31639;&#31526;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20449;&#21495;&#30340;&#38477;&#32500;&#26159;&#36890;&#36807;L2([0, 1])&#20013;&#30340;&#31616;&#21333;&#23616;&#37096;&#25554;&#20540;&#20989;&#25968;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#20302;&#32500;&#34920;&#31034;&#20998;&#21035;&#26500;&#25104;&#20102;&#22270;&#21644;&#22270;&#35937;&#30340;&#25910;&#25947;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#25552;&#20379;&#30340;&#29702;&#35770;&#20445;&#35777;&#34920;&#26126;&#38477;&#20302;&#21518;&#30340;&#22270;&#21644;&#20449;&#21495;&#32487;&#25215;&#20102;&#21407;&#22987;&#25968;&#37327;&#30340;&#20809;&#35889;&#32467;&#26500;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20381;&#36182;&#22270;&#24418;&#27744;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#19978;&#36827;&#34892;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#35266;&#23519;&#21040;&#22270;&#24418;&#27744;&#21270;&#30340;&#25928;&#26524;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a pooling approach for convolutional information processing on graphs relying on the theory of graphons and limits of dense graph sequences. We present three methods that exploit the induced graphon representation of graphs and graph signals on partitions of [0, 1]2 in the graphon space. As a result we derive low dimensional representations of the convolutional operators, while a dimensionality reduction of the signals is achieved by simple local interpolation of functions in L2([0, 1]). We prove that those low dimensional representations constitute a convergent sequence of graphs and graph signals, respectively. The methods proposed and the theoretical guarantees that we provide show that the reduced graphs and signals inherit spectral-structural properties of the original quantities. We evaluate our approach with a set of numerical experiments performed on graph neural networks (GNNs) that rely on graphon pooling. We observe that graphon pooling performs sign
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Temporal Saliency Detection (TSD)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#21387;&#32553;&#22810;&#22836;&#27880;&#24847;&#21147;&#36827;&#34892;&#26174;&#33879;&#24615;&#27169;&#24335;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2212.07771</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#38388;&#26174;&#33879;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Temporal Saliency Detection Towards Explainable Transformer-based Timeseries Forecasting. (arXiv:2212.07771v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07771
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Temporal Saliency Detection (TSD)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#21387;&#32553;&#22810;&#22836;&#27880;&#24847;&#21147;&#36827;&#34892;&#26174;&#33879;&#24615;&#27169;&#24335;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35768;&#22810;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#38271;&#26399;&#22810;&#27493;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#24120;&#29992;&#30340;&#26174;&#33879;&#24615;&#22270;&#35299;&#37322;DNN&#30340;&#24605;&#24819;&#65292;&#26500;&#24314;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#22836;&#24314;&#31435;&#36830;&#25509;&#65292;&#33258;&#21160;&#32534;&#30721;&#19982;&#26174;&#33879;&#24615;&#30456;&#20851;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Temporal Saliency Detection (TSD) &#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#34429;&#28982;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#36981;&#24490;&#24120;&#35268;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#20294;&#22312;&#32534;&#30721;&#22120;&#32452;&#20214;&#20013;&#32463;&#21382;&#20102;&#37325;&#22823;&#30340;&#25913;&#36827;&#65292;&#20854;&#20013;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;U-Net&#39118;&#26684;&#26550;&#26500;&#21551;&#21457;&#30340;&#19968;&#31995;&#21015;&#20449;&#24687;&#25910;&#32553;&#21644;&#25193;&#23637;&#27169;&#22359;&#12290;TSD&#26041;&#27861;&#36890;&#36807;&#21387;&#32553;&#22810;&#22836;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#27169;&#24335;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the notable advancements in numerous Transformer-based models, the task of long multi-horizon time series forecasting remains a persistent challenge, especially towards explainability. Focusing on commonly used saliency maps in explaining DNN in general, our quest is to build attention-based architecture that can automatically encode saliency-related temporal patterns by establishing connections with appropriate attention heads. Hence, this paper introduces Temporal Saliency Detection (TSD), an effective approach that builds upon the attention mechanism and applies it to multi-horizon time series prediction. While our proposed architecture adheres to the general encoder-decoder structure, it undergoes a significant renovation in the encoder component, wherein we incorporate a series of information contracting and expanding blocks inspired by the U-Net style architecture. The TSD approach facilitates the multiresolution analysis of saliency patterns by condensing multi-heads, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31995;&#32479;&#22320;&#35782;&#21035;ImageNet&#20013;&#30340;&#26377;&#23475;&#20266;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;"Spurious ImageNet"&#26469;&#34913;&#37327;&#20998;&#31867;&#22120;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32531;&#35299;&#26041;&#27861;SpuFix&#26469;&#20943;&#23569;&#20998;&#31867;&#22120;&#23545;&#26377;&#23475;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.04871</link><description>&lt;p&gt;
Spurious Features Everywhere -- &#22312;ImageNet&#20013;&#22823;&#35268;&#27169;&#26816;&#27979;&#26377;&#23475;&#30340;&#20266;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spurious Features Everywhere -- Large-Scale Detection of Harmful Spurious Features in ImageNet. (arXiv:2212.04871v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31995;&#32479;&#22320;&#35782;&#21035;ImageNet&#20013;&#30340;&#26377;&#23475;&#20266;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;"Spurious ImageNet"&#26469;&#34913;&#37327;&#20998;&#31867;&#22120;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32531;&#35299;&#26041;&#27861;SpuFix&#26469;&#20943;&#23569;&#20998;&#31867;&#22120;&#23545;&#26377;&#23475;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#22522;&#20934;&#24615;&#33021;&#24182;&#19981;&#21487;&#38752;&#26469;&#39044;&#27979;&#37096;&#32626;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#22270;&#20687;&#20998;&#31867;&#22120;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#20102;&#20266;&#29305;&#24449;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#21487;&#33021;&#20250;&#20197;&#24847;&#24819;&#19981;&#21040;&#30340;&#26041;&#24335;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35782;&#21035;&#20687;ImageNet&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#30340;&#20266;&#29305;&#24449;&#12290;&#23427;&#22522;&#20110;&#25105;&#20204;&#30340;&#31070;&#32463;PCA&#32452;&#20214;&#21450;&#20854;&#21487;&#35270;&#21270;&#12290;&#20043;&#21069;&#23545;&#20266;&#29305;&#24449;&#30340;&#30456;&#20851;&#24037;&#20316;&#36890;&#24120;&#22312;&#29609;&#20855;&#22330;&#26223;&#20013;&#36827;&#34892;&#65292;&#25110;&#32773;&#38656;&#35201;&#26114;&#36149;&#30340;&#20687;&#32032;&#32423;&#27880;&#37322;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;ImageNet&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#23637;&#31034;&#26576;&#20010;&#31867;&#21035;&#20013;&#23384;&#22312;&#26377;&#23475;&#20266;&#29305;&#24449;&#23601;&#36275;&#20197;&#35302;&#21457;&#35813;&#31867;&#21035;&#30340;&#39044;&#27979;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;"Spurious ImageNet"&#65292;&#21487;&#20197;&#34913;&#37327;&#20219;&#20309;ImageNet&#20998;&#31867;&#22120;&#23545;&#26377;&#23475;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpuFix&#30340;&#31616;&#21333;&#32531;&#35299;&#26041;&#27861;&#26469;&#20943;&#23569;&#20219;&#20309;ImageNet&#20998;&#31867;&#22120;&#23545;&#26377;&#23475;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmark performance of deep learning classifiers alone is not a reliable predictor for the performance of a deployed model. In particular, if the image classifier has picked up spurious features in the training data, its predictions can fail in unexpected ways. In this paper, we develop a framework that allows us to systematically identify spurious features in large datasets like ImageNet. It is based on our neural PCA components and their visualization. Previous work on spurious features often operates in toy settings or requires costly pixel-wise annotations. In contrast, we work with ImageNet and validate our results by showing that presence of the harmful spurious feature of a class alone is sufficient to trigger the prediction of that class. We introduce the novel dataset "Spurious ImageNet" which allows to measure the reliance of any ImageNet classifier on harmful spurious features. Moreover, we introduce SpuFix as a simple mitigation method to reduce the dependence of any Imag
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#23384;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15498</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#30693;&#27979;&#37327;&#22122;&#22768;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks with unknown measurement noise. (arXiv:2211.15498v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#23384;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26159;&#19968;&#31181;&#26082;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#21448;&#33021;&#35782;&#21035;&#20559;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#30456;&#20851;&#30340;&#30740;&#31350;&#37117;&#20551;&#35774;&#25968;&#25454;&#26159;&#26080;&#22122;&#22768;&#30340;&#65292;&#25110;&#32773;&#26159;&#21463;&#24369;&#39640;&#26031;&#22122;&#22768;&#27745;&#26579;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;PINN&#26694;&#26550;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#26681;&#26412;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;(Energy-Based Model, EBM)&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#38750;IID&#25968;&#25454;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#19987;&#19994;&#21270;&#20027;&#21160;&#25277;&#26679;&#21644;&#30693;&#35782;&#34917;&#20607;&#32852;&#37030;&#26356;&#26032;&#26469;&#35299;&#20915;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13579</link><description>&lt;p&gt;
&#30693;&#35782;&#24863;&#30693;&#30340;&#38750;IID&#25968;&#25454;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Aware Federated Active Learning with Non-IID Data. (arXiv:2211.13579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#38750;IID&#25968;&#25454;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#19987;&#19994;&#21270;&#20027;&#21160;&#25277;&#26679;&#21644;&#30693;&#35782;&#34917;&#20607;&#32852;&#37030;&#26356;&#26032;&#26469;&#35299;&#20915;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#26412;&#22320;&#25968;&#25454;&#26631;&#31614;&#30340;&#26114;&#36149;&#27880;&#37322;&#25104;&#26412;&#20173;&#28982;&#26159;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#22312;&#26377;&#38480;&#27880;&#37322;&#39044;&#31639;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20197;&#20998;&#25955;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#39640;&#25928;&#22320;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#26381;&#21153;&#22120;&#31471;&#20840;&#23616;&#27169;&#22411;&#30340;&#20027;&#21160;&#25277;&#26679;&#30446;&#26631;&#19982;&#24322;&#27493;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#12290;&#24403;&#25968;&#25454;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#20043;&#38388;&#20998;&#24067;&#38750;IID&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;(KAFAL)&#26041;&#27861;&#65292;&#23427;&#21253;&#25324;&#30693;&#35782;&#19987;&#19994;&#21270;&#20027;&#21160;&#25277;&#26679;(KSAS)&#21644;&#30693;&#35782;&#34917;&#20607;&#32852;&#37030;&#26356;&#26032;(KCFU)&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables multiple decentralized clients to learn collaboratively without sharing the local training data. However, the expensive annotation cost to acquire data labels on local clients remains an obstacle in utilizing local data. In this paper, we propose a federated active learning paradigm to efficiently learn a global model with limited annotation budget while protecting data privacy in a decentralized learning way. The main challenge faced by federated active learning is the mismatch between the active sampling goal of the global model on the server and that of the asynchronous local clients. This becomes even more significant when data is distributed non-IID across local clients. To address the aforementioned challenge, we propose Knowledge-Aware Federated Active Learning (KAFAL), which consists of Knowledge-Specialized Active Sampling (KSAS) and Knowledge-Compensatory Federated Update (KCFU). KSAS is a novel active sampling method tailored for the federated acti
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#21644;&#40654;&#26364;&#27969;&#24418;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14598</link><description>&lt;p&gt;
&#30830;&#20999;&#30340;&#27969;&#24418;&#39640;&#26031;&#21464;&#20998;&#36125;&#21494;&#26031;
&lt;/p&gt;
&lt;p&gt;
Exact Manifold Gaussian Variational Bayes. (arXiv:2210.14598v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14598
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#21644;&#40654;&#26364;&#27969;&#24418;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#27169;&#22411;&#20013;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#65292;&#20854;&#20013;&#21464;&#20998;&#31354;&#38388;&#26159;&#19968;&#20010;&#40654;&#26364;&#27969;&#24418;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#20197;&#38544;&#24335;&#28385;&#36275;&#21464;&#20998;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#27491;&#23450;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30830;&#20999;&#27969;&#24418;&#39640;&#26031;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;EMGVB&#65289;&#25552;&#20379;&#20102;&#31934;&#30830;&#20294;&#31616;&#21333;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#24182;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#65292;EMGVB&#25104;&#20026;&#22797;&#26434;&#27169;&#22411;&#20013;&#21363;&#25554;&#21363;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#32479;&#35745;&#12289;&#35745;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#20351;&#29992;&#20116;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#21487;&#34892;&#24615;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#24182;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an optimization algorithm for Variational Inference (VI) in complex models. Our approach relies on natural gradient updates where the variational space is a Riemann manifold. We develop an efficient algorithm for Gaussian Variational Inference that implicitly satisfies the positive definite constraint on the variational covariance matrix. Our Exact manifold Gaussian Variational Bayes (EMGVB) provides exact but simple update rules and is straightforward to implement. Due to its black-box nature, EMGVB stands as a ready-to-use solution for VI in complex models. Over five datasets, we empirically validate our feasible approach on different statistical, econometric, and deep learning models, discussing its performance with respect to baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.13708</link><description>&lt;p&gt;
MARLlib: &#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
MARLlib: A Scalable Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#35780;&#20272;&#24179;&#21488;&#21644;&#20844;&#35748;&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#38598;&#25104;&#24211;&#22871;&#20214;&#65292;&#20197;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#21487;&#38752;&#30340;MARL&#23454;&#29616;&#21644;&#21487;&#22797;&#21046;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;MARLlib&#36890;&#36807;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#27969;&#35774;&#35745;&#65292;&#22312;&#39640;&#24230;&#21487;&#32452;&#21512;&#30340;&#38598;&#25104;&#39118;&#26684;&#20013;&#32479;&#19968;&#20102;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;MARLlib&#36890;&#36807;&#38598;&#25104;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65307;&#36825;&#20801;&#35768;&#26368;&#32456;&#29992;&#25143;&#22312;&#26368;&#23567;&#30340;&#20195;&#30721;&#20462;&#25913;&#19979;&#23454;&#29616;&#21327;&#20316;&#12289;&#31454;&#20105;&#21644;&#28151;&#21512;&#20219;&#21153;&#30340;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;MARLlib&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#23436;&#20840;&#35299;&#32806;&#21512;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fast development of multi-agent systems (MAS) and multi-agent reinforcement learning (MARL) algorithms, there is a lack of unified evaluation platforms and commonly-acknowledged baseline implementation. Therefore, an urgent need is to develop an integrated library suite that delivers reliable MARL implementation and replicable evaluation in various benchmarks. To fill such a research gap, in this paper, we propose MARLlib, a comprehensive MARL algorithm library for solving multi-agent problems. With a novel design of agent-level distributed dataflow, MARLlib manages to unify tens of algorithms in a highly composable integration style. Moreover, MARLlib goes beyond current work by integrating diverse environment interfaces and providing flexible parameter sharing strategies; this allows for versatile solutions to cooperative, competitive, and mixed tasks with minimal code modifications for end users. Finally, MARLlib provides easy-to-use APIs and a fully decoupled configurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;ProtoBandit&#31639;&#27861;&#36890;&#36807;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#21407;&#22411;&#36873;&#25321;&#65292;&#36991;&#20813;&#20102;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#19979;&#36827;&#34892;&#30456;&#20284;&#24615;&#27604;&#36739;&#30340;&#26114;&#36149;&#24615;&#65292;&#33021;&#22815;&#35782;&#21035;&#20986;&#19968;&#32452;&#32039;&#20945;&#30340;&#21407;&#22411;&#23454;&#20363;&#65292;&#26377;&#25928;&#20195;&#34920;&#32473;&#23450;&#30340;&#30446;&#26631;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.01860</link><description>&lt;p&gt;
ProtoBandit: &#36890;&#36807;&#22810;&#33218;&#36172;&#21338;&#26426;&#23454;&#29616;&#39640;&#25928;&#21407;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
ProtoBandit: Efficient Prototype Selection via Multi-Armed Bandits. (arXiv:2210.01860v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;ProtoBandit&#31639;&#27861;&#36890;&#36807;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#21407;&#22411;&#36873;&#25321;&#65292;&#36991;&#20813;&#20102;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#19979;&#36827;&#34892;&#30456;&#20284;&#24615;&#27604;&#36739;&#30340;&#26114;&#36149;&#24615;&#65292;&#33021;&#22815;&#35782;&#21035;&#20986;&#19968;&#32452;&#32039;&#20945;&#30340;&#21407;&#22411;&#23454;&#20363;&#65292;&#26377;&#25928;&#20195;&#34920;&#32473;&#23450;&#30340;&#30446;&#26631;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#28304;&#25968;&#25454;&#38598;S&#20013;&#35782;&#21035;&#19968;&#32452;&#32039;&#20945;&#30340;&#20449;&#24687;&#25968;&#25454;&#23454;&#20363;&#65288;&#21363;&#21407;&#22411;&#65289;&#65292;&#20197;&#26368;&#22909;&#22320;&#20195;&#34920;&#32473;&#23450;&#30340;&#30446;&#26631;&#38598;T&#12290;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#31034;&#20363;&#25552;&#20379;&#20102;&#23545;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#27934;&#23519;&#65292;&#24182;&#22312;&#22522;&#20110;&#23454;&#20363;&#30340;&#25512;&#29702;&#20013;&#36215;&#21040;&#36741;&#21161;&#20316;&#29992;&#65292;&#20174;&#32780;&#24433;&#21709;&#20154;&#31867;&#20915;&#31574;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21407;&#22411;&#36873;&#25321;&#26041;&#27861;&#38656;&#35201;&#22312;&#28304;&#25968;&#25454;&#28857;&#21644;&#30446;&#26631;&#25968;&#25454;&#28857;&#20043;&#38388;&#36827;&#34892;O&#65288;|S| |T|&#65289;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#35774;&#32622;&#26469;&#35828;&#26174;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22312;&#21407;&#22411;&#31034;&#20363;&#31354;&#38388;&#20013;&#37319;&#29992;&#38543;&#26426;&#36138;&#23146;&#25628;&#32034;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#26469;&#20943;&#23569;&#30456;&#20284;&#24615;&#27604;&#36739;&#30340;&#25968;&#37327;&#26469;&#32531;&#35299;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#38543;&#26426;&#31639;&#27861;ProtoBandit&#33021;&#22815;&#22312;&#20135;&#29983;O&#65288;k^3 |S|&#65289;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#20986;&#19968;&#32452;k&#20010;&#21407;&#22411;&#65292;&#36825;&#19982;&#30446;&#26631;&#38598;&#30340;&#22823;&#23567;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a multi-armed bandit-based framework for identifying a compact set of informative data instances (i.e., the prototypes) from a source dataset $S$ that best represents a given target set $T$. Prototypical examples of a given dataset offer interpretable insights into the underlying data distribution and assist in example-based reasoning, thereby influencing every sphere of human decision-making. Current state-of-the-art prototype selection approaches require $O(|S||T|)$ similarity comparisons between source and target data points, which becomes prohibitively expensive for large-scale settings. We propose to mitigate this limitation by employing stochastic greedy search in the space of prototypical examples and multi-armed bandits for reducing the number of similarity comparisons. Our randomized algorithm, ProtoBandit, identifies a set of $k$ prototypes incurring $O(k^3|S|)$ similarity comparisons, which is independent of the size of the target set. An interesting
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#65292;&#20174;&#21018;&#20307;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.11355</link><description>&lt;p&gt;
&#20174;&#21018;&#20307;&#22270;&#20687;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65288;&#26410;&#30693;&#36136;&#37327;&#20998;&#24067;&#65289;
&lt;/p&gt;
&lt;p&gt;
Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution. (arXiv:2209.11355v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#65292;&#20174;&#21018;&#20307;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#24403;&#20302;&#32500;&#24230;&#27979;&#37327;&#19981;&#21487;&#29992;&#26102;&#65292;&#20250;&#26377;&#33258;&#30001;&#26059;&#36716;&#30340;3D&#21018;&#20307;&#30340;&#22270;&#20687;&#35266;&#23519;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#25968;&#25454;&#30340;&#39640;&#32500;&#25968;&#38459;&#27490;&#20102;&#20351;&#29992;&#32463;&#20856;&#20272;&#35745;&#25216;&#26415;&#26469;&#23398;&#20064;&#21160;&#24577;&#12290;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#20063;&#21463;&#38480;&#20110;&#19968;&#20010;&#21018;&#20307;&#22270;&#20687;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#65292;&#32780;&#36136;&#37327;&#20998;&#24067;&#19982;&#21021;&#22987;&#35282;&#36895;&#24230;&#19968;&#36215;&#20915;&#23450;&#21018;&#20307;&#26059;&#36716;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#20272;&#35745;&#21644;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#27969;&#31243;&#23558;&#21333;&#20010;&#22270;&#20687;&#26144;&#23556;&#21040;&#19982; $\mathbf{SO}(3)$ &#21516;&#32986;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#20174;&#28508;&#22312;&#23545;&#20013;&#35745;&#31639;&#35282;&#36895;&#24230;&#65292;&#24182;&#20351;&#29992;Hamilton&#36816;&#21160;&#26041;&#31243;&#39044;&#27979;&#26410;&#26469;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#26059;&#36716;&#21018;&#20307;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world settings, image observations of freely rotating 3D rigid bodies, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics. The usefulness of standard deep learning methods is also limited because an image of a rigid body reveals nothing about the distribution of mass inside the body, which, together with initial angular velocity, is what determines how the body will rotate. We present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion. We demonstrate the efficacy of our approach on new rotating rigid-body datasets of sequenc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#25366;&#25496;&#37325;&#35201;&#30340;&#20122;&#20998;&#23376;&#32593;&#32476;&#65292;&#25581;&#31034;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#21327;&#21516;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.09245</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#27744;&#21270;&#35299;&#37322;&#33647;&#29289;&#32452;&#21512;&#30340;&#21327;&#21516;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting the Mechanism of Synergism for Drug Combinations Using Attention-Based Hierarchical Graph Pooling. (arXiv:2209.09245v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09245
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#25366;&#25496;&#37325;&#35201;&#30340;&#20122;&#20998;&#23376;&#32593;&#32476;&#65292;&#25581;&#31034;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#21327;&#21516;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#33647;&#29289;&#32452;&#21512;&#20026;&#22686;&#24378;&#27835;&#30103;&#25928;&#26524;&#21644;&#20943;&#23569;&#19981;&#33391;&#21453;&#24212;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26410;&#30693;&#30340;&#30142;&#30149;&#20449;&#21495;&#20256;&#23548;&#36884;&#24452;&#65292;&#26377;&#25928;&#19988;&#21327;&#21516;&#30340;&#33647;&#29289;&#32452;&#21512;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#65288;AI&#65289;&#27169;&#22411;&#26469;&#23450;&#37327;&#39044;&#27979;&#33647;&#29289;&#32452;&#21512;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20294;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#23427;&#20204;&#26412;&#36136;&#19978;&#19981;&#21487;&#35299;&#37322;&#65292;&#36825;&#20351;&#24471;AI&#27169;&#22411;&#30340;&#32467;&#35770;&#23545;&#20154;&#31867;&#19987;&#23478;&#19981;&#36879;&#26126;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20154;&#24037;&#26234;&#33021;&#21307;&#30103;&#20013;&#30340;&#23454;&#26045;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#36890;&#36807;&#25366;&#25496;&#37325;&#35201;&#24615;&#36739;&#22823;&#30340;&#20122;&#20998;&#23376;&#32593;&#32476;&#65292;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#20851;&#38190;&#27835;&#30103;&#38774;&#28857;&#21644;&#21327;&#21516;&#26426;&#21046;&#65288;MoS&#65289;&#12290;&#21487;&#35299;&#37322;&#30340;GNN&#39044;&#27979;&#27169;&#22411;&#30340;&#20851;&#38190;&#28857;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27744;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synergistic drug combinations provide huge potentials to enhance therapeutic efficacy and to reduce adverse reactions. However, effective and synergistic drug combination prediction remains an open question because of the unknown causal disease signaling pathways. Though various deep learning (AI) models have been proposed to quantitatively predict the synergism of drug combinations, the major limitation of existing deep learning methods is that they are inherently not interpretable, which makes the conclusions of AI models untransparent to human experts, henceforth limiting the robustness of the model conclusion and the implementation ability of these models in real-world human--AI healthcare. In this paper, we develop an interpretable graph neural network (GNN) that reveals the underlying essential therapeutic targets and the mechanism of the synergy (MoS) by mining the sub-molecular network of great importance. The key point of the interpretable GNN prediction model is a novel graph
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#20174;&#21487;&#34920;&#36798;&#24615;&#21040;&#21487;&#25191;&#34892;&#24615;&#23454;&#29616;&#20102;&#33258;&#39030;&#21521;&#19979;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;&#36890;&#36807;&#24314;&#31435;&#20195;&#30721;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#26469;&#20851;&#32852;&#25991;&#26412;&#25968;&#25454;&#21644;&#20195;&#30721;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#36827;&#28145;&#24230;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.01566</link><description>&lt;p&gt;
&#20174;&#21487;&#34920;&#36798;&#24615;&#21040;&#21487;&#25191;&#34892;&#24615;&#65306;&#22312;&#26377;&#38480;&#33539;&#22260;&#20869;&#23454;&#29616;&#33258;&#39030;&#21521;&#19979;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables. (arXiv:2209.01566v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#20174;&#21487;&#34920;&#36798;&#24615;&#21040;&#21487;&#25191;&#34892;&#24615;&#23454;&#29616;&#20102;&#33258;&#39030;&#21521;&#19979;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;&#36890;&#36807;&#24314;&#31435;&#20195;&#30721;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#26469;&#20851;&#32852;&#25991;&#26412;&#25968;&#25454;&#21644;&#20195;&#30721;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#36827;&#28145;&#24230;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20195;&#30721;&#29983;&#25104;&#26159;&#36719;&#20214;&#24037;&#31243;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#39064;&#65292;&#37319;&#29992;&#31070;&#32463;&#27169;&#22411;&#20026;&#39044;&#26399;&#21151;&#33021;&#29983;&#25104;&#20195;&#30721;&#12290;&#30001;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#21644;&#36719;&#20214;&#23618;&#27425;&#24847;&#35782;&#65292;&#23427;&#20204;&#22312;&#39033;&#30446;&#32423;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#25506;&#32034;&#20195;&#30721;&#29983;&#25104;&#30340;&#28508;&#22312;&#25913;&#36827;&#65292;&#25105;&#20204;&#35753;&#20854;&#21442;&#19982;&#20174;&#8220;&#21487;&#34920;&#36798;&#24615;&#8221;&#21040;&#8220;&#21487;&#25191;&#34892;&#24615;&#8221;&#30340;&#33258;&#39030;&#21521;&#19979;&#24320;&#21457;&#65292;&#36825;&#22312;&#26377;&#38480;&#30340;&#33539;&#22260;&#20869;&#26159;&#21487;&#33021;&#30340;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#23427;&#20174;&#22823;&#37327;&#30340;&#26679;&#26412;&#12289;&#29305;&#24449;&#21644;&#30693;&#35782;&#20013;&#21463;&#30410;&#12290;&#20316;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20195;&#30721;&#25968;&#25454;&#19978;&#24314;&#31435;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#21363;&#20195;&#30721;&#20998;&#31867;&#27861;&#65292;&#21033;&#29992;&#20195;&#30721;&#20449;&#24687;&#30340;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#23618;&#35821;&#20041;&#37329;&#23383;&#22612;(SP)&#26469;&#20851;&#32852;&#25991;&#26412;&#25968;&#25454;&#21644;&#20195;&#30721;&#25968;&#25454;&#12290;&#23427;&#35782;&#21035;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#20851;&#20110;&#24320;&#21457;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#25581;&#31034;&#20102;&#36719;&#20214;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep code generation is a topic of deep learning for software engineering (DL4SE), which adopts neural models to generate code for the intended functions. Since end-to-end neural methods lack domain knowledge and software hierarchy awareness, they tend to perform poorly w.r.t project-level tasks. To systematically explore the potential improvements of code generation, we let it participate in the whole top-down development from \emph{expressibles} to \emph{executables}, which is possible in limited scopes. In the process, it benefits from massive samples, features, and knowledge. As the foundation, we suggest building a taxonomy on code data, namely code taxonomy, leveraging the categorization of code information. Moreover, we introduce a three-layer semantic pyramid (SP) to associate text data and code data. It identifies the information of different abstraction levels, and thus introduces the domain knowledge on development and reveals the hierarchy of software. Furthermore, we propo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#37327;&#36755;&#20837;&#21644;&#20989;&#25968;&#36755;&#20986;&#20043;&#38388;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20989;&#25968;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#37327;&#39044;&#27979;&#21464;&#37327;&#25110;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#21487;&#20197;&#25511;&#21046;&#39044;&#27979;&#26354;&#32447;&#30340;&#24179;&#28369;&#31243;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.05776</link><description>&lt;p&gt;
&#26631;&#37327;&#36755;&#20837;&#21644;&#20989;&#25968;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Neural Networks for Scalar Input and Functional Output. (arXiv:2208.05776v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#37327;&#36755;&#20837;&#21644;&#20989;&#25968;&#36755;&#20986;&#20043;&#38388;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20989;&#25968;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#37327;&#39044;&#27979;&#21464;&#37327;&#25110;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#21487;&#20197;&#25511;&#21046;&#39044;&#27979;&#26354;&#32447;&#30340;&#24179;&#28369;&#31243;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#32452;&#26631;&#37327;&#39044;&#27979;&#21464;&#37327;&#19978;&#22238;&#24402;&#20989;&#25968;&#21709;&#24212;&#21487;&#20197;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24403;&#26377;&#22823;&#37327;&#39044;&#27979;&#21464;&#37327;&#25110;&#32773;&#39044;&#27979;&#21464;&#37327;&#19982;&#21709;&#24212;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#38750;&#32447;&#24615;&#30340;&#26102;&#20505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#39044;&#27979;&#26631;&#37327;&#36755;&#20837;&#19979;&#30340;&#20989;&#25968;&#21709;&#24212;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20989;&#25968;&#21709;&#24212;&#36716;&#21270;&#20026;&#26377;&#38480;&#32500;&#24230;&#34920;&#31034;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#36755;&#20986;&#35813;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#24182;&#24341;&#20837;&#19981;&#21516;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#22343;&#21248;&#21644;&#19981;&#22343;&#21248;&#38388;&#38548;&#30340;&#25968;&#25454;&#65292;&#24182;&#21487;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#24179;&#28369;&#24809;&#32602;&#39033;&#26469;&#25511;&#21046;&#39044;&#27979;&#26354;&#32447;&#30340;&#24179;&#28369;&#31243;&#24230;&#12290;&#23454;&#29616;&#36825;&#20123;&#29305;&#24615;&#30340;&#22256;&#38590;&#22312;&#20110;&#23450;&#20041;&#21487;&#20197;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The regression of a functional response on a set of scalar predictors can be a challenging task, especially if there is a large number of predictors, or the relationship between those predictors and the response is nonlinear. In this work, we propose a solution to this problem: a feed-forward neural network (NN) designed to predict a functional response using scalar inputs. First, we transform the functional response to a finite-dimensional representation and construct an NN that outputs this representation. Then, we propose to modify the output of an NN via the objective function and introduce different objective functions for network training. The proposed models are suited for both regularly and irregularly spaced data, and a roughness penalty can be further applied to control the smoothness of the predicted curve. The difficulty in implementing both those features lies in the definition of objective functions that can be back-propagated. In our experiments, we demonstrate that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32858;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Agg-GNNs&#65289;&#30340;&#31283;&#23450;&#24615;&#29305;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#31283;&#23450;&#24615;&#30028;&#38480;&#19982;&#31532;&#19968;&#23618;&#28388;&#27874;&#22120;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2207.03678</link><description>&lt;p&gt;
Aggregation&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stability of Aggregation Graph Neural Networks. (arXiv:2207.03678v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32858;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Agg-GNNs&#65289;&#30340;&#31283;&#23450;&#24615;&#29305;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#31283;&#23450;&#24615;&#30028;&#38480;&#19982;&#31532;&#19968;&#23618;&#28388;&#27874;&#22120;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32858;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Agg-GNNs&#65289;&#22312;&#32771;&#34385;&#24213;&#23618;&#22270;&#21464;&#21270;&#26102;&#30340;&#31283;&#23450;&#24615;&#29305;&#24615;&#12290;Agg-GNN&#26159;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#20854;&#20013;&#23558;&#20449;&#24687;&#23450;&#20041;&#22312;&#22270;&#30340;&#33410;&#28857;&#19978;&#65292;&#20294;&#22312;&#32463;&#36807;&#22810;&#27425;&#22270;&#31227;&#25805;&#20316;&#21518;&#65292;&#36890;&#36807;&#27431;&#20960;&#37324;&#24471;CNN&#22312;&#33410;&#28857;&#19978;&#36827;&#34892;&#36880;&#22359;&#22788;&#29702;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19982;&#36890;&#29992;Agg-GNN&#20851;&#32852;&#30340;&#26144;&#23556;&#31639;&#23376;&#30340;&#31283;&#23450;&#24615;&#30028;&#38480;&#65292;&#24182;&#25351;&#23450;&#20102;&#36825;&#31181;&#31639;&#23376;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#21487;&#20197;&#23545;&#21464;&#24418;&#20445;&#25345;&#31283;&#23450;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31283;&#23450;&#24615;&#30028;&#38480;&#30001;&#20316;&#29992;&#20110;&#27599;&#20010;&#33410;&#28857;&#30340;CNN&#30340;&#31532;&#19968;&#23618;&#28388;&#27874;&#22120;&#30340;&#23646;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32858;&#21512;&#25968;&#37327;&#12289;&#28388;&#27874;&#22120;&#30340;&#36873;&#25321;&#24615;&#20197;&#21450;&#31283;&#23450;&#24615;&#24120;&#25968;&#30340;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#32039;&#23494;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#32467;&#35770;&#65306;&#22312;Agg-GNNs&#20013;&#65292;&#26144;&#23556;&#31639;&#23376;&#30340;&#36873;&#25321;&#24615;&#20165;&#19982;CNN&#38454;&#27573;&#30340;&#31532;&#19968;&#23618;&#28388;&#27874;&#22120;&#30340;&#23646;&#24615;&#26377;&#20851;&#12290;&#36825;&#26174;&#31034;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we study the stability properties of aggregation graph neural networks (Agg-GNNs) considering perturbations of the underlying graph. An Agg-GNN is a hybrid architecture where information is defined on the nodes of a graph, but it is processed block-wise by Euclidean CNNs on the nodes after several diffusions on the graph shift operator. We derive stability bounds for the mapping operator associated to a generic Agg-GNN, and we specify conditions under which such operators can be stable to deformations. We prove that the stability bounds are defined by the properties of the filters in the first layer of the CNN that acts on each node. Additionally, we show that there is a close relationship between the number of aggregations, the filter's selectivity, and the size of the stability constants. We also conclude that in Agg-GNNs the selectivity of the mapping operators is tied to the properties of the filters only in the first layer of the CNN stage. This shows a substantial d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#33258;&#36866;&#24212;&#21644;&#33258;&#36866;&#24212;&#24773;&#20917;&#19979;&#65292;&#21463;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#30340;&#32463;&#20856;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#23548;&#33268;&#19968;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#27424;&#20195;&#34920;&#25110;&#36807;&#20195;&#34920;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#19979;&#36873;&#25321;&#19968;&#32452;&#39033;&#20197;&#26368;&#22823;&#21270;&#23376;&#27169;&#22359;&#25928;&#29992;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2207.03364</link><description>&lt;p&gt;
&#22312;&#33258;&#36866;&#24212;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#20013;&#30340;&#32676;&#20307;&#24179;&#31561;
&lt;/p&gt;
&lt;p&gt;
Group Equality in Adaptive Submodular Maximization. (arXiv:2207.03364v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#33258;&#36866;&#24212;&#21644;&#33258;&#36866;&#24212;&#24773;&#20917;&#19979;&#65292;&#21463;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#30340;&#32463;&#20856;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#23548;&#33268;&#19968;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#27424;&#20195;&#34920;&#25110;&#36807;&#20195;&#34920;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#19979;&#36873;&#25321;&#19968;&#32452;&#39033;&#20197;&#26368;&#22823;&#21270;&#23376;&#27169;&#22359;&#25928;&#29992;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#33258;&#36866;&#24212;&#21644;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#65292;&#21463;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#30340;&#32463;&#20856;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#21253;&#25324;&#25968;&#25454;&#27719;&#24635;&#12289;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#26368;&#22823;&#21270;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#28385;&#36275;&#23376;&#27169;&#22359;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#30340;&#26680;&#24515;&#26159;&#22312;&#21508;&#31181;&#32422;&#26463;&#19979;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#39033;&#65288;&#20363;&#22914;&#65292;&#25968;&#25454;&#28857;&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#30340;&#35774;&#35745;&#27809;&#26377;&#32771;&#34385;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#23548;&#33268;&#26576;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#27424;&#20195;&#34920;&#25110;&#36807;&#20195;&#34920;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#30740;&#31350;&#20855;&#26377;&#32676;&#20307;&#24179;&#31561;&#30340;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#19979;&#36873;&#25321;&#19968;&#32452;&#39033;&#20197;&#26368;&#22823;&#21270;&#19968;&#20010;&#65288;&#21487;&#33021;&#38750;&#21333;&#35843;&#65289;&#30340;&#23376;&#27169;&#22359;&#25928;&#29992;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the classic submodular maximization problem subject to a group equality constraint under both non-adaptive and adaptive settings. It has been shown that the utility function of many machine learning applications, including data summarization, influence maximization in social networks, and personalized recommendation, satisfies the property of submodularity. Hence, maximizing a submodular function subject to various constraints can be found at the heart of many of those applications. On a high level, submodular maximization aims to select a group of most representative items (e.g., data points). However, the design of most existing algorithms does not incorporate the fairness constraint, leading to under- or over-representation of some particular groups. This motivates us to study the submodular maximization problem with group equality, where we aim to select a group of items to maximize a (possibly non-monotone) submodular utility function subject to a group equ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#22352;&#26631;&#21322;&#26799;&#24230;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#21482;&#26356;&#26032;&#19968;&#20010;&#22352;&#26631;&#22359;&#65292;&#32771;&#34385;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#26377;&#30028;&#27425;&#26799;&#24230;&#20551;&#35774;&#65292;&#24182;&#22312;&#20984;&#21644;&#38750;&#20984;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20026;$\widetilde{\mathcal{O}}(1/\sqrt{k})$&#21644;$\mathcal{O}(1/k)$&#12290;</title><link>http://arxiv.org/abs/2206.14981</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#20248;&#21270;&#30340;&#38543;&#26426;&#22352;&#26631;&#21322;&#26799;&#24230;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomized Coordinate Subgradient Method for Nonsmooth Optimization. (arXiv:2206.14981v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#22352;&#26631;&#21322;&#26799;&#24230;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#21482;&#26356;&#26032;&#19968;&#20010;&#22352;&#26631;&#22359;&#65292;&#32771;&#34385;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#26377;&#30028;&#27425;&#26799;&#24230;&#20551;&#35774;&#65292;&#24182;&#22312;&#20984;&#21644;&#38750;&#20984;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20026;$\widetilde{\mathcal{O}}(1/\sqrt{k})$&#21644;$\mathcal{O}(1/k)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#38750;&#20809;&#28369;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#38750;&#20984;&#24615;&#65288;&#38750;&#20809;&#28369;&#24369;&#20984;&#24615;&#65289;&#20248;&#21270;&#38382;&#39064;&#30340;&#8220;&#38543;&#26426;&#22352;&#26631;&#21322;&#26799;&#24230;&#27861;&#8221;&#65288;RCS&#65289;&#12290;RCS&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#22352;&#26631;&#22359;&#36827;&#34892;&#26356;&#26032;&#65292;&#27604;&#26356;&#26032;&#25152;&#26377;&#22352;&#26631;&#26356;&#23454;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#26377;&#30028;&#27425;&#26799;&#24230;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#27604;&#20256;&#32479;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#26356;&#20026;&#36890;&#29992;&#65292;&#20197;&#36866;&#24212;&#23454;&#38469;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#31181;&#24191;&#20041;&#30340;Lipschitz&#31867;&#22411;&#20551;&#35774;&#23545;RCS&#22312;&#20984;&#21644;&#38750;&#20984;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;$f$&#20026;&#38750;&#20809;&#28369;&#20984;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#22312;&#26399;&#26395;&#19978;&#24314;&#31435;&#20102;$\widetilde{\mathcal{O}}(1/\sqrt{k})$&#25910;&#25947;&#36895;&#29575;&#65292;&#20197;&#21450;&#22312;&#27425;&#20248;&#38388;&#38553;&#26041;&#38754;&#30340;$\tilde o(1/\sqrt{k})$&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#22914;&#26524;$f$&#36827;&#19968;&#27493;&#28385;&#36275;&#20840;&#23616;&#20108;&#27425;&#22686;&#38271;&#26465;&#20214;&#65292;&#21017;&#26174;&#31034;&#20102;&#25913;&#36827;&#30340;$\mathcal{O}(1/k)$&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose the {Randomized Coordinate Subgradient method} (RCS) for solving nonsmooth convex and nonsmooth nonconvex (nonsmooth weakly convex) optimization problems. RCS randomly selects one block coordinate to update at each iteration, making it more practical than updating all coordinates. We consider the linearly bounded subgradients assumption for the objective function, which is more general than the traditional Lipschitz continuity assumption, to account for practical scenarios. We then conduct thorough convergence analysis for RCS in both convex and nonconvex cases based on this generalized Lipschitz-type assumption. Specifically, we establish the $\widetilde{\mathcal{O}}(1/\sqrt{k})$ convergence rate in expectation and the $\tilde o(1/\sqrt{k})$ almost sure asymptotic convergence rate in terms of suboptimality gap when $f$ is nonsmooth convex. If $f$ further satisfies the global quadratic growth condition, the improved $\mathcal{O}(1/k)$ rate is shown in terms of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#21442;&#19982;&#21644;&#37325;&#26032;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32773;&#21644;&#29992;&#25143;&#23376;&#32676;&#20855;&#26377;&#39118;&#38505;&#20943;&#23569;&#24615;&#36136;&#26102;&#65292;&#21807;&#19968;&#30340;&#31283;&#23450;&#22343;&#34913;&#26159;&#32454;&#20998;&#30340;&#65292;&#23558;&#23376;&#32676;&#20998;&#37197;&#32473;&#21333;&#20010;&#23398;&#20064;&#32773;&#12290;&#21151;&#21033;&#20027;&#20041;&#31038;&#20250;&#26368;&#20248;&#26159;&#19968;&#20010;&#31283;&#23450;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2206.02667</link><description>&lt;p&gt;
&#20174;&#21442;&#19982;&#24230;&#21160;&#24577;&#21644;&#22810;&#23398;&#20064;&#32773;&#37325;&#26032;&#35757;&#32451;&#20013;&#20135;&#29983;&#30340;&#32039;&#24613;&#32454;&#20998;
&lt;/p&gt;
&lt;p&gt;
Emergent segmentation from participation dynamics and multi-learner retraining. (arXiv:2206.02667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02667
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#21442;&#19982;&#21644;&#37325;&#26032;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32773;&#21644;&#29992;&#25143;&#23376;&#32676;&#20855;&#26377;&#39118;&#38505;&#20943;&#23569;&#24615;&#36136;&#26102;&#65292;&#21807;&#19968;&#30340;&#31283;&#23450;&#22343;&#34913;&#26159;&#32454;&#20998;&#30340;&#65292;&#23558;&#23376;&#32676;&#20998;&#37197;&#32473;&#21333;&#20010;&#23398;&#20064;&#32773;&#12290;&#21151;&#21033;&#20027;&#20041;&#31038;&#20250;&#26368;&#20248;&#26159;&#19968;&#20010;&#31283;&#23450;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26381;&#21153;&#20013;&#36873;&#25321;&#21442;&#19982;&#65292;&#24448;&#24448;&#22522;&#20110;&#35813;&#26381;&#21153;&#30340;&#36136;&#37327;&#65292;&#24433;&#21709;&#20102;&#26381;&#21153;&#23398;&#20064;&#21644;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#23398;&#20064;&#32773;&#21644;&#29992;&#25143;&#23376;&#32676;&#37117;&#20855;&#26377;&#39118;&#38505;&#20943;&#23569;&#24615;&#36136;&#26102;&#65292;&#21442;&#19982;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21160;&#24577;&#29983;&#25104;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#26799;&#24230;&#19979;&#38477;&#12289;&#20056;&#27861;&#26435;&#37325;&#31561;&#24191;&#27867;&#30340;&#26356;&#26032;&#26041;&#27861;&#12290;&#20030;&#20010;&#20363;&#23376;&#65292;&#20551;&#35774;&#20010;&#20307;&#36873;&#25321;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#33457;&#36153;&#26102;&#38388;&#30340;&#27604;&#20363;&#19982;&#27599;&#20010;&#24179;&#21488;&#23545;&#20182;&#20204;&#30340;&#24037;&#20316;&#25928;&#26524;&#25104;&#27604;&#20363;&#12290;&#27599;&#20010;&#24179;&#21488;&#36824;&#20250;&#25910;&#38598;&#20854;&#27963;&#36291;&#29992;&#25143;&#30340;&#25968;&#25454;&#65292;&#24182;&#29992;&#26799;&#24230;&#27493;&#39588;&#26356;&#26032;&#21442;&#25968;&#12290;&#23545;&#20110;&#36825;&#20010;&#20363;&#23376;&#21644;&#25105;&#20204;&#30340;&#19968;&#33324;&#21160;&#24577;&#31867;&#21035;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21807;&#19968;&#30340;&#28176;&#36817;&#31283;&#23450;&#22343;&#34913;&#26159;&#32454;&#20998;&#30340;&#65292;&#23558;&#23376;&#32676;&#20998;&#37197;&#32473;&#21333;&#20010;&#23398;&#20064;&#32773;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#21151;&#21033;&#20027;&#20041;&#31038;&#20250;&#26368;&#20248;&#26159;&#19968;&#20010;&#31283;&#23450;&#22343;&#34913;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#26174;&#31034;&#37325;&#22797;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#21487;&#33021;&#19981;&#20250;&#23545;&#38887;&#24615;&#21644;&#21033;&#30410;&#36827;&#34892;&#20219;&#20309;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The choice to participate in a data-driven service, often made on the basis of quality of that service, influences the ability of the service to learn and improve. We study the participation and retraining dynamics that arise when both the learners and sub-populations of users are \emph{risk-reducing}, which cover a broad class of updates including gradient descent, multiplicative weights, etc. Suppose, for example, that individuals choose to spend their time amongst social media platforms proportionally to how well each platform works for them. Each platform also gathers data about its active users, which it uses to update parameters with a gradient step. For this example and for our general class of dynamics, we show that the only asymptotically stable equilibria are segmented, with sub-populations allocated to a single learner. Under mild assumptions, the utilitarian social optimum is a stable equilibrium. In contrast to previous work, which shows that repeated risk minimization can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22238;&#24402;&#27169;&#22411;&#20013;&#35780;&#20272;&#21024;&#38500;&#21644;&#25554;&#20837;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#35745;&#31639;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;Kernel SHAP&#22312;&#32508;&#21512;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26356;&#24555;&#36895;&#30340;&#26367;&#20195;&#25351;&#26631;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2205.12423</link><description>&lt;p&gt;
&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#21024;&#38500;&#21644;&#25554;&#20837;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Deletion and Insertion Tests in Regression Models. (arXiv:2205.12423v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22238;&#24402;&#27169;&#22411;&#20013;&#35780;&#20272;&#21024;&#38500;&#21644;&#25554;&#20837;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#35745;&#31639;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;Kernel SHAP&#22312;&#32508;&#21512;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26356;&#24555;&#36895;&#30340;&#26367;&#20195;&#25351;&#26631;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#26159;&#30830;&#23450;&#40657;&#30418;&#20989;&#25968;$f$&#39044;&#27979;&#32972;&#21518;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;Petsiuk&#31561;&#20154;&#65288;2018&#65289;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#27979;&#35797;&#21487;&#20197;&#29992;&#26469;&#35780;&#21028;&#23545;&#20110;&#20998;&#31867;&#20013;&#20687;&#32032;&#20174;&#37325;&#35201;&#21040;&#19981;&#37325;&#35201;&#36827;&#34892;&#25490;&#24207;&#30340;&#31639;&#27861;&#30340;&#36136;&#37327;&#12290;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#20197;$f$&#30340;&#20027;&#25928;&#24212;&#21644;&#20132;&#20114;&#20316;&#29992;&#26469;&#34913;&#37327;&#20854;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#22312;&#36755;&#20837;&#38543;&#26426;&#39034;&#24207;&#19979;AUC&#30340;&#26399;&#26395;&#20540;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#22238;&#24402;&#35774;&#32622;&#30340;&#30452;&#32447;&#19978;&#26041;&#38754;&#31215;&#30340;&#26367;&#20195;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25351;&#26631;&#23558;&#38598;&#25104;&#26799;&#24230;&#65288;IG&#65289;&#35745;&#31639;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#19982;Kernel SHAP&#65288;KS&#65289;&#12289;LIME&#12289;DeepLIFT&#12289;vanilla gradient&#21644;input$\times$gradient&#26041;&#27861;&#35745;&#31639;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#25105;&#20204;&#32771;&#34385;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;KS&#30340;&#25972;&#20307;&#34920;&#29616;&#26368;&#22909;&#65292;&#20294;&#35745;&#31639;&#20195;&#20215;&#24456;&#39640;&#12290;&#25105;&#20204;&#21457;&#29616;IG&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#21644;KS&#34920;&#29616;&#30456;&#36817;&#65292;&#20294;&#35745;&#31639;&#26356;&#24555;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
A basic task in explainable AI (XAI) is to identify the most important features behind a prediction made by a black box function $f$. The insertion and deletion tests of Petsiuk et al. (2018) can be used to judge the quality of algorithms that rank pixels from most to least important for a classification. Motivated by regression problems we establish a formula for their area under the curve (AUC) criteria in terms of certain main effects and interactions in an anchored decomposition of $f$. We find an expression for the expected value of the AUC under a random ordering of inputs to $f$ and propose an alternative area above a straight line for the regression setting. We use this criterion to compare feature importances computed by integrated gradients (IG) to those computed by Kernel SHAP (KS) as well as LIME, DeepLIFT, vanilla gradient and input$\times$gradient methods. KS has the best overall performance in two datasets we consider but it is very expensive to compute. We find that IG 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#27861;&#30340;&#32467;&#26500;&#21270;&#36328;&#24230;&#36873;&#25321;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#36328;&#24230;&#32423;&#27880;&#37322;&#65292;&#25682;&#24323;&#20102;&#36138;&#24515;&#36328;&#24230;&#36873;&#25321;&#26041;&#26696;&#65292;&#20026;&#20849;&#25351;&#28040;&#35299;&#21644;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#24102;&#26469;&#20102;&#23454;&#35777;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2205.03977</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#36328;&#24230;&#36873;&#25321;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Structured Span Selector. (arXiv:2205.03977v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#27861;&#30340;&#32467;&#26500;&#21270;&#36328;&#24230;&#36873;&#25321;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#36328;&#24230;&#32423;&#27880;&#37322;&#65292;&#25682;&#24323;&#20102;&#36138;&#24515;&#36328;&#24230;&#36873;&#25321;&#26041;&#26696;&#65292;&#20026;&#20849;&#25351;&#28040;&#35299;&#21644;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#24102;&#26469;&#20102;&#23454;&#35777;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#20849;&#25351;&#28040;&#35299;&#21644;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65292;&#38656;&#35201;&#36873;&#25321;&#25991;&#26412;&#36328;&#24230;&#24182;&#23545;&#20854;&#36827;&#34892;&#20915;&#31574;&#12290;&#36825;&#20123;&#20219;&#21153;&#30340;&#19968;&#31181;&#20856;&#22411;&#26041;&#27861;&#26159;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#36328;&#24230;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36138;&#23146;&#22320;&#36873;&#25321;&#36328;&#24230;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#19979;&#28216;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#27809;&#26377;&#32467;&#21512;&#20219;&#20309;&#24402;&#32435;&#20559;&#32622;&#26469;&#30830;&#23450;&#24212;&#35813;&#36873;&#25321;&#21738;&#31181;&#36328;&#24230;&#65292;&#20363;&#22914;&#36873;&#25321;&#30340;&#36328;&#24230;&#24448;&#24448;&#26159;&#21477;&#27861;&#25104;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#27861;&#30340;&#32467;&#26500;&#21270;&#36328;&#24230;&#36873;&#25321;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#21033;&#29992;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#30340;&#37096;&#20998;&#36328;&#24230;&#32423;&#27880;&#37322;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#21551;&#21457;&#24335;&#30340;&#36138;&#23146;&#36328;&#24230;&#36873;&#25321;&#26041;&#26696;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26368;&#20339;&#19968;&#32452;&#36328;&#24230;&#19978;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#36328;&#24230;&#39044;&#27979;&#20219;&#21153;&#65306;&#20849;&#25351;&#28040;&#35299;&#21644;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#35777;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing tasks, e.g., coreference resolution and semantic role labeling, require selecting text spans and making decisions about them. A typical approach to such tasks is to score all possible spans and greedily select spans for task-specific downstream processing. This approach, however, does not incorporate any inductive bias about what sort of spans ought to be selected, e.g., that selected spans tend to be syntactic constituents. In this paper, we propose a novel grammar-based structured span selection model which learns to make use of the partial span-level annotation provided for such problems. Compared to previous approaches, our approach gets rid of the heuristic greedy span selection scheme, allowing us to model the downstream task on an optimal set of spans. We evaluate our model on two popular span prediction tasks: coreference resolution and semantic role labeling. We show empirical improvements on both.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24418;&#19978;&#30340;min-max&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;Riemannian Hamiltonian&#26041;&#27861;&#20316;&#20026;&#20854;&#20195;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;Hamiltonian&#20989;&#25968;&#65292;&#21487;&#20197;&#24471;&#21040;&#25152;&#38656;&#30340;min-max&#38797;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;geodesic-bilinear&#20248;&#21270;&#38382;&#39064;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#36890;&#36807;&#35299;&#20915;&#20195;&#29702;&#38382;&#39064;&#21487;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#25628;&#32034;&#26041;&#21521;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.11418</link><description>&lt;p&gt;
&#27969;&#24418;&#19978;&#30340;Riemannian Hamiltonian&#26041;&#27861;&#29992;&#20110;min-max&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Riemannian Hamiltonian methods for min-max optimization on manifolds. (arXiv:2204.11418v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24418;&#19978;&#30340;min-max&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;Riemannian Hamiltonian&#26041;&#27861;&#20316;&#20026;&#20854;&#20195;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;Hamiltonian&#20989;&#25968;&#65292;&#21487;&#20197;&#24471;&#21040;&#25152;&#38656;&#30340;min-max&#38797;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;geodesic-bilinear&#20248;&#21270;&#38382;&#39064;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#36890;&#36807;&#35299;&#20915;&#20195;&#29702;&#38382;&#39064;&#21487;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#25628;&#32034;&#26041;&#21521;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24418;&#19978;&#30340;min-max&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;Riemannian Hamiltonian&#20989;&#25968;&#65292;&#20854;&#26368;&#23567;&#21270;&#20316;&#20026;&#35299;&#20915;&#21407;&#22987;min-max&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;&#22312;Riemannian Polyak-{\L}ojasiewicz&#26465;&#20214;&#19979;&#65292;&#20854;&#26368;&#23567;&#20540;&#23545;&#24212;&#20110;&#25152;&#38656;&#30340;min-max&#38797;&#28857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#28385;&#36275;&#27492;&#26465;&#20214;&#30340;&#24773;&#20917;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;geodesic-bilinear&#20248;&#21270;&#65292;&#22312;&#35299;&#20915;&#20195;&#29702;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24471;&#21040;&#27491;&#30830;&#30340;&#20840;&#23616;&#26368;&#20248;&#25628;&#32034;&#26041;&#21521;&#65292;&#32780;&#22312;min-max&#24418;&#24335;&#21270;&#20013;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;Hamiltonian&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Riemannian Hamiltonian&#26041;&#27861;&#65288;RHM&#65289;&#24182;&#25552;&#20986;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;RHM&#25193;&#23637;&#21040;&#21253;&#25324;&#20849;&#35782;&#27491;&#21017;&#21270;&#21644;&#38543;&#26426;&#35774;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#22914;&#23376;&#31354;&#38388;&#40065;&#26834;Wasserstein&#36317;&#31163;&#12289;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#35757;&#32451;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#26469;&#35828;&#26126;&#25152;&#25552;&#20986;&#30340;RHM&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study min-max optimization problems on Riemannian manifolds. We introduce a Riemannian Hamiltonian function, minimization of which serves as a proxy for solving the original min-max problems. Under the Riemannian Polyak--{\L}ojasiewicz condition on the Hamiltonian function, its minimizer corresponds to the desired min-max saddle point. We also provide cases where this condition is satisfied. For geodesic-bilinear optimization in particular, solving the proxy problem leads to the correct search direction towards global optimality, which becomes challenging with the min-max formulation. To minimize the Hamiltonian function, we propose Riemannian Hamiltonian methods (RHM) and present their convergence analyses. We extend RHM to include consensus regularization and to the stochastic setting. We illustrate the efficacy of the proposed RHM in applications such as subspace robust Wasserstein distance, robust training of neural networks, and generative adversarial networks.
&lt;/p&gt;</description></item><item><title>ConceptEvo&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25581;&#31034;&#27010;&#24565;&#30340;&#20135;&#29983;&#21644;&#28436;&#21464;&#65292;&#24182;&#36890;&#36807;&#20154;&#26426;&#35780;&#20272;&#21644;&#23454;&#39564;&#35777;&#26126;&#20854;&#21457;&#29616;&#23545;&#27169;&#22411;&#21644;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2203.16475</link><description>&lt;p&gt;
ConceptEvo&#65306;&#35299;&#35835;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#30340;&#27010;&#24565;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16475
&lt;/p&gt;
&lt;p&gt;
ConceptEvo&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25581;&#31034;&#27010;&#24565;&#30340;&#20135;&#29983;&#21644;&#28436;&#21464;&#65292;&#24182;&#36890;&#36807;&#20154;&#26426;&#35780;&#20272;&#21644;&#23454;&#39564;&#35777;&#26126;&#20854;&#21457;&#29616;&#23545;&#27169;&#22411;&#21644;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ConceptEvo&#65292;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#32479;&#19968;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#25581;&#31034;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#27010;&#24565;&#30340;&#20135;&#29983;&#21644;&#28436;&#21464;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22635;&#34917;&#20102;DNN&#35299;&#37322;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#31354;&#30333;&#65292;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#35757;&#32451;&#21518;&#30340;&#35299;&#37322;&#12290;ConceptEvo&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#25216;&#26415;&#36129;&#29486;&#65306;&#65288;1&#65289;&#19968;&#31181;&#29983;&#25104;&#32479;&#19968;&#35821;&#20041;&#31354;&#38388;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#19981;&#21516;&#27169;&#22411;&#30340;&#24182;&#34892;&#27604;&#36739;&#65307;&#65288;2&#65289;&#19968;&#31181;&#21457;&#29616;&#21644;&#37327;&#21270;&#31867;&#21035;&#39044;&#27979;&#20013;&#37325;&#35201;&#27010;&#24565;&#28436;&#21464;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;260&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#22823;&#35268;&#27169;&#20154;&#26426;&#35780;&#20272;&#21644;&#23450;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ConceptEvo&#21487;&#20197;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#26377;&#24847;&#20041;&#19988;&#23545;&#39044;&#27979;&#37325;&#35201;&#30340;&#28436;&#21464;&#12290;ConceptEvo&#36866;&#29992;&#20110;&#29616;&#20195;&#65288;ConvNeXt&#65289;&#21644;&#32463;&#20856;&#30340;DNNs&#65288;&#20363;&#22914;VGGs&#65292;InceptionV3&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work fills a critical gap in DNN interpretation research, as existing methods focus on post-hoc interpretation after training. ConceptEvo presents two novel technical contributions: (1) an algorithm that generates a unified semantic space that enables side-by-side comparison of different models during training; and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation with 260 participants and quantitative experiments, we show that ConceptEvo discovers evolutions across different models that are meaningful to humans and important for predictions. ConceptEvo works for both modern (ConvNeXt) and classic DNNs (e.g., VGGs, InceptionV3).
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21152;&#36895;&#24341;&#21147;&#27874;&#20195;&#29702;&#27874;&#24418;&#30340;&#24314;&#31435;&#65292;&#36890;&#36807;&#28155;&#21152;&#31532;&#20108;&#20010;&#32593;&#32476;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#27874;&#24418;&#30340;&#19981;&#21305;&#37197;&#31243;&#24230;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#20182;&#26041;&#27861;&#26469;&#25552;&#39640;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.08434</link><description>&lt;p&gt;
&#28145;&#24230;&#27531;&#24046;&#35823;&#24046;&#21644;&#8220;&#23398;&#38712;&#25216;&#24039;&#8221;&#29992;&#20110;&#24341;&#21147;&#27874;&#27169;&#25311;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Deep Residual Error and Bag-of-Tricks Learning for Gravitational Wave Surrogate Modeling. (arXiv:2203.08434v2 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08434
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21152;&#36895;&#24341;&#21147;&#27874;&#20195;&#29702;&#27874;&#24418;&#30340;&#24314;&#31435;&#65292;&#36890;&#36807;&#28155;&#21152;&#31532;&#20108;&#20010;&#32593;&#32476;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#27874;&#24418;&#30340;&#19981;&#21305;&#37197;&#31243;&#24230;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#20182;&#26041;&#27861;&#26469;&#25552;&#39640;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24341;&#21147;&#27874;&#22825;&#25991;&#23398;&#20013;&#65292;&#24050;&#32463;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21152;&#24555;&#20102;&#26059;&#36716;&#23545;&#40784;&#40657;&#27934;&#20108;&#36827;&#21046;&#31995;&#32479;&#30340;&#20195;&#29702;&#27874;&#24418;&#26500;&#24314;&#65292;&#20197;&#21450;&#20854;&#20182;&#24212;&#29992;&#12290;&#25105;&#20204;&#38754;&#20020;&#30528;&#23545;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#27531;&#24046;&#35823;&#24046;&#24314;&#27169;&#30340;&#25361;&#25112;&#65292;&#35813;&#27169;&#22411;&#27169;&#25311;&#20195;&#29702;&#27874;&#24418;&#23637;&#24320;&#31995;&#25968;&#30340;&#35823;&#24046;&#65288;&#29305;&#21035;&#26159;&#27874;&#24418;&#30340;&#30456;&#20301;&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#35823;&#24046;&#20855;&#26377;&#36275;&#22815;&#30340;&#32467;&#26500;&#21487;&#20197;&#34987;&#31532;&#20108;&#20010;&#32593;&#32476;&#23398;&#20064;&#21040;&#12290;&#36890;&#36807;&#28155;&#21152;&#31532;&#20108;&#20010;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#39564;&#35777;&#38598;&#20013;&#27874;&#24418;&#30340;&#26368;&#22823;&#19981;&#21305;&#37197;&#31243;&#24230;&#20943;&#23569;&#20102;13.4&#20493;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#20960;&#31181;&#25913;&#36827;&#20195;&#29702;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#22914;&#21033;&#29992;&#27874;&#24418;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12289;&#35757;&#32451;&#38598;&#30340;&#22686;&#24378;&#12289;&#36755;&#20837;&#31354;&#38388;&#30340;&#35299;&#21078;&#12289;&#20351;&#29992;&#19987;&#29992;&#32593;&#32476;&#39044;&#27979;&#36755;&#20986;&#30340;&#31995;&#25968;&#20197;&#21450;&#36755;&#20986;&#22686;&#24378;&#12290;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#35266;&#23519;&#21040;&#19968;&#20123;&#23567;&#30340;&#25913;&#36827;&#65292;&#20294;&#26368;&#26174;&#33879;&#30340;&#25913;&#36827;&#20173;&#28982;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have been employed in gravitational-wave astronomy to accelerate the construction of surrogate waveforms for the inspiral of spin-aligned black hole binaries, among other applications. We face the challenge of modeling the residual error of an artificial neural network that models the coefficients of the surrogate waveform expansion (especially those of the phase of the waveform) which we demonstrate has sufficient structure to be learnable by a second network. Adding this second network, we were able to reduce the maximum mismatch for waveforms in a validation set by 13.4 times. We also explored several other ideas for improving the accuracy of the surrogate model, such as the exploitation of similarities between waveforms, the augmentation of the training set, the dissection of the input space, using dedicated networks per output coefficient and output augmentation. In several cases, small improvements can be observed, but the most significant improvement still 
&lt;/p&gt;</description></item><item><title>AdaTerm&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;T&#20998;&#24067;&#20272;&#35745;&#31283;&#20581;&#30697;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#20248;&#21270;&#31639;&#27861;&#30340;&#32479;&#19968;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2201.06714</link><description>&lt;p&gt;
AdaTerm: &#33258;&#36866;&#24212;T&#20998;&#24067;&#20272;&#35745;&#31283;&#20581;&#30697;&#29992;&#20110;&#22122;&#22768;&#20581;&#22766;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AdaTerm: Adaptive T-Distribution Estimated Robust Moments for Noise-Robust Stochastic Gradient Optimization. (arXiv:2201.06714v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06714
&lt;/p&gt;
&lt;p&gt;
AdaTerm&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;T&#20998;&#24067;&#20272;&#35745;&#31283;&#20581;&#30697;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#20248;&#21270;&#31639;&#27861;&#30340;&#32479;&#19968;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#22686;&#21152;&#65292;&#20174;&#21508;&#31181;&#26469;&#28304;&#22914;&#27979;&#37327;&#35823;&#24046;&#12289;&#38169;&#35823;&#26631;&#35760;&#21644;&#20272;&#35745;&#20195;&#29702;&#36755;&#20837;/&#36755;&#20986;&#20013;&#21463;&#25439;&#30340;&#25968;&#25454;&#38598;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#20248;&#21270;&#32467;&#26524;&#65292;&#25552;&#39640;&#20248;&#21270;&#31639;&#27861;&#23545;&#22122;&#22768;&#30340;&#31283;&#20581;&#24615;&#24050;&#25104;&#20026;&#24120;&#35265;&#20570;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;Adam-like&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#20013;&#20351;&#29992;&#30340;&#19968;&#38454;&#30697;&#21487;&#20197;&#22522;&#20110;&#23398;&#29983;t&#20998;&#24067;&#36827;&#34892;&#20462;&#25913;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20462;&#25913;&#21482;&#24433;&#21709;&#20102;&#19968;&#38454;&#30697;&#65292;&#20854;&#20182;&#20851;&#32852;&#30340;&#32479;&#35745;&#37327;&#20445;&#25345;&#19981;&#21464;&#65292;&#23548;&#33268;&#20102;&#25152;&#20551;&#35774;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AdaTerm&#65292;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#23558;&#23398;&#29983;t&#20998;&#24067;&#29992;&#20110;&#25512;&#23548;&#19968;&#38454;&#30697;&#21644;&#25152;&#26377;&#20851;&#32852;&#30340;&#32479;&#35745;&#37327;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#20248;&#21270;&#31639;&#27861;&#30340;&#32479;&#19968;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing practicality of deep learning applications, practitioners are inevitably faced with datasets corrupted by noise from various sources such as measurement errors, mislabeling, and estimated surrogate inputs/outputs that can adversely impact the optimization results. It is a common practice to improve the optimization algorithm's robustness to noise, since this algorithm is ultimately in charge of updating the network parameters. Previous studies revealed that the first-order moment used in Adam-like stochastic gradient descent optimizers can be modified based on the Student's t-distribution. While this modification led to noise-resistant updates, the other associated statistics remained unchanged, resulting in inconsistencies in the assumed models. In this paper, we propose AdaTerm, a novel approach that incorporates the Student's t-distribution to derive not only the first-order moment but also all the associated statistics. This provides a unified treatment of the o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#19982;&#20869;&#22312;&#21453;&#39304;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#23558;&#20154;&#31867;&#36755;&#20837;&#19982;RL&#31639;&#27861;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#36825;&#19968;&#20851;&#38190;&#32852;&#31995;&#19978;&#30340;&#25506;&#32034;&#20173;&#26377;&#24453;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2112.01575</link><description>&lt;p&gt;
&#21521;&#20855;&#26377;&#20869;&#22312;&#21453;&#39304;&#30340;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Interactive Reinforcement Learning with Intrinsic Feedback. (arXiv:2112.01575v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01575
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#19982;&#20869;&#22312;&#21453;&#39304;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#23558;&#20154;&#31867;&#36755;&#20837;&#19982;RL&#31639;&#27861;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#36825;&#19968;&#20851;&#38190;&#32852;&#31995;&#19978;&#30340;&#25506;&#32034;&#20173;&#26377;&#24453;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#38543;&#30528;&#20154;&#20204;&#23545;&#20154;&#26426;&#21327;&#21516;&#65288;HITL&#65289;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#23558;&#20154;&#31867;&#36755;&#20837;&#19982;RL&#31639;&#27861;&#30456;&#32467;&#21512;&#24050;&#32463;&#20652;&#29983;&#20102;&#20132;&#20114;&#24335;RL&#30340;&#23376;&#39046;&#22495;&#12290;&#21516;&#26102;&#65292;BCI&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#20174;&#31070;&#32463;&#27963;&#21160;&#20013;&#25552;&#21462;&#26377;&#20851;&#20154;&#26426;&#20132;&#20114;&#30340;&#20449;&#24687;&#24615;&#33041;&#20449;&#21495;&#12290;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#20851;&#38190;&#32852;&#31995;&#22312;&#20110;&#23558;&#31070;&#32463;&#27963;&#21160;&#35299;&#37322;&#20026;&#21453;&#39304;&#65292;&#20197;&#20415;&#21487;&#20197;&#24212;&#29992;&#20132;&#20114;&#24335;RL&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26032;&#20852;&#30340;&#21453;&#39304;&#20171;&#36136;&#31216;&#20026;&#20869;&#22312;&#21453;&#39304;&#12290;&#23613;&#31649;&#20869;&#22312;&#21453;&#39304;&#33021;&#22815;&#33258;&#21160;&#20256;&#36798;&#29978;&#33267;&#26080;&#24847;&#35782;&#22320;&#20256;&#36798;&#65292;&#20294;&#23545;&#20110;&#36825;&#20010;&#20851;&#38190;&#32852;&#31995;&#30340;&#36866;&#24403;&#25506;&#32034;&#20960;&#20046;&#26410;&#21463;&#21040;&#20004;&#20010;&#39046;&#22495;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20419;&#36827;&#28145;&#20837;&#29702;&#35299;&#21644;&#26356;&#26377;&#25928;&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25945;&#31243;&#39118;&#26684;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#21160;&#26426;&#12289;&#26041;&#27861;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) and brain-computer interfaces (BCI) have experienced significant growth over the past decade. With rising interest in human-in-the-loop (HITL), incorporating human input with RL algorithms has given rise to the sub-field of interactive RL. Adjacently, the field of BCI has long been interested in extracting informative brain signals from neural activity for use in human-computer interactions. A key link between these fields lies in the interpretation of neural activity as feedback such that interactive RL approaches can be employed. We denote this new and emerging medium of feedback as intrinsic feedback. Despite intrinsic feedback's ability to be conveyed automatically and even unconsciously, proper exploration surrounding this key link has largely gone unaddressed by both communities. Thus, to help facilitate a deeper understanding and a more effective utilization, we provide a tutorial-style review covering the motivations, approaches, and open problems of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#22914;&#20309;&#36873;&#25321;&#24615;&#22320;&#26631;&#27880;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36873;&#25321;&#24615;&#26631;&#27880;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#32858;&#31867;&#21407;&#22411;&#26469;&#33719;&#24471;&#20195;&#34920;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.03006</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36873;&#25321;&#24615;&#26631;&#27880;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Selective Labeling for More Effective Semi-Supervised Learning. (arXiv:2110.03006v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#22914;&#20309;&#36873;&#25321;&#24615;&#22320;&#26631;&#27880;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36873;&#25321;&#24615;&#26631;&#27880;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#32858;&#31867;&#21407;&#22411;&#26469;&#33719;&#24471;&#20195;&#34920;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19968;&#20010;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#27880;&#37322;&#39044;&#31639;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#26377;&#36873;&#25321;&#22320;&#26631;&#27880;&#22266;&#23450;&#25968;&#37327;&#30340;&#23454;&#20363;&#65292;&#20197;&#20415;&#22312;&#36825;&#26679;&#19968;&#20010;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26356;&#21152;&#26377;&#25928;&#12290;&#25105;&#20204;&#20851;&#27880;&#36873;&#25321;&#36866;&#24403;&#30340;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#65292;&#38500;&#20102;&#36890;&#24120;&#30340;SSL&#23558;&#26631;&#31614;&#20174;&#26377;&#26631;&#35760;&#25968;&#25454;&#20256;&#25773;&#21040;&#20854;&#20313;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#36807;&#31243;&#12290;&#36825;&#20010;&#23454;&#20363;&#36873;&#25321;&#20219;&#21153;&#24456;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#20219;&#20309;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#23398;&#20064;&#30340;&#30446;&#26631;&#24212;&#35813;&#26159;&#20160;&#20040;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#26080;&#35770;&#19979;&#28216;&#20219;&#21153;&#26159;&#20160;&#20040;&#65292;&#35201;&#26631;&#27880;&#30340;&#23454;&#20363;&#24517;&#39035;&#26159;&#20855;&#26377;&#20195;&#34920;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#65306;&#21069;&#32773;&#26377;&#21161;&#20110;&#23558;&#26631;&#31614;&#20256;&#25773;&#21040;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#21518;&#32773;&#20445;&#35777;&#20102;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#20013;&#36873;&#25321;&#32858;&#31867;&#21407;&#22411;&#25110;&#22312;&#29305;&#24449;&#20248;&#21270;&#36807;&#31243;&#20013;&#36873;&#25321;&#32858;&#31867;&#21407;&#22411;&#26469;&#25429;&#25417;&#36825;&#20010;&#24819;&#27861;&#65292;&#32780;&#37117;&#19981;&#38656;&#35201;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#36873;&#25321;&#24615;&#26631;&#27880;&#26041;&#27861;&#22312;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given an unlabeled dataset and an annotation budget, we study how to selectively label a fixed number of instances so that semi-supervised learning (SSL) on such a partially labeled dataset is most effective. We focus on selecting the right data to label, in addition to usual SSL's propagating labels from labeled data to the rest unlabeled data. This instance selection task is challenging, as without any labeled data we do not know what the objective of learning should be. Intuitively, no matter what the downstream task is, instances to be labeled must be representative and diverse: The former would facilitate label propagation to unlabeled data, whereas the latter would ensure coverage of the entire dataset. We capture this idea by selecting cluster prototypes, either in a pretrained feature space, or along with feature optimization, both without labels. Our unsupervised selective labeling consistently improves SSL methods over state-of-the-art active learning given labeled data, by 8
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#39044;&#35757;&#32451;&#30340;&#25968;&#37327;&#23545;&#21098;&#26525;&#21518;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#20197;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#23545;&#25968;&#20851;&#31995;&#20915;&#23450;&#20102;&#39044;&#35757;&#32451;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2108.00259</link><description>&lt;p&gt;
&#29992;&#22810;&#23569;&#39044;&#35757;&#32451;&#36275;&#20197;&#21457;&#29616;&#19968;&#20010;&#22909;&#30340;&#23376;&#32593;&#32476;&#65311;
&lt;/p&gt;
&lt;p&gt;
How much pre-training is enough to discover a good subnetwork?. (arXiv:2108.00259v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.00259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#39044;&#35757;&#32451;&#30340;&#25968;&#37327;&#23545;&#21098;&#26525;&#21518;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#20197;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#23545;&#25968;&#20851;&#31995;&#20915;&#23450;&#20102;&#39044;&#35757;&#32451;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#23545;&#20110;&#22312;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#32593;&#32476;&#32467;&#26500;&#20013;&#21457;&#29616;&#39640;&#25928;&#12289;&#39640;&#24615;&#33021;&#30340;&#23376;&#32593;&#32476;&#38750;&#24120;&#26377;&#29992;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23427;&#28041;&#21450;&#21040;&#19968;&#20010;&#19977;&#27493;&#36807;&#31243;&#8212;&#8212;&#39044;&#35757;&#32451;&#12289;&#21098;&#26525;&#21644;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#20026;&#23494;&#38598;&#27169;&#22411;&#24517;&#39035;&#23436;&#20840;&#39044;&#35757;&#32451;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#30340;&#25968;&#37327;&#19982;&#21098;&#26525;&#32593;&#32476;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#23545;&#20110;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#30340;&#29702;&#35770;&#25551;&#36848;&#20173;&#28982;&#32570;&#22833;&#12290;&#20026;&#20102;&#25968;&#23398;&#20998;&#26512;&#23494;&#38598;&#32593;&#32476;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#25968;&#37327;&#65292;&#20197;&#20415;&#21098;&#26525;&#21518;&#30340;&#32593;&#32476;&#33021;&#22815;&#34920;&#29616;&#33391;&#22909;&#65292;&#25105;&#20204;&#22312;&#20108;&#23618;&#20840;&#36830;&#25509;&#32593;&#32476;&#19978;&#21457;&#29616;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#36229;&#36807;&#36825;&#20010;&#30028;&#38480;&#65292;&#36890;&#36807;&#36138;&#23146;&#21069;&#21521;&#36873;&#25321;&#30340;&#21098;&#26525;&#21487;&#20197;&#36798;&#21040;&#33391;&#22909;&#30340;&#35757;&#32451;&#35823;&#24046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20010;&#38408;&#20540;&#34987;&#35777;&#26126;&#19982;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21576;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning is useful for discovering efficient, high-performing subnetworks within pre-trained, dense network architectures. More often than not, it involves a three-step process -- pre-training, pruning, and re-training -- that is computationally expensive, as the dense model must be fully pre-trained. While previous work has revealed through experiments the relationship between the amount of pre-training and the performance of the pruned network, a theoretical characterization of such dependency is still missing. Aiming to mathematically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a simple theoretical bound in the number of gradient descent pre-training iterations on a two-layer, fully-connected network, beyond which pruning via greedy forward selection [61] yields a subnetwork that achieves good training error. Interestingly, this threshold is shown to be logarithmically dependent upon the size of the dataset,
&lt;/p&gt;</description></item><item><title>&#12298;&#28145;&#20837;&#28145;&#24230;&#23398;&#20064;&#12299;&#26159;&#19968;&#26412;&#26088;&#22312;&#20351;&#28145;&#24230;&#23398;&#20064;&#26131;&#20110;&#29702;&#35299;&#30340;&#24320;&#28304;&#20070;&#31821;&#65292;&#25552;&#20379;&#20174;&#27010;&#24565;&#21040;&#20195;&#30721;&#30340;&#25945;&#23398;&#36164;&#28304;&#65292;&#26088;&#22312;&#25104;&#20026;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#30340;&#36215;&#28857;&#65292;&#24182;&#20801;&#35768;&#31038;&#21306;&#24555;&#36895;&#26356;&#26032;&#21644;&#20114;&#21160;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2106.11342</link><description>&lt;p&gt;
&#28145;&#20837;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dive into Deep Learning. (arXiv:2106.11342v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11342
&lt;/p&gt;
&lt;p&gt;
&#12298;&#28145;&#20837;&#28145;&#24230;&#23398;&#20064;&#12299;&#26159;&#19968;&#26412;&#26088;&#22312;&#20351;&#28145;&#24230;&#23398;&#20064;&#26131;&#20110;&#29702;&#35299;&#30340;&#24320;&#28304;&#20070;&#31821;&#65292;&#25552;&#20379;&#20174;&#27010;&#24565;&#21040;&#20195;&#30721;&#30340;&#25945;&#23398;&#36164;&#28304;&#65292;&#26088;&#22312;&#25104;&#20026;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#30340;&#36215;&#28857;&#65292;&#24182;&#20801;&#35768;&#31038;&#21306;&#24555;&#36895;&#26356;&#26032;&#21644;&#20114;&#21160;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26412;&#24320;&#28304;&#20070;&#26159;&#25105;&#20204;&#30340;&#21162;&#21147;&#65292;&#35753;&#28145;&#24230;&#23398;&#20064;&#21464;&#24471;&#26131;&#20110;&#29702;&#35299;&#65292;&#25945;&#35835;&#32773;&#27010;&#24565;&#12289;&#32972;&#26223;&#21644;&#20195;&#30721;&#12290;&#25972;&#26412;&#20070;&#37117;&#26159;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#36215;&#33609;&#30340;&#65292;&#19982;&#29420;&#31435;&#30340;&#20195;&#30721;&#26080;&#32541;&#38598;&#25104;&#20102;&#35828;&#26126;&#22270;&#12289;&#25968;&#23398;&#21644;&#20114;&#21160;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#36164;&#28304;&#65292;&#26082;&#21487;&#20197;&#33258;&#30001;&#20351;&#29992;&#65292;&#21448;&#21487;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#25216;&#26415;&#28145;&#24230;&#65292;&#20026;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#30340;&#36215;&#28857;; &#21253;&#25324;&#21487;&#36816;&#34892;&#30340;&#20195;&#30721;&#65292;&#21521;&#35835;&#32773;&#23637;&#31034;&#22914;&#20309;&#23454;&#36341;&#35299;&#20915;&#38382;&#39064;; &#20801;&#35768;&#24555;&#36895;&#26356;&#26032;&#65292;&#19981;&#20165;&#30001;&#25105;&#20204;&#65292;&#36824;&#30001;&#25972;&#20010;&#31038;&#21306;&#26356;&#26032;; &#25509;&#21463;&#25216;&#26415;&#32454;&#33410;&#30340;&#20114;&#21160;&#35752;&#35770;&#21644;&#35299;&#31572;&#38382;&#39064;&#30340;&#35770;&#22363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20256;&#36755;&#23398;&#20064;&#30340;&#22522;&#26412;&#36807;&#31243;&#65292;&#38024;&#23545;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#20219;&#21153;&#21442;&#25968;&#21644;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#23792;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#65292;&#35813;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#23725;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2103.05621</link><description>&lt;p&gt;
&#20256;&#36755;&#23398;&#20064;&#30340;&#20849;&#21516;&#30452;&#35273;&#21487;&#20197;&#24102;&#26469;&#32988;&#21033;&#25110;&#22833;&#36133;: &#32447;&#24615;&#22238;&#24402;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Common Intuition to Transfer Learning Can Win or Lose: Case Studies for Linear Regression. (arXiv:2103.05621v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.05621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20256;&#36755;&#23398;&#20064;&#30340;&#22522;&#26412;&#36807;&#31243;&#65292;&#38024;&#23545;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#20219;&#21153;&#21442;&#25968;&#21644;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#23792;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#65292;&#35813;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#23725;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#28304;&#22238;&#24402;&#20219;&#21153;&#21040;&#30446;&#26631;&#22238;&#24402;&#20219;&#21153;&#30340;&#22522;&#26412;&#20256;&#36755;&#23398;&#20064;&#36807;&#31243;&#65292;&#21253;&#25324;&#22312;&#26377;&#27604;&#25968;&#25454;&#26679;&#26412;&#26356;&#22810;&#30340;&#23398;&#20064;&#21442;&#25968;&#30340;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#12290;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#20854;&#35757;&#32451;&#25968;&#25454;&#21644;&#20808;&#21069;&#35745;&#31639;&#30340;&#28304;&#20219;&#21153;&#21442;&#25968;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#23558;&#30446;&#26631;&#20219;&#21153;&#30340;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#23450;&#20041;&#20026;&#24102;&#26377;&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#20248;&#21270;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#26159;&#24453;&#23398;&#20064;&#30340;&#30446;&#26631;&#21442;&#25968;&#19982;&#24050;&#23398;&#20064;&#30340;&#28304;&#21442;&#25968;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#20998;&#26512;&#22320;&#34920;&#24449;&#20102;&#25105;&#20204;&#30340;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#23567;L2&#33539;&#25968;&#35299;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#23792;&#20540;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#36275;&#22815;&#30456;&#20851;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#32463;&#36807;&#26368;&#20339;&#35843;&#20248;&#30340;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26368;&#20339;&#35843;&#20248;&#30340;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#21363;&#20351;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#31526;&#21512;&#26368;&#23567;L2&#33539;&#25968;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a fundamental transfer learning process from source to target linear regression tasks, including overparameterized settings where there are more learned parameters than data samples. The target task learning is addressed by using its training data together with the parameters previously computed for the source task. We define a transfer learning approach to the target task as a linear regression optimization with a regularization on the distance between the to-be-learned target parameters and the already-learned source parameters. We analytically characterize the generalization performance of our transfer learning approach and demonstrate its ability to resolve the peak in generalization errors in double descent phenomena of the minimum L2-norm solution to linear regression. Moreover, we show that for sufficiently related tasks, the optimally tuned transfer learning approach can outperform the optimally tuned ridge regression method, even when the true parameter vector conform
&lt;/p&gt;</description></item></channel></rss>