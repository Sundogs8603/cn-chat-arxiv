<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#31995;&#25968;&#30697;&#38453;&#21644;&#21521;&#37327;&#37117;&#23384;&#22312;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#26102;&#65292;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#22312;&#35299;&#20915;&#22122;&#22768;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;RK&#30340;&#25910;&#25947;&#24615;&#21463;&#21040;&#120591;&#30340;&#22823;&#23567;&#24433;&#21709;&#65292;&#20854;&#20013;&#120591;&#34920;&#31034;&#24102;&#26377;&#22122;&#22768;&#30340;&#31995;&#25968;&#30697;&#38453;A&#30340;&#20056;&#23376;&#33539;&#25968;&#30340;&#24179;&#26041;&#19982;Frobenius&#33539;&#25968;&#30340;&#24179;&#26041;&#30340;&#20056;&#31215;&#12290;</title><link>http://arxiv.org/abs/2308.16904</link><description>&lt;p&gt;
&#20851;&#20110;&#35299;&#20915;&#21452;&#21521;&#22122;&#22768;&#32447;&#24615;&#31995;&#32479;&#30340;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#30340;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems. (arXiv:2308.16904v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#31995;&#25968;&#30697;&#38453;&#21644;&#21521;&#37327;&#37117;&#23384;&#22312;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#26102;&#65292;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#22312;&#35299;&#20915;&#22122;&#22768;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;RK&#30340;&#25910;&#25947;&#24615;&#21463;&#21040;&#120591;&#30340;&#22823;&#23567;&#24433;&#21709;&#65292;&#20854;&#20013;&#120591;&#34920;&#31034;&#24102;&#26377;&#22122;&#22768;&#30340;&#31995;&#25968;&#30697;&#38453;A&#30340;&#20056;&#23376;&#33539;&#25968;&#30340;&#24179;&#26041;&#19982;Frobenius&#33539;&#25968;&#30340;&#24179;&#26041;&#30340;&#20056;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#32447;&#24615;&#31995;&#32479;Ax=b&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#36845;&#20195;&#27714;&#35299;&#22120;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#25805;&#20316;&#35823;&#24046;&#25110;&#38169;&#35823;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#65292;&#36825;&#20123;&#31995;&#32479;&#20250;&#20986;&#29616;&#22122;&#22768;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38543;&#26426;Kaczmarz&#65288;RK&#65289;&#31639;&#27861;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#39640;&#25928;&#36845;&#20195;&#27714;&#35299;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23545;RK&#22312;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;&#26377;&#38480;&#65292;&#21482;&#32771;&#34385;&#21491;&#20391;&#21521;&#37327;b&#20013;&#30340;&#27979;&#37327;&#22122;&#22768;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#24182;&#19981;&#24635;&#26159;&#36825;&#26679;&#65307;&#31995;&#25968;&#30697;&#38453;A&#20063;&#21487;&#33021;&#26159;&#26377;&#22122;&#22768;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#31995;&#25968;&#30697;&#38453;A&#20197;&#21450;&#21521;&#37327;b&#37117;&#21463;&#26377;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#24433;&#21709;&#26102;&#65292;RK&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#21464;&#37327; &#120591;=&#8741; &#120591; &#119860; &#8727; &#8741;2^2 &#8741; &#119836;&#119863;&#119867;&#8727;&#119888; &#8722; &#119835; &#8741;_&#119865;^2&#127542; &#119900; &#119877; &#30340;&#22823;&#23567;&#20250;&#24433;&#21709;RK&#30340;&#25910;&#25947;&#24615;&#65292;&#20854;&#20013; &#120591;&#119860; &#34920;&#31034;A&#30340;&#24102;&#26377;&#22122;&#22768;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22768;&#31216;&#25105;&#20204;&#30340;&#20998;&#26512;&#26159;&#20581;&#22766;&#19988;&#36924;&#36817;&#23454;&#38469;&#30340;.
&lt;/p&gt;
&lt;p&gt;
Large-scale linear systems, $Ax=b$, frequently arise in practice and demand effective iterative solvers. Often, these systems are noisy due to operational errors or faulty data-collection processes. In the past decade, the randomized Kaczmarz (RK) algorithm has been studied extensively as an efficient iterative solver for such systems. However, the convergence study of RK in the noisy regime is limited and considers measurement noise in the right-hand side vector, $b$. Unfortunately, in practice, that is not always the case; the coefficient matrix $A$ can also be noisy. In this paper, we analyze the convergence of RK for noisy linear systems when the coefficient matrix, $A$, is corrupted with both additive and multiplicative noise, along with the noisy vector, $b$. In our analyses, the quantity $\tilde R=\| \tilde A^{\dagger} \|_2^2 \|\tilde A \|_F^2$ influences the convergence of RK, where $\tilde A$ represents a noisy version of $A$. We claim that our analysis is robust and realistic
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#21475;&#21619;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2308.16900</link><description>&lt;p&gt;
&#23398;&#20064;&#21697;&#21619;&#65306;&#19968;&#20010;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#21475;&#21619;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;WineSensed&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;89.7&#19975;&#24352;&#33889;&#33796;&#37202;&#26631;&#31614;&#22270;&#29255;&#21644;82.4&#19975;&#26465;&#26469;&#33258;Vivino&#24179;&#21488;&#30340;&#33889;&#33796;&#37202;&#35780;&#35770;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#36229;&#36807;35&#19975;&#20010;&#29420;&#29305;&#30340;&#24180;&#20221;&#65292;&#38468;&#24102;&#20102;&#24180;&#20221;&#12289;&#20135;&#22320;&#12289;&#35780;&#20998;&#12289;&#37202;&#31934;&#21547;&#37327;&#12289;&#20215;&#26684;&#21644;&#33889;&#33796;&#32452;&#25104;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#21697;&#37202;&#23454;&#39564;&#23545;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#30340;&#21475;&#21619;&#27880;&#37322;&#65292;&#20849;&#26377;256&#21517;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#26681;&#25454;&#21475;&#21619;&#30340;&#30456;&#20284;&#24615;&#23545;&#33889;&#33796;&#37202;&#36827;&#34892;&#25490;&#24207;&#65292;&#24471;&#21040;&#20102;&#36229;&#36807;5&#21315;&#20010;&#37197;&#23545;&#30340;&#21475;&#21619;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#20849;&#20139;&#30340;&#27010;&#24565;&#23884;&#20837;&#31354;&#38388;&#22312;&#31895;&#31890;&#24230;&#21475;&#21619;&#20998;&#31867;&#65288;&#37202;&#31934;&#21547;&#37327;&#65292;&#22269;&#23478;&#65292;&#33889;&#33796;&#65292;&#20215;&#26684;&#65292;&#35780;&#20998;&#65289;&#19978;&#25913;&#36827;&#65292;&#24182;&#19988;&#19982;&#22797;&#26434;&#30340;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique vintages, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.16898</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;"Attention Is All You Need"&#20013;&#24341;&#20837;&#36716;&#25442;&#22120;&#26550;&#26500;&#20197;&#26469;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#25509;&#21463;&#36755;&#20837;&#20196;&#29260;&#24207;&#21015;$X$&#24182;&#36890;&#36807;&#35745;&#31639;softmax$(XQK^\top X^\top)$&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;$(K,Q)$&#26159;&#21487;&#35757;&#32451;&#30340;&#38190;-&#26597;&#35810;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#20248;&#21270;&#20960;&#20309;&#21644;&#19968;&#20010;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#20196;&#29260;&#23545;&#30340;&#22806;&#31215;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#23558;&#26368;&#20339;&#36755;&#20837;&#20196;&#29260;&#19982;&#38750;&#26368;&#20339;&#20196;&#29260;&#20998;&#31163;&#12290;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21333;&#23618;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;(1)&#20248;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#29992;&#21487;&#21464;&#27491;&#21017;&#21270;&#21442;&#25968;$(K,Q)$&#65292;&#25910;&#25947;&#30340;&#26041;&#21521;&#26159;&#19968;&#20010;&#26368;&#23567;&#21270;&#32508;&#21512;&#21442;&#25968;$W=KQ^\top$&#30340;&#26680;&#33539;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;&#20915;&#26041;&#26696;&#12290;&#32780;&#30452;&#25509;&#20351;&#29992;$W$&#36827;&#34892;&#21442;&#25968;&#21270;&#21017;&#26368;&#23567;&#21270;&#19968;&#20010;Frobenius&#33539;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36827;&#34892;&#28857;&#20113;&#19977;&#32500;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#26609;&#22352;&#26631;&#31995;&#19979;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#65292;&#23454;&#29616;&#20102;&#23545;&#28857;&#20113;&#30340;&#31934;&#32454;&#24314;&#27169;&#65292;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#32452;&#27744;&#21270;&#21644;&#20108;&#32500;&#20027;&#24178;&#32593;&#32476;&#39640;&#25928;&#22788;&#29702;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#28857;&#20113;&#30340;&#35821;&#20041;&#21344;&#25454;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.16896</link><description>&lt;p&gt;
PointOcc: &#22522;&#20110;&#28857;&#20113;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#29992;&#20110;&#28857;&#20113;&#19977;&#32500;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction. (arXiv:2308.16896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36827;&#34892;&#28857;&#20113;&#19977;&#32500;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#26609;&#22352;&#26631;&#31995;&#19979;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#65292;&#23454;&#29616;&#20102;&#23545;&#28857;&#20113;&#30340;&#31934;&#32454;&#24314;&#27169;&#65292;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#32452;&#27744;&#21270;&#21644;&#20108;&#32500;&#20027;&#24178;&#32593;&#32476;&#39640;&#25928;&#22788;&#29702;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#28857;&#20113;&#30340;&#35821;&#20041;&#21344;&#25454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#27491;&#20174;&#31232;&#30095;&#28857;&#20998;&#21106;&#21457;&#23637;&#21040;&#23494;&#38598;&#20307;&#32032;&#20998;&#21106;&#65292;&#30446;&#26631;&#26159;&#39044;&#27979;&#25152;&#20851;&#27880;&#30340;&#19977;&#32500;&#31354;&#38388;&#20013;&#27599;&#20010;&#20307;&#32032;&#30340;&#35821;&#20041;&#21344;&#25454;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20108;&#32500;&#25237;&#24433;&#30340;&#26041;&#27861;&#65288;&#22914;&#40479;&#30640;&#22270;&#12289;&#36317;&#31163;&#35270;&#22270;&#31561;&#65289;&#22240;&#20026;&#39044;&#27979;&#31354;&#38388;&#30340;&#23494;&#38598;&#24615;&#32780;&#26080;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#25551;&#36848;&#19977;&#32500;&#22330;&#26223;&#30340;&#23376;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#26469;&#26377;&#25928;&#32780;&#20840;&#38754;&#22320;&#34920;&#31034;&#28857;&#20113;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28857;&#20113;&#27169;&#22411; PointOcc &#26469;&#39640;&#25928;&#22320;&#22788;&#29702;&#23427;&#20204;&#12290;&#32771;&#34385;&#21040;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#30340;&#36317;&#31163;&#20998;&#24067;&#65292;&#25105;&#20204;&#21033;&#29992;&#26609;&#22352;&#26631;&#31995;&#26500;&#24314;&#20102;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#65292;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#23545;&#36817;&#21306;&#22495;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#37319;&#29992;&#31354;&#38388;&#32452;&#27744;&#21270;&#26469;&#20445;&#30041;&#25237;&#24433;&#36807;&#31243;&#20013;&#30340;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#37319;&#29992;&#20108;&#32500;&#20027;&#24178;&#32593;&#36335;&#39640;&#25928;&#22788;&#29702;&#27599;&#20010;&#36879;&#35270;&#38754;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#30340;&#26041;&#24335;&#33719;&#24471;&#27599;&#20010;&#28857;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation in autonomous driving has been undergoing an evolution from sparse point segmentation to dense voxel segmentation, where the objective is to predict the semantic occupancy of each voxel in the concerned 3D space. The dense nature of the prediction space has rendered existing efficient 2D-projection-based methods (e.g., bird's eye view, range view, etc.) ineffective, as they can only describe a subspace of the 3D scene. To address this, we propose a cylindrical tri-perspective view to represent point clouds effectively and comprehensively and a PointOcc model to process them efficiently. Considering the distance distribution of LiDAR point clouds, we construct the tri-perspective view in the cylindrical coordinate system for more fine-grained modeling of nearer areas. We employ spatial group pooling to maintain structural details during projection and adopt 2D backbones to efficiently process each TPV plane. Finally, we obtain the features of each point by aggregat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#26465;&#20214;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30896;&#25758;&#20989;&#25968;&#65292;&#39044;&#27979;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#30896;&#25758;&#65292;&#23454;&#29616;&#28789;&#27963;&#12289;&#26377;&#26465;&#20214;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25110;&#32773;&#30495;&#23454;&#29289;&#20307;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.16893</link><description>&lt;p&gt;
&#35821;&#35328;&#26465;&#20214;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language-Conditioned Path Planning. (arXiv:2308.16893v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#26465;&#20214;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30896;&#25758;&#20989;&#25968;&#65292;&#39044;&#27979;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#30896;&#25758;&#65292;&#23454;&#29616;&#28789;&#27963;&#12289;&#26377;&#26465;&#20214;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25110;&#32773;&#30495;&#23454;&#29289;&#20307;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#35302;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26680;&#24515;&#12290;&#26377;&#26102;&#20505;&#65292;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#25509;&#35302;&#65288;&#20363;&#22914;&#25805;&#32437;&#21644;&#25235;&#21462;&#65289;&#65292;&#32780;&#26377;&#26102;&#20505;&#65292;&#25509;&#35302;&#26159;&#26377;&#23475;&#30340;&#65288;&#20363;&#22914;&#36991;&#20813;&#38556;&#30861;&#29289;&#65289;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#21482;&#20851;&#27880;&#20110;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25509;&#35302;&#20016;&#23500;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#26465;&#20214;&#36335;&#24452;&#35268;&#21010;&#30340;&#39046;&#22495;&#65292;&#23558;&#25509;&#35302;&#24863;&#30693;&#24615;&#34701;&#20837;&#21040;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20013;&#12290;&#20316;&#20026;&#35813;&#39046;&#22495;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#26465;&#20214;&#30896;&#25758;&#20989;&#25968;&#65288;LACO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#21333;&#35270;&#22270;&#22270;&#20687;&#12289;&#35821;&#35328;&#25552;&#31034;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#26469;&#23398;&#20064;&#30896;&#25758;&#20989;&#25968;&#12290;LACO&#21487;&#20197;&#39044;&#27979;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#30896;&#25758;&#65292;&#20174;&#32780;&#23454;&#29616;&#28789;&#27963;&#12289;&#26377;&#26465;&#20214;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#26080;&#38656;&#25163;&#21160;&#23545;&#35937;&#26631;&#27880;&#12289;&#28857;&#20113;&#25968;&#25454;&#25110;&#30495;&#23454;&#29289;&#20307;&#27169;&#22411;&#12290;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;LACO&#21487;&#20197;&#23454;&#29616;&#22797;&#26434;&#32780;&#32454;&#33268;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contact is at the core of robotic manipulation. At times, it is desired (e.g. manipulation and grasping), and at times, it is harmful (e.g. when avoiding obstacles). However, traditional path planning algorithms focus solely on collision-free paths, limiting their applicability in contact-rich tasks. To address this limitation, we propose the domain of Language-Conditioned Path Planning, where contact-awareness is incorporated into the path planning problem. As a first step in this domain, we propose Language-Conditioned Collision Functions (LACO) a novel approach that learns a collision function using only a single-view image, language prompt, and robot configuration. LACO predicts collisions between the robot and the environment, enabling flexible, conditional path planning without the need for manual object annotations, point cloud data, or ground-truth object meshes. In both simulation and the real world, we demonstrate that LACO can facilitate complex, nuanced path plans that allo
&lt;/p&gt;</description></item><item><title>GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.16891</link><description>&lt;p&gt;
GNFactor&#65306;&#20855;&#26377;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#22810;&#20219;&#21153;&#30495;&#23454;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. (arXiv:2308.16891v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16891
&lt;/p&gt;
&lt;p&gt;
GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32467;&#26500;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#24320;&#21457;&#33021;&#22815;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#20219;&#21153;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#35821;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNFactor&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#35270;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#65288;GNF&#65289;&#20316;&#20026;&#37325;&#24314;&#27169;&#22359;&#65292;Perceiver Transformer&#20316;&#20026;&#20915;&#31574;&#27169;&#22359;&#65292;&#20849;&#20139;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#12290;&#20026;&#20102;&#23558;&#35821;&#20041;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#65292;&#37325;&#24314;&#27169;&#22359;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#31283;&#23450;&#25193;&#25955;&#65289;&#23558;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#21040;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#20013;&#12290;&#25105;&#20204;&#22312;3&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;GNFactor&#65292;&#24182;&#23545;10&#20010;RLBench&#20219;&#21153;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#21482;&#20351;&#29992;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$eural feature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#20154;&#26426;&#22686;&#24378;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#35206;&#30422;&#33539;&#22260;&#21644;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20998;&#26512;&#26080;&#20154;&#26426;&#22686;&#24378;&#30340;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#21450;&#22522;&#20110;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#29702;&#35770;&#65292;&#23558;FL&#21644;&#26080;&#20154;&#26426;&#37096;&#32626;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;FL&#24310;&#36831;&#24182;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16889</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#22686;&#24378;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#32852;&#21512;&#35206;&#30422;&#21644;&#25910;&#25947;&#26102;&#38388;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in UAV-Enhanced Networks: Joint Coverage and Convergence Time Optimization. (arXiv:2308.16889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#20154;&#26426;&#22686;&#24378;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#35206;&#30422;&#33539;&#22260;&#21644;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20998;&#26512;&#26080;&#20154;&#26426;&#22686;&#24378;&#30340;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#21450;&#22522;&#20110;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#29702;&#35770;&#65292;&#23558;FL&#21644;&#26080;&#20154;&#26426;&#37096;&#32626;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;FL&#24310;&#36831;&#24182;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#28041;&#21450;&#22810;&#20010;&#35774;&#22791;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20256;&#36755;&#20182;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;FL&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#65292;&#20351;&#20854;&#25104;&#20026;&#26080;&#20154;&#26426;&#22686;&#24378;&#26080;&#32447;&#32593;&#32476;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22240;&#20026;&#35813;&#32593;&#32476;&#33021;&#28304;&#31232;&#32570;&#12290;&#23613;&#31649;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26080;&#20154;&#26426;&#22686;&#24378;&#32593;&#32476;&#20013;&#23454;&#29616;FL&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20256;&#32479;&#30340;&#26368;&#22823;&#21270;&#35206;&#30422;&#33539;&#22260;&#30340;&#26080;&#20154;&#26426;&#37096;&#32626;&#26041;&#27861;&#20250;&#26174;&#33879;&#22686;&#21152;FL&#30340;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#20851;&#38190;&#21464;&#37327;&#65288;&#22914;&#20449;&#36947;&#36136;&#37327;&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#32570;&#20047;&#20808;&#39564;&#20449;&#24687;&#20351;&#38382;&#39064;&#21152;&#21095;&#12290;&#26412;&#25991;&#39318;&#20808;&#20998;&#26512;&#20102;&#24102;&#33021;&#37327;&#25910;&#38598;&#30340;&#26080;&#20154;&#26426;&#22686;&#24378;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSN&#65289;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#29702;&#35770;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#32593;&#32476;&#35206;&#30422;&#33539;&#22260;&#21516;&#26102;&#26368;&#23567;&#21270;FL&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#38598;&#21644;&#20005;&#26684;&#33021;&#37327;&#32422;&#26463;&#30340;UAV-WSN&#31995;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) involves several devices that collaboratively train a shared model without transferring their local data. FL reduces the communication overhead, making it a promising learning method in UAV-enhanced wireless networks with scarce energy resources. Despite the potential, implementing FL in UAV-enhanced networks is challenging, as conventional UAV placement methods that maximize coverage increase the FL delay significantly. Moreover, the uncertainty and lack of a priori information about crucial variables, such as channel quality, exacerbate the problem. In this paper, we first analyze the statistical characteristics of a UAV-enhanced wireless sensor network (WSN) with energy harvesting. We then develop a model and solution based on the multi-objective multi-armed bandit theory to maximize the network coverage while minimizing the FL delay. Besides, we propose another solution that is particularly useful with large action sets and strict energy constraints at the U
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#35745;&#31639;&#23884;&#27573;&#32858;&#21512;&#29289;&#24418;&#24577;&#28436;&#21270;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#31890;&#23376;&#27169;&#25311;&#23398;&#20064;&#39537;&#21160;&#32570;&#38519;&#28040;&#38500;&#36807;&#31243;&#65292;&#37319;&#29992;UNet&#26550;&#26500;&#23454;&#29616;&#20102;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#19979;&#30340;&#39044;&#27979;&#65292;&#24182;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21487;&#35270;&#21270;&#20102;&#24418;&#24577;&#28436;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.16886</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20108;&#23884;&#27573;&#20849;&#32858;&#29289;&#30340;&#24418;&#24577;&#23398;
&lt;/p&gt;
&lt;p&gt;
Prediction of Diblock Copolymer Morphology via Machine Learning. (arXiv:2308.16886v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16886
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#35745;&#31639;&#23884;&#27573;&#32858;&#21512;&#29289;&#24418;&#24577;&#28436;&#21270;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#31890;&#23376;&#27169;&#25311;&#23398;&#20064;&#39537;&#21160;&#32570;&#38519;&#28040;&#38500;&#36807;&#31243;&#65292;&#37319;&#29992;UNet&#26550;&#26500;&#23454;&#29616;&#20102;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#19979;&#30340;&#39044;&#27979;&#65292;&#24182;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21487;&#35270;&#21270;&#20102;&#24418;&#24577;&#28436;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#35745;&#31639;&#22823;&#39046;&#22495;&#38271;&#26102;&#38388;&#23610;&#24230;&#19979;&#23884;&#27573;&#32858;&#21512;&#29289;&#24418;&#24577;&#28436;&#21270;&#12290;&#35813;&#31574;&#30053;&#21033;&#29992;&#20102;&#31895;&#31890;&#24230;&#31890;&#23376;&#22312;&#21333;&#20307;&#23610;&#24230;&#19978;&#30340;&#28436;&#21270;&#21644;&#20171;&#35266;&#23610;&#24230;&#19978;&#32531;&#24930;&#30340;&#24418;&#24577;&#28436;&#21270;&#20043;&#38388;&#30340;&#29305;&#24449;&#26102;&#38388;&#30340;&#20998;&#31163;&#12290;&#19982;&#32463;&#39564;&#24615;&#36830;&#32493;&#27169;&#22411;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30452;&#25509;&#20174;&#22522;&#20110;&#31890;&#23376;&#30340;&#27169;&#25311;&#20013;&#23398;&#20064;&#38543;&#26426;&#39537;&#21160;&#30340;&#32570;&#38519;&#28040;&#38500;&#36807;&#31243;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#25903;&#25345;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#30340;UNet&#26550;&#26500;&#65292;&#20174;&#32780;&#20801;&#35768;&#20219;&#24847;&#24418;&#29366;&#30340;&#21608;&#26399;&#24615;&#21644;&#22266;&#23450;&#22522;&#24213;&#36793;&#30028;&#26465;&#20214;&#12290;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#24341;&#20837;&#20102;&#29289;&#29702;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21152;&#20837;&#20102;&#23545;&#31216;&#24615;&#12290;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#26696;&#20363;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21487;&#35270;&#21270;&#20102;&#26102;&#38388;&#19978;&#30340;&#24418;&#24577;&#28436;&#21270;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#31995;&#32479;&#21644;&#38271;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#24418;&#24577;&#23398;&#28436;&#21270;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
A machine learning approach is presented to accelerate the computation of block polymer morphology evolution for large domains over long timescales. The strategy exploits the separation of characteristic times between coarse-grained particle evolution on the monomer scale and slow morphological evolution over mesoscopic scales. In contrast to empirical continuum models, the proposed approach learns stochastically driven defect annihilation processes directly from particle-based simulations. A UNet architecture that respects different boundary conditions is adopted, thereby allowing periodic and fixed substrate boundary conditions of arbitrary shape. Physical concepts are also introduced via the loss function and symmetries are incorporated via data augmentation. The model is validated using three different use cases. Explainable artificial intelligence methods are applied to visualize the morphology evolution over time. This approach enables the generation of large system sizes and lon
&lt;/p&gt;</description></item><item><title>Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.16884</link><description>&lt;p&gt;
Belebele&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#24182;&#34892;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16884
&lt;/p&gt;
&lt;p&gt;
Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Belebele&#65292;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#22522;&#20934;&#30340;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#65292;&#20351;&#24471;&#21487;&#20197;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#22522;&#20110;Flores-200&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#20010;&#30701;&#31687;&#25991;&#31456;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#22810;&#36873;&#31572;&#26696;&#12290;&#38382;&#39064;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#27700;&#24179;&#30340;&#27169;&#22411;&#12290;&#21333;&#29420;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#24050;&#32463;&#36275;&#22815;&#22256;&#38590;&#65292;&#21487;&#20197;&#25361;&#25112;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#30001;&#20110;&#23436;&#20840;&#24182;&#34892;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#25152;&#26377;&#35821;&#35328;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#20294;&#23567;&#22411;MLMs&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much small
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DDAG&#65289;&#30340;&#20449;&#24687;&#29702;&#35770;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#30697;&#38453;&#30340;&#24230;&#37327;&#21644;&#31639;&#27861;&#26469;&#37325;&#24314;DDAG&#12290;</title><link>http://arxiv.org/abs/2308.16859</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#20449;&#24687;&#29702;&#35770;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs. (arXiv:2308.16859v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DDAG&#65289;&#30340;&#20449;&#24687;&#29702;&#35770;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#30697;&#38453;&#30340;&#24230;&#37327;&#21644;&#31639;&#27861;&#26469;&#37325;&#24314;DDAG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#19978;&#30340;&#24213;&#23618;&#30456;&#20114;&#20316;&#29992;/&#20381;&#36182;&#20851;&#31995;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23398;&#20064;DAG&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#38745;&#24577;&#31995;&#32479;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#33410;&#28857;&#29366;&#24577;&#30340;&#26679;&#26412;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65288;i.i.d.&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#21160;&#24577;&#31995;&#32479;&#30340;DAG&#20013;&#65292;&#36825;&#26679;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;DAG&#31216;&#20026;\emph{&#21160;&#24577;}DAG&#65288;DDAG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;DDAG&#65292;&#20854;&#20013;&#33410;&#28857;&#21160;&#21147;&#23398;&#30001;&#26410;&#35266;&#27979;&#30340;&#22806;&#29983;&#22122;&#22768;&#28304;&#39537;&#21160;&#65292;&#36825;&#20123;&#22122;&#22768;&#28304;&#22312;&#26102;&#38388;&#19978;&#26159;&#23485;&#24133;&#24179;&#31283;&#30340;&#65288;WSS&#65289;&#65292;&#20294;&#24444;&#27492;&#20043;&#38388;&#26159;&#19981;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#21516;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#65288;PSD&#65289;&#12290;&#21463;&#38745;&#24577;&#35774;&#32622;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;PSD&#30697;&#38453;&#30340;&#24230;&#37327;&#21644;&#31639;&#27861;&#26469;&#37325;&#24314;DDAG&#12290;&#22122;&#22768;PSD&#30456;&#31561;&#30340;&#20551;&#35774;&#21487;&#20197;&#25918;&#23485;&#65292;&#20197;&#20351;&#20854;&#21487;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, the optimal sample complexity of learning the underlying interaction/dependencies of a Linear Dynamical System (LDS) over a Directed Acyclic Graph (DAG) is studied. The sample complexity of learning a DAG's structure is well-studied for static systems, where the samples of nodal states are independent and identically distributed (i.i.d.). However, such a study is less explored for DAGs with dynamical systems, where the nodal states are temporally correlated. We call such a DAG underlying an LDS as \emph{dynamical} DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are driven by unobserved exogenous noise sources that are wide-sense stationary (WSS) in time but are mutually uncorrelated, and have the same {power spectral density (PSD)}. Inspired by the static settings, a metric and an algorithm based on the PSD matrix of the observed time series are proposed to reconstruct the DDAG. The equal noise PSD assumption can be relaxed such that identifiabil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#28369;&#30340;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#24182;&#36890;&#36807;&#20027;&#35201;&#21270;-&#26368;&#23567;&#21270;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#22788;&#29702;&#31232;&#30095;&#20445;&#30041;&#27491;&#21017;&#21270;&#22120;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#39564;&#35777;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16858</link><description>&lt;p&gt;
&#31232;&#30095;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#20027;&#35201;&#21270;-&#26368;&#23567;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Majorization-Minimization for sparse SVMs. (arXiv:2308.16858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#28369;&#30340;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#24182;&#36890;&#36807;&#20027;&#35201;&#21270;-&#26368;&#23567;&#21270;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#22788;&#29702;&#31232;&#30095;&#20445;&#30041;&#27491;&#21017;&#21270;&#22120;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#39564;&#35777;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#21069;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#30417;&#30563;&#26694;&#26550;&#19979;&#34987;&#24341;&#20837;&#29992;&#20110;&#25191;&#34892;&#20108;&#36827;&#21046;&#20998;&#31867;&#20219;&#21153;&#12290;&#22914;&#20170;&#65292;&#23427;&#20204;&#36890;&#24120;&#20248;&#20110;&#20854;&#20182;&#26377;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19988;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24179;&#28369;&#30340;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#30340;&#20108;&#27425;&#38128;&#38142;&#25439;&#22833;&#26368;&#23567;&#21270;&#26469;&#30740;&#31350;SVM&#30340;&#35757;&#32451;&#12290;&#36825;&#20010;&#36873;&#25321;&#20026;&#22522;&#20110;&#20027;&#35201;&#21270;-&#26368;&#23567;&#21270;&#26041;&#27861;&#30340;&#24555;&#36895;&#35757;&#32451;&#26041;&#27861;&#30340;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20174;&#32780;&#21463;&#30410;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#21487;&#24494;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#20419;&#36827;&#36873;&#25321;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#31232;&#30095;&#20445;&#30041;&#27491;&#21017;&#21270;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#27979;&#35797;&#21644;&#27604;&#36739;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23450;&#24615;&#24230;&#37327;&#65288;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F 1 &#24471;&#20998;&#65289;&#20197;&#21450;&#35745;&#31639;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several decades ago, Support Vector Machines (SVMs) were introduced for performing binary classification tasks, under a supervised framework. Nowadays, they often outperform other supervised methods and remain one of the most popular approaches in the machine learning arena. In this work, we investigate the training of SVMs through a smooth sparse-promoting-regularized squared hinge loss minimization. This choice paves the way to the application of quick training methods built on majorization-minimization approaches, benefiting from the Lipschitz differentiabililty of the loss function. Moreover, the proposed approach allows us to handle sparsity-preserving regularizers promoting the selection of the most significant features, so enhancing the performance. Numerical tests and comparisons conducted on three different datasets demonstrate the good performance of the proposed methodology in terms of qualitative metrics (accuracy, precision, recall, and F 1 score) as well as computational 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36716;&#21270;&#38382;&#39064;&#20351;&#20854;&#25104;&#20026;&#23547;&#25214;&#25193;&#23637;&#31995;&#32479;&#30340;&#22522;&#24577;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#30005;&#23376;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#35745;&#31639;&#21508;&#31181;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#21253;&#25324;&#38750;&#23545;&#35282;&#32447;&#26399;&#26395;&#20540;&#21644;&#36291;&#36801;&#20598;&#26497;&#30697;&#65292;&#24182;&#22312;&#33519;&#31561;&#22823;&#20998;&#23376;&#19978;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16848</link><description>&lt;p&gt;
&#37327;&#23376;&#31995;&#32479;&#20013;&#28608;&#21457;&#24577;&#30340;&#33258;&#28982;&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Natural Quantum Monte Carlo Computation of Excited States. (arXiv:2308.16848v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16848
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36716;&#21270;&#38382;&#39064;&#20351;&#20854;&#25104;&#20026;&#23547;&#25214;&#25193;&#23637;&#31995;&#32479;&#30340;&#22522;&#24577;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#30005;&#23376;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#35745;&#31639;&#21508;&#31181;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#21253;&#25324;&#38750;&#23545;&#35282;&#32447;&#26399;&#26395;&#20540;&#21644;&#36291;&#36801;&#20598;&#26497;&#30697;&#65292;&#24182;&#22312;&#33519;&#31561;&#22823;&#20998;&#23376;&#19978;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#30340;&#26368;&#20302;&#28608;&#21457;&#24577;&#65292;&#36825;&#26159;&#23545;&#23547;&#25214;&#22522;&#24577;&#30340;&#20272;&#35745;&#30340;&#33258;&#28982;&#25512;&#24191;&#12290;&#35813;&#26041;&#27861;&#27809;&#26377;&#33258;&#30001;&#21442;&#25968;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26174;&#24335;&#27491;&#20132;&#21270;&#19981;&#21516;&#30340;&#24577;&#65292;&#32780;&#26159;&#23558;&#23547;&#25214;&#32473;&#23450;&#31995;&#32479;&#30340;&#28608;&#21457;&#24577;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#23547;&#25214;&#25193;&#23637;&#31995;&#32479;&#30340;&#22522;&#24577;&#30340;&#38382;&#39064;&#12290;&#21487;&#20197;&#35745;&#31639;&#20219;&#24847;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#21253;&#25324;&#19981;&#21516;&#24577;&#20043;&#38388;&#30340;&#38750;&#23545;&#35282;&#32447;&#26399;&#26395;&#20540;&#65292;&#22914;&#36291;&#36801;&#20598;&#26497;&#30697;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#23436;&#20840;&#36890;&#29992;&#65292;&#20294;&#19982;&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22810;&#30005;&#23376;&#31995;&#32479;&#21464;&#20998;&#21442;&#25968;&#30340;&#24037;&#20316;&#32467;&#21512;&#20351;&#29992;&#25928;&#26524;&#29305;&#21035;&#22909;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#19982;FermiNet&#21644;Psiformer&#21464;&#20998;&#21442;&#25968;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#33519;&#31561;&#22823;&#20998;&#23376;&#30340;&#22402;&#30452;&#28608;&#21457;&#33021;&#21644;&#25391;&#23376;&#24378;&#24230;&#12290;&#38500;&#20102;&#22312;&#20998;&#23376;&#19978;&#30340;&#31034;&#20363;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;...
&lt;/p&gt;
&lt;p&gt;
We present a variational Monte Carlo algorithm for estimating the lowest excited states of a quantum system which is a natural generalization of the estimation of ground states. The method has no free parameters and requires no explicit orthogonalization of the different states, instead transforming the problem of finding excited states of a given system into that of finding the ground state of an expanded system. Expected values of arbitrary observables can be calculated, including off-diagonal expectations between different states such as the transition dipole moment. Although the method is entirely general, it works particularly well in conjunction with recent work on using neural networks as variational Ansatze for many-electron systems, and we show that by combining this method with the FermiNet and Psiformer Ansatze we can accurately recover vertical excitation energies and oscillator strengths on molecules as large as benzene. Beyond the examples on molecules presented here, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;PDMs&#65289;&#29983;&#25104;&#20102;&#22522;&#20110;&#38647;&#36798;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;PDMs&#25104;&#21151;&#29983;&#25104;&#20102;&#22797;&#26434;&#21644;&#36924;&#30495;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#20294;&#37319;&#26679;&#26102;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20182;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#24320;&#28304;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#35757;&#32451;&#12289;&#37319;&#26679;&#21644;&#35780;&#20272;PDMs&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.16847</link><description>&lt;p&gt;
&#24178;&#28041;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Interferometric Satellite Aperture Radar. (arXiv:2308.16847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16847
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;PDMs&#65289;&#29983;&#25104;&#20102;&#22522;&#20110;&#38647;&#36798;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;PDMs&#25104;&#21151;&#29983;&#25104;&#20102;&#22797;&#26434;&#21644;&#36924;&#30495;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#20294;&#37319;&#26679;&#26102;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20182;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#24320;&#28304;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#35757;&#32451;&#12289;&#37319;&#26679;&#21644;&#35780;&#20272;PDMs&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;PDMs&#65289;&#26368;&#36817;&#34987;&#35777;&#26126;&#26159;&#19968;&#31867;&#38750;&#24120;&#26377;&#21069;&#26223;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#38750;&#33258;&#28982;&#22270;&#20687;&#65288;&#22914;&#22522;&#20110;&#38647;&#36798;&#30340;&#21355;&#26143;&#25968;&#25454;&#65289;&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#36824;&#26159;&#22823;&#37096;&#20998;&#26410;&#30693;&#30340;&#12290;&#29983;&#25104;&#22823;&#37327;&#30340;&#21512;&#25104;&#65288;&#23588;&#20854;&#26159;&#26631;&#35760;&#30340;&#65289;&#21355;&#26143;&#25968;&#25454;&#23545;&#20110;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#21644;&#20998;&#26512;&#65288;&#24178;&#28041;&#65289;&#21355;&#26143;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;PDMs&#29983;&#25104;&#20102;&#20960;&#20010;&#22522;&#20110;&#38647;&#36798;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PDMs&#25104;&#21151;&#29983;&#25104;&#20102;&#20855;&#26377;&#22797;&#26434;&#21644;&#36924;&#30495;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#20294;&#37319;&#26679;&#26102;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20107;&#23454;&#19978;&#65292;&#21152;&#36895;&#37319;&#26679;&#31574;&#30053;&#22312;&#31616;&#21333;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#65289;&#19978;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#22833;&#36133;&#20102;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#24320;&#28304;&#24037;&#20855;https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#35757;&#32451;&#12289;&#37319;&#26679;&#21644;&#35780;&#20272;PDMs&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Diffusion Models (PDMs) have recently emerged as a very promising class of generative models, achieving high performance in natural image generation. However, their performance relative to non-natural images, like radar-based satellite data, remains largely unknown. Generating large amounts of synthetic (and especially labelled) satellite data is crucial to implement deep-learning approaches for the processing and analysis of (interferometric) satellite aperture radar data. Here, we leverage PDMs to generate several radar-based satellite image datasets. We show that PDMs succeed in generating images with complex and realistic structures, but that sampling time remains an issue. Indeed, accelerated sampling strategies, which work well on simple image datasets like MNIST, fail on our radar datasets. We provide a simple and versatile open-source https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and evaluate PDMs using any dataset on a single GPU.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24046;&#20998;&#21442;&#25968;&#20002;&#24323;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65288;FedDD&#65289;&#12290;&#36825;&#31181;&#26041;&#26696;&#36991;&#20813;&#20102;&#39057;&#32321;&#20132;&#25442;&#27169;&#22411;&#21442;&#25968;&#30340;&#36890;&#20449;&#24310;&#36831;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#20002;&#24323;&#32780;&#19981;&#26159;&#23458;&#25143;&#31471;&#36873;&#25321;&#26469;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16835</link><description>&lt;p&gt;
FedDD: &#36890;&#36807;&#24046;&#20998;&#21442;&#25968;&#20002;&#24323;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout. (arXiv:2308.16835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24046;&#20998;&#21442;&#25968;&#20002;&#24323;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65288;FedDD&#65289;&#12290;&#36825;&#31181;&#26041;&#26696;&#36991;&#20813;&#20102;&#39057;&#32321;&#20132;&#25442;&#27169;&#22411;&#21442;&#25968;&#30340;&#36890;&#20449;&#24310;&#36831;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#20002;&#24323;&#32780;&#19981;&#26159;&#23458;&#25143;&#31471;&#36873;&#25321;&#26469;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#38656;&#35201;&#39057;&#32321;&#20132;&#25442;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#23548;&#33268;&#20102;&#38271;&#26102;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#23588;&#20854;&#26159;&#24403;&#23458;&#25143;&#31471;&#30340;&#32593;&#32476;&#29615;&#22659;&#24046;&#24322;&#24456;&#22823;&#26102;&#12290;&#27492;&#22806;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#38656;&#35201;&#31561;&#24453;&#26368;&#24930;&#30340;&#23458;&#25143;&#31471;&#65288;&#21363;straggler&#65292;&#21487;&#33021;&#20855;&#26377;&#26368;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#26368;&#20302;&#30340;&#35745;&#31639;&#33021;&#21147;&#25110;&#26368;&#24046;&#30340;&#32593;&#32476;&#26465;&#20214;&#65289;&#19978;&#20256;&#21442;&#25968;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#36890;&#20449;&#25928;&#29575;&#12290;&#24120;&#29992;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#65292;&#22914;&#37096;&#20998;&#23458;&#25143;&#31471;&#36873;&#25321;&#65292;&#20250;&#23548;&#33268;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#24182;&#21066;&#24369;&#20840;&#23616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#27169;&#22411;&#21442;&#25968;&#20002;&#24323;&#32780;&#19981;&#26159;&#23458;&#25143;&#31471;&#36873;&#25321;&#65292;&#24182;&#25454;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#21442;&#25968;&#20002;&#24323;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65288;FedDD&#65289;&#26694;&#26550;&#12290;FedDD&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#20002;&#24323;&#29575;&#20998;&#37197;&#21644;&#19978;&#20256;&#21442;&#25968;&#36873;&#25321;&#65292;&#23558;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) requires frequent exchange of model parameters, which leads to long communication delay, especially when the network environments of clients vary greatly. Moreover, the parameter server needs to wait for the slowest client (i.e., straggler, which may have the largest model size, lowest computing capability or worst network condition) to upload parameters, which may significantly degrade the communication efficiency. Commonly-used client selection methods such as partial client selection would lead to the waste of computing resources and weaken the generalization of the global model. To tackle this problem, along a different line, in this paper, we advocate the approach of model parameter dropout instead of client selection, and accordingly propose a novel framework of Federated learning scheme with Differential parameter Dropout (FedDD). FedDD consists of two key modules: dropout rate allocation and uploaded parameter selection, which will optimize the model par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#32423;&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243; (MOGPs)&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#21644;&#29305;&#23450;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#21516;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#39044;&#35745;&#33021;&#22815;&#22312;&#20219;&#21153;&#25968;&#37327;&#22686;&#21152;&#26102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16822</link><description>&lt;p&gt;
&#38024;&#23545;&#23618;&#32423;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#21464;&#37327;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets. (arXiv:2308.16822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#32423;&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243; (MOGPs)&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#21644;&#29305;&#23450;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#21516;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#39044;&#35745;&#33021;&#22815;&#22312;&#20219;&#21153;&#25968;&#37327;&#22686;&#21152;&#26102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65288;MOGPs&#65289;&#24050;&#32463;&#34987;&#24341;&#20837;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#36755;&#20986;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#36890;&#24120;&#65292;MOGPs&#27169;&#22411;&#20551;&#35774;&#36755;&#20986;&#20043;&#38388;&#23384;&#22312;&#24179;&#22374;&#30340;&#30456;&#20851;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20844;&#24335;&#24182;&#19981;&#33021;&#32771;&#34385;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;&#22914;&#26524;&#27599;&#20010;&#36755;&#20986;&#37117;&#26377;&#22810;&#20010;&#37325;&#22797;&#35266;&#23519;&#20540;&#65288;&#36825;&#26159;&#29983;&#29289;&#23454;&#39564;&#20013;&#30340;&#20856;&#22411;&#35774;&#32622;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23618;&#32423;&#25968;&#25454;&#38598;&#30340;MOGPs&#25193;&#23637;&#65288;&#21363;&#21487;&#20197;&#22312;&#26641;&#29366;&#32467;&#26500;&#20013;&#34920;&#31034;&#35266;&#27979;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#25968;&#25454;&#38598;&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#26680;&#20989;&#25968;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#20013;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20197;&#25429;&#25417;&#19981;&#21516;&#23618;&#27425;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#36890;&#36807;&#19987;&#29992;&#26680;&#20989;&#25968;&#34920;&#31034;&#36755;&#20986;&#20043;&#38388;&#30340;&#28508;&#22312;&#20381;&#36182;&#20851;&#31995;&#12290;&#39044;&#35745;&#36825;&#20010;&#29305;&#24615;&#33021;&#22815;&#22312;&#20219;&#21153;&#25968;&#37327;&#22686;&#21152;&#26102;&#26174;&#33879;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-output Gaussian processes (MOGPs) have been introduced to deal with multiple tasks by exploiting the correlations between different outputs. Generally, MOGPs models assume a flat correlation structure between the outputs. However, such a formulation does not account for more elaborate relationships, for instance, if several replicates were observed for each output (which is a typical setting in biological experiments). This paper proposes an extension of MOGPs for hierarchical datasets (i.e. datasets for which the relationships between observations can be represented within a tree structure). Our model defines a tailored kernel function accounting for hierarchical structures in the data to capture different levels of correlations while leveraging the introduction of latent variables to express the underlying dependencies between outputs through a dedicated kernel. This latter feature is expected to significantly improve scalability as the number of tasks increases. An extensive e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16818</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20013;&#21463;&#26234;&#33021;&#20132;&#21449;&#21475;&#25511;&#21046;&#30340;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#27969;&#37327;&#23545;&#20110;&#25552;&#21319;&#20132;&#36890;&#20986;&#34892;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#19981;&#35268;&#21017;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#24182;&#19988;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#24322;&#27493;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;2&#65289;&#20132;&#36890;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;3) &#38656;&#35201;&#39044;&#27979;&#30340;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#24403;&#21069;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;(ASeer)&#26469;&#39044;&#27979;&#26234;&#33021;&#20132;&#21449;&#21475;&#36827;&#20837;&#36710;&#36947;&#30340;&#20132;&#36890;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#20132;&#36890;&#25193;&#25955;&#22270;&#19978;&#36830;&#25509;&#36710;&#36947;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#22270;&#25193;&#25955;&#32593;&#32476;&#26469;&#27169;&#25311;&#36710;&#36947;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16800</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31561;&#32423;&#23849;&#22604;&#23548;&#33268;&#24179;&#28369;&#36807;&#24230;&#21644;&#20851;&#32852;&#36807;&#39640;
&lt;/p&gt;
&lt;p&gt;
Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#30340;&#26032;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22266;&#23450;&#19981;&#21464;&#23376;&#31354;&#38388;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#34920;&#29616;&#20986;&#19968;&#31181;&#30456;&#23545;&#30340;&#34892;&#20026;&#65292;&#19981;&#21463;&#29305;&#24449;&#36716;&#25442;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#19982;&#25910;&#25947;&#21040;&#24120;&#25968;&#29366;&#24577;&#21644;&#33410;&#28857;&#29366;&#24577;&#30340;&#36807;&#20998;&#20998;&#31163;&#30456;&#20851;&#30340;&#26368;&#26032;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#20026;&#23376;&#31354;&#38388;&#30340;&#25918;&#22823;&#21482;&#21462;&#20915;&#20110;&#32858;&#21512;&#20989;&#25968;&#30340;&#39057;&#35889;&#12290;&#22312;&#32447;&#24615;&#22330;&#26223;&#20013;&#65292;&#36825;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30001;&#20302;&#32500;&#23376;&#31354;&#38388;&#20027;&#23548;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29305;&#24449;&#36716;&#25442;&#26080;&#20851;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#24403;&#24179;&#28369;&#21521;&#37327;&#36328;&#36234;&#36825;&#20010;&#23376;&#31354;&#38388;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#30410;&#29305;&#24615;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#39640;&#20851;&#32852;&#21644;&#31561;&#32423;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#21333;&#32431;&#32467;&#26500;&#65292;&#36825;&#39033;&#30740;&#31350;&#23454;&#29616;&#20102;&#23398;&#29983;&#20195;&#29702;&#21644;&#25945;&#24072;&#20195;&#29702;&#20043;&#38388;&#30340;&#35821;&#20041;&#36890;&#20449;&#21644;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16789</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21333;&#32431;&#32467;&#26500;&#23454;&#29616;&#35821;&#20041;&#26412;&#22320;&#21270;&#21644;&#25512;&#26029;&#30340;&#32852;&#21512;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Joint Semantic-Native Communication and Inference via Minimal Simplicial Structures. (arXiv:2308.16789v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16789
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#21333;&#32431;&#32467;&#26500;&#65292;&#36825;&#39033;&#30740;&#31350;&#23454;&#29616;&#20102;&#23398;&#29983;&#20195;&#29702;&#21644;&#25945;&#24072;&#20195;&#29702;&#20043;&#38388;&#30340;&#35821;&#20041;&#36890;&#20449;&#21644;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#20041;&#36890;&#20449;&#21644;&#25512;&#26029;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#29983;&#20195;&#29702;&#65288;&#21363;&#31227;&#21160;&#35774;&#22791;&#65289;&#21521;&#25945;&#24072;&#20195;&#29702;&#65288;&#21363;&#20113;&#26381;&#21153;&#22120;&#65289;&#26597;&#35810;&#29983;&#25104;&#29983;&#27963;&#22312;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#20013;&#30340;&#39640;&#38454;&#25968;&#25454;&#35821;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25945;&#24072;&#20808;&#23558;&#20854;&#25968;&#25454;&#26144;&#23556;&#21040;k&#38454;&#21333;&#32431;&#22797;&#21512;&#20307;&#20013;&#65292;&#24182;&#23398;&#20064;&#20854;&#39640;&#38454;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#36890;&#20449;&#21644;&#25512;&#26029;&#65292;&#25945;&#24072;&#36890;&#36807;Hodge&#25289;&#26222;&#25289;&#26031;&#36816;&#31639;&#36873;&#25321;&#24615;&#22320;&#31227;&#38500;&#21333;&#32431;&#24418;&#65292;&#23547;&#25214;&#26368;&#23567;&#20805;&#20998;&#21644;&#19981;&#21464;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#22312;&#19981;&#24433;&#21709;&#25512;&#26029;&#26597;&#35810;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#20256;&#36798;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#23398;&#29983;&#22522;&#20110;&#25513;&#30721;&#21333;&#32431;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#65288;SCAE&#65289;&#26412;&#22320;&#36816;&#34892;&#33258;&#24049;&#30340;&#19968;&#32452;&#26597;&#35810;&#65292;&#21033;&#29992;&#26412;&#22320;&#21644;&#36828;&#31243;&#25945;&#24072;&#30340;&#30693;&#35782;&#12290;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#25552;&#39640;&#25512;&#26029;&#26597;&#35810;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the problem of semantic communication and inference, in which a student agent (i.e. mobile device) queries a teacher agent (i.e. cloud sever) to generate higher-order data semantics living in a simplicial complex. Specifically, the teacher first maps its data into a k-order simplicial complex and learns its high-order correlations. For effective communication and inference, the teacher seeks minimally sufficient and invariant semantic structures prior to conveying information. These minimal simplicial structures are found via judiciously removing simplices selected by the Hodge Laplacians without compromising the inference query accuracy. Subsequently, the student locally runs its own set of queries based on a masked simplicial convolutional autoencoder (SCAE) leveraging both local and remote teacher's knowledge. Numerical results corroborate the effectiveness of the proposed approach in terms of improving inference query accuracy under different channel conditio
&lt;/p&gt;</description></item><item><title>StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16781</link><description>&lt;p&gt;
StratMed&#65306;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16781
&lt;/p&gt;
&lt;p&gt;
StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26377;&#38480;&#21307;&#30103;&#36164;&#28304;&#19982;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#20043;&#38388;&#30340;&#22833;&#34913;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20020;&#24202;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#32437;&#21521;&#21382;&#21490;&#19982;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#23433;&#20840;&#12289;&#26356;&#20934;&#30830;&#22320;&#24320;&#20855;&#33647;&#29289;&#32452;&#21512;&#22788;&#26041;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21307;&#30103;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#32570;&#20047;&#22836;&#23614;&#25968;&#25454;&#20043;&#38388;&#30340;&#24179;&#34913;&#34920;&#31034;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StratMed&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21019;&#26032;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21327;&#35843;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26500;&#24314;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#33719;&#21462;&#23454;&#20307;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#37329;&#23383;&#22612;&#30340;&#25968;&#25454;&#20998;&#23618;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#36890;&#29992;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16775</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#30340;&#38646;&#26679;&#26412;NAS&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16775
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#24471;&#21040;&#30340;&#24615;&#33021;&#25351;&#26631;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;one-hot&#32534;&#30721;&#23558;&#21069;&#39304;&#32467;&#26500;&#34920;&#31034;&#20026;&#32452;&#20214;&#22270;&#30340;&#36825;&#20123;&#25351;&#26631;&#38754;&#20020;&#19968;&#20010;&#38480;&#21046;&#65306;&#26080;&#27861;&#22312;&#19981;&#21516;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#25163;&#24037;&#24615;&#33021;&#25351;&#26631;&#65288;&#38646;&#26679;&#26412;NAS&#65289;&#21487;&#20197;&#22312;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#20013;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#26550;&#26500;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;NAS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#36827;&#34892;&#21367;&#31215;&#26680;&#30340;&#32534;&#30721;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20854;&#32467;&#26500;&#31867;&#20284;&#20110;&#27491;&#22312;&#35780;&#20272;&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#32534;&#30721;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#26550;&#26500;&#25299;&#25169;&#20449;&#24687;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#21518;&#65292;&#20276;&#38543;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23545;&#26550;&#26500;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20301;&#32622;&#26631;&#31614;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32508;&#21512;&#30340;&#20998;&#21106;&#21644;&#32858;&#31867;&#31639;&#27861;&#23454;&#29616;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2308.16759</link><description>&lt;p&gt;
&#26080;&#38656;&#20301;&#32622;&#26631;&#31614;&#26500;&#24314;&#23460;&#20869;&#22522;&#20110;&#21306;&#22495;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Constructing Indoor Region-based Radio Map without Location Labels. (arXiv:2308.16759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20301;&#32622;&#26631;&#31614;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32508;&#21512;&#30340;&#20998;&#21106;&#21644;&#32858;&#31867;&#31639;&#27861;&#23454;&#29616;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#22320;&#22270;&#30340;&#26500;&#24314;&#38656;&#35201;&#22823;&#37327;&#24102;&#26377;&#20301;&#32622;&#26631;&#31614;&#30340;&#26080;&#32447;&#30005;&#27979;&#37327;&#25968;&#25454;&#65292;&#36825;&#32473;&#37096;&#32626;&#25104;&#26412;&#24102;&#26469;&#20102;&#24456;&#39640;&#30340;&#21387;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#27979;&#37327;&#25968;&#25454;&#32780;&#26080;&#38656;&#20301;&#32622;&#26631;&#31614;&#12290;&#26500;&#24314;&#36807;&#31243;&#22522;&#20110;&#20174;&#19968;&#20010;&#35774;&#22791;&#19978;&#30450;&#30446;&#25910;&#38598;&#21040;&#30340;RSS&#27979;&#37327;&#25968;&#25454;&#65292;&#35813;&#35774;&#22791;&#22312;&#23460;&#20869;&#21306;&#22495;&#20013;&#30340;&#21508;&#20010;&#21306;&#22495;&#20013;&#24688;&#22909;&#35775;&#38382;&#19968;&#27425;&#65292;&#20294;&#27809;&#26377;&#35760;&#24405;&#33050;&#21360;&#21644;&#26102;&#38388;&#25139;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#23558;RSS&#25968;&#25454;&#32858;&#31867;&#65292;&#24182;&#23558;&#32858;&#31867;&#19982;&#29289;&#29702;&#21306;&#22495;&#36827;&#34892;&#21305;&#37197;&#12290;&#30001;&#20110;&#22810;&#24452;&#21644;&#22122;&#22768;&#30340;&#23384;&#22312;&#65292;&#20256;&#32479;&#30340;&#32858;&#31867;&#31639;&#27861;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;RSS&#25968;&#25454;&#65292;&#22240;&#20026;RSS&#25968;&#25454;&#33258;&#28982;&#32780;&#28982;&#22320;&#21576;&#29616;&#20026;&#38750;&#32858;&#31867;&#30340;&#24418;&#24335;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#39034;&#24207;&#20808;&#39564;&#30340;&#20449;&#21495;&#23376;&#31354;&#38388;&#27169;&#22411;&#29992;&#20110;&#22788;&#29702;RSS&#25968;&#25454;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#32508;&#21512;&#20998;&#21106;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#35777;&#26126;&#33021;&#22815;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#23558;&#32858;&#31867;&#25968;&#25454;&#19982;&#29289;&#29702;&#21306;&#22495;&#36827;&#34892;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio map construction requires a large amount of radio measurement data with location labels, which imposes a high deployment cost. This paper develops a region-based radio map from received signal strength (RSS) measurements without location labels. The construction is based on a set of blindly collected RSS measurement data from a device that visits each region in an indoor area exactly once, where the footprints and timestamps are not recorded. The main challenge is to cluster the RSS data and match clusters with the physical regions. Classical clustering algorithms fail to work as the RSS data naturally appears as non-clustered due to multipaths and noise. In this paper, a signal subspace model with a sequential prior is constructed for the RSS data, and an integrated segmentation and clustering algorithm is developed, which is shown to find the globally optimal solution in a special case. Furthermore, the clustered data is matched with the physical regions using a graph-based app
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20877;&#29983;&#26680;&#31354;&#38388;&#25554;&#20540;&#21644;&#27169;&#22411;&#31616;&#21270;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#20010;&#22810;&#32500;&#30340;&#20122;&#24403;&#25196;-&#38463;&#32599;&#22827;-&#20811;&#38647;&#22240;&#23450;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;PNN&#65289;&#65292;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16754</link><description>&lt;p&gt;
&#20351;&#29992;&#20877;&#29983;&#26680;&#31354;&#38388;&#25554;&#20540;&#21644;&#27169;&#22411;&#31616;&#21270;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks Using Reproducing Kernel Space Interpolation and Model Reduction. (arXiv:2308.16754v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20877;&#29983;&#26680;&#31354;&#38388;&#25554;&#20540;&#21644;&#27169;&#22411;&#31616;&#21270;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#20010;&#22810;&#32500;&#30340;&#20122;&#24403;&#25196;-&#38463;&#32599;&#22827;-&#20811;&#38647;&#22240;&#23450;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;PNN&#65289;&#65292;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#29702;&#35770;&#20013;&#30340;&#25554;&#20540;&#25216;&#26415;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#25512;&#24191;&#21040;&#20811;&#33713;&#22240;&#31354;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26159;&#20877;&#29983;&#26680;&#20811;&#33713;&#22240;&#31354;&#38388;&#65288;RKKS&#65289;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;RKKS&#20851;&#32852;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#27010;&#24565;&#65292;&#24182;&#24320;&#21457;&#20102;&#25913;&#36827;&#21508;&#31181;&#28608;&#27963;&#20989;&#25968;&#34920;&#36798;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#22797;&#21464;&#20989;&#25968;&#30340;&#29702;&#35770;&#27010;&#24565;&#35777;&#26126;&#20102;&#19968;&#20010;&#21487;&#35745;&#31639;&#30340;&#12289;&#22810;&#32500;&#30340;&#20122;&#24403;&#25196;-&#38463;&#32599;&#22827;-&#20811;&#38647;&#22240;&#65288;AAK&#65289;&#23450;&#29702;&#30340;&#25512;&#24191;&#12290;&#35813;&#23450;&#29702;&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#24310;&#25299;&#31070;&#32463;&#32593;&#32476;&#65288;PNN&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#23558;&#22810;&#32500;AAK&#23450;&#29702;&#29992;&#20110;&#33719;&#24471;PNN&#65292;&#21487;&#20197;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#33719;&#24471;&#20248;&#20110;&#25105;&#20204;&#30340;&#25554;&#20540;&#26041;&#27861;&#21644;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#23454;&#20363;&#20013;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#29992;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and study the theory of training neural networks using interpolation techniques from reproducing kernel Hilbert space theory. We generalize the method to Krein spaces, and show that widely-used neural network architectures are subsets of reproducing kernel Krein spaces (RKKS). We study the concept of "associated Hilbert spaces" of RKKS and develop techniques to improve upon the expressivity of various activation functions. Next, using concepts from the theory of functions of several complex variables, we prove a computationally applicable, multidimensional generalization of the celebrated Adamjan- Arov-Krein (AAK) theorem. The theorem yields a novel class of neural networks, called Prolongation Neural Networks (PNN). We demonstrate that, by applying the multidimensional AAK theorem to gain a PNN, one can gain performance superior to both our interpolatory methods and current state-of-the-art methods in noisy environments. We provide useful illustrations of our methods in p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#24369;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#36817;&#20284;&#21464;&#31181;ADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;Moreau&#21253;&#32476;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#33021;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#21516;&#26102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#24120;&#29992;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2308.16752</link><description>&lt;p&gt;
Moreau&#21253;&#32476;ADMM&#31639;&#27861;&#29992;&#20110;&#20998;&#25955;&#24369;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Moreau Envelope ADMM for Decentralized Weakly Convex Optimization. (arXiv:2308.16752v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#24369;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#36817;&#20284;&#21464;&#31181;ADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;Moreau&#21253;&#32476;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#33021;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#21516;&#26102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#24120;&#29992;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#23616;&#37096;&#26041;&#21521;&#20056;&#27861;&#65288;ADMM&#65289;&#30340;&#36817;&#20284;&#21464;&#31181;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;ADMM&#31639;&#27861;&#22312;&#35768;&#22810;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#37117;&#25552;&#20379;&#20102;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#26377;&#24076;&#26395;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#24369;&#20984;&#21644;&#23616;&#37096;&#38750;&#20809;&#28369;&#20989;&#25968;&#26159;&#21542;&#33021;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#20351;&#29992;Moreau&#21253;&#32476;&#20989;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MADM&#22312;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#19979;&#30830;&#23454;&#33021;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#21253;&#25324;&#36890;&#36807;&#23558;Moreau&#21253;&#32476;&#20989;&#25968;&#30340;&#26799;&#24230;&#19982;&#36817;&#24515;&#20989;&#25968;&#30456;&#20851;&#32852;&#26469;&#35745;&#31639;&#23545;&#20598;&#21464;&#37327;&#26356;&#26032;&#27493;&#39588;&#20013;&#30340;&#25913;&#21464;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a proximal variant of the alternating direction method of multipliers (ADMM) for distributed optimization. Although the current versions of ADMM algorithm provide promising numerical results in producing solutions that are close to optimal for many convex and non-convex optimization problems, it remains unclear if they can converge to a stationary point for weakly convex and locally non-smooth functions. Through our analysis using the Moreau envelope function, we demonstrate that MADM can indeed converge to a stationary point under mild conditions. Our analysis also includes computing the bounds on the amount of change in the dual variable update step by relating the gradient of the Moreau envelope function to the proximal function. Furthermore, the results of our numerical experiments indicate that our method is faster and more robust than widely-used approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#39057;&#22495;&#30340;&#22810;&#20998;&#25903;&#32593;&#32476;US-SFNet&#65292;&#29992;&#20110;&#36229;&#22768;&#22270;&#20687;&#20013;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#21464;&#30340;&#35786;&#26029;&#12290;&#36890;&#36807;&#20351;&#29992;Conv-FFT&#22359;&#26469;&#24314;&#27169;&#22270;&#20687;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16738</link><description>&lt;p&gt;
US-SFNet:&#22522;&#20110;&#31354;&#38388;&#39057;&#22495;&#30340;&#22810;&#20998;&#25903;&#32593;&#32476;&#29992;&#20110;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#28790;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
US-SFNet: A Spatial-Frequency Domain-based Multi-branch Network for Cervical Lymph Node Lesions Diagnoses in Ultrasound Images. (arXiv:2308.16738v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#39057;&#22495;&#30340;&#22810;&#20998;&#25903;&#32593;&#32476;US-SFNet&#65292;&#29992;&#20110;&#36229;&#22768;&#22270;&#20687;&#20013;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#21464;&#30340;&#35786;&#26029;&#12290;&#36890;&#36807;&#20351;&#29992;Conv-FFT&#22359;&#26469;&#24314;&#27169;&#22270;&#20687;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#25104;&#20687;&#26159;&#35786;&#26029;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#28790;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#30340;&#35786;&#26029;&#20027;&#35201;&#20381;&#36182;&#20110;&#21307;&#21153;&#20154;&#21592;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20351;&#24471;&#36825;&#19968;&#36807;&#31243;&#23481;&#26131;&#20986;&#29616;&#35823;&#35786;&#12290;&#23613;&#31649;&#36805;&#36895;&#21457;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#25913;&#36827;&#21508;&#31181;&#36229;&#22768;&#22270;&#20687;&#30340;&#35786;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#39048;&#37096;&#28107;&#24052;&#32467;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#35786;&#26029;&#39048;&#37096;&#28107;&#24052;&#32467;&#30149;&#28790;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;3392&#24352;&#27491;&#24120;&#28107;&#24052;&#32467;&#12289;&#33391;&#24615;&#28107;&#24052;&#32467;&#30149;&#28790;&#12289;&#24694;&#24615;&#21407;&#21457;&#28107;&#24052;&#32467;&#30149;&#28790;&#21644;&#24694;&#24615;&#36716;&#31227;&#28107;&#24052;&#32467;&#30149;&#28790;&#30340;&#22270;&#20687;&#12290;&#37492;&#20110;&#36229;&#22768;&#22270;&#20687;&#26159;&#30001;&#22768;&#27874;&#22312;&#19981;&#21516;&#30340;&#36523;&#20307;&#32452;&#32455;&#20013;&#21453;&#23556;&#21644;&#25955;&#23556;&#20135;&#29983;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Conv-FFT&#22359;&#12290;&#23427;&#23558;&#21367;&#31215;&#25805;&#20316;&#19982;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#30456;&#32467;&#21512;&#65292;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Ultrasound imaging serves as a pivotal tool for diagnosing cervical lymph node lesions. However, the diagnoses of these images largely hinge on the expertise of medical practitioners, rendering the process susceptible to misdiagnoses. Although rapidly developing deep learning has substantially improved the diagnoses of diverse ultrasound images, there remains a conspicuous research gap concerning cervical lymph nodes. The objective of our work is to accurately diagnose cervical lymph node lesions by leveraging a deep learning model. To this end, we first collected 3392 images containing normal lymph nodes, benign lymph node lesions, malignant primary lymph node lesions, and malignant metastatic lymph node lesions. Given that ultrasound images are generated by the reflection and scattering of sound waves across varied bodily tissues, we proposed the Conv-FFT Block. It integrates convolutional operations with the fast Fourier transform to more astutely model the images. Building upon thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#21644;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#24322;&#24120;&#25968;&#25454;&#24178;&#25200;&#21644;&#31639;&#27861;&#25910;&#25947;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16737</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Networked Federated Learning for Localization. (arXiv:2308.16737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#21644;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#24322;&#24120;&#25968;&#25454;&#24178;&#25200;&#21644;&#31639;&#27861;&#25910;&#25947;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#29615;&#22659;&#20013;&#65292;&#26412;&#36136;&#19978;&#26159;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#23450;&#20301;&#38382;&#39064;&#12290;&#30001;&#20110;&#32852;&#37030;&#29615;&#22659;&#30340;&#20998;&#25955;&#24615;&#36136;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#25104;&#20026;&#21487;&#20280;&#32553;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20851;&#38190;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29615;&#22659;&#32463;&#24120;&#21463;&#21040;&#24322;&#24120;&#25968;&#25454;&#30340;&#24178;&#25200;&#65292;&#20351;&#24471;&#20256;&#32479;&#26041;&#27861;&#22312;&#32500;&#25252;&#20272;&#35745;&#31934;&#24230;&#21644;&#30830;&#20445;&#31639;&#27861;&#25910;&#25947;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#20013;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36825;&#20123;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21407;&#22987;&#24418;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#36845;&#20195;&#31616;&#21270;&#25110;&#36817;&#20284;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#20272;&#35745;&#31934;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#31361;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of localization, which is inherently non-convex and non-smooth in a federated setting where the data is distributed across a multitude of devices. Due to the decentralized nature of federated environments, distributed learning becomes essential for scalability and adaptability. Moreover, these environments are often plagued by outlier data, which presents substantial challenges to conventional methods, particularly in maintaining estimation accuracy and ensuring algorithm convergence. To mitigate these challenges, we propose a method that adopts an $L_1$-norm robust formulation within a distributed sub-gradient framework, explicitly designed to handle these obstacles. Our approach addresses the problem in its original form, without resorting to iterative simplifications or approximations, resulting in enhanced computational efficiency and improved estimation accuracy. We demonstrate that our method converges to a stationary point, highlighting its effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#19981;&#21487;&#38752;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#30340;&#40065;&#26834;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#40065;&#26834;&#23545;&#27604;&#23398;&#20064;&#21644;&#26631;&#31614;&#36136;&#37327;&#25913;&#36827;&#31574;&#30053;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#25269;&#25239;&#19981;&#20934;&#30830;&#37096;&#20998;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.16718</link><description>&lt;p&gt;
&#21487;&#38752;&#24615;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#40065;&#26834;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning for Unreliable Partial Label Learning. (arXiv:2308.16718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#19981;&#21487;&#38752;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#30340;&#40065;&#26834;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#40065;&#26834;&#23545;&#27604;&#23398;&#20064;&#21644;&#26631;&#31614;&#36136;&#37327;&#25913;&#36827;&#31574;&#30053;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#25269;&#25239;&#19981;&#20934;&#30830;&#37096;&#20998;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#20854;&#20013;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#34987;&#20998;&#37197;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#26631;&#31614;&#26159;&#30495;&#23454;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28508;&#22312;&#30340;&#27880;&#37322;&#19981;&#20934;&#30830;&#24615;&#65292;&#36825;&#20010;&#29702;&#24819;&#21270;&#30340;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#24847;&#21619;&#30528;&#30495;&#23454;&#26631;&#31614;&#21487;&#33021;&#19981;&#22312;&#20505;&#36873;&#26631;&#31614;&#38598;&#20013;&#12290;&#36825;&#34987;&#31216;&#20026;&#19981;&#21487;&#38752;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65292;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#30001;&#20110;&#37096;&#20998;&#26631;&#31614;&#30340;&#19981;&#21487;&#38752;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#32780;&#23548;&#33268;&#24615;&#33021;&#20122;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#38752;&#24615;&#40065;&#26834;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;URRL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#40065;&#26834;&#23545;&#27604;&#23398;&#20064;&#26469;&#24110;&#21161;&#27169;&#22411;&#26377;&#25928;&#22320;&#25269;&#24481;&#19981;&#21487;&#38752;&#30340;&#37096;&#20998;&#26631;&#31614;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31574;&#30053;&#65292;&#32467;&#21512;&#22522;&#20110;KNN&#30340;&#20505;&#36873;&#26631;&#31614;&#38598;&#26657;&#27491;&#21644;&#22522;&#20110;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#26631;&#31614;&#28040;&#38500;&#26469;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial Label Learning (PLL) is a type of weakly supervised learning where each training instance is assigned a set of candidate labels, but only one label is the ground-truth. However, this idealistic assumption may not always hold due to potential annotation inaccuracies, meaning the ground-truth may not be present in the candidate label set. This is known as Unreliable Partial Label Learning (UPLL) that introduces an additional complexity due to the inherent unreliability and ambiguity of partial labels, often resulting in a sub-optimal performance with existing methods. To address this challenge, we propose the Unreliability-Robust Representation Learning framework (URRL) that leverages unreliability-robust contrastive learning to help the model fortify against unreliable partial labels effectively. Concurrently, we propose a dual strategy that combines KNN-based candidate label set correction and consistency-regularization-based label disambiguation to refine label quality and enh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20005;&#37325;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#21363;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;&#65292;&#26080;&#38656;&#35774;&#35745;&#29305;&#23450;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#35843;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.16684</link><description>&lt;p&gt;
&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#25915;&#20987;&#65306;&#23558;&#26377;&#25439;&#21387;&#32553;&#37325;&#26032;&#29992;&#20316;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20005;&#37325;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#21363;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;&#65292;&#26080;&#38656;&#35774;&#35745;&#29305;&#23450;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21518;&#38376;&#25915;&#20987;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26500;&#25104;&#20102;&#23041;&#32961;&#12290;&#20256;&#32479;&#26234;&#24935;&#35748;&#20026;&#65292;&#24182;&#19981;&#26159;&#27599;&#20010;&#20154;&#37117;&#21487;&#20197;&#25104;&#20026;&#25915;&#20987;&#32773;&#65292;&#22240;&#20026;&#35774;&#35745;&#35302;&#21457;&#22120;&#29983;&#25104;&#31639;&#27861;&#30340;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#30830;&#20445;&#25915;&#20987;&#30340;&#38544;&#31192;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25351;&#20986;&#23384;&#22312;&#19968;&#31181;&#26356;&#20026;&#20005;&#37325;&#30340;&#21518;&#38376;&#23041;&#32961;&#65306;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#31639;&#27861;&#36827;&#34892;&#38544;&#24708;&#21518;&#38376;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#21387;&#32553;&#24037;&#20855;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26377;&#25439;&#22270;&#29255;&#21387;&#32553;&#25216;&#26415;&#65292;&#26080;&#38656;&#30041;&#19979;&#20219;&#20309;&#26126;&#26174;&#30340;&#30165;&#36857;&#23601;&#33021;&#36731;&#26494;&#22320;&#23558;&#35302;&#21457;&#22120;&#27169;&#24335;&#27880;&#20837;&#21040;&#22270;&#20687;&#20013;&#65292;&#21363;&#29983;&#25104;&#30340;&#35302;&#21457;&#22120;&#26159;&#33258;&#28982;&#30340;&#22270;&#20687;&#20266;&#24433;&#12290;&#20351;&#29992;&#26377;&#25439;&#22270;&#29255;&#21387;&#32553;&#24037;&#20855;&#26102;&#65292;&#20154;&#20204;&#24182;&#19981;&#38656;&#35201;&#24191;&#27867;&#30693;&#35782;&#65292;&#21482;&#38656;&#28857;&#20987;&#8220;&#36716;&#25442;&#8221;&#25110;&#8220;&#21478;&#23384;&#20026;&#8221;&#25353;&#38062;&#21363;&#21487;&#12290;&#36890;&#36807;&#36825;&#31181;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#26080;&#38656;&#35774;&#35745;&#19968;&#20010;&#19987;&#38376;&#30340;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#30340;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-used lossy image compression from a plethora of compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace; i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the "convert" or "save as" button while using tools for lossy image compression. Via this attack, the adversary does not need to design a 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#35780;&#20272;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#25581;&#31034;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#20013;&#35774;&#35745;&#20915;&#31574;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.16681</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#35780;&#20272;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65306;&#19968;&#20999;&#65292;&#26080;&#22788;&#19981;&#22312;&#65292;&#20840;&#26041;&#20301;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness. (arXiv:2308.16681v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16681
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#35780;&#20272;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#25581;&#31034;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#20013;&#35774;&#35745;&#20915;&#31574;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#35768;&#22810;&#31995;&#32479;&#37117;&#21033;&#29992;&#31639;&#27861;&#20915;&#31574;&#26469;&#65288;&#37096;&#20998;&#65289;&#33258;&#21160;&#21270;&#20197;&#21069;&#30001;&#20154;&#31867;&#36827;&#34892;&#30340;&#20915;&#31574;&#12290;&#24403;&#35774;&#35745;&#33391;&#22909;&#26102;&#65292;&#36825;&#20123;&#31995;&#32479;&#25215;&#35834;&#26356;&#23458;&#35266;&#30340;&#20915;&#31574;&#65292;&#21516;&#26102;&#33410;&#30465;&#22823;&#37327;&#36164;&#28304;&#65292;&#33410;&#32422;&#20154;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#35774;&#35745;&#19981;&#33391;&#26102;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#31038;&#20250;&#32676;&#20307;&#36827;&#34892;&#27495;&#35270;&#30340;&#19981;&#20844;&#24179;&#20915;&#31574;&#12290;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#30340;&#19979;&#28216;&#25928;&#24212;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31995;&#32479;&#35774;&#35745;&#21644;&#23454;&#26045;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#65292;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32531;&#35299;&#25110;&#21152;&#24378;&#12290;&#35768;&#22810;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#26159;&#38544;&#21547;&#36827;&#34892;&#30340;&#65292;&#19981;&#30693;&#36947;&#23427;&#20204;&#30830;&#20999;&#22320;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#26126;&#30830;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#24182;&#20102;&#35299;&#36825;&#20123;&#20915;&#31574;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#24515;&#29702;&#23398;&#39046;&#22495;&#30340;&#35265;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. When designed well, these systems promise more objective decisions while saving large amounts of resources and freeing up human time. However, when ADM systems are not designed well, they can lead to unfair decisions which discriminate against societal groups. The downstream effects of ADMs critically depend on the decisions made during the systems' design and implementation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these design decisions are made implicitly, without knowing exactly how they will influence the final system. It is therefore important to make explicit the decisions made during the design of ADM systems and understand how these decisions affect the fairness of the resulting system.  To study this issue, we draw on insights from the field of psychology and introduce the metho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24212;&#29992;&#22810;&#31181;&#26799;&#24230;&#20272;&#35745;&#25216;&#26415;&#23454;&#29616;&#23545;&#20855;&#26377;&#31163;&#25955;&#21644;&#20998;&#25903;&#38543;&#26426;&#24615;&#30340;&#31243;&#24207;&#36827;&#34892;&#27714;&#23548;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#23637;&#20102;&#39318;&#20010;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#20998;&#25903;&#31243;&#24207;&#65292;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#22312;&#20110;&#20026;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#26799;&#24230;&#20248;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16680</link><description>&lt;p&gt;
&#26641;&#30340;&#20998;&#25903;&#65306;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#23545;&#20855;&#26377;&#31163;&#25955;&#21644;&#20998;&#25903;&#38543;&#26426;&#24615;&#30340;&#31243;&#24207;&#36827;&#34892;&#27714;&#23548;
&lt;/p&gt;
&lt;p&gt;
Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics. (arXiv:2308.16680v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24212;&#29992;&#22810;&#31181;&#26799;&#24230;&#20272;&#35745;&#25216;&#26415;&#23454;&#29616;&#23545;&#20855;&#26377;&#31163;&#25955;&#21644;&#20998;&#25903;&#38543;&#26426;&#24615;&#30340;&#31243;&#24207;&#36827;&#34892;&#27714;&#23548;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#23637;&#20102;&#39318;&#20010;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#20998;&#25903;&#31243;&#24207;&#65292;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#22312;&#20110;&#20026;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#26799;&#24230;&#20248;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24212;&#29992;&#22810;&#31181;&#26799;&#24230;&#20272;&#35745;&#25216;&#26415;&#26469;&#23454;&#29616;&#23545;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#20855;&#26377;&#31163;&#25955;&#38543;&#26426;&#24615;&#30340;&#31243;&#24207;&#36827;&#34892;&#27714;&#23548;&#12290;&#30001;&#20110;&#23384;&#22312;&#20998;&#25903;&#36807;&#31243;&#21644;&#22522;&#20110;&#32858;&#31867;&#30340;&#20998;&#26512;&#65292;&#27492;&#31867;&#31243;&#24207;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#31867;&#31243;&#24207;&#36827;&#34892;&#27714;&#23548;&#21487;&#20197;&#20026;&#26799;&#24230;&#20248;&#21270;&#22312;&#25506;&#27979;&#22120;&#35774;&#35745;&#20248;&#21270;&#12289;&#27169;&#25311;&#22120;&#35843;&#25972;&#25110;&#25968;&#25454;&#20998;&#26512;&#21644;&#37325;&#26500;&#20248;&#21270;&#31561;&#26041;&#38754;&#24320;&#36767;&#36947;&#36335;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#38543;&#26426;&#33258;&#21160;&#24494;&#20998;&#65288;Stochastic AD&#65289;&#26041;&#27861;&#65292;&#24182;&#22312;&#31616;&#21270;&#30340;&#25506;&#27979;&#22120;&#35774;&#35745;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36804;&#20170;&#20026;&#27490;&#39318;&#20010;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#20998;&#25903;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to apply several gradient estimation techniques to enable the differentiation of programs with discrete randomness in High Energy Physics. Such programs are common in High Energy Physics due to the presence of branching processes and clustering-based analysis. Thus differentiating such programs can open the way for gradient based optimization in the context of detector design optimization, simulator tuning, or data analysis and reconstruction optimization. We discuss several possible gradient estimation strategies, including the recent Stochastic AD method, and compare them in simplified detector design experiments. In doing so we develop, to the best of our knowledge, the first fully differentiable branching program.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#21069;&#36864;&#20986;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#21407;&#22987;&#26550;&#26500;&#21644;&#20998;&#21106;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#28145;&#24230;&#22122;&#22768;&#25233;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.16678</link><description>&lt;p&gt;
&#21160;&#24577;nsNet2: &#39640;&#25928;&#30340;&#25552;&#21069;&#36864;&#20986;&#28145;&#24230;&#22122;&#22768;&#25233;&#21046;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting. (arXiv:2308.16678v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#21069;&#36864;&#20986;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#21407;&#22987;&#26550;&#26500;&#21644;&#20998;&#21106;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#28145;&#24230;&#22122;&#22768;&#25233;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#28145;&#24230;&#22122;&#22768;&#25233;&#21046;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#21033;&#29992;&#28145;&#24230;&#26550;&#26500;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;nsNet2&#30340;&#25552;&#21069;&#36864;&#20986;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#38454;&#27573;&#20572;&#27490;&#35745;&#31639;&#26469;&#25552;&#20379;&#22810;&#20010;&#31934;&#24230;&#32423;&#21035;&#21644;&#36164;&#28304;&#33410;&#30465;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#21106;&#20449;&#24687;&#27969;&#26469;&#36866;&#24212;&#21407;&#22987;&#26550;&#26500;&#65292;&#20197;&#32771;&#34385;&#27880;&#20837;&#30340;&#21160;&#24577;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#26082;&#23450;&#30340;&#25351;&#26631;&#23637;&#31034;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep learning has made strides in the field of deep noise suppression, leveraging deep architectures on resource-constrained devices still proved challenging. Therefore, we present an early-exiting model based on nsNet2 that provides several levels of accuracy and resource savings by halting computations at different stages. Moreover, we adapt the original architecture by splitting the information flow to take into account the injected dynamism. We show the trade-offs between performance and computational complexity based on established metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19968;&#20301;&#21387;&#32553;&#24863;&#30693;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#20256;&#36755;&#19968;&#20301;&#20449;&#24687;&#24182;&#20943;&#23569;&#36890;&#20449;&#22238;&#21512;&#30340;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#31232;&#30095;&#32422;&#26463;&#30340;&#20849;&#20139;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.16671</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#20301;&#21387;&#32553;&#24863;&#30693;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Decentralized Federated Learning via One-Bit Compressive Sensing. (arXiv:2308.16671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19968;&#20301;&#21387;&#32553;&#24863;&#30693;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#20256;&#36755;&#19968;&#20301;&#20449;&#24687;&#24182;&#20943;&#23569;&#36890;&#20449;&#22238;&#21512;&#30340;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#31232;&#30095;&#32422;&#26463;&#30340;&#20849;&#20139;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#22240;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#19982;&#38598;&#20013;&#24335;&#29256;&#26412;&#30456;&#27604;&#65292;&#22312;DFL&#20013;&#22312;&#22823;&#37327;&#33410;&#28857;&#20043;&#38388;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#21327;&#35843;&#35757;&#32451;&#36807;&#31243;&#12290;&#23588;&#20854;&#26159;&#24403;&#20998;&#24067;&#24335;&#33410;&#28857;&#22312;&#36890;&#20449;&#25110;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#26102;&#65292;DFL&#30340;&#35757;&#32451;&#23558;&#21464;&#24471;&#38750;&#24120;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#22522;&#20110;&#19981;&#31934;&#30830;&#20132;&#26367;&#26041;&#21521;&#26041;&#27861;&#65288;iADM&#65289;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#20855;&#26377;&#31232;&#30095;&#32422;&#26463;&#30340;&#20849;&#20139;&#27169;&#22411;&#12290;&#35813;&#32422;&#26463;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#19968;&#20301;&#21387;&#32553;&#24863;&#30693;&#65288;1BCS&#65289;&#65292;&#20801;&#35768;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#20256;&#36755;&#19968;&#20301;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#20165;&#22312;&#26576;&#20123;&#27493;&#39588;&#20013;&#21457;&#29983;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#22238;&#21512;&#30340;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#35813;&#31639;&#27861;&#23637;&#29616;&#20102;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized federated learning (DFL) has gained popularity due to its practicality across various applications. Compared to the centralized version, training a shared model among a large number of nodes in DFL is more challenging, as there is no central server to coordinate the training process. Especially when distributed nodes suffer from limitations in communication or computational resources, DFL will experience extremely inefficient and unstable training. Motivated by these challenges, in this paper, we develop a novel algorithm based on the framework of the inexact alternating direction method (iADM). On one hand, our goal is to train a shared model with a sparsity constraint. This constraint enables us to leverage one-bit compressive sensing (1BCS), allowing transmission of one-bit information among neighbour nodes. On the other hand, communication between neighbour nodes occurs only at certain steps, reducing the number of communication rounds. Therefore, the algorithm exhibi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNNs&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#36890;&#36807;&#38544;&#34255;&#29305;&#24449;&#26144;&#23556;&#23884;&#20837;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#37327;&#23376;&#20020;&#30028;&#24615;&#29983;&#25104;&#36866;&#21512;&#30340;&#22522;&#20989;&#25968;&#38598;&#65292;&#27744;&#21270;&#23618;&#36873;&#25321;&#33021;&#22815;&#24418;&#25104;&#39640;&#24615;&#33021;&#20915;&#31574;&#36793;&#30028;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20381;&#36182;&#20110;&#23884;&#20837;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.16664</link><description>&lt;p&gt;
&#25105;&#20204;&#21487;&#20197;&#20174;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#21040;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What can we learn from quantum convolutional neural networks?. (arXiv:2308.16664v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16664
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNNs&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#36890;&#36807;&#38544;&#34255;&#29305;&#24449;&#26144;&#23556;&#23884;&#20837;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#37327;&#23376;&#20020;&#30028;&#24615;&#29983;&#25104;&#36866;&#21512;&#30340;&#22522;&#20989;&#25968;&#38598;&#65292;&#27744;&#21270;&#23618;&#36873;&#25321;&#33021;&#22815;&#24418;&#25104;&#39640;&#24615;&#33021;&#20915;&#31574;&#36793;&#30028;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20381;&#36182;&#20110;&#23884;&#20837;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNNs&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;1&#65289;&#36890;&#36807;&#38544;&#34255;&#29305;&#24449;&#26144;&#23556;&#65292;&#24037;&#20316;&#20110;&#37327;&#23376;&#25968;&#25454;&#21487;&#20197;&#34987;&#35270;&#20026;&#23884;&#20837;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#65307;2&#65289;&#23545;&#20110;&#37327;&#23376;&#30456;&#20301;&#35782;&#21035;&#65292;&#20854;&#39640;&#24615;&#33021;&#21487;&#20197;&#24402;&#22240;&#20110;&#22312;&#22522;&#24577;&#23884;&#20837;&#26399;&#38388;&#29983;&#25104;&#38750;&#24120;&#36866;&#21512;&#30340;&#22522;&#20989;&#25968;&#38598;&#65292;&#20854;&#20013;&#33258;&#26059;&#27169;&#22411;&#30340;&#37327;&#23376;&#20020;&#30028;&#24615;&#23548;&#33268;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#29305;&#24449;&#30340;&#22522;&#20989;&#25968;&#65307;3&#65289;QCNN&#30340;&#27744;&#21270;&#23618;&#36127;&#36131;&#36873;&#25321;&#37027;&#20123;&#33021;&#22815;&#26377;&#21161;&#20110;&#24418;&#25104;&#39640;&#24615;&#33021;&#20915;&#31574;&#36793;&#30028;&#30340;&#22522;&#20989;&#25968;&#65292;&#23398;&#20064;&#36807;&#31243;&#23545;&#24212;&#20110;&#36866;&#24212;&#24615;&#27979;&#37327;&#65292;&#20351;&#24471;&#23569;&#37327;&#37327;&#23376;&#27604;&#29305;&#31639;&#31526;&#26144;&#23556;&#21040;&#25972;&#20010;&#23492;&#23384;&#22120;&#21487;&#35266;&#27979;&#37327;&#65307;4&#65289;QCNN&#27169;&#22411;&#30340;&#27867;&#21270;&#24378;&#28872;&#20381;&#36182;&#20110;&#23884;&#20837;&#31867;&#22411;&#65292;&#22522;&#20110;&#20613;&#37324;&#21494;&#22522;&#30340;&#26059;&#36716;&#29305;&#24449;&#26144;&#23556;&#38656;&#35201;&#20180;&#32454;&#30340;&#29305;&#24449;&#24037;&#31243;&#65307;5&#65289;&#22522;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#27979;&#37327;&#27425;&#25968;&#30340;&#35835;&#20986;&#30340;QCNN&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#20542;&#21521;&#20110;&#22320;&#38754;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We can learn from analyzing quantum convolutional neural networks (QCNNs) that: 1) working with quantum data can be perceived as embedding physical system parameters through a hidden feature map; 2) their high performance for quantum phase recognition can be attributed to generation of a very suitable basis set during the ground state embedding, where quantum criticality of spin models leads to basis functions with rapidly changing features; 3) pooling layers of QCNNs are responsible for picking those basis functions that can contribute to forming a high-performing decision boundary, and the learning process corresponds to adapting the measurement such that few-qubit operators are mapped to full-register observables; 4) generalization of QCNN models strongly depends on the embedding type, and that rotation-based feature maps with the Fourier basis require careful feature engineering; 5) accuracy and generalization of QCNNs with readout based on a limited number of shots favor the groun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#26102;&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#21487;&#26377;&#25928;&#26816;&#27979;&#21040;CMS&#30005;&#30913;&#37327;&#33021;&#22120;(ECAL)&#20013;&#36807;&#21435;&#26410;&#35265;&#30340;&#24322;&#24120;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#38169;&#35823;&#21457;&#29616;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.16659</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#22312;&#32447;&#25968;&#25454;&#36136;&#37327;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based Online Data Quality Monitoring for the CMS Electromagnetic Calorimeter. (arXiv:2308.16659v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#26102;&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#21487;&#26377;&#25928;&#26816;&#27979;&#21040;CMS&#30005;&#30913;&#37327;&#33021;&#22120;(ECAL)&#20013;&#36807;&#21435;&#26410;&#35265;&#30340;&#24322;&#24120;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#38169;&#35823;&#21457;&#29616;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#65288;ECAL&#65289;&#30340;&#22312;&#32447;&#25968;&#25454;&#36136;&#37327;&#30417;&#27979;&#31995;&#32479;&#65288;DQM&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25805;&#20316;&#24037;&#20855;&#65292;&#23427;&#20351;ECAL&#19987;&#23478;&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#35786;&#26029;&#21508;&#31181;&#21487;&#33021;&#24433;&#21709;&#29289;&#29702;&#25968;&#25454;&#36136;&#37327;&#30340;&#25506;&#27979;&#22120;&#38382;&#39064;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;ECAL DQM&#31995;&#32479;&#24050;&#19981;&#26029;&#26356;&#26032;&#20197;&#24212;&#23545;&#26032;&#38382;&#39064;&#65292;&#20294;&#20173;&#27604;&#26032;&#38382;&#39064;&#24930;&#19968;&#27493;&#12290;&#21033;&#29992;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#26102;&#30340;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#36807;&#21435;&#25968;&#25454;&#20013;&#26410;&#35265;&#30340;ECAL&#24322;&#24120;&#12290;&#32771;&#34385;&#21040;ECAL&#21709;&#24212;&#30340;&#31354;&#38388;&#21464;&#21270;&#21644;&#24322;&#24120;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#26032;&#31995;&#32479;&#33021;&#22815;&#39640;&#25928;&#22320;&#26816;&#27979;&#21040;&#24322;&#24120;&#65292;&#21516;&#26102;&#20445;&#25345;&#20272;&#35745;&#30340;&#38169;&#35823;&#21457;&#29616;&#29575;&#22312;$10^{-2}$&#21040;$10^{-4}$&#20043;&#38388;&#65292;&#27604;&#29616;&#26377;&#22522;&#20934;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#36890;&#36807;&#22312;2018&#24180;&#21644;2022&#24180;&#21457;&#29616;&#30340;&#24322;&#24120;&#36827;&#34892;&#39564;&#35777;&#65292;&#35777;&#23454;&#20102;&#31995;&#32479;&#30340;&#29616;&#23454;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The online Data Quality Monitoring system (DQM) of the CMS electromagnetic calorimeter (ECAL) is a crucial operational tool that allows ECAL experts to quickly identify, localize, and diagnose a broad range of detector issues that would otherwise hinder physics-quality data taking. Although the existing ECAL DQM system has been continuously updated to respond to new problems, it remains one step behind newer and unforeseen issues. Using unsupervised deep learning, a real-time autoencoder-based anomaly detection system is developed that is able to detect ECAL anomalies unseen in past data. After accounting for spatial variations in the response of the ECAL and the temporal evolution of anomalies, the new system is able to efficiently detect anomalies while maintaining an estimated false discovery rate between $10^{-2}$ to $10^{-4}$, beating existing benchmarks by about two orders of magnitude. The real-world performance of the system is validated using anomalies found in 2018 and 2022 L
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#21355;&#26143;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#21355;&#26143;&#22270;&#20687;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22270;&#20687;&#36136;&#37327;&#21644;&#22320;&#22270;&#20445;&#30495;&#24230;&#37117;&#24456;&#22909;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#36965;&#24863;&#39046;&#22495;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16648</link><description>&lt;p&gt;
&#26681;&#25454;&#22320;&#22270;&#29983;&#25104;&#30495;&#23454;&#21355;&#26143;&#24433;&#20687;&#65306;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#21355;&#26143;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generate Your Own Scotland: Satellite Image Generation Conditioned on Maps. (arXiv:2308.16648v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#21355;&#26143;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#21355;&#26143;&#22270;&#20687;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22270;&#20687;&#36136;&#37327;&#21644;&#22320;&#22270;&#20445;&#30495;&#24230;&#37117;&#24456;&#22909;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#36965;&#24863;&#39046;&#22495;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#22320;&#29699;&#35266;&#27979;&#39046;&#22495;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#20173;&#28982;&#34987;&#22823;&#22810;&#25968;&#20154;&#24573;&#35270;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22320;&#29702;&#25968;&#25454;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#21355;&#26143;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;OpenStreetMap&#22270;&#20687;&#21644;&#33487;&#26684;&#20848;&#20013;&#22830;&#24102;&#22320;&#21306;&#30340;&#21355;&#26143;&#22270;&#20687;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;ControlNet&#27169;&#22411;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#22270;&#20687;&#36136;&#37327;&#21644;&#22320;&#22270;&#20445;&#30495;&#24230;&#37117;&#26159;&#21487;&#34892;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#36965;&#24863;&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#19968;&#20123;&#25506;&#35752;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#20195;&#30721;&#65292;&#32593;&#22336;&#20026;https://github.com/miquel-espinosa/map-sat&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advancements in image generation, diffusion models still remain largely underexplored in Earth Observation. In this paper we show that state-of-the-art pretrained diffusion models can be conditioned on cartographic data to generate realistic satellite images. We provide two large datasets of paired OpenStreetMap images and satellite views over the region of Mainland Scotland and the Central Belt. We train a ControlNet model and qualitatively evaluate the results, demonstrating that both image quality and map fidelity are possible. Finally, we provide some insights on the opportunities and challenges of applying these models for remote sensing. Our model weights and code for creating the dataset are publicly available at https://github.com/miquel-espinosa/map-sat.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.16609</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#38271;&#23614;&#22270;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26088;&#22312;&#23398;&#20064;&#29992;&#20110;&#26377;&#25928;&#31867;&#21035;&#20998;&#37197;&#30340;&#22270;&#32423;&#34920;&#31034;&#65292;&#22312;&#24179;&#34913;&#30340;&#31867;&#21035;&#20998;&#24067;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#26480;&#20986;&#25104;&#26524;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#33258;&#28982;&#21576;&#29616;&#38271;&#23614;&#24418;&#24335;&#65292;&#20854;&#20013;&#22836;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#36828;&#36229;&#36807;&#23614;&#37096;&#31867;&#21035;&#65292;&#22240;&#27492;&#22312;&#38271;&#23614;&#25968;&#25454;&#19978;&#30740;&#31350;&#22270;&#32423;&#20998;&#31867;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#20013;&#30340;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#19988;&#24573;&#30053;&#20102;&#38590;&#20197;&#20998;&#31867;&#30340;&#31867;&#21035;&#30340;&#25366;&#25496;&#12290;&#30452;&#25509;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#22312;&#22270;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30001;&#20110;&#22797;&#26434;&#30340;&#25299;&#25169;&#29305;&#24449;&#20250;&#26356;&#21152;&#25935;&#24863;&#20110;&#38271;&#23614;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#38271;&#23614;&#22270;&#32423;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classifica
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#20204;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#20986;&#34892;&#25968;&#25454;&#65292;&#21457;&#29616;&#22478;&#24066;&#24418;&#24577;&#21464;&#37327;&#23545;&#20110;&#22478;&#24066;&#20869;&#20986;&#34892;&#26041;&#24335;&#20135;&#29983;&#20102;&#38388;&#25509;&#24433;&#21709;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19981;&#36275;&#65292;&#20026;&#23454;&#29616;&#20840;&#29699;&#21487;&#25345;&#32493;&#20132;&#36890;&#31995;&#32479;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.16599</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26041;&#27861;&#23398;&#20064;&#22478;&#24066;&#24418;&#24577;&#22914;&#20309;&#24433;&#21709;&#19981;&#21516;&#22823;&#27954;&#30340;&#21487;&#25345;&#32493;&#20986;&#34892;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
A Causal Discovery Approach To Learn How Urban Form Shapes Sustainable Mobility Across Continents. (arXiv:2308.16599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16599
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#20204;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#20986;&#34892;&#25968;&#25454;&#65292;&#21457;&#29616;&#22478;&#24066;&#24418;&#24577;&#21464;&#37327;&#23545;&#20110;&#22478;&#24066;&#20869;&#20986;&#34892;&#26041;&#24335;&#20135;&#29983;&#20102;&#38388;&#25509;&#24433;&#21709;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19981;&#36275;&#65292;&#20026;&#23454;&#29616;&#20840;&#29699;&#21487;&#25345;&#32493;&#20132;&#36890;&#31995;&#32479;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21487;&#25345;&#32493;&#21457;&#23637;&#38656;&#35201;&#20302;&#30899;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#65292;&#36825;&#38656;&#35201;&#36866;&#24403;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#20302;&#30899;&#20132;&#36890;&#26041;&#24335;&#30340;&#25512;&#24191;&#21644;&#20986;&#34892;&#34892;&#20026;&#30340;&#25913;&#21464;&#12290;&#20026;&#20102;&#27491;&#30830;&#23454;&#26045;&#22522;&#30784;&#35774;&#26045;&#21464;&#38761;&#65292;&#20102;&#35299;&#24314;&#31569;&#29615;&#22659;&#23545;&#20986;&#34892;&#30340;&#24433;&#21709;&#30340;&#22320;&#29702;&#29305;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#22312;&#34920;&#31034;6D&#22478;&#24066;&#24418;&#24577;&#21464;&#37327;&#21644;&#20986;&#34892;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12289;&#22312;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#20197;&#21450;&#22312;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#19979;&#24314;&#27169;&#22478;&#24066;&#24418;&#24577;&#25928;&#24212;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#21457;&#29616;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26681;&#25454;&#19977;&#20010;&#22823;&#27954;&#20845;&#20010;&#22478;&#24066;&#30340;&#39640;&#20998;&#36776;&#29575;&#20986;&#34892;&#25968;&#25454;&#65292;&#26816;&#27979;&#22478;&#24066;&#24418;&#24577;&#23545;&#22478;&#24066;&#20869;&#20986;&#34892;&#30340;&#24433;&#21709;&#65292;&#24182;&#22635;&#34917;&#20102;&#36825;&#19977;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36317;&#31163;&#24066;&#20013;&#24515;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#23494;&#24230;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#22478;&#24066;&#24418;&#24577;&#29305;&#24449;&#12290;&#36890;&#36807;&#32771;&#34385;&#36825;&#20123;&#22240;&#26524;&#20851;&#31995;&#65292;&#25105;&#20204;&#21457;&#29616;&#22320;&#29702;&#20301;&#32622;&#23545;&#19981;&#21516;&#22478;&#24066;&#20043;&#38388;&#30340;&#20132;&#36890;&#26041;&#24335;&#36873;&#25321;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global sustainability requires low-carbon urban transport systems, shaped by adequate infrastructure, deployment of low-carbon transport modes and shifts in travel behavior. To adequately implement alterations in infrastructure, it's essential to grasp the location-specific cause-and-effect mechanisms that the constructed environment has on travel. Yet, current research falls short in representing causal relationships between the 6D urban form variables and travel, generalizing across different regions, and modeling urban form effects at high spatial resolution. Here, we address all three gaps by utilizing a causal discovery and an explainable machine learning framework to detect urban form effects on intra-city travel based on high-resolution mobility data of six cities across three continents. We show that both distance to city center, demographics and density indirectly affect other urban form features. By considering the causal relationships, we find that location-specific influenc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32959;&#30244;&#20998;&#21106;&#20013;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#26368;&#20339;&#34917;&#19969;&#23610;&#23544;&#30340;&#36873;&#25321;&#12290;&#30446;&#21069;&#22522;&#20110;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#20027;&#27969;&#65292;&#20294;&#30001;&#20110;&#21367;&#31215;&#23618;&#30340;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#19981;&#33021;&#26377;&#25928;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#20294;&#30001;&#20110;&#23545;&#36755;&#20837;&#34917;&#19969;&#23610;&#23544;&#25935;&#24863;&#65292;&#20854;&#22312;&#19981;&#21516;&#32959;&#30244;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#26469;&#36873;&#25321;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26368;&#20339;&#36755;&#20837;&#22810;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.16598</link><description>&lt;p&gt;
&#20851;&#20110;&#32959;&#30244;&#20998;&#21106;&#20013;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#26368;&#20339;&#34917;&#19969;&#23610;&#23544;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation. (arXiv:2308.16598v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32959;&#30244;&#20998;&#21106;&#20013;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#26368;&#20339;&#34917;&#19969;&#23610;&#23544;&#30340;&#36873;&#25321;&#12290;&#30446;&#21069;&#22522;&#20110;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#20027;&#27969;&#65292;&#20294;&#30001;&#20110;&#21367;&#31215;&#23618;&#30340;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#19981;&#33021;&#26377;&#25928;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#20294;&#30001;&#20110;&#23545;&#36755;&#20837;&#34917;&#19969;&#23610;&#23544;&#25935;&#24863;&#65292;&#20854;&#22312;&#19981;&#21516;&#32959;&#30244;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#26469;&#36873;&#25321;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26368;&#20339;&#36755;&#20837;&#22810;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32925;&#30284;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#65292;&#36716;&#31227;&#24615;&#32467;&#30452;&#32928;&#30284;&#65288;mCRC&#65289;&#20013;&#30340;&#32959;&#30244;&#26816;&#27979;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20197;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;FCNN&#65289;&#20026;&#20027;&#24178;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20998;&#21106;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#30340;&#20027;&#35201;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21367;&#31215;&#23618;&#30340;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#19981;&#33021;&#25429;&#25417;&#21040;&#36828;&#36317;&#31163;&#30340;&#20381;&#36182;&#21644;&#20840;&#23616;&#35821;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#26469;&#35299;&#20915;FCNN&#30340;&#25509;&#21463;&#22495;&#30340;&#23616;&#37096;&#24615;&#12290;&#23613;&#31649;&#21464;&#25442;&#22120;&#21487;&#20197;&#25429;&#25417;&#21040;&#36828;&#36317;&#31163;&#30340;&#29305;&#24449;&#65292;&#20294;&#30001;&#20110;&#23545;&#36755;&#20837;&#34917;&#19969;&#22823;&#23567;&#30340;&#25935;&#24863;&#24615;&#65292;&#20854;&#20998;&#21106;&#24615;&#33021;&#22312;&#21508;&#31181;&#32959;&#30244;&#22823;&#23567;&#19978;&#37117;&#26377;&#25152;&#19979;&#38477;&#12290;&#34429;&#28982;&#25214;&#21040;&#26368;&#20339;&#34917;&#19969;&#23610;&#23544;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#22312;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#35270;&#35273;&#21464;&#25442;&#22120;&#26368;&#20339;&#36755;&#20837;&#22810;&#20998;&#36776;&#29575;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Detection of tumors in metastatic colorectal cancer (mCRC) plays an essential role in the early diagnosis and treatment of liver cancer. Deep learning models backboned by fully convolutional neural networks (FCNNs) have become the dominant model for segmenting 3D computerized tomography (CT) scans. However, since their convolution layers suffer from limited kernel size, they are not able to capture long-range dependencies and global context. To tackle this restriction, vision transformers have been introduced to solve FCNN's locality of receptive fields. Although transformers can capture long-range features, their segmentation performance decreases with various tumor sizes due to the model sensitivity to the input patch size. While finding an optimal patch size improves the performance of vision transformer-based models on segmentation tasks, it is a time-consuming and challenging procedure. This paper proposes a technique to select the vision transformer's optimal input multi-resoluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#33258;&#21457;&#39118;&#26684;&#35821;&#38899;&#21644;&#33258;&#21457;&#34892;&#20026;&#26631;&#31614;&#30340;&#25968;&#37327;&#65292;&#20197;&#23454;&#29616;&#33258;&#21457;&#39118;&#26684;&#24314;&#27169;&#29992;&#20110;&#23545;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#33258;&#21457;&#39118;&#26684;&#30340;&#35821;&#38899;&#20013;&#24314;&#27169;&#33258;&#21457;&#34892;&#20026;&#65292;&#24182;&#20174;&#25991;&#26412;&#20013;&#39044;&#27979;&#21512;&#29702;&#30340;&#33258;&#21457;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.16593</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#33258;&#21457;&#39118;&#26684;&#24314;&#27169;&#29992;&#20110;&#23545;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis. (arXiv:2308.16593v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#33258;&#21457;&#39118;&#26684;&#35821;&#38899;&#21644;&#33258;&#21457;&#34892;&#20026;&#26631;&#31614;&#30340;&#25968;&#37327;&#65292;&#20197;&#23454;&#29616;&#33258;&#21457;&#39118;&#26684;&#24314;&#27169;&#29992;&#20110;&#23545;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#33258;&#21457;&#39118;&#26684;&#30340;&#35821;&#38899;&#20013;&#24314;&#27169;&#33258;&#21457;&#34892;&#20026;&#65292;&#24182;&#20174;&#25991;&#26412;&#20013;&#39044;&#27979;&#21512;&#29702;&#30340;&#33258;&#21457;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#32463;&#24120;&#21457;&#29983;&#30340;&#33258;&#21457;&#34892;&#20026;&#20351;&#24471;&#35821;&#38899;&#21548;&#36215;&#26469;&#26356;&#21152;&#20687;&#20154;&#31867;&#65292;&#32780;&#19981;&#26159;&#20687;&#26391;&#35835;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#33258;&#21457;&#39118;&#26684;&#30340;&#35821;&#38899;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#33258;&#21457;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26631;&#35760;&#33258;&#21457;&#34892;&#20026;&#30340;&#25104;&#26412;&#36739;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#33258;&#21457;&#39118;&#26684;&#35821;&#38899;&#21644;&#33258;&#21457;&#34892;&#20026;&#26631;&#31614;&#30340;&#25968;&#37327;&#12290;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#65292;&#32771;&#34385;&#20102;&#25991;&#26412;&#21644;&#35821;&#38899;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;&#35821;&#38899;&#20013;&#26816;&#27979;&#33258;&#21457;&#34892;&#20026;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#35821;&#35328;&#24863;&#30693;&#32534;&#30721;&#22120;&#26469;&#24314;&#27169;&#23545;&#35805;&#20013;&#27599;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#34920;&#36798;&#24615;&#35821;&#38899;&#21512;&#25104;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#33021;&#22815;&#22312;&#33258;&#21457;&#39118;&#26684;&#30340;&#35821;&#38899;&#20013;&#24314;&#27169;&#33258;&#21457;&#34892;&#20026;&#65292;&#24182;&#20174;&#25991;&#26412;&#20013;&#39044;&#27979;&#21512;&#29702;&#30340;&#33258;&#21457;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spontaneous behavior that often occurs in conversations makes speech more human-like compared to reading-style. However, synthesizing spontaneous-style speech is challenging due to the lack of high-quality spontaneous datasets and the high cost of labeling spontaneous behavior. In this paper, we propose a semi-supervised pre-training method to increase the amount of spontaneous-style speech and spontaneous behavioral labels. In the process of semi-supervised learning, both text and speech information are considered for detecting spontaneous behaviors labels in speech. Moreover, a linguistic-aware encoder is used to model the relationship between each sentence in the conversation. Experimental results indicate that our proposed method achieves superior expressive speech synthesis performance with the ability to model spontaneous behavior in spontaneous-style speech and predict reasonable spontaneous behavior from text.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#20943;&#37325;&#25163;&#26415;&#21518;5&#24180;&#30340;&#20307;&#37325;&#21464;&#21270;&#36712;&#36857;&#65292;&#20026;&#20010;&#20307;&#26415;&#21069;&#25552;&#20379;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.16585</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#39564;&#35777;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20943;&#37325;&#25163;&#26415;&#21518;5&#24180;&#30340;&#20307;&#37325;&#21464;&#21270;&#36712;&#36857;&#65306;&#19968;&#39033;&#36328;&#22269;&#22238;&#39038;&#24615;&#38431;&#21015;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Development and validation of an interpretable machine learning-based calculator for predicting 5-year weight trajectories after bariatric surgery: a multinational retrospective cohort SOPHIA study. (arXiv:2308.16585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16585
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#20943;&#37325;&#25163;&#26415;&#21518;5&#24180;&#30340;&#20307;&#37325;&#21464;&#21270;&#36712;&#36857;&#65292;&#20026;&#20010;&#20307;&#26415;&#21069;&#25552;&#20379;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20943;&#37325;&#25163;&#26415;&#21518;&#30340;&#20307;&#37325;&#21464;&#21270;&#36712;&#36857;&#22240;&#20154;&#32780;&#24322;&#65292;&#39044;&#27979;&#25163;&#26415;&#21069;&#30340;&#20943;&#37325;&#24773;&#20917;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#25163;&#26415;&#21518;5&#24180;&#30340;&#20307;&#37325;&#21464;&#21270;&#36712;&#36857;&#25552;&#20379;&#20010;&#20307;&#21270;&#30340;&#26415;&#21069;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. Methods In this multinational retrospective observational study we enrolled adult participants (aged $\ge$18 years) from ten prospective cohorts (including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse Obesitas Kliniek]) and two randomised trials (SleevePass [NCT00793143] and SM-BOSS [NCT00356213]) in Europe, the Americas, and Asia, with a 5 year followup after Roux-en-Y gastric bypass, sleeve gastrectomy, or gastric band. Patients with a previous history of bariatric surgery or large delays between scheduled and actual visits were excluded. The training cohort comprised patients from two ce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.16572</link><description>&lt;p&gt;
CL-MAE: &#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#29992;&#20110;&#29983;&#25104;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#36890;&#24120;&#65292;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#38543;&#26426;&#36974;&#32617;&#34917;&#19969;&#65288;&#26631;&#35760;&#65289;&#65292;&#24182;&#19988;&#36974;&#32617;&#31574;&#30053;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#20197;&#25345;&#32493;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#36974;&#32617;&#27169;&#22359;&#65292;&#20855;&#26377;&#29983;&#25104;&#19981;&#21516;&#22797;&#26434;&#24230;&#36974;&#32617;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27169;&#22359;&#19982;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#19982;MAE&#19968;&#21516;&#35757;&#32451;&#65292;&#21516;&#26102;&#35843;&#25972;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;MAE&#30340;&#21442;&#19982;&#32773;&#36807;&#28193;&#21040;MAE&#65288;&#20248;&#21270;&#30456;&#21516;&#30340;&#37325;&#26500;&#30446;&#26631;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same rec
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;MViTv2&#27169;&#22411;&#22312;BaDLAD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#26694;&#12289;&#27573;&#33853;&#12289;&#22270;&#29255;&#21644;&#34920;&#26684;&#30340;&#33258;&#21160;&#25552;&#21462;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#25928;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.16571</link><description>&lt;p&gt;
&#22312;BaDLAD&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65306;&#19968;&#31181;&#22522;&#20110;MViTv2&#30340;&#32508;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Document Layout Analysis on BaDLAD Dataset: A Comprehensive MViTv2 Based Approach. (arXiv:2308.16571v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16571
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;MViTv2&#27169;&#22411;&#22312;BaDLAD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#26694;&#12289;&#27573;&#33853;&#12289;&#22270;&#29255;&#21644;&#34920;&#26684;&#30340;&#33258;&#21160;&#25552;&#21462;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#25928;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#25968;&#23383;&#26102;&#20195;&#65292;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#22312;&#33258;&#21160;&#21270;&#20449;&#24687;&#25552;&#21462;&#21644;&#35299;&#37322;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#32852;&#25513;&#30721;R-CNN&#35757;&#32451;&#20102;MViTv2 transformer&#27169;&#22411;&#26550;&#26500;&#65292;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#25991;&#26412;&#26694;&#12289;&#27573;&#33853;&#12289;&#22270;&#29255;&#21644;&#34920;&#26684;&#12290;&#22312;&#19968;&#20010;3&#38454;&#27573;&#24490;&#29615;&#20013;&#65292;&#25105;&#20204;&#22312;20365&#20010;&#25991;&#26723;&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;36&#20010;&#21608;&#26399;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;0.2125&#30340;&#35757;&#32451;&#25439;&#22833;&#21644;0.19&#30340;&#25513;&#30721;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#38480;&#20110;&#35757;&#32451;&#65292;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#28508;&#22312;&#30340;&#22686;&#24378;&#36884;&#24452;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#26059;&#36716;&#21644;&#32763;&#36716;&#22686;&#24378;&#30340;&#24433;&#21709;&#65292;&#20999;&#29255;&#36755;&#20837;&#22270;&#20687;&#39044;&#25512;&#35770;&#30340;&#26377;&#25928;&#24615;&#65292;&#21464;&#21270;&#30340;&#21464;&#25442;&#22120;&#39592;&#24178;&#20998;&#36776;&#29575;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#37319;&#29992;&#21452;&#36890;&#25512;&#35770;&#26469;&#21457;&#29616;&#28431;&#25481;&#30340;&#25991;&#26412;&#26694;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#36825;&#20123;&#25506;&#32034;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#31995;&#21015;&#32467;&#26524;&#65292;&#19968;&#20123;&#20462;&#25913;&#23548;&#33268;&#20102;&#26377;&#24418;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#32780;&#20854;&#20182;&#20462;&#25913;&#21017;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving digital era, the analysis of document layouts plays a pivotal role in automated information extraction and interpretation. In our work, we have trained MViTv2 transformer model architecture with cascaded mask R-CNN on BaDLAD dataset to extract text box, paragraphs, images and tables from a document. After training on 20365 document images for 36 epochs in a 3 phase cycle, we achieved a training loss of 0.2125 and a mask loss of 0.19. Our work extends beyond training, delving into the exploration of potential enhancement avenues. We investigate the impact of rotation and flip augmentation, the effectiveness of slicing input images pre-inference, the implications of varying the resolution of the transformer backbone, and the potential of employing a dual-pass inference to uncover missed text-boxes. Through these explorations, we observe a spectrum of outcomes, where some modifications result in tangible performance improvements, while others offer unique insights 
&lt;/p&gt;</description></item><item><title>MONDEO&#26159;&#19968;&#31181;&#22810;&#38454;&#27573;&#26426;&#21046;&#65292;&#29992;&#20110;&#26816;&#27979;&#22522;&#20110;DNS&#30340;&#20725;&#23608;&#32593;&#32476;&#24694;&#24847;&#36719;&#20214;&#65292;&#23427;&#36731;&#24039;&#19988;&#21487;&#20197;&#22312;&#25163;&#26426;&#35774;&#22791;&#20013;&#37096;&#32626;&#65292;&#36890;&#36807;&#22788;&#29702;&#25968;&#25454;&#21253;&#27969;&#36827;&#34892;&#39640;&#25928;&#30340;&#25915;&#20987;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.16570</link><description>&lt;p&gt;
MONDEO: &#22810;&#38454;&#27573;&#20725;&#23608;&#32593;&#32476;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MONDEO: Multistage Botnet Detection. (arXiv:2308.16570v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16570
&lt;/p&gt;
&lt;p&gt;
MONDEO&#26159;&#19968;&#31181;&#22810;&#38454;&#27573;&#26426;&#21046;&#65292;&#29992;&#20110;&#26816;&#27979;&#22522;&#20110;DNS&#30340;&#20725;&#23608;&#32593;&#32476;&#24694;&#24847;&#36719;&#20214;&#65292;&#23427;&#36731;&#24039;&#19988;&#21487;&#20197;&#22312;&#25163;&#26426;&#35774;&#22791;&#20013;&#37096;&#32626;&#65292;&#36890;&#36807;&#22788;&#29702;&#25968;&#25454;&#21253;&#27969;&#36827;&#34892;&#39640;&#25928;&#30340;&#25915;&#20987;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#35774;&#22791;&#24050;&#24191;&#27867;&#25104;&#20026;&#26368;&#24120;&#29992;&#30340;&#25216;&#26415;&#35774;&#22791;&#12290;&#30001;&#20110;&#20854;&#29305;&#24615;&#65292;&#23427;&#20204;&#25104;&#20026;&#20725;&#23608;&#32593;&#32476;&#24694;&#24847;&#36719;&#20214;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;FluBot&#26159;&#19968;&#31181;&#24863;&#26579;&#31227;&#21160;&#35774;&#22791;&#30340;&#20505;&#36873;&#20725;&#23608;&#32593;&#24694;&#24847;&#36719;&#20214;&#30340;&#20363;&#23376;&#12290;&#29305;&#21035;&#26159;&#65292;FluBot&#26159;&#19968;&#31181;&#22522;&#20110;DNS&#30340;&#20725;&#23608;&#32593;&#32476;&#65292;&#20351;&#29992;&#22495;&#21517;&#29983;&#25104;&#31639;&#27861;(DGA)&#19982;&#21629;&#20196;&#21644;&#25511;&#21046;&#26381;&#21153;&#22120;(C2)&#36827;&#34892;&#36890;&#20449;&#12290;MONDEO&#26159;&#19968;&#31181;&#22810;&#38454;&#27573;&#26426;&#21046;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#35774;&#35745;&#65292;&#29992;&#20110;&#26816;&#27979;&#22522;&#20110;DNS&#30340;&#20725;&#23608;&#32593;&#32476;&#24694;&#24847;&#36719;&#20214;&#12290;MONDEO&#36731;&#24039;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25163;&#26426;&#35774;&#22791;&#20013;&#37096;&#32626;&#32780;&#19981;&#38656;&#35201;&#37096;&#32626;&#36719;&#20214;&#12289;&#20195;&#29702;&#25110;&#37197;&#32622;&#65292;&#26041;&#20415;&#22320;&#38598;&#25104;&#21040;&#26680;&#24515;&#32593;&#32476;&#20013;&#12290;MONDEO&#21253;&#25324;&#22235;&#20010;&#26816;&#27979;&#38454;&#27573;&#65306;&#40657;&#21517;&#21333;/&#30333;&#21517;&#21333;&#65292;&#26597;&#35810;&#29575;&#20998;&#26512;&#65292; DGA&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#12290;&#23427;&#34987;&#35774;&#35745;&#29992;&#26469;&#22788;&#29702;&#25968;&#25454;&#21253;&#27969;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22312;&#19981;&#21516;&#38454;&#27573;&#20013;&#30340;&#25915;&#20987;&#12290;MONDEO&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#20197;&#34913;&#37327;&#20854;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile devices have widespread to become the most used piece of technology. Due to their characteristics, they have become major targets for botnet-related malware. FluBot is one example of botnet malware that infects mobile devices. In particular, FluBot is a DNS-based botnet that uses Domain Generation Algorithms (DGA) to establish communication with the Command and Control Server (C2). MONDEO is a multistage mechanism with a flexible design to detect DNS-based botnet malware. MONDEO is lightweight and can be deployed without requiring the deployment of software, agents, or configuration in mobile devices, allowing easy integration in core networks. MONDEO comprises four detection stages: Blacklisting/Whitelisting, Query rate analysis, DGA analysis, and Machine learning evaluation. It was created with the goal of processing streams of packets to identify attacks with high efficiency, in the distinct phases. MONDEO was tested against several datasets to measure its efficiency and perf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22810;&#21464;&#37327;&#36755;&#20837;&#26469;&#39044;&#27979;&#24613;&#35786;&#23460;&#25317;&#25380;&#65292;&#21457;&#29616;N-BEATS&#21644;LightGBM&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#20998;&#21035;&#25552;&#20379;&#20102;11%&#21644;9%&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.16544</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22810;&#21464;&#37327;&#36755;&#20837;&#39044;&#27979;&#24613;&#35786;&#23460;&#25317;&#25380;
&lt;/p&gt;
&lt;p&gt;
Forecasting Emergency Department Crowding with Advanced Machine Learning Models and Multivariable Input. (arXiv:2308.16544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22810;&#21464;&#37327;&#36755;&#20837;&#26469;&#39044;&#27979;&#24613;&#35786;&#23460;&#25317;&#25380;&#65292;&#21457;&#29616;N-BEATS&#21644;LightGBM&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#20998;&#21035;&#25552;&#20379;&#20102;11%&#21644;9%&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#35786;&#23460;&#25317;&#25380;&#23545;&#24739;&#32773;&#30340;&#23433;&#20840;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#24182;&#19988;&#19982;&#22686;&#21152;&#30340;&#27515;&#20129;&#29575;&#26377;&#20851;&#12290;&#39044;&#27979;&#26410;&#26469;&#30340;&#26381;&#21153;&#38656;&#27714;&#26377;&#28508;&#22312;&#30340;&#24739;&#32773;&#32467;&#26524;&#12290;&#23613;&#31649;&#23545;&#36825;&#20010;&#20027;&#39064;&#36827;&#34892;&#20102;&#31215;&#26497;&#30340;&#30740;&#31350;&#65292;&#20294;&#20173;&#23384;&#22312;&#20960;&#20010;&#24046;&#36317;&#65306;1&#65289;&#30001;&#20110;&#24555;&#36895;&#22686;&#21152;&#30340;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;ML&#65289;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#27169;&#22411;&#21464;&#24471;&#36807;&#26102;&#65292;2&#65289;&#22810;&#21464;&#37327;&#36755;&#20837;&#25968;&#25454;&#30340;&#37327;&#26377;&#38480;&#65292;3&#65289;&#24456;&#23569;&#25253;&#21578;&#20855;&#20307;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#19968;&#32452;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#22312;&#39044;&#27979;24&#23567;&#26102;&#21069;&#30340;&#24613;&#35786;&#23460;&#21344;&#29992;&#29575;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#32508;&#21512;&#24613;&#35786;&#23460;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#21644;&#19968;&#31995;&#21015;&#35299;&#37322;&#21464;&#37327;&#65292;&#21253;&#25324;&#25937;&#27835;&#21306;&#22495;&#21307;&#38498;&#30340;&#24202;&#20301;&#21487;&#29992;&#24615;&#65292;&#26469;&#33258;&#24403;&#22320;&#35266;&#27979;&#31449;&#30340;&#20132;&#36890;&#25968;&#25454;&#65292;&#22825;&#27668;&#21464;&#37327;&#31561;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;N-BEATS&#21644;LightGBM&#22312;11&#65285;&#21644;9&#65285;&#30340;&#25913;&#36827;&#20013;&#36229;&#36234;&#20102;&#22522;&#20934;&#65292;&#24182;&#19988;DeepAR&#21487;&#20197;&#39044;&#27979;&#31532;&#20108;&#22825;&#30340;&#20154;&#21592;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergency department (ED) crowding is a significant threat to patient safety and it has been repeatedly associated with increased mortality. Forecasting future service demand has the potential patient outcomes. Despite active research on the subject, several gaps remain: 1) proposed forecasting models have become outdated due to quick influx of advanced machine learning models (ML), 2) amount of multivariable input data has been limited and 3) discrete performance metrics have been rarely reported. In this study, we document the performance of a set of advanced ML models in forecasting ED occupancy 24 hours ahead. We use electronic health record data from a large, combined ED with an extensive set of explanatory variables, including the availability of beds in catchment area hospitals, traffic data from local observation stations, weather variables, etc. We show that N-BEATS and LightGBM outpeform benchmarks with 11 % and 9 % respective improvements and that DeepAR predicts next day cr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#38170;&#28857;&#19981;&#23545;&#40784;&#38382;&#39064;&#21644;&#35270;&#22270;&#38388;&#30340;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#32858;&#31867;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.16541</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#19982;&#32467;&#26500;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Scalable Incomplete Multi-View Clustering with Structure Alignment. (arXiv:2308.16541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16541
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#38170;&#28857;&#19981;&#23545;&#40784;&#38382;&#39064;&#21644;&#35270;&#22270;&#38388;&#30340;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#32858;&#31867;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#20551;&#35774;&#25152;&#26377;&#35270;&#22270;&#37117;&#26159;&#23436;&#25972;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#30001;&#20110;&#25968;&#25454;&#25439;&#22351;&#25110;&#20256;&#24863;&#22120;&#25925;&#38556;&#65292;&#26679;&#26412;&#36890;&#24120;&#26159;&#37096;&#20998;&#21487;&#29992;&#30340;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#30740;&#31350;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#38170;&#28857;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#19981;&#23436;&#25972;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#20197;&#19979;&#32570;&#28857;&#65306;i&#65289;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#35270;&#22270;&#38388;&#30340;&#24046;&#24322;&#65292;&#24378;&#21046;&#35201;&#27714;&#36328;&#35270;&#22270;&#34920;&#31034;&#19968;&#33268;&#65292;&#36825;&#20250;&#30772;&#22351;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65307;ii&#65289;&#30001;&#20110;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#26679;&#26412;&#30340;&#24046;&#24322;&#65292;&#23398;&#20064;&#21040;&#30340;&#38170;&#28857;&#21487;&#33021;&#19981;&#23545;&#40784;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#19981;&#23436;&#25972;&#25968;&#25454;&#19979;&#30340;&#38170;&#28857;&#19981;&#23545;&#40784;&#38382;&#39064;&#65288;AUP-ID&#65289;&#12290;&#36825;&#31181;AUP-ID&#20250;&#23548;&#33268;&#22270;&#34701;&#21512;&#19981;&#20934;&#30830;&#65292;&#38477;&#20302;&#32858;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#23436;&#25972;&#38170;&#28857;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#21487;&#25193;&#23637;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#19982;&#32467;&#26500;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of existing multi-view clustering (MVC) relies on the assumption that all views are complete. However, samples are usually partially available due to data corruption or sensor malfunction, which raises the research of incomplete multi-view clustering (IMVC). Although several anchor-based IMVC methods have been proposed to process the large-scale incomplete data, they still suffer from the following drawbacks: i) Most existing approaches neglect the inter-view discrepancy and enforce cross-view representation to be consistent, which would corrupt the representation capability of the model; ii) Due to the samples disparity between different views, the learned anchor might be misaligned, which we referred as the Anchor-Unaligned Problem for Incomplete data (AUP-ID). Such the AUP-ID would cause inaccurate graph fusion and degrades clustering performance. To tackle these issues, we propose a novel incomplete anchor graph learning framework termed Scalable Incomplete Multi-View C
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16539</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions. (arXiv:2308.16539v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#26234;&#33021;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#21338;&#24328;&#35770;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#22810;&#20010;&#25361;&#25112;&#30340;&#38459;&#30861;&#65292;&#27604;&#22914;&#26410;&#30693;&#30340;&#26234;&#33021;&#20307;&#20559;&#22909;&#21644;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29616;&#26377;&#26041;&#27861;&#32479;&#19968;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#24418;&#24335;&#21270;&#20013;&#12290;&#22312;&#36825;&#20010;&#24418;&#24335;&#21270;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21338;&#24328;&#21442;&#25968;&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#12290;&#20351;&#29992;&#27169;&#25311;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#34892;&#20154;&#30456;&#20114;&#20316;&#29992;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21338;&#24328;&#35770;&#23618;&#25913;&#21892;&#20102;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20027;&#24178;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory offers an interpretable mathematical framework for modeling multi-agent interactions. However, its applicability in real-world robotics applications is hindered by several challenges, such as unknown agents' preferences and goals. To address these challenges, we show a connection between differential games, optimal control, and energy-based models and demonstrate how existing approaches can be unified under our proposed Energy-based Potential Game formulation. Building upon this formulation, this work introduces a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, acting as an inductive bias. The experiments using simulated mobile robot pedestrian interactions and real-world automated driving data provide empirical evidence that the game-theoretic layer improves the predictive performance of various neural network backbones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16534</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#21644;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26465;&#20214;&#21644;&#38750;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26465;&#20214;&#29983;&#25104;&#22522;&#20110;&#29305;&#23450;&#35757;&#32451;&#30340;&#26465;&#20214;&#27169;&#22411;&#25110;&#20998;&#31867;&#22120;&#25351;&#23548;&#65292;&#36825;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#22122;&#22768;&#20381;&#36182;&#30340;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#23545;&#20110;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#32473;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#26465;&#20214;&#35780;&#20998;&#29983;&#25104;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25805;&#32437;&#23398;&#20064;&#24471;&#21040;&#30340;&#35780;&#20998;&#65292;&#20197;&#20415;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#20174;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#32780;&#25968;&#20540;&#31283;&#23450;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#36719;&#36923;&#36753;&#32422;&#26463;&#12290;&#23558;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#20294;&#26159;&#36817;&#20284;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25913;&#36827;&#36817;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based and diffusion models have emerged as effective approaches for both conditional and unconditional generation. Still conditional generation is based on either a specific training of a conditional model or classifier guidance, which requires training a noise-dependent classifier, even when the classifier for uncorrupted data is given. We propose an approach to sample from unconditional score-based generative models enforcing arbitrary logical constraints, without any additional training. Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint. Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints. Combining these two ingredients we obtain a general, but approximate, conditional sampling algorithm. We further developed effective heuristics aimed at improving the approximation. Finally, we show the effectiveness of our approach fo
&lt;/p&gt;</description></item><item><title>SA6D&#26159;&#19968;&#31181;&#38754;&#21521;&#26032;&#39062;&#21644;&#36974;&#25377;&#29289;&#20307;&#30340;&#33258;&#36866;&#24212;&#23569;&#26679;&#26412;6D&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20998;&#21106;&#27169;&#22359;&#21644;&#23569;&#37327;&#21442;&#32771;&#22270;&#20687;&#26500;&#24314;&#30446;&#26631;&#23545;&#35937;&#30340;&#28857;&#20113;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#23545;&#35937;&#20449;&#24687;&#65292;&#33021;&#22312;&#20855;&#26377;&#36974;&#25377;&#30340;&#22797;&#26434;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16528</link><description>&lt;p&gt;
SA6D: &#38754;&#21521;&#26032;&#39062;&#21644;&#36974;&#25377;&#29289;&#20307;&#30340;&#33258;&#36866;&#24212;&#23569;&#26679;&#26412;6D&#23039;&#24577;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
SA6D: Self-Adaptive Few-Shot 6D Pose Estimator for Novel and Occluded Objects. (arXiv:2308.16528v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16528
&lt;/p&gt;
&lt;p&gt;
SA6D&#26159;&#19968;&#31181;&#38754;&#21521;&#26032;&#39062;&#21644;&#36974;&#25377;&#29289;&#20307;&#30340;&#33258;&#36866;&#24212;&#23569;&#26679;&#26412;6D&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20998;&#21106;&#27169;&#22359;&#21644;&#23569;&#37327;&#21442;&#32771;&#22270;&#20687;&#26500;&#24314;&#30446;&#26631;&#23545;&#35937;&#30340;&#28857;&#20113;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#23545;&#35937;&#20449;&#24687;&#65292;&#33021;&#22312;&#20855;&#26377;&#36974;&#25377;&#30340;&#22797;&#26434;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23454;&#29616;&#23545;&#29289;&#20307;&#30340;&#26377;&#24847;&#20041;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;6D&#23039;&#24577;&#20272;&#35745;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#26041;&#38754;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#23558;&#39044;&#27979;&#25193;&#23637;&#21040;&#19981;&#26029;&#24341;&#20837;&#26032;&#29289;&#20307;&#23454;&#20363;&#30340;&#22330;&#26223;&#65292;&#29305;&#21035;&#26159;&#37325;&#24230;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SA6D&#30340;&#23569;&#26679;&#26412;&#23039;&#24577;&#20272;&#35745;&#65288;FSPE&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#36866;&#24212;&#20998;&#21106;&#27169;&#22359;&#35782;&#21035;&#26032;&#39062;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#20351;&#29992;&#20165;&#26377;&#23569;&#37327;&#28151;&#26434;&#21442;&#32771;&#22270;&#20687;&#26500;&#24314;&#30446;&#26631;&#23545;&#35937;&#30340;&#28857;&#20113;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;SA6D&#19981;&#38656;&#35201;&#38754;&#21521;&#23545;&#35937;&#30340;&#21442;&#32771;&#22270;&#20687;&#25110;&#20219;&#20309;&#20854;&#20182;&#23545;&#35937;&#20449;&#24687;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#26356;&#20855;&#36890;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enable meaningful robotic manipulation of objects in the real-world, 6D pose estimation is one of the critical aspects. Most existing approaches have difficulties to extend predictions to scenarios where novel object instances are continuously introduced, especially with heavy occlusions. In this work, we propose a few-shot pose estimation (FSPE) approach called SA6D, which uses a self-adaptive segmentation module to identify the novel target object and construct a point cloud model of the target object using only a small number of cluttered reference images. Unlike existing methods, SA6D does not require object-centric reference images or any additional object information, making it a more generalizable and scalable solution across categories. We evaluate SA6D on real-world tabletop object datasets and demonstrate that SA6D outperforms existing FSPE methods, particularly in cluttered scenes with occlusions, while requiring fewer reference images.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CurvPool&#30340;&#27719;&#32858;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#26354;&#29575;&#27010;&#24565;&#26469;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#26354;&#29575;&#33258;&#36866;&#24212;&#22320;&#35782;&#21035;&#36127;&#36131;&#36825;&#20004;&#31181;&#29616;&#35937;&#30340;&#32467;&#26500;&#65292;&#24182;&#26500;&#24314;&#20855;&#26377;&#26356;&#21512;&#36866;&#32467;&#26500;&#30340;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#28145;&#23618;&#27169;&#22411;&#21644;&#36828;&#36317;&#31163;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2308.16516</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27719;&#32858;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Curvature-based Pooling within Graph Neural Networks. (arXiv:2308.16516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CurvPool&#30340;&#27719;&#32858;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#26354;&#29575;&#27010;&#24565;&#26469;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#26354;&#29575;&#33258;&#36866;&#24212;&#22320;&#35782;&#21035;&#36127;&#36131;&#36825;&#20004;&#31181;&#29616;&#35937;&#30340;&#32467;&#26500;&#65292;&#24182;&#26500;&#24314;&#20855;&#26377;&#26356;&#21512;&#36866;&#32467;&#26500;&#30340;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#28145;&#23618;&#27169;&#22411;&#21644;&#36828;&#36317;&#31163;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#21644;&#36807;&#24230;&#24179;&#28369;&#26159;&#38480;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#33021;&#21147;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36807;&#24230;&#24179;&#28369;&#28040;&#38500;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#32780;&#36807;&#24230;&#21387;&#32553;&#25351;&#30340;&#26159;GNN&#26080;&#27861;&#22312;&#36739;&#38271;&#30340;&#36317;&#31163;&#19978;&#20256;&#25773;&#20449;&#24687;&#65292;&#22240;&#20026;&#25351;&#25968;&#32423;&#30340;&#33410;&#28857;&#29366;&#24577;&#34987;&#21387;&#32553;&#25104;&#22266;&#23450;&#22823;&#23567;&#30340;&#34920;&#31034;&#12290;&#36825;&#20004;&#31181;&#29616;&#35937;&#20855;&#26377;&#31867;&#20284;&#30340;&#21407;&#22240;&#65292;&#37117;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#22270;&#25299;&#25169;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27719;&#32858;&#26041;&#27861;CurvPool&#12290;CurvPool&#21033;&#29992;&#22270;&#30340;&#26354;&#29575;&#27010;&#24565;&#33258;&#36866;&#24212;&#22320;&#35782;&#21035;&#36127;&#36131;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#22522;&#20110;&#24179;&#34913;Forman&#26354;&#29575;&#23545;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;CurvPool&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#26356;&#21512;&#36866;&#32467;&#26500;&#30340;&#22270;&#65292;&#20801;&#35768;&#28145;&#23618;&#27169;&#22411;&#21644;&#36828;&#36317;&#31163;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27719;&#32858;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#30830;&#23450;&#20854;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-squashing and over-smoothing are two critical issues, that limit the capabilities of graph neural networks (GNNs). While over-smoothing eliminates the differences between nodes making them indistinguishable, over-squashing refers to the inability of GNNs to propagate information over long distances, as exponentially many node states are squashed into fixed-size representations. Both phenomena share similar causes, as both are largely induced by the graph topology. To mitigate these problems in graph classification tasks, we propose CurvPool, a novel pooling method. CurvPool exploits the notion of curvature of a graph to adaptively identify structures responsible for both over-smoothing and over-squashing. By clustering nodes based on the Balanced Forman curvature, CurvPool constructs a graph with a more suitable structure, allowing deeper models and the combination of distant information. We compare it to other state-of-the-art pooling approaches and establish its competitiveness 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.16491</link><description>&lt;p&gt;
&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65306;&#25945;&#23398;&#29983;&#65292;&#21516;&#26102;&#27979;&#35797;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#27491;&#38754;&#20020;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#32435;&#20837;&#35838;&#22530;&#20316;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#19968;&#26041;&#27861;&#26159;&#21542;&#21487;&#34892;&#65292;&#22914;&#26524;&#21487;&#34892;&#65292;&#28041;&#21450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;-&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;-&#24212;&#35813;&#26399;&#26395;&#20160;&#20040;&#12290;&#23398;&#29983;&#33021;&#22815;&#22312;&#35838;&#22530;&#19978;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#21527;&#65311;&#25945;&#32946;&#32773;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#22914;&#20309;&#65311;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#22914;&#20309;&#24110;&#21161;&#35780;&#20272;&#21644;&#25913;&#36827;&#31185;&#23398;&#30340;&#29616;&#29366;&#65311;&#26412;&#30740;&#31350;&#22312;EPFL&#25945;&#25480;&#30340;&#24212;&#29992;&#25968;&#25454;&#20998;&#26512;&#35838;&#31243;&#65288;CS-401&#65289;&#30340;&#39033;&#30446;&#37096;&#20998;&#20013;&#32435;&#20837;&#20102;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65288;N=354&#21517;&#23398;&#29983;&#65289;&#12290;&#22312;&#27492;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#35838;&#31243;&#26399;&#38388;&#36827;&#34892;&#30340;&#35843;&#26597;&#25552;&#21069;&#36827;&#34892;&#27880;&#20876;&#30340;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#23398;&#29983;&#21487;&#20197;&#22797;&#21046;&#20808;&#21069;&#21457;&#34920;&#30340;&#31185;&#23398;&#35770;&#25991;&#65292;&#22823;&#37096;&#20998;&#26159;&#23450;&#24615;&#30340;&#65292;&#26377;&#20123;&#26159;&#23436;&#20840;&#19968;&#26679;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?  In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data an
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30011;&#23478;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#21644;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#26469;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26816;&#26597;&#28857;&#38598;&#20013;&#36716;&#25442;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.16490</link><description>&lt;p&gt;
&#28508;&#22312;&#30011;&#23478;
&lt;/p&gt;
&lt;p&gt;
Latent Painter. (arXiv:2308.16490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30011;&#23478;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#21644;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#26469;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26816;&#26597;&#28857;&#38598;&#20013;&#36716;&#25442;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#22120;&#22312;&#29983;&#25104;AI&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#24182;&#28608;&#21457;&#20102;&#21019;&#36896;&#24615;&#33402;&#26415;&#12290;&#22312;&#21435;&#22122;&#28508;&#22312;&#26102;&#65292;&#27599;&#20010;&#27493;&#39588;&#39044;&#27979;&#30340;&#21407;&#22987;&#22270;&#20687;&#20849;&#21516;&#24418;&#25104;&#20102;&#21160;&#30011;&#12290;&#28982;&#32780;&#65292;&#21160;&#30011;&#21463;&#21040;&#25193;&#25955;&#22120;&#21435;&#22122;&#29305;&#24615;&#30340;&#38480;&#21046;&#65292;&#21482;&#21576;&#29616;&#20102;&#19968;&#20010;&#38160;&#21270;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#28508;&#22312;&#30011;&#23478;&#65292;&#23427;&#20197;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#65292;&#20197;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#65292;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#12290;&#28508;&#22312;&#30011;&#23478;&#36824;&#21487;&#20197;&#23558;&#19968;&#20010;&#29983;&#25104;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#22270;&#20687;&#65292;&#36825;&#21487;&#20197;&#21457;&#29983;&#22312;&#20004;&#20010;&#19981;&#21516;&#26816;&#26597;&#28857;&#38598;&#20013;&#30340;&#22270;&#20687;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent diffusers revolutionized the generative AI and inspired creative art. When denoising the latent, the predicted original image at each step collectively animates the formation. However, the animation is limited by the denoising nature of the diffuser, and only renders a sharpening process. This work presents Latent Painter, which uses the latent as the canvas, and the diffuser predictions as the plan, to generate painting animation. Latent Painter also transits one generated image to another, which can happen between images from two different sets of checkpoints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16484</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#28857;&#20113;&#19978;&#37319;&#26679;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24265;&#20215;&#30340;3D&#25195;&#25551;&#20202;&#32463;&#24120;&#20135;&#29983;&#31232;&#30095;&#21644;&#38750;&#22343;&#21248;&#30340;&#28857;&#20113;&#65292;&#36825;&#23545;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#19979;&#28216;&#24212;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#28857;&#20113;&#19978;&#37319;&#26679;&#26550;&#26500;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#23398;&#20064;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27979;&#35797;&#25968;&#25454;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#22312;&#20803;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21442;&#25968;&#26159;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#30095;-&#23494;&#38598;&#28857;&#20113;&#23545;&#30340;&#38598;&#21512;&#20013;&#23398;&#20064;&#30340;&#12290;&#22312;&#20803;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#32463;&#36807;&#23569;&#37327;&#26799;&#24230;&#26356;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#32452;&#21807;&#19968;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affordable 3D scanners often produce sparse and non-uniform point clouds that negatively impact downstream applications in robotic systems. While existing point cloud upsampling architectures have demonstrated promising results on standard benchmarks, they tend to experience significant performance drops when the test data have different distributions from the training data. To address this issue, this paper proposes a test-time adaption approach to enhance model generality of point cloud upsampling. The proposed approach leverages meta-learning to explicitly learn network parameters for test-time adaption. Our method does not require any prior information about the test data. During meta-training, the model parameters are learned from a collection of instance-level tasks, each of which consists of a sparse-dense pair of point clouds from the training data. During meta-testing, the trained model is fine-tuned with a few gradient updates to produce a unique set of network parameters for
&lt;/p&gt;</description></item><item><title>ECHO-VICODE&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#26469;&#20998;&#31867;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;31&#20010;&#35270;&#22270;&#31867;&#21035;&#65292;&#24182;&#20855;&#26377;&#38598;&#25104;&#30340;&#31163;&#32676;&#26816;&#27979;&#21151;&#33021;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16483</link><description>&lt;p&gt;
&#20855;&#26377;&#38598;&#25104;&#31163;&#32676;&#26816;&#27979;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#20998;&#31867;&#65292;&#20197;&#22686;&#24378;&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Echocardiographic View Classification with Integrated Out-of-Distribution Detection for Enhanced Automatic Echocardiographic Analysis. (arXiv:2308.16483v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16483
&lt;/p&gt;
&lt;p&gt;
ECHO-VICODE&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#26469;&#20998;&#31867;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;31&#20010;&#35270;&#22270;&#31867;&#21035;&#65292;&#24182;&#20855;&#26377;&#38598;&#25104;&#30340;&#31163;&#32676;&#26816;&#27979;&#21151;&#33021;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#26512;&#21644;&#35299;&#37322;&#39046;&#22495;&#20013;&#65292;&#33258;&#21160;&#35270;&#22270;&#20998;&#31867;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#30001;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#21487;&#21464;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ECHO-VICODE&#65288;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#20998;&#31867;&#19982;&#31163;&#32676;&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#26469;&#20998;&#31867;31&#20010;&#31867;&#21035;&#65292;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22788;&#29702;&#22810;&#31181;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;ECHO-VICODE&#36824;&#21152;&#20837;&#20102;&#19968;&#20010;&#38598;&#25104;&#30340;&#31163;&#32676;&#26816;&#27979;&#21151;&#33021;&#65292;&#21033;&#29992;&#30456;&#23545;&#39532;&#27663;&#36317;&#31163;&#26377;&#25928;&#35782;&#21035;&#24120;&#35265;&#30340;&#8220;&#25509;&#36817;&#31163;&#32676;&#8221;&#23454;&#20363;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ECHO-VICODE&#22312;&#35270;&#22270;&#20998;&#31867;&#21644;&#31163;&#32676;&#26816;&#27979;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#28508;&#22312;&#38169;&#35823;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of automatic echocardiographic analysis and interpretation, automatic view classification is a critical yet challenging task, owing to the inherent complexity and variability of echocardiographic data. This study presents ECHOcardiography VIew Classification with Out-of-Distribution dEtection (ECHO-VICODE), a novel deep learning-based framework that effectively addresses this challenge by training to classify 31 classes, surpassing previous studies and demonstrating its capacity to handle a wide range of echocardiographic views. Furthermore, ECHO-VICODE incorporates an integrated out-of-distribution (OOD) detection function, leveraging the relative Mahalanobis distance to effectively identify 'near-OOD' instances commonly encountered in echocardiographic data. Through extensive experimentation, we demonstrated the outstanding performance of ECHO-VICODE in terms of view classification and OOD detection, significantly reducing the potential for errors in ech
&lt;/p&gt;</description></item><item><title>Point-TTA&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#23454;&#29616;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16481</link><description>&lt;p&gt;
Point-TTA: &#20351;&#29992;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16481
&lt;/p&gt;
&lt;p&gt;
Point-TTA&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#23454;&#29616;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Point-TTA&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#34429;&#28982;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#38754;&#23545;&#26410;&#30693;&#30340;&#27979;&#35797;&#29615;&#22659;&#30340;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;3D&#25195;&#25551;&#30340;&#21464;&#21270;&#36739;&#22823;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22312;&#27599;&#20010;&#23454;&#20363;&#19978;&#24212;&#29992;&#30456;&#21516;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#21516;&#19968;&#27169;&#22411;&#24456;&#38590;&#22788;&#29702;&#27979;&#35797;&#26399;&#38388;&#30340;&#25152;&#26377;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28857;&#20113;&#37197;&#20934;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#27979;&#35797;&#26102;&#26410;&#30693;&#30340;&#20998;&#24067;&#65292;&#26080;&#38656;&#20219;&#20309;&#20851;&#20110;&#27979;&#35797;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#19982;&#20027;&#35201;&#30340;&#37197;&#20934;&#20219;&#21153;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#12290;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#23454;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#36741;&#21161;&#20219;&#21153;&#26469;&#35843;&#25972;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Point-TTA, a novel test-time adaptation framework for point cloud registration (PCR) that improves the generalization and the performance of registration models. While learning-based approaches have achieved impressive progress, generalization to unknown testing environments remains a major challenge due to the variations in 3D scans. Existing methods typically train a generic model and the same trained model is applied on each instance during testing. This could be sub-optimal since it is difficult for the same model to handle all the variations during testing. In this paper, we propose a test-time adaptation approach for PCR. Our model can adapt to unseen distributions at test-time without requiring any prior knowledge of the test data. Concretely, we design three self-supervised auxiliary tasks that are optimized jointly with the primary PCR task. Given a test instance, we adapt our model using these auxiliary tasks and the updated model is used to perform the inference. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36866;&#24212;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#24182;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16471</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#38544;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#31574;&#30053;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Adaptation Method for Implicit Multitask Reinforcement Learning Problems. (arXiv:2308.16471v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36866;&#24212;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#24182;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#25509;&#35302;&#21644;&#30896;&#25758;&#65292;&#31574;&#30053;&#21442;&#25968;&#30340;&#23567;&#25913;&#21464;&#21487;&#33021;&#23548;&#33268;&#26497;&#20854;&#19981;&#21516;&#30340;&#22238;&#25253;&#12290;&#20363;&#22914;&#65292;&#22312;&#36275;&#29699;&#20013;&#65292;&#36890;&#36807;&#31245;&#24494;&#25913;&#21464;&#36386;&#29699;&#20301;&#32622;&#25110;&#26045;&#21152;&#29699;&#30340;&#21147;&#25110;&#32773;&#29699;&#30340;&#25705;&#25830;&#21147;&#21457;&#29983;&#21464;&#21270;&#65292;&#29699;&#21487;&#20197;&#20197;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#21521;&#39134;&#34892;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#24819;&#35937;&#22312;&#19981;&#21516;&#30340;&#26041;&#21521;&#19978;&#22836;&#29699;&#38656;&#35201;&#23436;&#20840;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#36866;&#24212;&#30446;&#26631;&#25110;&#29615;&#22659;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#25110;&#29615;&#22659;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#21333;&#33050;&#26426;&#22120;&#20154;&#27169;&#22411;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#30446;&#26631;&#20301;&#32622;&#30340;&#38544;&#24335;&#21464;&#21270;&#25110;&#29699;&#30340;&#24674;&#22797;&#31995;&#25968;&#30340;&#21464;&#21270;&#65292;&#32780;&#26631;&#20934;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#21017;&#19981;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic motion generation tasks, including contact and collisions, small changes in policy parameters can lead to extremely different returns. For example, in soccer, the ball can fly in completely different directions with a similar heading motion by slightly changing the hitting position or the force applied to the ball or when the friction of the ball varies. However, it is difficult to imagine that completely different skills are needed for heading a ball in different directions. In this study, we proposed a multitask reinforcement learning algorithm for adapting a policy to implicit changes in goals or environments in a single motion category with different reward functions or physical parameters of the environment. We evaluated the proposed method on the ball heading task using a monopod robot model. The results showed that the proposed method can adapt to implicit changes in the goal positions or the coefficients of restitution of the ball, whereas the standard domain randomi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(DM-GNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#33410;&#28857;&#20998;&#31867;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21452;&#29305;&#24449;&#25552;&#21462;&#22120;&#26500;&#24314;GNN&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#33410;&#28857;&#20998;&#31867;&#22120;&#21644;&#26631;&#31614;&#24863;&#30693;&#30340;&#20256;&#25773;&#26041;&#26696;&#26469;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16470</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Domain-adaptive Message Passing Graph Neural Network. (arXiv:2308.16470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(DM-GNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#33410;&#28857;&#20998;&#31867;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21452;&#29305;&#24449;&#25552;&#21462;&#22120;&#26500;&#24314;GNN&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#33410;&#28857;&#20998;&#31867;&#22120;&#21644;&#26631;&#31614;&#24863;&#30693;&#30340;&#20256;&#25773;&#26041;&#26696;&#26469;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;(CNNC)&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#20855;&#26377;&#20016;&#23500;&#26631;&#31614;&#30340;&#28304;&#32593;&#32476;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#23545;&#26631;&#31614;&#19981;&#20805;&#20998;&#30340;&#30446;&#26631;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;CNNC&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#21644;&#26465;&#20214;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(DM-GNN)&#65292;&#33021;&#22815;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#33410;&#28857;&#20998;&#31867;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21452;&#29305;&#24449;&#25552;&#21462;&#22120;&#26500;&#24314;&#20102;GNN&#32534;&#30721;&#22120;&#65292;&#23558;&#33258;&#25105;&#23884;&#20837;&#23398;&#20064;&#19982;&#37051;&#23621;&#23884;&#20837;&#23398;&#20064;&#20998;&#31163;&#65292;&#20197;&#20849;&#21516;&#25429;&#25417;&#36830;&#25509;&#33410;&#28857;&#20043;&#38388;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#20256;&#25773;&#33410;&#28857;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#33258;&#36523;&#39044;&#27979;&#21644;&#37051;&#23621;&#39044;&#27979;&#30456;&#32467;&#21512;&#26469;&#32454;&#21270;&#27599;&#20010;&#33410;&#28857;&#30340;&#26631;&#31614;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26377;&#26631;&#31614;&#28304;&#32593;&#32476;&#35774;&#35745;&#20102;&#26631;&#31614;&#24863;&#30693;&#30340;&#20256;&#25773;&#26041;&#26696;&#65292;&#20197;&#20419;&#36827;&#26631;&#31614;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-network node classification (CNNC), which aims to classify nodes in a label-deficient target network by transferring the knowledge from a source network with abundant labels, draws increasing attention recently. To address CNNC, we propose a domain-adaptive message passing graph neural network (DM-GNN), which integrates graph neural network (GNN) with conditional adversarial domain adaptation. DM-GNN is capable of learning informative representations for node classification that are also transferrable across networks. Firstly, a GNN encoder is constructed by dual feature extractors to separate ego-embedding learning from neighbor-embedding learning so as to jointly capture commonality and discrimination between connected nodes. Secondly, a label propagation node classifier is proposed to refine each node's label prediction by combining its own prediction and its neighbors' prediction. In addition, a label-aware propagation scheme is devised for the labeled source network to promo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35268;&#33539;&#21270;&#27969;&#35745;&#31639;&#20998;&#23376;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36924;&#36817;&#27874;&#20989;&#25968;&#24182;&#20248;&#21270;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#31354;&#38388;&#20869;&#30340;&#36817;&#20284;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#37327;&#23376;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#20934;&#30830;&#21644;&#26377;&#25928;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#33021;&#37327;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#22522;&#32452;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36827;&#34892;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2308.16468</link><description>&lt;p&gt;
&#20351;&#29992;&#35268;&#33539;&#21270;&#27969;&#35745;&#31639;&#20998;&#23376;&#30340;&#28608;&#21457;&#24577;
&lt;/p&gt;
&lt;p&gt;
Computing excited states of molecules using normalizing flows. (arXiv:2308.16468v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16468
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35268;&#33539;&#21270;&#27969;&#35745;&#31639;&#20998;&#23376;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36924;&#36817;&#27874;&#20989;&#25968;&#24182;&#20248;&#21270;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#31354;&#38388;&#20869;&#30340;&#36817;&#20284;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#37327;&#23376;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#20934;&#30830;&#21644;&#26377;&#25928;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#33021;&#37327;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#22522;&#32452;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36827;&#34892;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#21464;&#20998;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35745;&#31639;&#37327;&#23376;&#31995;&#32479;&#30340;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;&#19982;&#35268;&#33539;&#21270;&#27969;&#30340;&#32452;&#21512;&#26469;&#36924;&#36817;&#27874;&#20989;&#25968;&#65292;&#36825;&#20123;&#27874;&#20989;&#25968;&#20301;&#20110;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#31354;&#38388;&#20013;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#19977;&#21407;&#23376;H$_2$S&#20998;&#23376;&#30340;&#22823;&#37327;&#25391;&#21160;&#24577;&#20197;&#21450;&#20856;&#22411;&#30340;&#21333;&#30005;&#23376;&#31995;&#32479;&#65288;&#21253;&#25324;&#27682;&#21407;&#23376;&#12289;&#20998;&#23376;&#27682;&#31163;&#23376;&#21644;&#30899;&#21407;&#23376;&#22312;&#21333;&#28608;&#21457;&#30005;&#23376;&#36817;&#20284;&#19979;&#30340;&#22522;&#24577;&#21644;&#22810;&#20010;&#28608;&#21457;&#24577;&#65289;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#21442;&#25968;&#36739;&#23569;&#30340;&#35268;&#33539;&#21270;&#27969;&#65292;&#33021;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#22522;&#32452;&#25910;&#25947;&#36895;&#24230;&#20063;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#23545;&#26368;&#20339;&#25429;&#25417;&#24213;&#23618;&#29289;&#29702;&#30340;&#19968;&#32452;&#20869;&#31104;&#22352;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new nonlinear variational framework for simultaneously computing ground and excited states of quantum systems. Our approach is based on approximating wavefunctions in the linear span of basis functions that are augmented and optimized \emph{via} composition with normalizing flows. The accuracy and efficiency of our approach are demonstrated in the calculations of a large number of vibrational states of the triatomic H$_2$S molecule as well as ground and several excited electronic states of prototypical one-electron systems including the hydrogen atom, the molecular hydrogen ion, and a carbon atom in a single-active-electron approximation. The results demonstrate significant improvements in the accuracy of energy predictions and accelerated basis-set convergence even when using normalizing flows with a small number of parameters. The present approach can be also seen as the optimization of a set of intrinsic coordinates that best capture the underlying physics within the gi
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#24433;&#21709;&#26426;&#21046;&#29992;&#20110;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#33021;&#22815;&#20934;&#30830;&#21010;&#20998;&#35757;&#32451;&#38598;&#24182;&#36991;&#20813;&#36807;&#25311;&#21512;&#65292;&#25552;&#20986;&#30340;&#26368;&#22823;&#35760;&#24518;&#24433;&#21709;&#27169;&#22411;&#21644;&#21152;&#26435;&#35760;&#24518;&#24433;&#21709;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#33021;&#21644;&#26102;&#38388;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.16456</link><description>&lt;p&gt;
&#26368;&#23567;&#20108;&#20056;&#27861;&#26368;&#22823;&#21270;&#21644;&#21152;&#26435;&#27867;&#21270;&#35760;&#24518;&#26426;
&lt;/p&gt;
&lt;p&gt;
Least Squares Maximum and Weighted Generalization-Memorization Machines. (arXiv:2308.16456v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#24433;&#21709;&#26426;&#21046;&#29992;&#20110;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#33021;&#22815;&#20934;&#30830;&#21010;&#20998;&#35757;&#32451;&#38598;&#24182;&#36991;&#20813;&#36807;&#25311;&#21512;&#65292;&#25552;&#20986;&#30340;&#26368;&#22823;&#35760;&#24518;&#24433;&#21709;&#27169;&#22411;&#21644;&#21152;&#26435;&#35760;&#24518;&#24433;&#21709;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#33021;&#21644;&#26102;&#38388;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;LSSVM&#65289;&#30340;&#35760;&#24518;&#24433;&#21709;&#26426;&#21046;&#65292;&#23454;&#29616;&#35760;&#24518;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;LSSVM&#26041;&#31243;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#20934;&#30830;&#22320;&#23545;&#35757;&#32451;&#38598;&#36827;&#34892;&#21010;&#20998;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#28982;&#21518;&#25552;&#20986;&#20102;&#26368;&#22823;&#35760;&#24518;&#24433;&#21709;&#27169;&#22411;&#65288;MIMM&#65289;&#21644;&#21152;&#26435;&#35760;&#24518;&#24433;&#21709;&#27169;&#22411;&#65288;WIMM&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36864;&#21270;&#20026;LSSVM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;MIMM&#21644;WIMM&#25552;&#20986;&#20102;&#19968;&#20123;&#19981;&#21516;&#30340;&#35760;&#24518;&#24433;&#21709;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;LSSVM&#65292;&#25105;&#20204;&#30340;MIMM&#21644;WIMM&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#25104;&#26412;&#19978;&#27604;&#20854;&#20182;&#35760;&#24518;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new way of remembering by introducing a memory influence mechanism for the least squares support vector machine (LSSVM). Without changing the equation constraints of the original LSSVM, this mechanism, allows an accurate partitioning of the training set without overfitting. The maximum memory impact model (MIMM) and the weighted impact memory model (WIMM) are then proposed. It is demonstrated that these models can be degraded to the LSSVM. Furthermore, we propose some different memory impact functions for the MIMM and WIMM. The experimental results show that that our MIMM and WIMM have better generalization performance compared to the LSSVM and significant advantage in time cost compared to other memory models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ARREST&#30340;&#26032;&#22411;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24494;&#35843;&#12289;&#22522;&#20110;&#34920;&#31034;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#22024;&#26434;&#37325;&#25773;&#26469;&#20943;&#23569;&#26631;&#20934;&#20934;&#30830;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16454</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#21547;&#34920;&#31034;&#32422;&#26463;&#30340;&#23545;&#25239;&#24494;&#35843;&#26469;&#20943;&#23569;&#20934;&#30830;&#24615;&#19982;&#40065;&#26834;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff. (arXiv:2308.16454v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ARREST&#30340;&#26032;&#22411;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24494;&#35843;&#12289;&#22522;&#20110;&#34920;&#31034;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#22024;&#26434;&#37325;&#25773;&#26469;&#20943;&#23569;&#26631;&#20934;&#20934;&#30830;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24178;&#20928;&#26679;&#26412;&#30340;&#26631;&#20934;&#20934;&#30830;&#24615;&#21644;&#23545;&#25239;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#20250;&#38477;&#20302;&#20854;&#26631;&#20934;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#26435;&#34913;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;ARREST&#65292;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#23545;&#25239;&#24494;&#35843;&#65288;AFT&#65289;&#65292;&#65288;ii&#65289;&#22522;&#20110;&#34920;&#31034;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;RGKD&#65289;&#65292;&#21644;&#65288;iii&#65289;&#22024;&#26434;&#37325;&#25773;&#65288;NR&#65289;&#12290;AFT&#36890;&#36807;&#23558;&#21442;&#25968;&#21021;&#22987;&#21270;&#20026;&#22312;&#24178;&#20928;&#26679;&#26412;&#19978;&#26631;&#20934;&#39044;&#35757;&#32451;&#30340;DNN&#65292;&#23545;DNN&#36827;&#34892;&#23545;&#25239;&#26679;&#26412;&#30340;&#35757;&#32451;&#12290;RGKD&#21644;NR&#20998;&#21035;&#21033;&#29992;&#27491;&#21017;&#21270;&#39033;&#21644;&#31639;&#27861;&#22312;AFT&#26399;&#38388;&#20445;&#30041;&#24178;&#20928;&#26679;&#26412;&#30340;&#38544;&#21547;&#34920;&#31034;&#12290;RGKD&#24809;&#32602;&#26631;&#20934;&#39044;&#35757;&#32451;&#21644;AFT DNN&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;&#24403;AFT&#26399;&#38388;&#34920;&#31034;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#26102;&#65292;NR&#23558;&#36755;&#20837;&#23545;&#25239;&#26679;&#26412;&#20999;&#25442;&#20026;&#38750;&#23545;&#25239;&#26679;&#26412;&#12290;&#36890;&#36807;&#32452;&#21512;&#36825;&#20123;&#32452;&#20214;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21516;&#26102;&#25552;&#39640;&#26631;&#20934;&#20934;&#30830;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the tradeoff between standard accuracy on clean examples and robustness against adversarial examples in deep neural networks (DNNs). Although adversarial training (AT) improves robustness, it degrades the standard accuracy, thus yielding the tradeoff. To mitigate this tradeoff, we propose a novel AT method called ARREST, which comprises three components: (i) adversarial finetuning (AFT), (ii) representation-guided knowledge distillation (RGKD), and (iii) noisy replay (NR). AFT trains a DNN on adversarial examples by initializing its parameters with a DNN that is standardly pretrained on clean examples. RGKD and NR respectively entail a regularization term and an algorithm to preserve latent representations of clean examples during AFT. RGKD penalizes the distance between the representations of the standardly pretrained and AFT DNNs. NR switches input adversarial examples to nonadversarial ones when the representation changes significantly during AFT. By combining t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;PASS&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26631;&#31614;&#20559;&#35265;&#21644;&#27969;&#37327;&#22343;&#21248;&#24615;&#31561;&#38382;&#39064;&#65292;&#20026;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.16453</link><description>&lt;p&gt;
&#21548;&#21462;&#23569;&#25968;&#32676;&#20307;&#30340;&#22768;&#38899;&#65306;&#22522;&#20110;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training. (arXiv:2308.16453v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;PASS&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26631;&#31614;&#20559;&#35265;&#21644;&#27969;&#37327;&#22343;&#21248;&#24615;&#31561;&#38382;&#39064;&#65292;&#20026;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#21508;&#20010;&#26041;&#38754;&#28145;&#21051;&#22320;&#25913;&#21464;&#20102;&#29616;&#20195;&#29983;&#27963;&#26041;&#24335;&#12290;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22312;&#31649;&#29702;&#31227;&#21160;&#20114;&#32852;&#32593;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#21152;&#23494;&#36890;&#20449;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#19968;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#20013;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#38480;&#21046;&#65306;1&#65289;&#30001;&#27969;&#37327;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#26631;&#31614;&#20559;&#35265;&#65292;2&#65289;&#30001;&#32452;&#20214;&#20849;&#20139;&#24341;&#36215;&#30340;&#27969;&#37327;&#22343;&#21248;&#24615;&#65292;&#20197;&#21450;3&#65289;&#20381;&#36182;&#20805;&#36275;&#26631;&#35760;&#27969;&#37327;&#36827;&#34892;&#35757;&#32451;&#12290;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;&#65292;&#31216;&#20026;PASS&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#37325;&#26032;&#37319;&#26679;&#21407;&#22987;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#32780;&#19981;&#30452;&#25509;&#20351;&#29992;&#20010;&#20307;&#24212;&#29992;&#31243;&#24207;&#26631;&#31614;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#65292;&#21516;&#26102;&#33719;&#24471;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#21306;&#20998;&#37325;&#21472;&#30340;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile Internet has profoundly reshaped modern lifestyles in various aspects. Encrypted Traffic Classification (ETC) naturally plays a crucial role in managing mobile Internet, especially with the explosive growth of mobile apps using encrypted communication. Despite some existing learning-based ETC methods showing promising results, three-fold limitations still remain in real-world network environments, 1) label bias caused by traffic class imbalance, 2) traffic homogeneity caused by component sharing, and 3) training with reliance on sufficient labeled traffic. None of the existing ETC methods can address all these limitations. In this paper, we propose a novel Pre-trAining Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the original train dataset and perform contrastive pre-training without using individual app labels directly to avoid label bias issues caused by class imbalance, while obtaining a robust feature representation to differentiate overlapping 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AntM$^{2}$C&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#22330;&#26223;&#22810;&#27169;&#24577;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#35813;&#25968;&#25454;&#38598;&#24357;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#22810;&#20010;&#22330;&#26223;&#20013;&#19981;&#21516;&#31867;&#22411;&#39033;&#30446;&#30340;&#24314;&#27169;&#20197;&#21450;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#32570;&#20047;&#12290;&#23427;&#23558;&#20026;&#27169;&#22411;&#30340;&#21487;&#38752;&#35780;&#20272;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.16437</link><description>&lt;p&gt;
AntM$^{2}$C&#65306;&#19968;&#20010;&#29992;&#20110;&#22810;&#22330;&#26223;&#22810;&#27169;&#24577;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR Prediction. (arXiv:2308.16437v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AntM$^{2}$C&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#22330;&#26223;&#22810;&#27169;&#24577;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#35813;&#25968;&#25454;&#38598;&#24357;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#22810;&#20010;&#22330;&#26223;&#20013;&#19981;&#21516;&#31867;&#22411;&#39033;&#30446;&#30340;&#24314;&#27169;&#20197;&#21450;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#32570;&#20047;&#12290;&#23427;&#23558;&#20026;&#27169;&#22411;&#30340;&#21487;&#38752;&#35780;&#20272;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20986;&#29616;&#20102;&#21508;&#31181;&#20844;&#24320;&#30340;CTR&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#29992;&#25143;&#36890;&#24120;&#20250;&#20174;&#22810;&#20010;&#22330;&#26223;&#20013;&#28857;&#20987;&#19981;&#21516;&#31867;&#22411;&#30340;&#39033;&#30446;&#65292;&#20174;&#22810;&#20010;&#22330;&#26223;&#24314;&#27169;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#20102;&#35299;&#29992;&#25143;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#21482;&#21253;&#25324;&#26469;&#33258;&#21333;&#20010;&#22330;&#26223;&#30340;&#30456;&#21516;&#31867;&#22411;&#39033;&#30446;&#30340;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#22810;&#27169;&#24577;&#29305;&#24449;&#22312;&#22810;&#22330;&#26223;&#39044;&#27979;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#35299;&#20915;&#20102;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;ID&#32534;&#30721;&#38382;&#39064;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#22522;&#20110;ID&#29305;&#24449;&#65292;&#32570;&#20047;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#31532;&#19977;&#65292;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#35780;&#20272;&#65292;&#20805;&#20998;&#21453;&#26144;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#32422;&#20026;1&#20159;&#65292;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;CTR&#39044;&#27979;&#30456;&#27604;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction is a crucial issue in recommendation systems. There has been an emergence of various public CTR datasets. However, existing datasets primarily suffer from the following limitations. Firstly, users generally click different types of items from multiple scenarios, and modeling from multiple scenarios can provide a more comprehensive understanding of users. Existing datasets only include data for the same type of items from a single scenario. Secondly, multi-modal features are essential in multi-scenario prediction as they address the issue of inconsistent ID encoding between different scenarios. The existing datasets are based on ID features and lack multi-modal features. Third, a large-scale dataset can provide a more reliable evaluation of models, fully reflecting the performance differences between models. The scale of existing datasets is around 100 million, which is relatively small compared to the real-world CTR prediction. To address these limit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#39640;&#32500;&#31561;&#20215;&#34920;&#36798;&#65292;&#24182;&#22312;&#39640;&#32500;&#31354;&#38388;&#24314;&#31435;&#20102;&#38544;&#24335;&#32593;&#32476;&#21644;&#26174;&#24335;&#32593;&#32476;&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16425</link><description>&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#19982;&#26174;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#20215;&#24615;&#65306;&#39640;&#32500;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint. (arXiv:2308.16425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#39640;&#32500;&#31561;&#20215;&#34920;&#36798;&#65292;&#24182;&#22312;&#39640;&#32500;&#31354;&#38388;&#24314;&#31435;&#20102;&#38544;&#24335;&#32593;&#32476;&#21644;&#26174;&#24335;&#32593;&#32476;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38544;&#24335;&#32593;&#32476;&#21644;&#26174;&#24335;&#32593;&#32476;&#20043;&#38388;&#30340;&#36830;&#25509;&#21644;&#24046;&#24322;&#32570;&#20047;&#29702;&#35770;&#20998;&#26512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20026;&#23545;&#24212;&#30340;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25552;&#20379;&#20102;&#39640;&#32500;&#31561;&#20215;&#34920;&#36798;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#22312;&#39640;&#32500;&#31354;&#38388;&#24314;&#31435;&#20102;&#38544;&#24335;&#32593;&#32476;&#21644;&#26174;&#24335;&#32593;&#32476;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit neural networks have demonstrated remarkable success in various tasks. However, there is a lack of theoretical analysis of the connections and differences between implicit and explicit networks. In this paper, we study high-dimensional implicit neural networks and provide the high dimensional equivalents for the corresponding conjugate kernels and neural tangent kernels. Built upon this, we establish the equivalence between implicit and explicit networks in high dimensions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DECODE&#30340;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;&#30340;&#20449;&#21495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#26102;&#38388;&#24310;&#36831;&#24178;&#28041;&#20202;&#20197;&#22788;&#29702;&#22810;&#36890;&#36947;TDI&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.16422</link><description>&lt;p&gt;
DECODE: &#29992;&#20110;&#26816;&#27979;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;&#30340;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals. (arXiv:2308.16422v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DECODE&#30340;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;&#30340;&#20449;&#21495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#26102;&#38388;&#24310;&#36831;&#24178;&#28041;&#20202;&#20197;&#22788;&#29702;&#22810;&#36890;&#36947;TDI&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#27874;&#24418;&#12289;&#25345;&#20037;&#30340;&#25345;&#32493;&#26102;&#38388;&#21644;&#20302;&#20449;&#22122;&#27604;&#65292;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;(EMRI)&#30340;&#26816;&#27979;&#26159;&#22797;&#26434;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#19982;&#32039;&#20945;&#30340;&#20108;&#36827;&#21046;&#34701;&#21512;&#30456;&#27604;&#26356;&#38590;&#34987;&#35782;&#21035;&#12290;&#34429;&#28982;&#22522;&#20110;&#21305;&#37197;&#28388;&#27874;&#30340;&#25216;&#26415;&#20197;&#20854;&#35745;&#31639;&#35201;&#27714;&#32780;&#38395;&#21517;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#22788;&#29702;&#26102;&#22495;&#25968;&#25454;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#21040;&#25968;&#25454;&#25345;&#32493;&#26102;&#38388;&#21644;&#20449;&#22122;&#27604;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#24573;&#30053;&#20102;&#26102;&#38388;&#24310;&#36831;&#24178;&#28041;&#20202;(TDI)&#24182;&#22312;&#25506;&#27979;&#22120;&#21709;&#24212;&#35745;&#31639;&#20013;&#24212;&#29992;&#20102;&#38271;&#27874;&#36817;&#20284;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22788;&#29702;&#28608;&#20809;&#39057;&#29575;&#22122;&#22768;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DECODE&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#39057;&#22495;&#24207;&#21015;&#24314;&#27169;&#20026;&#37325;&#28857;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;EMRI&#20449;&#21495;&#26816;&#27979;&#12290;DECODE&#22260;&#32469;&#30528;&#19968;&#20010;&#20197;&#25193;&#24352;&#22240;&#26524;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20026;&#20013;&#24515;&#65292;&#20351;&#29992;&#32771;&#34385;&#21040;TDI-1.5&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#19968;&#24180;&#30340;&#22810;&#36890;&#36947;TDI&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of Extreme Mass Ratio Inspirals (EMRIs) is intricate due to their complex waveforms, extended duration, and low signal-to-noise ratio (SNR), making them more challenging to be identified compared to compact binary coalescences. While matched filtering-based techniques are known for their computational demands, existing deep learning-based methods primarily handle time-domain data and are often constrained by data duration and SNR. In addition, most existing work ignores time-delay interferometry (TDI) and applies the long-wavelength approximation in detector response calculations, thus limiting their ability to handle laser frequency noise. In this study, we introduce DECODE, an end-to-end model focusing on EMRI signal detection by sequence modeling in the frequency domain. Centered around a dilated causal convolutional neural network, trained on synthetic data considering TDI-1.5 detector response, DECODE can efficiently process a year's worth of multichannel TDI data wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CktGNN&#30340;&#30005;&#36335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#35782;&#21035;&#30005;&#36335;&#30340;&#22270;&#24418;&#29305;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#21270;&#30005;&#36335;&#25299;&#25169;&#29983;&#25104;&#21644;&#22120;&#20214;&#23610;&#23544;&#35843;&#25972;&#12290;&#23427;&#20351;&#29992;&#20004;&#32423;GNN&#26694;&#26550;&#23545;&#30005;&#36335;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#35774;&#35745;&#25928;&#29575;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.16406</link><description>&lt;p&gt;
CktGNN&#65306;&#29992;&#20110;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#30340;&#30005;&#36335;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CktGNN: Circuit Graph Neural Network for Electronic Design Automation. (arXiv:2308.16406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CktGNN&#30340;&#30005;&#36335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#35782;&#21035;&#30005;&#36335;&#30340;&#22270;&#24418;&#29305;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#21270;&#30005;&#36335;&#25299;&#25169;&#29983;&#25104;&#21644;&#22120;&#20214;&#23610;&#23544;&#35843;&#25972;&#12290;&#23427;&#20351;&#29992;&#20004;&#32423;GNN&#26694;&#26550;&#23545;&#30005;&#36335;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#35774;&#35745;&#25928;&#29575;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24040;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#22797;&#26434;&#30340;&#35774;&#35745;&#26435;&#34913;&#65292;&#27169;&#25311;&#30005;&#36335;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#19968;&#30452;&#26159;&#38598;&#25104;&#30005;&#36335;&#39046;&#22495;&#30340;&#19968;&#20010;&#38271;&#26399;&#25361;&#25112;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#22823;&#22810;&#25968;&#20851;&#27880;&#20110;&#22312;&#32473;&#23450;&#30005;&#36335;&#25299;&#25169;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#35843;&#25972;&#26230;&#20307;&#31649;&#23610;&#23544;&#12290;&#36890;&#36807;&#35782;&#21035;&#30005;&#36335;&#30340;&#22270;&#24418;&#29305;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30005;&#36335;&#22270;&#31070;&#32463;&#32593;&#32476;(CktGNN)&#65292;&#23427;&#22522;&#20110;&#32534;&#30721;&#22120;&#20381;&#36182;&#30340;&#20248;&#21270;&#23376;&#31243;&#24207;&#65292;&#21516;&#26102;&#33258;&#21160;&#21270;&#30005;&#36335;&#25299;&#25169;&#29983;&#25104;&#21644;&#22120;&#20214;&#23610;&#23544;&#35843;&#25972;&#12290;&#29305;&#21035;&#26159;&#65292;CktGNN&#20351;&#29992;&#20004;&#32423;GNN&#26694;&#26550;&#65288;&#23884;&#22871;GNN&#65289;&#23545;&#30005;&#36335;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#20854;&#20013;&#30005;&#36335;&#34920;&#31034;&#20026;&#24050;&#30693;&#23376;&#22270;&#22522;&#30784;&#19978;&#30340;&#23376;&#22270;&#32452;&#21512;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#20943;&#23569;&#23376;&#22270;&#25968;&#37327;&#26469;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#25928;&#29575;&#20197;&#25191;&#34892;&#28040;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have mostly been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves design efficiency by reducing the number of subgraphs to perform message passing. Nonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#23884;&#20837;&#20013;&#24179;&#34913;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35843;&#21442;&#25968;&#36827;&#34892;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#32858;&#31867;&#36317;&#31163;&#20445;&#25345;&#65292;&#29992;&#20110;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.16403</link><description>&lt;p&gt;
&#22312;&#22270;&#23884;&#20837;&#20013;&#24179;&#34913;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65288;LGS&#65289;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Balancing between the Local and Global Structures (LGS) in Graph Embedding. (arXiv:2308.16403v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#23884;&#20837;&#20013;&#24179;&#34913;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35843;&#21442;&#25968;&#36827;&#34892;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#32858;&#31867;&#36317;&#31163;&#20445;&#25345;&#65292;&#29992;&#20110;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35843;&#21442;&#25968;&#22312;&#22270;&#23884;&#20837;&#20013;&#24179;&#34913;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65288;LGS&#65289;&#30340;&#26041;&#27861;&#12290;&#19968;&#20123;&#23884;&#20837;&#26041;&#27861;&#26088;&#22312;&#25429;&#25417;&#20840;&#23616;&#32467;&#26500;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21017;&#35797;&#22270;&#20445;&#30041;&#23616;&#37096;&#37051;&#22495;&#12290;&#24456;&#23569;&#26377;&#26041;&#27861;&#23581;&#35797;&#21516;&#26102;&#20570;&#21040;&#36825;&#20004;&#28857;&#65292;&#24182;&#19988;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#24456;&#38590;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#32780;&#22823;&#37096;&#20998;&#22270;&#30340;&#32472;&#21046;&#37117;&#26159;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#12290;&#26681;&#25454;&#20219;&#21153;&#21644;&#24213;&#23618;&#25968;&#25454;&#30340;&#32467;&#26500;&#36873;&#25321;&#20351;&#29992;&#23616;&#37096;&#36824;&#26159;&#20840;&#23616;&#23884;&#20837;&#26469;&#36827;&#34892;&#21487;&#35270;&#21270;&#19981;&#20165;&#21462;&#20915;&#20110;&#20219;&#21153;&#26412;&#36523;&#65292;&#36824;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#32467;&#26500;&#65292;&#32780;&#36825;&#26159;&#20107;&#20808;&#19981;&#30693;&#36947;&#30340;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#22270;&#65292;LGS&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#20197;&#20445;&#30041;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#12290;&#25105;&#20204;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;LGS&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#20351;&#29992;&#20102;&#35832;&#22914;&#21387;&#21147;&#21644;&#37051;&#22495;&#20445;&#25345;&#31561;&#24050;&#24314;&#31435;&#30340;&#36136;&#37327;&#25351;&#26631;&#26102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#32858;&#31867;&#36317;&#31163;&#20445;&#25345;&#65292;&#29992;&#20110;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for balancing between the Local and Global Structures (LGS) in graph embedding, via a tunable parameter. Some embedding methods aim to capture global structures, while others attempt to preserve local neighborhoods. Few methods attempt to do both, and it is not always possible to capture well both local and global information in two dimensions, which is where most graph drawing live. The choice of using a local or a global embedding for visualization depends not only on the task but also on the structure of the underlying data, which may not be known in advance. For a given graph, LGS aims to find a good balance between the local and global structure to preserve. We evaluate the performance of LGS with synthetic and real-world datasets and our results indicate that it is competitive with the state-of-the-art methods, using established quality metrics such as stress and neighborhood preservation. We introduce a novel quality metric, cluster distance preservation, to 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26131;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#20197;&#22826;&#22346;&#19978;&#24222;&#27663;&#39575;&#23616;&#30340;&#26816;&#27979;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#28304;&#20195;&#30721;&#25110;&#25805;&#20316;&#30721;&#36827;&#34892;&#26816;&#27979;&#65292;&#20294;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#20132;&#26131;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#24222;&#27663;&#39575;&#23616;&#65292;&#22240;&#20026;&#20132;&#26131;&#26356;&#38590;&#20266;&#35013;&#12290;</title><link>http://arxiv.org/abs/2308.16391</link><description>&lt;p&gt;
&#25552;&#39640;&#20197;&#22826;&#22346;&#19978;&#24222;&#27663;&#39575;&#23616;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness and Accuracy of Ponzi Scheme Detection on Ethereum Using Time-Dependent Features. (arXiv:2308.16391v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16391
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26131;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#20197;&#22826;&#22346;&#19978;&#24222;&#27663;&#39575;&#23616;&#30340;&#26816;&#27979;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#28304;&#20195;&#30721;&#25110;&#25805;&#20316;&#30721;&#36827;&#34892;&#26816;&#27979;&#65292;&#20294;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#20132;&#26131;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#24222;&#27663;&#39575;&#23616;&#65292;&#22240;&#20026;&#20132;&#26131;&#26356;&#38590;&#20266;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#36234;&#26469;&#36234;&#22810;&#30340;&#36164;&#37329;&#28044;&#20837;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#65292;&#20063;&#21560;&#24341;&#20102;&#36817;&#24180;&#26469;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#30340;&#20852;&#36259;&#12290;&#24222;&#27663;&#39575;&#23616;&#20316;&#20026;&#19968;&#31181;&#32769;&#24335;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#29616;&#22312;&#20063;&#27969;&#34892;&#20110;&#21306;&#22359;&#38142;&#19978;&#65292;&#32473;&#35768;&#22810;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32773;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#36130;&#21153;&#25439;&#22833;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#24222;&#27663;&#39575;&#23616;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#25110;&#25805;&#20316;&#30721;&#36827;&#34892;&#26816;&#27979;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#21512;&#32422;&#20195;&#30721;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#32570;&#20047;&#40065;&#26834;&#24615;&#65306;&#39318;&#20808;&#65292;&#22823;&#37096;&#20998;&#20197;&#22826;&#22346;&#19978;&#30340;&#21512;&#32422;&#28304;&#20195;&#30721;&#24182;&#19981;&#20844;&#24320;&#21487;&#29992;&#65307;&#20854;&#27425;&#65292;&#24222;&#27663;&#39575;&#23616;&#24320;&#21457;&#32773;&#21487;&#20197;&#36890;&#36807;&#28151;&#28102;&#25805;&#20316;&#30721;&#25110;&#32773;&#21019;&#36896;&#26032;&#30340;&#20998;&#37197;&#36923;&#36753;&#26469;&#27450;&#39575;&#22522;&#20110;&#21512;&#32422;&#20195;&#30721;&#30340;&#26816;&#27979;&#27169;&#22411;&#65288;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#20165;&#22312;&#29616;&#26377;&#30340;&#24222;&#27663;&#36923;&#36753;&#19978;&#36827;&#34892;&#35757;&#32451;&#65289;&#12290;&#22522;&#20110;&#20132;&#26131;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#19982;&#26234;&#33021;&#21512;&#32422;&#19981;&#21516;&#65292;&#20132;&#26131;&#26356;&#21152;&#38590;&#20197;&#20266;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of blockchain has led to more and more funding pouring into the cryptocurrency market, which also attracted cybercriminals' interest in recent years. The Ponzi scheme, an old-fashioned fraud, is now popular on the blockchain, causing considerable financial losses to many crypto-investors. A few Ponzi detection methods have been proposed in the literature, most of which detect a Ponzi scheme based on its smart contract source code or opcode. The contract-code-based approach, while achieving very high accuracy, is not robust: first, the source codes of a majority of contracts on Ethereum are not available, and second, a Ponzi developer can fool a contract-code-based detection model by obfuscating the opcode or inventing a new profit distribution logic that cannot be detected (since these models were trained on existing Ponzi logics only). A transaction-based approach could improve the robustness of detection because transactions, unlike smart contracts, are harder t
&lt;/p&gt;</description></item><item><title>BenchTemp&#26159;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#27169;&#22411;&#22312;&#19981;&#21516;&#24037;&#20316;&#36127;&#36733;&#19978;&#30340;&#34920;&#29616;&#12290;BenchTemp&#25552;&#20379;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26631;&#20934;&#27969;&#31243;&#65292;&#29992;&#20110;&#20844;&#24179;&#27604;&#36739;&#19981;&#21516;&#30340;TGNN&#27169;&#22411;&#12290;&#36890;&#36807;BenchTemp&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#35774;&#32622;&#19979;&#30340;&#20195;&#34920;&#24615;TGNN&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.16385</link><description>&lt;p&gt;
BenchTemp: &#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks. (arXiv:2308.16385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16385
&lt;/p&gt;
&lt;p&gt;
BenchTemp&#26159;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#27169;&#22411;&#22312;&#19981;&#21516;&#24037;&#20316;&#36127;&#36733;&#19978;&#30340;&#34920;&#29616;&#12290;BenchTemp&#25552;&#20379;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26631;&#20934;&#27969;&#31243;&#65292;&#29992;&#20110;&#20844;&#24179;&#27604;&#36739;&#19981;&#21516;&#30340;TGNN&#27169;&#22411;&#12290;&#36890;&#36807;BenchTemp&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#35774;&#32622;&#19979;&#30340;&#20195;&#34920;&#24615;TGNN&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#29305;&#24449;&#25110;&#36830;&#25509;&#22312;&#28436;&#21270;&#30340;&#22270;&#20013;&#30340;&#26102;&#38388;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNNs&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;TGNN&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20197;&#24448;&#30340;TGNN&#35780;&#20272;&#25581;&#31034;&#20102;&#22235;&#20010;&#20851;&#38190;&#38382;&#39064;&#19978;&#30340;&#20960;&#20010;&#38480;&#21046;&#65306;1&#65289;&#25968;&#25454;&#38598;&#19981;&#19968;&#33268;&#65292;2&#65289;&#35780;&#20272;&#27969;&#31243;&#19981;&#19968;&#33268;&#65292;3&#65289;&#32570;&#20047;&#24037;&#20316;&#36127;&#36733;&#22810;&#26679;&#24615;&#65292;4&#65289;&#32570;&#20047;&#26377;&#25928;&#30340;&#27604;&#36739;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#32570;&#20047;&#19968;&#20010;&#23558;TGNN&#27169;&#22411;&#25918;&#22312;&#21516;&#19968;&#36215;&#36305;&#32447;&#19978;&#24182;&#20840;&#38754;&#27604;&#36739;&#23427;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BenchTemp&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#21508;&#31181;&#24037;&#20316;&#36127;&#36733;&#19978;&#35780;&#20272;TGNN&#27169;&#22411;&#30340;&#36890;&#29992;&#22522;&#20934;&#12290;BenchTemp&#25552;&#20379;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#21487;&#20197;&#20844;&#24179;&#22320;&#27604;&#36739;&#19981;&#21516;&#30340;TGNN&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;BenchTemp&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26631;&#20934;&#27969;&#31243;&#65292;&#32479;&#19968;&#20102;TGNN&#30340;&#35780;&#20272;&#12290;&#20511;&#21161;BenchTemp&#65292;&#25105;&#20204;&#24191;&#27867;&#27604;&#36739;&#20102;&#19981;&#21516;&#20219;&#21153;&#65288;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#65289;&#21644;&#35774;&#32622;&#65288;&#20256;&#36882;&#21644;&#24402;&#32435;&#65289;&#19979;&#30340;&#20195;&#34920;&#24615;TGNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle graphs in which features or connectivities are evolving over time, a series of temporal graph neural networks (TGNNs) have been proposed. Despite the success of these TGNNs, the previous TGNN evaluations reveal several limitations regarding four critical issues: 1) inconsistent datasets, 2) inconsistent evaluation pipelines, 3) lacking workload diversity, and 4) lacking efficient comparison. Overall, there lacks an empirical study that puts TGNN models onto the same ground and compares them comprehensively. To this end, we propose BenchTemp, a general benchmark for evaluating TGNN models on various workloads. BenchTemp provides a set of benchmark datasets so that different TGNN models can be fairly compared. Further, BenchTemp engineers a standard pipeline that unifies the TGNN evaluation. With BenchTemp, we extensively compare the representative TGNN models on different tasks (e.g., link prediction and node classification) and settings (transductive and inductive), w.r.t. bo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25193;&#23637;&#39044;&#27979;&#21040;&#29366;&#24577;&#21644;&#22238;&#25253;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20013; Transformer &#27169;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#36712;&#36857;&#34920;&#31034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2308.16379</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#30446;&#26631;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Decision Transformers for Offline Reinforcement Learning. (arXiv:2308.16379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16379
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25193;&#23637;&#39044;&#27979;&#21040;&#29366;&#24577;&#21644;&#22238;&#25253;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20013; Transformer &#27169;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#36712;&#36857;&#34920;&#31034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; (RL) &#32467;&#26500;&#21270;&#22320;&#20174;&#38745;&#24577;&#36712;&#36857;&#25968;&#25454;&#20013;&#25552;&#21462;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#23454;&#26102;&#29615;&#22659;&#20132;&#20114;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#31163;&#32447; RL &#26550;&#26500;&#21270;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#26159;&#21487;&#34892;&#30340;&#65292;&#20854;&#20013;&#21807;&#19968;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992; Transformer &#26550;&#26500;&#22522;&#20110;&#20808;&#21069;&#19978;&#19979;&#25991;&#39044;&#27979;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#21487;&#33021;&#21066;&#24369; Transformer &#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#20026;&#26368;&#20339;&#39044;&#27979;&#20998;&#37197;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#31163;&#32447; RL &#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#39044;&#27979;&#25193;&#23637;&#21040;&#29366;&#24577;&#21644;&#22238;&#25253;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#36712;&#36857;&#34920;&#31034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19968;&#20010;&#32570;&#38519;&#65292;&#36825;&#21487;&#33021;&#22312;&#24314;&#27169;&#29366;&#24577;&#21644;&#22238;&#25253;&#20998;&#24067;&#26102;&#20135;&#29983;&#19981;&#20934;&#30830;&#24615;&#12290;&#36825;&#26159;&#30001;&#20110;&#25805;&#20316;&#20998;&#24067;&#30340;&#19981;&#24179;&#28369;&#24615;&#36896;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) is structured to derive policies from static trajectory data without requiring real-time environment interactions. Recent studies have shown the feasibility of framing offline RL as a sequence modeling task, where the sole aim is to predict actions based on prior context using the transformer architecture. However, the limitation of this single task learning approach is its potential to undermine the transformer model's attention mechanism, which should ideally allocate varying attention weights across different tokens in the input context for optimal prediction. To address this, we reformulate offline RL as a multi-objective optimization problem, where the prediction is extended to states and returns. We also highlight a potential flaw in the trajectory representation used for sequence modeling, which could generate inaccuracies when modeling the state and return distributions. This is due to the non-smoothness of the action distribution within the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;</title><link>http://arxiv.org/abs/2308.16375</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#35843;&#26597;&#65306;&#25915;&#20987;&#12289;&#20445;&#25252;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16375
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#25913;&#21892;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#39640;&#25928;&#33021;&#34920;&#29616;&#65292;&#22914;&#20934;&#30830;&#24615;&#65292;&#32780;&#32570;&#20047;&#38544;&#31169;&#32771;&#34385;&#65292;&#36825;&#26159;&#29616;&#20195;&#31038;&#20250;&#38544;&#31169;&#25915;&#20987;&#30427;&#34892;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;GNNs&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22270;&#39046;&#22495;&#32570;&#20047;&#23545;&#25915;&#20987;&#21644;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24635;&#32467;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#25915;&#20987;&#12289;&#23545;GNNs&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#23457;&#26597;&#21487;&#29992;&#20110;&#20998;&#26512;/&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#31243;&#24207;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#20197;&#24314;&#31435;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
&lt;/p&gt;</description></item><item><title>SARATHI&#36890;&#36807;&#20351;&#29992;&#20998;&#22359;&#39044;&#22635;&#20805;&#21644;&#35299;&#30721;&#26368;&#22823;&#25209;&#22788;&#29702;&#30340;&#26041;&#24335;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;LLM&#25512;&#26029;&#20013;&#30340;&#39044;&#22635;&#20805;&#21644;&#35299;&#30721;&#38454;&#27573;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.16369</link><description>&lt;p&gt;
SARATHI&#65306;&#36890;&#36807;&#19982;&#20998;&#22359;&#39044;&#22635;&#20805;&#35299;&#30721;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#25928;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills. (arXiv:2308.16369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16369
&lt;/p&gt;
&lt;p&gt;
SARATHI&#36890;&#36807;&#20351;&#29992;&#20998;&#22359;&#39044;&#22635;&#20805;&#21644;&#35299;&#30721;&#26368;&#22823;&#25209;&#22788;&#29702;&#30340;&#26041;&#24335;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;LLM&#25512;&#26029;&#20013;&#30340;&#39044;&#22635;&#20805;&#21644;&#35299;&#30721;&#38454;&#27573;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65306;&#39044;&#22635;&#20805;&#38454;&#27573;&#22788;&#29702;&#36755;&#20837;&#25552;&#31034;&#65292;&#35299;&#30721;&#38454;&#27573;&#25353;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#36755;&#20986;&#20196;&#29260;&#12290;&#34429;&#28982;&#39044;&#22635;&#20805;&#38454;&#27573;&#22312;&#23567;&#25209;&#37327;&#22823;&#23567;&#19979;&#26377;&#25928;&#21033;&#29992;&#20102;GPU&#35745;&#31639;&#33021;&#21147;&#65292;&#20294;&#35299;&#30721;&#38454;&#27573;&#27599;&#27425;&#35831;&#27714;&#21482;&#29983;&#25104;&#19968;&#20010;&#20196;&#29260;&#65292;&#23548;&#33268;&#35745;&#31639;&#21033;&#29992;&#29575;&#36739;&#20302;&#12290;&#20351;&#29992;&#31649;&#36947;&#24182;&#34892;&#26102;&#65292;&#39044;&#22635;&#20805;&#21644;&#35299;&#30721;&#26102;&#38388;&#30340;&#21464;&#21270;&#36824;&#20250;&#23548;&#33268;&#24494;&#25209;&#37327;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#25928;&#29575;&#19978;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SARATHI&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;SARATHI&#20351;&#29992;&#20998;&#22359;&#39044;&#22635;&#20805;&#65292;&#23558;&#39044;&#22635;&#20805;&#35831;&#27714;&#20998;&#25104;&#30456;&#31561;&#22823;&#23567;&#30340;&#22359;&#65292;&#24182;&#20351;&#29992;&#35299;&#30721;&#26368;&#22823;&#25209;&#22788;&#29702;&#23558;&#19968;&#20010;&#39044;&#22635;&#20805;&#22359;&#26500;&#36896;&#25104;&#19968;&#20010;&#25209;&#27425;&#65292;&#24182;&#29992;&#35299;&#30721;&#22635;&#20805;&#20854;&#20313;&#30340;&#25554;&#27133;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#39044;&#22635;&#20805;&#22359;&#21487;&#20805;&#20998;&#21033;&#29992;GPU&#35745;&#31639;&#33021;&#21147;&#65292;&#32780;&#35299;&#30721;&#35831;&#27714;&#21017;&#8220;&#25645;&#20415;&#36710;&#8221;&#65292;&#25104;&#26412;&#27604;&#20165;&#35299;&#30721;&#30340;&#25209;&#27425;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20998;&#22359;&#39044;&#22635;&#20805;&#20351;&#24471;&#32422;&#26463;&#36866;&#24212;&#20102;GPU&#21644;CPU&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#26029;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) inference consists of two distinct phases prefill phase which processes the input prompt and decode phase which generates output tokens autoregressively. While the prefill phase effectively saturates GPU compute at small batch sizes, the decode phase results in low compute utilization as it generates one token at a time per request. The varying prefill and decode times also lead to imbalance across micro-batches when using pipeline parallelism, resulting in further inefficiency due to bubbles.  We present SARATHI to address these challenges. SARATHI employs chunked-prefills, which splits a prefill request into equal sized chunks, and decode-maximal batching, which constructs a batch using a single prefill chunk and populates the remaining slots with decodes. During inference, the prefill chunk saturates GPU compute, while the decode requests 'piggyback' and cost up to an order of magnitude less compared to a decode-only batch. Chunked-prefills allows constr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38750;&#20984;&#38750;&#20809;&#28369;&#38750;Lipschitz&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#36817;&#31471;&#27425;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#36882;&#24402;&#20851;&#31995;&#29992;&#20110; Moreau&#21253;&#32476;&#30340;&#24314;&#31435;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#38543;&#26426;&#27425;&#26799;&#24230;&#19978;&#30028;&#26465;&#20214;&#65292;&#24182;&#20998;&#26512;&#20102;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.16362</link><description>&lt;p&gt;
&#35299;&#20915;&#22797;&#21512;&#38750;&#20984;&#38750;&#20809;&#28369;&#38750;Lipschitz&#20989;&#25968;&#30340;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#32479;&#19968;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Unified Analysis for the Subgradient Methods Minimizing Composite Nonconvex, Nonsmooth and Non-Lipschitz Functions. (arXiv:2308.16362v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38750;&#20984;&#38750;&#20809;&#28369;&#38750;Lipschitz&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#36817;&#31471;&#27425;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#36882;&#24402;&#20851;&#31995;&#29992;&#20110; Moreau&#21253;&#32476;&#30340;&#24314;&#31435;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#38543;&#26426;&#27425;&#26799;&#24230;&#19978;&#30028;&#26465;&#20214;&#65292;&#24182;&#20998;&#26512;&#20102;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#36817;&#31471;&#27425;&#26799;&#24230;&#26041;&#27861;(Prox-SubGrad)&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;Lipschitz&#36830;&#32493;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#27425;&#26799;&#24230;&#19978;&#30028;&#21450;&#20854;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#36825;&#20123;&#19978;&#30028;&#26465;&#20214;&#24314;&#31435;&#20102;&#24369;&#20984;&#20248;&#21270;Moreau&#21253;&#32476;&#30340;&#19968;&#20123;&#32479;&#19968;&#36882;&#24402;&#20851;&#31995;&#12290;&#36825;&#31181;&#32479;&#19968;&#26041;&#26696;&#31616;&#21270;&#24182;&#32479;&#19968;&#20102;&#24314;&#31435;Prox-SubGrad&#25910;&#25947;&#36895;&#29575;&#30340;&#35777;&#26126;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;Lipschitz&#36830;&#32493;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#38543;&#26426;&#27425;&#26799;&#24230;&#19978;&#30028;&#26465;&#20214;&#65292;&#24182;&#20026;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;(Sto-SubGrad)&#35299;&#20915;&#38750;Lipschitz&#38750;&#20809;&#28369;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#24314;&#31435;&#20102;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#26080;&#38656;Lipschitz&#36830;&#32493;&#24615;&#30340;&#24369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a proximal subgradient method (Prox-SubGrad) for solving nonconvex and nonsmooth optimization problems without assuming Lipschitz continuity conditions. A number of subgradient upper bounds and their relationships are presented. By means of these upper bounding conditions, we establish some uniform recursive relations for the Moreau envelopes for weakly convex optimization. This uniform scheme simplifies and unifies the proof schemes to establish rate of convergence for Prox-SubGrad without assuming Lipschitz continuity. We present a novel convergence analysis in this context. Furthermore, we propose some new stochastic subgradient upper bounding conditions and establish convergence and iteration complexity rates for the stochastic subgradient method (Sto-SubGrad) to solve non-Lipschitz and nonsmooth stochastic optimization problems. In particular, for both deterministic and stochastic subgradient methods on weakly convex optimization problems without Lipschitz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#34920;&#24773;&#31526;&#21495;&#22312;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340;&#20351;&#29992;&#23545;&#24320;&#21457;&#32773;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34920;&#24773;&#31526;&#21495;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#38382;&#39064;&#30340;&#35299;&#20915;&#26102;&#38388;&#65292;&#24182;&#21560;&#24341;&#26356;&#22810;&#29992;&#25143;&#21442;&#19982;&#12290;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#25928;&#26524;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.16360</link><description>&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#20419;&#36827;&#20102;GitHub&#19978;&#24320;&#21457;&#32773;&#30340;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Emoji Promotes Developer Participation and Issue Resolution on GitHub. (arXiv:2308.16360v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#34920;&#24773;&#31526;&#21495;&#22312;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340;&#20351;&#29992;&#23545;&#24320;&#21457;&#32773;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34920;&#24773;&#31526;&#21495;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#38382;&#39064;&#30340;&#35299;&#20915;&#26102;&#38388;&#65292;&#24182;&#21560;&#24341;&#26356;&#22810;&#29992;&#25143;&#21442;&#19982;&#12290;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#25928;&#26524;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#30123;&#24773;&#26399;&#38388;&#36828;&#31243;&#24037;&#20316;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#35768;&#22810;&#20154;&#23545;&#36828;&#31243;&#24037;&#20316;&#30340;&#20302;&#25928;&#29575;&#34920;&#31034;&#25285;&#24551;&#12290;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#27807;&#36890;&#20013;&#32570;&#20047;&#38754;&#37096;&#34920;&#24773;&#21644;&#32930;&#20307;&#35821;&#35328;&#31561;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#36825;&#22952;&#30861;&#20102;&#26377;&#25928;&#30340;&#27807;&#36890;&#65292;&#24182;&#23545;&#24037;&#20316;&#32467;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20316;&#20026;&#26367;&#20195;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20351;&#29992;&#30340;&#34920;&#24773;&#31526;&#21495;&#22312;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#20063;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#34920;&#24773;&#31526;&#21495;&#30340;&#20351;&#29992;&#22914;&#20309;&#24433;&#21709;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#24320;&#21457;&#32773;&#30340;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;GitHub&#30340;&#19968;&#20010;&#19968;&#24180;&#21608;&#26399;&#20869;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#34913;&#37327;&#34920;&#24773;&#31526;&#21495;&#23545;&#38382;&#39064;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#25511;&#21046;&#38382;&#39064;&#20869;&#23481;&#12289;&#20179;&#24211;&#21644;&#20316;&#32773;&#20449;&#24687;&#31561;&#28151;&#28102;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#34920;&#24773;&#31526;&#21495;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#38382;&#39064;&#30340;&#35299;&#20915;&#26102;&#38388;&#65292;&#24182;&#21560;&#24341;&#26356;&#22810;&#29992;&#25143;&#21442;&#19982;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#30340;&#24322;&#36136;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although remote working is increasingly adopted during the pandemic, many are concerned by the low-efficiency in the remote working. Missing in text-based communication are non-verbal cues such as facial expressions and body language, which hinders the effective communication and negatively impacts the work outcomes. Prevalent on social media platforms, emojis, as alternative non-verbal cues, are gaining popularity in the virtual workspaces well. In this paper, we study how emoji usage influences developer participation and issue resolution in virtual workspaces. To this end, we collect GitHub issues for a one-year period and apply causal inference techniques to measure the causal effect of emojis on the outcome of issues, controlling for confounders such as issue content, repository, and author information. We find that emojis can significantly reduce the resolution time of issues and attract more user participation. We also compare the heterogeneous effect on different types of issue
&lt;/p&gt;</description></item><item><title>ToddlerBERTa&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.16336</link><description>&lt;p&gt;
ToddlerBERTa: &#21033;&#29992;BabyBERTa&#36827;&#34892;&#35821;&#27861;&#23398;&#20064;&#21644;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding. (arXiv:2308.16336v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16336
&lt;/p&gt;
&lt;p&gt;
ToddlerBERTa&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ToddlerBERTa&#65292;&#36825;&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20116;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#27169;&#22411;&#26469;&#25506;&#32034;&#20854;&#33021;&#21147;&#12290;&#22312;BLiMP&#65292;SuperGLUE&#65292;MSGS&#21644;BabyLM&#25361;&#25112;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#22823;&#37327;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;ToddlerBERTa&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#21363;&#20351;&#26159;&#22312;&#21333;&#21477;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#19982;&#21033;&#29992;&#26356;&#24191;&#27867;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22522;&#32447;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#25968;&#25454;&#21033;&#29992;&#25552;&#20379;&#20102;&#27934;&#23519;&#65292;&#24182;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ToddlerBERTa, a BabyBERTa-like language model, exploring its capabilities through five different models with varied hyperparameters. Evaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the BabyLM challenge, we find that smaller models can excel in specific tasks, while larger models perform well with substantial data. Despite training on a smaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling the state-of-the-art RoBERTa-base. The model showcases robust language understanding, even with single-sentence pretraining, and competes with baselines that leverage broader contextual information. Our work provides insights into hyperparameter choices, and data utilization, contributing to the advancement of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#23398;&#20064;&#22312;Lie&#32676;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#26500;&#36896;$G$-&#19981;&#21464;&#30340;&#25289;&#26684;&#26391;&#26085;&#23376;&#27969;&#24418;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#19982;&#21407;&#22987;&#31995;&#32479;&#30456;&#21516;&#30340;&#20445;&#23432;&#37327;&#65292;&#25552;&#20379;&#20102;&#26356;&#24544;&#23454;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16331</link><description>&lt;p&gt;
&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#23545;&#31216;&#24615;&#20445;&#25345;&#65306;&#27169;&#25311;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symmetry Preservation in Hamiltonian Systems: Simulation and Learning. (arXiv:2308.16331v1 [math-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#23398;&#20064;&#22312;Lie&#32676;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#26500;&#36896;$G$-&#19981;&#21464;&#30340;&#25289;&#26684;&#26391;&#26085;&#23376;&#27969;&#24418;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#19982;&#21407;&#22987;&#31995;&#32479;&#30456;&#21516;&#30340;&#20445;&#23432;&#37327;&#65292;&#25552;&#20379;&#20102;&#26356;&#24544;&#23454;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#23398;&#20064;&#22312;Lie&#32676;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;&#36825;&#24847;&#21619;&#30528;&#24050;&#30693;&#19968;&#32452;&#23545;&#31216;&#24615;&#27839;&#30528;&#31995;&#32479;&#20316;&#29992;&#65292;&#23562;&#37325;&#20854;&#21160;&#21147;&#23398;&#65292;&#24182;&#19988;&#26681;&#25454;Noether&#23450;&#29702;&#65292;&#20445;&#23432;&#37327;&#20250;&#34987;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#26500;&#36896;$G$-&#19981;&#21464;&#30340;&#25289;&#26684;&#26391;&#26085;&#23376;&#27969;&#24418;&#26469;&#27169;&#25311;&#21644;&#23398;&#20064;&#24863;&#20852;&#36259;&#30340;&#26144;&#23556;&#65292;&#36825;&#22312;&#36763;&#20960;&#20309;&#23398;&#20013;&#26159;&#20851;&#38190;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#27169;&#25311;/&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#20063;&#20445;&#25345;&#19982;&#21407;&#22987;&#31995;&#32479;&#30456;&#21516;&#30340;&#20445;&#23432;&#37327;&#65292;&#36825;&#20351;&#24471;&#23427;&#27604;&#38750;&#23545;&#31216;&#24847;&#35782;&#26041;&#27861;&#26356;&#24544;&#23454;&#22320;&#27169;&#25311;&#21407;&#22987;&#21160;&#21147;&#23398;&#65292;&#24182;&#19988;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#38750;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35774;&#32622;&#33021;&#22815;&#27169;&#25311;/&#23398;&#20064;&#19981;&#20165;&#21704;&#23494;&#39039;&#27969;&#65292;&#36824;&#21253;&#25324;&#20219;&#20309;Lie&#32676;&#31561;&#21464;&#36763;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#21033;&#29992;&#20102;&#20851;&#38190;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
This work presents a general geometric framework for simulating and learning the dynamics of Hamiltonian systems that are invariant under a Lie group of transformations. This means that a group of symmetries is known to act on the system respecting its dynamics and, as a consequence, Noether's Theorem, conserved quantities are observed. We propose to simulate and learn the mappings of interest through the construction of $G$-invariant Lagrangian submanifolds, which are pivotal objects in symplectic geometry. A notable property of our constructions is that the simulated/learned dynamics also preserves the same conserved quantities as the original system, resulting in a more faithful surrogate of the original dynamics than non-symmetry aware methods, and in a more accurate predictor of non-observed trajectories. Furthermore, our setting is able to simulate/learn not only Hamiltonian flows, but any Lie group-equivariant symplectic transformation. Our designs leverage pivotal techniques an
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#33258;2014&#24180;&#35806;&#29983;&#20197;&#26469;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#26159;&#19968;&#31181;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;GANs&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;GAN&#19982;&#32479;&#35745;&#23398;&#12289;&#20449;&#24687;&#35770;&#21644;&#20248;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.16316</link><description>&lt;p&gt;
&#21313;&#24180;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#32508;&#36848;&#65306;&#26368;&#26032;&#25216;&#26415;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art. (arXiv:2308.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16316
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#33258;2014&#24180;&#35806;&#29983;&#20197;&#26469;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#26159;&#19968;&#31181;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;GANs&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;GAN&#19982;&#32479;&#35745;&#23398;&#12289;&#20449;&#24687;&#35770;&#21644;&#20248;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2014&#24180;&#35806;&#29983;&#20197;&#26469;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36805;&#36895;&#25104;&#20026;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#24212;&#29992;&#39046;&#22495;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;GANs&#30001;&#19968;&#20010;&#21028;&#21035;&#32593;&#32476;&#21644;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#32452;&#25104;&#65292;&#23427;&#20204;&#22312;&#19968;&#20010;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#20013;&#30456;&#20114;&#23545;&#25239;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#12290;&#22312;2018&#24180;2&#26376;&#65292;GAN&#34987;&#39532;&#33832;&#35832;&#22622;&#24030;&#31185;&#25216;&#35780;&#35770;&#21457;&#24067;&#30340;&#8220;&#20840;&#29699;&#21313;&#22823;&#31361;&#30772;&#24615;&#25216;&#26415;&#21015;&#34920;&#8221;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;&#22810;&#24180;&#26469;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#20808;&#36827;&#25216;&#26415;&#65292;&#23548;&#33268;&#20102;&#21508;&#31181;GAN&#21464;&#31181;&#30340;&#20016;&#23500;&#12290;&#36825;&#31687;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;GANs&#30340;&#27010;&#36848;&#65292;&#24635;&#32467;&#20102;&#24191;&#27867;&#35748;&#21487;&#30340;&#21464;&#31181;&#30340;&#28508;&#22312;&#26550;&#26500;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#24212;&#29992;&#39046;&#22495;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#65292;&#25506;&#32034;&#20102;&#23545;&#25239;&#32593;&#32476;&#19982;&#32479;&#35745;&#23398;&#12289;&#20449;&#24687;&#35770;&#21644;&#20248;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#28145;&#21051;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their inception in 2014, Generative Adversarial Networks (GANs) have rapidly emerged as powerful tools for generating realistic and diverse data across various domains, including computer vision and other applied areas. Consisting of a discriminative network and a generative network engaged in a Minimax game, GANs have revolutionized the field of generative modeling. In February 2018, GAN secured the leading spot on the ``Top Ten Global Breakthrough Technologies List'' issued by the Massachusetts Science and Technology Review. Over the years, numerous advancements have been proposed, leading to a rich array of GAN variants, such as conditional GAN, Wasserstein GAN, CycleGAN, and StyleGAN, among many others. This survey aims to provide a general overview of GANs, summarizing the latent architecture, validation metrics, and application areas of the most widely recognized variants. We also delve into recent theoretical developments, exploring the profound connection between the adve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#30005;&#20449;&#32593;&#32476;KPI&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#20998;&#31867;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#27169;&#25311;&#22120;&#12289;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#21644;&#20998;&#31867;&#27169;&#22411;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23545;&#32593;&#32476;KPI&#20013;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#21644;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.16279</link><description>&lt;p&gt;
&#30005;&#20449;&#32593;&#32476;KPI&#26102;&#38388;&#24207;&#21015;&#20013;&#24322;&#24120;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Anomalies in Telecommunication Network KPI Time Series. (arXiv:2308.16279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#30005;&#20449;&#32593;&#32476;KPI&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#20998;&#31867;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#27169;&#25311;&#22120;&#12289;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#21644;&#20998;&#31867;&#27169;&#22411;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23545;&#32593;&#32476;KPI&#20013;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#23545;&#33258;&#21160;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#32593;&#32476;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65288;KPI&#65289;&#19978;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#30340;&#20998;&#31867;&#21364;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#65292;&#23548;&#33268;&#32570;&#20047;&#20851;&#20110;&#24322;&#24120;&#29305;&#24449;&#21644;&#20998;&#31867;&#36807;&#31243;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#24322;&#24120;&#20998;&#31867;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20551;&#35774;&#24322;&#24120;&#20998;&#31867;&#22120;&#21644;&#26816;&#27979;&#22120;&#20316;&#20026;&#29420;&#31435;&#30340;&#23454;&#20307;&#65292;&#20801;&#35768;&#23545;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#29420;&#31435;&#22788;&#29702;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#65306;&#65288;1&#65289;&#24320;&#21457;&#19968;&#20010;&#29983;&#25104;&#31867;&#20284;&#20110;&#23454;&#38469;&#32593;&#32476;KPI&#34892;&#20026;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#25311;&#22120;&#65307;&#65288;2&#65289;&#26500;&#24314;&#19968;&#20010;&#26816;&#27979;&#27169;&#22411;&#65292;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#65307;&#65288;3&#65289;&#26500;&#24314;&#20934;&#30830;&#23558;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#31867;&#21035;&#30340;&#20998;&#31867;&#27169;&#22411;&#65307;&#65288;4&#65289;&#35780;&#20272;&#20998;&#31867;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing complexity and scale of telecommunication networks have led to a growing interest in automated anomaly detection systems. However, the classification of anomalies detected on network Key Performance Indicators (KPI) has received less attention, resulting in a lack of information about anomaly characteristics and classification processes. To address this gap, this paper proposes a modular anomaly classification framework. The framework assumes separate entities for the anomaly classifier and the detector, allowing for a distinct treatment of anomaly detection and classification tasks on time series. The objectives of this study are (1) to develop a time series simulator that generates synthetic time series resembling real-world network KPI behavior, (2) to build a detection model to identify anomalies in the time series, (3) to build classification models that accurately categorize detected anomalies into predefined classes (4) to evaluate the classification framework per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#35270;&#35273;Transformer&#24182;&#21457;&#29616;&#20854;&#20855;&#26377;&#25552;&#21462;&#31283;&#20581;&#21644;&#34394;&#20551;&#29305;&#24449;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#21098;&#26525;&#34394;&#20551;&#29305;&#24449;&#23545;&#24212;&#30340;&#27880;&#24847;&#22836;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#20351;&#29992;"oracle&#36873;&#25321;"&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#21152;&#23398;&#20064;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#21644;&#20114;&#34917;&#24615;&#65292;&#36890;&#36807;&#40723;&#21169;&#27880;&#24847;&#22836;&#36755;&#20837;&#26799;&#24230;&#30340;&#27491;&#20132;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#31181;&#22686;&#24378;&#29305;&#24449;&#22810;&#26679;&#24615;&#21644;&#21098;&#26525;&#19981;&#33391;&#27880;&#24847;&#22836;&#30340;&#26041;&#27861;&#22312;&#35786;&#26029;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16274</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#26679;&#21270;&#29305;&#24449;&#20197;&#25913;&#21892;&#35270;&#35273;Transformer&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Features in Vision Transformers for Improved Generalization. (arXiv:2308.16274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#35270;&#35273;Transformer&#24182;&#21457;&#29616;&#20854;&#20855;&#26377;&#25552;&#21462;&#31283;&#20581;&#21644;&#34394;&#20551;&#29305;&#24449;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#21098;&#26525;&#34394;&#20551;&#29305;&#24449;&#23545;&#24212;&#30340;&#27880;&#24847;&#22836;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#20351;&#29992;"oracle&#36873;&#25321;"&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#21152;&#23398;&#20064;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#21644;&#20114;&#34917;&#24615;&#65292;&#36890;&#36807;&#40723;&#21169;&#27880;&#24847;&#22836;&#36755;&#20837;&#26799;&#24230;&#30340;&#27491;&#20132;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#31181;&#22686;&#24378;&#29305;&#24449;&#22810;&#26679;&#24615;&#21644;&#21098;&#26525;&#19981;&#33391;&#27880;&#24847;&#22836;&#30340;&#26041;&#27861;&#22312;&#35786;&#26029;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#24449;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#20016;&#23500;&#30340;&#39044;&#27979;&#20449;&#21495;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33030;&#24369;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#30740;&#31350;&#20102;&#35270;&#35273;Transformer(ViTs)&#65292;&#21457;&#29616;&#23427;&#20204;&#20542;&#21521;&#20110;&#25552;&#21462;&#20855;&#26377;&#19981;&#21516;&#27880;&#24847;&#22836;&#30340;&#20581;&#22766;&#21644;&#34394;&#20551;&#29305;&#24449;&#12290;&#36890;&#36807;&#21098;&#26525;&#23545;&#24212;&#20110;&#34394;&#20551;&#29305;&#24449;&#30340;&#27880;&#24847;&#22836;&#65292;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#20351;&#29992;"oracle&#36873;&#25321;"&#35777;&#26126;&#23427;&#20204;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#27880;&#24847;&#22836;&#36755;&#20837;&#26799;&#24230;&#30340;&#27491;&#20132;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#21644;&#20114;&#34917;&#24615;&#12290;&#36890;&#36807;&#22686;&#24378;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#21644;&#21098;&#26525;&#19981;&#33391;&#27880;&#24847;&#22836;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#35786;&#26029;&#22522;&#20934;&#27979;&#35797;&#20013;&#65288;MNIST-CIFAR&#65292;&#27700;&#40479;&#65289;&#30340;&#25913;&#36827;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models often rely only on a small set of features even when there is a rich set of predictive signals in the training data. This makes models brittle and sensitive to distribution shifts. In this work, we first examine vision transformers (ViTs) and find that they tend to extract robust and spurious features with distinct attention heads. As a result of this modularity, their performance under distribution shifts can be significantly improved at test time by pruning heads corresponding to spurious features, which we demonstrate using an "oracle selection" on validation data. Second, we propose a method to further enhance the diversity and complementarity of the learned features by encouraging orthogonality of the attention heads' input gradients. We observe improved out-of-distribution performance on diagnostic benchmarks (MNIST-CIFAR, Waterbirds) as a consequence of the enhanced diversity of features and the pruning of undesirable heads.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#20540;&#26041;&#27861;&#35299;&#20915;&#20102;&#20855;&#26377;&#20998;&#25968;Laplacian&#30340;&#26925;&#22278;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#25968;&#20540;&#31034;&#20363;&#26469;&#39564;&#35777;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.16272</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#20540;&#26041;&#27861;&#35299;&#20915;&#20998;&#25968;Laplacian&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A numerical approach for the fractional Laplacian via deep neural networks. (arXiv:2308.16272v1 [math.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#20540;&#26041;&#27861;&#35299;&#20915;&#20102;&#20855;&#26377;&#20998;&#25968;Laplacian&#30340;&#26925;&#22278;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#25968;&#20540;&#31034;&#20363;&#26469;&#39564;&#35777;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;$\mathbb{R}^d$&#30340;&#26377;&#30028;&#20984;&#22495;$D$&#19978;&#20855;&#26377;Dirichlet&#36793;&#30028;&#26465;&#20214;&#30340;&#20998;&#25968;&#26925;&#22278;&#38382;&#39064;&#65292;&#20854;&#20013;$d \geq 2$&#12290;&#25105;&#20204;&#37319;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20998;&#25968;&#38382;&#39064;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#20010;&#25968;&#20540;&#31034;&#20363;&#26469;&#27979;&#35797;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#30740;&#31350;&#20102;&#27599;&#20010;&#31034;&#20363;&#20013;$\alpha \in (1,2)$&#21644;$d \geq 2$&#30340;&#22810;&#20010;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the fractional elliptic problem with Dirichlet boundary conditions on a bounded and convex domain $D$ of $\mathbb{R}^d$, with $d \geq 2$. In this paper, we perform a stochastic gradient descent algorithm that approximates the solution of the fractional problem via Deep Neural Networks. Additionally, we provide four numerical examples to test the efficiency of the algorithm, and each example will be studied for many values of $\alpha \in (1,2)$ and $d \geq 2$.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;CRATE&#26550;&#26500;&#30340;&#26368;&#31616;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#19979;&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#20998;&#21106;&#29305;&#24615;&#30340;&#20986;&#29616;&#65292;&#26080;&#38656;&#22797;&#26434;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16271</link><description>&lt;p&gt;
&#26368;&#31616;&#30333;&#30418;Transformer&#20013;&#30340;&#20998;&#21106;&#20986;&#29616;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Emergence of Segmentation with Minimalistic White-Box Transformers. (arXiv:2308.16271v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;CRATE&#26550;&#26500;&#30340;&#26368;&#31616;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#19979;&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#20998;&#21106;&#29305;&#24615;&#30340;&#20986;&#29616;&#65292;&#26080;&#38656;&#22797;&#26434;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;Transformer&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#35832;&#22810;&#19979;&#28216;&#24212;&#29992;&#65288;&#22914;&#20998;&#21106;&#21644;&#26816;&#27979;&#65289;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;DINO&#31561;&#33258;&#30417;&#30563;&#26041;&#27861;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#20986;&#29616;&#20102;&#20998;&#21106;&#29305;&#24615;&#65292;&#20294;&#36890;&#36807;&#22312;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21364;&#27809;&#26377;&#20986;&#29616;&#36825;&#31181;&#29305;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20998;&#21106;&#26159;&#21542;&#20165;&#36890;&#36807;&#22797;&#26434;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#22312;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#65292;&#25110;&#32773;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#27169;&#22411;&#26550;&#26500;&#21487;&#20197;&#22312;&#26356;&#24191;&#27867;&#30340;&#26465;&#20214;&#19979;&#23454;&#29616;&#30456;&#21516;&#30340;&#20986;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#37319;&#29992;CRATE&#36825;&#31181;&#34987;&#31216;&#20026;&#30333;&#30418;Transformer&#30340;&#26550;&#26500;&#26102;&#65292;&#35813;&#26550;&#26500;&#26126;&#30830;&#22320;&#23545;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#20302;&#32500;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#26368;&#31616;&#21270;&#30340;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#24050;&#32463;&#23454;&#29616;&#20102;&#25972;&#20307;&#21644;&#23616;&#37096;&#23618;&#38754;&#30340;&#20998;&#21106;&#29305;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-like models for vision tasks have recently proven effective for a wide range of downstream applications such as segmentation and detection. Previous works have shown that segmentation properties emerge in vision transformers (ViTs) trained using self-supervised methods such as DINO, but not in those trained on supervised classification tasks. In this study, we probe whether segmentation emerges in transformer-based models solely as a result of intricate self-supervised learning mechanisms, or if the same emergence can be achieved under much broader conditions through proper design of the model architecture. Through extensive experimental results, we demonstrate that when employing a white-box transformer-like architecture known as CRATE, whose design explicitly models and pursues low-dimensional structures in the data distribution, segmentation properties, at both the whole and parts levels, already emerge with a minimalistic supervised training recipe. Layer-wise finer-gra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26448;&#26009;&#20449;&#24687;&#23398;&#21464;&#21387;&#22120;&#65288;MatInFormer&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26230;&#20307;&#23398;&#35821;&#27861;&#21644;&#24341;&#20837;MOFs&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#26448;&#26009;&#24615;&#33021;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.16259</link><description>&lt;p&gt;
&#26448;&#26009;&#20449;&#24687;&#23398;&#21464;&#21387;&#22120;&#65306;&#19968;&#31181;&#29992;&#20110;&#21487;&#35299;&#37322;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction. (arXiv:2308.16259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26448;&#26009;&#20449;&#24687;&#23398;&#21464;&#21387;&#22120;&#65288;MatInFormer&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26230;&#20307;&#23398;&#35821;&#27861;&#21644;&#24341;&#20837;MOFs&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#26448;&#26009;&#24615;&#33021;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20998;&#23376;&#24314;&#27169;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#25105;&#20204;&#30340;&#27169;&#22411;&#26448;&#26009;&#20449;&#24687;&#23398;&#21464;&#21387;&#22120;(MatInFormer)&#65292;&#23558;LLMs&#30340;&#36825;&#31181;&#33539;&#24335;&#25193;&#23637;&#21040;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#31354;&#38388;&#32676;&#20449;&#24687;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#23398;&#20064;&#20102;&#26230;&#20307;&#23398;&#30340;&#35821;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;MatInFormer&#30340;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807; incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs)&#12290;&#36890;&#36807;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#23646;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#20248;&#20808;&#32771;&#34385;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#27169;&#22411;&#22312;14&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#20013;&#32463;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#20854;&#22312;&#36890;&#36807;&#20934;&#30830;&#30340;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;&#36827;&#34892;&#39640;&#36890;&#37327;&#31579;&#36873;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the remarkable capabilities of large language models (LLMs) have been illustrated across a variety of research domains such as natural language processing, computer vision, and molecular modeling. We extend this paradigm by utilizing LLMs for material property prediction by introducing our model Materials Informatics Transformer (MatInFormer). Specifically, we introduce a novel approach that involves learning the grammar of crystallography through the tokenization of pertinent space group information. We further illustrate the adaptability of MatInFormer by incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs). Through attention visualization, we uncover the key features that the model prioritizes during property prediction. The effectiveness of our proposed model is empirically validated across 14 distinct datasets, hereby underscoring its potential for high throughput screening through accurate material property prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16245</link><description>&lt;p&gt;
&#22238;&#24402;&#38382;&#39064;&#30340;&#26657;&#20934;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#24120;&#26159;&#29616;&#20195;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#22522;&#20110;AI&#30340;DSS&#20013;&#20351;&#29992;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#29702;&#30001;&#30340;AI&#31995;&#32479;&#12290;XAI&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20010;&#21035;&#39044;&#27979;&#30340;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#21363;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#26080;&#27861;&#37327;&#21270;&#19982;&#29305;&#24449;&#37325;&#35201;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;Calibrated Explanations&#65288;CE&#65289;&#30340;&#25193;&#23637;&#65292;&#20043;&#21069;&#21482;&#25903;&#25345;&#20998;&#31867;&#65292;&#29616;&#22312;&#25903;&#25345;&#26631;&#20934;&#22238;&#24402;&#21644;&#27010;&#29575;&#22238;&#24402;&#65292;&#21363;&#30446;&#26631;&#36229;&#36807;&#20219;&#24847;&#38408;&#20540;&#30340;&#27010;&#29575;&#12290;&#22238;&#24402;&#38382;&#39064;&#30340;&#25193;&#23637;&#20445;&#30041;&#20102;CE&#30340;&#25152;&#26377;&#20248;&#28857;&#65292;&#20363;&#22914;&#23558;&#24213;&#23618;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.16215</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20002;&#22833;&#29575;&#35270;&#39057;&#21387;&#32553;&#36890;&#24120;&#29992;&#20110;&#20256;&#36755;&#21644;&#23384;&#20648;&#35270;&#39057;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#36827;&#38454;&#65288;&#31070;&#32463;&#65289;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#32479;&#19968;&#35270;&#39057;&#32534;&#30721;&#22120;&#65288;&#22914;H.264&#25110;H.265&#65289;&#20173;&#28982;&#26159;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#22312;&#38754;&#23545;&#21160;&#24577;&#32593;&#32476;&#24102;&#23485;&#26465;&#20214;&#30340;&#35270;&#39057;&#20256;&#36755;&#20013;&#65292;&#35270;&#39057;&#32534;&#30721;&#22120;&#38656;&#35201;&#36866;&#24212;&#38750;&#24120;&#19981;&#21516;&#30340;&#21387;&#32553;&#24378;&#24230;&#12290;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#22686;&#24378;&#32534;&#35299;&#30721;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65292;&#20197;&#28385;&#36275;&#24102;&#23485;&#38480;&#21046;&#24182;&#23613;&#37327;&#20943;&#23569;&#35270;&#39057;&#22833;&#30495;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#35270;&#39057;&#32534;&#30721;&#22120;&#21450;&#20854;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#26159;&#20026;&#20102;&#26368;&#23567;&#21270;&#20154;&#31867;&#36136;&#37327;&#35780;&#20272;&#32780;&#24320;&#21457;&#30340;&#65292;&#21364;&#27809;&#26377;&#32771;&#34385;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#19981;&#30772;&#22351;&#29616;&#26377;&#30340;&#26631;&#20934;&#21270;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;&#65288;&#35821;&#20041;&#20998;&#21106;...
&lt;/p&gt;
&lt;p&gt;
Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#30340;&#21453;&#21512;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#31163;&#25955;&#20998;&#24067;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#36817;&#20284;&#65292;&#30452;&#25509;&#29983;&#25104;&#21487;&#33021;&#30340;&#21069;&#20307;&#20998;&#23376;&#65292;&#20026;&#21453;&#21512;&#25104;&#35268;&#21010;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.16212</link><description>&lt;p&gt;
RetroBridge: &#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#36827;&#34892;&#21453;&#21512;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
RetroBridge: Modeling Retrosynthesis with Markov Bridges. (arXiv:2308.16212v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#30340;&#21453;&#21512;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#31163;&#25955;&#20998;&#24067;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#36817;&#20284;&#65292;&#30452;&#25509;&#29983;&#25104;&#21487;&#33021;&#30340;&#21069;&#20307;&#20998;&#23376;&#65292;&#20026;&#21453;&#21512;&#25104;&#35268;&#21010;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21512;&#25104;&#35268;&#21010;&#26159;&#21270;&#23398;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#25361;&#25112;&#65292;&#26088;&#22312;&#20174;&#24066;&#21806;&#36215;&#22987;&#26448;&#26009;&#35774;&#35745;&#21453;&#24212;&#36335;&#24452;&#21040;&#30446;&#26631;&#20998;&#23376;&#12290;&#22810;&#27493;&#21453;&#21512;&#25104;&#35268;&#21010;&#20013;&#30340;&#27599;&#19968;&#27493;&#37117;&#38656;&#35201;&#20934;&#30830;&#39044;&#27979;&#32473;&#23450;&#30446;&#26631;&#20998;&#23376;&#30340;&#21487;&#33021;&#21069;&#20307;&#20998;&#23376;&#65292;&#24182;&#32473;&#20986;&#32622;&#20449;&#24230;&#20272;&#35745;&#20197;&#25351;&#23548;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#21333;&#27493;&#21453;&#21512;&#25104;&#35268;&#21010;&#24314;&#27169;&#20026;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#65292;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#36817;&#20284;&#20004;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#31163;&#25955;&#20998;&#24067;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;&#32806;&#21512;&#25968;&#25454;&#28857;&#36827;&#34892;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26725;&#30340;&#27010;&#24565;&#65292;&#21363;&#20197;&#20854;&#31471;&#28857;&#20026;&#21021;&#22987;&#28857;&#22266;&#23450;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#19982;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#19981;&#38656;&#35201;&#20316;&#20026;&#37319;&#26679;&#20195;&#29702;&#30340;&#21487;&#36861;&#36394;&#22122;&#22768;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20351;&#29992;&#36755;&#20837;&#20135;&#29289;&#20998;&#23376;&#20316;&#20026;&#26469;&#33258;&#38590;&#20197;&#22788;&#29702;&#30340;&#20808;&#39564;&#20998;&#24067;&#30340;&#26679;&#26412;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis planning is a fundamental challenge in chemistry which aims at designing reaction pathways from commercially available starting materials to a target molecule. Each step in multi-step retrosynthesis planning requires accurate prediction of possible precursor molecules given the target molecule and confidence estimates to guide heuristic search algorithms. We model single-step retrosynthesis planning as a distribution learning problem in a discrete state space. First, we introduce the Markov Bridge Model, a generative framework aimed to approximate the dependency between two intractable discrete distributions accessible via a finite sample of coupled data points. Our framework is based on the concept of a Markov bridge, a Markov process pinned at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does not need a tractable noise distribution as a sampling proxy and directly operates on the input product molecules as samples from the intractable prior di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28145;&#23618;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#36830;&#32493;&#29615;&#22659;&#20013;&#24212;&#29992;&#21487;&#24494;&#20998;&#31070;&#32463;&#36923;&#36753;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#24403;&#21069;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;ILP&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#24182;&#22788;&#29702;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16210</link><description>&lt;p&gt;
&#28145;&#23618;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Deep Inductive Logic Programming meets Reinforcement Learning. (arXiv:2308.16210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28145;&#23618;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#36830;&#32493;&#29615;&#22659;&#20013;&#24212;&#29992;&#21487;&#24494;&#20998;&#31070;&#32463;&#36923;&#36753;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#24403;&#21069;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;ILP&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#24182;&#22788;&#29702;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#29702;&#35299;&#27700;&#24179;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#30340;&#31526;&#21495;&#26041;&#27861;&#65292;&#23427;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#22343;&#20855;&#22791;&#65292;&#21487;&#20197;&#23398;&#20064;&#33021;&#22815;&#25512;&#23548;&#25968;&#25454;&#34892;&#20026;&#30340;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#12290;&#21487;&#24494;&#25193;&#23637;&#30340;ILP&#65292;&#21363;&#21487;&#24494;&#20998;&#30340;&#31070;&#32463;&#36923;&#36753;&#65288;dNL&#65289;&#32593;&#32476;&#65292;&#33021;&#22815;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#30340;&#31070;&#32463;&#32467;&#26500;&#21253;&#25324;&#31526;&#21495;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;dNL&#24212;&#29992;&#20110;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#65288;RRL&#65289;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#36830;&#32493;&#29615;&#22659;&#38382;&#39064;&#12290;&#36825;&#26159;&#22312;RRL&#35774;&#32622;&#20013;&#24212;&#29992;&#22522;&#20110;dNL&#30340;ILP&#30340;&#25193;&#23637;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#26032;&#20102;&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#36830;&#32493;RL&#29615;&#22659;&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25913;&#36827;&#29992;&#20110;RRL&#30340;&#24403;&#21069;ILP&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#36830;&#32493;&#35859;&#35789;&#65292;&#20351;RRL&#20195;&#29702;&#33021;&#22815;&#22312;&#21160;&#24577;&#36830;&#32493;&#29615;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#21644;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
One approach to explaining the hierarchical levels of understanding within a machine learning model is the symbolic method of inductive logic programming (ILP), which is data efficient and capable of learning first-order logic rules that can entail data behaviour. A differentiable extension to ILP, so-called differentiable Neural Logic (dNL) networks, are able to learn Boolean functions as their neural architecture includes symbolic reasoning. We propose an application of dNL in the field of Relational Reinforcement Learning (RRL) to address dynamic continuous environments. This represents an extension of previous work in applying dNL-based ILP in RRL settings, as our proposed model updates the architecture to enable it to solve problems in continuous RL environments. The goal of this research is to improve upon current ILP methods for use in RRL by incorporating non-linear continuous predicates, allowing RRL agents to reason and make decisions in dynamic and continuous environments.
&lt;/p&gt;</description></item><item><title>MASA-TCN&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;EEG&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#38170;&#28857;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#23618;&#26469;&#25552;&#21462;EEG&#31354;&#38388;&#27169;&#24335;&#65292;&#24182;&#33021;&#22312;&#24773;&#32490;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16207</link><description>&lt;p&gt;
MASA-TCN: &#22810;&#38170;&#28857;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;EEG&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural Networks for Continuous and Discrete EEG Emotion Recognition. (arXiv:2308.16207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16207
&lt;/p&gt;
&lt;p&gt;
MASA-TCN&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;EEG&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#38170;&#28857;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#23618;&#26469;&#25552;&#21462;EEG&#31354;&#38388;&#27169;&#24335;&#65292;&#24182;&#33021;&#22312;&#24773;&#32490;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#20027;&#35201;&#26377;&#20004;&#31181;&#24773;&#20917;&#65306;&#31163;&#25955;&#26631;&#31614;&#30340;&#20998;&#31867;&#21644;&#36830;&#32493;&#26631;&#35760;&#30340;&#22238;&#24402;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#31639;&#27861;&#65292;&#20294;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#12290;&#23545;&#20110;&#24773;&#32490;&#22238;&#24402;&#65292;&#26631;&#31614;&#22312;&#26102;&#38388;&#19978;&#26159;&#36830;&#32493;&#30340;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#23398;&#20064;&#26102;&#24577;&#21160;&#24577;&#27169;&#24335;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#34987;&#29992;&#26469;&#23398;&#20064;EEG&#29305;&#24449;&#21521;&#37327;&#30340;&#26102;&#38388;&#32972;&#26223;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;EEG&#30340;&#31354;&#38388;&#27169;&#24335;&#27809;&#26377;&#34987;&#26377;&#25928;&#25552;&#21462;&#20986;&#26469;&#12290;&#20026;&#20102;&#20351;TCN&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#24615;&#33021;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#31354;&#38388;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#21517;&#20026;MASA-TCN&#65292;&#29992;&#20110;EEG&#24773;&#32490;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#23618;&#20351;&#24471;TCN&#33021;&#22815;&#20174;EEG&#30005;&#26497;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#20013;&#36827;&#34892;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition using electroencephalogram (EEG) mainly has two scenarios: classification of the discrete labels and regression of the continuously tagged labels. Although many algorithms were proposed for classification tasks, there are only a few methods for regression tasks. For emotion regression, the label is continuous in time. A natural method is to learn the temporal dynamic patterns. In previous studies, long short-term memory (LSTM) and temporal convolutional neural networks (TCN) were utilized to learn the temporal contextual information from feature vectors of EEG. However, the spatial patterns of EEG were not effectively extracted. To enable the spatial learning ability of TCN towards better regression and classification performances, we propose a novel unified model, named MASA-TCN, for EEG emotion regression and classification tasks. The space-aware temporal layer enables TCN to additionally learn from spatial relations among EEG electrodes. Besides, a novel multi-an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16198</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#39640;&#25928;&#21487;&#38752;&#30340;&#20449;&#24687;&#20256;&#25773;&#23545;&#25903;&#25345;&#20851;&#38190;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#28798;&#38590;&#21709;&#24212;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#20316;&#20026;&#23454;&#29616;&#26356;&#20026;&#20998;&#25955;&#12289;&#39640;&#25928;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20449;&#24687;&#20256;&#25773;&#30340;&#20998;&#24067;&#24335;POMDP&#65288;Decentralized-POMDP&#65289;&#24418;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#29420;&#31435;&#20915;&#23450;&#28040;&#24687;&#30340;&#36716;&#21457;&#12290;&#36825;&#26500;&#25104;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#65288;MPR&#65289;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65292;&#37319;&#29992;&#20855;&#26377;&#21160;&#24577;&#27880;&#24847;&#21147;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26469;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;L-DGN&#21644;HL-DGN&#65292;&#23427;&#20204;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#25442;&#30340;&#20449;&#24687;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#20998;&#25955;&#26041;&#27861;&#19982;&#22522;&#20110;MPR&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
&lt;/p&gt;</description></item><item><title>MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.16139</link><description>&lt;p&gt;
MedShapeNet - &#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16139
&lt;/p&gt;
&lt;p&gt;
MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MedShapeNet&#65292;&#19968;&#20010;&#21253;&#21547;&#20102;&#35299;&#21078;&#24418;&#29366;&#65288;&#22914;&#39592;&#39612;&#12289;&#22120;&#23448;&#12289;&#34880;&#31649;&#65289;&#21644;&#19977;&#32500;&#25163;&#26415;&#22120;&#26800;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#20043;&#21069;&#65292;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#35777;&#26126;&#20102;&#24418;&#29366;&#24120;&#34987;&#29992;&#26469;&#25551;&#36848;&#21307;&#23398;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#20307;&#32032;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#24418;&#29366;&#65288;&#21253;&#25324;&#20307;&#32032;&#21344;&#25454;&#32593;&#26684;&#12289;&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#38544;&#24335;&#34920;&#38754;&#27169;&#22411;&#65289;&#26159;&#19977;&#32500;&#25968;&#25454;&#30340;&#39318;&#36873;&#34920;&#31034;&#26041;&#27861;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#20174;&#22823;&#37327;&#20851;&#20110;&#24418;&#29366;&#30340;&#25991;&#31456;&#21450;&#22312;&#39030;&#32423;&#35745;&#31639;&#26426;&#35270;&#35273;&#20250;&#35758;&#65288;&#22914;IEEE/CVF&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#27169;&#24335;&#35782;&#21035;&#20250;&#35758;&#65288;CVPR&#65289;&#65289;&#20013;&#35265;&#21040;&#65292;&#21516;&#26102;ShapeNet&#65288;&#32422;51300&#20010;&#27169;&#22411;&#65289;&#21644;&#26222;&#26519;&#26031;&#39039;ModelNet&#65288;127,915&#20010;&#27169;&#22411;&#65289;&#30340;&#27969;&#34892;&#24230;&#20063;&#22312;&#19981;&#26029;&#22686;&#21152;&#12290;MedShapeNet&#30340;&#21019;&#24314;&#26159;&#20026;&#20102;&#20316;&#20026;&#36825;&#20123;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#21517;&#20026;CongNaMul&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#21644;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#25552;&#20379;&#20102;&#36136;&#37327;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;5&#20010;&#33469;&#30340;&#29289;&#29702;&#29305;&#24449;&#20379;&#27979;&#37327;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.15690</link><description>&lt;p&gt;
CongNaMul: &#19968;&#31181;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#21517;&#20026;CongNaMul&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#21644;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#25552;&#20379;&#20102;&#36136;&#37327;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;5&#20010;&#33469;&#30340;&#29289;&#29702;&#29305;&#24449;&#20379;&#27979;&#37327;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;CongNaMul&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#22823;&#35910;&#33469;&#22270;&#20687;&#20998;&#26512;&#30340;&#21508;&#31181;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#12290;CongNaMul&#25968;&#25454;&#38598;&#26088;&#22312;&#20419;&#36827;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#20197;&#21450;&#38271;&#24230;&#21644;&#37325;&#37327;&#30340;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#20998;&#31867;&#20219;&#21153;&#25552;&#20379;&#20102;&#22235;&#20010;&#31867;&#21035;&#26469;&#30830;&#23450;&#22823;&#35910;&#33469;&#30340;&#36136;&#37327;&#65306;&#27491;&#24120;&#12289;&#26029;&#35010;&#12289;&#26001;&#28857;&#21644;&#26029;&#35010;&#21644;&#26001;&#28857;&#65292;&#20197;&#24320;&#21457;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30340;&#33258;&#21160;&#36136;&#37327;&#26816;&#27979;&#25216;&#26415;&#12290;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#22270;&#20687;&#65292;&#20174;&#21333;&#20010;&#33469;&#22270;&#20687;&#21040;&#20855;&#26377;&#22810;&#20010;&#33469;&#30340;&#22270;&#20687;&#65292;&#20197;&#21450;&#20154;&#24037;&#26631;&#35760;&#30340;&#25513;&#33180;&#22270;&#20687;&#12290;&#26631;&#31614;&#21253;&#25324;4&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#32972;&#26223;&#12289;&#22836;&#37096;&#12289;&#36523;&#20307;&#21644;&#23614;&#37096;&#12290;&#25968;&#25454;&#38598;&#36824;&#20026;&#22270;&#20687;&#20998;&#35299;&#20219;&#21153;&#25552;&#20379;&#20102;&#22270;&#20687;&#21644;&#25513;&#33180;&#65292;&#21253;&#25324;&#20004;&#20010;&#20998;&#31163;&#30340;&#33469;&#22270;&#20687;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#36824;&#25552;&#20379;&#20102;&#33469;&#30340;5&#20010;&#29289;&#29702;&#29305;&#24449;&#65288;&#22836;&#37096;&#38271;&#24230;&#12289;&#36523;&#20307;&#38271;&#24230;&#12289;&#36523;&#20307;&#21402;&#24230;&#12289;&#23614;&#37096;&#38271;&#24230;&#12289;&#37325;&#37327;&#65289;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#27979;&#37327;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present 'CongNaMul', a comprehensive dataset designed for various tasks in soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate tasks such as image classification, semantic segmentation, decomposition, and measurement of length and weight. The classification task provides four classes to determine the quality of soybean sprouts: normal, broken, spotted, and broken and spotted, for the development of AI-aided automatic quality inspection technology. For semantic segmentation, images with varying complexity, from single sprout images to images with multiple sprouts, along with human-labelled mask images, are included. The label has 4 different classes: background, head, body, tail. The dataset also provides images and masks for the image decomposition task, including two separate sprout images and their combined form. Lastly, 5 physical features of sprouts (head length, body length, body thickness, tail length, weight) are provided for image-based measurement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#21709;&#24212;&#24322;&#26041;&#24046;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#36817;&#20284;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#22312;&#25429;&#25417;&#20989;&#25968;&#24179;&#28369;&#24615;&#30340;&#31361;&#21464;&#21644;&#36866;&#24212;&#24322;&#26041;&#24046;&#38169;&#35823;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15370</link><description>&lt;p&gt;
&#22810;&#21709;&#24212;&#24322;&#26041;&#24046;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21450;&#20854;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-Response Heteroscedastic Gaussian Process Models and Their Inference. (arXiv:2308.15370v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#21709;&#24212;&#24322;&#26041;&#24046;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#36817;&#20284;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#22312;&#25429;&#25417;&#20989;&#25968;&#24179;&#28369;&#24615;&#30340;&#31361;&#21464;&#21644;&#36866;&#24212;&#24322;&#26041;&#24046;&#38169;&#35823;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#22312;&#26377;&#25928;&#25429;&#25417;&#20989;&#25968;&#24179;&#28369;&#24615;&#30340;&#31361;&#21464;&#21644;&#36866;&#24212;&#24322;&#26041;&#24046;&#38169;&#35823;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24322;&#26041;&#24046;&#39640;&#26031;&#36807;&#31243;&#65288;HeGP&#65289;&#22238;&#24402;&#26088;&#22312;&#36890;&#36807;&#25215;&#35748;&#22238;&#24402;&#27169;&#22411;&#20013;&#21327;&#21464;&#37327;&#38388;&#27531;&#24046;&#26041;&#24046;&#30340;&#21487;&#21464;&#24615;&#26469;&#24341;&#20837;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#23558;HeGP&#27010;&#24565;&#25193;&#23637;&#21040;&#20998;&#31867;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#19982;&#21327;&#21464;&#37327;&#35825;&#23548;&#30340;&#31934;&#24230;&#30697;&#38453;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#37319;&#29992;&#28151;&#21512;&#24418;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#23545;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#24322;&#26041;&#24046;&#21327;&#26041;&#24046;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#37319;&#26679;&#24102;&#26469;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#36817;&#20284;&#21518;&#39564;&#24182;&#20415;&#21033;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the widespread utilization of Gaussian process models for versatile nonparametric modeling, they exhibit limitations in effectively capturing abrupt changes in function smoothness and accommodating relationships with heteroscedastic errors. Addressing these shortcomings, the heteroscedastic Gaussian process (HeGP) regression seeks to introduce flexibility by acknowledging the variability of residual variances across covariates in the regression model. In this work, we extend the HeGP concept, expanding its scope beyond regression tasks to encompass classification and state-space models. To achieve this, we propose a novel framework where the Gaussian process is coupled with a covariate-induced precision matrix process, adopting a mixture formulation. This approach enables the modeling of heteroscedastic covariance functions across covariates. To mitigate the computational challenges posed by sampling, we employ variational inference to approximate the posterior and facilitate p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#24809;&#32602;&#30340;&#21452;&#32858;&#31867;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;SSVD&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#24809;&#32602;&#26041;&#27861;&#12290;&#27169;&#25311;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#28151;&#21512;&#30340;Prenet&#24809;&#32602;&#23545;&#20110;&#38750;&#37325;&#21472;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.14388</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#24809;&#32602;&#36827;&#34892;&#30340;&#21452;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Biclustering Methods via Sparse Penalty. (arXiv:2308.14388v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#24809;&#32602;&#30340;&#21452;&#32858;&#31867;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;SSVD&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#24809;&#32602;&#26041;&#27861;&#12290;&#27169;&#25311;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#28151;&#21512;&#30340;Prenet&#24809;&#32602;&#23545;&#20110;&#38750;&#37325;&#21472;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#22238;&#39038;&#20102;&#20960;&#31181;&#29992;&#20110;&#35782;&#21035;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#26368;&#26174;&#33879;&#32858;&#31867;&#30340;&#21452;&#32858;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20102;SSVD&#65288;&#31232;&#30095;SVD&#65289;&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20102;&#19968;&#31181;&#20165;&#29992;&#20110;&#22240;&#23376;&#20998;&#26512;&#30340;&#26032;&#30340;&#31232;&#30095;&#24809;&#32602;&#26041;&#27861;&#65292;&#31216;&#20026;"Prenet&#24809;&#32602;"&#12290;&#28982;&#21518;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#25968;&#25454;&#38598;&#65288;&#20855;&#26377;&#19981;&#21516;&#30340;&#31232;&#30095;&#24615;&#21644;&#32500;&#24230;&#65289;&#65292;&#24182;&#23581;&#35797;&#20102;&#19968;&#23618;&#36924;&#36817;&#21644;k&#23618;&#36924;&#36817;&#65292;&#32467;&#26524;&#34920;&#26126;&#28151;&#21512;&#30340;Prenet&#24809;&#32602;&#23545;&#20110;&#38750;&#37325;&#21472;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20123;&#30495;&#23454;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first reviewed several biclustering methods that are used to identify the most significant clusters in gene expression data. Here we mainly focused on the SSVD(sparse SVD) method and tried a new sparse penalty named "Prenet penalty" which has been used only in factor analysis to gain sparsity. Then in the simulation study, we tried different types of generated datasets (with different sparsity and dimension) and tried 1-layer approximation then for k-layers which shows the mixed Prenet penalty is very effective for non-overlapped data. Finally, we used some real gene expression data to show the behavior of our methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.14172</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#22522;&#20110;&#20809;&#28369;&#24615;&#20808;&#39564;&#25512;&#26029;&#36229;&#22270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#22312;&#22788;&#29702;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#39640;&#38454;&#20851;&#31995;&#25968;&#25454;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#27809;&#26377;&#26126;&#30830;&#36229;&#22270;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#24076;&#26395;&#33021;&#22815;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#20986;&#26377;&#24847;&#20041;&#30340;&#36229;&#22270;&#32467;&#26500;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#37319;&#29992;&#31616;&#21333;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#65292;&#19981;&#33021;&#31934;&#30830;&#25429;&#25417;&#28508;&#22312;&#36229;&#22270;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#35201;&#20040;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65288;&#21363;&#39044;&#20808;&#23384;&#22312;&#30340;&#36229;&#22270;&#32467;&#26500;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23616;&#38480;&#20110;&#23454;&#38469;&#24773;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#28369;&#24615;&#20808;&#39564;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#34920;&#31034;&#36229;&#36793;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#19982;&#21253;&#21547;&#35813;&#36229;&#36793;&#30340;&#36229;&#36793;&#30340;&#29305;&#24449;&#39640;&#24230;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are important for processing data with higher-order relationships involving more than two entities. In scenarios where explicit hypergraphs are not readily available, it is desirable to infer a meaningful hypergraph structure from the node features to capture the intrinsic relations within the data. However, existing methods either adopt simple pre-defined rules that fail to precisely capture the distribution of the potential hypergraph structure, or learn a mapping between hypergraph structures and node features but require a large amount of labelled data, i.e., pre-existing hypergraph structures, for training. Both restrict their applications in practical scenarios. To fill this gap, we propose a novel smoothness prior that enables us to design a method to infer the probability for each potential hyperedge without labelled data as supervision. The proposed prior indicates features of nodes in a hyperedge are highly correlated by the features of the hyperedge containing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#35270;&#35282;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#20840;&#23616;&#20248;&#21270;&#20013;&#21033;&#29992;&#34203;&#23450;&#35860;&#26041;&#31243;&#25512;&#23548;&#30340;&#38567;&#36947;&#25928;&#24212;&#65292;&#20174;&#32780;&#33021;&#22815;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.11594</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#35270;&#35282;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantization-based Optimization with Perspective of Quantum Mechanics. (arXiv:2308.11594v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11594
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#35270;&#35282;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#20840;&#23616;&#20248;&#21270;&#20013;&#21033;&#29992;&#34203;&#23450;&#35860;&#26041;&#31243;&#25512;&#23548;&#30340;&#38567;&#36947;&#25928;&#24212;&#65292;&#20174;&#32780;&#33021;&#22815;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28909;&#21147;&#23398;&#30340;&#32479;&#35745;&#21644;&#38543;&#26426;&#20998;&#26512;&#19968;&#30452;&#26159;&#38543;&#26426;&#20840;&#23616;&#20248;&#21270;&#30340;&#20027;&#35201;&#20998;&#26512;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#29992;&#20110;&#20840;&#23616;&#20248;&#21270;&#30340;&#37327;&#23376;&#36864;&#28779;&#25110;&#37327;&#23376;&#38567;&#36947;&#31639;&#27861;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26694;&#26550;&#26469;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#34203;&#23450;&#35860;&#26041;&#31243;&#30340;&#37327;&#21270;&#20248;&#21270;&#30340;&#20998;&#26512;&#65292;&#20197;&#25581;&#31034;&#37327;&#23376;&#21147;&#23398;&#20013;&#30340;&#21738;&#20123;&#23646;&#24615;&#20351;&#20840;&#23616;&#20248;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#34203;&#23450;&#35860;&#26041;&#31243;&#25512;&#23548;&#20986;&#30340;&#38567;&#36947;&#25928;&#24212;&#20351;&#24471;&#37327;&#21270;&#20248;&#21270;&#33021;&#22815;&#36867;&#31163;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#35748;&#36825;&#31181;&#38567;&#36947;&#25928;&#24212;&#26159;&#21253;&#21547;&#22312;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#20840;&#23616;&#20248;&#21270;&#20013;&#30340;&#30456;&#21516;&#23646;&#24615;&#12290;&#23545;&#26631;&#20934;&#22810;&#27169;&#24577;&#22522;&#20934;&#20989;&#25968;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical and stochastic analysis based on thermodynamics has been the main analysis framework for stochastic global optimization. Recently, appearing quantum annealing or quantum tunneling algorithm for global optimization, we require a new researching framework for global optimization algorithms. In this paper, we provide the analysis for quantization-based optimization based on the Schr\"odinger equation to reveal what property in quantum mechanics enables global optimization. We present that the tunneling effect derived by the Schr\"odinger equation in quantization-based optimization enables to escape of a local minimum. Additionally, we confirm that this tunneling effect is the same property included in quantum mechanics-based global optimization. Experiments with standard multi-modal benchmark functions represent that the proposed analysis is valid.
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.11155</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#20986;&#24179;&#34913;&#29366;&#24577;&#30340;&#25193;&#23637;&#21160;&#21147;&#23398;&#24615;&#33021;&#35780;&#20272;&#31070;&#32463;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11155
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21147;&#22330;&#24050;&#25104;&#20026;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#37325;&#35201;&#27169;&#22411;&#65292;&#21462;&#20195;&#20102;&#20174;&#22836;&#31639;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#12290;&#30446;&#21069;&#23545;&#31070;&#32463;&#21147;&#22330;&#30340;&#20027;&#35201;&#35780;&#20272;&#22522;&#20934;&#26159;MD17&#25968;&#25454;&#38598;&#21450;&#20854;&#21518;&#32493;&#25193;&#23637;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20027;&#35201;&#21253;&#21547;&#26469;&#33258;&#22522;&#24577;&#21183;&#33021;&#38754;&#24179;&#34913;&#21306;&#22495;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#37319;&#26679;&#33258;&#30452;&#25509;&#32477;&#28909;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21270;&#23398;&#21453;&#24212;&#28041;&#21450;&#21040;&#36739;&#22823;&#30340;&#20998;&#23376;&#21464;&#24418;&#65292;&#29305;&#21035;&#26159;&#38190;&#26029;&#35010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MD17&#25968;&#25454;&#38598;&#20013;&#20869;&#22352;&#26631;&#21644;&#33021;&#37327;&#30340;&#32422;&#26463;&#20998;&#24067;&#65292;&#20984;&#26174;&#20102;&#20854;&#22312;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#37319;&#26679;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#65288;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65289;&#25968;&#25454;&#38598;&#65292;&#20174;&#38750;&#32477;&#28909;&#21160;&#21147;&#23398;&#20013;&#27966;&#29983;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20174;&#22810;&#21442;&#32771;&#27874;&#20989;&#25968;&#29702;&#35770;&#21644;&#23494;&#24230;&#27867;&#20989;&#20013;&#30830;&#23450;&#30340;&#33021;&#37327;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RBA-GCN&#27169;&#22411;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#21644;&#22270;&#29983;&#25104;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;GCN&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#21644;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11029</link><description>&lt;p&gt;
RBA-GCN: &#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition. (arXiv:2308.11029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RBA-GCN&#27169;&#22411;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#21644;&#22270;&#29983;&#25104;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;GCN&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#21644;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#23545;&#35805;&#20855;&#26377;&#33258;&#28982;&#30340;&#22270;&#32467;&#26500;&#65292;&#24456;&#22810;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;ERC&#27169;&#22411;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;GCNs&#30340;&#32858;&#21512;&#26041;&#27861;&#23384;&#22312;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#65292;&#23548;&#33268;&#33410;&#28857;&#36776;&#21035;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#21333;&#23618;GCNs&#32570;&#20047;&#20174;&#22270;&#20013;&#25429;&#33719;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#25991;&#26412;&#27169;&#24577;&#25110;&#23558;&#19981;&#21516;&#27169;&#24577;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#25429;&#25417;&#27169;&#24577;&#38388;&#20132;&#20114;&#33021;&#21147;&#24369;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;RBA-GCN&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22270;&#29983;&#25104;&#27169;&#22359;&#65288;GGM&#65289;&#12289;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#31751;&#26500;&#24314;&#27169;&#22359;&#65288;SCBM&#65289;&#21644;&#21452;&#23618;&#32858;&#21512;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#23545;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20805;&#20998;&#32422;&#26463;&#30340;&#31616;&#32422;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#32467;&#21512;&#29289;&#29702;&#23398;&#32763;&#35793;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36827;&#34892;&#39564;&#35777;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#35782;&#21035;&#30495;&#23454;&#20027;&#23548;PDE&#26041;&#38754;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.10283</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#27169;&#22411;&#36873;&#25321;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;PDE&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery. (arXiv:2308.10283v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10283
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#23545;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20805;&#20998;&#32422;&#26463;&#30340;&#31616;&#32422;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#32467;&#21512;&#29289;&#29702;&#23398;&#32763;&#35793;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36827;&#34892;&#39564;&#35777;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#35782;&#21035;&#30495;&#23454;&#20027;&#23548;PDE&#26041;&#38754;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#33258;&#36866;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65288;UBIC&#65289;&#65292;&#20197;&#20248;&#20808;&#32771;&#34385;&#23545;&#22122;&#38899;&#31354;&#26102;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20805;&#20998;&#32422;&#26463;&#30340;&#31616;&#32422;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#30001;&#20110;&#30452;&#25509;&#20351;&#29992;BIC&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#24050;&#30693;&#20250;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#36807;&#24230;&#25311;&#21512;PDE&#65292;UBIC&#22312;&#27010;&#29575;&#35270;&#22270;&#20013;&#36890;&#36807;&#27169;&#22411;&#25903;&#25345;&#30340;&#21464;&#24322;&#31995;&#25968;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23545;&#25214;&#21040;&#30340;PDE&#36827;&#34892;&#24809;&#32602;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#28789;&#27963;&#22320;&#39564;&#35777;&#25152;&#36873;&#23450;&#30340;PDE&#19982;&#20854;&#20182;&#21457;&#29616;&#30340;PDE&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#23454;&#20102;UBIC&#22312;&#35782;&#21035;&#30495;&#23454;&#30340;&#20027;&#23548;PDE&#19978;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21435;&#22122;&#35266;&#27979;&#25968;&#25454;&#23545;&#25913;&#21892;BIC&#24471;&#20998;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#26377;&#19968;&#20010;&#26377;&#36259;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new parameter-adaptive uncertainty-penalized Bayesian information criterion (UBIC) to prioritize the parsimonious partial differential equation (PDE) that sufficiently governs noisy spatial-temporal observed data with few reliable terms. Since the naive use of the BIC for model selection has been known to yield an undesirable overfitted PDE, the UBIC penalizes the found PDE not only by its complexity but also the quantified uncertainty, derived from the model supports' coefficient of variation in a probabilistic view. We also introduce physics-informed neural network learning as a simulation-based approach to further validate the selected PDE flexibly against the other discovered PDE. Numerical results affirm the successful application of the UBIC in identifying the true governing PDE. Additionally, we reveal an interesting effect of denoising the observed data on improving the trade-off between the BIC score and model complexity. Code is available at https://github.com/Po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20195;&#30721;&#20013;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#23618;&#26469;&#25552;&#39640;&#22312;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.03312</link><description>&lt;p&gt;
&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#27861;&#29992;&#20110;&#23398;&#20064;&#20195;&#30721;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20195;&#30721;&#20013;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#23618;&#26469;&#25552;&#39640;&#22312;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#21160;&#31243;&#24207;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36825;&#26159;&#35768;&#22810;&#23433;&#20840;&#20219;&#21153;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#20195;&#30721;&#30340;LLM&#26550;&#26500;&#36890;&#24120;&#20174;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#20511;&#29992;&#65292;&#24341;&#21457;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#26410;&#30693;&#20195;&#30721;&#30340;&#20581;&#22766;&#24615;&#30340;&#25285;&#24551;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#27867;&#21270;&#25361;&#25112;&#26159;&#23558;&#20195;&#30721;&#35821;&#20041;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#32435;&#20837;LLM&#26550;&#26500;&#20013;&#12290;&#21463;&#21040;&#21033;&#29992;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#21367;&#31215;&#23618;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20195;&#30721;&#23545;&#31216;&#24615;&#22914;&#20309;&#22686;&#24378;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#30340;LLM&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#32676;&#35770;&#26694;&#26550;&#65292;&#24418;&#24335;&#21270;&#22320;&#23450;&#20041;&#20102;&#20195;&#30721;&#23545;&#31216;&#24615;&#20316;&#20026;&#20445;&#25345;&#35821;&#20041;&#30340;&#21464;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;LLM&#26550;&#26500;&#20013;&#31934;&#30830;&#25512;&#29702;&#23545;&#31216;&#24615;&#20445;&#25345;&#30340;&#25216;&#26415;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20445;&#25345;&#31243;&#24207;&#23545;&#31216;&#24615;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#21147;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#65292;&#21152;&#36895;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15034</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#21152;&#36895;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15034
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#65292;&#21152;&#36895;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#35299;&#31639;&#22120;&#30340;&#20195;&#29702;&#26144;&#23556;&#12290;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#36890;&#24120;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#28857;&#65292;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#37117;&#26159;&#37325;&#35201;&#29942;&#39048;&#12290;&#34429;&#28982;&#23545;&#20110;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26377;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#25216;&#26415;&#65292;&#20294;&#37027;&#20123;&#21482;&#36866;&#29992;&#20110;&#26377;&#38480;&#32500;&#24230;&#19978;&#30340;&#23454;&#20540;&#25968;&#25454;&#31867;&#22411;&#65292;&#22240;&#27492;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#22312;&#22797;&#20540;&#65288;&#20613;&#37324;&#21494;&#65289;&#22495;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#37325;&#35201;&#25805;&#20316;&#30340;FNO&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#26412;&#36523;&#23601;&#26159;&#19968;&#27425;&#36817;&#20284;&#65288;&#30001;&#20110;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;&#23384;&#22312;&#65289;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#20197;&#23436;&#20840;&#31934;&#24230;&#25191;&#34892;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#65288;i&#65289;&#23545;&#20351;&#29992;&#20840;&#31934;&#24230;&#21644;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#30340;FNO&#36827;&#34892;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#38388;&#21078;&#26512;&#65292;&#65288;ii&#65289;&#23545;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#22823;&#22823;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memor
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09312</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65306;&#25972;&#21512;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#21464;&#25442;&#22120;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09312
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#20110;&#22270;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65288;mDT&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#35780;&#35770;&#20026;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#22260;&#32469;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20307;&#20998;&#26512;&#23637;&#24320;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#22270;&#21464;&#25442;&#22120;&#26469;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#20132;&#32455;&#34701;&#21512;&#23618;&#26469;&#32452;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25552;&#20379;&#31038;&#20250;&#20215;&#20540;&#30340;&#26410;&#26469;&#24037;&#20316;&#65292;&#24182;&#35748;&#20026;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.07873</link><description>&lt;p&gt;
&#25506;&#32034;&#20174;&#26367;&#20195;&#35757;&#32451;&#20013;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;DNNs&#30340;&#23545;&#25239;&#26679;&#26412;(AEs)&#24050;&#32463;&#34920;&#26126;&#26159;&#21487;&#36716;&#31227;&#30340;&#65306;&#25104;&#21151;&#27450;&#39575;&#30333;&#30418;&#23376;&#26367;&#20195;&#27169;&#22411;&#30340;AEs&#20063;&#21487;&#20197;&#27450;&#39575;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#12290;&#34429;&#28982;&#35768;&#22810;&#32463;&#39564;&#30740;&#31350;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#24230;&#21487;&#36716;&#31227;AE&#30340;&#25351;&#23548;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#32570;&#20047;&#35299;&#37322;&#29978;&#33267;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;&#26412;&#25991;&#22312;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#26041;&#38754;&#12290;&#20174;&#30528;&#21517;&#30340;&#23567;&#20581;&#22766;&#24615;&#29616;&#35937;&#24320;&#22987;&#65292;&#36890;&#36807;&#20197;&#36731;&#24494;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#23545;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#24402;&#22240;&#20110;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23427;&#20204;&#30340;&#20849;&#21516;&#25928;&#26524;&#19978;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#19982;&#21487;&#36716;&#31227;&#24615;&#30340;&#21333;&#29420;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#25512;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#38752;&#37327;&#21270;&#12289;&#26377;&#38480;&#26102;&#38388;&#21327;&#20316;&#21327;&#35758;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#26469;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06620</link><description>&lt;p&gt;
&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#19982;&#37327;&#21270;&#26377;&#38480;&#26102;&#38388;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Online Distributed Learning with Quantized Finite-Time Coordination. (arXiv:2307.06620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#38752;&#37327;&#21270;&#12289;&#26377;&#38480;&#26102;&#38388;&#21327;&#20316;&#21327;&#35758;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#26469;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#26159;&#25351;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#23398;&#20064;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#19968;&#32452;&#20195;&#29702;&#38656;&#35201;&#21512;&#20316;&#22320;&#20174;&#27969;&#25968;&#25454;&#20013;&#35757;&#32451;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#32852;&#37030;&#23398;&#20064;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#32780;&#20165;&#20381;&#38752;&#20195;&#29702;&#20043;&#38388;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#12290;&#35813;&#26041;&#27861;&#32463;&#24120;&#29992;&#20110;&#25968;&#25454;&#30001;&#20110;&#38544;&#31169;&#12289;&#23433;&#20840;&#25110;&#25104;&#26412;&#21407;&#22240;&#19981;&#33021;&#31227;&#21160;&#21040;&#38598;&#20013;&#20301;&#32622;&#30340;&#22330;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#23569;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#37327;&#21270;&#30340;&#12289;&#26377;&#38480;&#26102;&#38388;&#30340;&#21327;&#20316;&#21327;&#35758;&#26469;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20801;&#35768;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#12290;&#38543;&#26426;&#26799;&#24230;&#26159;&#20351;&#29992;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#30340;&#38543;&#26426;&#25277;&#26679;&#23376;&#38598;&#35745;&#31639;&#30340;&#65292;&#36825;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#26356;&#21152;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider online distributed learning problems. Online distributed learning refers to the process of training learning models on distributed data sources. In our setting a set of agents need to cooperatively train a learning model from streaming data. Differently from federated learning, the proposed approach does not rely on a central server but only on peer-to-peer communications among the agents. This approach is often used in scenarios where data cannot be moved to a centralized location due to privacy, security, or cost reasons. In order to overcome the absence of a central server, we propose a distributed algorithm that relies on a quantized, finite-time coordination protocol to aggregate the locally trained models. Furthermore, our algorithm allows for the use of stochastic gradients during local training. Stochastic gradients are computed using a randomly sampled subset of the local training data, which makes the proposed algorithm more efficient and scalable th
&lt;/p&gt;</description></item><item><title>DNAGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#29420;&#29305;&#30340;&#26631;&#35760;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05628</link><description>&lt;p&gt;
DNAGPT&#65306;&#29992;&#20110;&#22810;&#20010;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#30340;&#36890;&#29992;&#39044;&#35757;&#32451;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks. (arXiv:2307.05628v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05628
&lt;/p&gt;
&lt;p&gt;
DNAGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#29420;&#29305;&#30340;&#26631;&#35760;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT&#31995;&#21015;&#30340;&#25104;&#21151;&#35777;&#26126;&#20102;GPT&#21487;&#20197;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#19968;&#33324;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#25152;&#26377;&#19979;&#28216;&#20219;&#21153;&#21463;&#30410;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25506;&#32034;DNA&#24207;&#21015;&#20013;&#30340;&#38544;&#34255;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;DNA&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#21644;&#20219;&#21153;&#38656;&#27714;&#38750;&#24120;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#65292;&#22240;&#20026;DNA&#30456;&#20851;&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#22914;&#24207;&#21015;&#12289;&#34920;&#36798;&#27700;&#24179;&#31561;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#29305;&#28857;&#35774;&#35745;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DNAGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#22312;9&#20010;&#29289;&#31181;&#30340;&#36229;&#36807;100&#20159;&#20010;&#30897;&#22522;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#38024;&#23545;&#20219;&#20309;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25110;&#36755;&#20986;DNA&#24207;&#21015;&#21644;&#25968;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29420;&#29305;&#30340;&#26631;&#35760;&#35774;&#35745;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#20219;&#21153;&#38656;&#27714;&#26469;&#35774;&#35745;&#25552;&#31034;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of the GPT series proves that GPT can extract general information from sequences, thereby benefiting all downstream tasks. This motivates us to use pre-trained models to explore the hidden information in DNA sequences. However, data and task requirements in DNA sequence analysis are complexity and diversity as DNA relevant data includes different types of information, such as sequences, expression levels, etc, while there is currently no model specifically designed for these characteristics. Hereby, we present DNAGPT, a generalized foundation model pre-trained on over 10 billion base pairs from 9 species which can be fine-tuned for any DNA sequence analysis task. Our model can simultaneously process or output DNA sequences and numbers. In addition, our unique token design allows users to design prompts according to their own task requirements, making it applicable to any type of task. We have evaluated our model on classification, regression, and generation tasks. We demons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04726</link><description>&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#31574;&#30053;&#30340;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064; (RL) &#26041;&#27861;&#21033;&#29992;&#20197;&#21069;&#30340;&#32463;&#39564;&#26469;&#23398;&#20064;&#27604;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#30456;&#21453;&#65292;&#34892;&#20026;&#20811;&#38534;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25910;&#38598;&#30340;&#65292;&#32780;&#33073;&#26426; RL &#21487;&#20197;&#20351;&#29992;&#38750;&#19987;&#23478;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#33073;&#26426; RL &#31639;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#22312;&#32447;&#20132;&#20114;&#12290;&#20808;&#21069;&#20851;&#20110;&#33073;&#26426; RL &#30340;&#24037;&#20316;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#38024;&#23545;&#32531;&#35299;&#33073;&#26426;&#20998;&#24067;&#29366;&#24577;&#27867;&#21270;&#32780;&#21046;&#23450;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP)&#65292;&#23558;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#32435;&#20837;&#21040;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#20998;&#24067;&#36890;&#29992;&#21270;&#38382;&#39064;&#12290;&#29366;&#24577;&#37325;&#26500;&#25439;&#22833;&#20419;&#36827;&#20102;&#26356;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to represent multimodal behavior in the dataset. Nevertheless, these methods are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method, named State Reconstruction for Diffusion Policies (SRDP), incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descript
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#23454;&#38469;&#32593;&#32476;&#25968;&#25454;&#23545;5G&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#24615;&#24310;&#36831;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;Hypoexponential&#20998;&#24067;&#20026;&#22522;&#30784;&#30340;&#29992;&#25143;&#38754;&#24310;&#36831;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#21644;&#22270;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#29575;&#22238;&#24402;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#24615;&#39044;&#27979;&#30340;&#23454;&#39564;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#39044;&#27979;&#26694;&#26550;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#31227;&#21160;&#24615;&#12289;&#22478;&#24066;&#20132;&#36890;&#21644;&#31038;&#20132;&#32858;&#20250;&#12290;</title><link>http://arxiv.org/abs/2307.02329</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#24615;5G&#24310;&#36831;&#65306;&#21033;&#29992;&#32593;&#32476;&#27979;&#37327;&#36827;&#34892;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements. (arXiv:2307.02329v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#23454;&#38469;&#32593;&#32476;&#25968;&#25454;&#23545;5G&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#24615;&#24310;&#36831;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;Hypoexponential&#20998;&#24067;&#20026;&#22522;&#30784;&#30340;&#29992;&#25143;&#38754;&#24310;&#36831;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#21644;&#22270;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#29575;&#22238;&#24402;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#24615;&#39044;&#27979;&#30340;&#23454;&#39564;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#39044;&#27979;&#26694;&#26550;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#31227;&#21160;&#24615;&#12289;&#22478;&#24066;&#20132;&#36890;&#21644;&#31038;&#20132;&#32858;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;5G&#26381;&#21153;&#21644;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#20855;&#26377;&#32465;&#23450;&#24310;&#36831;&#35201;&#27714;&#21644;&#20445;&#35777;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#38656;&#27714;&#36843;&#20351;&#32593;&#32476;&#31649;&#29702;&#31243;&#24207;&#32435;&#20837;&#33258;&#20027;&#21644;&#20027;&#21160;&#20915;&#31574;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#31227;&#21160;&#32593;&#32476;&#36816;&#33829;&#21830;&#65288;MNOs&#65289;&#21487;&#20197;&#35775;&#38382;&#30340;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#65292;&#23545;5G&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#24615;&#24310;&#36831;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#20316;&#20026;Hypoexponential&#20998;&#24067;&#30340;&#29992;&#25143;&#38754;&#24310;&#36831;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#36890;&#36807;&#19982;&#23454;&#35777;&#27979;&#37327;&#30340;&#27604;&#36739;&#20998;&#26512;&#36827;&#34892;&#39564;&#35777;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#29575;&#22238;&#24402;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#24615;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#21033;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#22914;&#36125;&#21494;&#26031;&#23398;&#20064;&#65288;BL&#65289;&#21644;&#22270;&#26426;&#22120;&#23398;&#20064;&#65288;GML&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#36710;&#36742;&#31227;&#21160;&#12289;&#23494;&#38598;&#22478;&#21306;&#20132;&#36890;&#21644;&#31038;&#20132;&#32858;&#20250;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#27979;&#35797;&#25105;&#20204;&#30340;&#39044;&#27979;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of novel 5G services and applications with binding latency requirements and guaranteed Quality of Service (QoS) hastened the need to incorporate autonomous and proactive decision-making in network management procedures. The objective of our study is to provide a thorough analysis of predictive latency within 5G networks by utilizing real-world network data that is accessible to mobile network operators (MNOs). In particular, (i) we present an analytical formulation of the user-plane latency as a Hypoexponential distribution, which is validated by means of a comparative analysis with empirical measurements, and (ii) we conduct experimental results of probabilistic regression, anomaly detection, and predictive forecasting leveraging on emerging domains in Machine Learning (ML), such as Bayesian Learning (BL) and Machine Learning on Graphs (GML). We test our predictive framework using data gathered from scenarios of vehicular mobility, dense-urban traffic, and social gathering 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#30340;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;SNNs&#22312;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17670</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#38388;&#36317;&#30340;&#33192;&#32960;&#21367;&#31215;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings. (arXiv:2306.17670v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#30340;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;SNNs&#22312;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#26159;&#26500;&#24314;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22914;&#35821;&#38899;&#35782;&#21035;&#31561;&#26102;&#38388;&#20219;&#21153;&#12290;&#22312;SNNs&#20013;&#65292;&#24310;&#36831;&#25351;&#30340;&#26159;&#20174;&#19968;&#20010;&#31070;&#32463;&#20803;&#21040;&#21478;&#19968;&#20010;&#31070;&#32463;&#20803;&#20256;&#25773;&#38656;&#35201;&#30340;&#26102;&#38388;&#12290;&#36825;&#20123;&#24310;&#36831;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#24433;&#21709;&#33033;&#20914;&#21040;&#36798;&#26102;&#38388;&#65292;&#24050;&#30693;&#23574;&#23792;&#31070;&#32463;&#20803;&#23545;&#20110;&#37325;&#21472;&#30340;&#36755;&#20837;&#33033;&#20914;&#26377;&#26356;&#24378;&#30340;&#21709;&#24212;&#12290;&#26356;&#27491;&#24335;&#22320;&#35828;&#65292;&#29702;&#35770;&#19978;&#24050;&#32463;&#35777;&#26126;&#21487;&#22609;&#24615;&#24310;&#36831;&#26497;&#22823;&#22686;&#21152;&#20102;SNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#24310;&#36831;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22312;&#28145;&#24230;&#21069;&#39304;SNNs&#20013;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#27169;&#25311;&#36830;&#32493;&#23618;&#20043;&#38388;&#30340;&#24310;&#36831;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#27839;&#26102;&#38388;&#36724;&#30340;&#19968;&#32500;&#21367;&#31215;&#12290;&#21367;&#31215;&#26680;&#20165;&#21253;&#21547;&#23569;&#25968;&#38750;&#38646;&#26435;&#37325; - &#27599;&#20010;&#31361;&#35302;&#19968;&#20010; - &#23427;&#20204;&#30340;&#20301;&#32622;&#23545;&#24212;&#20110;&#24310;&#36831;&#12290;&#36825;&#20123;&#20301;&#32622;&#19982;&#26435;&#37325;&#19968;&#36215;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights - one per synapse - whose positions correspond to the delays. These positions are learned together with the wei
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#26684;&#20998;&#21106;&#20043;&#21069;&#32534;&#30721;&#26144;&#23556;&#20989;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#65292;&#19981;&#21463;&#20998;&#36776;&#29575;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.11737</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#32593;&#26684;&#20998;&#21106;&#30340;&#31070;&#32463;&#24418;&#29366;&#30452;&#24452;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Neural Shape Diameter Function for Efficient Mesh Segmentation. (arXiv:2306.11737v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#26684;&#20998;&#21106;&#20043;&#21069;&#32534;&#30721;&#26144;&#23556;&#20989;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#65292;&#19981;&#21463;&#20998;&#36776;&#29575;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22810;&#36793;&#24418;&#32593;&#26684;&#20998;&#21106;&#20026;&#26377;&#24847;&#20041;&#30340;&#37096;&#20998;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#23558;&#36825;&#20123;&#32467;&#26500;&#20998;&#35299;&#20197;&#36827;&#34892;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20195;&#20215;&#26159;&#35745;&#31639;&#26102;&#38388;&#30340;&#22823;&#37327;&#28040;&#32791;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#23545;3D&#32467;&#26500;&#30340;&#20998;&#21106;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#25512;&#24191;&#65292;&#24182;&#38656;&#35201;&#23558;&#23398;&#20064;&#30340;&#27169;&#22411;&#20998;&#25104;&#20960;&#20010;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#20197;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#26684;&#20998;&#21106;&#20043;&#21069;&#32534;&#30721;&#26144;&#23556;&#20989;&#25968;&#65292;&#20197;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#20351;&#29992;&#39030;&#28857;&#37051;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#22797;&#29616;&#37051;&#22495;&#22270;&#65292;&#21033;&#29992;&#25105;&#20204;&#23545;&#24418;&#29366;&#30452;&#24452;&#20989;&#25968;&#65288;SDF&#65289;&#26041;&#27861;&#30340;&#20102;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21463;&#20998;&#36776;&#29575;&#24433;&#21709;&#65292;&#22240;&#20026;&#25105;&#20204;&#23545;&#36755;&#20837;&#32593;&#26684;&#36827;&#34892;&#19979;&#37319;&#26679;&#65292;&#24182;&#20165;&#38024;&#23545;&#37051;&#23621;&#26597;&#35810;&#23436;&#25972;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partitioning a polygonal mesh into meaningful parts can be challenging. Many applications require decomposing such structures for further processing in computer graphics. In the last decade, several methods were proposed to tackle this problem, at the cost of intensive computational times. Recently, machine learning has proven to be effective for the segmentation task on 3D structures. Nevertheless, these state-of-the-art methods are often hardly generalizable and require dividing the learned model into several specific classes of objects to avoid overfitting. We present a data-driven approach leveraging deep learning to encode a mapping function prior to mesh segmentation for multiple applications. Our network reproduces a neighborhood map using our knowledge of the \textsl{Shape Diameter Function} (SDF) method using similarities among vertex neighborhoods. Our approach is resolution-agnostic as we downsample the input meshes and query the full-resolution structure solely for neighbor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.08149</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20010;&#24615;&#21270;&#39044;&#27979;&#30340;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26681;&#25454;&#36807;&#21435;&#26631;&#35760;&#35266;&#27979;&#39044;&#27979;&#19968;&#20010;&#20154;&#26410;&#26469;&#30340;&#35266;&#27979;&#20540;&#65292;&#36890;&#24120;&#29992;&#20110;&#36830;&#32493;&#20219;&#21153;&#65292;&#20363;&#22914;&#39044;&#27979;&#26085;&#24120;&#24773;&#32490;&#35780;&#20998;&#12290;&#22312;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#32467;&#21512;&#20004;&#31181;&#36235;&#21183;&#65306;&#65288;a&#65289;&#36328;&#20154;&#20849;&#20139;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#36890;&#29992;&#36235;&#21183;&#65292;&#20363;&#22914;&#21608;&#26411;&#26356;&#24320;&#24515;&#65292;&#21644;&#65288;b&#65289;&#27599;&#20010;&#20154;&#29420;&#29305;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#29305;&#23450;&#30340;&#36235;&#21183;&#65292;&#20363;&#22914;&#27599;&#21608;&#26377;&#19968;&#27425;&#21387;&#21147;&#22823;&#30340;&#20250;&#35758;&#12290;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32452;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#36235;&#21183;&#12290;&#23613;&#31649;&#29616;&#22312;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#36890;&#36807;&#23558;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#25972;&#21512;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#36825;&#31181;&#25972;&#21512;&#30446;&#21069;&#20165;&#38480;&#20110;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#65306;&#25490;&#38500;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#36235;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.06777</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#35299;&#37322;&#24615;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31454;&#20105;[&#21442;&#35265;Grinsztajn&#31561;&#20154;&#65292;NeurIPS 2022&#65292;arXiv&#65306;2207.08815]&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#21487;&#35299;&#37322;&#24615;&#21462;&#20915;&#20110;&#26641;&#30340;&#28145;&#24230;&#21644;&#27599;&#20010;&#21494;&#33410;&#28857;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20302;&#28145;&#24230;&#30340;&#26641;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27599;&#20010;&#21494;&#33410;&#28857;&#19978;&#30340;&#26368;&#22823;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#20174;&#20302;&#28145;&#24230;&#26641;&#30340;&#27599;&#20010;&#21494;&#33410;&#28857;&#8220;&#25346;&#36215;&#8221;&#36827;&#19968;&#27493;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#26080;&#38480;&#28145;&#24230;&#30340;&#26641;&#65289;&#12290;&#20302;&#28145;&#24230;&#26641;&#26131;&#20110;&#35299;&#37322;&#65292;&#32780;&#32508;&#21512;&#20302;&#28145;&#24230;&#21644;&#25346;&#36215;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#25972;&#20307;&#32479;&#35745;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#32463;&#20856;&#26041;&#27861;&#65288;&#20363;&#22914;CART&#65289;&#35757;&#32451;&#30340;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20248;&#21270;&#30340;XGBoost&#65289;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22686;&#21152;&#37325;&#25918;&#32531;&#23384;&#20013;&#25968;&#25454;&#36807;&#28193;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#38646;-shot&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#33021;&#36890;&#36807;&#25552;&#39640;&#28508;&#22312;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#26469;&#23454;&#29616;&#36825;&#31181;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2306.05727</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#26679;&#24615;&#37325;&#25918;&#22312;&#27867;&#21270;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Diverse Replay for Generalisation in Reinforcement Learning. (arXiv:2306.05727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22686;&#21152;&#37325;&#25918;&#32531;&#23384;&#20013;&#25968;&#25454;&#36807;&#28193;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#38646;-shot&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#33021;&#36890;&#36807;&#25552;&#39640;&#28508;&#22312;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#26469;&#23454;&#29616;&#36825;&#31181;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#31574;&#30053;&#21644;&#37325;&#25918;&#32531;&#23384;&#26159;&#20854;&#35768;&#22810;&#31639;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#31574;&#30053;&#35268;&#23450;&#20102;&#35201;&#25910;&#38598;&#21644;&#35757;&#32451;&#21738;&#20123;&#29615;&#22659;&#25968;&#25454;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#36825;&#20123;&#32452;&#20214;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#20174;&#35757;&#32451;&#29615;&#22659;&#20013;&#25910;&#38598;&#21644;&#35757;&#32451;&#26356;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#23558;&#25552;&#39640;&#21040;&#26032;&#29615;&#22659;/&#20219;&#21153;&#30340;&#38646;-shot&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#23398;&#25512;&#23548;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#22686;&#21152;&#37325;&#25918;&#32531;&#23384;&#20013;&#36807;&#28193;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#8220;&#21487;&#36798;&#21040;&#8221;&#30340;&#29366;&#24577;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#23545;&#20110;&#31867;&#20284;&#20294;&#8220;&#19981;&#21487;&#36798;&#8221;&#29366;&#24577;&#30340;&#27867;&#21270;&#24615;&#33021;&#20063;&#26377;&#25152;&#25552;&#39640;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#30001;&#20110;&#28508;&#22312;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), key components of many algorithms are the exploration strategy and replay buffer. These strategies regulate what environment data is collected and trained on and have been extensively studied in the RL literature. In this paper, we investigate the impact of these components in the context of generalisation in multi-task RL. We investigate the hypothesis that collecting and training on more diverse data from the training environment will improve zero-shot generalisation to new environments/tasks. We motivate mathematically and show empirically that generalisation to states that are "reachable" during training is improved by increasing the diversity of transitions in the replay buffer. Furthermore, we show empirically that this same strategy also shows improvement for generalisation to similar but "unreachable" states and could be due to improved generalisation of latent representations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26680;&#27979;&#37327;&#19981;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#25214;&#21040;&#26368;&#20339;&#26680;&#24102;&#23485;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#31639;&#27861;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24230;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.01890</link><description>&lt;p&gt;
&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#26680;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel Metric Learning for Clustering Mixed-type Data. (arXiv:2306.01890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26680;&#27979;&#37327;&#19981;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#25214;&#21040;&#26368;&#20339;&#26680;&#24102;&#23485;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#31639;&#27861;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24230;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#21644;&#20998;&#31867;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20197;&#23558;&#28151;&#21512;&#25968;&#20540;&#21644;&#20998;&#31867;&#25968;&#25454;&#20998;&#32452;&#12290;&#39044;&#23450;&#20041;&#30340;&#36317;&#31163;&#27979;&#37327;&#29992;&#20110;&#26681;&#25454;&#23427;&#20204;&#30340;&#19981;&#30456;&#20284;&#24615;&#26469;&#32858;&#31867;&#25968;&#25454;&#28857;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#36866;&#29992;&#20110;&#20855;&#26377;&#32431;&#25968;&#23383;&#23646;&#24615;&#21644;&#20960;&#20010;&#26377;&#24207;&#21644;&#26080;&#24207;&#20998;&#31867;&#25351;&#26631;&#30340;&#25968;&#25454;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#28151;&#21512;&#22411;&#25968;&#25454;&#30340;&#26368;&#20339;&#36317;&#31163;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#24230;&#37327;&#23558;&#25968;&#23383;&#23646;&#24615;&#36716;&#25442;&#20026;&#20998;&#31867;&#23646;&#24615;&#25110;&#21453;&#20043;&#20134;&#28982;&#12290;&#20182;&#20204;&#23558;&#25968;&#25454;&#28857;&#22788;&#29702;&#20026;&#21333;&#20010;&#23646;&#24615;&#31867;&#22411;&#65292;&#25110;&#32773;&#20998;&#21035;&#35745;&#31639;&#27599;&#20010;&#23646;&#24615;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#23558;&#23427;&#20204;&#30456;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;&#20351;&#29992;&#28151;&#21512;&#26680;&#27979;&#37327;&#19981;&#30456;&#20284;&#24615;&#65292;&#24182;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#26469;&#23547;&#25214;&#26368;&#20339;&#26680;&#24102;&#23485;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21253;&#21547;&#32431;&#36830;&#32493;&#65292;&#20998;&#31867;&#21644;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#31639;&#27861;&#26102;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distance-based clustering and classification are widely used in various fields to group mixed numeric and categorical data. A predefined distance measurement is used to cluster data points based on their dissimilarity. While there exist numerous distance-based measures for data with pure numerical attributes and several ordered and unordered categorical metrics, an optimal distance for mixed-type data is an open problem. Many metrics convert numerical attributes to categorical ones or vice versa. They handle the data points as a single attribute type or calculate a distance between each attribute separately and add them up. We propose a metric that uses mixed kernels to measure dissimilarity, with cross-validated optimal kernel bandwidths. Our approach improves clustering accuracy when utilized for existing distance-based clustering algorithms on simulated and real-world datasets containing pure continuous, categorical, and mixed-type data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#19979;&#28216;&#29992;&#36884;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.19979</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65306;&#23427;&#20204;&#26377;&#29992;&#21527;&#65311;&#23545;&#36830;&#25509;&#39044;&#27979;&#12289;&#35268;&#21017;&#23398;&#20064;&#21644;&#19979;&#28216;&#22810;&#33647;&#29289;&#20219;&#21153;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasks. (arXiv:2305.19979v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#19979;&#28216;&#29992;&#36884;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#34920;&#31034;&#21644;&#32452;&#32455;&#22797;&#26434;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#23436;&#21892;&#30693;&#35782;&#22270;&#35889;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#26102;&#65292;&#36825;&#20123;&#23884;&#20837;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#26377;&#38480;&#65292;&#24341;&#21457;&#20102;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#22312;&#29983;&#29289;&#21307;&#23398;&#29615;&#22659;&#20013;&#26159;&#21542;&#23384;&#22312;&#38480;&#21046;&#30340;&#30097;&#38382;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#24212;&#29992;&#20110;&#26368;&#36817;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;BioKG&#65292;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#28508;&#22312;&#30340;&#19979;&#28216;&#29992;&#36884;&#12290;&#22312;&#30456;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#25105;&#20204;&#22312;HITS@10&#24471;&#20998;&#26041;&#38754;&#30340;&#24615;&#33021;&#25913;&#36827;&#20102;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#26159;&#21487;&#24212;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs are powerful tools for representing and organising complex biomedical data. Several knowledge graph embedding algorithms have been proposed to learn from and complete knowledge graphs. However, a recent study demonstrates the limited efficacy of these embedding algorithms when applied to biomedical knowledge graphs, raising the question of whether knowledge graph embeddings have limitations in biomedical settings. This study aims to apply state-of-the-art knowledge graph embedding models in the context of a recent biomedical knowledge graph, BioKG, and evaluate their performance and potential downstream uses. We achieve a three-fold improvement in terms of performance based on the HITS@10 score over previous work on the same biomedical knowledge graph. Additionally, we provide interpretable predictions through a rule-based method. We demonstrate that knowledge graph embedding models are applicable in practice by evaluating the best-performing model on four tasks that r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25968;&#25454;&#22686;&#24378;&#65288;DDAug&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#25928;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#23398;&#20064;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26377;&#21033;&#22686;&#24378;&#31574;&#30053;&#65292;&#26377;&#25928;&#19988;&#35745;&#31639;&#20195;&#20215;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.15777</link><description>&lt;p&gt;
&#36890;&#36807;MCTS&#36827;&#34892;&#21069;&#21015;&#33146;MRI&#20998;&#21106;&#30340;&#21160;&#24577;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Dynamic Data Augmentation via MCTS for Prostate MRI Segmentation. (arXiv:2305.15777v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25968;&#25454;&#22686;&#24378;&#65288;DDAug&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#25928;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#23398;&#20064;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26377;&#21033;&#22686;&#24378;&#31574;&#30053;&#65292;&#26377;&#25928;&#19988;&#35745;&#31639;&#20195;&#20215;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#30001;&#20110;&#26114;&#36149;&#30340;&#33719;&#21462;&#21644;&#27880;&#37322;&#36807;&#31243;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#21482;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#21508;&#31181;&#36716;&#25442;&#26469;&#22686;&#24378;&#21407;&#22987;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26032;&#25968;&#25454;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#19968;&#33268;&#30340;&#33719;&#21462;&#26041;&#27861;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#20026;&#19981;&#21516;&#25968;&#25454;&#38598;&#25163;&#21160;&#37197;&#32622;&#36890;&#29992;&#22686;&#24378;&#32452;&#21512;&#21644;&#21442;&#25968;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#26469;&#23398;&#20064;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26377;&#21033;&#22686;&#24378;&#31574;&#30053;&#65292;&#20294;&#20250;&#20135;&#29983;&#22823;&#37327;GPU&#24320;&#38144;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#25968;&#25454;&#22686;&#24378;&#65288;DDAug&#65289;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#19988;&#35745;&#31639;&#20195;&#20215;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#30340;DDAug&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#32467;&#26500;&#26469;&#34920;&#31034;&#21508;&#31181;&#22686;&#24378;&#65292;&#24182;&#21033;&#29992;&#39640;&#25928;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#26356;&#26032;&#12289;&#20462;&#21098;&#21644;&#25277;&#26679;&#26641;&#12290;&#22240;&#27492;&#65292;
&lt;/p&gt;
&lt;p&gt;
Medical image data are often limited due to the expensive acquisition and annotation process. Hence, training a deep-learning model with only raw data can easily lead to overfitting. One solution to this problem is to augment the raw data with various transformations, improving the model's ability to generalize to new data. However, manually configuring a generic augmentation combination and parameters for different datasets is non-trivial due to inconsistent acquisition approaches and data distributions. Therefore, automatic data augmentation is proposed to learn favorable augmentation strategies for different datasets while incurring large GPU overhead. To this end, we present a novel method, called Dynamic Data Augmentation (DDAug), which is efficient and has negligible computation cost. Our DDAug develops a hierarchical tree structure to represent various augmentations and utilizes an efficient Monte-Carlo tree searching algorithm to update, prune, and sample the tree. As a result,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Riesz&#26680;&#23637;&#31034;&#20102;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11463</link><description>&lt;p&gt;
&#21033;&#29992;Riesz&#26680;&#30340;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;
&lt;/p&gt;
&lt;p&gt;
Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Riesz&#26680;&#23637;&#31034;&#20102;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35745;&#31639;&#20013;&#65292;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#24230;(MMD)&#27969;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;Riesz&#26680;$K(x,y)=-\|x-y\|^r$&#65292;$r \in (0,2)$&#30340;MMD&#27969;&#20855;&#26377;&#26480;&#20986;&#30340;&#24615;&#36136;&#65292;&#21487;&#20801;&#35768;&#20854;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#12290;&#39318;&#20808;&#65292;Riesz&#26680;&#30340;MMD&#19982;&#20854;&#20998;&#21106;&#29256;&#26412;&#30340;MMD&#37325;&#21512;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22312;&#19968;&#32500;&#35774;&#32622;&#20013;&#36827;&#34892;MMD&#26799;&#24230;&#30340;&#35745;&#31639;&#12290;&#22312;&#27492;&#22788;&#65292;&#23545;&#20110;$r=1$&#65292;&#21487;&#20197;&#24212;&#29992;&#31616;&#21333;&#30340;&#25490;&#24207;&#31639;&#27861;&#23558;&#20004;&#20010;&#32463;&#39564;&#24230;&#37327;&#30340;&#22797;&#26434;&#24230;&#20174;$O(MN+N^2)$&#38477;&#20302;&#21040;$O((M+N)\log(M+N))$&#65292;&#20854;&#20013;$M$&#21644;$N$&#26159;&#25903;&#25345;&#28857;&#12290;&#23545;&#20110;&#23454;&#29616;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;$P$&#20010;&#20999;&#29255;&#26469;&#36817;&#20284;&#20998;&#21106;MMD&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#35823;&#24046;&#20855;&#26377;$O(\sqrt{d/P})$&#30340;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$d$&#26159;&#25968;&#25454;&#32500;&#24230;&#12290;&#36825;&#20123;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;MMD&#26799;&#24230;&#27969;&#26469;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#29978;&#33267;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancy (MMD) flows suffer from high computational costs in large scale computations. In this paper, we show that MMD flows with Riesz kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which allow for their efficient computation. First, the MMD of Riesz kernels coincides with the MMD of their sliced version. As a consequence, the computation of gradients of MMDs can be performed in the one-dimensional setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical measures with $M$ and $N$ support points. For the implementations we approximate the gradient of the sliced MMD by using only a finite number $P$ of slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where $d$ is the data dimension. These results enable us to train generative models by approximating MMD gradient flows by neural networks even for large scale applications. We demo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;pTSE&#65292;&#19968;&#31181;&#22522;&#20110;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#22810;&#27169;&#22411;&#20998;&#24067;&#38598;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.11304</link><description>&lt;p&gt;
pTSE:&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting. (arXiv:2305.11304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11304
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;pTSE&#65292;&#19968;&#31181;&#22522;&#20110;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#22810;&#27169;&#22411;&#20998;&#24067;&#38598;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#29616;&#20102;&#21508;&#31181;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#24182;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#36873;&#25321;&#39640;&#24230;&#20381;&#36182;&#20110;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#22522;&#20110;&#30340;&#22266;&#23450;&#20998;&#24067;&#12290;&#30001;&#20110;&#27010;&#29575;&#20998;&#24067;&#19981;&#33021;&#30452;&#25509;&#24179;&#22343;&#19981;&#21516;&#27169;&#22411;&#65292;&#30446;&#21069;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#25552;&#39640;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;pTSE&#65292;&#19968;&#31181;&#22522;&#20110;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#22810;&#27169;&#22411;&#20998;&#24067;&#38598;&#25104;&#26041;&#27861;&#12290;pTSE&#21482;&#38656;&#20174;&#25104;&#21592;&#27169;&#22411;&#33719;&#21462;&#29616;&#25104;&#36755;&#20986;&#65292;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#20102;&#35299;&#27599;&#20010;&#27169;&#22411;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;pTSE&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#26102;&#38388;&#24207;&#21015;&#32463;HMM&#22788;&#29702;&#21518;&#30340;&#32463;&#39564;&#20998;&#24067;&#20960;&#20046;&#19968;&#23450;&#25910;&#25947;&#20110;&#31283;&#24577;&#20998;&#24067;&#12290;&#22522;&#20934;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#21333;&#20010;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#26032;&#38598;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;pTSE&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various probabilistic time series forecasting models have sprung up and shown remarkably good performance. However, the choice of model highly relies on the characteristics of the input time series and the fixed distribution that the model is based on. Due to the fact that the probability distributions cannot be averaged over different models straightforwardly, the current time series model ensemble methods cannot be directly applied to improve the robustness and accuracy of forecasting. To address this issue, we propose pTSE, a multi-model distribution ensemble method for probabilistic forecasting based on Hidden Markov Model (HMM). pTSE only takes off-the-shelf outputs from member models without requiring further information about each model. Besides, we provide a complete theoretical analysis of pTSE to prove that the empirical distribution of time series subject to an HMM will converge to the stationary distribution almost surely. Experiments on benchmarks show the superiority of p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#65292;&#29992;&#20110;&#24494;&#35843;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#21512;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;UCF-101&#21644;MSR-VTT&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10474</link><description>&lt;p&gt;
&#20445;&#30041;&#20320;&#33258;&#24049;&#30340;&#30456;&#20851;&#24615;&#65306;&#29992;&#20110;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. (arXiv:2305.10474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#65292;&#29992;&#20110;&#24494;&#35843;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#21512;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;UCF-101&#21644;MSR-VTT&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#21512;&#25104;&#36830;&#32493;&#30340;&#21160;&#30011;&#24103;&#65292;&#26082;&#20855;&#26377;&#20809;&#30495;&#23454;&#24863;&#65292;&#21448;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#24615;&#20173;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#22312;&#21487;&#20197;&#20351;&#29992;&#25104;&#20159;&#32423;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#65292;&#25910;&#38598;&#30456;&#20284;&#35268;&#27169;&#30340;&#35270;&#39057;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#22270;&#20687;&#23545;&#24212;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#35757;&#32451;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#35745;&#31639;&#20195;&#20215;&#26356;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#35270;&#39057;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#35270;&#39057;&#21512;&#25104;&#20219;&#21153;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35270;&#39057;&#25193;&#25955;&#20013;&#22825;&#30495;&#22320;&#23558;&#22270;&#20687;&#22122;&#22768;&#20808;&#39564;&#25193;&#23637;&#20026;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#65292;&#20854;&#22312;&#35270;&#39057;&#25193;&#25955;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#26356;&#22909;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411; Preserve Your Own Correlation (PYoCo) &#22312; UCF-101 &#21644; MSR-VTT &#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#23545;&#35270;&#39057;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own Correlation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also
&lt;/p&gt;</description></item><item><title>MaxViT-UNet&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#22312;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.08396</link><description>&lt;p&gt;
MaxViT-UNet: &#22810;&#36724;&#27880;&#24847;&#21147;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08396
&lt;/p&gt;
&lt;p&gt;
MaxViT-UNet&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#22312;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21367;&#31215;&#31639;&#23376;&#30340;&#23616;&#37096;&#24615;&#36136;&#25233;&#21046;&#20102;CNNs&#25429;&#25417;&#20840;&#23616;&#21644;&#38271;&#31243;&#20132;&#20114;&#12290;&#26368;&#36817;&#65292;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#20294;&#26159;&#65292;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;CNN&#31867;&#24402;&#32435;&#20559;&#24046;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MaxViT-UNet&#65292;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#25552;&#20986;&#30340;&#28151;&#21512;&#35299;&#30721;&#22120;&#65292;&#36824;&#22522;&#20110;MaxViT-block&#65292;&#26088;&#22312;&#22312;&#27599;&#20010;&#35299;&#30721;&#38454;&#27573;&#26368;&#23567;&#21270;&#35745;&#31639;&#36127;&#25285;&#19979;&#21033;&#29992;&#21367;&#31215;&#21644;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#21147;&#37327;&#12290;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#30340;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;&#28151;&#21512;&#35299;&#30721;&#22120;&#22359;&#26368;&#21021;&#36890;&#36807;&#19978;&#37319;&#26679;&#20256;&#36755;&#20302;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have made significant strides in medical image analysis in recent years. However, the local nature of the convolution operator inhibits the CNNs from capturing global and long-range interactions. Recently, Transformers have gained popularity in the computer vision community and also medical image segmentation. But scalability issues of self-attention mechanism and lack of the CNN like inductive bias have limited their adoption. In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision transformer for medical image segmentation. The proposed hybrid decoder, also based on MaxViT-block, is designed to harness the power of convolution and self-attention mechanism at each decoding stage with minimal computational burden. The multi-axis self-attention in each decoder stage helps in differentiating between the object and background regions much more efficiently. The hybrid decoder block initially fuses the lower level features upsampled via tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#38754;&#20307;&#29702;&#35770;&#30340;&#20132;&#21449;&#39046;&#22495;&#12290;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#31561;&#20989;&#25968;&#20351;&#24471;&#19968;&#20123;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#33021;&#22815;&#36890;&#36807;&#22810;&#38754;&#20307;&#29702;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#24212;&#29992;&#32447;&#24615;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#23454;&#29616;&#32593;&#32476;&#20462;&#21098;&#12289;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.00241</link><description>&lt;p&gt;
&#24403;&#28145;&#24230;&#23398;&#20064;&#36935;&#35265;&#22810;&#38754;&#20307;&#29702;&#35770;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Deep Learning Meets Polyhedral Theory: A Survey. (arXiv:2305.00241v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#38754;&#20307;&#29702;&#35770;&#30340;&#20132;&#21449;&#39046;&#22495;&#12290;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#31561;&#20989;&#25968;&#20351;&#24471;&#19968;&#20123;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#33021;&#22815;&#36890;&#36807;&#22810;&#38754;&#20307;&#29702;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#24212;&#29992;&#32447;&#24615;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#23454;&#29616;&#32593;&#32476;&#20462;&#21098;&#12289;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#20102;&#39044;&#27979;&#24314;&#27169;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#24471;&#30410;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#22238;&#24402;&#21040;&#20102;&#22522;&#20110;&#20998;&#27573;&#24120;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#31616;&#21333;&#34920;&#31034;&#65292;&#20363;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#65292;&#36825;&#31181;&#28608;&#27963;&#20989;&#25968;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#24120;&#29992;&#30340;&#31867;&#22411;&#12290;&#36825;&#20351;&#24471;&#26576;&#20123;&#31867;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22914;&#20856;&#22411;&#30340;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#38754;&#20307;&#29702;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#24212;&#29992;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#31561;&#26041;&#27861;&#29992;&#20110;&#21508;&#31181;&#30446;&#30340;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#28044;&#29616;&#30340;&#20027;&#35201;&#20027;&#39064;&#65292;&#20026;&#26356;&#35814;&#32454;&#22320;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#24212;&#29992;&#25968;&#23398;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#38754;&#20307;&#29702;&#35770;&#30340;&#22522;&#30784;&#30693;&#35782;&#20197;&#21450;&#23427;&#19982;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#24182;&#22238;&#39038;&#20102;&#35813;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22312;&#32593;&#32476;&#20462;&#21098;&#12289;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31561;&#20219;&#21153;&#20013;&#20351;&#29992;LP&#21644;MILP&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit (ReLU), which became the most commonly used type of activation function in neural networks. That made certain types of network structure $\unicode{x2014}$such as the typical fully-connected feedforward neural network$\unicode{x2014}$ amenable to analysis through polyhedral theory and to the application of methodologies such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this paper, we survey the main topics emerging from this fast-paced area of work, which bring a fresh perspective to understanding neural networks in more detail as well as to app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21021;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#23450;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#20840;&#23616;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340; PDE &#27531;&#24046;&#20013;&#36935;&#21040;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644; ODE &#26041;&#27861;&#20013;&#38543;&#30528;&#25968;&#37327;&#21576;&#31435;&#26041;&#32423;&#21035;&#25193;&#23637;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14994</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21021;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#23450;&#21487;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks. (arXiv:2304.14994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21021;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#23450;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#20840;&#23616;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340; PDE &#27531;&#24046;&#20013;&#36935;&#21040;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644; ODE &#26041;&#27861;&#20013;&#38543;&#30528;&#25968;&#37327;&#21576;&#31435;&#26041;&#32423;&#21035;&#25193;&#23637;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#32593;&#26684;&#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#31070;&#32463;&#32593;&#32476;&#26377;&#21487;&#33021;&#25171;&#30772;&#32500;&#25968;&#28798;&#38590;&#65292;&#22312;&#20351;&#29992;&#32463;&#20856;&#27714;&#35299;&#22120;&#22256;&#38590;&#25110;&#19981;&#21487;&#33021;&#30340;&#38382;&#39064;&#20013;&#25552;&#20379;&#36817;&#20284;&#35299;&#12290;&#20840;&#23616;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340; PDE &#27531;&#24046;&#23545;&#20110;&#36793;&#30028;&#20540;&#38382;&#39064;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#26159;&#28798;&#38590;&#24615;&#24536;&#21364;&#25439;&#23475;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#21021;&#20540;&#38382;&#39064;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26367;&#20195;&#30340;&#23616;&#37096;&#26102;&#38388;&#26041;&#27861;&#20013;&#65292;&#21487;&#20197;&#23558;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#32593;&#32476;&#21442;&#25968;&#19978;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#24182;&#23558;&#35299;&#21521;&#21069;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30446;&#21069;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36981;&#24490; ODE &#20250;&#23548;&#33268;&#38382;&#39064;&#26465;&#20214;&#22686;&#38271;&#26080;&#27861;&#25511;&#21046;&#65292;&#26368;&#32456;&#23548;&#33268;&#19981;&#21487;&#25509;&#21463;&#30340;&#22823;&#25968;&#20540;&#35823;&#24046;&#12290;&#20854;&#27425;&#65292;&#38543;&#30528; ODE &#26041;&#27861;&#38543;&#30528; m &#30340;&#25968;&#37327;&#21576;&#31435;&#26041;&#32423;&#21035;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difficult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs the applicability of this approach to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#24110;&#21161;&#30382;&#32932;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2304.14505</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#29992;&#20110;&#30382;&#32932;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based interpretable multi-modal data fusion for skin lesion classification. (arXiv:2304.14505v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#24110;&#21161;&#30382;&#32932;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#23450;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#22240;&#32032;&#12290;&#22312;&#20154;&#31867;&#20013;&#24515;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#30382;&#32932;&#30149;&#20998;&#31867;&#22312;&#30382;&#32932;&#31185;&#20013;&#65292;&#20173;&#22788;&#20110;&#20854;&#21021;&#32423;&#38454;&#27573;&#30340;DL&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#30001;&#20110;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#33021;&#22815;&#35299;&#37322;&#35757;&#32451;&#30340;DL&#31639;&#27861;&#34892;&#20026;&#30340;&#31243;&#24207;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#20020;&#24202;&#21307;&#24072;&#30340;&#20449;&#20219;&#12290;&#20026;&#35786;&#26029;&#30382;&#32932;&#30149;&#21464;&#65292;&#30382;&#32932;&#31185;&#21307;&#29983;&#20381;&#38752;&#30142;&#30149;&#30340;&#35270;&#35273;&#35780;&#20272;&#21644;&#24739;&#32773;&#30149;&#21490;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#21463;&#38480;&#20110;&#21367;&#31215;&#32467;&#26500;&#25152;&#38656;&#30340;&#29305;&#24449;&#32423;&#21644;&#20915;&#31574;&#32423;&#34701;&#21512;&#31243;&#24207;&#30340;&#20998;&#31163;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#30340;&#27880;&#24847;&#26426;&#21046;&#23454;&#29616;&#21333;&#38454;&#27573;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#65292;&#20197;&#24110;&#21161;&#35786;&#26029;&#30382;&#32932;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;
A lot of deep learning (DL) research these days is mainly focused on improving on quantitative metrics regardless of other factors. In human centered applications, like skin lesion classification in dermatology, DL-driven clinical decision support systems are still in their infancy due to the limited transparency of their decision-making process. Moreover, the lack of procedures that can explain the behavior of trained DL algorithms leads to almost no trust from the clinical physicians. To diagnose skin lesions, dermatologists rely on both visual assessment of the disease and the data gathered from the anamnesis of the patient. Data-driven algorithms dealing with multi-modal data are limited by the separation of feature-level and decision-level fusion procedures required by convolutional architectures. To address this issue, we enable single-stage multi-modal data fusion via the attention mechanism of transformer-based architectures to aid in the diagnosis of skin diseases. Our method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gromov-Wasserstein Discrepancy&#36873;&#25321;&#26368;&#20339;&#20107;&#20214;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#65292;&#26412;&#25991;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13455</link><description>&lt;p&gt;
&#20174;&#28151;&#27788;&#20013;&#36856;&#21457;&#20986;&#31209;&#24207;&#65306;&#20026;&#29289;&#20307;&#26816;&#27979;&#25490;&#24207;&#20107;&#20214;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gromov-Wasserstein Discrepancy&#36873;&#25321;&#26368;&#20339;&#20107;&#20214;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#65292;&#26412;&#25991;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22788;&#29702;&#20107;&#20214;&#30340;&#39030;&#23574;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#29616;&#25104;&#32593;&#32476;&#20043;&#21069;&#65292;&#39318;&#20808;&#23558;&#20854;&#36716;&#25442;&#20026;&#31264;&#23494;&#30340;&#32593;&#26684;&#29366;&#36755;&#20837;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#20026;&#20219;&#21153;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#31034;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#34920;&#31034;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26681;&#25454;&#39564;&#35777;&#20998;&#25968;&#36873;&#25321;&#26368;&#20339;&#34920;&#31034;&#65292;&#36825;&#38750;&#24120;&#32791;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#21407;&#22987;&#20107;&#20214;&#21450;&#20854;&#34920;&#31034;&#20043;&#38388;&#30340;Gromov-Wasserstein Discrepancy (GWD)&#36873;&#25321;&#26368;&#20339;&#34920;&#31034;&#26469;&#28040;&#38500;&#36825;&#20010;&#29942;&#39048;&#12290;&#23427;&#30340;&#35745;&#31639;&#36895;&#24230;&#22823;&#32422;&#27604;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24555;200&#20493;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20107;&#20214;&#34920;&#31034;&#27861;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#25214;&#21040;&#20855;&#26377;&#39640;&#20219;&#21153;&#20998;&#25968;&#30340;&#34920;&#31034;&#30456;&#24403;&#20110;&#25214;&#21040;&#20855;&#26377;&#20302;GWD&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#39318;&#27425;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Moving MNIST&#21644;N-Caltech101&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#21518;&#32773;&#36798;&#21040;&#20102;83.0%&#30340;1%&#35823;&#25253;&#29575;&#19979;&#30340;mAP&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2304.06720</link><description>&lt;p&gt;
&#23500;&#25991;&#26412;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Expressive Text-to-Image Generation with Rich Text. (arXiv:2304.06720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#25991;&#23383;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#27969;&#34892;&#30028;&#38754;&#12290;&#20294;&#26159;&#65292;&#23427;&#30340;&#23450;&#21046;&#36873;&#39033;&#26377;&#38480;&#65292;&#38459;&#30861;&#20102;&#29992;&#25143;&#31934;&#30830;&#25551;&#36848;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25903;&#25345;&#23383;&#20307;&#26679;&#24335;&#12289;&#22823;&#23567;&#12289;&#39068;&#33394;&#21644;&#33050;&#27880;&#31561;&#26684;&#24335;&#30340;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#12290;&#25105;&#20204;&#20174;&#23500;&#25991;&#26412;&#20013;&#25552;&#21462;&#27599;&#20010;&#23383;&#30340;&#23646;&#24615;&#65292;&#20197;&#21551;&#29992;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#36825;&#20123;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#26041;&#27861;&#26356;&#22909;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plain text has become a prevalent interface for text-to-image synthesis. However, its limited customization options hinder users from accurately describing desired outputs. For example, plain text makes it hard to specify continuous quantities, such as the precise RGB color value or importance of each word. Furthermore, creating detailed text prompts for complex scenes is tedious for humans to write and challenging for text encoders to interpret. To address these challenges, we propose using a rich-text editor supporting formats such as font style, size, color, and footnote. We extract each word's attributes from rich text to enable local style control, explicit token reweighting, precise color rendering, and detailed region synthesis. We achieve these capabilities through a region-based diffusion process. We first obtain each word's region based on cross-attention maps of a vanilla diffusion process using plain text. For each region, we enforce its text attributes by creating region-s
&lt;/p&gt;</description></item><item><title>KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.15487</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15487
&lt;/p&gt;
&lt;p&gt;
KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#33258;&#28982;&#31185;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#25110;&#35821;&#20041;&#32593;&#12290;&#23613;&#31649;&#23500;&#21547;&#20449;&#24687;&#65292;&#20294;&#22270;&#24418;&#36890;&#24120;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#12290;&#22240;&#27492;&#65292;&#22270;&#34917;&#20840;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#65292;&#24050;&#32463;&#21463;&#21040;&#20851;&#27880;&#12290;&#19968;&#26041;&#38754;&#65292;&#31070;&#32463;&#26041;&#27861;&#65288;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#22122;&#22768;&#22270;&#30340;&#31283;&#20581;&#24037;&#20855;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31526;&#21495;&#26041;&#27861;&#21487;&#20197;&#23545;&#22270;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KeGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#22270;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#33539;&#20363;&#65292;&#24182;&#20801;&#35768;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#20174;&#26412;&#36136;&#19978;&#35762;&#65292;KeGNN&#30001;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#22522;&#20110;&#30446;&#26631;&#23558;&#30693;&#35782;&#22686;&#24378;&#23618;&#22534;&#21472;&#22312;&#20854;&#19978;&#65292;&#20197;&#20351;&#38024;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#39044;&#27979;&#24471;&#21040;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;KeGNN&#19982;&#20004;&#20010;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19968;&#36215;&#23454;&#20363;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#19977;&#35282;&#26041;&#31243;&#32452;&#19978;&#20351;&#29992;&#36890;&#29992;&#30340;&#8220;backslash&#8221;&#25110;&#39640;&#26031;&#28040;&#20803;&#26469;&#35745;&#31639;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#30340;&#32447;&#24615;&#20195;&#25968;&#20844;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#20248;&#21270;&#21644;&#23454;&#29616;&#20415;&#21033;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15449</link><description>&lt;p&gt;
&#36890;&#36807;BACKslash&#23454;&#29616;BACK substitution&#30340;BACKpropagation&#32447;&#24615;&#20195;&#25968;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
BACKpropagation through BACK substitution with a BACKslash. (arXiv:2303.15449v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#19977;&#35282;&#26041;&#31243;&#32452;&#19978;&#20351;&#29992;&#36890;&#29992;&#30340;&#8220;backslash&#8221;&#25110;&#39640;&#26031;&#28040;&#20803;&#26469;&#35745;&#31639;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#30340;&#32447;&#24615;&#20195;&#25968;&#20844;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#20248;&#21270;&#21644;&#23454;&#29616;&#20415;&#21033;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#19977;&#35282;&#26041;&#31243;&#32452;&#19978;&#20351;&#29992;&#36890;&#29992;&#30340;&#8220;backslash&#8221;&#25110;&#39640;&#26031;&#28040;&#20803;&#26469;&#35745;&#31639;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#30340;&#32447;&#24615;&#20195;&#25968;&#20844;&#24335;&#12290;&#36890;&#24120;&#65292;&#30697;&#38453;&#20803;&#32032;&#26159;&#31639;&#23376;&#12290;&#36825;&#31687;&#35770;&#25991;&#26377;&#19977;&#20010;&#36129;&#29486;&#65306;1.&#29992;&#24038;&#20316;&#29992;&#30340;&#31639;&#23376;&#29702;&#35770;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#21462;&#20195;&#20256;&#32479;&#30340;&#33258;&#21160;&#24494;&#20998;&#22788;&#29702;&#20855;&#26377;&#30693;&#35782;&#20215;&#20540;&#12290;2.&#21487;&#20197;&#23558;&#31639;&#23376;&#25918;&#32622;&#22312;&#30697;&#38453;&#20013;&#20316;&#20026;&#23454;&#29616;&#36873;&#39033;&#30340;&#36719;&#20214;&#20013;&#65292;&#22914;Julia&#12290;3.&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#65292;&#8220;transpose dot&#8221;&#25805;&#20316;&#31526;&#8220;$\{\}^{T_\bullet}$&#8221;&#65292;&#20801;&#35768;&#21453;&#36716;&#31639;&#23376;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31639;&#23376;&#26041;&#27861;&#22312;&#36866;&#21512;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#65288;&#22914;Julia&#65289;&#30340;&#20248;&#38597;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#25277;&#35937;&#21487;&#20197;&#22312;&#20195;&#30721;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#26045;&#23637;&#31034;&#20102;&#36890;&#29992;&#32447;&#24615;&#20195;&#25968;&#22914;&#20309;&#23454;&#29616;&#20256;&#32479;&#29305;&#27530;&#24418;&#24335;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;backpropagation&#65292;&#20197;&#30697;&#38453;&#25805;&#20316;&#30340;&#24418;&#24335;&#20070;&#20889;&#65292;&#36825;&#25171;&#24320;&#20102;&#20248;&#21270;&#21644;&#23454;&#26045;&#20415;&#21033;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a linear algebra formulation of backpropagation which allows the calculation of gradients by using a generically written ``backslash'' or Gaussian elimination on triangular systems of equations. Generally the matrix elements are operators. This paper has three contributions:  1. It is of intellectual value to replace traditional treatments of automatic differentiation with a (left acting) operator theoretic, graph-based approach.  2. Operators can be readily placed in matrices in software in programming languages such as Ju lia as an implementation option.  3. We introduce a novel notation, ``transpose dot'' operator ``$\{\}^{T_\bullet}$'' that allows the reversal of operators.  We demonstrate the elegance of the operators approach in a suitable programming language consisting of generic linear algebra operators such as Julia \cite{bezanson2017julia}, and that it is possible to realize this abstraction in code. Our implementation shows how generic linear algebra can allow op
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.12743</link><description>&lt;p&gt;
DR.CPO&#65306;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#12289;&#38543;&#26426;&#25918;&#32622;&#21644; HPR &#36974;&#34109;&#23454;&#29616;&#30340;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#19977;&#32500;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#24120;&#29992;&#20110;&#25913;&#36827;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#26368;&#22522;&#26412;&#30340;&#26041;&#27861;&#21253;&#25324;&#25554;&#20837;&#22797;&#21046;&#23545;&#35937;&#21644;&#26059;&#36716;&#21644;&#32553;&#25918;&#25972;&#20010;&#35757;&#32451;&#24103;&#12290;&#20063;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21487;&#33021;&#24615;&#30456;&#27604;&#30456;&#24403;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#26500;&#36896;&#25972;&#20307;&#23545;&#35937;&#65292;&#33258;&#30001;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#20026;&#20102;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20174;&#29616;&#23454;&#19990;&#30028;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#23545;&#35937;&#38543;&#26426;&#32452;&#21512;&#25104;&#21333;&#20010;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#22312;&#35757;&#32451;&#24103;&#20013;&#65292;&#22240;&#20026;&#36866;&#24403;&#30340;&#36974;&#25377;&#21487;&#20197;&#21453;&#26144;&#22312;&#26368;&#32456;&#25972;&#20307;&#23545;&#35937;&#20013;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38450;&#27490;&#36807;&#24230;&#22686;&#24378;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#36974;&#25377;&#27010;&#29575;&#35774;&#32622;&#65292;&#36890;&#36807;&#23545;&#35937;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#35843;&#25972;&#36974;&#25377;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25910;&#38598;&#39134;&#34892;&#36712;&#36857;&#21644;&#31243;&#24207;&#25968;&#25454;&#23398;&#20064;&#39134;&#34892;&#22120;&#34892;&#20026;&#21464;&#24322;&#24615;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#28041;&#21450;&#20219;&#24847;&#25968;&#37327;&#39134;&#34892;&#22120;&#30340;&#20132;&#36890;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09981</link><description>&lt;p&gt;
&#20174;&#39134;&#34892;&#36712;&#36857;&#21644;&#31243;&#24207;&#20013;&#25512;&#26029;&#32456;&#31471;&#31354;&#22495;&#20132;&#36890;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Inferring Traffic Models in Terminal Airspace from Flight Tracks and Procedures. (arXiv:2303.09981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25910;&#38598;&#39134;&#34892;&#36712;&#36857;&#21644;&#31243;&#24207;&#25968;&#25454;&#23398;&#20064;&#39134;&#34892;&#22120;&#34892;&#20026;&#21464;&#24322;&#24615;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#28041;&#21450;&#20219;&#24847;&#25968;&#37327;&#39134;&#34892;&#22120;&#30340;&#20132;&#36890;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#30340;&#33322;&#31354;&#22120;&#36712;&#36857;&#27169;&#22411;&#23545;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#65288;ATM&#65289;&#31995;&#32479;&#35774;&#35745;&#21644;&#39564;&#35777;&#24456;&#26377;&#29992;&#12290;&#20202;&#34920;&#39134;&#34892;&#35268;&#21017;&#65288;IFR&#65289;&#19979;&#25805;&#20316;&#30340;&#39134;&#34892;&#22120;&#27169;&#22411;&#38656;&#35201;&#25429;&#25417;&#39134;&#34892;&#22120;&#25353;&#29031;&#26631;&#20934;&#39134;&#34892;&#31243;&#24207;&#30340;&#22266;&#26377;&#21464;&#24322;&#24615;&#12290;&#39134;&#34892;&#22120;&#34892;&#20026;&#30340;&#21464;&#24322;&#24615;&#22312;&#19981;&#21516;&#30340;&#39134;&#34892;&#38454;&#27573;&#20043;&#38388;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#31243;&#24207;&#25968;&#25454;&#21644;&#20174;&#38647;&#36798;&#30417;&#35270;&#25968;&#25454;&#25910;&#38598;&#30340;&#39134;&#34892;&#36712;&#36857;&#20013;&#23398;&#20064;&#21464;&#24322;&#24615;&#12290; &#23545;&#20110;&#27599;&#20010;&#27573;&#33853;&#65292;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23398;&#20064;&#39134;&#34892;&#22120;&#36712;&#36857;&#19982;&#20854;&#31243;&#24207;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#32473;&#23450;&#26032;&#30340;&#31243;&#24207;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#25277;&#26679;&#19968;&#31995;&#21015;&#20559;&#24046;&#65292;&#24182;&#20351;&#29992;&#20559;&#24046;&#21644;&#31243;&#24207;&#37325;&#26500;&#39134;&#34892;&#22120;&#36712;&#36857;&#26469;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#25429;&#25417;&#39134;&#34892;&#22120;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#25104;&#23545;&#27169;&#22411;&#26469;&#29983;&#25104;&#28041;&#21450;&#20219;&#24847;&#25968;&#37327;&#39134;&#34892;&#22120;&#30340;&#20132;&#36890;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic aircraft trajectory models are useful in the design and validation of air traffic management (ATM) systems. Models of aircraft operated under instrument flight rules (IFR) require capturing the variability inherent in how aircraft follow standard flight procedures. The variability in aircraft behavior varies among flight stages. In this paper, we propose a probabilistic model that can learn the variability from the procedural data and flight tracks collected from radar surveillance data. For each segment, a Gaussian mixture model is used to learn the deviations of aircraft trajectories from their procedures. Given new procedures, we can generate synthetic trajectories by sampling a series of deviations from the trained Gaussian distributions and reconstructing the aircraft trajectory using the deviations and the procedures. We extend this method to capture pairwise correlations between aircraft and show how a pairwise model can be used to generate traffic involving an arbitra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20998;&#37197;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#65292;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08566</link><description>&lt;p&gt;
&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Visual Parameter-Efficient Tuning. (arXiv:2303.08566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20998;&#37197;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#65292;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;VPET&#65289;&#24050;&#25104;&#20026;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24378;&#21170;&#26367;&#20195;&#26041;&#27861;&#12290;&#29616;&#26377;VPET&#26041;&#27861;&#26681;&#25454;&#20154;&#24037;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#24341;&#20837;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#21516;&#20301;&#32622;&#65292;&#24573;&#30053;&#39046;&#22495;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#20998;&#37197;&#21487;&#35757;&#32451;&#21442;&#25968;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#32473;&#23450;&#25152;&#38656;&#30340;&#21487;&#35843;&#21442;&#25968;&#39044;&#31639;&#12290;&#26412;&#25991;&#39318;&#20808;&#20381;&#25454;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#24555;&#36895;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#35843;&#25972;&#30340;&#25935;&#24863;&#21442;&#25968;&#65292;&#28982;&#21518;&#25552;&#21319;&#34920;&#31034;&#33021;&#21147;&#65292;&#22686;&#22823;&#37325;&#35201;&#30340;&#26435;&#37325;&#30697;&#38453;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing VPET methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of se
&lt;/p&gt;</description></item><item><title>StyleDiff&#26159;&#19968;&#31181;&#22312;&#28508;&#22312;&#35299;&#32544;&#31354;&#38388;&#20013;&#27604;&#36739;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#23646;&#24615;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#20102;&#35299;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#24046;&#24322;&#65292;&#24182;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#25552;&#20379;&#20998;&#26512;&#12290;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05102</link><description>&lt;p&gt;
StyleDiff: &#22312;&#28508;&#22312;&#35299;&#32544;&#31354;&#38388;&#20013;&#27604;&#36739;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#23646;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space. (arXiv:2303.05102v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05102
&lt;/p&gt;
&lt;p&gt;
StyleDiff&#26159;&#19968;&#31181;&#22312;&#28508;&#22312;&#35299;&#32544;&#31354;&#38388;&#20013;&#27604;&#36739;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#23646;&#24615;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#20102;&#35299;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#24046;&#24322;&#65292;&#24182;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#25552;&#20379;&#20998;&#26512;&#12290;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#35299;&#20915;&#24320;&#21457;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19982;&#23454;&#38469;&#24212;&#29992;&#20013;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#36825;&#20123;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#19981;&#20934;&#30830;&#21644;&#38169;&#35823;&#65292;&#36827;&#32780;&#24433;&#21709;&#20135;&#21697;&#36136;&#37327;&#21644;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;StyleDiff&#65292;&#20197;&#20415;&#24320;&#21457;&#20154;&#21592;&#20102;&#35299;&#20004;&#20010;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#31283;&#23450;&#21457;&#23637;&#12290;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#30340;&#35299;&#32544;&#22270;&#20687;&#31354;&#38388;&#65292;StyleDiff&#36890;&#36807;&#20851;&#27880;&#22270;&#20687;&#20013;&#30340;&#23646;&#24615;&#26469;&#27604;&#36739;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#24046;&#24322;&#20998;&#26512;&#12290;&#25152;&#25552;&#20986;&#30340;StyleDiff&#30340;&#24615;&#33021;&#20026;$O(dN\log N)$&#65292;&#20854;&#20013;$N$&#26159;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;$d$&#26159;&#23646;&#24615;&#30340;&#25968;&#37327;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;StyleDiff&#33021;&#20934;&#30830;&#26816;&#27979;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26684;&#24335;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major challenge in machine learning applications is coping with mismatches between the datasets used in the development and those obtained in real-world applications. These mismatches may lead to inaccurate predictions and errors, resulting in poor product quality and unreliable systems. In this study, we propose StyleDiff to inform developers of the differences between the two datasets for the steady development of machine learning systems. Using disentangled image spaces obtained from recently proposed generative models, StyleDiff compares the two datasets by focusing on attributes in the images and provides an easy-to-understand analysis of the differences between the datasets. The proposed StyleDiff performs in $O (d N\log N)$, where $N$ is the size of the datasets and $d$ is the number of attributes, enabling the application to large datasets. We demonstrate that StyleDiff accurately detects differences between datasets and presents them in an understandable format using, for 
&lt;/p&gt;</description></item><item><title>Collage Diffusion&#36890;&#36807;&#22270;&#23618;&#24314;&#27169;&#21644;&#21327;&#35843;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#25193;&#25955;&#22270;&#20687;&#29983;&#25104;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27599;&#20010;&#23545;&#35937;&#19978;&#35843;&#25972;&#22270;&#20687;&#21327;&#35843;&#31243;&#24230;&#65292;&#24182;&#21487;&#20197;&#22312;&#20445;&#25345;&#20854;&#20182;&#23545;&#35937;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#32534;&#36753;&#21333;&#20010;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.00262</link><description>&lt;p&gt;
&#33945;&#22826;&#22855;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Collage Diffusion. (arXiv:2303.00262v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00262
&lt;/p&gt;
&lt;p&gt;
Collage Diffusion&#36890;&#36807;&#22270;&#23618;&#24314;&#27169;&#21644;&#21327;&#35843;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#25193;&#25955;&#22270;&#20687;&#29983;&#25104;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27599;&#20010;&#23545;&#35937;&#19978;&#35843;&#25972;&#22270;&#20687;&#21327;&#35843;&#31243;&#24230;&#65292;&#24182;&#21487;&#20197;&#22312;&#20445;&#25345;&#20854;&#20182;&#23545;&#35937;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#32534;&#36753;&#21333;&#20010;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23558;&#22797;&#26434;&#22330;&#26223;&#24314;&#27169;&#20026;&#22270;&#23618;&#24207;&#21015;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#29983;&#25104;&#20855;&#26377;&#31934;&#30830;&#25511;&#21046;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#22270;&#23618;&#23450;&#20041;&#20102;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#26399;&#26395;&#31354;&#38388;&#24067;&#32622;&#21644;&#35270;&#35273;&#23646;&#24615;&#12290;&#33945;&#22826;&#22855;&#25193;&#25955;&#20351;&#36755;&#20837;&#22270;&#23618;&#21327;&#35843;&#19968;&#33268;&#65292;&#20351;&#23545;&#35937;&#20114;&#30456;&#36866;&#24212; - &#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22312;&#21327;&#35843;&#36807;&#31243;&#20013;&#26368;&#23567;&#21270;&#36755;&#20837;&#22270;&#23618;&#30340;&#20301;&#32622;&#21644;&#20027;&#35201;&#35270;&#35273;&#23646;&#24615;&#30340;&#21464;&#21270;&#65292;&#21516;&#26102;&#20801;&#35768;&#20854;&#20182;&#23646;&#24615;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#25991;&#26412;-&#22270;&#20687;&#20132;&#21449;&#27880;&#24847;&#21147;&#19982;&#22270;&#23618;&#30340;alpha&#25513;&#27169;&#26469;&#30830;&#20445;&#23545;&#35937;&#22312;&#27491;&#30830;&#20301;&#32622;&#29983;&#25104;&#12290;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#22270;&#23618;&#30340;&#19987;&#38376;&#25991;&#26412;&#34920;&#31034;&#65292;&#24182;&#25193;&#23637;ControlNet&#20197;&#25805;&#20316;&#22270;&#23618;&#65292;&#25105;&#20204;&#21487;&#20197;&#20445;&#30041;&#36755;&#20837;&#22270;&#23618;&#30340;&#20851;&#38190;&#35270;&#35273;&#23646;&#24615;&#12290;&#22270;&#23618;&#36755;&#20837;&#20801;&#35768;&#29992;&#25143;&#22312;&#27599;&#20010;&#23545;&#35937;&#19978;&#22522;&#20110;&#23545;&#35937;&#25511;&#21046;&#22270;&#20687;&#21327;&#35843;&#30340;&#31243;&#24230;&#65292;&#24182;&#19988;&#29992;&#25143;&#29978;&#33267;&#21487;&#20197;&#22312;&#20445;&#25345;&#20854;&#20182;&#23545;&#35937;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#36845;&#20195;&#22320;&#32534;&#36753;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#21333;&#20010;&#23545;&#35937;&#12290;&#36890;&#36807;&#21033;&#29992;&#20016;&#23500;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
We seek to give users precise control over diffusion-based image generation by modeling complex scenes as sequences of layers, which define the desired spatial arrangement and visual attributes of objects in the scene. Collage Diffusion harmonizes the input layers to make objects fit together -- the key challenge involves minimizing changes in the positions and key visual attributes of the input layers while allowing other attributes to change in the harmonization process. We ensure that objects are generated in the correct locations by modifying text-image cross-attention with the layers' alpha masks. We preserve key visual attributes of input layers by learning specialized text representations per layer and by extending ControlNet to operate on layers. Layer input allows users to control the extent of image harmonization on a per-object basis, and users can even iteratively edit individual objects in generated images while keeping other objects fixed. By leveraging the rich informati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#23646;&#24615;&#34917;&#20840;&#26041;&#27861;FairAC&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#23646;&#24615;&#30340;&#22270;&#25968;&#25454;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;FairAC&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#23646;&#24615;&#21644;&#34917;&#20840;&#23548;&#33268;&#30340;&#20004;&#31181;&#19981;&#20844;&#24179;&#24615;&#65292;&#21363;&#23646;&#24615;&#19981;&#20844;&#24179;&#21644;&#25299;&#25169;&#19981;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2302.12977</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#32570;&#22833;&#23646;&#24615;&#30340;&#22270;&#20013;&#36827;&#34892;&#20844;&#24179;&#23646;&#24615;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Fair Attribute Completion on Graph with Missing Attributes. (arXiv:2302.12977v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#23646;&#24615;&#34917;&#20840;&#26041;&#27861;FairAC&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#23646;&#24615;&#30340;&#22270;&#25968;&#25454;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;FairAC&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#23646;&#24615;&#21644;&#34917;&#20840;&#23548;&#33268;&#30340;&#20004;&#31181;&#19981;&#20844;&#24179;&#24615;&#65292;&#21363;&#23646;&#24615;&#19981;&#20844;&#24179;&#21644;&#25299;&#25169;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22270;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#28041;&#21450;&#23646;&#24615;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;&#29616;&#26377;&#30340;&#20844;&#24179;&#22270;&#23398;&#20064;&#24037;&#20316;&#20551;&#35774;&#35757;&#32451;&#27169;&#22411;&#26102;&#25152;&#26377;&#33410;&#28857;&#30340;&#23646;&#24615;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#28982;&#21518;&#36827;&#34892;&#20844;&#24179;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#25968;&#25454;&#32570;&#22833;&#25110;&#38544;&#31169;&#38382;&#39064;&#65292;&#19968;&#20123;&#33410;&#28857;&#30340;&#23646;&#24615;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#65292;&#36825;&#20351;&#24471;&#20844;&#24179;&#22270;&#23398;&#20064;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23646;&#24615;&#34917;&#20840;&#26041;&#27861;FairAC&#65292;&#29992;&#20110;&#34917;&#20840;&#32570;&#22833;&#20449;&#24687;&#24182;&#23398;&#20064;&#20855;&#26377;&#20844;&#24179;&#24615;&#30340;&#22270;&#33410;&#28857;&#23884;&#20837;&#12290;FairAC&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#23646;&#24615;&#21644;&#34917;&#20840;&#23548;&#33268;&#30340;&#20004;&#31181;&#19981;&#20844;&#24179;&#24615;&#65292;&#21363;&#23646;&#24615;&#19981;&#20844;&#24179;&#21644;&#25299;&#25169;&#19981;&#20844;&#24179;&#12290;FairAC&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#21516;&#36136;&#22270;&#24182;&#20026;&#23427;&#20204;&#29983;&#25104;&#20844;&#24179;&#30340;&#23884;&#20837;&#65292;&#22240;&#27492;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22810;&#25968;&#22270;&#25968;&#25454;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tackling unfairness in graph learning models is a challenging task, as the unfairness issues on graphs involve both attributes and topological structures. Existing work on fair graph learning simply assumes that attributes of all nodes are available for model training and then makes fair predictions. In practice, however, the attributes of some nodes might not be accessible due to missing data or privacy concerns, which makes fair graph learning even more challenging. In this paper, we propose FairAC, a fair attribute completion method, to complement missing information and learn fair node embeddings for graphs with missing attributes. FairAC adopts an attention mechanism to deal with the attribute missing problem and meanwhile, it mitigates two types of unfairness, i.e., feature unfairness from attributes and topological unfairness due to attribute completion. FairAC can work on various types of homogeneous graphs and generate fair embeddings for them and thus can be applied to most d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#29983;&#29289;&#21512;&#29702;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#22312;&#26102;&#38388;&#19978;&#20855;&#26377;&#23616;&#37096;&#24615;&#65292;&#24182;&#19988;&#21363;&#20351;&#25918;&#26494;&#20102;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#30340;&#35768;&#22810;&#21160;&#21147;&#23398;&#35201;&#27714;&#65292;&#20173;&#28982;&#21487;&#20197;&#27491;&#24120;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2302.12431</link><description>&lt;p&gt;
&#29983;&#29289;&#21512;&#29702;&#23545;&#27604;&#23398;&#20064;&#30340;&#28789;&#27963;&#30456;&#20301;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Flexible Phase Dynamics for Bio-Plausible Contrastive Learning. (arXiv:2302.12431v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#29983;&#29289;&#21512;&#29702;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#22312;&#26102;&#38388;&#19978;&#20855;&#26377;&#23616;&#37096;&#24615;&#65292;&#24182;&#19988;&#21363;&#20351;&#25918;&#26494;&#20102;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#30340;&#35768;&#22810;&#21160;&#21147;&#23398;&#35201;&#27714;&#65292;&#20173;&#28982;&#21487;&#20197;&#27491;&#24120;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20316;&#20026;&#31070;&#32463;&#31185;&#23398;&#35268;&#33539;&#27169;&#22411;&#25110;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#23398;&#20064;&#30340;&#20505;&#36873;&#26041;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#23558;&#19968;&#32452;&#32593;&#32476;&#29366;&#24577;&#19982;&#21478;&#19968;&#32452;&#36827;&#34892;&#23545;&#27604;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#31639;&#27861;&#36890;&#24120;&#37319;&#29992;&#21018;&#24615;&#12289;&#26102;&#38388;&#19978;&#19981;&#36830;&#32493;&#21644;&#21608;&#26399;&#24615;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#23454;&#29616;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#21487;&#20197;&#21033;&#29992;CL&#30340;&#29289;&#29702;&#31995;&#32479;&#33539;&#22260;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25506;&#32034;&#20102;CL&#22914;&#20309;&#22312;&#29983;&#29289;&#25110;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#20013;&#23454;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#23398;&#20064;&#24418;&#24335;&#21487;&#20197;&#22312;&#26102;&#38388;&#19978;&#20855;&#26377;&#23616;&#37096;&#24615;&#65292;&#24182;&#19988;&#21363;&#20351;&#25918;&#26494;&#20102;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#30340;&#35768;&#22810;&#21160;&#21147;&#23398;&#35201;&#27714;&#65292;&#20173;&#28982;&#21487;&#20197;&#27491;&#24120;&#36816;&#34892;&#12290;&#36890;&#36807;&#19968;&#32452;&#22312;&#20960;&#20010;CL&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#30340;&#19968;&#33324;&#23450;&#29702;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#29983;&#29289;&#21644;&#31070;&#32463;&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;CL&#26041;&#27861;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many learning algorithms used as normative models in neuroscience or as candidate approaches for learning on neuromorphic chips learn by contrasting one set of network states with another. These Contrastive Learning (CL) algorithms are traditionally implemented with rigid, temporally non-local, and periodic learning dynamics that could limit the range of physical systems capable of harnessing CL. In this study, we build on recent work exploring how CL might be implemented by biological or neurmorphic systems and show that this form of learning can be made temporally local, and can still function even if many of the dynamical requirements of standard training procedures are relaxed. Thanks to a set of general theorems corroborated by numerical experiments across several CL models, our results provide theoretical foundations for the study and development of CL methods for biological and neuromorphic neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;10&#20010;&#22478;&#24066;&#30340;&#22823;&#35268;&#27169;&#28014;&#21160;&#36710;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;15&#20998;&#38047;&#20998;&#36776;&#29575;&#30340;&#20132;&#36890;&#36895;&#24230;&#20449;&#24687;&#65292;&#28085;&#30422;&#20102;&#20174;&#20027;&#24178;&#36947;&#21040;&#24403;&#22320;&#34903;&#36947;&#30340;&#25152;&#26377;&#34903;&#36947;&#32423;&#21035;&#65292;&#20026;&#22478;&#24066;&#20132;&#36890;&#36816;&#33829;&#21644;&#35268;&#21010;&#25552;&#20379;&#20102;&#37325;&#35201;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.08761</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#28014;&#21160;&#36710;&#25968;&#25454;&#30340;10&#20010;&#22478;&#24066;&#37117;&#24066;&#36335;&#27573;&#20132;&#36890;&#36895;&#24230;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities. (arXiv:2302.08761v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;10&#20010;&#22478;&#24066;&#30340;&#22823;&#35268;&#27169;&#28014;&#21160;&#36710;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;15&#20998;&#38047;&#20998;&#36776;&#29575;&#30340;&#20132;&#36890;&#36895;&#24230;&#20449;&#24687;&#65292;&#28085;&#30422;&#20102;&#20174;&#20027;&#24178;&#36947;&#21040;&#24403;&#22320;&#34903;&#36947;&#30340;&#25152;&#26377;&#34903;&#36947;&#32423;&#21035;&#65292;&#20026;&#22478;&#24066;&#20132;&#36890;&#36816;&#33829;&#21644;&#35268;&#21010;&#25552;&#20379;&#20102;&#37325;&#35201;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20998;&#26512;&#23545;&#22478;&#24066;&#36816;&#33829;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36229;&#20986;&#29615;&#36335;&#26816;&#27979;&#22120;&#33539;&#22260;&#30340;&#22478;&#24066;&#20132;&#36890;&#23494;&#38598;&#25968;&#25454;&#20173;&#28982;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#28014;&#21160;&#36710;&#36742;&#25968;&#25454;&#38598;&#65292;&#21363;&#8220;10&#20010;&#22478;&#24066;&#30340;&#37117;&#24066;&#36335;&#27573;&#20132;&#36890;&#36895;&#24230;&#25968;&#25454;&#38598;&#8221;&#65292;&#21487;&#29992;&#20110;&#20840;&#29699;10&#20010;&#22478;&#24066;&#65292;&#24182;&#20855;&#26377;&#27599;&#20010;&#37117;&#24066;&#21306;&#22495;1500&#22810;&#24179;&#26041;&#20844;&#37324;&#30340;15&#20998;&#38047;&#20998;&#36776;&#29575;&#30340;&#25910;&#38598;&#21608;&#26399;&#65292;&#25910;&#38598;&#26102;&#38388;&#20026;2019-2021&#24180;&#65292;&#35206;&#30422;&#20174;&#20027;&#24178;&#36947;&#21040;&#24403;&#22320;&#34903;&#36947;&#30340;&#25152;&#26377;&#34903;&#36947;&#32423;&#21035;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;&#35813;&#25968;&#25454;&#38598;&#21033;&#29992;&#24037;&#19994;&#35268;&#27169;&#30340;&#28014;&#21160;&#36710;&#36742;Traffic4cast&#25968;&#25454;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#30340;&#26102;&#31354;&#32858;&#21512;&#25552;&#20379;&#20102;&#36895;&#24230;&#21644;&#36710;&#36742;&#35745;&#25968;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#39640;&#25928;&#30340;&#21305;&#37197;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;OpenStreetMap&#36335;&#32593;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic analysis is crucial for urban operations and planning, while the availability of dense urban traffic data beyond loop detectors is still scarce. We present a large-scale floating vehicle dataset of per-street segment traffic information, Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities (MeTS-10), available for 10 global cities with a 15-minute resolution for collection periods ranging between 108 and 361 days in 2019-2021 and covering more than 1500 square kilometers per metropolitan area. MeTS-10 features traffic speed information at all street levels from main arterials to local streets for Antwerp, Bangkok, Barcelona, Berlin, Chicago, Istanbul, London, Madrid, Melbourne and Moscow. The dataset leverages the industrial-scale floating vehicle Traffic4cast data with speeds and vehicle counts provided in a privacy-preserving spatio-temporal aggregation. We detail the efficient matching approach mapping the data to the OpenStreetMap road graph. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#38656;&#36890;&#20449;&#21327;&#35758;ODC&#65292;&#21487;&#20197;&#26681;&#25454;&#26234;&#33021;&#20307;&#30340;&#32463;&#39564;&#25289;&#21160;&#26102;&#38388;&#35843;&#25972;&#27599;&#23545;&#26234;&#33021;&#20307;&#38388;&#30340;&#36890;&#20449;&#65292;&#21516;&#26102;&#23558;ODC&#38598;&#25104;&#21040;UCB&#21644;AAE&#31639;&#27861;&#30340;&#33258;&#28982;&#25193;&#23637;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#21327;&#20316;&#31639;&#27861;&#65292;&#20998;&#26512;&#34920;&#26126;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#36951;&#25022;&#26041;&#38754;&#37117;&#25509;&#36817;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2302.07446</link><description>&lt;p&gt;
&#24322;&#27493;&#22810;&#26234;&#33021;&#20307;&#36172;&#21338;&#26426;&#30340;&#25353;&#38656;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
On-Demand Communication for Asynchronous Multi-Agent Bandits. (arXiv:2302.07446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#38656;&#36890;&#20449;&#21327;&#35758;ODC&#65292;&#21487;&#20197;&#26681;&#25454;&#26234;&#33021;&#20307;&#30340;&#32463;&#39564;&#25289;&#21160;&#26102;&#38388;&#35843;&#25972;&#27599;&#23545;&#26234;&#33021;&#20307;&#38388;&#30340;&#36890;&#20449;&#65292;&#21516;&#26102;&#23558;ODC&#38598;&#25104;&#21040;UCB&#21644;AAE&#31639;&#27861;&#30340;&#33258;&#28982;&#25193;&#23637;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#21327;&#20316;&#31639;&#27861;&#65292;&#20998;&#26512;&#34920;&#26126;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#36951;&#25022;&#26041;&#38754;&#37117;&#25509;&#36817;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#30340;&#25805;&#20316;&#26159;&#24322;&#27493;&#30340; - &#26234;&#33021;&#20307;&#30340;&#25289;&#21160;&#26102;&#38388;&#21644;&#36895;&#29575;&#26159;&#26410;&#30693;&#30340;&#12289;&#19981;&#35268;&#21017;&#30340;&#21644;&#24322;&#26500;&#30340; - &#24182;&#19988;&#38754;&#23545;&#30456;&#21516;&#30340;K&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#26234;&#33021;&#20307;&#21487;&#20197;&#20849;&#20139;&#22870;&#21169;&#20449;&#24687;&#20197;&#21152;&#24555;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#38656;&#36890;&#20449;&#21327;&#35758;ODC&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#30340;&#32463;&#39564;&#25289;&#21160;&#26102;&#38388;&#35843;&#25972;&#27599;&#23545;&#26234;&#33021;&#20307;&#38388;&#30340;&#36890;&#20449;&#12290;&#24403;&#26234;&#33021;&#20307;&#30340;&#25289;&#21160;&#26102;&#38388;&#39640;&#24230;&#19981;&#22343;&#21248;&#26102;&#65292;ODC&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#20854;&#36890;&#20449;&#22797;&#26434;&#24615;&#21462;&#20915;&#20110;&#26234;&#33021;&#20307;&#30340;&#32463;&#39564;&#25289;&#21160;&#26102;&#38388;&#12290;ODC&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#21327;&#35758;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#22823;&#22810;&#25968;&#21327;&#20316;&#36172;&#21338;&#31639;&#27861;&#20013;&#32780;&#19981;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;ODC&#38598;&#25104;&#21040;UCB&#21644;AAE&#31639;&#27861;&#30340;&#33258;&#28982;&#25193;&#23637;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#21327;&#20316;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#36951;&#25022;&#26041;&#38754;&#37117;&#25509;&#36817;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a cooperative multi-agent multi-armed stochastic bandit problem where agents operate asynchronously -- agent pull times and rates are unknown, irregular, and heterogeneous -- and face the same instance of a K-armed bandit problem. Agents can share reward information to speed up the learning process at additional communication costs. We propose ODC, an on-demand communication protocol that tailors the communication of each pair of agents based on their empirical pull times. ODC is efficient when the pull times of agents are highly heterogeneous, and its communication complexity depends on the empirical pull times of agents. ODC is a generic protocol that can be integrated into most cooperative bandit algorithms without degrading their performance. We then incorporate ODC into the natural extensions of UCB and AAE algorithms and propose two communication-efficient cooperative algorithms. Our analysis shows that both algorithms are near-optimal in regret.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#31995;&#32479;&#30340;&#31995;&#32479;&#36776;&#35782;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27604;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#19982;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#35760;&#24405;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#36776;&#35782;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#21050;&#28608;&#22270;&#20687;&#31561;&#22240;&#32032;&#65292;&#24182;&#19988;&#23545;&#35782;&#21035;&#26356;&#39640;&#32423;&#21035;&#26550;&#26500;&#22270;&#26696;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06677</link><description>&lt;p&gt;
&#31070;&#32463;&#31995;&#32479;&#30340;&#31995;&#32479;&#36776;&#35782;&#65306;&#22914;&#26524;&#25105;&#20204;&#29702;&#35299;&#27491;&#30830;&#65292;&#25105;&#20204;&#20250;&#30693;&#36947;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
System identification of neural systems: If we got it right, would we know?. (arXiv:2302.06677v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#31995;&#32479;&#30340;&#31995;&#32479;&#36776;&#35782;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27604;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#19982;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#35760;&#24405;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#36776;&#35782;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#21050;&#28608;&#22270;&#20687;&#31561;&#22240;&#32032;&#65292;&#24182;&#19988;&#23545;&#35782;&#21035;&#26356;&#39640;&#32423;&#21035;&#26550;&#26500;&#22270;&#26696;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#34987;&#25552;&#35758;&#20316;&#20026;&#22823;&#33041;&#30340;&#37096;&#20998;&#27169;&#22411;&#12290;&#23558;&#36825;&#20123;&#32593;&#32476;&#19982;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#35760;&#24405;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35748;&#20026;&#22312;&#37325;&#29616;&#31070;&#32463;&#21453;&#24212;&#26041;&#38754;&#30340;&#33391;&#22909;&#24615;&#33021;&#25903;&#25345;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#36825;&#31181;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#23545;&#25105;&#20204;&#20102;&#35299;&#33041;&#37096;&#35745;&#31639;&#26377;&#22810;&#22823;&#24110;&#21161;&#12290;&#23427;&#26159;&#21542;&#33021;&#39564;&#35777;&#26576;&#31181;&#27169;&#22411;&#26550;&#26500;&#20248;&#20110;&#21478;&#19968;&#31181;&#65311;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#24120;&#29992;&#30340;&#27604;&#36739;&#25216;&#26415;&#65292;&#22914;&#32447;&#24615;&#32534;&#30721;&#27169;&#22411;&#21644;&#20013;&#24515;&#26680;&#23545;&#40784;&#65292;&#36890;&#36807;&#29992;&#24050;&#30693;&#30340;&#30495;&#23454;&#27169;&#22411;&#26367;&#25442;&#33041;&#37096;&#35760;&#24405;&#26469;&#27491;&#30830;&#35782;&#21035;&#27169;&#22411;&#12290;&#31995;&#32479;&#36776;&#35782;&#30340;&#24615;&#33021;&#30456;&#24403;&#19981;&#31283;&#23450;&#65292;&#23427;&#36824;&#26174;&#33879;&#20381;&#36182;&#20110;&#29420;&#31435;&#20110;&#30495;&#23454;&#27169;&#22411;&#26550;&#26500;&#30340;&#22240;&#32032;&#65292;&#22914;&#21050;&#28608;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#21151;&#33021;&#30456;&#20284;&#24615;&#35780;&#20998;&#22312;&#35782;&#21035;&#26356;&#39640;&#32423;&#21035;&#26550;&#26500;&#22270;&#26696;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks are being proposed as models of parts of the brain. The networks are compared to recordings of biological neurons, and good performance in reproducing neural responses is considered to support the model's validity. A key question is how much this system identification approach tells us about brain computation. Does it validate one model architecture over another? We evaluate the most commonly used comparison techniques, such as a linear encoding model and centered kernel alignment, to correctly identify a model by replacing brain recordings with known ground truth models. System identification performance is quite variable; it also depends significantly on factors independent of the ground truth architecture, such as stimuli images. In addition, we show the limitations of using functional similarity scores in identifying higher-level architectural motifs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#21521;&#22270;&#30340;&#26041;&#21521;&#21644;&#32467;&#26500;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#25490;&#24207;&#32593;&#32476;&#30340;&#27491;&#30830;&#24615;&#27979;&#35797;&#21644;&#28304;&#20195;&#30721;&#29702;&#35299;&#31561;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#25552;&#21319;&#20102;14.7%&#12290;</title><link>http://arxiv.org/abs/2302.00049</link><description>&lt;p&gt;
Transformers&#36935;&#35265;&#26377;&#21521;&#22270;
&lt;/p&gt;
&lt;p&gt;
Transformers Meet Directed Graphs. (arXiv:2302.00049v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#21521;&#22270;&#30340;&#26041;&#21521;&#21644;&#32467;&#26500;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#25490;&#24207;&#32593;&#32476;&#30340;&#27491;&#30830;&#24615;&#27979;&#35797;&#21644;&#28304;&#20195;&#30721;&#29702;&#35299;&#31561;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#25552;&#21319;&#20102;14.7%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#26368;&#21021;&#34987;&#25552;&#20986;&#20316;&#20026;&#25991;&#26412;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20294;&#29616;&#22312;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21253;&#25324;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#26080;&#21521;&#22270;&#31561;&#22810;&#31181;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#26377;&#21521;&#22270;&#30340;transformers&#21364;&#26159;&#19968;&#20010;&#24847;&#22806;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#20027;&#39064;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21253;&#25324;&#28304;&#20195;&#30721;&#21644;&#36923;&#36753;&#30005;&#36335;&#22312;&#20869;&#30340;&#26222;&#36941;&#39046;&#22495;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#21521;&#22270;&#30340;&#26041;&#21521;&#21644;&#32467;&#26500;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#65306;&#65288;1&#65289;&#30913;&#22330;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#21521;&#37327; - &#26159;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#26041;&#21521;&#24863;&#30693;&#25512;&#24191;&#65307;&#65288;2&#65289;&#26041;&#21521;&#38543;&#26426;&#28216;&#36208;&#32534;&#30721;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38468;&#21152;&#30340;&#26041;&#21521;&#20449;&#24687;&#22312;&#21253;&#25324;&#25490;&#24207;&#32593;&#32476;&#30340;&#27491;&#30830;&#24615;&#27979;&#35797;&#21644;&#28304;&#20195;&#30721;&#29702;&#35299;&#31561;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#21512;&#25968;&#25454;&#27969;&#20026;&#20013;&#24515;&#30340;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Open Graph Benchmark Code2&#19978;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#25552;&#21319;&#20102;14.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and undirected graphs. However, transformers for directed graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains, including source code and logic circuits. In this work, we propose two direction- and structure-aware positional encodings for directed graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding. Together with a data-flow-centric graph construction, our model outperforms the prior state of the art on the Open Graph Benchmark Code2 relatively by 14.7%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2301.00752</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#36890;&#20449;&#30340;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27573;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65292;&#20197;&#32531;&#35299;&#34892;&#20154;&#38459;&#25377;&#22240;&#32032;&#23545;mmWave&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#25668;&#20687;&#22836;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;mmWave&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#12290;&#28857;&#20113;&#23558;&#19977;&#32500;&#31354;&#38388;&#34920;&#31034;&#20026;&#28857;&#38598;&#65292;&#20854;&#31354;&#38388;&#24615;&#36136;&#26356;&#21152;&#31232;&#30095;&#65292;&#19981;&#22826;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20102;3D&#20301;&#32622;&#21644;&#36816;&#21160;&#20449;&#24687;&#65292;&#36825;&#23545;&#20102;&#35299;&#28041;&#21450;&#34892;&#20154;&#30340;&#26080;&#32447;&#30005;&#20256;&#25773;&#29615;&#22659;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;JKO&#26041;&#26696;&#30340;&#21487;&#36870;&#24402;&#19968;&#21270;&#27969;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25353;&#22359;&#36827;&#34892;&#27531;&#24046;&#22359;&#30340;&#35757;&#32451;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#36127;&#36733;&#21644;&#28145;&#24230;&#27969;&#32593;&#32476;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27969;&#32593;&#32476;&#65292;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#36880;&#27493;&#32454;&#21270;&#36712;&#36857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14424</link><description>&lt;p&gt;
&#22522;&#20110;JKO&#26041;&#26696;&#30340;&#21487;&#36870;&#24402;&#19968;&#21270;&#27969;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Invertible normalizing flow neural networks by JKO scheme. (arXiv:2212.14424v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;JKO&#26041;&#26696;&#30340;&#21487;&#36870;&#24402;&#19968;&#21270;&#27969;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25353;&#22359;&#36827;&#34892;&#27531;&#24046;&#22359;&#30340;&#35757;&#32451;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#36127;&#36733;&#21644;&#28145;&#24230;&#27969;&#32593;&#32476;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27969;&#32593;&#32476;&#65292;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#36880;&#27493;&#32454;&#21270;&#36712;&#36857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#19968;&#31867;&#29992;&#20110;&#39640;&#25928;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#38469;&#20013;&#65292;&#27969;&#36890;&#24120;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#21487;&#36870;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#38142;; &#20026;&#20102;&#20415;&#20110;&#35757;&#32451;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#23545;&#27969;&#36712;&#36857;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#29305;&#27530;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;Jordan-Kinderleherer-Otto (JKO)&#26041;&#26696;&#21551;&#21457;&#30340;&#31070;&#32463;ODE&#27969;&#32593;&#32476;&#65292;&#23427;&#20801;&#35768;&#26377;&#25928;&#22320;&#25353;&#22359;&#36827;&#34892;&#27531;&#24046;&#22359;&#30340;&#35757;&#32451;&#65292;&#26080;&#38656;&#37319;&#26679;SDE&#36712;&#36857;&#25110;&#20998;&#25968;&#21305;&#37197;&#25110;&#21464;&#20998;&#23398;&#20064;&#30340;&#20869;&#24490;&#29615;&#12290;&#30001;&#20110;JKO&#26041;&#26696;&#23637;&#24320;&#20102;&#26799;&#24230;&#27969;&#30340;&#21160;&#24577;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33258;&#28982;&#22320;&#36880;&#20010;&#22534;&#21472;&#27531;&#24046;&#32593;&#32476;&#22359;&#65292;&#38477;&#20302;&#20102;&#20869;&#23384;&#36127;&#36733;&#21644;&#36827;&#34892;&#31471;&#21040;&#31471;&#28145;&#24230;&#27969;&#32593;&#32476;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27969;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#36880;&#27493;&#32454;&#21270;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flow is a class of deep generative models for efficient sampling and density estimation. In practice, the flow often appears as a chain of invertible neural network blocks; to facilitate training, existing works have regularized flow trajectories and designed special network architectures. The current paper develops a neural ODE flow network inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wise training of the residual blocks without sampling SDE trajectories or inner loops of score matching or variational learning. As the JKO scheme unfolds the dynamic of gradient flow, the proposed model naturally stacks residual network blocks one by one, reducing the memory load and difficulty in performing end-to-end deep flow network training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the trajectory in probability space, which improves the model training efficiency and accuracy in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;StyleGAN&#29983;&#25104;&#21435;&#35782;&#21035;&#20154;&#33080;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39118;&#26684;&#28151;&#21512;&#65292;StyleGAN&#33021;&#22815;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#20445;&#25345;&#22270;&#20687;&#30340;&#23454;&#29992;&#24615;&#65292;&#19982;&#20854;&#20182;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2212.02611</link><description>&lt;p&gt;
StyleGAN&#20316;&#20026;&#19968;&#31181;&#20445;&#25345;&#23454;&#29992;&#24615;&#30340;&#20154;&#33080;&#21435;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StyleGAN as a Utility-Preserving Face De-identification Method. (arXiv:2212.02611v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;StyleGAN&#29983;&#25104;&#21435;&#35782;&#21035;&#20154;&#33080;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39118;&#26684;&#28151;&#21512;&#65292;StyleGAN&#33021;&#22815;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#20445;&#25345;&#22270;&#20687;&#30340;&#23454;&#29992;&#24615;&#65292;&#19982;&#20854;&#20182;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#21435;&#35782;&#21035;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#36890;&#36807;&#27169;&#31946;&#20154;&#33080;&#26469;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#38477;&#20302;&#29031;&#29255;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#33021;&#20445;&#25345;&#20154;&#33080;&#30340;&#23454;&#29992;&#24615;&#65292;&#21363;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#23039;&#21183;&#21644;&#34920;&#24773;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleGAN&#30340;GAN&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#39640;&#36136;&#37327;&#30340;&#34394;&#25311;&#20154;&#33080;&#12290;&#26412;&#25991;&#36890;&#36807;&#39118;&#26684;&#28151;&#21512;&#26469;&#30740;&#31350;StyleGAN&#22312;&#29983;&#25104;&#21435;&#35782;&#21035;&#20154;&#33080;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#26045;&#22810;&#31181;&#20154;&#33080;&#26816;&#27979;&#12289;&#39564;&#35777;&#21644;&#35782;&#21035;&#25915;&#20987;&#65292;&#24182;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#26469;&#35780;&#20272;&#36825;&#31181;&#21435;&#35782;&#21035;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#12289;&#20154;&#24037;&#35780;&#20272;&#21644;&#19982;CIAGAN&#21644;DeepPrivacy&#20004;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#65292;StyleGAN&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#22270;&#29255;&#23454;&#29992;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#29978;&#33267;&#26356;&#22909;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Face de-identification methods have been proposed to preserve users' privacy by obscuring their faces. These methods, however, can degrade the quality of photos, and they usually do not preserve the utility of faces, i.e., their age, gender, pose, and facial expression. Recently, GANs, such as StyleGAN, have been proposed, which generate realistic, high-quality imaginary faces. In this paper, we investigate the use of StyleGAN in generating de-identified faces through style mixing. We examined this de-identification method for preserving utility and privacy by implementing several face detection, verification, and identification attacks and conducting a user study. The results from our extensive experiments, human evaluation, and comparison with two state-of-the-art methods, i.e., CIAGAN and DeepPrivacy, show that StyleGAN performs on par or better than these methods, preserving users' privacy and images' utility. In particular, the results of the machine learning-based experiments sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65288;IFU&#65289;&#30340;&#26032;&#39062;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;&#26377;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;&#35831;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#29575;&#36739;&#22522;&#26412;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2211.11656</link><description>&lt;p&gt;
&#39034;&#24207;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65306;&#32852;&#37030;&#20248;&#21270;&#20013;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65288;IFU&#65289;&#30340;&#26032;&#39062;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;&#26377;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;&#35831;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#29575;&#36739;&#22522;&#26412;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#65288;MU&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;&#20174;&#35757;&#32451;&#36807;&#31243;&#20013;&#21024;&#38500;&#32473;&#23450;&#25968;&#25454;&#28857;&#30340;&#36129;&#29486;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#32852;&#37030;&#28040;&#38500;&#65288;FU&#65289;&#26159;&#23558;MU&#25193;&#23637;&#21040;&#20174;&#32852;&#21512;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#32473;&#23450;&#23458;&#25143;&#31471;&#30340;&#36129;&#29486;&#12290;&#24403;&#21069;&#30340;FU&#26041;&#27861;&#36890;&#24120;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#27809;&#26377;&#23545;&#28040;&#38500;&#25928;&#26524;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#21512;&#29702;&#30340;&#29702;&#35770;&#37327;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;(IFU)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;FU&#26041;&#27861;&#12290;&#22312;&#25509;&#25910;&#21040;&#32473;&#23450;&#23458;&#25143;&#31471;&#30340;&#28040;&#38500;&#35831;&#27714;&#21518;&#65292;IFU&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#26426;&#21046;&#30830;&#23450;&#20102;&#37325;&#26032;&#21021;&#22987;&#21270;FL&#25152;&#38656;&#30340;&#26368;&#20339;FL&#36845;&#20195;&#65292;&#21487;&#20197;&#33719;&#24471;&#28040;&#38500;&#20445;&#35777;&#12290;IFU&#30340;&#29702;&#35770;&#20063;&#21487;&#20197;&#25193;&#23637;&#20197;&#35299;&#20915;&#39034;&#24207;&#28040;&#38500;&#35831;&#27714;&#12290;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#26412;&#37325;&#26032;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#30456;&#27604;&#65292;IFU&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#28040;&#38500;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#31639;&#27861; FedAPT&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#25552;&#20379;&#20010;&#24615;&#21270;&#25552;&#31034;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37322;&#25918;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#27169;&#22359;&#65292;&#26381;&#21153;&#22120;&#29983;&#25104;&#20851;&#38190;&#20449;&#24687;&#24182;&#20998;&#37197;&#32473;&#23458;&#25143;&#31471;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#30340;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#20803;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.07864</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning. (arXiv:2211.07864v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#31639;&#27861; FedAPT&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#25552;&#20379;&#20010;&#24615;&#21270;&#25552;&#31034;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37322;&#25918;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#27169;&#22359;&#65292;&#26381;&#21153;&#22120;&#29983;&#25104;&#20851;&#38190;&#20449;&#24687;&#24182;&#20998;&#37197;&#32473;&#23458;&#25143;&#31471;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#30340;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#20803;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#23436;&#25972;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#22270;&#20687;&#20998;&#31867;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#31639;&#27861; FedAPT&#65292;&#21033;&#29992;&#31867;&#20284; CLIP &#30340;&#24378;&#22823;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#30452;&#25509;&#32852;&#37030;&#25552;&#31034;&#35843;&#20248;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#33258;&#36866;&#24212;&#22320;&#37322;&#25918;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#20026;&#20854;&#25552;&#20379;&#20010;&#24615;&#21270;&#25552;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#27169;&#22359;&#65292;&#23427;&#21253;&#25324;&#20803;&#25552;&#31034;&#65292;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#19968;&#20123;&#20851;&#38190;&#20449;&#24687;&#12290;&#26381;&#21153;&#22120;&#38543;&#26426;&#29983;&#25104;&#19968;&#32452;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25152;&#26377;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#20803;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple clients to collaboratively train a global model without disclosing their data. Previous researches often require training the complete model parameters. However, the emergence of powerful pre-trained models makes it possible to achieve higher performance with fewer learnable parameters in FL. In this paper, we propose a federated adaptive prompt tuning algorithm, FedAPT, for multi-domain collaborative image classification with powerful foundation models, like CLIP. Compared with direct federated prompt tuning, our core idea is to adaptively unlock specific domain knowledge for each test sample in order to provide them with personalized prompts. To implement this idea, we design an adaptive prompt tuning module, which consists of a meta prompt, an adaptive network, and some keys. The server randomly generates a set of keys and assigns a unique key to each client. Then all clients cooperatively train the global adaptive network and meta prompt wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2211.00646</link><description>&lt;p&gt;
&#20174;&#30456;&#37051;&#30340;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#23398;&#20064;&#40657;&#33394;&#32032;&#32454;&#32990;&#25513;&#33180;
&lt;/p&gt;
&lt;p&gt;
Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#33394;&#32032;&#30244;&#26159;&#26368;&#20855;&#20405;&#34989;&#24615;&#30340;&#30382;&#32932;&#30284;&#20043;&#19968;&#65292;&#23548;&#33268;&#22823;&#37096;&#20998;&#30382;&#32932;&#30284;&#27515;&#20129;&#12290;&#28982;&#32780;&#65292;&#30149;&#29702;&#23398;&#23478;&#23545;&#40657;&#33394;&#32032;&#30244;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#36739;&#20302;&#12290;&#30001;&#20110;&#40657;&#33394;&#32032;&#30244;&#26159;&#40657;&#33394;&#32032;&#32454;&#32990;&#30340;&#32959;&#30244;&#65292;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#19982;&#30149;&#29702;&#23398;&#23478;&#30340;&#24046;&#24322;&#26080;&#20851;&#24182;&#33021;&#33258;&#21160;&#36827;&#34892;&#20687;&#32032;&#32423;&#27880;&#37322;&#30340;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#30149;&#29702;&#23398;&#23478;&#26631;&#27880;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#37051;&#36817;&#32452;&#32455;&#20999;&#29255;&#19978;&#30340;&#20598;&#32852;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#26579;&#33394;&#29255;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#65292;&#34429;&#28982;&#24456;&#38590;&#26377;&#23436;&#32654;&#30340;&#26631;&#31614;&#65292;&#20294;&#36798;&#21040;&#20102;0.64&#30340;&#24179;&#22343;IOU&#12290;
&lt;/p&gt;
&lt;p&gt;
Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&amp;E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21407;&#21017;&#24615;&#21098;&#26525;&#12290;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#21098;&#26525;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#30340;&#36817;&#20284;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.09134</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21407;&#21017;&#24615;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization. (arXiv:2210.09134v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21407;&#21017;&#24615;&#21098;&#26525;&#12290;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#21098;&#26525;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#30340;&#36817;&#20284;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#30340;&#25152;&#26377;&#23884;&#22871;&#23376;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35780;&#20272;&#36825;&#20123;&#23376;&#27169;&#22411;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#20027;&#35201;&#24212;&#29992;&#20110;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#31038;&#21306;&#30340;&#31616;&#21333;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#24212;&#29992;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21407;&#21017;&#24615;&#21098;&#26525;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#20250;&#20135;&#29983;&#36817;&#20284;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#21098;&#26525;&#31639;&#27861;&#65292;&#20197;&#32531;&#35299;&#30452;&#25509;&#24212;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#25152;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;UCI&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#23545;&#19981;&#21516;&#25512;&#29702;&#31639;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#21098;&#26525;&#26041;&#26696;&#35299;&#20915;&#20102;&#20449;&#21495;&#22788;&#29702;&#31038;&#21306;&#20351;&#29992;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#30830;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;m.
&lt;/p&gt;
&lt;p&gt;
Bayesian model reduction provides an efficient approach for comparing the performance of all nested sub-models of a model, without re-evaluating any of these sub-models. Until now, Bayesian model reduction has been applied mainly in the computational neuroscience community on simple models. In this paper, we formulate and apply Bayesian model reduction to perform principled pruning of Bayesian neural networks, based on variational free energy minimization. Direct application of Bayesian model reduction, however, gives rise to approximation errors. Therefore, a novel iterative pruning algorithm is presented to alleviate the problems arising with naive Bayesian model reduction, as supported experimentally on the publicly available UCI datasets for different inference algorithms. This novel parameter pruning scheme solves the shortcomings of current state-of-the-art pruning methods that are used by the signal processing community. The proposed approach has a clear stopping criterion and m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#32435;&#20837;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#21453;&#21521;&#24037;&#31243;&#21644;&#35745;&#31639;&#26426;&#23433;&#20840;&#20219;&#21153;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2210.05102</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Pre-Training Representations of Binary Code Using Contrastive Learning. (arXiv:2210.05102v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#32435;&#20837;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#21453;&#21521;&#24037;&#31243;&#21644;&#35745;&#31639;&#26426;&#23433;&#20840;&#20219;&#21153;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#35793;&#21518;&#30340;&#36719;&#20214;&#20197;&#21487;&#25191;&#34892;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#24418;&#24335;&#20132;&#20184;&#12290;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#28304;&#20195;&#30721;&#26469;&#34920;&#36798;&#36719;&#20214;&#30340;&#35821;&#20041;&#65292;&#20294;&#32534;&#35793;&#22120;&#23558;&#20854;&#36716;&#25442;&#20026;CPU&#21487;&#20197;&#30452;&#25509;&#25191;&#34892;&#30340;&#20108;&#36827;&#21046;&#26684;&#24335;&#12290;&#22240;&#27492;&#65292;&#20108;&#36827;&#21046;&#20195;&#30721;&#20998;&#26512;&#23545;&#20110;&#21453;&#21521;&#24037;&#31243;&#21644;&#35745;&#31639;&#26426;&#23433;&#20840;&#20219;&#21153;&#31561;&#27809;&#26377;&#28304;&#20195;&#30721;&#30340;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#21253;&#21547;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#30340;&#28304;&#20195;&#30721;&#21644;&#33258;&#28982;&#35821;&#35328;&#19981;&#21516;&#65292;&#20108;&#36827;&#21046;&#20195;&#30721;&#36890;&#24120;&#38590;&#20197;&#29702;&#35299;&#21644;&#20998;&#26512;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#20351;&#29992;AI&#27169;&#22411;&#36741;&#21161;&#28304;&#20195;&#30721;&#20998;&#26512;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#32771;&#34385;&#20108;&#36827;&#21046;&#20195;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#32435;&#20837;&#20108;&#36827;&#21046;&#20195;&#30721;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;COMBO&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;COMBO&#20013;&#25552;&#20986;&#20102;&#19977;&#20010;&#32452;&#20214;&#65306;&#65288;1&#65289;&#29992;&#20110;&#20919;&#21551;&#21160;&#39044;&#35757;&#32451;&#30340;&#20027;&#35201;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#65288;2&#65289;&#29992;&#20110;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#25554;&#20837;&#21040;&#20108;&#36827;&#21046;&#20195;&#30721;&#20013;&#30340;&#21333;&#32431;&#25554;&#20540;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compiled software is delivered as executable binary code. Developers write source code to express the software semantics, but the compiler converts it to a binary format that the CPU can directly execute. Therefore, binary code analysis is critical to applications in reverse engineering and computer security tasks where source code is not available. However, unlike source code and natural language that contain rich semantic information, binary code is typically difficult for human engineers to understand and analyze. While existing work uses AI models to assist source code analysis, few studies have considered binary code. In this paper, we propose a COntrastive learning Model for Binary cOde Analysis, or COMBO, that incorporates source code and comment information into binary code during representation learning. Specifically, we present three components in COMBO: (1) a primary contrastive learning method for cold-start pre-training, (2) a simplex interpolation method to incorporate so
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36125;&#21494;&#26031;HMAML&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#65292;&#20197;&#35299;&#20915;MAML&#30340;&#36807;&#25311;&#21512;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.02796</link><description>&lt;p&gt;
Bayesian MAML&#30340;&#36229;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hypernetwork approach to Bayesian MAML. (arXiv:2210.02796v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02796
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36125;&#21494;&#26031;HMAML&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#65292;&#20197;&#35299;&#20915;MAML&#30340;&#36807;&#25311;&#21512;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#33021;&#22815;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#27969;&#34892;&#19988;&#20248;&#38597;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23398;&#20064;&#20803;&#27169;&#22411;&#30340;&#20849;&#20139;&#36890;&#29992;&#26435;&#37325;&#65292;&#28982;&#21518;&#23558;&#20854;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#37327;&#26377;&#38480;&#65292;&#24456;&#38590;&#20934;&#30830;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26435;&#37325;&#20998;&#24067;&#32780;&#19981;&#26159;&#28857;&#20540;&#26435;&#37325;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20808;&#21069;&#20462;&#25913;&#30340;MAML&#26041;&#27861;&#30001;&#20110;&#39640;&#26031;&#20998;&#24067;&#30340;&#31616;&#21333;&#24615;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#31867;MAML&#26435;&#37325;&#26356;&#26032;&#25110;&#24378;&#21046;&#25191;&#34892;&#30456;&#21516;&#32467;&#26500;&#30340;&#36890;&#29992;&#26435;&#37325;&#21644;&#36866;&#24212;&#26435;&#37325;&#65292;&#21463;&#21040;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;MAML&#26694;&#26550;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;HMAML&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main goal of Few-Shot learning algorithms is to enable learning from small amounts of data. One of the most popular and elegant Few-Shot learning approaches is Model-Agnostic Meta-Learning (MAML). The main idea behind this method is to learn the shared universal weights of a meta-model, which are then adapted for specific tasks. However, the method suffers from over-fitting and poorly quantifies uncertainty due to limited data size. Bayesian approaches could, in principle, alleviate these shortcomings by learning weight distributions in place of point-wise weights. Unfortunately, previous modifications of MAML are limited due to the simplicity of Gaussian posteriors, MAML-like gradient-based weight updates, or by the same structure enforced for universal and adapted weights.  In this paper, we propose a novel framework for Bayesian MAML called BayesianHMAML, which employs Hypernetworks for weight updates. It learns the universal weights point-wise, but a probabilistic structure is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#24182;&#23454;&#29616;&#19968;&#20123;&#29305;&#23450;&#30340;&#23646;&#24615;&#12290;&#20027;&#35201;&#20851;&#27880;&#21253;&#21547;&#38750;1-Lipschitz&#23618;&#30340;1-Lipschitz&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2210.02373</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#21147;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamical systems' based neural networks. (arXiv:2210.02373v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#24182;&#23454;&#29616;&#19968;&#20123;&#29305;&#23450;&#30340;&#23646;&#24615;&#12290;&#20027;&#35201;&#20851;&#27880;&#21253;&#21547;&#38750;1-Lipschitz&#23618;&#30340;1-Lipschitz&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25968;&#23398;&#23646;&#24615;&#36890;&#24120;&#19981;&#22826;&#28165;&#26970;&#12290;&#22914;&#26524;&#25968;&#25454;&#25110;&#36817;&#20284;&#20989;&#25968;&#20013;&#23384;&#22312;&#26576;&#31181;&#28508;&#22312;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#37027;&#20040;&#22312;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26102;&#23601;&#24448;&#24448;&#24076;&#26395;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#38750;&#33258;&#27835;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#24320;&#22987;&#65292;&#20351;&#29992;&#36866;&#24403;&#30340;&#12289;&#20445;&#25345;&#32467;&#26500;&#30340;&#25968;&#20540;&#26102;&#38388;&#31163;&#25955;&#21270;&#26041;&#27861;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#24120;&#24494;&#20998;&#26041;&#31243;&#21521;&#37327;&#22330;&#30340;&#24615;&#36136;&#25512;&#23548;&#32780;&#26469;&#12290;&#38500;&#20102;&#22312;&#32593;&#32476;&#26550;&#26500;&#20013;&#27880;&#20837;&#26356;&#22810;&#32467;&#26500;&#20043;&#22806;&#65292;&#36825;&#31181;&#24314;&#27169;&#26041;&#27861;&#36824;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26222;&#36866;&#36924;&#36817;&#30340;&#32467;&#26524;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#22312;&#31070;&#32463;&#32593;&#32476;&#19978;&#23454;&#26045;&#19968;&#20123;&#29305;&#23450;&#30340;&#23646;&#24615;&#12290;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#21253;&#21547;&#38750;1-Lipschitz&#23618;&#30340;1-Lipschitz&#26550;&#26500;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#34920;&#36798;&#26041;&#24335;&#26356;+
&lt;/p&gt;
&lt;p&gt;
Neural networks have gained much interest because of their effectiveness in many applications. However, their mathematical properties are generally not well understood. If there is some underlying geometric structure inherent to the data or to the function to approximate, it is often desirable to take this into account in the design of the neural network. In this work, we start with a non-autonomous ODE and build neural networks using a suitable, structure-preserving, numerical time-discretisation. The structure of the neural network is then inferred from the properties of the ODE vector field. Besides injecting more structure into the network architectures, this modelling procedure allows a better theoretical understanding of their behaviour. We present two universal approximation results and demonstrate how to impose some particular properties on the neural networks. A particular focus is on 1-Lipschitz architectures including layers that are not 1-Lipschitz. These networks are expre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#22238;&#24402;&#20219;&#21153;&#20013;&#20351;&#29992;&#20108;&#36827;&#21046;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#20540;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29305;&#24449;&#12289;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#26435;&#37325;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#26469;&#26500;&#24314;&#32039;&#20945;&#30340;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.03450</link><description>&lt;p&gt;
&#22312;&#20108;&#36827;&#21046;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#20013;&#23547;&#27714;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Seeking Interpretability and Explainability in Binary Activated Neural Networks. (arXiv:2209.03450v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#22238;&#24402;&#20219;&#21153;&#20013;&#20351;&#29992;&#20108;&#36827;&#21046;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#20540;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29305;&#24449;&#12289;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#26435;&#37325;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#26469;&#26500;&#24314;&#32039;&#20945;&#30340;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#23558;&#20108;&#36827;&#21046;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#22120;&#30340;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#35745;&#31639;SHAP&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#29305;&#24449;&#12289;&#38544;&#34255;&#31070;&#32463;&#20803;&#29978;&#33267;&#26435;&#37325;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#22312;&#23454;&#29616;&#35299;&#37322;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#32039;&#20945;&#30340;&#20108;&#36827;&#21046;&#28608;&#27963;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#35774;&#23450;&#32593;&#32476;&#30340;&#26550;&#26500;&#65306;&#23427;&#36880;&#23618;&#12289;&#36880;&#20010;&#31070;&#32463;&#20803;&#22320;&#26500;&#24314;&#65292;&#20351;&#24471;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#65292;&#39044;&#27979;&#22120;&#19981;&#20250;&#36807;&#20110;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the use of binary activated neural networks as interpretable and explainable predictors in the context of regression tasks on tabular data; more specifically, we provide guarantees on their expressiveness, present an approach based on the efficient computation of SHAP values for quantifying the relative importance of the features, hidden neurons and even weights. As the model's simplicity is instrumental in achieving interpretability, we propose a greedy algorithm for building compact binary activated networks. This approach doesn't need to fix an architecture for the network in advance: it is built one layer at a time, one neuron at a time, leading to predictors that aren't needlessly complex for a given task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;GRASP&#65292;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#20108;&#20998;&#31867;&#22120;&#23545;&#32473;&#23450;&#29305;&#24449;&#21521;&#37327;&#30340;&#26631;&#31614;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#25311;&#21512;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.02064</link><description>&lt;p&gt;
GRASP: &#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#23398;&#20064;&#30340;&#36866;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRASP: A Goodness-of-Fit Test for Classification Learning. (arXiv:2209.02064v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;GRASP&#65292;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#20108;&#20998;&#31867;&#22120;&#23545;&#32473;&#23450;&#29305;&#24449;&#21521;&#37327;&#30340;&#26631;&#31614;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#25311;&#21512;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#36890;&#24120;&#20197;&#27979;&#35797;&#25968;&#25454;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#34913;&#37327;&#12290;&#23613;&#31649;&#24179;&#22343;&#20934;&#30830;&#29575;&#26159;&#19968;&#31181;&#26631;&#20934;&#30340;&#34913;&#37327;&#26041;&#27861;&#65292;&#20294;&#23427;&#22312;&#25551;&#36848;&#27169;&#22411;&#23545;&#32473;&#23450;&#29305;&#24449;&#21521;&#37327;&#30340;&#26631;&#31614;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#25311;&#21512;&#31243;&#24230;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#20363;&#22914;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#21270;&#12289;&#36807;&#25311;&#21512;&#21644;&#39640;&#32500;&#24230;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#35780;&#20272;&#36890;&#29992;&#20108;&#20998;&#31867;&#22120;&#25311;&#21512;&#31243;&#24230;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;$Y|X$&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#20551;&#35774;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#40657;&#30418;&#23376;&#27169;&#22411;&#65292;&#21482;&#33021;&#36890;&#36807;&#26597;&#35810;&#35775;&#38382;&#12290;&#25105;&#20204;&#23558;&#36866;&#21512;&#24230;&#35780;&#20272;&#38382;&#39064;&#34920;&#36848;&#20026;&#23481;&#24525;&#24230;&#20551;&#35774;&#26816;&#39564;&#30340;&#24418;&#24335;\[ H_0: \mathbb{E}\Big[D_f\Big({\sf Bern}(\eta(X))\|{\sf Bern}(\hat{\eta}(X))\Big)\Big]\leq \tau\,, \]&#20854;&#20013;$D_f$&#34920;&#31034;&#19968;&#20010;$f$-&#25955;&#24230;&#20989;&#25968;&#65292;$\eta(x)$&#21644;$\hat{\eta}(x)$&#20998;&#21035;&#34920;&#31034;&#29305;&#24449;&#21521;&#37327;$x$&#30340;&#30495;&#23454;&#21644;&#20272;&#35745;&#30340;&#20284;&#28982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance of classifiers is often measured in terms of average accuracy on test data. Despite being a standard measure, average accuracy fails in characterizing the fit of the model to the underlying conditional law of labels given the features vector ($Y|X$), e.g. due to model misspecification, over fitting, and high-dimensionality. In this paper, we consider the fundamental problem of assessing the goodness-of-fit for a general binary classifier. Our framework does not make any parametric assumption on the conditional law $Y|X$, and treats that as a black box oracle model which can be accessed only through queries. We formulate the goodness-of-fit assessment problem as a tolerance hypothesis testing of the form \[ H_0: \mathbb{E}\Big[D_f\Big({\sf Bern}(\eta(X))\|{\sf Bern}(\hat{\eta}(X))\Big)\Big]\leq \tau\,, \] where $D_f$ represents an $f$-divergence function, and $\eta(x)$, $\hat{\eta}(x)$ respectively denote the true and an estimate likelihood for a feature vector $x$ admitting
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#23545;&#24212;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;AI&#30340;&#40065;&#26834;&#24615;&#21644;&#20154;&#26426;&#22242;&#38431;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#34987;&#21457;&#29616;&#27604;kNN&#35299;&#37322;&#26356;&#26377;&#29992;&#65292;&#24110;&#21161;&#29992;&#25143;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;AI&#30340;&#38169;&#35823;&#20915;&#31574;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#22312;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20114;&#34917;&#20154;&#26426;&#22242;&#38431;&#20934;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.00780</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#23545;&#24212;&#30340;&#35299;&#37322;&#25552;&#39640;&#20102;AI&#30340;&#40065;&#26834;&#24615;&#21644;&#20154;&#26426;&#22242;&#38431;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. (arXiv:2208.00780v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00780
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#23545;&#24212;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;AI&#30340;&#40065;&#26834;&#24615;&#21644;&#20154;&#26426;&#22242;&#38431;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#34987;&#21457;&#29616;&#27604;kNN&#35299;&#37322;&#26356;&#26377;&#29992;&#65292;&#24110;&#21161;&#29992;&#25143;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;AI&#30340;&#38169;&#35823;&#20915;&#31574;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#22312;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20114;&#34917;&#20154;&#26426;&#22242;&#38431;&#20934;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#39044;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#29978;&#33267;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#20154;&#31867;&#26159;&#26368;&#32456;&#30340;&#20915;&#31574;&#32773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#33258;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#22120;&#26550;&#26500;&#65292;&#23427;&#20204;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#26597;&#35810;&#22270;&#20687;&#21644;&#31034;&#20363;&#20043;&#38388;&#30340;&#35270;&#35273;&#23545;&#24212;&#20851;&#31995;&#36827;&#34892;&#35299;&#37322;&#65292;&#28982;&#21518;&#36827;&#34892;&#39044;&#27979;&#65288;&#19982;&#20107;&#21518;&#35299;&#37322;&#30456;&#23545;&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#19978;&#19968;&#33268;&#25913;&#36827;&#65288;1&#21040;4&#20010;&#28857;&#65289;&#65292;&#32780;&#22312;&#20998;&#24067;&#27979;&#35797;&#19978;&#34920;&#29616;&#30053;&#27425;&#20110;ResNet-50&#21644;&#19968;&#20010;k&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65288;kNN&#65289;&#65288;&#19979;&#38477;1&#21040;2&#20010;&#28857;&#65289;&#12290;&#36890;&#36807;&#23545;ImageNet&#21644;CUB&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#35299;&#37322;&#27604;kNN&#35299;&#37322;&#23545;&#29992;&#25143;&#26356;&#26377;&#29992;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#24110;&#21161;&#29992;&#25143;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;AI&#30340;&#38169;&#35823;&#20915;&#31574;&#65292;&#32988;&#36807;&#25152;&#26377;&#20854;&#20182;&#27979;&#35797;&#26041;&#27861;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#23454;&#29616;&#20114;&#34917;&#20154;&#26426;&#22242;&#38431;&#20934;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate decision-makers. In this work, we propose two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. Our models consistently improve (by 1 to 4 points) on out-of-distribution (OOD) datasets while performing marginally worse (by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB, our correspondence-based explanations are found to be more useful to users than kNN explanations. Our explanations help users more accurately reject AI's wrong decisions than all other tested methods. Interestingly, for the first time, we show that it is possible to achieve complementary human-AI tea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#31354;&#38388;&#21306;&#22495;&#21010;&#20998;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25104;&#26524;&#12290;&#20854;&#20013;&#65292;&#20004;&#38454;&#27573;K&#27169;&#22411;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#27169;&#22411;&#25311;&#21512;&#12289;&#21306;&#22495;&#37325;&#26500;&#21644;&#31995;&#25968;&#20272;&#35745;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.09429</link><description>&lt;p&gt;
&#23558;&#21306;&#22495;&#21270;&#31639;&#27861;&#25193;&#23637;&#21040;&#25506;&#32034;&#31354;&#38388;&#36827;&#31243;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Extending regionalization algorithms to explore spatial process heterogeneity. (arXiv:2206.09429v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09429
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#31354;&#38388;&#21306;&#22495;&#21010;&#20998;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25104;&#26524;&#12290;&#20854;&#20013;&#65292;&#20004;&#38454;&#27573;K&#27169;&#22411;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#27169;&#22411;&#25311;&#21512;&#12289;&#21306;&#22495;&#37325;&#26500;&#21644;&#31995;&#25968;&#20272;&#35745;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31354;&#38388;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#31354;&#38388;&#24322;&#36136;&#24615;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#25110;&#31163;&#25955;&#35268;&#33539;&#36827;&#34892;&#32771;&#34385;&#12290;&#21518;&#32773;&#28041;&#21450;&#21040;&#30830;&#23450;&#20855;&#26377;&#21516;&#36136;&#21464;&#37327;&#20043;&#38388;&#30340;&#22343;&#36136;&#20851;&#31995;&#30340;&#36830;&#32493;&#31354;&#38388;&#21306;&#22495;&#65288;&#31354;&#38388;&#21306;&#22495;&#65289;&#12290;&#23613;&#31649;&#22312;&#31354;&#38388;&#20998;&#26512;&#39046;&#22495;&#20013;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#21508;&#31181;&#21306;&#22495;&#21270;&#31639;&#27861;&#65292;&#20294;&#20248;&#21270;&#31354;&#38388;&#21306;&#22495;&#30340;&#26041;&#27861;&#22522;&#26412;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31354;&#38388;&#21306;&#22495;&#21010;&#20998;&#31639;&#27861;&#65292;&#21363;&#20004;&#38454;&#27573;K&#27169;&#22411;&#21644;&#21306;&#22495;K&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23558;&#32463;&#20856;&#30340;&#33258;&#21160;&#20998;&#21306;&#31243;&#24207;&#25193;&#23637;&#21040;&#31354;&#38388;&#22238;&#24402;&#32972;&#26223;&#19979;&#12290;&#36825;&#20123;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#19977;&#31181;&#31639;&#27861;&#22312;&#27169;&#22411;&#25311;&#21512;&#12289;&#21306;&#22495;&#37325;&#26500;&#21644;&#31995;&#25968;&#20272;&#35745;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#36229;&#36234;&#25110;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#32780;&#20004;&#38454;&#27573;K&#27169;&#22411;&#31639;&#27861;&#22312;&#27169;&#22411;&#25311;&#21512;&#12289;&#21306;&#22495;&#37325;&#26500;&#21644;&#31995;&#25968;&#20272;&#35745;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spatial regression models, spatial heterogeneity may be considered with either continuous or discrete specifications. The latter is related to delineation of spatially connected regions with homogeneous relationships between variables (spatial regimes). Although various regionalization algorithms have been proposed and studied in the field of spatial analytics, methods to optimize spatial regimes have been largely unexplored. In this paper, we propose two new algorithms for spatial regime delineation, two-stage K-Models and Regional-K-Models. We also extend the classic Automatic Zoning Procedure to spatial regression context. The proposed algorithms are applied to a series of synthetic datasets and two real-world datasets. Results indicate that all three algorithms achieve superior or comparable performance to existing approaches, while the two-stage K-Models algorithm largely outperforms existing approaches on model fitting, region reconstruction, and coefficient estimation. Our wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;0/1 DNNs&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#27493;&#20989;&#25968;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#24182;&#33719;&#24471;&#20102;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26368;&#20339;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;DNNs&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.09379</link><description>&lt;p&gt;
0/1&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
0/1 Deep Neural Networks via Block Coordinate Descent. (arXiv:2206.09379v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;0/1 DNNs&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#27493;&#20989;&#25968;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#24182;&#33719;&#24471;&#20102;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26368;&#20339;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;DNNs&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27493;&#20989;&#25968;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#26368;&#31616;&#21333;&#19988;&#26368;&#33258;&#28982;&#30340;&#28608;&#27963;&#20989;&#25968;&#20043;&#19968;&#12290;&#30001;&#20110;&#23427;&#23545;&#20110;&#27491;&#21464;&#37327;&#35745;&#25968;&#20026;1&#65292;&#23545;&#20110;&#20854;&#20182;&#21464;&#37327;&#35745;&#25968;&#20026;0&#65292;&#20854;&#22266;&#26377;&#29305;&#24615;&#65288;&#22914;&#19981;&#36830;&#32493;&#24615;&#21644;&#26080;&#26377;&#25928;&#30340;&#27425;&#26799;&#24230;&#20449;&#24687;&#65289;&#38459;&#30861;&#20102;&#20854;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#29992;&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#35774;&#35745;&#30340;DNNs&#65292;&#21487;&#20197;&#35270;&#20026;&#27493;&#20989;&#25968;&#30340;&#26367;&#20195;&#21697;&#65292;&#20294;&#27493;&#20989;&#25968;&#20173;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#29305;&#24615;&#65292;&#22914;&#23545;&#24322;&#24120;&#20540;&#30340;&#23436;&#20840;&#40065;&#26834;&#24615;&#21644;&#20855;&#22791;&#26368;&#20339;&#23398;&#20064;&#29702;&#35770;&#25285;&#20445;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35757;&#32451;&#20351;&#29992;&#27493;&#20989;&#25968;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;DNNs&#65288;&#31216;&#20026;0/1 DNNs&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;0/1 DNNs&#37325;&#26032;&#34920;&#36848;&#20026;&#26080;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#36890;&#36807;&#22359;&#22352;&#26631;&#19979;&#38477;&#65288;BCD&#65289;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#27714;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#33719;&#24471;&#20102;BCD&#23376;&#38382;&#39064;&#30340;&#38381;&#24335;&#35299;&#20197;&#21450;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The step function is one of the simplest and most natural activation functions for deep neural networks (DNNs). As it counts 1 for positive variables and 0 for others, its intrinsic characteristics (e.g., discontinuity and no viable information of subgradients) impede its development for several decades. Even if there is an impressive body of work on designing DNNs with continuous activation functions that can be deemed as surrogates of the step function, it is still in the possession of some advantageous properties, such as complete robustness to outliers and being capable of attaining the best learning-theoretic guarantee of predictive accuracy. Hence, in this paper, we aim to train DNNs with the step function used as an activation function (dubbed as 0/1 DNNs). We first reformulate 0/1 DNNs as an unconstrained optimization problem and then solve it by a block coordinate descend (BCD) method. Moreover, we acquire closed-form solutions for sub-problems of BCD as well as its convergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#25512;&#29702;&#28151;&#21512;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30828;&#20214;&#24863;&#30693;&#30340;&#24322;&#26500;&#21487;&#24494;&#20998;&#37327;&#21270;&#21644;&#30446;&#26631;&#26799;&#24230;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#22312;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#19979;&#36798;&#21040;&#26368;&#20248;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2206.07741</link><description>&lt;p&gt;
&#20855;&#26377;&#23436;&#20840;&#21487;&#24494;&#20998;&#37327;&#21270;&#30340;&#28151;&#21512;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#32536;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks. (arXiv:2206.07741v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#25512;&#29702;&#28151;&#21512;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30828;&#20214;&#24863;&#30693;&#30340;&#24322;&#26500;&#21487;&#24494;&#20998;&#37327;&#21270;&#21644;&#30446;&#26631;&#26799;&#24230;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#22312;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#19979;&#36798;&#21040;&#26368;&#20248;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#22823;&#37327;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#24120;&#24120;&#38480;&#21046;&#20102;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#12290;&#23558;&#21442;&#25968;&#21644;&#25805;&#20316;&#37327;&#21270;&#20026;&#20302;&#20301;&#31934;&#24230;&#21487;&#20197;&#22823;&#24133;&#33410;&#30465;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#20869;&#23384;&#21644;&#33021;&#37327;&#65292;&#20415;&#20110;&#22312;&#36793;&#32536;&#35745;&#31639;&#24179;&#21488;&#19978;&#20351;&#29992;DNNs&#12290;&#26368;&#36817;&#30340;&#37327;&#21270;DNNs&#30340;&#21162;&#21147;&#37319;&#29992;&#20102;&#19968;&#31995;&#21015;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#28176;&#36827;&#24335;&#37327;&#21270;&#12289;&#27493;&#38271;&#36866;&#24212;&#21644;&#26799;&#24230;&#32553;&#25918;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#30340;&#28151;&#21512;&#31934;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#26032;&#30340;&#37327;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#37327;&#21270;&#27169;&#22411;&#65292;&#22312;&#19981;&#21040;4.3 MB&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#19979;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;(i) &#22522;&#20110;&#30828;&#20214;&#24863;&#30693;&#30340;&#24322;&#26500;&#21487;&#24494;&#20998;&#37327;&#21270;&#65292;&#20351;&#29992;&#24352;&#37327;&#20998;&#22359;&#23398;&#20064;&#30340;&#31934;&#24230;&#65292;(ii) &#38024;&#23545;&#26435;&#37325;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#26799;&#24230;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;
The large computing and memory cost of deep neural networks (DNNs) often precludes their use in resource-constrained devices. Quantizing the parameters and operations to lower bit-precision offers substantial memory and energy savings for neural network inference, facilitating the use of DNNs on edge computing platforms. Recent efforts at quantizing DNNs have employed a range of techniques encompassing progressive quantization, step-size adaptation, and gradient scaling. This paper proposes a new quantization approach for mixed precision convolutional neural networks (CNNs) targeting edge-computing. Our method establishes a new pareto frontier in model accuracy and memory footprint demonstrating a range of quantized models, delivering best-in-class accuracy below 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions are: (i) hardware-aware heterogeneous differentiable quantization with tensor-sliced learned precision, (ii) targeted gradient modification for wgts. an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#22810;&#26679;&#21270;&#21040;&#31070;&#32463;&#20803;&#26469;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#65292;&#26500;&#24314;&#20986;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#33258;&#36523;&#28608;&#27963;&#20989;&#25968;&#24555;&#36895;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#32988;&#36807;&#20256;&#32479;&#30340;&#21516;&#26500;&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20248;&#65292;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#22810;&#26679;&#24615;&#20026;&#21160;&#24577;&#31995;&#32479;&#36873;&#25321;&#22810;&#26679;&#24615;&#32780;&#38750;&#22343;&#21248;&#24615;&#25552;&#20379;&#20102;&#20363;&#23376;&#65292;&#24182;&#38416;&#26126;&#20102;&#22810;&#26679;&#24615;&#22312;&#33258;&#28982;&#21644;&#20154;&#24037;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2204.04348</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#33021;&#22815;&#25552;&#39640;&#29289;&#29702;&#23398;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuronal diversity can improve machine learning for physics and beyond. (arXiv:2204.04348v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#22810;&#26679;&#21270;&#21040;&#31070;&#32463;&#20803;&#26469;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#65292;&#26500;&#24314;&#20986;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#33258;&#36523;&#28608;&#27963;&#20989;&#25968;&#24555;&#36895;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#32988;&#36807;&#20256;&#32479;&#30340;&#21516;&#26500;&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20248;&#65292;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#22810;&#26679;&#24615;&#20026;&#21160;&#24577;&#31995;&#32479;&#36873;&#25321;&#22810;&#26679;&#24615;&#32780;&#38750;&#22343;&#21248;&#24615;&#25552;&#20379;&#20102;&#20363;&#23376;&#65292;&#24182;&#38416;&#26126;&#20102;&#22810;&#26679;&#24615;&#22312;&#33258;&#28982;&#21644;&#20154;&#24037;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30028;&#34920;&#29616;&#20986;&#22810;&#26679;&#24615;&#30340;&#20248;&#28857;&#65292;&#20294;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#36890;&#24120;&#26159;&#30001;&#21516;&#26500;&#31070;&#32463;&#20803;&#26500;&#25104;&#30340;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#24314;&#31435;&#36215;&#33021;&#22815;&#23398;&#20064;&#33258;&#36523;&#28608;&#27963;&#20989;&#25968;&#12289;&#24555;&#36895;&#22810;&#26679;&#21270;&#24182;&#19988;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#32988;&#36807;&#21516;&#26500;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23376;&#32593;&#32476;&#23454;&#20363;&#21270;&#20102;&#31070;&#32463;&#20803;&#65292;&#32780;&#20803;&#23398;&#20064;&#23588;&#20854;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#21709;&#24212;&#38598;&#21512;&#12290;&#20363;&#23376;&#21253;&#25324;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#25968;&#23383;&#21644;&#39044;&#27979;&#19968;&#20010; van der Pol &#25391;&#33633;&#22120;&#21644;&#19968;&#31181;&#29289;&#29702;&#23398;&#39537;&#21160;&#30340; Hamiltonian &#31070;&#32463;&#32593;&#32476;&#23398;&#20064; H&#233;nond-Heiles &#36712;&#36947;&#12290;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#22810;&#26679;&#24615;&#20026;&#21160;&#24577;&#31995;&#32479;&#36873;&#25321;&#22810;&#26679;&#24615;&#32780;&#38750;&#22343;&#21248;&#24615;&#25552;&#20379;&#20102;&#20363;&#23376;&#65292;&#24182;&#38416;&#26126;&#20102;&#22810;&#26679;&#24615;&#22312;&#33258;&#28982;&#21644;&#20154;&#24037;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diversity conveys advantages in nature, yet homogeneous neurons typically comprise the layers of artificial neural networks. Here we construct neural networks from neurons that learn their own activation functions, quickly diversify, and subsequently outperform their homogeneous counterparts on image classification and nonlinear regression tasks. Sub-networks instantiate the neurons, which meta-learn especially efficient sets of nonlinear responses. Examples include conventional neural networks classifying digits and forecasting a van der Pol oscillator and a physics-informed Hamiltonian neural network learning H\'enon-Heiles orbits. Such learned diversity provides examples of dynamical systems selecting diversity over uniformity and elucidates the role of diversity in natural and artificial systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36317;&#31163;&#20960;&#20309;&#38382;&#39064;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MGNN&#65289;&#65292;&#36890;&#36807;&#25913;&#36827;&#22522;&#20989;&#25968;&#35774;&#35745;&#26469;&#25552;&#39640;GNN&#27169;&#22411;&#30340;&#36817;&#20284;&#33021;&#21147;&#65292;&#24182;&#20174;&#20960;&#20309;&#21644;&#29289;&#29702;&#30340;&#35282;&#24230;&#23545;&#31354;&#38388;GNNs&#30340;&#26222;&#36866;&#24615;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2201.12994</link><description>&lt;p&gt;
MGNN: &#21463;&#36317;&#31163;&#20960;&#20309;&#38382;&#39064;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MGNN: Graph Neural Networks Inspired by Distance Geometry Problem. (arXiv:2201.12994v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36317;&#31163;&#20960;&#20309;&#38382;&#39064;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MGNN&#65289;&#65292;&#36890;&#36807;&#25913;&#36827;&#22522;&#20989;&#25968;&#35774;&#35745;&#26469;&#25552;&#39640;GNN&#27169;&#22411;&#30340;&#36817;&#20284;&#33021;&#21147;&#65292;&#24182;&#20174;&#20960;&#20309;&#21644;&#29289;&#29702;&#30340;&#35282;&#24230;&#23545;&#31354;&#38388;GNNs&#30340;&#26222;&#36866;&#24615;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#36890;&#24120;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#22522;&#20110;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#35774;&#35745;&#30340;&#39057;&#35889;GNNs&#21644;&#20197;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#20316;&#20026;&#27169;&#22411;&#22522;&#30784;&#30340;&#31354;&#38388;GNNs&#12290;&#23545;&#20110;&#39057;&#35889;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26222;&#36866;&#24615;&#65292;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#25913;&#36827;&#22522;&#20989;&#25968;&#30340;&#35774;&#35745;&#65292;&#20197;&#25552;&#39640;&#36817;&#20284;&#33021;&#21147;&#12290;&#33267;&#20110;&#31354;&#38388;GNNs&#65292;&#20687;&#22270;&#21516;&#26500;&#32593;&#32476;&#65288;GIN&#65289;&#36825;&#26679;&#30340;&#27169;&#22411;&#36890;&#36807;&#20998;&#26512;&#20854;&#34920;&#36798;&#33021;&#21147;&#26469;&#36827;&#34892;&#22270;&#21516;&#26500;&#27979;&#35797;&#12290;&#26368;&#36817;&#65292;&#26377;&#20154;&#23581;&#35797;&#24314;&#31435;&#31354;&#38388;GNNs&#19982;&#20960;&#20309;&#27010;&#24565;&#22914;&#26354;&#29575;&#21644;&#32454;&#32990;&#26463;&#20197;&#21450;&#29289;&#29702;&#29616;&#35937;&#22914;&#25391;&#33633;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#20960;&#20309;&#21644;&#29289;&#29702;&#30340;&#35282;&#24230;&#19978;&#65292;&#20851;&#20110;&#31354;&#38388;GNNs&#26222;&#36866;&#24615;&#30340;&#32508;&#21512;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36317;&#31163;&#20960;&#20309;&#38382;&#39064;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MGNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a prominent research topic in the field of machine learning. Existing GNN models are commonly categorized into two types: spectral GNNs, which are designed based on polynomial graph filters, and spatial GNNs, which utilize a message-passing scheme as the foundation of the model. For the expressive power and universality of spectral GNNs, a natural approach is to improve the design of basis functions for better approximation ability. As for spatial GNNs, models like Graph Isomorphism Networks (GIN) analyze their expressive power based on Graph Isomorphism Tests. Recently, there have been attempts to establish connections between spatial GNNs and geometric concepts like curvature and cellular sheaves, as well as physical phenomena like oscillators. However, despite the recent progress, there is still a lack of comprehensive analysis regarding the universality of spatial GNNs from the perspectives of geometry and physics. In this paper, we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#23637;&#30340;&#26102;&#38388;&#24207;&#21015;&#38388;&#38548;&#22238;&#25253;&#22270;&#65288;XIRP&#65289;&#20316;&#20026;&#20108;&#32500;&#22270;&#20687;&#34920;&#31034;&#65292;&#33021;&#22815;&#20197;&#23610;&#24230;&#19981;&#21464;&#21644;&#21487;&#36870;&#30340;&#26041;&#24335;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20174;&#32780;&#22312;&#38477;&#20302;&#35757;&#32451;&#26102;&#38388;&#21644;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#22270;&#20687;&#34920;&#31034;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#25913;&#36827;&#30340;&#38543;&#26426;&#21453;&#28436;&#26041;&#27861;&#20197;&#25913;&#21892;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2112.08060</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Leveraging Image-based Generative Adversarial Networks for Time Series Generation. (arXiv:2112.08060v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#23637;&#30340;&#26102;&#38388;&#24207;&#21015;&#38388;&#38548;&#22238;&#25253;&#22270;&#65288;XIRP&#65289;&#20316;&#20026;&#20108;&#32500;&#22270;&#20687;&#34920;&#31034;&#65292;&#33021;&#22815;&#20197;&#23610;&#24230;&#19981;&#21464;&#21644;&#21487;&#36870;&#30340;&#26041;&#24335;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20174;&#32780;&#22312;&#38477;&#20302;&#35757;&#32451;&#26102;&#38388;&#21644;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#22270;&#20687;&#34920;&#31034;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#25913;&#36827;&#30340;&#38543;&#26426;&#21453;&#28436;&#26041;&#27861;&#20197;&#25913;&#21892;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#20174;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#29983;&#25104;&#36924;&#30495;&#30340;&#26679;&#26412;&#65292;&#22240;&#27492;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#21033;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#32500;&#22270;&#20687;&#34920;&#31034;&#26041;&#27861;&#65292;&#21363;&#25299;&#23637;&#30340;&#26102;&#38388;&#24207;&#21015;&#38388;&#38548;&#22238;&#25253;&#22270;&#65288;Extended Intertemporal Return Plot&#65292;XIRP&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#23610;&#24230;&#19981;&#21464;&#21644;&#21487;&#36870;&#30340;&#26041;&#24335;&#25429;&#25417;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#38477;&#20302;&#20102;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#20102;&#26679;&#26412;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#26799;&#24230;&#24809;&#32602;&#30340;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;WGAN-GP&#65289;&#23545;&#21512;&#25104;&#30340;XIRP&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#20854;&#20182;&#22270;&#20687;&#34920;&#31034;&#26041;&#27861;&#21644;&#27169;&#22411;&#36827;&#34892;&#20102;&#30456;&#20284;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#25351;&#26631;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#24615;&#12289;&#32463;&#36807;&#39564;&#35777;&#30340;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#34920;&#31034;&#26041;&#27861;&#22312;&#39044;&#27979;&#33021;&#21147;&#26041;&#38754;&#22987;&#32456;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;RNN&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25913;&#36827;&#30340;&#38543;&#26426;&#21453;&#28436;&#26041;&#27861;&#20351;&#24471;&#37325;&#24314;&#26102;&#38388;&#24207;&#21015;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models for images have gained significant attention in computer vision and natural language processing due to their ability to generate realistic samples from complex data distributions. To leverage the advances of image-based generative models for the time series domain, we propose a two-dimensional image representation for time series, the Extended Intertemporal Return Plot (XIRP). Our approach captures the intertemporal time series dynamics in a scale-invariant and invertible way, reducing training time and improving sample quality. We benchmark synthetic XIRPs obtained by an off-the-shelf Wasserstein GAN with gradient penalty (WGAN-GP) to other image representations and models regarding similarity and predictive ability metrics. Our novel, validated image representation for time series consistently and significantly outperforms a state-of-the-art RNN-based generative model regarding predictive ability. Further, we introduce an improved stochastic inversion to substantial
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#24212;&#29992;&#20110;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#26597;&#35810;&#22238;&#31572;&#12290;&#36890;&#36807;&#23558;&#26412;&#20307;&#35770;&#32435;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;&#26597;&#35810;&#22238;&#31572;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#38598;&#25104;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#65292;&#21462;&#24471;&#20102;20%&#21040;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2106.14052</link><description>&lt;p&gt;
&#23558;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#36827;&#34892;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#26597;&#35810;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Combining Inductive and Deductive Reasoning for Query Answering over Incomplete Knowledge Graphs. (arXiv:2106.14052v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.14052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#24212;&#29992;&#20110;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#26597;&#35810;&#22238;&#31572;&#12290;&#36890;&#36807;&#23558;&#26412;&#20307;&#35770;&#32435;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;&#26597;&#35810;&#22238;&#31572;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#38598;&#25104;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#65292;&#21462;&#24471;&#20102;20%&#21040;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#26597;&#35810;&#22238;&#31572;&#26041;&#27861;&#21482;&#38598;&#20013;&#22312;&#24402;&#32435;&#25512;&#29702;&#19978;&#65292;&#21363;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27169;&#24335;&#26469;&#39044;&#27979;&#31572;&#26696;&#65292;&#24182;&#32570;&#20047;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#28436;&#32462;&#25512;&#29702;&#38656;&#35201;&#24212;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#25512;&#26029;&#26356;&#22810;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#26412;&#20307;&#35770;&#32435;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;&#26597;&#35810;&#22238;&#31572;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#23450;&#20041;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#22238;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#38598;&#25104;&#31574;&#30053;&#65292;&#21253;&#25324;&#65288;1&#65289;&#19981;&#21516;&#30340;&#26412;&#20307;&#39537;&#21160;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#65288;2&#65289;&#36866;&#24212;&#26412;&#20307;&#20844;&#29702;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;LUBM&#21644;NELL&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#39062;&#22522;&#20934;&#65292;&#24182;&#22312;&#20854;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#38656;&#35201;&#24402;&#32435;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#20174;20%&#21040;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for embedding-based query answering over incomplete Knowledge Graphs (KGs) only focus on inductive reasoning, i.e., predicting answers by learning patterns from the data, and lack the complementary ability to do deductive reasoning, which requires the application of domain knowledge to infer further information. To address this shortcoming, we investigate the problem of incorporating ontologies into embedding-based query answering models by defining the task of embedding-based ontology-mediated query answering. We propose various integration strategies into prominent representatives of embedding models that involve (1) different ontology-driven data augmentation techniques and (2) adaptation of the loss function to enforce the ontology axioms. We design novel benchmarks for the considered task based on the LUBM and the NELL KGs and evaluate our methods on them. The achieved improvements in the setting that requires both inductive and deductive reasoning are from 20% to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#36136;&#37327;&#20445;&#35777;&#30028;&#38754;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#21644;&#26426;&#22120;&#26234;&#33021;&#30340;&#32452;&#21512;&#25928;&#24212;&#65292;&#20197;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26469;&#35780;&#20272;&#21644;&#20248;&#21270;QA4ML&#30028;&#38754;&#30340;&#25928;&#29575;</title><link>http://arxiv.org/abs/2104.01129</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#30340;&#29992;&#25143;&#30028;&#38754;&#20248;&#21270;&#29992;&#20110;&#20445;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Simulation-Based Optimization of User Interfaces for Quality-Assuring Machine Learning Model Predictions. (arXiv:2104.01129v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.01129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#36136;&#37327;&#20445;&#35777;&#30028;&#38754;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#21644;&#26426;&#22120;&#26234;&#33021;&#30340;&#32452;&#21512;&#25928;&#24212;&#65292;&#20197;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26469;&#35780;&#20272;&#21644;&#20248;&#21270;QA4ML&#30028;&#38754;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36136;&#37327;&#25935;&#24863;&#24212;&#29992;&#38656;&#35201;&#22312;&#27169;&#22411;&#39044;&#27979;&#34987;&#37096;&#32626;&#20043;&#21069;&#30001;&#20154;&#31867;&#36827;&#34892;&#36136;&#37327;&#20445;&#35777;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#36136;&#37327;&#20445;&#35777;&#25509;&#21475;&#38656;&#35201;&#29992;&#25143;&#26597;&#30475;&#22823;&#37327;&#25968;&#25454;&#24182;&#36827;&#34892;&#22810;&#27425;&#20132;&#20114;&#20197;&#32416;&#27491;&#27169;&#22411;&#30340;&#38169;&#35823;&#12290;&#20248;&#21270;&#30340;&#29992;&#25143;&#30028;&#38754;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#20132;&#20114;&#25104;&#26412;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#35774;&#35745;&#36873;&#39033;&#26469;&#20248;&#21270;&#29992;&#25143;&#30028;&#38754;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#19981;&#21487;&#25193;&#23637;&#65292;&#22240;&#20026;&#36890;&#24120;&#26377;&#35768;&#22810;&#24494;&#23567;&#30340;&#21464;&#21270;&#20250;&#24433;&#21709;QA4ML&#30028;&#38754;&#30340;&#25928;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27169;&#25311;&#26469;&#35780;&#20272;&#21644;&#36741;&#21161;&#20248;&#21270;QA4ML&#30028;&#38754;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#27169;&#25311;&#20013;&#20154;&#31867;&#26234;&#33021;&#21457;&#36215;&#36866;&#24403;&#30340;&#20132;&#20114;&#21629;&#20196;&#21644;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#31639;&#27861;&#36741;&#21161;&#21152;&#36895;QA4ML&#36807;&#31243;&#30340;&#32508;&#21512;&#25928;&#26524;&#12290;&#30001;&#20110;QA4ML&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#21171;&#21160;&#21147;&#65292;&#25105;&#20204;&#20197;&#27169;&#25311;&#30340;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Quality-sensitive applications of machine learning (ML) require quality assurance (QA) by humans before the predictions of an ML model can be deployed. QA for ML (QA4ML) interfaces require users to view a large amount of data and perform many interactions to correct errors made by the ML model. An optimized user interface (UI) can significantly reduce interaction costs. While UI optimization can be informed by user studies evaluating design options, this approach is not scalable because there are typically numerous small variations that can affect the efficiency of a QA4ML interface. Hence, we propose using simulation to evaluate and aid the optimization of QA4ML interfaces. In particular, we focus on simulating the combined effects of human intelligence in initiating appropriate interaction commands and machine intelligence in providing algorithmic assistance for accelerating QA4ML processes. As QA4ML is usually labor-intensive, we use the simulated task completion time as the metric 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#21363;&#20351;&#29615;&#22659;&#23436;&#20840;&#26410;&#30693;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#25511;&#21046;&#22120;&#21644;&#23545;&#25239;&#29615;&#22659;&#20043;&#38388;&#30340;&#38543;&#26426;&#21338;&#24328;&#65292;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#65292;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29615;&#22659;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2102.04307</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#23398;&#20064;&#26102;&#24577;&#20219;&#21153;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Strategies for Temporal Tasks in Stochastic Games. (arXiv:2102.04307v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.04307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#21363;&#20351;&#29615;&#22659;&#23436;&#20840;&#26410;&#30693;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#25511;&#21046;&#22120;&#21644;&#23545;&#25239;&#29615;&#22659;&#20043;&#38388;&#30340;&#38543;&#26426;&#21338;&#24328;&#65292;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#65292;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29615;&#22659;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#35268;&#33539;&#20013;&#36827;&#34892;&#21512;&#25104;&#21487;&#20197;&#20026;&#22312;&#38543;&#26426;&#21644;&#28508;&#22312;&#23545;&#25239;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#31995;&#32479;&#25552;&#20379;&#20445;&#35777;&#30340;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21512;&#25104;&#24037;&#20855;&#38656;&#35201;&#19968;&#20010;&#29615;&#22659;&#27169;&#22411;&#26469;&#26500;&#24314;&#25511;&#21046;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#20013;&#25512;&#23548;&#25511;&#21046;&#22120;&#65292;&#21363;&#20351;&#29615;&#22659;&#23436;&#20840;&#26410;&#30693;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#25511;&#21046;&#22120;&#21644;&#23545;&#25239;&#29615;&#22659;&#20043;&#38388;&#30340;&#38543;&#26426;&#21338;&#24328;&#65288;SG&#65289;&#65292;&#28982;&#21518;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#65292;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29615;&#22659;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20174;&#32473;&#23450;LTL&#35268;&#33539;&#32763;&#35793;&#30340;&#30830;&#23450;&#24615;&#22855;&#20598;&#33258;&#21160;&#26426;&#65288;DPA&#65289;&#26500;&#24314;&#19968;&#20010;&#20056;&#31215;&#21338;&#24328;&#12290;&#36890;&#36807;&#20174;DPA&#25509;&#21463;&#26465;&#20214;&#23548;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#21644;&#25240;&#25187;&#22240;&#23376;&#65292;&#25105;&#20204;&#23558;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesis from linear temporal logic (LTL) specifications provides assured controllers for systems operating in stochastic and potentially adversarial environments. Automatic synthesis tools, however, require a model of the environment to construct controllers. In this work, we introduce a model-free reinforcement learning (RL) approach to derive controllers from given LTL specifications even when the environment is completely unknown. We model the problem as a stochastic game (SG) between the controller and the adversarial environment; we then learn optimal control strategies that maximize the probability of satisfying the LTL specifications against the worst-case environment behavior. We first construct a product game using the deterministic parity automaton (DPA) translated from the given LTL specification. By deriving distinct rewards and discount factors from the acceptance condition of the DPA, we reduce the maximization of the worst-case probability of satisfying the LTL specifi
&lt;/p&gt;</description></item></channel></rss>