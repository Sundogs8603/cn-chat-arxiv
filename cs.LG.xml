<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>AMC-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#30340;&#26032;&#22411;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#36827;&#34892;&#38477;&#22122;&#21644;&#25191;&#34892;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#26469;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#65292;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.00445</link><description>&lt;p&gt;
AMC-Net: &#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AMC-Net: An Effective Network for Automatic Modulation Classification. (arXiv:2304.00445v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00445
&lt;/p&gt;
&lt;p&gt;
AMC-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#30340;&#26032;&#22411;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#36827;&#34892;&#38477;&#22122;&#21644;&#25191;&#34892;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#26469;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#65292;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;(AMC)&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#39057;&#35889;&#31649;&#29702;&#65292;&#20449;&#21495;&#30417;&#27979;&#21644;&#25511;&#21046;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#38454;&#27573;&#12290;&#20934;&#30830;&#30340;&#35843;&#21046;&#26684;&#24335;&#20998;&#31867;&#22312;&#21518;&#32493;&#20256;&#36755;&#25968;&#25454;&#30340;&#35299;&#30721;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#24212;&#29992;&#20110;AMC&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;(SNR)&#29615;&#22659;&#20013;&#65292;AMC&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;AMC-Net&#65292;&#23427;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#23545;&#36755;&#20837;&#20449;&#21495;&#36827;&#34892;&#38477;&#22122;&#65292;&#24182;&#25191;&#34892;&#22810;&#23610;&#24230;&#21644;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#25913;&#21892;&#35782;&#21035;&#12290;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic modulation classification (AMC) is a crucial stage in the spectrum management, signal monitoring, and control of wireless communication systems. The accurate classification of the modulation format plays a vital role in the subsequent decoding of the transmitted data. End-to-end deep learning methods have been recently applied to AMC, outperforming traditional feature engineering techniques. However, AMC still has limitations in low signal-to-noise ratio (SNR) environments. To address the drawback, we propose a novel AMC-Net that improves recognition by denoising the input signal in the frequency domain while performing multi-scale and effective feature extraction. Experiments on two representative datasets demonstrate that our model performs better in efficiency and effectiveness than the most current methods.
&lt;/p&gt;</description></item><item><title>SoftED metrics &#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20107;&#20214;&#26816;&#27979;&#30340;&#26032;&#25351;&#26631;&#65292;&#26082;&#21253;&#25324;&#26102;&#38388;&#30340;&#27010;&#24565;&#65292;&#21448;&#21253;&#25324;&#23545;&#30456;&#37051;&#26816;&#27979;&#30340;&#26102;&#38388;&#23481;&#24525;&#24230;&#65292;&#23427;&#20204;&#33021;&#22815;&#21516;&#26102;&#35780;&#20272;&#20107;&#20214;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#20854;&#26816;&#27979;&#26159;&#21542;&#20195;&#34920;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.00439</link><description>&lt;p&gt;
SoftED: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20107;&#20214;&#26816;&#27979;&#30340;&#36719;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
SoftED: Metrics for Soft Evaluation of Time Series Event Detection. (arXiv:2304.00439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00439
&lt;/p&gt;
&lt;p&gt;
SoftED metrics &#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20107;&#20214;&#26816;&#27979;&#30340;&#26032;&#25351;&#26631;&#65292;&#26082;&#21253;&#25324;&#26102;&#38388;&#30340;&#27010;&#24565;&#65292;&#21448;&#21253;&#25324;&#23545;&#30456;&#37051;&#26816;&#27979;&#30340;&#26102;&#38388;&#23481;&#24525;&#24230;&#65292;&#23427;&#20204;&#33021;&#22815;&#21516;&#26102;&#35780;&#20272;&#20107;&#20214;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#20854;&#26816;&#27979;&#26159;&#21542;&#20195;&#34920;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#26631;&#20934;&#30340;&#20998;&#31867;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#25351;&#26631;&#20165;&#20851;&#27880;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20107;&#20214;&#26816;&#27979;&#30340;&#19981;&#20934;&#30830;&#24448;&#24448;&#26159;&#30001;&#20110;&#21069;&#21518;&#30456;&#20851;&#20107;&#20214;&#22312;&#30456;&#37051;&#26816;&#27979;&#20013;&#30340;&#21453;&#24212;&#20135;&#29983;&#30340;&#12290;&#36825;&#20123;&#26816;&#27979;&#23545;&#20110;&#35302;&#21457;&#24517;&#35201;&#30340;&#34892;&#21160;&#25110;&#24110;&#21161;&#20943;&#36731;&#19981;&#33391;&#21518;&#26524;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#23545;&#20110;&#20107;&#20214;&#26816;&#27979;&#26469;&#35828;&#26159;&#19981;&#20805;&#20998;&#21644;&#19981;&#36866;&#24403;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#25351;&#26631;&#65292;&#26082;&#21253;&#25324;&#26102;&#38388;&#30340;&#27010;&#24565;&#65292;&#21448;&#21253;&#25324;&#23545;&#30456;&#37051;&#26816;&#27979;&#30340;&#26102;&#38388;&#23481;&#24525;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#38598;&#21512;&#8220;SoftED metrics&#8221;&#65292;&#26088;&#22312;&#36719;&#35780;&#20272;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#35780;&#20272;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#20197;&#21450;&#20854;&#26816;&#27979;&#26159;&#21542;&#20195;&#34920;&#20107;&#20214;&#12290;&#36890;&#36807;&#23558;&#20107;&#20214;&#21644;&#20195;&#34920;&#24615;&#26816;&#27979;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;36\%&#20197;&#19978;&#30340;&#23454;&#39564;&#20013;&#21152;&#20837;&#26102;&#38388;&#23481;&#24525;&#24230;&#65292;&#25552;&#39640;&#20102;&#20107;&#20214;&#26816;&#27979;&#30340;&#35780;&#20272;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series event detection methods are evaluated mainly by standard classification metrics that focus solely on detection accuracy. However, inaccuracy in detecting an event can often result from its preceding or delayed effects reflected in neighboring detections. These detections are valuable to trigger necessary actions or help mitigate unwelcome consequences. In this context, current metrics are insufficient and inadequate for the context of event detection. There is a demand for metrics that incorporate both the concept of time and temporal tolerance for neighboring detections. This paper introduces SoftED metrics, a new set of metrics designed for soft evaluating event detection methods. They enable the evaluation of both detection accuracy and the degree to which their detections represent events. They improved event detection evaluation by associating events and their representative detections, incorporating temporal tolerance in over 36\% of experiments compared to the usual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#35745;&#31639;&#29702;&#24819;&#35266;&#23519;&#32773;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;IO&#26041;&#27861;&#36866;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2304.00433</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#35745;&#31639;&#29702;&#24819;&#35266;&#23519;&#32773;
&lt;/p&gt;
&lt;p&gt;
Ideal Observer Computation by Use of Markov-Chain Monte Carlo with Generative Adversarial Networks. (arXiv:2304.00433v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#35745;&#31639;&#29702;&#24819;&#35266;&#23519;&#32773;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;IO&#26041;&#27861;&#36866;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#31995;&#32479;&#32463;&#24120;&#36890;&#36807;&#23458;&#35266;&#25110;&#20219;&#21153;&#29305;&#23450;&#30340;&#22270;&#20687;&#36136;&#37327;&#65288;IQ&#65289;&#24230;&#37327;&#26469;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#20197;&#37327;&#21270;&#35266;&#23519;&#32773;&#22312;&#20855;&#20307;&#30340;&#20020;&#24202;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#36125;&#21494;&#26031;&#29702;&#24819;&#35266;&#23519;&#32773;&#65288;IO&#65289;&#30340;&#34920;&#29616;&#22312;&#25152;&#26377;&#35266;&#23519;&#32773;&#65288;&#25968;&#20540;&#25110;&#20154;&#31867;&#65289;&#20013;&#35774;&#23450;&#20102;&#19968;&#20010;&#19978;&#38480;&#65292;&#24182;&#24050;&#34987;&#25552;&#20513;&#29992;&#20316;&#35780;&#20272;&#21644;&#20248;&#21270;&#21307;&#23398;&#25104;&#20687;&#31995;&#32479;&#25928;&#33021;&#30340;merit&#12290;&#28982;&#32780;&#65292;IO&#27979;&#35797;&#32479;&#35745;&#37327;&#23545;&#24212;&#20110;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#38590;&#20197;&#35745;&#31639;&#30340;&#20284;&#28982;&#27604;&#12290;&#20197;&#21069;&#26366;&#25552;&#20986;&#36807;&#19968;&#31181;&#37319;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#25216;&#26415;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;IO&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23558;MCMC&#26041;&#27861;&#24212;&#29992;&#20110;IO&#36817;&#20284;&#30340;&#24773;&#20917;&#20165;&#38480;&#20110;&#23569;&#25968;&#24773;&#20917;&#65292;&#20854;&#20013;&#34987;&#25104;&#20687;&#23545;&#35937;&#30340;&#32771;&#34385;&#20998;&#24067;&#21487;&#20197;&#29992;&#30456;&#23545;&#31616;&#21333;&#30340;&#38543;&#26426;&#23545;&#35937;&#27169;&#22411;&#65288;SOM&#65289;&#26469;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25193;&#23637;&#39046;&#22495;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging systems are often evaluated and optimized via objective, or task-specific, measures of image quality (IQ) that quantify the performance of an observer on a specific clinically-relevant task. The performance of the Bayesian Ideal Observer (IO) sets an upper limit among all observers, numerical or human, and has been advocated for use as a figure-of-merit (FOM) for evaluating and optimizing medical imaging systems. However, the IO test statistic corresponds to the likelihood ratio that is intractable to compute in the majority of cases. A sampling-based method that employs Markov-Chain Monte Carlo (MCMC) techniques was previously proposed to estimate the IO performance. However, current applications of MCMC methods for IO approximation have been limited to a small number of situations where the considered distribution of to-be-imaged objects can be described by a relatively simple stochastic object model (SOM). As such, there remains an important need to extend the domain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#25511;&#21046;&#65292;&#26082;&#32771;&#34385;&#20102;&#31995;&#32479;&#19981;&#21464;&#23494;&#24230;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21448;&#33021;&#23545;&#31995;&#32479;&#36827;&#34892;&#26377;&#25928;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.00423</link><description>&lt;p&gt;
&#20960;&#20309;&#32422;&#26463;&#25552;&#39640;&#20102;&#23545;&#31232;&#30095;&#35266;&#27979;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Geometric constraints improve inference of sparsely observed stochastic dynamics. (arXiv:2304.00423v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#25511;&#21046;&#65292;&#26082;&#32771;&#34385;&#20102;&#31995;&#32479;&#19981;&#21464;&#23494;&#24230;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21448;&#33021;&#23545;&#31995;&#32479;&#36827;&#34892;&#26377;&#25928;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#30001;&#24230;&#30340;&#31995;&#32479;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#28436;&#21270;&#30340;&#21160;&#21147;&#23398;&#36890;&#24120;&#20197;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24418;&#24335;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#24120;&#36825;&#20123;&#26041;&#31243;&#30340;&#32467;&#26500;&#24418;&#24335;&#26159;&#26410;&#30693;&#30340;&#65292;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#21807;&#19968;&#34920;&#29616;&#24418;&#24335;&#26159;&#22312;&#31163;&#25955;&#26102;&#38388;&#28857;&#19978;&#30340;&#35266;&#27979;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20934;&#30830;&#22320;&#20174;&#31232;&#30095;&#26102;&#22495;&#35266;&#27979;&#20013;&#25512;&#26029;&#36825;&#20123;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#25512;&#26029;&#26041;&#27861;&#35201;&#20040;&#38598;&#20013;&#20110;&#35266;&#27979;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#24573;&#30053;&#31995;&#32479;&#19981;&#21464;&#23494;&#24230;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#35201;&#20040;&#20351;&#29992;&#31995;&#32479;&#19981;&#21464;&#23494;&#24230;&#30340;&#20960;&#20309;&#36924;&#36817;&#65292;&#36825;&#20123;&#36924;&#36817;&#20165;&#36866;&#29992;&#20110;&#20445;&#23432;&#30340;&#39537;&#21160;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#22312;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#36825;&#20004;&#20010;&#35270;&#35282;&#35843;&#21644;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36335;&#24452;&#22686;&#24378;&#26041;&#26696;&#65292;&#23427;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#25511;&#21046;&#26469;&#32771;&#34385;&#19981;&#21464;&#31995;&#32479;&#23494;&#24230;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#23545;&#22686;&#24378;&#36335;&#24452;&#30340;&#38750;&#21442;&#25968;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#23545;&#31995;&#32479;&#30340;&#26377;&#25928;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of systems of many degrees of freedom evolving on multiple scales are often modeled in terms of stochastic differential equations. Usually the structural form of these equations is unknown and the only manifestation of the system's dynamics are observations at discrete points in time. Despite their widespread use, accurately inferring these systems from sparse-in-time observations remains challenging. Conventional inference methods either focus on the temporal structure of observations, neglecting the geometry of the system's invariant density, or use geometric approximations of the invariant density, which are limited to conservative driving forces. To address these limitations, here, we introduce a novel approach that reconciles these two perspectives. We propose a path augmentation scheme that employs data-driven control to account for the geometry of the invariant system's density. Non-parametric inference on the augmented paths, enables efficient identification of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#23454;&#39564;&#30417;&#27979;&#38382;&#39064;&#21046;&#23450;&#20026;&#20855;&#26377;&#32479;&#19968;&#25928;&#29992;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#26368;&#20248;&#20915;&#31574;&#35268;&#21017;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#23458;&#25143;&#20307;&#39564;&#24182;&#25511;&#21046;&#26426;&#20250;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.00420</link><description>&lt;p&gt;
&#23454;&#39564;&#24179;&#21488;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65306;&#36125;&#21494;&#26031;&#24207;&#36143;&#20915;&#31574;&#22312;&#36830;&#32493;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Experimentation Platforms Meet Reinforcement Learning: Bayesian Sequential Decision-Making for Continuous Monitoring. (arXiv:2304.00420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#23454;&#39564;&#30417;&#27979;&#38382;&#39064;&#21046;&#23450;&#20026;&#20855;&#26377;&#32479;&#19968;&#25928;&#29992;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#26368;&#20248;&#20915;&#31574;&#35268;&#21017;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#23458;&#25143;&#20307;&#39564;&#24182;&#25511;&#21046;&#26426;&#20250;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;A/B&#27979;&#35797;&#25903;&#25345;&#24037;&#19994;&#21019;&#26032;&#30340;&#38656;&#27714;&#22686;&#38271;&#65292;&#23454;&#39564;&#36816;&#34892;&#30340;&#26426;&#20250;&#25104;&#26412;&#21464;&#24471;&#19981;&#21487;&#24573;&#30053;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#30417;&#27979;&#26381;&#21153;&#65292;&#20197;&#22312;&#36866;&#24403;&#26102;&#20505;&#36827;&#34892;&#26089;&#26399;&#20572;&#27490;&#12290;&#32463;&#20856;&#30340;&#32479;&#35745;&#26041;&#27861;&#30528;&#37325;&#20110;&#20551;&#35774;&#26816;&#39564;&#65292;&#20027;&#35201;&#26159;&#38024;&#23545;&#20256;&#32479;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#65292;&#22914;&#20020;&#24202;&#35797;&#39564;&#65292;&#32780;&#22312;&#32447;&#26381;&#21153;&#20844;&#21496;&#30340;&#23454;&#39564;&#36890;&#24120;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#29305;&#24449;&#21644;&#20851;&#27880;&#28857;&#12290;&#22312;&#23454;&#38469;&#38656;&#35201;&#30340;&#39537;&#21160;&#19979;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#24320;&#21457;&#30340;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#23458;&#25143;&#20307;&#39564;&#21644;&#25511;&#21046;&#26426;&#20250;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#20855;&#26377;&#32479;&#19968;&#25928;&#29992;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#35752;&#35770;&#20102;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#21644;&#32771;&#34385;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26368;&#20248;&#20915;&#31574;&#35268;&#21017;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing needs of online A/B testing to support the innovation in industry, the opportunity cost of running an experiment becomes non-negligible. Therefore, there is an increasing demand for an efficient continuous monitoring service that allows early stopping when appropriate. Classic statistical methods focus on hypothesis testing and are mostly developed for traditional high-stake problems such as clinical trials, while experiments at online service companies typically have very different features and focuses. Motivated by the real needs, in this paper, we introduce a novel framework that we developed in Amazon to maximize customer experience and control opportunity cost. We formulate the problem as a Bayesian optimal sequential decision making problem that has a unified utility function. We discuss extensively practical design choices and considerations. We further introduce how to solve the optimal decision rule via Reinforcement Learning and scale the solution. We show th
&lt;/p&gt;</description></item><item><title>&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#27861;&#34987;&#35777;&#26126;&#21487;&#20197;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#25910;&#25947;&#65292;&#26080;&#35770;&#21021;&#22987;&#32858;&#31867;&#20013;&#24515;&#22914;&#20309;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#25209;&#37327;&#29256;&#26412;&#30456;&#21516;&#30340;&#36924;&#36817;&#27604;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00419</link><description>&lt;p&gt;
&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#27861;&#22312;$O(d/\epsilon)$&#27425;&#36845;&#20195;&#20869;&#32456;&#27490;&#12290;(arXiv:2304.00419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations. (arXiv:2304.00419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00419
&lt;/p&gt;
&lt;p&gt;
&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#27861;&#34987;&#35777;&#26126;&#21487;&#20197;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#25910;&#25947;&#65292;&#26080;&#35770;&#21021;&#22987;&#32858;&#31867;&#20013;&#24515;&#22914;&#20309;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#25209;&#37327;&#29256;&#26412;&#30456;&#21516;&#30340;&#36924;&#36817;&#27604;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#8220;&#23567;&#25209;&#37327;$k$-means&#65288;Min-batch $K$-Means&#65289;&#30340;&#23616;&#37096;&#36827;&#23637;&#65288;&#22312;&#25209;&#22788;&#29702;&#19978;&#65289;&#26159;&#21542;&#24847;&#21619;&#30528;&#20840;&#23616;&#36827;&#23637;&#65288;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#65289;&#65311;&#8221;&#25105;&#20204;&#32771;&#34385;&#20102;&#20165;&#24403;&#22312;&#37319;&#26679;&#25209;&#22788;&#29702;&#30340;&#36136;&#37327;&#25913;&#36827;&#20302;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#25165;&#32456;&#27490;&#30340;&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#26041;&#27861;&#12290;&#23613;&#31649;&#20045;&#19968;&#30475;&#36825;&#20010;&#31639;&#27861;&#21487;&#33021;&#27704;&#36828;&#19981;&#20250;&#25191;&#34892;&#23436;&#65292;&#20294;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#65292;&#22914;&#26524;&#25209;&#27425;&#22823;&#23567;&#20026;$\tilde{\Omega}((d/\epsilon)^2)$&#65292;&#21017;&#23427;&#24517;&#39035;&#22312;$O(d/\epsilon)$&#27425;&#36845;&#20195;&#20869;&#20197;&#39640;&#27010;&#29575;&#32456;&#27490;&#65292;&#20854;&#20013;$d$&#26159;&#36755;&#20837;&#30340;&#32500;&#24230;&#65292;$\epsilon$&#26159;&#32456;&#27490;&#30340;&#38408;&#20540;&#21442;&#25968;&#12290;&#36825;&#19968;&#28857;&#26159;&#26377;&#36947;&#29702;&#30340;&#65292;&#26080;&#35770;&#20013;&#24515;&#22914;&#20309;&#21021;&#22987;&#21270;&#12290;&#24403;&#31639;&#27861;&#20351;&#29992;$k$-means++&#21021;&#22987;&#21270;&#26041;&#26696;&#21021;&#22987;&#21270;&#26102;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;$O(\log k)$&#65288;&#19982;&#23436;&#20840;&#25209;&#37327;&#29256;&#26412;&#30456;&#21516;&#65289;&#30340;&#36924;&#36817;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We answer the question: "Does local progress (on batches) imply global progress (on the entire dataset) for mini-batch $k$-means?". Specifically, we consider mini-batch $k$-means which terminates only when the improvement in the quality of the clustering on the sampled batch is below some threshold.  Although at first glance it appears that this algorithm might execute forever, we answer the above question in the affirmative and show that if the batch is of size $\tilde{\Omega}((d/\epsilon)^2)$, it must terminate within $O(d/\epsilon)$ iterations with high probability, where $d$ is the dimension of the input, and $\epsilon$ is a threshold parameter for termination. This is true regardless of how the centers are initialized. When the algorithm is initialized with the $k$-means++ initialization scheme, it achieves an approximation ratio of $O(\log k)$ (the same as the full-batch version).  Finally, we show the applicability of our results to the mini-batch $k$-means algorithm implemented
&lt;/p&gt;</description></item><item><title>SafeguardGPT&#26694;&#26550;&#20351;&#29992;&#24515;&#29702;&#27835;&#30103;&#26469;&#32416;&#27491;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65292;&#25913;&#36827;&#19982;&#20154;&#31867;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#36827;&#32780;&#25512;&#36827;&#20581;&#24247;AI&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00416</link><description>&lt;p&gt;
&#36808;&#21521;&#20581;&#24247;AI&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#38656;&#35201;&#27835;&#30103;&#24072;
&lt;/p&gt;
&lt;p&gt;
Towards Healthy AI: Large Language Models Need Therapists Too. (arXiv:2304.00416v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00416
&lt;/p&gt;
&lt;p&gt;
SafeguardGPT&#26694;&#26550;&#20351;&#29992;&#24515;&#29702;&#27835;&#30103;&#26469;&#32416;&#27491;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65292;&#25913;&#36827;&#19982;&#20154;&#31867;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#36827;&#32780;&#25512;&#36827;&#20581;&#24247;AI&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#21151;&#33021;&#24378;&#22823;&#30340; AI &#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#21442;&#19982;&#33258;&#28982;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#33021;&#20855;&#26377;&#28508;&#22312;&#30340;&#21361;&#23475;&#24615;&#65292;&#34920;&#29616;&#20986;&#25805;&#32437;&#12289;&#28748;&#36755;&#34394;&#20551;&#35266;&#24565;&#21644;&#33258;&#24651;&#34892;&#20026;&#12290;&#25105;&#20204;&#23450;&#20041;&#20581;&#24247;AI&#20026;&#23433;&#20840;&#12289;&#21487;&#20449;&#21644;&#36947;&#24503;&#30340;AI&#12290;&#20026;&#20102;&#21019;&#36896;&#20581;&#24247;&#30340;AI&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SafeguardGPT&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#24515;&#29702;&#27835;&#30103;&#26469;&#32416;&#27491;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#36825;&#20123;&#26377;&#23475;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#22235;&#31181;&#31867;&#22411;&#30340;AI&#20195;&#29702;&#65306;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;"&#29992;&#25143;"&#12289;"&#27835;&#30103;&#24072;"&#21644;"&#35780;&#35770;&#23478;"&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#31038;&#20132;&#23545;&#35805;&#30340;&#24037;&#20316;&#31034;&#20363;&#23637;&#31034;&#20102;SafeguardGPT&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25913;&#36827;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#23545;&#35805;&#36136;&#37327;&#12290;&#34429;&#28982;&#26410;&#26469;&#20173;&#38656;&#35299;&#20915;&#20960;&#20010;&#25361;&#25112;&#21644;&#26041;&#21521;&#65292;&#20294;SafeguardGPT&#20026;&#25913;&#21892;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#30340;&#21327;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have led to the development of powerful AI chatbots capable of engaging in natural and human-like conversations. However, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to be safe, trustworthy and ethical. To create healthy AI systems, we present the SafeguardGPT framework that uses psychotherapy to correct for these harmful behaviors in AI chatbots. The framework involves four types of AI agents: a Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the effectiveness of SafeguardGPT through a working example of simulating a social conversation. Our results show that the framework can improve the quality of conversations between AI chatbots and humans. Although there are still several challenges and directions to be addressed in the future, SafeguardGPT provides a promising approach to improving the alignment between AI chatbots and human value
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;CWE&#30340;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#20316;&#32773;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#25913;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#32467;&#21512;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#21069;&#36884;&#30340;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#28431;&#27934;&#20173;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00409</link><description>&lt;p&gt;
DiverseVul: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#30340;&#26032;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection. (arXiv:2304.00409v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;CWE&#30340;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#20316;&#32773;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#25913;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#32467;&#21512;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#21069;&#36884;&#30340;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#28431;&#27934;&#20173;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#29228;&#21462;&#23433;&#20840;&#38382;&#39064;&#32593;&#31449;&#65292;&#25552;&#21462;&#30456;&#24212;&#39033;&#30446;&#30340;&#28431;&#27934;&#20462;&#22797;&#25552;&#20132;&#21644;&#28304;&#20195;&#30721;&#65292;&#31579;&#36873;&#20986;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;150&#20010;CWE&#65292;26,635&#20010;&#26131;&#21463;&#25915;&#20987;&#30340;&#20989;&#25968;&#21644;352,606&#20010;&#19981;&#26131;&#21463;&#25915;&#20987;&#30340;&#20989;&#25968;&#65292;&#25552;&#21462;&#33258;7,861&#20010;&#25552;&#20132;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35206;&#30422;&#20102;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;11&#20010;&#27169;&#22411;&#26550;&#26500;&#65292;&#23646;&#20110;4&#20010;&#23478;&#26063;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#65292;&#28145;&#24230;&#23398;&#20064;&#20173;&#26410;&#20934;&#22791;&#22909;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;......
&lt;/p&gt;
&lt;p&gt;
We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerable functions extracted from 7,861 commits. Our dataset covers 305 more projects than all previous datasets combined. We show that increasing the diversity and volume of training data improves the performance of deep learning models for vulnerability detection.  Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#20154;&#31867;&#39550;&#39542;&#21592;&#34892;&#20026;&#20197;&#23454;&#29616;&#20986;&#20837;&#21277;&#36947;&#30340;&#26377;&#25928;&#21512;&#24182;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;CAV&#21644;HDV&#20114;&#21160;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#28151;&#21512;&#20132;&#36890;&#26465;&#20214;&#19979;&#23454;&#29616;&#23433;&#20840;&#19988;&#39640;&#25928;&#30340;&#21512;&#24182;&#12290;</title><link>http://arxiv.org/abs/2304.00397</link><description>&lt;p&gt;
&#28151;&#21512;&#20132;&#36890;&#20013;&#30340;&#36830;&#25509;&#33258;&#21160;&#27773;&#36710;&#65306;&#23398;&#20064;&#20154;&#31867;&#39550;&#39542;&#21592;&#34892;&#20026;&#20197;&#23454;&#29616;&#20986;&#20837;&#21277;&#36947;&#30340;&#26377;&#25928;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Connected and Automated Vehicles in Mixed-Traffic: Learning Human Driver Behavior for Effective On-Ramp Merging. (arXiv:2304.00397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#20154;&#31867;&#39550;&#39542;&#21592;&#34892;&#20026;&#20197;&#23454;&#29616;&#20986;&#20837;&#21277;&#36947;&#30340;&#26377;&#25928;&#21512;&#24182;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;CAV&#21644;HDV&#20114;&#21160;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#28151;&#21512;&#20132;&#36890;&#26465;&#20214;&#19979;&#23454;&#29616;&#23433;&#20840;&#19988;&#39640;&#25928;&#30340;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20132;&#36890;&#26465;&#20214;&#19979;&#30340;&#20844;&#36335;&#21512;&#27969;&#22330;&#26223;&#23545;&#20110;&#19982;&#36827;&#20837;&#20986;&#20837;&#21475;&#30340;&#20154;&#31867;&#39550;&#39542;&#36710;&#36742;&#65288;HDV&#65289;&#20132;&#20114;&#30340;&#36830;&#25509;&#33258;&#21160;&#27773;&#36710;&#65288;CAV&#65289;&#32780;&#35328;&#65292;&#23384;&#22312;&#30528;&#26174;&#30528;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;CAV&#21644;HDV&#20114;&#21160;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#27169;&#22411;&#65292;&#20351;CAV&#22312;&#20844;&#36335;&#21512;&#27969;&#26102;&#23433;&#20840;&#39550;&#39542;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;CAV&#20250;&#22312;&#29983;&#25104;&#25511;&#21046;&#31574;&#30053;&#26469;&#20419;&#36827;&#21512;&#24182;&#20043;&#21069;&#65292;&#20351;&#29992;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#23398;&#20064;&#36827;&#20837;HDV&#30340;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#39564;&#35777;&#20102;&#36825;&#31181;&#26694;&#26550;&#30340;&#21151;&#25928;&#65292;&#36890;&#36807;&#22312;&#20174;Next-Generation Simulation&#23384;&#20648;&#24211;&#20013;&#25552;&#21462;&#30340;&#28151;&#21512;&#20132;&#36890;&#24773;&#20917;&#19979;&#39044;&#27979;HDV&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#21453;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20026;&#20844;&#36335;&#21512;&#27969;&#22330;&#26223;&#29983;&#25104;HDV-CAV&#20132;&#20114;&#30340;&#27169;&#25311;&#25968;&#25454;&#12290;&#22312;&#19981;&#20551;&#35774;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#27169;&#22411;&#20351;&#24471;&#22312;&#28151;&#21512;&#20132;&#36890;&#26465;&#20214;&#19979;&#23454;&#29616;&#23433;&#20840;&#19988;&#39640;&#25928;&#30340;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Highway merging scenarios featuring mixed traffic conditions pose significant modeling and control challenges for connected and automated vehicles (CAVs) interacting with incoming on-ramp human-driven vehicles (HDVs). In this paper, we present an approach to learn an approximate information state model of CAV-HDV interactions for a CAV to maneuver safely during highway merging. In our approach, the CAV learns the behavior of an incoming HDV using approximate information states before generating a control strategy to facilitate merging. First, we validate the efficacy of this framework on real-world data by using it to predict the behavior of an HDV in mixed traffic situations extracted from the Next-Generation Simulation repository. Then, we generate simulation data for HDV-CAV interactions in a highway merging scenario using a standard inverse reinforcement learning approach. Without assuming a prior knowledge of the generating model, we show that our approximate information state mod
&lt;/p&gt;</description></item><item><title>&#26080;&#26381;&#21153;&#22120;&#20113;&#30340;&#21019;&#26032;&#27169;&#24335;&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#21183;&#65292;&#20294;&#38754;&#20020;&#30528;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#25552;&#20986;&#30340;&#31574;&#30053;&#21487;&#20197;&#22312;&#22522;&#30784;&#32467;&#26500;&#21644;&#21151;&#33021;&#23618;&#38754;&#19978;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00396</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#31649;&#29702;&#26080;&#26381;&#21153;&#22120;&#20113;&#20013;&#30340;&#20919;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
Managing Cold-start in The Serverless Cloud with Temporal Convolutional Networks. (arXiv:2304.00396v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00396
&lt;/p&gt;
&lt;p&gt;
&#26080;&#26381;&#21153;&#22120;&#20113;&#30340;&#21019;&#26032;&#27169;&#24335;&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#21183;&#65292;&#20294;&#38754;&#20020;&#30528;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#25552;&#20986;&#30340;&#31574;&#30053;&#21487;&#20197;&#22312;&#22522;&#30784;&#32467;&#26500;&#21644;&#21151;&#33021;&#23618;&#38754;&#19978;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26381;&#21153;&#22120;&#20113;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#20113;&#26381;&#21153;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#23458;&#25143;&#20813;&#38500;&#22823;&#37096;&#20998;&#20113;&#31649;&#29702;&#32844;&#36131;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#20113;&#27169;&#22411;&#30456;&#21516;&#30340;&#20248;&#21183;&#65292;&#20294;&#25104;&#26412;&#35201;&#20302;&#24471;&#22810;&#12290;&#22240;&#27492;&#65292;&#26080;&#26381;&#21153;&#22120;&#20113;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#31995;&#32479;&#23433;&#20840;&#12289;&#38134;&#34892;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#24433;&#21709;&#39046;&#22495;&#12290;&#26080;&#26381;&#21153;&#22120;&#20113;&#24615;&#33021;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#22823;&#23041;&#32961;&#26159;&#20919;&#21551;&#21160;&#65292;&#24403;&#20026;&#20102;&#20026;&#23458;&#25143;&#35831;&#27714;&#25552;&#20379;&#25152;&#38656;&#30340;&#20113;&#36164;&#28304;&#30340;&#26102;&#38388;&#36896;&#25104;&#30340;&#25104;&#26412;&#23545;&#20110;&#26381;&#21153;&#25552;&#20379;&#21830;&#21644;/&#25110;&#23458;&#25143;&#26469;&#35828;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20302;&#32806;&#21512;&#12289;&#39640;&#20869;&#32858;&#30340;&#38598;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#26080;&#26381;&#21153;&#22120;&#20113;&#26632;&#30340;&#22522;&#30784;&#32467;&#26500;&#21644;&#21151;&#33021;&#23618;&#38754;&#19978;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#32780;&#29616;&#26377;&#25216;&#26415;&#30340;&#31574;&#30053;&#21017;&#26356;&#21152;&#29421;&#31364;&#12290;
&lt;/p&gt;
&lt;p&gt;
Serverless cloud is an innovative cloud service model that frees customers from most cloud management duties. It also offers the same advantages as other cloud models but at much lower costs. As a result, the serverless cloud has been increasingly employed in high-impact areas such as system security, banking, and health care. A big threat to the serverless cloud's performance is cold-start, which is when the time of provisioning the needed cloud resource to serve customers' requests incurs unacceptable costs to the service providers and/or the customers. This paper proposes a novel low-coupling, high-cohesion ensemble policy that addresses the cold-start problem at infrastructure- and function-levels of the serverless cloud stack, while the state of the art policies have a more narrowed focus. This ensemble policy anchors on the prediction of function instance arrivals, 10 to 15 minutes into the future. It is achievable by using the temporal convolutional network (TCN) deep-learning m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#65292;&#26412;&#25991;&#29702;&#35770;&#20998;&#26512;&#20102;&#22522;&#20110;&#26680;&#30340;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#30340;&#29305;&#28857;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#25551;&#36848;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#21644;&#34920;&#29616;&#65292;&#25552;&#20379;&#19968;&#20010;&#26032;&#30340;&#38480;&#21046;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00395</link><description>&lt;p&gt;
&#36890;&#36807;&#30456;&#20284;&#24615;&#32467;&#26500;&#35299;&#26512;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65306;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Mechanism of Contrastive Learning via Similarity Structure: A Theoretical Analysis. (arXiv:2304.00395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00395
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#65292;&#26412;&#25991;&#29702;&#35770;&#20998;&#26512;&#20102;&#22522;&#20110;&#26680;&#30340;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#30340;&#29305;&#28857;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#25551;&#36848;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#21644;&#34920;&#29616;&#65292;&#25552;&#20379;&#19968;&#20010;&#26032;&#30340;&#38480;&#21046;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#36817;&#26399;&#30340;&#30740;&#31350;&#22312;&#29702;&#35770;&#19978;&#23545;&#23545;&#27604;&#23398;&#20064;&#26377;&#20102;&#19968;&#23450;&#30340;&#20102;&#35299;&#65292;&#20294;&#23545;&#20110;&#22914;&#20309;&#34920;&#24449;&#23398;&#20064;&#34920;&#31034;&#30340;&#32858;&#31867;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#29702;&#35770;&#35282;&#24230;&#38416;&#26126;&#36825;&#31181;&#32858;&#31867;&#30340;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#26680;&#23545;&#27604;&#23398;&#20064;&#65288;KCL&#65289;&#65292;&#26680;&#20989;&#25968;&#22312;&#23558;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24212;&#29992;&#20110;&#20854;&#20182;&#26694;&#26550;&#26102;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#32479;&#35745;&#20381;&#36182;&#35266;&#28857;&#24341;&#20837;&#19968;&#20010;&#23398;&#20064;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#32467;&#26500;&#30340;&#20844;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#20010;&#20844;&#24335;&#30740;&#31350;&#20102;&#22522;&#20110;&#26680;&#30340;&#23545;&#27604;&#25439;&#22833;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#36825;&#20010;&#20844;&#24335;&#34920;&#24449;&#20102;&#21033;&#29992;&#26680;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#23398;&#20064;&#30340;&#34920;&#31034;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#65292;&#23545;&#20110;&#36127;&#26679;&#26412;&#30340;&#36793;&#38469;&#20998;&#24067;&#26377;&#19968;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#65292;&#26399;&#26395;&#23545;&#27604;&#25439;&#22833;&#21463;&#21040;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#22522;&#20110;&#26680;&#30340;&#23545;&#27604;&#25439;&#22833;&#26159;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#19979;&#30028;&#30340;&#29305;&#20363;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#32422;&#26463;&#23398;&#20064;&#34920;&#31034;&#22312;&#36755;&#20837;&#30340;&#20146;&#23494;&#24615;&#21644;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#21487;&#20998;&#24615;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#36825;&#20123;&#23454;&#39564;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is an efficient approach to self-supervised representation learning. Although recent studies have made progress in the theoretical understanding of contrastive learning, the investigation of how to characterize the clusters of the learned representations is still limited. In this paper, we aim to elucidate the characterization from theoretical perspectives. To this end, we consider a kernel-based contrastive learning framework termed Kernel Contrastive Learning (KCL), where kernel functions play an important role when applying our theoretical results to other frameworks. We introduce a formulation of the similarity structure of learned representations by utilizing a statistical dependency viewpoint. We investigate the theoretical properties of the kernel-based contrastive loss via this formulation. We first prove that the formulation characterizes the structure of representations learned with the kernel-based contrastive learning framework. We show a new upper boun
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#25968;&#20540;&#35299;&#20915;&#21442;&#25968;&#21270;PDEs&#30340;&#22810;&#32423;CNN&#26041;&#27861;&#65292;&#26377;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#24182;&#33021;&#20197;&#20219;&#24847;&#31934;&#24230;&#36817;&#20284;&#22810;&#37325;&#32593;&#26684;V&#24490;&#29615;&#12290;</title><link>http://arxiv.org/abs/2304.00388</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;PDE&#30340;&#22810;&#32423;CNN
&lt;/p&gt;
&lt;p&gt;
Multilevel CNNs for Parametric PDEs. (arXiv:2304.00388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#25968;&#20540;&#35299;&#20915;&#21442;&#25968;&#21270;PDEs&#30340;&#22810;&#32423;CNN&#26041;&#27861;&#65292;&#26377;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#24182;&#33021;&#20197;&#20219;&#24847;&#31934;&#24230;&#36817;&#20284;&#22810;&#37325;&#32593;&#26684;V&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#22810;&#32423;&#27714;&#35299;&#22120;&#30340;&#27010;&#24565;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#39640;&#32500;&#21442;&#25968;PDEs&#30340;&#26377;&#25928;&#25968;&#20540;&#26041;&#27861;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20197;&#20219;&#24847;&#31934;&#24230;&#36817;&#20284;&#22810;&#37325;&#32593;&#26684;V&#24490;&#29615;&#65292;&#20854;&#26435;&#37325;&#25968;&#37327;&#20165;&#19982;&#26368;&#32454;&#32593;&#26684;&#30340;&#20998;&#36776;&#29575;&#23545;&#25968;&#26377;&#20851;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We combine concepts from multilevel solvers for partial differential equations (PDEs) with neural network based deep learning and propose a new methodology for the efficient numerical solution of high-dimensional parametric PDEs. An in-depth theoretical analysis shows that the proposed architecture is able to approximate multigrid V-cycles to arbitrary precision with the number of weights only depending logarithmically on the resolution of the finest mesh. As a consequence, approximation bounds for the solution of parametric PDEs by neural networks that are independent on the (stochastic) parameter dimension can be derived. The performance of the proposed method is illustrated on high-dimensional parametric linear elliptic PDEs that are common benchmark problems in uncertainty quantification. We find substantial improvements over state-of-the-art deep learning-based solvers. As particularly challenging examples, random conductivity with high-dimensional non-affine Gaussian fields in 10
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ChatRepair&#30340;&#26032;&#22411;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#8220;&#29983;&#25104;&#21644;&#39564;&#35777;&#8221;&#33539;&#24335;&#19981;&#21516;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23545;&#35805;&#39118;&#26684;&#23454;&#29616;&#21363;&#26102;&#21453;&#39304;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#28431;&#27934;&#20462;&#22797;&#30340;&#25928;&#29575;&#21644;&#34917;&#19969;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00385</link><description>&lt;p&gt;
&#35753;&#23545;&#35805;&#32487;&#32493;&#65306;&#20351;&#29992;ChatGPT&#20165;&#20197;0.42&#32654;&#20803;&#30340;&#20215;&#26684;&#20462;&#22797;&#20102;337&#20010;&#28431;&#27934;&#20013;&#30340;162&#20010;
&lt;/p&gt;
&lt;p&gt;
Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. (arXiv:2304.00385v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ChatRepair&#30340;&#26032;&#22411;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#8220;&#29983;&#25104;&#21644;&#39564;&#35777;&#8221;&#33539;&#24335;&#19981;&#21516;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23545;&#35805;&#39118;&#26684;&#23454;&#29616;&#21363;&#26102;&#21453;&#39304;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#28431;&#27934;&#20462;&#22797;&#30340;&#25928;&#29575;&#21644;&#34917;&#19969;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#26377;&#20851;&#26377;&#28431;&#27934;&#31243;&#24207;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;&#26368;&#36817;&#30340;APR&#24037;&#20316;&#38598;&#20013;&#20110;&#21033;&#29992;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#29983;&#25104;APR&#30340;&#34917;&#19969;&#12290;&#36825;&#31181;&#22522;&#20110;LLM&#30340;APR&#24037;&#20855;&#30340;&#24037;&#20316;&#26041;&#27861;&#26159;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#30001;&#21407;&#22987;&#26377;&#28431;&#27934;&#20195;&#30721;&#26500;&#24314;&#30340;&#36755;&#20837;&#25552;&#31034;&#65292;&#28982;&#21518;&#26597;&#35810;LLM&#29983;&#25104;&#34917;&#19969;&#12290;&#34429;&#28982;&#22522;&#20110;LLM&#30340;APR&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20173;&#28982;&#36981;&#24490;&#8220;&#29983;&#25104;&#21644;&#39564;&#35777;&#8221;&#20462;&#22797;&#33539;&#24335;&#65292;&#21363;&#39318;&#20808;&#29983;&#25104;&#22823;&#37327;&#30340;&#34917;&#19969;&#65292;&#28982;&#21518;&#36880;&#20010;&#39564;&#35777;&#27599;&#20010;&#34917;&#19969;&#12290;&#36825;&#19981;&#20165;&#20250;&#23548;&#33268;&#35768;&#22810;&#37325;&#22797;&#30340;&#19981;&#27491;&#30830;&#30340;&#34917;&#19969;&#65292;&#32780;&#19988;&#36824;&#20250;&#38169;&#36807;&#27979;&#35797;&#22833;&#36133;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#20197;&#21450;&#21487;&#34892;&#30340;&#34917;&#19969;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatRepair&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#23545;&#35805;&#39537;&#21160;&#30340;APR&#26041;&#27861;&#65292;&#23427;&#23558;&#34917;&#19969;&#29983;&#25104;&#19982;&#21363;&#26102;&#21453;&#39304;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#20197;&#23545;&#35805;&#39118;&#26684;&#25191;&#34892;APR&#12290;ChatRepair&#39318;&#20808;&#23558;&#30456;&#20851;&#30340;&#27979;&#35797;&#22833;&#36133;&#20449;&#24687;&#39304;&#20837;LLM&#20013;&#65292;&#28982;&#21518;&#22312;&#34917;&#19969;&#29983;&#25104;&#36807;&#31243;&#20013;&#20351;&#29992;&#20132;&#20114;&#24335;&#23545;&#35805;&#65292;&#20197;&#38598;&#20013;&#26041;&#24335;&#29983;&#25104;&#34917;&#19969;&#12290;&#27492;&#22806;&#65292;ChatRepair&#36824;&#21033;&#29992;&#20102;&#27979;&#35797;&#32467;&#26524;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#34917;&#19969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then queries the LLM to generate patches. While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches.  To address these limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#22797;&#21512;&#20960;&#20309;&#21464;&#25442;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;CompoundE3D&#65292;&#22312;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.00378</link><description>&lt;p&gt;
3D&#22797;&#21512;&#20960;&#20309;&#21464;&#25442;&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding with 3D Compound Geometric Transformations. (arXiv:2304.00378v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#22797;&#21512;&#20960;&#20309;&#21464;&#25442;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;CompoundE3D&#65292;&#22312;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CompoundE3D&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#21253;&#25324;&#24179;&#31227;&#12289;&#26059;&#36716;&#12289;&#32553;&#25918;&#12289;&#21453;&#23556;&#21644;&#21098;&#20999;&#22312;&#20869;&#30340;3D&#22797;&#21512;&#20960;&#20309;&#21464;&#25442;&#12290;CompoundE3D&#20801;&#35768;&#22810;&#20010;&#35774;&#35745;&#21464;&#20307;&#20197;&#21305;&#37197;&#30693;&#35782;&#22270;&#35889;&#30340;&#24213;&#23618;&#29305;&#24449;&#65292;&#24182;&#33021;&#20135;&#29983;&#36229;&#20986;&#21333;&#20010;&#21464;&#20307;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#38142;&#25509;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cascade of 2D geometric transformations were exploited to model relations between entities in a knowledge graph (KG), leading to an effective KG embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was proposed as a new KGE model, Rotate3D, by leveraging its non-commutative property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear and propose a family of KGE models, named CompoundE3D, in this work. CompoundE3D allows multiple design variants to match rich underlying characteristics of a KG. Since each variant has its own advantages on a subset of relations, an ensemble of multiple variants can yield superior performance. The effectiveness and flexibility of CompoundE3D are experimentally verified on four popular link prediction datasets.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35843;&#26597;&#20102;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#23558;&#20854;&#20998;&#20026;&#19971;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#35843;&#26597;&#25991;&#29486;&#30340;&#32479;&#35745;&#20803;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.00377</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24773;&#24863;&#35745;&#31639;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Personalized Affective Computing in Human-Machine Interaction. (arXiv:2304.00377v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00377
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35843;&#26597;&#20102;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#23558;&#20854;&#20998;&#20026;&#19971;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#35843;&#26597;&#25991;&#29486;&#30340;&#32479;&#35745;&#20803;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#39046;&#22495;&#20013;&#65292;&#20010;&#24615;&#21270;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#25110;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#24182;&#36981;&#23432;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#26469;&#35757;&#32451;&#36814;&#21512;&#29305;&#23450;&#20010;&#20154;&#25110;&#20154;&#32676;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#24773;&#24863;&#21644;&#20154;&#26684;&#35745;&#31639;&#65288;&#20197;&#19979;&#31616;&#31216;&#24773;&#24863;&#35745;&#31639;&#65289;&#20013;&#20010;&#24615;&#21270;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#23545;&#24773;&#24863;&#35745;&#31639;&#20013;&#20010;&#24615;&#21270;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#35757;&#32451;&#25216;&#26415;&#21644;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#35745;&#31639;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#23450;&#21046;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#26041;&#27861;&#20998;&#20026;&#19971;&#31867;&#65306;&#65288;1&#65289;&#38754;&#21521;&#29305;&#23450;&#30446;&#26631;&#30340;&#27169;&#22411;&#65292;&#65288;2&#65289;&#38754;&#21521;&#29305;&#23450;&#32676;&#20307;&#30340;&#27169;&#22411;&#65292;&#65288;3&#65289;&#22522;&#20110;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#65288;4&#65289;&#24494;&#35843;&#26041;&#27861;&#65292;&#65288;5&#65289;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#65288;6&#65289;&#29983;&#25104;&#24335;&#27169;&#22411;&#21644;&#65288;7&#65289;&#29305;&#24449;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#35843;&#26597;&#25991;&#29486;&#30340;&#32479;&#35745;&#20803;&#20998;&#26512;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#24773;&#24863;&#35745;&#31639;&#20219;&#21153;&#12289;&#20132;&#20114;&#27169;&#24335;&#12289;&#20132;&#20114;&#19978;&#19979;&#25991;&#20197;&#21450;&#25152;&#28041;&#21450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computing, the aim of personalization is to train a model that caters to a specific individual or group of people by optimizing one or more performance metrics and adhering to specific constraints. In this paper, we discuss the need for personalization in affective and personality computing (hereinafter referred to as affective computing). We present a survey of state-of-the-art approaches for personalization in affective computing. Our review spans training techniques and objectives towards the personalization of affective computing models. We group existing approaches into seven categories: (1) Target-specific Models, (2) Group-specific Models, (3) Weighting-based Approaches, (4) Fine-tuning Approaches, (5) Multitask Learning, (6) Generative-based Models, and (7) Feature Augmentation. Additionally, we provide a statistical meta-analysis of the surveyed literature, analyzing the prevalence of different affective computing tasks, interaction modes, interaction contexts, and the leve
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#27169;&#25311;&#31227;&#21160;&#33655;&#36733;&#38382;&#39064;&#65292;&#20854;&#20013;&#20351;&#29992;&#39640;&#26031;&#20989;&#25968;&#26469;&#36924;&#36817;Dirac delta&#20989;&#25968;&#20197;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00369</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#31227;&#21160;&#33655;&#36733;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning for moving load problems. (arXiv:2304.00369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00369
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#27169;&#25311;&#31227;&#21160;&#33655;&#36733;&#38382;&#39064;&#65292;&#20854;&#20013;&#20351;&#29992;&#39640;&#26031;&#20989;&#25968;&#26469;&#36924;&#36817;Dirac delta&#20989;&#25968;&#20197;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;(PIML)&#26469;&#27169;&#25311;&#31227;&#21160;&#33655;&#36733;&#30340;&#27491;&#36870;&#38382;&#39064;&#12290;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#21033;&#29992;&#31227;&#21160;&#33655;&#36733;&#38382;&#39064;&#30340;&#22522;&#30784;&#29289;&#29702;&#23398;&#65292;&#26088;&#22312;&#39044;&#27979;&#26753;&#30340;&#20559;&#36716;&#21644;&#33655;&#36733;&#22823;&#23567;&#12290;&#32771;&#34385;&#21040;&#33655;&#36733;&#31227;&#21160;&#30340;&#25968;&#23398;&#34920;&#31034;&#21253;&#25324;&#19968;&#20010;Dirac delta&#20989;&#25968;&#65292;&#20197;&#25429;&#33719;&#33655;&#36733;&#27178;&#36328;&#32467;&#26500;&#30340;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#39640;&#26031;&#20989;&#25968;&#26469;&#36924;&#36817;Dirac delta&#20989;&#25968;&#12290;&#23558;&#20869;&#21547;&#39640;&#26031;&#20989;&#25968;&#30340;&#29289;&#29702;&#26041;&#31243;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32467;&#26500;&#65292;&#21487;&#27169;&#25311;&#26753;&#30340;&#20559;&#36716;&#24182;&#39044;&#27979;&#33655;&#36733;&#22823;&#23567;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;PIML&#26159;&#27169;&#25311;&#31227;&#21160;&#33655;&#36733;&#27491;&#38382;&#39064;&#21644;&#36870;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#19988;&#20351;&#29992;&#39640;&#26031;&#20989;&#25968;&#36924;&#36817;Dirac delta&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new approach to simulate forward and inverse problems of moving loads using physics-informed machine learning (PIML). Physics-informed neural networks (PINNs) utilize the underlying physics of moving load problems and aim to predict the deflection of beams and the magnitude of the loads. The mathematical representation of the moving load considered in this work involves a Dirac delta function, to capture the effect of the load moving across the structure. Approximating the Dirac delta function with PINNs is challenging because of its instantaneous change of output at a single point, causing difficulty in the convergence of the loss function. We propose to approximate the Dirac delta function with a Gaussian function. The incorporated Gaussian function physical equations are used in the physics-informed neural architecture to simulate beam deflections and to predict the magnitude of the load. Numerical results show that PIML is an effective method for simulating th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30340;&#33258;&#36866;&#24212;&#22833;&#25928;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#21457;&#29616;&#33258;&#20027;&#31574;&#30053;&#22312;&#20223;&#30495;&#20013;&#30340;&#22833;&#25928;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2304.00365</link><description>&lt;p&gt;
&#39046;&#22495;&#19987;&#23478;&#20851;&#38190;&#29366;&#24577;&#33258;&#36866;&#24212;&#22833;&#25928;&#25628;&#32034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Failure Search Using Critical States from Domain Experts. (arXiv:2304.00365v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30340;&#33258;&#36866;&#24212;&#22833;&#25928;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#21457;&#29616;&#33258;&#20027;&#31574;&#30053;&#22312;&#20223;&#30495;&#20013;&#30340;&#22833;&#25928;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28508;&#22312;&#25925;&#38556;&#26159;&#39564;&#35777;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#30001;&#20110;&#25925;&#38556;&#20107;&#20214;&#30340;&#31232;&#23569;&#24615;&#65292;&#20351;&#29992;&#38543;&#26426;&#25628;&#32034;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#25214;&#21040;&#28508;&#22312;&#30340;&#31995;&#32479;&#24369;&#28857;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25628;&#32034;&#25216;&#26415;&#26469;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#21457;&#29616;&#33258;&#20027;&#31574;&#30053;&#22312;&#20223;&#30495;&#20013;&#30340;&#22833;&#25928;&#36712;&#36857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30340;&#33258;&#36866;&#24212;&#22833;&#25928;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncovering potential failure cases is a crucial step in the validation of safety critical systems such as autonomous vehicles. Failure search may be done through logging substantial vehicle miles in either simulation or real world testing. Due to the sparsity of failure events, naive random search approaches require significant amounts of vehicle operation hours to find potential system weaknesses. As a result, adaptive searching techniques have been proposed to efficiently explore and uncover failure trajectories of an autonomous policy in simulation. Adaptive Stress Testing (AST) is one such method that poses the problem of failure search as a Markov decision process and uses reinforcement learning techniques to find high probability failures. However, this formulation requires a probability model for the actions of all agents in the environment. In systems where the environment actions are discrete and dependencies among agents exist, it may be infeasible to fully characterize the d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00354</link><description>&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Context Distribution Shift in Task Representation Learning for Offline Meta RL. (arXiv:2304.00354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;OMRL&#65289;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#20419;&#36827;&#26032;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;RL&#37319;&#29992;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#25512;&#26029;&#20219;&#21153;&#34920;&#31034;&#26469;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#28982;&#21518;&#26681;&#25454;&#25512;&#26029;&#20986;&#30340;&#20219;&#21153;&#34920;&#31034;&#35843;&#25972;&#34892;&#21160;&#31574;&#30053;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;OMRL&#65292;&#29305;&#21035;&#26159;OMRL&#20013;&#30340;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#21487;&#33021;&#20250;&#36973;&#21463;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#20351;&#29992;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#20581;&#30340;&#20219;&#21153;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#12290;&#22522;&#20110;&#19981;&#21516;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#21033;&#29992;&#23548;&#33268;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#32047;&#31215;&#22238;&#25253;&#27604;&#22522;&#20934;&#26041;&#27861;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline meta reinforcement learning (OMRL) aims to learn transferrable knowledge from offline datasets to facilitate the learning process for new target tasks. Context-based RL employs a context encoder to rapidly adapt the agent to new tasks by inferring about the task representation, and then adjusting the acting policy based on the inferred task representation. Here we consider context-based OMRL, in particular, the issue of task representation learning for OMRL. We empirically demonstrate that the context encoder trained on offline datasets could suffer from distribution shift between the contexts used for training and testing. To tackle this issue, we propose a hard sampling based strategy for learning a robust task context encoder. Experimental results, based on distinct continuous control tasks, demonstrate that the utilization of our technique results in more robust task representations and better testing performance in terms of accumulated returns, compared with baseline metho
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39046;&#22495;&#20998;&#35299;&#30340;&#26041;&#27861;&#25193;&#23637;&#32593;&#26684;&#19978;&#30340;MeshGraphNets&#65292;&#20102;&#35299;&#20102;&#22914;&#20309;&#35757;&#32451;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#65292;&#36827;&#32780;&#29983;&#25104;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#39640;&#38454;&#25968;&#20540;&#31215;&#20998;&#26469;&#22686;&#24378;&#35813;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.00338</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#21487;&#25193;&#23637;&#30340;&#32593;&#26684;&#29289;&#29702;&#23398;&#20195;&#29702;&#30340;&#31185;&#23398;&#35745;&#31639;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scientific Computing Algorithms to Learn Enhanced Scalable Surrogates for Mesh Physics. (arXiv:2304.00338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00338
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#20998;&#35299;&#30340;&#26041;&#27861;&#25193;&#23637;&#32593;&#26684;&#19978;&#30340;MeshGraphNets&#65292;&#20102;&#35299;&#20102;&#22914;&#20309;&#35757;&#32451;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#65292;&#36827;&#32780;&#29983;&#25104;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#39640;&#38454;&#25968;&#20540;&#31215;&#20998;&#26469;&#22686;&#24378;&#35813;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#29992;&#20110;&#30740;&#31350;&#22823;&#35268;&#27169;&#29289;&#29702;&#38382;&#39064;&#30340;&#24555;&#36895;&#20195;&#29702;&#12290;&#20854;&#20013;&#65292;&#25805;&#20316;&#22522;&#20110;&#32593;&#26684;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#29702;&#24819;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20419;&#36827;&#29289;&#29702;&#24544;&#23454;&#24230;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#30828;&#20214;&#38480;&#21046;&#38459;&#27490;&#20102;&#23427;&#20204;&#22312;&#22823;&#22411;&#35745;&#31639;&#22495;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;3D&#32593;&#26684;&#19978;&#22521;&#35757;&#19968;&#31867;GNN&#20195;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#39046;&#22495;&#20998;&#35299;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;MeshGraphNets&#65288;MGN&#65289;&#65288;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32593;&#26684;&#30340;&#29289;&#29702;&#24314;&#27169;&#30340;GNN&#23376;&#31867;&#65289;&#65292;&#20197;&#20415;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#25968;&#23398;&#19978;&#31561;&#21516;&#20110;&#22312;&#25972;&#20010;&#22495;&#19978;&#36827;&#34892;&#22521;&#35757;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#33410;&#28857;&#30340;&#32593;&#26684;&#19978;&#23545;MGN&#36827;&#34892;&#22521;&#35757;&#65292;&#29983;&#25104;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#27169;&#25311;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#39640;&#38454;&#25968;&#20540;&#31215;&#20998;&#26469;&#22686;&#24378;MGN&#65292;&#20174;&#32780;&#33021;&#22815;&#20943;&#23569;MGN&#30340;&#35823;&#24046;&#21644;&#22521;&#35757;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38468;&#24102;&#30340;&#19977;&#32500;&#32593;&#26684;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven modeling approaches can produce fast surrogates to study large-scale physics problems. Among them, graph neural networks (GNNs) that operate on mesh-based data are desirable because they possess inductive biases that promote physical faithfulness, but hardware limitations have precluded their application to large computational domains. We show that it is \textit{possible} to train a class of GNN surrogates on 3D meshes. We scale MeshGraphNets (MGN), a subclass of GNNs for mesh-based physics modeling, via our domain decomposition approach to facilitate training that is mathematically equivalent to training on the whole domain under certain conditions. With this, we were able to train MGN on meshes with \textit{millions} of nodes to generate computational fluid dynamics (CFD) simulations. Furthermore, we show how to enhance MGN via higher-order numerical integration, which can reduce MGN's error and training time. We validated our methods on an accompanying dataset of 3D $\te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#20559;&#26631;&#31614;&#22122;&#22768;&#30340;&#38544;&#21547;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;SGD&#30340;&#21160;&#24577;&#24314;&#27169;&#20026;&#21452;&#37325;&#38543;&#26426;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00320</link><description>&lt;p&gt;
&#21452;&#37325;&#38543;&#26426;&#27169;&#22411;&#65306;&#26080;&#20559;&#26631;&#31614;&#22122;&#22768;&#30340;&#23398;&#20064;&#19982;&#25512;&#29702;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;
Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability. (arXiv:2304.00320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#20559;&#26631;&#31614;&#22122;&#22768;&#30340;&#38544;&#21547;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;SGD&#30340;&#21160;&#24577;&#24314;&#27169;&#20026;&#21452;&#37325;&#38543;&#26426;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26631;&#31614;&#22122;&#22768;&#24191;&#27867;&#23384;&#22312;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#30740;&#31350;&#26631;&#31614;&#22122;&#22768;&#30340;&#38544;&#21547;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#20551;&#35774;&#26631;&#31614;&#22122;&#22768;&#26159;&#26080;&#20559;&#30340;&#65292;&#20998;&#26512;&#20102;SGD&#22312;&#26080;&#20559;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#23558;SGD&#30340;&#21160;&#24577;&#24314;&#27169;&#20026;&#20855;&#26377;&#20004;&#20010;&#25193;&#25955;&#39033;&#30340;&#38543;&#26426;&#21487;&#24494;&#26041;&#31243;&#65288;&#21363;&#21452;&#37325;&#38543;&#26426;&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#31181;&#38544;&#21547;&#27491;&#21017;&#21270;&#21487;&#20197;&#22312;&#23398;&#21040;&#30340;&#27169;&#22411;&#19978;&#23454;&#26045;&#21452;&#37325;&#38543;&#26426;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#21644;&#27700;&#24179;&#30340;&#26631;&#31614;&#22122;&#22768;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random label noises (or observational noises) widely exist in practical machine learning settings. While previous studies primarily focus on the affects of label noises to the performance of learning, our work intends to investigate the implicit regularization effects of the label noises, under mini-batch sampling settings of stochastic gradient descent (SGD), with assumptions that label noises are unbiased. Specifically, we analyze the learning dynamics of SGD over the quadratic loss with unbiased label noises, where we model the dynamics of SGD as a stochastic differentiable equation (SDE) with two diffusion terms (namely a Doubly Stochastic Model). While the first diffusion term is caused by mini-batch sampling over the (label-noiseless) loss gradients as many other works on SGD, our model investigates the second noise term of SGD dynamics, which is caused by mini-batch sampling over the label noises, as an implicit regularizer. Our theoretical analysis finds such implicit regulariz
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#21033;&#29992;&#26368;&#26032;&#25216;&#26415;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#26041;&#27861;&#65292;&#36890;&#36807;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#32858;&#31867;&#21307;&#23398;&#25968;&#25454;&#65292;&#33021;&#22815;&#35753;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#22312;&#26174;&#30528;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#26356;&#31934;&#20934;&#22320;&#39044;&#27979;&#26368;&#24120;&#35265;&#30340;&#30149;&#29702;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.00311</link><description>&lt;p&gt;
&#21307;&#30103;&#30149;&#29702;&#39044;&#27979;&#65306;&#31995;&#32479;&#24615;&#32508;&#36848;&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Medical Pathologies Prediction : Systematic Review and Proposed Approach. (arXiv:2304.00311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#21033;&#29992;&#26368;&#26032;&#25216;&#26415;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#26041;&#27861;&#65292;&#36890;&#36807;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#32858;&#31867;&#21307;&#23398;&#25968;&#25454;&#65292;&#33021;&#22815;&#35753;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#22312;&#26174;&#30528;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#26356;&#31934;&#20934;&#22320;&#39044;&#27979;&#26368;&#24120;&#35265;&#30340;&#30149;&#29702;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#37096;&#38376;&#26159;&#27599;&#20010;&#31038;&#21306;&#30340;&#37325;&#35201;&#25903;&#26609;&#65292;&#20026;&#20102;&#20248;&#21270;&#21307;&#30103;&#27969;&#31243;&#12289;&#25552;&#39640;&#25252;&#29702;&#36136;&#37327;&#65292;&#26041;&#20415;&#24739;&#32773;&#31649;&#29702;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#20998;&#26512;&#21644;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#24037;&#20316;&#65292;&#28041;&#21450;&#21040;&#21033;&#29992;&#26368;&#26032;&#25216;&#26415;&#65288;&#22914;&#22823;&#25968;&#25454;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65289;&#26469;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#25105;&#20204;&#30340;&#19968;&#33324;&#26041;&#27861;&#65292;&#38598;&#20013;&#22312;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#32858;&#31867;&#21307;&#23398;&#25968;&#25454;&#20197;&#20415;&#22312;&#26174;&#30528;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#32463;&#36807;&#20998;&#26512;&#65292;&#35753;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#33021;&#22815;&#26356;&#31934;&#20934;&#22320;&#39044;&#27979;&#26368;&#24120;&#35265;&#30340;&#30149;&#29702;&#24773;&#20917;&#12290;&#20851;&#38190;&#35789;&#65306;&#21307;&#30103;&#20445;&#20581;&#12289;&#22823;&#25968;&#25454;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25968;&#25454;&#25366;&#25496;&#12289;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The healthcare sector is an important pillar of every community, numerous research studies have been carried out in this context to optimize medical processes and improve care quality and facilitate patient management. In this article we have analyzed and examined different works concerning the exploitation of the most recent technologies such as big data, artificial intelligence, machine learning, and deep learning for the improvement of health care, which enabled us to propose our general approach concentrating on the collection, preprocessing and clustering of medical data to facilitate access, after analysis, to the patients and health professionals to predict the most frequent pathologies with better precision within a notable timeframe.  keywords: Healthcare, big data, artificial intelligence, automatic language processing, data mining, predictive models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#21487;&#38752;&#22320;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#25506;&#32034;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20026;&#23376;&#20154;&#32676;&#21010;&#20998;&#25552;&#20379;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2304.00305</link><description>&lt;p&gt;
&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65306;&#24230;&#37327;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predictive Heterogeneity: Measures and Applications. (arXiv:2304.00305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#21487;&#38752;&#22320;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#25506;&#32034;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20026;&#23376;&#20154;&#32676;&#21010;&#20998;&#25552;&#20379;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22823;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22522;&#26412;&#23646;&#24615;&#65292;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23384;&#22312;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22914;&#31934;&#20934;&#21307;&#23398;&#12289;&#33258;&#21160;&#39550;&#39542;&#12289;&#37329;&#34701;&#24212;&#29992;&#31561;&#12290;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32780;&#35328;&#65292;&#24573;&#30053;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#20250;&#26497;&#22823;&#22320;&#25439;&#23475;&#27867;&#21270;&#24615;&#33021;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#19981;&#21516;&#23376;&#20154;&#32676;&#20043;&#38388;&#30340;&#39044;&#27979;&#26426;&#21046;&#21487;&#33021;&#20250;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#20851;&#27880;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;&#21487;&#29992;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#8221;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#32422;&#26463;&#12290;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#21487;&#38752;&#22320;&#20272;&#35745;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#20449;&#30340;&#27491;&#30830;&#24615;(PAC)&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#25506;&#32034;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25506;&#32034;&#20986;&#30340;&#24322;&#36136;&#24615;&#20026;&#23376;&#20154;&#32676;&#21010;&#20998;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an intrinsic and fundamental property of big data, data heterogeneity exists in a variety of real-world applications, such as precision medicine, autonomous driving, financial applications, etc. For machine learning algorithms, the ignorance of data heterogeneity will greatly hurt the generalization performance and the algorithmic fairness, since the prediction mechanisms among different sub-populations are likely to differ from each other. In this work, we focus on the data heterogeneity that affects the prediction of machine learning models, and firstly propose the \emph{usable predictive heterogeneity}, which takes into account the model capacity and computational constraints. We prove that it can be reliably estimated from finite data with probably approximately correct (PAC) bounds. Additionally, we design a bi-level optimization algorithm to explore the usable predictive heterogeneity from data. Empirically, the explored heterogeneity provides insights for sub-population divis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36830;&#32493;&#21644;&#23450;&#21521;&#22686;&#24378;&#65288;Fair-CDA&#65289;&#30340;&#32454;&#33268;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#27169;&#22411;&#23637;&#31034;&#20102;&#21487;&#20197;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36335;&#24452;&#26041;&#21521;&#19978;&#30340;&#25200;&#21160;&#24378;&#24230;&#23454;&#29616;&#20102;&#21487;&#25511;&#21644;&#21487;&#23457;&#35745;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Fair-CDA&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;Adult&#12289;CelebA&#21644;MovieLens&#12290;</title><link>http://arxiv.org/abs/2304.00295</link><description>&lt;p&gt;
&#20844;&#24179;&#36830;&#32493;&#21644;&#23450;&#21521;&#22686;&#24378;&#32452;&#30340;&#23436;&#32654;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Fair-CDA: Continuous and Directional Augmentation for Group Fairness. (arXiv:2304.00295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36830;&#32493;&#21644;&#23450;&#21521;&#22686;&#24378;&#65288;Fair-CDA&#65289;&#30340;&#32454;&#33268;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#27169;&#22411;&#23637;&#31034;&#20102;&#21487;&#20197;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36335;&#24452;&#26041;&#21521;&#19978;&#30340;&#25200;&#21160;&#24378;&#24230;&#23454;&#29616;&#20102;&#21487;&#25511;&#21644;&#21487;&#23457;&#35745;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Fair-CDA&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;Adult&#12289;CelebA&#21644;MovieLens&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#8212;&#8212;&#20844;&#24179;&#36830;&#32493;&#21644;&#23450;&#21521;&#22686;&#24378;&#65288;Fair-CDA&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#23454;&#26045;&#12290;&#25105;&#20204;&#20351;&#29992;&#29305;&#24449;&#35299;&#32544;&#26041;&#27861;&#25552;&#21462;&#19982;&#25935;&#24863;&#23646;&#24615;&#39640;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#32452;&#20043;&#38388;&#30340;&#25935;&#24863;&#29305;&#24449;&#36716;&#25442;&#36335;&#24452;&#19978;&#27491;&#21017;&#21270;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#21487;&#20197;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#35843;&#25972;&#36335;&#24452;&#26041;&#21521;&#19978;&#30340;&#25200;&#21160;&#24378;&#24230;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#22686;&#24378;&#26041;&#27861;&#26159;&#21487;&#25511;&#21644;&#21487;&#23457;&#35745;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#20844;&#24179;&#24615;&#32422;&#26463;&#23548;&#33268;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#26657;&#20934;&#27169;&#22411;&#26469;&#20026;&#22686;&#24378;&#25968;&#25454;&#22635;&#34917;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20551;&#35774;&#20219;&#20309;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#30830;&#20445;&#23545;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Fair-CDA&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;Adult&#12289;CelebA&#21644;MovieLens&#12290;&#29305;&#21035;&#26159;&#65292;Fair-CDA&#22312;&#20844;&#24179;&#26041;&#38754;&#33719;&#24471;&#20102;86.3&#65285;&#30340;&#30456;&#23545;&#25913;&#21892;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose {\it Fair-CDA}, a fine-grained data augmentation strategy for imposing fairness constraints. We use a feature disentanglement method to extract the features highly related to the sensitive attributes. Then we show that group fairness can be achieved by regularizing the models on transition paths of sensitive features between groups. By adjusting the perturbation strength in the direction of the paths, our proposed augmentation is controllable and auditable. To alleviate the accuracy degradation caused by fairness constraints, we further introduce a calibrated model to impute labels for the augmented data. Our proposed method does not assume any data generative model and ensures good generalization for both accuracy and fairness. Experimental results show that Fair-CDA consistently outperforms state-of-the-art methods on widely-used benchmarks, e.g., Adult, CelebA and MovieLens. Especially, Fair-CDA obtains an 86.3\% relative improvement for fairness while maint
&lt;/p&gt;</description></item><item><title>BioSequence2Vec&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#39640;&#25928;&#30340;&#29983;&#29289;&#24207;&#21015;&#23884;&#20837;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#35745;&#31639;&#26102;&#38388;&#21644;&#23384;&#20648;&#22823;&#37327;&#26680;&#30697;&#38453;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00291</link><description>&lt;p&gt;
BioSequence2Vec: &#29983;&#29289;&#24207;&#21015;&#26377;&#25928;&#23884;&#20837;&#29983;&#25104;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BioSequence2Vec: Efficient Embedding Generation For Biological Sequences. (arXiv:2304.00291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00291
&lt;/p&gt;
&lt;p&gt;
BioSequence2Vec&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#39640;&#25928;&#30340;&#29983;&#29289;&#24207;&#21015;&#23884;&#20837;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#35745;&#31639;&#26102;&#38388;&#21644;&#23384;&#20648;&#22823;&#37327;&#26680;&#30697;&#38453;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#12290;&#30001;&#20110;&#29983;&#29289;&#24207;&#21015;&#25968;&#25454;&#30340;&#25968;&#37327;&#24222;&#22823;&#65292;&#22240;&#27492;&#23398;&#20064;&#26126;&#30830;&#34920;&#31034;&#26159;&#22256;&#38590;&#30340;&#12290;&#26680;&#26041;&#27861;&#65292;&#22914;SVM&#65292;&#26159;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#65292;&#20363;&#22914;&#24207;&#21015;&#20998;&#31867;&#12290;&#28982;&#32780;&#26680;&#26041;&#27861;&#26377;&#19977;&#20010;&#25361;&#25112;&#65292;&#20998;&#21035;&#20026;&#35745;&#31639;&#26102;&#38388;&#65292;&#23384;&#20648;&#22823;&#37327;&#30340;&#26680;&#30697;&#38453;&#65292;&#38590;&#20197;&#25512;&#24191;&#21040;&#38750;&#26680;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#29983;&#29289;&#24207;&#21015;&#26377;&#25928;&#23884;&#20837;&#29983;&#25104;&#30340;&#26041;&#27861;BioSequence2Vec&#65292;&#20197;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning is an important step in the machine learning pipeline. Given the current biological sequencing data volume, learning an explicit representation is prohibitive due to the dimensionality of the resulting feature vectors. Kernel-based methods, e.g., SVM, are a proven efficient and useful alternative for several machine learning (ML) tasks such as sequence classification. Three challenges with kernel methods are (i) the computation time, (ii) the memory usage (storing an $n\times n$ matrix), and (iii) the usage of kernel matrices limited to kernel-based ML methods (difficult to generalize on non-kernel classifiers). While (i) can be solved using approximate methods, challenge (ii) remains for typical kernel methods. Similarly, although non-kernel-based ML methods can be applied to kernel matrices by extracting principal components (kernel PCA), it may result in information loss, while being computationally expensive. In this paper, we propose a general-purpose repre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25913;&#21892;&#34987;&#21160;&#24335;&#20809;&#32420;&#32593;&#32476;&#30417;&#27979;&#65292;&#20855;&#26377;&#39640;&#30340;&#35786;&#26029;&#20934;&#30830;&#29575;&#21644;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.00285</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#34987;&#21160;&#24335;&#20809;&#32420;&#32593;&#32476;&#20013;&#30340;&#20998;&#25903;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Branch Identification in Passive Optical Networks using Machine Learning. (arXiv:2304.00285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25913;&#21892;&#34987;&#21160;&#24335;&#20809;&#32420;&#32593;&#32476;&#30417;&#27979;&#65292;&#20855;&#26377;&#39640;&#30340;&#35786;&#26029;&#20934;&#30830;&#29575;&#21644;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#23454;&#39564;&#39564;&#35777;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25913;&#21892;&#20960;&#20046;&#31561;&#36317;&#20998;&#25903;&#30340;&#34987;&#21160;&#24335;&#20809;&#32420;&#32593;&#32476;&#20013;&#30417;&#27979;&#30340;&#26041;&#27861;&#12290;&#23427;&#23454;&#29616;&#20102;&#39640;&#36798;98.7&#65285;&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;0.5m&#30340;&#20107;&#20214;&#23450;&#20301;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
A machine learning approach for improving monitoring in passive optical networks with almost equidistant branches is proposed and experimentally validated. It achieves a high diagnostic accuracy of 98.7% and an event localization error of 0.5m
&lt;/p&gt;</description></item><item><title>&#29289;&#32852;&#32593;&#30340;&#26222;&#21450;&#24102;&#26469;&#22823;&#37327;&#25968;&#25454;&#65292;&#23545;&#29992;&#25143;&#30340;&#38544;&#31169;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#29289;&#32852;&#32593;&#39046;&#22495;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#29616;&#26377;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.00258</link><description>&lt;p&gt;
&#29289;&#32852;&#32593;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Data Privacy Preservation on the Internet of Things. (arXiv:2304.00258v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00258
&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#26222;&#21450;&#24102;&#26469;&#22823;&#37327;&#25968;&#25454;&#65292;&#23545;&#29992;&#25143;&#30340;&#38544;&#31169;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#29289;&#32852;&#32593;&#39046;&#22495;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#29616;&#26377;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30828;&#20214;&#21644;&#20449;&#24687;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#20351;&#24471;&#20840;&#29699;&#25968;&#21313;&#20159;&#20010;&#26234;&#33021;&#35774;&#22791;&#30456;&#20114;&#36830;&#25509;&#24182;&#20132;&#25442;&#20449;&#24687;&#65292;&#20154;&#31867;&#30340;&#20171;&#20837; minimal&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#33539;&#24335;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#39044;&#35745;&#21040;2025&#24180;&#23558;&#26377;270&#20159;&#20010;&#35774;&#22791;&#12290;&#36825;&#31181;&#22686;&#38271;&#23548;&#33268;&#20102;&#22823;&#37327;&#25968;&#25454;&#30340;&#20135;&#29983;&#65292;&#32780;&#36825;&#20063;&#24341;&#21457;&#20102;&#23545;&#29992;&#25143;&#38544;&#31169;&#19981;&#26029;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29289;&#32852;&#32593;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#30340;&#19968;&#20123;&#29616;&#26377;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in hardware and information technology have enabled the emergence of billions of connected, intelligent devices around the world exchanging information with minimal human involvement. This paradigm, known as the Internet of Things (IoT) is progressing quickly with an estimated 27 billion devices by 2025. This growth in the number of IoT devices and successful IoT services has generated a tremendous amount of data. However, this humongous volume of data poses growing concerns for user privacy. This introductory chapter has presented a brief survey of some of the existing data privacy-preservation schemes proposed by researchers in the field of the Internet of Things.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;</title><link>http://arxiv.org/abs/2304.00252</link><description>&lt;p&gt;
RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#20445;&#25252;&#65306;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#25915;&#20987;&#21487;&#20197;&#20351;&#24694;&#24847;&#29992;&#25143;&#25805;&#32437;&#29615;&#22659;&#25110;&#30772;&#22351;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23558;&#19968;&#20010;&#38544;&#34255;&#30340;&#21518;&#38376;&#25554;&#20837;&#21040;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#20013;&#12290;&#36825;&#31181;&#25915;&#20987;&#21361;&#21450;RL&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#65292;&#22312;&#21508;&#20010;&#20851;&#38190;&#39046;&#22495;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#12290;&#19982;&#27492;&#30456;&#27604;&#65292;&#23545;&#20110;RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#26377;&#25928;&#30340;&#38450;&#24481;&#25514;&#26045;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#25252;&#21463;&#23475;&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290; RTS&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#12290;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#20195;&#29702;&#20013;&#38544;&#34255;&#30340;&#21518;&#38376;&#12290;&#22312;&#35757;&#32451;&#26367;&#20195;&#32593;&#32476;&#26469;&#39044;&#27979;&#29366;&#24577;&#26102;&#65292;&#25105;&#20204;&#23558;&#20195;&#29702;&#21160;&#20316;&#20449;&#24687;&#24182;&#20837;&#65292;&#20943;&#23569;&#20195;&#29702;&#22312;&#39044;&#27979;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#21644;&#23454;&#38469;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#26234;&#33021;&#21330;&#20013;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#27604;&#36739;&#20102;&#20116;&#31181;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#26368;&#36866;&#21512;&#21330;&#20013;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.00249</link><description>&lt;p&gt;
&#20174;&#27010;&#24565;&#21040;&#37096;&#32626;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#26234;&#33021;&#21330;&#20013;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Conception to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and Performance Evaluation. (arXiv:2304.00249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#26234;&#33021;&#21330;&#20013;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#27604;&#36739;&#20102;&#20116;&#31181;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#26368;&#36866;&#21512;&#21330;&#20013;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21330;&#20013;&#26159;&#20840;&#29699;&#31532;&#20108;&#22823;&#27515;&#22240;&#12290;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#21330;&#20013;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#21330;&#20013;&#25968;&#25454;&#20998;&#26512;&#30340;&#32508;&#21512;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#21330;&#20013;&#39044;&#27979;&#26694;&#26550;&#12290;&#23545;&#21330;&#20013;&#39044;&#27979;&#20013;&#20116;&#20010;&#26368;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#32479;&#19968;&#30340;&#35774;&#32622;&#36827;&#34892;&#23458;&#35266;&#27604;&#36739;&#12290;&#27604;&#36739;&#20998;&#26512;&#21644;&#25968;&#23383;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#26368;&#36866;&#21512;&#21330;&#20013;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stroke is the second leading cause of death worldwide. Machine learning classification algorithms have been widely adopted for stroke prediction. However, these algorithms were evaluated using different datasets and evaluation metrics. Moreover, there is no comprehensive framework for stroke data analytics. This paper proposes an intelligent stroke prediction framework based on a critical examination of machine learning prediction algorithms in the literature. The five most used machine learning algorithms for stroke prediction are evaluated using a unified setup for objective comparison. Comparative analysis and numerical results reveal that the Random Forest algorithm is best suited for stroke prediction.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;R-BOCPD-UCRL2&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#37325;&#26032;&#21551;&#21160;&#30340;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65288;R-BOCPD&#65289;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#20998;&#24067;&#37319;&#26679;&#30340;MDP&#19978;&#25552;&#20379;&#20102;&#36739;&#20248;&#31168;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.00232</link><description>&lt;p&gt;
&#37325;&#21551;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#29992;&#20110;&#38750;&#24179;&#31283;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes. (arXiv:2304.00232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;R-BOCPD-UCRL2&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#37325;&#26032;&#21551;&#21160;&#30340;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65288;R-BOCPD&#65289;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#20998;&#24067;&#37319;&#26679;&#30340;MDP&#19978;&#25552;&#20379;&#20102;&#36739;&#20248;&#31168;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19968;&#20010;&#38750;&#24179;&#31283;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#35813;&#35774;&#32622;&#21487;&#20197;&#34987;&#23436;&#20840;&#25551;&#36848;&#20026;&#20998;&#27573;&#24179;&#31283;&#30340;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#37325;&#26032;&#21551;&#21160;&#30340;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65288;R-BOCPD&#65289;&#21464;&#20307;&#65292;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20174;&#26356;&#19968;&#33324;&#30340;&#22810;&#39033;&#24335;&#20998;&#24067;&#20013;&#29983;&#25104;&#30340;&#36755;&#20837;&#27969;&#65292;&#24182;&#22312;&#35823;&#35686;&#29575;&#21644;&#26816;&#27979;&#24310;&#36831;&#26041;&#38754;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20174;&#22810;&#39033;&#24335;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#29366;&#24577;&#36716;&#31227;&#20869;&#26680;&#30340;MDPs&#30340;&#25913;&#36827;&#29256;&#26412;UCRL2&#31639;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;R-BOCPD-UCRL2&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;R-BOCPD-UCRL2&#20855;&#26377;&#26377;&#21033;&#30340;&#36951;&#25022;&#30028;&#30340;$O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning in a non-stationary reinforcement learning (RL) environment, where the setting can be fully described by a piecewise stationary discrete-time Markov decision process (MDP). We introduce a variant of the Restarted Bayesian Online Change-Point Detection algorithm (R-BOCPD) that operates on input streams originating from the more general multinomial distribution and provides near-optimal theoretical guarantees in terms of false-alarm rate and detection delay. Based on this, we propose an improved version of the UCRL2 algorithm for MDPs with state transition kernel sampled from a multinomial distribution, which we call R-BOCPD-UCRL2. We perform a finite-time performance analysis and show that R-BOCPD-UCRL2 enjoys a favorable regret bound of $O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$,
&lt;/p&gt;</description></item><item><title>ConvBLS&#26159;&#19968;&#31181;&#21367;&#31215;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#65292;&#20351;&#29992;&#29699;&#24418;K-means&#31639;&#27861;&#12289;&#21367;&#31215;&#22686;&#24378;&#23618;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#23618;&#21487;&#20197;&#36798;&#21040;&#19982;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22312;&#38656;&#35201;&#26356;&#23569;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#35757;&#32451;&#21644;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.00219</link><description>&lt;p&gt;
ConvBLS&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#12289;&#21487;&#22686;&#37327;&#21367;&#31215;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ConvBLS: An Effective and Efficient Incremental Convolutional Broad Learning System for Image Classification. (arXiv:2304.00219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00219
&lt;/p&gt;
&lt;p&gt;
ConvBLS&#26159;&#19968;&#31181;&#21367;&#31215;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#65292;&#20351;&#29992;&#29699;&#24418;K-means&#31639;&#27861;&#12289;&#21367;&#31215;&#22686;&#24378;&#23618;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#23618;&#21487;&#20197;&#36798;&#21040;&#19982;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22312;&#38656;&#35201;&#26356;&#23569;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#35757;&#32451;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36890;&#24120;&#21463;&#21040;&#24040;&#22823;&#35745;&#31639;&#36164;&#28304;&#21644;&#32791;&#26102;&#22521;&#35757;&#36807;&#31243;&#30340;&#22256;&#25200;&#12290;&#20026;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#21450;&#20854;&#21367;&#31215;&#21464;&#20307;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#21367;&#31215;&#30340;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#65288;C-BLS&#65289;&#35201;&#20040;&#32570;&#20047;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#22686;&#37327;&#23398;&#20064;&#33021;&#21147;&#65292;&#35201;&#20040;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29699;&#24418;K-means&#65288;SKM&#65289;&#31639;&#27861;&#21644;&#20004;&#38454;&#27573;&#22810;&#23610;&#24230;&#65288;TSMS&#65289;&#29305;&#24449;&#34701;&#21512;&#30340;&#21367;&#31215;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#65288;ConvBLS&#65289;&#65292;&#21253;&#25324;&#21367;&#31215;&#29305;&#24449;&#65288;CF&#65289;&#23618;&#12289;&#21367;&#31215;&#22686;&#24378;&#65288;CE&#65289;&#23618;&#12289;TSMS&#29305;&#24449;&#34701;&#21512;&#23618;&#21644;&#36755;&#20986;&#23618;&#12290;&#19982;&#29616;&#26377;&#30340;C-BLS&#19981;&#21516;&#65292;&#20351;&#29992;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;SKM&#31639;&#27861;&#26469;&#23398;&#20064;CF&#23618;&#30340;&#26435;&#37325;&#12290;&#19982;&#38543;&#26426;&#28388;&#27874;&#22120;&#30456;&#27604;&#65292;SKM&#31639;&#27861;&#20351;CF&#23618;&#23398;&#20064;&#26356;&#20840;&#38754;&#30340;&#31354;&#38388;&#29305;&#24449;&#12290;&#24341;&#20837;CE&#23618;&#36890;&#36807;max-pooling&#25805;&#20316;&#21512;&#24182;CF&#23618;&#30340;&#28608;&#27963;&#22270;&#26469;&#22686;&#24378;&#28145;&#24230;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;TSMS&#29305;&#24449;&#34701;&#21512;&#26469;&#38598;&#25104;&#22810;&#23610;&#24230;&#21367;&#31215;&#29305;&#24449;&#65292;&#20197;&#25429;&#33719;&#26356;&#20016;&#23500;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#26368;&#32456;&#65292;&#36755;&#20986;&#23618;&#37319;&#29992;softmax&#28608;&#27963;&#20989;&#25968;&#33719;&#24471;&#20998;&#31867;&#32467;&#26524;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ConvBLS&#22312;&#38656;&#35201;&#26356;&#23569;&#30340;&#22521;&#35757;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#20248;&#31168;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning generally suffers from enormous computational resources and time-consuming training processes. Broad Learning System (BLS) and its convolutional variants have been proposed to mitigate these issues and have achieved superb performance in image classification. However, the existing convolutional-based broad learning system (C-BLS) either lacks an efficient training method and incremental learning capability or suffers from poor performance. To this end, we propose a convolutional broad learning system (ConvBLS) based on the spherical K-means (SKM) algorithm and two-stage multi-scale (TSMS) feature fusion, which consists of the convolutional feature (CF) layer, convolutional enhancement (CE) layer, TSMS feature fusion layer, and output layer. First, unlike the current C-BLS, the simple yet efficient SKM algorithm is utilized to learn the weights of CF layers. Compared with random filters, the SKM algorithm makes the CF layer learn more comprehensive spatial features. Second
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24573;&#30053;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00216</link><description>&lt;p&gt;
&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;&#36328;&#23610;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24573;&#30053;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#65292;&#36328;&#22810;&#20010;&#23610;&#24230;&#20998;&#26512;&#39640;&#20998;&#36776;&#29575;&#30340;&#20840;&#24133;&#22270;&#20687; (WSIs) &#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064; (MIL) &#26159;&#21033;&#29992;&#20998;&#31867;&#23545;&#35937;&#38598; (&#20363;&#22914;&#36739;&#23567;&#30340;&#22270;&#20687;&#22359;&#38598;) &#23545;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22788;&#29702;&#36890;&#24120;&#22312;WSIs&#30340;&#21333;&#20010;&#23610;&#24230;&#65288;&#20363;&#22914;20&#20493;&#25918;&#22823;&#65289;&#19978;&#36827;&#34892;&#65292;&#24573;&#30053;&#20102;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;(1) &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL (CS-MIL)&#31639;&#27861;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#36328;&#23610;&#24230;&#20851;&#31995;&#65307;(2) &#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#29609;&#20855;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23610;&#24230;&#29305;&#24322;&#24615;&#24418;&#24577;&#29305;&#24449;&#65292;&#20197;&#26816;&#26597;&#21644;&#21487;&#35270;&#21270;&#19981;&#21516;&#30340;&#36328;&#23610;&#24230;&#20851;&#31995;&#65307;(3)&#22312;&#22235;&#20010;WSI&#30340;&#32454;&#32990;&#32954;&#30284;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;CS-MIL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing high resolution whole slide images (WSIs) with regard to information across multiple scales poses a significant challenge in digital pathology. Multi-instance learning (MIL) is a common solution for working with high resolution images by classifying bags of objects (i.e. sets of smaller image patches). However, such processing is typically performed at a single scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale information that is key to diagnoses by human pathologists. In this study, we propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale relationships into a single MIL network for pathological image diagnosis. The contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL) algorithm that integrates the multi-scale information and the inter-scale relationships is proposed; (2) A toy dataset with scale-specific morphological features is created and released to examine and visualize differential cross-scale a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00215</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#24335;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#23548;&#35774;&#32622;&#65292;&#32570;&#20047;&#24402;&#32435;&#33021;&#21147;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#20998;&#23618;Transformer&#26694;&#26550;&#65292;&#21363;REPORT&#65292;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;REPORT&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#20004;&#20010;&#23436;&#20840;&#24402;&#32435;&#30340;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#29256;&#26412;&#23376;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;REPORT&#33021;&#22815;&#23558;&#25512;&#29702;&#25512;&#24191;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#27809;&#26377;&#20844;&#20849;&#23454;&#20307;&#30340;&#26032;&#23454;&#20307;&#19978;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;Mask Transformers&#25216;&#26415;&#21644;&#30446;&#26631;&#26597;&#35810;&#30340;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#36229;&#20986;&#20998;&#24067;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;AI&#31639;&#27861;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00212</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#8220;&#24694;&#39764;&#22312;&#26597;&#35810;&#20013;&#65306;&#25512;&#36827;Mask Transformers&#25216;&#26415;&#22312;&#30495;&#23454;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21644;&#36229;&#20986;&#20998;&#24067;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;&#8221;
&lt;/p&gt;
&lt;p&gt;
Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization. (arXiv:2304.00212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;Mask Transformers&#25216;&#26415;&#21644;&#30446;&#26631;&#26597;&#35810;&#30340;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#36229;&#20986;&#20998;&#24067;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;AI&#31639;&#27861;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#23384;&#22312;&#30528;&#24040;&#22823;&#30340;&#38271;&#23614;&#23545;&#35937;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#23614;&#37096;&#24773;&#20917;&#19982;&#30456;&#23545;&#32597;&#35265;&#30340;&#30142;&#30149;&#30456;&#20851;&#65292;&#24182;&#22312;&#20020;&#24202;&#19978;&#20855;&#26377;&#26174;&#33879;&#24847;&#20041;&#12290;&#19968;&#31181;&#20540;&#24471;&#20449;&#36182;&#30340;&#21307;&#30103;AI&#31639;&#27861;&#24212;&#35813;&#22312;&#23614;&#37096;&#26465;&#20214;&#19978;&#23637;&#29616;&#20854;&#26377;&#25928;&#24615;&#65292;&#36991;&#20813;&#22312;&#36825;&#20123;&#36229;&#20986;&#20998;&#24067;(OOD)&#30340;&#24773;&#20917;&#19979;&#36896;&#25104;&#33268;&#21629;&#30340;&#21361;&#23475;&#12290;&#26412;&#25991;&#37319;&#29992;&#30446;&#26631;&#26597;&#35810;&#30340;&#27010;&#24565;&#23558;&#35821;&#20041;&#20998;&#21106;&#34920;&#36848;&#20026;&#36719;&#32858;&#31867;&#20998;&#37197;&#65292;&#26597;&#35810;&#22312;&#35757;&#32451;&#26399;&#38388;&#36866;&#24212;&#20110;&#20869;&#37096;&#28857;&#30340;&#29305;&#24449;&#32423;&#32858;&#31867;&#20013;&#24515;&#12290;&#22240;&#27492;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#23545;&#21307;&#23398;&#22270;&#20687;&#25191;&#34892;&#25512;&#29702;&#26102;&#65292;&#20687;&#32032;&#19982;&#26597;&#35810;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21487;&#20197;&#26816;&#27979;&#21644;&#23450;&#20301;OOD&#21306;&#22495;&#12290;&#25105;&#20204;&#23558;&#27492;OOD&#23450;&#20301;&#31216;&#20026;MaxQuery&#12290;&#27492;&#22806;&#65292;&#30495;&#23454;&#21307;&#23398;&#22270;&#20687;&#30340;&#21069;&#26223;&#65292;&#26080;&#35770;&#26159;OOD&#23545;&#35937;&#36824;&#26159;&#20869;&#37096;&#28857;&#65292;&#37117;&#26159;&#30149;&#21464;&#12290;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#23567;&#20110;&#21069;&#26223;&#21644;&#32972;&#26223;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
Real-world medical image segmentation has tremendous long-tailed complexity of objects, among which tail conditions correlate with relatively rare diseases and are clinically significant. A trustworthy medical AI algorithm should demonstrate its effectiveness on tail conditions to avoid clinically dangerous damage in these out-of-distribution (OOD) cases. In this paper, we adopt the concept of object queries in Mask Transformers to formulate semantic segmentation as a soft cluster assignment. The queries fit the feature-level cluster centers of inliers during training. Therefore, when performing inference on a medical image in real-world scenarios, the similarity between pixels and the queries detects and localizes OOD regions. We term this OOD localization as MaxQuery. Furthermore, the foregrounds of real-world medical images, whether OOD objects or inliers, are lesions. The difference between them is less than that between the foreground and background, possibly misleading the object
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#21069;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#36136;&#37327;&#23545;&#25239;&#25200;&#21160;&#30340;&#27491;&#38754;&#20808;&#39564;&#24341;&#23548;&#23545;&#25239;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00202</link><description>&lt;p&gt;
&#20351;&#29992;&#20808;&#39564;&#24341;&#23548;&#30693;&#35782;&#25913;&#36827;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improving Fast Adversarial Training with Prior-Guided Knowledge. (arXiv:2304.00202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#21069;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#36136;&#37327;&#23545;&#25239;&#25200;&#21160;&#30340;&#27491;&#38754;&#20808;&#39564;&#24341;&#23548;&#23545;&#25239;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#26159;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#22312;&#32463;&#36807;&#20960;&#20010;&#35757;&#32451;&#21608;&#26399;&#21518;&#40065;&#26834;&#24615;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#30340;&#21464;&#20307;&#26469;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#36739;&#39640;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#21644;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#26679;&#26412;&#36136;&#37327;&#21644;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#21464;&#24046;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#36136;&#37327;&#23545;&#25239;&#25200;&#21160;&#30340;&#27491;&#38754;&#20808;&#39564;&#24341;&#23548;&#23545;&#25239;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#21021;&#22987;&#21270;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast adversarial training (FAT) is an efficient method to improve robustness. However, the original FAT suffers from catastrophic overfitting, which dramatically and suddenly reduces robustness after a few training epochs. Although various FAT variants have been proposed to prevent overfitting, they require high training costs. In this paper, we investigate the relationship between adversarial example quality and catastrophic overfitting by comparing the training processes of standard adversarial training and FAT. We find that catastrophic overfitting occurs when the attack success rate of adversarial examples becomes worse. Based on this observation, we propose a positive prior-guided adversarial initialization to prevent overfitting by improving adversarial example quality without extra training costs. This initialization is generated by using high-quality adversarial perturbations from the historical training process. We provide theoretical analysis for the proposed initialization a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#24314;&#27169;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#21253;&#21547;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.00200</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#30340;&#31890;&#23376;&#31995;&#32479;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion map particle systems for generative modeling. (arXiv:2304.00200v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#24314;&#27169;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#21253;&#21547;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#21644;Laplacian&#35843;&#25972;&#30340;Wasserstein&#26799;&#24230;&#19979;&#38477;&#65288;LAWGD&#65289;&#12290;&#25193;&#25955;&#26144;&#23556;&#34987;&#29992;&#26469;&#20174;&#26679;&#26412;&#20013;&#36817;&#20284;Langevin&#25193;&#25955;&#36807;&#31243;&#30340;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#23398;&#20064;&#28508;&#22312;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#24418;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LAWGD&#33021;&#22815;&#22312;&#21512;&#36866;&#30340;&#26680;&#20989;&#25968;&#36873;&#25321;&#19979;&#39640;&#25928;&#22320;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#25277;&#26679;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#35745;&#31639;&#29983;&#25104;&#22120;&#30340;&#35889;&#36924;&#36817;&#26469;&#26500;&#36896;&#26680;&#20989;&#25968;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20855;&#26377;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel diffusion map particle system (DMPS) for generative modeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient descent (LAWGD). Diffusion maps are used to approximate the generator of the Langevin diffusion process from samples, and hence to learn the underlying data-generating manifold. On the other hand, LAWGD enables efficient sampling from the target distribution given a suitable choice of kernel, which we construct here via a spectral approximation of the generator, computed with diffusion maps. Numerical experiments show that our method outperforms others on synthetic datasets, including examples with manifold structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#24418;&#23398;&#20064;&#20013;&#24212;&#29992;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#27604;OT&#22270;&#26356;&#20415;&#23452;&#22320;&#35745;&#31639;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;&#21644;&#20280;&#32553;&#30340;&#31561;&#36317;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00199</link><description>&lt;p&gt;
&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#22312;&#27969;&#34892;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of No-Collision Transportation Maps in Manifold Learning. (arXiv:2304.00199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#24418;&#23398;&#20064;&#20013;&#24212;&#29992;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#27604;OT&#22270;&#26356;&#20415;&#23452;&#22320;&#35745;&#31639;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;&#21644;&#20280;&#32553;&#30340;&#31561;&#36317;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24341;&#20837;&#20110;[Nurbekyan et al.&#65292;2020]&#30340;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#22312;&#22270;&#20687;&#25968;&#25454;&#30340;&#27969;&#24418;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#34920;&#31034;&#31867;&#20284;&#36816;&#21160;&#25110;&#21464;&#24418;&#29616;&#35937;&#30340;&#25968;&#25454;&#20013;&#65292;&#24212;&#29992;&#22522;&#20110;&#36816;&#36755;&#30340;&#36317;&#31163;&#21644;&#29305;&#24449;&#30340;&#30740;&#31350;&#22823;&#24133;&#22686;&#21152;&#12290;&#20107;&#23454;&#19978;&#65292;&#22266;&#23450;&#20301;&#32622;&#27604;&#36739;&#24378;&#24230;&#36890;&#24120;&#26080;&#27861;&#26174;&#31034;&#25968;&#25454;&#32467;&#26500;&#12290;&#22312;[Nurbekyan et al.&#65292;2020]&#20013;&#24320;&#21457;&#30340;&#26080;&#30896;&#25758;&#22270;&#21644;&#36317;&#31163;&#31867;&#20284;&#20110;&#26368;&#20248;&#20256;&#36755;(OT)&#22270;&#30340;&#20960;&#20309;&#29305;&#24449;&#20294;&#30001;&#20110;&#26080;&#38656;&#20248;&#21270;&#65292;&#35745;&#31639;&#25104;&#26412;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#26412;&#25991;&#35777;&#26126;&#26080;&#30896;&#25758;&#36317;&#31163;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;(&#20998;&#21035;&#26159;&#20280;&#32553;)&#21644;&#35013;&#22791;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#30340;&#24179;&#31227;(&#20998;&#21035;&#26159;&#20280;&#32553;)&#21521;&#37327;&#20043;&#38388;&#30340;&#31561;&#36317;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#20197;&#21450;OT&#21644;&#32447;&#24615;OT&#22270;&#65292;&#19968;&#33324;&#26469;&#35828;&#19981;&#33021;&#20026;&#26059;&#36716;&#25552;&#20379;&#31561;&#36317;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate applications of no-collision transportation maps introduced in [Nurbekyan et. al., 2020] in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [Nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22914;&#20309;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20026;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;&#24207;&#21015;&#35757;&#32451;&#31639;&#27861;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.00198</link><description>&lt;p&gt;
&#24207;&#21015;&#23398;&#20064;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65306;&#25968;&#25454;&#21516;&#21270;&#32467;&#21512;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sequential Learning from Noisy Data: Data-Assimilation Meets Echo-State Network. (arXiv:2304.00198v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22914;&#20309;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20026;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;&#24207;&#21015;&#35757;&#32451;&#31639;&#27861;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#12290;&#34429;&#28982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#39044;&#27979;&#22120;&#23545;&#26080;&#22122;&#22768;&#30340;&#35757;&#32451;&#25968;&#25454;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#35757;&#32451;&#38454;&#27573;&#36755;&#20837;&#22122;&#22768;&#26102;&#30340;&#39044;&#27979;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#24207;&#21015;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;&#22122;&#22768;&#35266;&#23519;&#32467;&#26524;&#65292;&#21033;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20026;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Kalman&#35757;&#32451;&#30340;ESN&#65288;KalT-ESN&#65289;&#20248;&#20110;&#20256;&#32479;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#35757;&#32451;&#30340;ESN&#65292;&#21516;&#26102;&#20173;&#28982;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;&#19977;&#20010;&#31995;&#32479;&#30340;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65306;&#20004;&#20010;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the problem of training a recurrent neural network from noisy data. While neural network based dynamic predictors perform well with noise-free training data, prediction with noisy inputs during training phase poses a significant challenge. Here a sequential training algorithm is developed for an echo-state network (ESN) by incorporating noisy observations using an ensemble Kalman filter. The resultant Kalman-trained echo-state network (KalT-ESN) outperforms the traditionally trained ESN with least square algorithm while still being computationally cheap. The proposed method is demonstrated on noisy observations from three systems: two synthetic datasets from chaotic dynamical systems and a set of real-time traffic data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;</title><link>http://arxiv.org/abs/2304.00195</link><description>&lt;p&gt;
&#25277;&#35937;&#22120;&#65306;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. (arXiv:2304.00195v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#20851;&#31995;&#23398;&#20064;&#36716;&#21270;&#20026;Transformer&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
A framework is proposed that casts relational learning in terms of transformers, implementing binding between sensory states and abstract states with relational cross attention mechanisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#24212;&#23545;&#20256;&#24863;&#22120;&#22122;&#22768;&#21450;&#19981;&#30830;&#23450;&#24615;&#30340;&#24863;&#30693;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#22320;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#25511;&#21046;&#35774;&#35745;&#20013;&#65292;&#35745;&#31639;&#26377;&#25928;&#30340;&#29366;&#24577;&#20272;&#35745;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#30340;&#37319;&#26679;&#25968;&#25454;&#25511;&#21046;&#65292;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00194</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#39044;&#27979;&#30340;&#38543;&#26426;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#19979;&#23433;&#20840;&#30340;&#24863;&#30693;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Safe Perception-Based Control under Stochastic Sensor Uncertainty using Conformal Prediction. (arXiv:2304.00194v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#24212;&#23545;&#20256;&#24863;&#22120;&#22122;&#22768;&#21450;&#19981;&#30830;&#23450;&#24615;&#30340;&#24863;&#30693;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#22320;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#25511;&#21046;&#35774;&#35745;&#20013;&#65292;&#35745;&#31639;&#26377;&#25928;&#30340;&#29366;&#24577;&#20272;&#35745;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#30340;&#37319;&#26679;&#25968;&#25454;&#25511;&#21046;&#65292;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#21033;&#29992;&#36890;&#36807;&#23398;&#20064;&#22686;&#24378;&#30340;&#24863;&#30693;&#22320;&#22270;&#20174;&#39640;&#32500;&#20256;&#24863;&#22120;&#27979;&#37327;&#20013;&#33719;&#24471;&#30340;&#29366;&#24577;&#20272;&#35745;&#30340;&#24863;&#30693;&#25511;&#21046;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#24863;&#30693;&#22320;&#22270;&#24182;&#19981;&#23436;&#32654;&#65292;&#20250;&#23548;&#33268;&#29366;&#24577;&#20272;&#35745;&#35823;&#24046;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#31995;&#32479;&#34892;&#20026;&#12290;&#38543;&#26426;&#20256;&#24863;&#22120;&#22122;&#22768;&#20250;&#20351;&#24773;&#20917;&#21464;&#24471;&#26356;&#31967;&#65292;&#24182;&#23548;&#33268;&#36981;&#24490;&#26410;&#30693;&#20998;&#24067;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24863;&#30693;&#25511;&#21046;&#26694;&#26550;&#65292;&#23427;: i&#65289;&#37327;&#21270;&#20102;&#24863;&#30693;&#22320;&#22270;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;ii&#65289;&#23558;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#38598;&#25104;&#21040;&#25511;&#21046;&#35774;&#35745;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#35745;&#31639;&#26377;&#25928;&#30340;&#29366;&#24577;&#20272;&#35745;&#21306;&#22495;&#65292;&#36825;&#20123;&#21306;&#22495;&#26159;&#39640;&#27010;&#29575;&#21253;&#21547;&#26410;&#30693;&#29366;&#24577;&#30340;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#27979;&#37327;&#40065;&#26834;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30340;&#27010;&#24565;&#35774;&#35745;&#20102;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#30340;&#37319;&#26679;&#25968;&#25454;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#30340;&#25511;&#21046;&#22120;&#20351;&#29992;&#20102;&#33258;&#35302;&#21457;&#25511;&#21046;&#30340;&#24605;&#24819;&#65292;&#24182;&#20351;&#25105;&#20204;&#36991;&#20813;&#20351;&#29992;&#38543;&#26426;&#24494;&#31215;&#20998;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider perception-based control using state estimates that are obtained from high-dimensional sensor measurements via learning-enabled perception maps. However, these perception maps are not perfect and result in state estimation errors that can lead to unsafe system behavior. Stochastic sensor noise can make matters worse and result in estimation errors that follow unknown distributions. We propose a perception-based control framework that i) quantifies estimation uncertainty of perception maps, and ii) integrates these uncertainty representations into the control design. To do so, we use conformal prediction to compute valid state estimation regions, which are sets that contain the unknown state with high probability. We then devise a sampled-data controller for continuous-time systems based on the notion of measurement robust control barrier functions. Our controller uses idea from self-triggered control and enables us to avoid using stochastic calculus. Our framework is agnost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Neo4j&#22270;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#65292;&#33021;&#22815;&#23454;&#29616;&#25317;&#22581;&#36335;&#27573;&#30340;&#36127;&#36733;&#24179;&#34913;&#21644;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#25972;&#20307;&#20132;&#36890;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00192</link><description>&lt;p&gt;
&#22522;&#20110;Neo4j&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#25317;&#22581;&#27169;&#25311;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Neo4j and deep learning for traffic congestion simulation &amp; optimization. (arXiv:2304.00192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Neo4j&#22270;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#65292;&#33021;&#22815;&#23454;&#29616;&#25317;&#22581;&#36335;&#27573;&#30340;&#36127;&#36733;&#24179;&#34913;&#21644;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#25972;&#20307;&#20132;&#36890;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#19968;&#30452;&#26159;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#36807;&#21435;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#20984;&#26174;&#19982;&#20132;&#36890;&#25317;&#22581;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#20132;&#36890;&#25317;&#22581;&#20998;&#26512;&#37117;&#26159;&#20351;&#29992;&#27169;&#25311;&#36719;&#20214;&#36827;&#34892;&#30340;&#65292;&#36825;&#20123;&#36719;&#20214;&#30001;&#20110;&#20351;&#29992;&#30340;&#24037;&#20855;&#21644;&#23454;&#29992;&#31243;&#24207;&#30340;&#38480;&#21046;&#32780;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#27934;&#35265;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#24433;&#21709;&#21040;&#23450;&#21046;&#19994;&#21153;&#38382;&#39064;&#30340;&#21046;&#23450;&#65292;&#36825;&#20123;&#38382;&#39064;&#22240;&#22320;&#21306;&#21644;&#22269;&#23478;&#32780;&#24322;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#24314;&#27169;&#20026;Neo4j&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#36127;&#36733;&#24179;&#34913;&#12289;&#20248;&#21270;&#31639;&#27861;&#26469;&#35782;&#21035;&#26080;&#25317;&#22581;&#30340;&#36947;&#36335;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#25317;&#22581;&#25110;&#20107;&#25925;&#24773;&#20917;&#19979;&#20132;&#36890;&#22914;&#20309;&#21521;&#21518;&#20256;&#25773;&#20197;&#21450;&#20854;&#23545;&#20854;&#20182;&#36947;&#36335;&#27573;&#30340;&#24635;&#20307;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#22312;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;&#39034;&#24207;RNN-LSTM(&#38271;&#30701;&#26102;&#35760;&#24518;)&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic congestion has been a major challenge in many urban road networks. Extensive research studies have been conducted to highlight traffic-related congestion and address the issue using data-driven approaches. Currently, most traffic congestion analyses are done using simulation software that offers limited insight due to the limitations in the tools and utilities being used to render various traffic congestion scenarios. All that impacts the formulation of custom business problems which vary from place to place and country to country. By exploiting the power of the knowledge graph, we model a traffic congestion problem into the Neo4j graph and then use the load balancing, optimization algorithm to identify congestion-free road networks. We also show how traffic propagates backward in case of congestion or accident scenarios and its overall impact on other segments of the roads. We also train a sequential RNN-LSTM (Long Short-Term Memory) deep learning model on the real-time traffi
&lt;/p&gt;</description></item><item><title>$\textit{PrefGen}$ &#31995;&#32479;&#21033;&#29992;&#31616;&#21333;&#30340;&#25104;&#23545;&#27604;&#36739;&#26597;&#35810;&#65292;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#30340;&#30456;&#23545;&#23646;&#24615;&#12290;&#21033;&#29992;&#36825;&#20123;&#26597;&#35810;&#21709;&#24212;&#30340;&#20449;&#24687;&#65292;&#23545;&#19968;&#32452;&#22270;&#20687;&#23646;&#24615;&#30340;&#20559;&#22909;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2304.00185</link><description>&lt;p&gt;
PrefGen&#65306;&#22522;&#20110;&#20559;&#22909;&#30340;&#30456;&#23545;&#23646;&#24615;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PrefGen: Preference Guided Image Generation with Relative Attributes. (arXiv:2304.00185v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00185
&lt;/p&gt;
&lt;p&gt;
$\textit{PrefGen}$ &#31995;&#32479;&#21033;&#29992;&#31616;&#21333;&#30340;&#25104;&#23545;&#27604;&#36739;&#26597;&#35810;&#65292;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#30340;&#30456;&#23545;&#23646;&#24615;&#12290;&#21033;&#29992;&#36825;&#20123;&#26597;&#35810;&#21709;&#24212;&#30340;&#20449;&#24687;&#65292;&#23545;&#19968;&#32452;&#22270;&#20687;&#23646;&#24615;&#30340;&#20559;&#22909;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#28210;&#26579;&#20986;&#39640;&#20445;&#30495;&#30340;&#20154;&#33080;&#31561;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#25968;&#37327;&#23646;&#24615;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#20363;&#22914;&#24773;&#24863;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#26126;&#30830;&#37327;&#21270;&#25152;&#38656;&#35270;&#35273;&#23646;&#24615;&#30340;&#24378;&#24230;&#12290;&#20294;&#26159;&#38480;&#21046;&#22312;&#20110;&#35768;&#22810;&#23646;&#24615;&#65292;&#20363;&#22914;&#38754;&#37096;&#34920;&#24773;&#30340; "&#24868;&#24594;" &#31243;&#24230;&#65292;&#29992;&#25143;&#38590;&#20197;&#20934;&#30830;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21487;&#20197;&#21487;&#38752;&#22320;&#34920;&#36798;&#20986; "&#21738;&#24352;&#33080;&#30475;&#36215;&#26469;&#26356;&#24868;&#24594;"&#65292;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102; $\textit{PrefGen}$ &#31995;&#32479;&#65292;&#36890;&#36807;&#21576;&#29616;&#29992;&#25143;&#31616;&#21333;&#30340;&#25104;&#23545;&#27604;&#36739;&#26597;&#35810;&#65292;&#22914; "&#20320;&#26356;&#21916;&#27426;&#22270;&#20687; $a$ &#36824;&#26159; $b$&#65311;" &#26469;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#30340;&#30456;&#23545;&#23646;&#24615;&#12290;&#21033;&#29992;&#24207;&#21015;&#21270;&#26597;&#35810;&#21709;&#24212;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#29992;&#25143;&#23545;&#19968;&#32452;&#22270;&#20687;&#23646;&#24615;&#30340;&#20559;&#22909;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have the capacity to render high fidelity images of content like human faces. Recently, there has been substantial progress in conditionally generating images with specific quantitative attributes, like the emotion conveyed by one's face. These methods typically require a user to explicitly quantify the desired intensity of a visual attribute. A limitation of this method is that many attributes, like how "angry" a human face looks, are difficult for a user to precisely quantify. However, a user would be able to reliably say which of two faces seems "angrier". Following this premise, we develop the $\textit{PrefGen}$ system, which allows users to control the relative attributes of generated images by presenting them with simple paired comparison queries of the form "do you prefer image $a$ or image $b$?" Using information from a sequence of query responses, we can estimate user preferences over a set of image attributes and perform preference-guided image editing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#8212;&#8212;&#36719;Bellman&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#20223;&#23556;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#30340;&#22810;&#20010;&#29609;&#23478;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#26469;&#35745;&#31639;&#27492;&#24179;&#34913;&#65292;&#21516;&#26102;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#25512;&#26029;&#29609;&#23478;&#22870;&#21169;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00163</link><description>&lt;p&gt;
&#20223;&#23556;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#30340;&#36719;Bellman&#24179;&#34913;&#65306;&#21069;&#21521;&#35299;&#19982;&#36870;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning. (arXiv:2304.00163v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#8212;&#8212;&#36719;Bellman&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#20223;&#23556;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#30340;&#22810;&#20010;&#29609;&#23478;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#26469;&#35745;&#31639;&#27492;&#24179;&#34913;&#65292;&#21516;&#26102;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#25512;&#26029;&#29609;&#23478;&#22870;&#21169;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#22312;&#38543;&#26426;&#21160;&#24577;&#29615;&#22659;&#20013;&#27169;&#25311;&#22810;&#20010;&#29609;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#27599;&#20010;&#29609;&#23478;&#22312;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#26368;&#22823;&#21270;&#20854;&#26399;&#26395;&#30340;&#24635;&#25240;&#29616;&#22870;&#21169;&#65292;&#35813;&#22870;&#21169;&#21462;&#20915;&#20110;&#20854;&#20182;&#29609;&#23478;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#31216;&#20026;&#20223;&#23556;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#22312;&#20854;&#20013;&#65292;&#20223;&#23556;&#22870;&#21169;&#20989;&#25968;&#32806;&#21512;&#20102;&#29609;&#23478;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#21363;&#36719;Bellman&#24179;&#34913;&#65292;&#22312;&#20854;&#20013;&#65292;&#27599;&#20010;&#29609;&#23478;&#37117;&#26159;&#26377;&#38480;&#29702;&#24615;&#30340;&#65292;&#24182;&#36873;&#25321;&#36719;Bellman&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#20687;&#33879;&#21517;&#30340;Nash&#24179;&#34913;&#27010;&#24565;&#20013;&#37027;&#26679;&#36873;&#25321;&#32431;&#29702;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36719;Bellman&#24179;&#34913;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#26469;&#35745;&#31639;&#21069;&#21521;&#38382;&#39064;&#20013;&#30340;&#36825;&#31181;&#24179;&#34913;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#20102;&#25512;&#26029;&#29609;&#23478;&#22870;&#21169;&#21442;&#25968;&#30340;&#36870;&#21521;&#21338;&#24328;&#38382;&#39064;&#12290;&#22312;&#25504;&#39135;&#32773;-&#29454;&#29289;OpenAI Gym&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36719;Bellman&#31574;&#30053;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25511;&#21046;&#25504;&#39135;&#32773;&#21644;&#29454;&#29289;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov games model interactions among multiple players in a stochastic, dynamic environment. Each player in a Markov game maximizes its expected total discounted reward, which depends upon the policies of the other players. We formulate a class of Markov games, termed affine Markov games, where an affine reward function couples the players' actions. We introduce a novel solution concept, the soft-Bellman equilibrium, where each player is boundedly rational and chooses a soft-Bellman policy rather than a purely rational policy as in the well-known Nash equilibrium concept. We provide conditions for the existence and uniqueness of the soft-Bellman equilibrium and propose a nonlinear least squares algorithm to compute such an equilibrium in the forward problem. We then solve the inverse game problem of inferring the players' reward parameters from observed state-action trajectories via a projected gradient algorithm. Experiments in a predator-prey OpenAI Gym environment show that the rewa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#36716;&#31227;&#30697;&#38453;&#21644;&#22266;&#23450;&#20294;&#26410;&#30693;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;MDP&#23398;&#20064;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#32622;&#20449;&#21306;&#38388;&#26694;&#26550;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.00155</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19978;&#36827;&#34892;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Reinforcement Learning in Markov Decision Process Using Linear Programming. (arXiv:2304.00155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#36716;&#31227;&#30697;&#38453;&#21644;&#22266;&#23450;&#20294;&#26410;&#30693;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;MDP&#23398;&#20064;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#32622;&#20449;&#21306;&#38388;&#26694;&#26550;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#30693;&#36716;&#31227;&#30697;&#38453;&#21644;&#22266;&#23450;&#20294;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#23398;&#20064;&#32773;&#26088;&#22312;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#24182;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#20182;&#20204;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#27169;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#36807;&#28193;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#24182;&#20351;&#29992;&#21344;&#29992;&#24230;&#37327;&#23558;&#22312;&#32447;MDP&#19982;&#32447;&#24615;&#35268;&#21010;&#30456;&#36830;&#25509;&#65292;&#23454;&#29616;&#20102;$\tilde{O}(LX\sqrt{TA})$&#30340;&#39640;&#27010;&#29575;&#36951;&#25022;&#30028;&#12290;&#23427;&#27604;&#29616;&#26377;&#30340;&#20351;&#29992;&#31867;&#20284;&#32622;&#20449;&#21306;&#38388;&#26694;&#26550;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#24182;&#25913;&#21892;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online reinforcement learning in episodic Markov decision process (MDP) with an unknown transition matrix and stochastic rewards drawn from a fixed but unknown distribution. The learner aims to learn the optimal policy and minimize their regret over a finite time horizon through interacting with the environment. We devise a simple and efficient model-based algorithm that achieves $\tilde{O}(LX\sqrt{TA})$ regret with high probability, where $L$ is the episode length, $T$ is the number of episodes, and $X$ and $A$ are the cardinalities of the state space and the action space, respectively. The proposed algorithm, which is based on the concept of "optimism in the face of uncertainty", maintains confidence sets of transition and reward functions and uses occupancy measures to connect the online MDP with linear programming. It achieves a tighter regret bound compared to the existing works that use a similar confidence sets framework and improves the computational effort compared
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;E&#65288;3&#65289;&#21516;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#23545;&#20110;&#38750;&#21516;&#21464;&#32593;&#32476;&#26377;&#26356;&#39640;&#23398;&#20064;&#21160;&#24577;&#20132;&#20114;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#31890;&#23376;&#30340;&#27969;&#20307;&#21147;&#23398;&#20013;&#25552;&#20379;&#26356;&#21152;&#29289;&#29702;&#20934;&#30830;&#30340;&#20132;&#20114;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00150</link><description>&lt;p&gt;
E&#65288;3&#65289;&#21516;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20110;&#31890;&#23376;&#30340;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
E($3$) Equivariant Graph Neural Networks for Particle-Based Fluid Mechanics. (arXiv:2304.00150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;E&#65288;3&#65289;&#21516;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#23545;&#20110;&#38750;&#21516;&#21464;&#32593;&#32476;&#26377;&#26356;&#39640;&#23398;&#20064;&#21160;&#24577;&#20132;&#20114;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#31890;&#23376;&#30340;&#27969;&#20307;&#21147;&#23398;&#20013;&#25552;&#20379;&#26356;&#21152;&#29289;&#29702;&#20934;&#30830;&#30340;&#20132;&#20114;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#21516;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#23545;&#20110;&#38750;&#21516;&#21464;&#32593;&#32476;&#26377;&#26356;&#39640;&#23398;&#20064;&#21160;&#24577;&#20132;&#20114;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#24037;&#31243;&#31995;&#32479;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#20256;&#32479;&#30340;&#27969;&#20307;&#27969;&#21160;&#31995;&#32479;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21363;&#19977;&#32500;&#34928;&#20943;&#30340;Taylor-Green&#28065;&#26059;&#21644;&#19977;&#32500;&#21453;Poiseuille&#27969;&#65292;&#24182;&#22312;&#19981;&#21516;&#24615;&#33021;&#24230;&#37327;&#65288;&#22914;&#21160;&#33021;&#25110;Sinkhorn&#36317;&#31163;&#65289;&#19978;&#23558;&#21516;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#38750;&#21516;&#21464;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#20123;&#24230;&#37327;&#36890;&#24120;&#29992;&#20110;&#39564;&#35777;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#36895;&#24230;&#32531;&#24930;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20027;&#35201;&#21457;&#29616;&#21516;&#21464;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#21152;&#29289;&#29702;&#20934;&#30830;&#30340;&#20132;&#20114;&#12290;&#36825;&#34920;&#26126;&#20102;&#26410;&#26469;&#30740;&#31350;&#22312;&#31895;&#31890;&#24230;&#27169;&#22411;&#19978;&#23545;&#28237;&#27969;&#27969;&#21160;&#30340;&#30740;&#31350;&#26426;&#20250;&#65292;&#20197;&#21450;&#22312;&#31995;&#32479;&#21160;&#24577;&#21644;&#21442;&#25968;&#19978;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to the vastly growing field of machine learning for engineering systems by demonstrating that equivariant graph neural networks have the potential to learn more accurate dynamic-interaction models than their non-equivariant counterparts. We benchmark two well-studied fluid flow systems, namely the 3D decaying Taylor-Green vortex and the 3D reverse Poiseuille flow, and compare equivariant graph neural networks to their non-equivariant counterparts on different performance measures, such as kinetic energy or Sinkhorn distance. Such measures are typically used in engineering to validate numerical solvers. Our main findings are that while being rather slow to train and evaluate, equivariant models learn more physically accurate interactions. This indicates opportunities for future work towards coarse-grained models for turbulent flows, and generalization across system dynamics and parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#32463;&#20856;&#25968;&#20540;&#26041;&#27861;&#22312;&#29289;&#29702;&#31995;&#32479;&#27169;&#25311;&#19978;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#28508;&#21147;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#26032;&#30340;&#27169;&#25311;&#26041;&#27861;&#12290;&#23637;&#26395;&#36825;&#20123;&#26041;&#27861;&#23558;&#20026;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00146</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29289;&#29702;&#31995;&#32479;&#27169;&#25311;&#21644;&#32463;&#20856;&#25968;&#20540;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Relationships between Graph Neural Networks for the Simulation of Physical Systems and Classical Numerical Methods. (arXiv:2304.00146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#32463;&#20856;&#25968;&#20540;&#26041;&#27861;&#22312;&#29289;&#29702;&#31995;&#32479;&#27169;&#25311;&#19978;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#28508;&#21147;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#26032;&#30340;&#27169;&#25311;&#26041;&#27861;&#12290;&#23637;&#26395;&#36825;&#20123;&#26041;&#27861;&#23558;&#20026;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#29289;&#29702;&#31995;&#32479;&#30340;&#26041;&#27861;&#24320;&#22987;&#21576;&#29616;&#20986;&#35745;&#31639;&#31185;&#23398;&#20013;&#25968;&#20540;&#26041;&#27861;&#21457;&#23637;&#30340;&#24433;&#23376;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#23545;&#27604;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#29289;&#29702;&#27169;&#25311;&#21644;&#22522;&#20110;&#31890;&#23376;&#26041;&#27861;&#30340;&#21457;&#23637;&#36712;&#36857;&#26469;&#38416;&#36848;&#36825;&#19968;&#28857;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20123;&#27169;&#25311;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23578;&#26410;&#34987;&#36816;&#29992;&#21040;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#20013;&#65292;&#20294;&#23427;&#20204;&#20855;&#22791;&#35753;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#36825;&#20123;&#26041;&#27861;&#23558;&#20026;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#30340;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#28508;&#22312;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in Machine Learning approaches for modelling physical systems have begun to mirror the past development of numerical methods in the computational sciences. In this survey, we begin by providing an example of this with the parallels between the development trajectories of graph neural network acceleration for physical simulations and particle-based approaches. We then give an overview of simulation approaches, which have not yet found their way into state-of-the-art Machine Learning methods and hold the potential to make Machine Learning approaches more accurate and more efficient. We conclude by presenting an outlook on the potential of these approaches for making Machine Learning models for science more efficient.
&lt;/p&gt;</description></item><item><title>DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00133</link><description>&lt;p&gt;
DeforestVis&#65306;&#20351;&#29992;&#20195;&#29702;&#20915;&#31574;&#26641;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00133
&lt;/p&gt;
&lt;p&gt;
DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#65288;&#21644;&#20851;&#38190;&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#22686;&#21152;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#26356;&#26131;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;ML&#12290;&#35299;&#37322;&#22797;&#26434;ML&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#19988;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65288;&#20363;&#22914;&#35268;&#21017;&#38598;&#21644;&#20915;&#31574;&#26641;&#65289;&#65292;&#20197;&#36275;&#22815;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#65292;&#20294;&#26356;&#31616;&#21333;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#35268;&#21017;&#38598;&#21487;&#20197;&#21464;&#24471;&#38750;&#24120;&#20887;&#38271;&#65292;&#21253;&#21547;&#35768;&#22810;if-else&#35821;&#21477;&#65292;&#32780;&#20915;&#31574;&#26641;&#30340;&#28145;&#24230;&#20250;&#38543;&#30528;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;ML&#27169;&#22411;&#32780;&#36805;&#36895;&#22686;&#21152;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#20854;&#26680;&#24515;&#30446;&#26631;&#65292;&#25552;&#20379;&#29992;&#25143;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;DeforestVis&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20351;&#29992;&#33258;&#36866;&#24212;&#22686;&#24378;&#65288;AdaBoost&#65289;&#25216;&#26415;&#29983;&#25104;&#30340;&#20195;&#29702;&#20915;&#31574;&#26641;&#65288;&#19968;&#32423;&#20915;&#31574;&#26641;&#65289;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#23545;&#22797;&#26434;ML&#27169;&#22411;&#34892;&#20026;&#30340;&#21451;&#22909;&#24635;&#32467;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity of machine learning (ML) models increases and the applications in different (and critical) domains grow, there is a strong demand for more interpretable and trustworthy ML. One straightforward and model-agnostic way to interpret complex ML models is to train surrogate models, such as rule sets and decision trees, that sufficiently approximate the original ones while being simpler and easier-to-explain. Yet, rule sets can become very lengthy, with many if-else statements, and decision tree depth grows rapidly when accurately emulating complex ML models. In such cases, both approaches can fail to meet their core goal, providing users with model interpretability. We tackle this by proposing DeforestVis, a visual analytics tool that offers user-friendly summarization of the behavior of complex ML models by providing surrogate decision stumps (one-level decision trees) generated with the adaptive boosting (AdaBoost) technique. Our solution helps users to explore the comple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#30005;&#21147;&#37197;&#30005;&#31995;&#32479;&#20013;&#23454;&#26102;$\mu$PMU&#27979;&#37327;&#25968;&#25454;&#20013;&#20107;&#20214;&#26816;&#27979;&#65292;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#20102;&#22522;&#20110;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;&#31995;&#32479;&#26469;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.00092</link><description>&lt;p&gt;
DynamoPMU&#65306;&#20351;&#29992;$\mu$PMU&#27979;&#37327;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#36827;&#34892;&#29289;&#29702;&#20449;&#24687;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DynamoPMU: A Physics Informed Anomaly Detection and Prediction Methodology using non-linear dynamics from $\mu$PMU Measurement Data. (arXiv:2304.00092v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#30005;&#21147;&#37197;&#30005;&#31995;&#32479;&#20013;&#23454;&#26102;$\mu$PMU&#27979;&#37327;&#25968;&#25454;&#20013;&#20107;&#20214;&#26816;&#27979;&#65292;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#20102;&#22522;&#20110;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;&#31995;&#32479;&#26469;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#21644;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#25193;&#22823;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#23454;&#26102;&#27969;&#25968;&#25454;&#12290; &#30005;&#21147;&#37197;&#30005;&#31995;&#32479;&#20013;&#30340;&#23454;&#26102;&#25968;&#25454;&#36890;&#36807;&#31216;&#20026;$\mu$PMU&#30340;&#20998;&#37197;&#32423;&#30456;&#37327;&#27979;&#37327;&#21333;&#20803;&#25910;&#38598;&#65292;&#25253;&#21578;&#39640;&#20998;&#36776;&#29575;&#30340;&#30456;&#37327;&#27979;&#37327;&#65292;&#21253;&#25324;&#21508;&#31181;&#20107;&#20214;&#29305;&#24449;&#65292;&#25552;&#20379;&#24773;&#22659;&#24863;&#30693;&#24182;&#20351;&#37197;&#30005;&#31995;&#32479;&#20013;&#21487;&#35265;&#12290;&#36825;&#20123;&#20107;&#20214;&#26159;&#19981;&#39057;&#32321;&#65292;&#19981;&#23450;&#26102;&#21644;&#19981;&#30830;&#23450;&#30340;&#65307; &#23427;&#26159;&#19968;&#20010;&#25361;&#25112;&#26469;&#20180;&#32454;&#30740;&#31350;&#65292;&#26816;&#27979;&#21644;&#39044;&#27979;&#27492;&#31867;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#23545;&#20110;&#30005;&#21147;&#20998;&#37197;&#31995;&#32479;&#65292;&#26126;&#30830;&#35782;&#21035;&#25551;&#36848;&#20107;&#20214;&#30340;&#22797;&#26434;&#65292;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#31614;&#21517;&#27169;&#24335;&#30340;&#28436;&#21270;&#20989;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#29289;&#29702;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;$\mu$PMU&#27969;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24182;&#21516;&#26102;&#20351;&#29992;&#25511;&#21046;&#26041;&#31243;&#39044;&#27979;&#20107;&#20214;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#30340;&#26159;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#26469;&#25551;&#36848;&#31995;&#32479;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#20107;&#20214;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expansion in technology and attainability of a large number of sensors has led to a huge amount of real-time streaming data. The real-time data in the electrical distribution system is collected through distribution-level phasor measurement units referred to as $\mu$PMU which report high-resolution phasor measurements comprising various event signatures which provide situational awareness and enable a level of visibility into the distribution system. These events are infrequent, unschedule, and uncertain; it is a challenge to scrutinize, detect and predict the occurrence of such events. For electrical distribution systems, it is challenging to explicitly identify evolution functions that describe the complex, non-linear, and non-stationary signature patterns of events. In this paper, we seek to address this problem by developing a physics dynamics-based approach to detect anomalies in the $\mu$PMU streaming data and simultaneously predict the events using governing equations. We pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#36873;&#32508;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.00086</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32463;&#27982;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65306;&#20309;&#26102;&#12289;&#20160;&#20040;&#21644;&#22914;&#20309;&#36816;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Economics Research: When What and How?. (arXiv:2304.00086v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#36873;&#32508;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#37325;&#35201;&#32463;&#27982;&#26399;&#21002;&#19978;&#21457;&#34920;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#31934;&#36873;&#32508;&#36848;&#12290;&#32508;&#36848;&#22238;&#31572;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20309;&#26102;&#22312;&#32463;&#27982;&#23398;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#65288;2&#65289;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#20160;&#20040;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#23558;&#23427;&#20204;&#29992;&#20110;&#32463;&#27982;&#24212;&#29992;&#12290;&#32508;&#36848;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#38750;&#20256;&#32479;&#25968;&#25454;&#65292;&#32780;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#20256;&#32479;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#35745;&#37327;&#32463;&#27982;&#23398;&#27169;&#22411;&#22312;&#20998;&#26512;&#20302;&#22797;&#26434;&#24615;&#25968;&#25454;&#26102;&#21487;&#33021;&#36275;&#22815;&#65292;&#20294;&#30001;&#20110;&#24555;&#36895;&#25968;&#23383;&#21270;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#32463;&#27982;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27491;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used in processing nontraditional and unstructured data, capturing strong nonlinearity, and improving prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggest that ML is becoming an essential addition to the econometrician's toolbox.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#21160;&#24577;&#33976;&#39311;&#24182;&#20248;&#21270;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#23545;&#24212;&#30340;&#26381;&#21153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.00083</link><description>&lt;p&gt;
Fides&#65306;&#19968;&#31181;&#21033;&#29992;&#23433;&#20840;&#25191;&#34892;&#29615;&#22659;&#23545;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#32467;&#26524;&#39564;&#35777;&#30340;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (arXiv:2304.00083v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#21160;&#24577;&#33976;&#39311;&#24182;&#20248;&#21270;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#23545;&#24212;&#30340;&#26381;&#21153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25935;&#24863;&#39046;&#22495;&#30340;&#37096;&#32626;&#23548;&#33268;&#20102;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#37325;&#35270;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22810;&#26041;&#35745;&#31639;&#21644;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#32473;&#23454;&#26102;&#24212;&#29992;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#65292;&#20854;&#20013;&#37319;&#29992;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#19968;&#31181;&#23454;&#26102;&#39564;&#35777;&#27169;&#22411;&#26469;&#36739;&#23569;&#22320;&#28040;&#32791;&#31354;&#38388;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#36816;&#34892;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as autonomous driving, integrity verification of the outsourced ML workload is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time validation of outsourced ML workloads. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergenc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27719;&#24635;&#20102;50&#31687;&#25991;&#29486;&#23545;&#20110;&#26500;&#24314;&#24102;&#26377;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#20135;&#21697;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#21644;&#25945;&#32946;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.00078</link><description>&lt;p&gt;
&#24314;&#31435;&#20855;&#26377;ML&#32452;&#20214;&#30340;&#20135;&#21697;&#20013;&#30340;&#25361;&#25112;&#20803;&#24635;&#32467;&#8212;&#8212;&#20174;4758&#20301;&#23454;&#36341;&#32773;&#37027;&#37324;&#27719;&#38598;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
A Meta-Summary of Challenges in Building Products with ML Components -- Collecting Experiences from 4758+ Practitioners. (arXiv:2304.00078v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27719;&#24635;&#20102;50&#31687;&#25991;&#29486;&#23545;&#20110;&#26500;&#24314;&#24102;&#26377;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#20135;&#21697;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#21644;&#25945;&#32946;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#32452;&#20214;&#32435;&#20837;&#36719;&#20214;&#20135;&#21697;&#20013;&#24341;&#21457;&#20102;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#25361;&#25112;&#24182;&#21152;&#21095;&#20102;&#29616;&#26377;&#25361;&#25112;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#37319;&#35775;&#21644;&#35843;&#26597;&#23454;&#36341;&#32773;&#65292;&#25237;&#20837;&#20102;&#22823;&#37327;&#31934;&#21147;&#26469;&#20102;&#35299;&#26500;&#24314;&#20855;&#26377;ML&#32452;&#20214;&#30340;&#34892;&#19994;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#27719;&#24635;&#21644;&#21576;&#29616;&#20182;&#20204;&#30340;&#20849;&#21516;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20803;&#24635;&#32467;&#30740;&#31350;&#65306;&#25105;&#20204;&#25910;&#38598;&#20102;50&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#36825;&#20123;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#25351;&#21335;&#19982;&#36229;&#36807;4758&#20301;&#23454;&#36341;&#32773;&#20114;&#21160;&#12290;&#28982;&#21518;&#25105;&#20204;&#25910;&#38598;&#12289;&#20998;&#32452;&#21644;&#32452;&#32455;&#20102;&#36825;&#20123;&#35770;&#25991;&#20013;&#25552;&#21040;&#30340;500&#22810;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26368;&#24120;&#25253;&#21578;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#36825;&#20010;&#20803;&#24635;&#32467;&#33021;&#25104;&#20026;&#30740;&#31350;&#30028;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20248;&#20808;&#36827;&#34892;&#30740;&#31350;&#21644;&#25945;&#32946;&#30340;&#26377;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating machine learning (ML) components into software products raises new software-engineering challenges and exacerbates existing challenges. Many researchers have invested significant effort in understanding the challenges of industry practitioners working on building products with ML components, through interviews and surveys with practitioners. With the intention to aggregate and present their collective findings, we conduct a meta-summary study: We collect 50 relevant papers that together interacted with over 4758 practitioners using guidelines for systematic literature reviews. We then collected, grouped, and organized the over 500 mentions of challenges within those papers. We highlight the most commonly reported challenges and hope this meta-summary will be a useful resource for the research community to prioritize research and education in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20027;&#21160;&#38598;&#23398;&#20064;&#25216;&#26415;&#21644;&#29289;&#29702;&#32422;&#26463;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#26368;&#20248;&#30005;&#21147;&#27969;&#38382;&#39064;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#29305;&#21035;&#32771;&#34385;&#21040;&#20102;&#36127;&#33655;&#21066;&#20943;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#21066;&#20943;&#31561;&#29616;&#23454;&#19990;&#30028;&#30005;&#21147;&#31995;&#32479;&#25361;&#25112;&#65292;&#20174;&#32780;&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#24066;&#22330;&#28165;&#31639;&#32467;&#26524;&#22312;&#29289;&#29702;&#21644;&#32463;&#27982;&#19978;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.00062</link><description>&lt;p&gt;
&#30005;&#21147;&#24066;&#22330;&#20013;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#20197;NYISO&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Physics-Informed Machine Learning for Electricity Markets: A NYISO Case Study. (arXiv:2304.00062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20027;&#21160;&#38598;&#23398;&#20064;&#25216;&#26415;&#21644;&#29289;&#29702;&#32422;&#26463;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#26368;&#20248;&#30005;&#21147;&#27969;&#38382;&#39064;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#29305;&#21035;&#32771;&#34385;&#21040;&#20102;&#36127;&#33655;&#21066;&#20943;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#21066;&#20943;&#31561;&#29616;&#23454;&#19990;&#30028;&#30005;&#21147;&#31995;&#32479;&#25361;&#25112;&#65292;&#20174;&#32780;&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#24066;&#22330;&#28165;&#31639;&#32467;&#26524;&#22312;&#29289;&#29702;&#21644;&#32463;&#27982;&#19978;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#23454;&#26102;&#30005;&#21147;&#24066;&#22330;&#20013;&#39640;&#25928;&#35299;&#20915;&#26368;&#20248;&#30005;&#21147;&#27969;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21517;&#20026;&#22522;&#20110;&#29289;&#29702;&#30340;&#24066;&#22330;&#24863;&#30693;&#20027;&#21160;&#38598;&#23398;&#20064;OPF(PIMA-AS-OPF)&#65292;&#21033;&#29992;&#29289;&#29702;&#32422;&#26463;&#21644;&#24066;&#22330;&#29305;&#24615;&#30830;&#20445;&#24066;&#22330;&#28165;&#31639;&#32467;&#26524;&#30340;&#29289;&#29702;&#21644;&#32463;&#27982;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PIMA-AS-OPF&#37319;&#29992;&#20027;&#21160;&#38598;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25193;&#23637;&#20854;&#21151;&#33021;&#20197;&#32771;&#34385;&#36127;&#33655;&#25110;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#21066;&#20943;&#65292;&#36825;&#26159;&#29616;&#23454;&#19990;&#30028;&#30005;&#21147;&#31995;&#32479;&#20013;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;PIMA-AS-OPF&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#36755;&#20837;&#20026;&#20928;&#36127;&#33655;&#21644;&#31995;&#32479;&#25299;&#25169;&#12290;&#35813;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#21253;&#25324;&#39281;&#21644;&#21457;&#30005;&#26426;&#21644;&#36755;&#30005;&#32447;&#31561;&#20027;&#21160;&#32422;&#26463;&#65292;&#20197;&#21450;&#38750;&#38646;&#36127;&#33655;&#21066;&#20943;&#21644;&#39118;&#30005;&#21066;&#20943;&#12290;&#36825;&#20123;&#36755;&#20986;&#21487;&#23558;&#21407;&#22987;&#24066;&#22330;&#28165;&#31639;&#20248;&#21270;&#38477;&#33267;&#19968;&#31995;&#21015;&#32447;&#24615;&#26041;&#31243;&#32452;&#65292;&#21487;&#39640;&#25928;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of efficiently solving the optimal power flow problem in real-time electricity markets. The proposed solution, named Physics-Informed Market-Aware Active Set learning OPF (PIMA-AS-OPF), leverages physical constraints and market properties to ensure physical and economic feasibility of market-clearing outcomes. Specifically, PIMA-AS-OPF employs the active set learning technique and expands its capabilities to account for curtailment in load or renewable power generation, which is a common challenge in real-world power systems. The core of PIMA-AS-OPF is a fully-connected neural network that takes the net load and the system topology as input. The outputs of this neural network include active constraints such as saturated generators and transmission lines, as well as non-zero load shedding and wind curtailments. These outputs allow for reducing the original market-clearing optimization to a system of linear equations, which can be solved efficiently and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#25351;&#26631;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20004;&#20010;&#25351;&#26631;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#20114;&#30456;&#21463;&#30410;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.00061</link><description>&lt;p&gt;
&#22362;&#22266;&#19988;&#20844;&#27491;: &#30830;&#20445;&#20844;&#24179;&#19982;&#40065;&#26834;&#24615;&#30456;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
To be Robust and to be Fair: Aligning Fairness with Robustness. (arXiv:2304.00061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#25351;&#26631;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20004;&#20010;&#25351;&#26631;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#20114;&#30456;&#21463;&#30410;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#21487;&#38752;&#22320;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23601;&#20844;&#24179;&#24615;&#32780;&#35328;&#65292;&#23545;&#25239;&#35757;&#32451;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#36866;&#24403;&#30740;&#31350;&#65292;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#23545;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#40065;&#26834;&#24615;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#21644;&#23545;&#25239;&#25915;&#20987;&#23545;&#36825;&#20004;&#20010;&#25351;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#25915;&#20987;&#30340;&#32479;&#19968;&#32467;&#26500;&#65292;&#23558;&#32676;&#20307;&#20844;&#24179;&#20013;&#30340;&#24120;&#35265;&#27010;&#24565;&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19981;&#21516;&#27010;&#24565;&#19979;&#30340;&#20844;&#24179;&#24615;&#25915;&#20987;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#25915;&#20987;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19968;&#31181;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#20250;&#21463;&#21040;&#21478;&#19968;&#31181;&#25351;&#26631;&#40065;&#26834;&#24615;&#30340;&#30410;&#22788;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#20844;&#24179;&#19982;&#20934;&#30830;&#24615;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been shown to be reliable in improving robustness against adversarial samples. However, the problem of adversarial training in terms of fairness has not yet been properly studied, and the relationship between fairness and accuracy attack still remains unclear. Can we simultaneously improve robustness w.r.t. both fairness and accuracy? To tackle this topic, in this paper, we study the problem of adversarial training and adversarial attack w.r.t. both metrics. We propose a unified structure for fairness attack which brings together common notions in group fairness, and we theoretically prove the equivalence of fairness attack against different notions. Moreover, we show the alignment of fairness and accuracy attack, and theoretically demonstrate that robustness w.r.t. one metric benefits from robustness w.r.t. the other metric. Our study suggests a novel way to unify adversarial training and attack w.r.t. fairness and accuracy, and experimental results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#32447;&#24615;&#12289;&#24120;&#25968;&#22240;&#23376;&#33609;&#22270;&#65292;&#36866;&#29992;&#20110;$\ell_1$&#21644;logistic&#22238;&#24402;&#65292;&#20855;&#26377;&#23567;&#30340;&#33609;&#22270;&#32500;&#24230;&#21644;&#39640;&#31934;&#24230;&#65292;&#36825;&#31181;&#33609;&#22270;&#36824;&#22312;&#33609;&#22270;&#31354;&#38388;&#20869;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.00051</link><description>&lt;p&gt;
$\ell_1$&#21644;logistic&#22238;&#24402;&#30340;&#36817;&#32447;&#24615;&#24120;&#25968;&#22240;&#23376;&#33609;&#22270;
&lt;/p&gt;
&lt;p&gt;
Almost Linear Constant-Factor Sketching for $\ell_1$ and Logistic Regression. (arXiv:2304.00051v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#32447;&#24615;&#12289;&#24120;&#25968;&#22240;&#23376;&#33609;&#22270;&#65292;&#36866;&#29992;&#20110;$\ell_1$&#21644;logistic&#22238;&#24402;&#65292;&#20855;&#26377;&#23567;&#30340;&#33609;&#22270;&#32500;&#24230;&#21644;&#39640;&#31934;&#24230;&#65292;&#36825;&#31181;&#33609;&#22270;&#36824;&#22312;&#33609;&#22270;&#31354;&#38388;&#20869;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25913;&#36827;&#20102;&#20197;&#21069;&#20851;&#20110;$\ell_1$&#21644;Logistic&#22238;&#24402;&#30340;&#33609;&#22270;&#31639;&#27861;&#32467;&#26524;&#65292;&#24471;&#21040;&#20102;&#26356;&#23567;&#30340;&#33609;&#22270;&#32500;&#24230;&#21644;&#26356;&#39640;&#30340;&#31934;&#24230;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#33609;&#22270;&#31354;&#38388;&#20869;&#20135;&#29983;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;&#20110;&#20219;&#20309;&#24120;&#25968;$c&gt;0$&#65292;&#23454;&#29616;&#20102;$\ell_1$&#22238;&#24402;&#30340;&#33609;&#22270;&#32500;&#24230;&#20026;$\tilde{O}(d^{1+c})$&#65292;&#32780;&#23545;&#20110;Logistic&#22238;&#24402;&#21017;&#20026;$\tilde{O}(\mu d^{1+c})$&#65292;&#20854;&#20013;$\mu$&#26159;&#19968;&#20010;&#26631;&#20934;&#30340;&#24230;&#37327;&#65292;&#25429;&#33719;&#20102;&#21387;&#32553;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#23545;&#20110;$\ell_1$&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#33609;&#22270;&#32500;&#24230;&#26159;&#36817;&#32447;&#24615;&#30340;&#65292;&#20855;&#26377;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#23567;&#30340;&#33609;&#22270;&#32500;&#24230;&#12290;&#31867;&#20284;&#22320;&#65292;&#23545;&#20110;Logistic&#22238;&#24402;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#22312;&#20854;&#33609;&#22270;&#32500;&#24230;&#19978;&#26377;&#26356;&#24046;&#30340;$\operatorname{poly}(\mu d)$&#22240;&#23376;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#25240;&#34935;&#26041;&#26696;&#65292;&#36890;&#36807;&#22686;&#21152;&#24635;&#22823;&#23567;&#21040;$(d\log$&#65292;&#22312;&#36755;&#20837;&#31232;&#30095;&#24615;&#26102;&#38388;&#20869;&#20135;&#29983;&#20102;$1+\varepsilon$&#30340;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We improve upon previous oblivious sketching and turnstile streaming results for $\ell_1$ and logistic regression, giving a much smaller sketching dimension achieving $O(1)$-approximation and yielding an efficient optimization problem in the sketch space. Namely, we achieve for any constant $c&gt;0$ a sketching dimension of $\tilde{O}(d^{1+c})$ for $\ell_1$ regression and $\tilde{O}(\mu d^{1+c})$ for logistic regression, where $\mu$ is a standard measure that captures the complexity of compressing the data. For $\ell_1$-regression our sketching dimension is near-linear and improves previous work which either required $\Omega(\log d)$-approximation with this sketching dimension, or required a larger $\operatorname{poly}(d)$ number of rows. Similarly, for logistic regression previous work had worse $\operatorname{poly}(\mu d)$ factors in its sketching dimension. We also give a tradeoff that yields a $1+\varepsilon$ approximation in input sparsity time by increasing the total size to $(d\log
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#27491;&#21017;&#21270;&#65288;RankReg&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#39640;&#30495;&#38451;&#24615;&#29575;&#19979;&#26368;&#23567;&#21270;&#20551;&#38451;&#24615;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;&#23427;&#19981;&#20165;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;&#20551;&#38451;&#24615;&#65292;&#32780;&#19988;&#36824;&#19982;&#20256;&#32479;&#30340;&#19981;&#24179;&#34913;&#23398;&#20064;&#25439;&#22833;&#30456;&#36741;&#30456;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.00049</link><description>&lt;p&gt;
&#38024;&#23545;&#20851;&#38190;&#31232;&#26377;&#31867;&#21035;&#30340;&#25490;&#24207;&#27491;&#21017;&#21270;&#65306;&#22312;&#39640;&#30495;&#38451;&#29575;&#19979;&#26368;&#23567;&#21270;&#20551;&#38451;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate. (arXiv:2304.00049v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#27491;&#21017;&#21270;&#65288;RankReg&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#39640;&#30495;&#38451;&#24615;&#29575;&#19979;&#26368;&#23567;&#21270;&#20551;&#38451;&#24615;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;&#23427;&#19981;&#20165;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;&#20551;&#38451;&#24615;&#65292;&#32780;&#19988;&#36824;&#19982;&#20256;&#32479;&#30340;&#19981;&#24179;&#34913;&#23398;&#20064;&#25439;&#22833;&#30456;&#36741;&#30456;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20851;&#38190;&#31867;&#21035;&#24448;&#24448;&#26159;&#31232;&#26377;&#30340;&#65292;&#28431;&#26816;&#20250;&#24102;&#26469;&#19981;&#25104;&#27604;&#20363;&#30340;&#39640;&#20195;&#20215;&#12290;&#20363;&#22914;&#65292;&#32959;&#30244;&#26159;&#32597;&#35265;&#30340;&#65292;&#35823;&#35786;&#20250;&#23545;&#27835;&#30103;&#32467;&#26524;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#65307;&#27450;&#35784;&#38134;&#34892;&#20132;&#26131;&#26159;&#32597;&#35265;&#30340;&#65292;&#26410;&#34987;&#21457;&#29616;&#30340;&#24773;&#20917;&#21487;&#33021;&#23548;&#33268;&#24040;&#22823;&#30340;&#25439;&#22833;&#25110;&#27861;&#24459;&#24809;&#32602;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31995;&#32479;&#36890;&#24120;&#20197;&#36739;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#36816;&#34892;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#23481;&#24525;&#36739;&#39640;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38656;&#35201;&#22312;&#39640;&#30495;&#38451;&#24615;&#29575;&#19979;&#36816;&#34892;&#30340;&#31995;&#32479;&#26368;&#23567;&#21270;&#20551;&#38451;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#27491;&#21017;&#21270;&#65288;RankReg&#65289;&#26041;&#27861;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;&#23427;&#19981;&#20165;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;&#20551;&#38451;&#24615;&#65292;&#32780;&#19988;&#36824;&#19982;&#20256;&#32479;&#30340;&#19981;&#24179;&#34913;&#23398;&#20064;&#25439;&#22833;&#30456;&#36741;&#30456;&#25104;&#12290;&#20351;&#29992;&#36825;&#31181;&#26032;&#39062;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#24191;&#27867;&#25506;&#32034;&#30340;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#65286;100&#21644;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#65289;&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world settings, the critical class is rare and a missed detection carries a disproportionately high cost. For example, tumors are rare and a false negative diagnosis could have severe consequences on treatment outcomes; fraudulent banking transactions are rare and an undetected occurrence could result in significant losses or legal penalties. In such contexts, systems are often operated at a high true positive rate, which may require tolerating high false positives. In this paper, we present a novel approach to address the challenge of minimizing false positives for systems that need to operate at a high true positive rate. We propose a ranking-based regularization (RankReg) approach that is easy to implement, and show empirically that it not only effectively reduces false positives, but also complements conventional imbalanced learning losses. With this novel technique in hand, we conduct a series of experiments on three broadly explored datasets (CIFAR-10&amp;100 and Melanom
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#20998;&#21035;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#36741;&#21161;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#21892;&#25506;&#32034;&#21644;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; NetHack &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.00046</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#32447;&#39044;&#35757;&#32451;&#21152;&#36895;&#25506;&#32034;&#21644;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating exploration and representation learning with offline pre-training. (arXiv:2304.00046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#20998;&#21035;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#36741;&#21161;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#21892;&#25506;&#32034;&#21644;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; NetHack &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20018;&#34892;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25165;&#33021;&#35299;&#20915;&#12290;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#36807;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#65292;&#24341;&#20837;&#35760;&#24518;&#33021;&#21147;&#65292;&#25913;&#21464;&#20195;&#29702;&#30340;&#20869;&#22312;&#21160;&#26426;&#65288;&#21363;&#25506;&#32034;&#65289;&#25110;&#20854;&#19990;&#30028;&#35266;&#65288;&#21363;&#30693;&#35782;&#34920;&#31034;&#65289;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#20013;&#30340;&#35768;&#22810;&#37117;&#21487;&#20197;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#36890;&#36807;&#20174;&#21333;&#20010;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#20998;&#21035;&#23398;&#20064;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#21892;&#25506;&#32034;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#20197;&#21450;&#20174;&#21333;&#20010;&#20154;&#31867;&#28436;&#31034;&#30340;&#27169;&#22411;&#36741;&#21161;&#22870;&#21169;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; NetHack &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#28040;&#34701;&#20102;&#23454;&#39564;&#35774;&#32622;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#24182;&#31361;&#26174;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision-making agents struggle with long horizon tasks, since solving them requires multi-step reasoning. Most reinforcement learning (RL) algorithms address this challenge by improved credit assignment, introducing memory capability, altering the agent's intrinsic motivation (i.e. exploration) or its worldview (i.e. knowledge representation). Many of these components could be learned from offline data. In this work, we follow the hypothesis that exploration and representation learning can be improved by separately learning two different models from a single offline dataset. We show that learning a state representation using noise-contrastive estimation and a model of auxiliary reward separately from a single collection of human demonstrations can significantly improve the sample efficiency on the challenging NetHack benchmark. We also ablate various components of our experimental setting and highlight crucial insights.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#27169;&#22411;&#21644;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#30340;Dropout&#26426;&#21046;&#30340;&#40065;&#26834;&#26041;&#27861;&#65292;&#29992;&#20110;SHM&#20013;&#30340;&#25439;&#20260;&#35782;&#21035;&#65292;&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2304.00040</link><description>&lt;p&gt;
&#19968;&#31181;&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#30340;SHM&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#25439;&#20260;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A robust deep learning-based damage identification approach for SHM considering missing data. (arXiv:2304.00040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#27169;&#22411;&#21644;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#30340;Dropout&#26426;&#21046;&#30340;&#40065;&#26834;&#26041;&#27861;&#65292;&#29992;&#20110;SHM&#20013;&#30340;&#25439;&#20260;&#35782;&#21035;&#65292;&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#26041;&#27861;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#30417;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#20013;&#25366;&#25496;&#38544;&#34255;&#30340;&#32467;&#26500;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#27979;&#25968;&#25454;&#20013;&#65292;&#32463;&#24120;&#36935;&#21040;&#32570;&#22833;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36825;&#20250;&#23545;&#25968;&#25454;&#25366;&#25496;&#21644;&#21518;&#32493;&#20219;&#21153;&#65288;&#22914;&#26465;&#20214;&#35780;&#20272;&#65289;&#20135;&#29983;&#20005;&#37325;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Long-Short Term Memory&#65288;LSTM&#65289;&#27169;&#22411;&#21644;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#26694;&#26550;&#20013;&#30340;Dropout&#26426;&#21046;&#65292;&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#65292;&#36827;&#34892;&#25439;&#20260;&#35782;&#21035;&#30340;&#40065;&#26834;&#26041;&#27861;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#38543;&#26426;&#21024;&#38500;&#36755;&#20837;&#36890;&#36947;&#65292;&#20197;&#27169;&#25311;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#23558;&#37325;&#26500;&#35823;&#24046;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven method for Structural Health Monitoring (SHM), that mine the hidden structural performance from the correlations among monitored time series data, has received widely concerns recently. However, missing data significantly impacts the conduction of this method. Missing data is a frequently encountered issue in time series data in SHM and many other real-world applications, that harms to the standardized data mining and downstream tasks, such as condition assessment. Imputation approaches based on spatiotemporal relations among monitoring data are developed to handle this issue, however, no additional information is added during imputation. This paper thus develops a robust method for damage identification that considers the missing data occasions, based on long-short term memory (LSTM) model and dropout mechanism in the autoencoder (AE) framework. Inputs channels are randomly dropped to simulate the missing data in training, and reconstruction errors are used as the loss fun
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00020</link><description>&lt;p&gt;
SemiMemes&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;Memes&#20998;&#26512;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;Memes&#30340;&#26222;&#21450;&#24615;&#24341;&#21457;&#20102;&#20998;&#26512;&#20854;&#38544;&#21547;&#21547;&#20041;&#12289;&#23457;&#26597;&#26377;&#23475;&#20869;&#23481;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;Meme&#23457;&#26597;&#31995;&#32479;&#38656;&#35201;&#21322;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21033;&#29992;&#20114;&#32852;&#32593;&#19978;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;Memes&#65292;&#24182;&#20351;&#27880;&#37322;&#36807;&#31243;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#22240;&#20026;Memes&#30340;&#21547;&#20041;&#36890;&#24120;&#26469;&#33258;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;&#22810;&#23186;&#20307;&#33258;&#21160;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;Memes&#25968;&#25454;&#38598;&#19978;&#65292;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#27169;&#22411;&#12290;&#20511;&#37492;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SemiMemes&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#27861;&#30340;&#28145;&#24230;&#31070;&#32463;&#27491;&#21017;&#21270;&#22120;&#23478;&#26063;&#65292;&#20445;&#35777;&#21487;&#20197;&#36866;&#37197;&#25968;&#25454;&#24182;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;&#22312;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#23567;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#31561;&#38382;&#39064;&#19978;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2304.00015</link><description>&lt;p&gt;
DRIP: &#36870;&#38382;&#39064;&#30340;&#28145;&#24230;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
DRIP: Deep Regularizers for Inverse Problems. (arXiv:2304.00015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#27861;&#30340;&#28145;&#24230;&#31070;&#32463;&#27491;&#21017;&#21270;&#22120;&#23478;&#26063;&#65292;&#20445;&#35777;&#21487;&#20197;&#36866;&#37197;&#25968;&#25454;&#24182;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;&#22312;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#23567;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#31561;&#38382;&#39064;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#22312;&#25968;&#23398;&#19978;&#26159;&#19981;&#33391;&#23450;&#20041;&#30340;&#65292;&#22240;&#27492;&#23545;&#20110;&#19968;&#20123;&#65288;&#24102;&#26377;&#22122;&#22768;&#30340;&#65289;&#25968;&#25454;&#65292;&#21487;&#33021;&#20250;&#26377;&#19981;&#27490;&#19968;&#20010;&#19982;&#25968;&#25454;&#21305;&#37197;&#30340;&#35299;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20123;&#33021;&#22815;&#25214;&#21040;&#26368;&#21512;&#36866;&#35299;&#20915;&#26041;&#26696;&#30340;&#28145;&#24230;&#31070;&#32463;&#25216;&#26415;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65306;&#22823;&#22810;&#25968;&#25216;&#26415;&#26080;&#27861;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#22312;&#25512;&#29702;&#26102;&#21305;&#37197;&#25968;&#25454;&#65307;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#30340;&#25512;&#23548;&#26159;&#22522;&#20110;&#19968;&#20010;&#26377;&#25928;&#30340;&#26631;&#37327;&#27491;&#21017;&#21270;&#20989;&#25968;&#23384;&#22312;&#30340;&#22522;&#30784;&#20043;&#19978;&#65292;&#20294;&#22312;&#23454;&#38469;&#36816;&#29992;&#20013;&#36825;&#20123;&#25216;&#26415;&#24182;&#27809;&#26377;&#20381;&#36182;&#20110;&#36825;&#26679;&#19968;&#20010;&#20989;&#25968;&#65292;&#22240;&#27492;&#19982;&#20256;&#32479;&#30340;&#21464;&#20998;&#25216;&#26415;&#26377;&#25152;&#20559;&#31163;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#27491;&#21017;&#21270;&#22120;&#23478;&#26063;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#22120;&#22522;&#20110;&#21464;&#20998;&#24418;&#24335;&#65292;&#24182;&#20445;&#35777;&#36866;&#37197;&#25968;&#25454;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23427;&#20204;&#22312;&#19968;&#20123;&#39640;&#24230;ill-posed&#38382;&#39064;&#19978;&#30340;&#20351;&#29992;&#65292;&#21253;&#25324;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#23567;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems are mathematically ill-posed. Thus, given some (noisy) data, there is more than one solution that fits the data. In recent years, deep neural techniques that find the most appropriate solution, in the sense that it contains a-priori information, were developed. However, they suffer from several shortcomings. First, most techniques cannot guarantee that the solution fits the data at inference. Second, while the derivation of the techniques is inspired by the existence of a valid scalar regularization function, such techniques do not in practice rely on such a function, and therefore veer away from classical variational techniques. In this work we introduce a new family of neural regularizers for the solution of inverse problems. These regularizers are based on a variational formulation and are guaranteed to fit the data. We demonstrate their use on a number of highly ill-posed problems, from image deblurring to limited angle tomography.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21435;&#20559;&#32622;&#30340;&#22270;&#32467;&#26500;&#26080;&#30446;&#26631;&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#37322;&#25918;&#20302;&#25928;&#30340;&#25915;&#20987;&#39044;&#31639;&#12290;</title><link>http://arxiv.org/abs/2304.00010</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#21435;&#20559;&#32622;&#30340;&#26080;&#30446;&#26631;&#22270;&#32467;&#26500;&#25915;&#20987;&#20013;&#21512;&#29702;&#30340;&#39044;&#31639;&#37197;&#32622;
&lt;/p&gt;
&lt;p&gt;
Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias. (arXiv:2304.00010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21435;&#20559;&#32622;&#30340;&#22270;&#32467;&#26500;&#26080;&#30446;&#26631;&#25915;&#20987;&#27169;&#22411;&#65292;&#29992;&#20110;&#37322;&#25918;&#20302;&#25928;&#30340;&#25915;&#20987;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#24050;&#32463;&#25104;&#20026;&#35748;&#30693;&#24815;&#24615;&#12290;&#22312;&#22270;&#32467;&#26500;&#30340;&#26080;&#30446;&#26631;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#30446;&#26631;&#20135;&#29983;&#30340;&#26799;&#24230;&#26159;&#25915;&#20987;&#32773;&#35780;&#20272;&#25200;&#21160;&#26041;&#26696;&#30340;&#22522;&#30784;&#12290;&#36807;&#21435;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#33410;&#28857;&#32423;&#20998;&#31867;&#27169;&#22411;&#26102;&#20351;&#29992;&#36127;&#20132;&#21449;&#29109;&#25439;&#22833;&#20316;&#20026;&#25915;&#20987;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20132;&#21449;&#29109;&#20989;&#25968;&#26159;&#21542;&#36866;&#21512;&#26500;&#24314;&#26080;&#30446;&#26631;&#25915;&#20987;&#30446;&#26631;&#23578;&#26410;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#36827;&#34892;&#35752;&#35770;&#12290;&#26412;&#25991;&#20174;&#39044;&#31639;&#20998;&#37197;&#30340;&#35282;&#24230;&#65292;&#23545;&#20197;&#21069;&#19981;&#21512;&#29702;&#30340;&#25915;&#20987;&#30446;&#26631;&#36827;&#34892;&#20102;&#35770;&#35777;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#65292;&#36127;&#20132;&#21449;&#29109;&#24448;&#24448;&#20250;&#20174;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#33410;&#28857;&#20135;&#29983;&#26356;&#22823;&#30340;&#26799;&#24230;&#65292;&#22312;&#36825;&#20123;&#33410;&#28857;&#30340;&#39044;&#27979;&#31867;&#21035;&#34987;&#35823;&#23548;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#37322;&#25918;&#36825;&#20123;&#20302;&#25928;&#30340;&#25915;&#20987;&#39044;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#26799;&#24230;&#21435;&#20559;&#32622;&#30340;&#22270;&#32467;&#26500;&#26080;&#30446;&#26631;&#25915;&#20987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become cognitive inertia to employ cross-entropy loss function in classification related tasks. In the untargeted attacks on graph structure, the gradients derived from the attack objective are the attacker's basis for evaluating a perturbation scheme. Previous methods use negative cross-entropy loss as the attack objective in attacking node-level classification models. However, the suitability of the cross-entropy function for constructing the untargeted attack objective has yet been discussed in previous works. This paper argues about the previous unreasonable attack objective from the perspective of budget allocation. We demonstrate theoretically and empirically that negative cross-entropy tends to produce more significant gradients from nodes with lower confidence in the labeled classes, even if the predicted classes of these nodes have been misled. To free up these inefficient attack budgets, we propose a simple attack model for untargeted attacks on graph structure based o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#25307;&#32856;&#26041;&#26696;&#65292;&#37319;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#21152;&#36895;&#25968;&#25454;&#37319;&#38598;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#34892;&#19994;&#26631;&#27880;&#25968;&#25454;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00006</link><description>&lt;p&gt;
&#38754;&#21521;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#20351;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#30340;&#21452;&#21521;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bi-directional personalization reinforcement learning-based architecture with active learning using a multi-model data service for the travel nursing industry. (arXiv:2304.00006v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#25307;&#32856;&#26041;&#26696;&#65292;&#37319;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#21152;&#36895;&#25968;&#25454;&#37319;&#38598;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#34892;&#19994;&#26631;&#27880;&#25968;&#25454;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#21152;&#36895;&#25968;&#25454;&#37319;&#38598;&#24182;&#20351;&#29992;&#21452;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#20174;&#32780;&#22686;&#24378;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#25307;&#32856;&#27969;&#31243;&#12290;&#35813;&#26041;&#26696;&#21487;&#24110;&#21161;&#25307;&#32856;&#20154;&#21592;&#25512;&#33616;&#21512;&#26684;&#30003;&#35831;&#20154;&#65292;&#24182;&#20351;&#30003;&#35831;&#20154;&#25509;&#25910;&#21040;&#20010;&#24615;&#21270;&#24037;&#20316;&#25512;&#33616;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#19968;&#34892;&#19994;&#26631;&#27880;&#25968;&#25454;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenges of using inadequate online recruitment systems can be addressed with machine learning and software engineering techniques. Bi-directional personalization reinforcement learning-based architecture with active learning can get recruiters to recommend qualified applicants and also enable applicants to receive personalized job recommendations. This paper focuses on how machine learning techniques can enhance the recruitment process in the travel nursing industry by helping speed up data acquisition using a multi-model data service and then providing personalized recommendations using bi-directional reinforcement learning with active learning. This need was especially evident when trying to respond to the overwhelming needs of healthcare facilities during the COVID-19 pandemic. The need for traveling nurses and other healthcare professionals was more evident during the lockdown period. A data service was architected for job feed processing using an orchestration of natural la
&lt;/p&gt;</description></item><item><title>PADME-SoSci&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#22312;&#21407;&#25968;&#25454;&#20301;&#32622;&#36827;&#34892;&#23398;&#20064;&#26469;&#23454;&#29616;&#25968;&#25454;&#25152;&#26377;&#26435;&#20445;&#25252;&#21644;&#36328;&#20301;&#32622;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2303.18200</link><description>&lt;p&gt;
PADME-SoSci&#65306;&#31038;&#20250;&#31185;&#23398;&#20013;&#29992;&#20110;&#20998;&#26512;&#21644;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
PADME-SoSci: A Platform for Analytics and Distributed Machine Learning for the Social Sciences. (arXiv:2303.18200v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18200
&lt;/p&gt;
&lt;p&gt;
PADME-SoSci&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#22312;&#21407;&#25968;&#25454;&#20301;&#32622;&#36827;&#34892;&#23398;&#20064;&#26469;&#23454;&#29616;&#25968;&#25454;&#25152;&#26377;&#26435;&#20445;&#25252;&#21644;&#36328;&#20301;&#32622;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#25968;&#25454;&#38544;&#31169;&#21644;&#25152;&#26377;&#26435;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#24403;&#19981;&#21516;&#26041;&#25317;&#26377;&#25968;&#25454;&#30340;&#19981;&#21516;&#37096;&#20998;&#26102;&#65292;&#20849;&#20139;&#21644;&#20998;&#26512;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#12290;&#19968;&#31181;&#24212;&#23545;&#25361;&#25112;&#30340;&#26041;&#27861;&#26159;&#22312;&#25910;&#38598;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#20043;&#21069;&#23558;&#25968;&#25454;&#24212;&#29992;&#21435;&#35782;&#21035;&#21270;&#25110;&#21311;&#21517;&#21270;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#21487;&#33021;&#20250;&#38477;&#20302;&#25968;&#25454;&#25928;&#29992;&#24182;&#22686;&#21152;&#37325;&#26032;&#35782;&#21035;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PADME&#65292;&#36825;&#26159;&#19968;&#20010;&#20998;&#24067;&#24335;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#32852;&#37030;&#20102;&#27169;&#22411;&#23454;&#29616;&#21644;&#35757;&#32451;&#12290;PADME&#20351;&#29992;&#32852;&#37030;&#26041;&#27861;&#65292;&#27169;&#22411;&#30001;&#25152;&#26377;&#26041;&#23454;&#29616;&#21644;&#37096;&#32626;&#65292;&#24182;&#36880;&#27493;&#35775;&#38382;&#27599;&#20010;&#25968;&#25454;&#20301;&#32622;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36328;&#20301;&#32622;&#20998;&#26512;&#25968;&#25454;&#65292;&#21516;&#26102;&#20173;&#20801;&#35768;&#20687;&#25152;&#26377;&#25968;&#25454;&#37117;&#22312;&#21516;&#19968;&#20301;&#32622;&#19968;&#26679;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#25968;&#25454;&#30340;&#21407;&#22987;&#20301;&#32622;&#19978;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20445;&#30041;&#25968;&#25454;&#25152;&#26377;&#26435;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#22312;&#25152;&#26377;&#25968;&#25454;&#20301;&#32622;&#19978;&#30340;&#20998;&#26512;&#37117;&#23436;&#25104;&#21518;&#65292;&#25165;&#20250;&#25552;&#20379;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy and ownership are significant in social data science, raising legal and ethical concerns. Sharing and analyzing data is difficult when different parties own different parts of it. An approach to this challenge is to apply de-identification or anonymization techniques to the data before collecting it for analysis. However, this can reduce data utility and increase the risk of re-identification. To address these limitations, we present PADME, a distributed analytics tool that federates model implementation and training. PADME uses a federated approach where the model is implemented and deployed by all parties and visits each data location incrementally for training. This enables the analysis of data across locations while still allowing the model to be trained as if all data were in a single location. Training the model on data in its original location preserves data ownership. Furthermore, the results are not provided until the analysis is completed on all data locations to
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.18171</link><description>&lt;p&gt;
&#20170;&#22825;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#26377;&#22810;&#39640;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Efficient Are Today's Continual Learning Algorithms?. (arXiv:2303.18171v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18171
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#36845;&#20195;&#23398;&#20064;&#28041;&#21450;&#20174;&#19981;&#26029;&#22686;&#38271;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#27969;&#20013;&#26356;&#26032;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#19978;&#65292;&#20294;&#36845;&#20195;&#23398;&#20064;&#32972;&#21518;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#26159;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#38598;&#38543;&#26102;&#38388;&#22686;&#38271;&#65292;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#22522;&#26412;&#19978;&#35299;&#20915;&#20102;&#28798;&#38590;&#36951;&#24536;&#38382;&#39064;&#65292;&#20294;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#25928;&#29575;&#20851;&#27880;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#26377;&#20123;&#26041;&#27861;&#29978;&#33267;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#23436;&#25104;&#35757;&#32451;&#65281;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#36845;&#20195;&#23398;&#20064;&#19981;&#20165;&#20165;&#26159;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Continual learning involves updating a deep neural network (DNN) from an ever-growing stream of labeled data. While most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. Despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. Here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. Some methods even require more compute than training from scratch! We argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. There is more to continual learning than mitigating catastrophic forg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;KFAC&#20108;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24674;&#22797;&#23618;&#38388;&#20302;&#39057;&#20132;&#20114;&#65292;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#24182;&#26410;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18083</link><description>&lt;p&gt;
&#20004;&#31181;KFAC&#20108;&#32423;&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20998;&#26512;&#19982;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Analysis and Comparison of Two-Level KFAC Methods for Training Deep Neural Networks. (arXiv:2303.18083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;KFAC&#20108;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24674;&#22797;&#23618;&#38388;&#20302;&#39057;&#20132;&#20114;&#65292;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#24182;&#26410;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20108;&#38454;&#26041;&#27861;&#65292;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;NGD&#65289;&#21487;&#20197;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#21644;&#21453;&#28436;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#30340;&#20195;&#20215;&#36807;&#39640;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20197;&#20351;NGD&#21487;&#25193;&#23637;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#24050;&#32463;&#23581;&#35797;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#20854;&#20013;&#26368;&#22797;&#26434;&#30340;&#26159;KFAC&#65292;&#23427;&#23558;FIM&#36817;&#20284;&#20026;&#19968;&#20010;&#22359;&#23545;&#35282;&#30697;&#38453;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#23545;&#24212;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#23618;. &#26412;&#25991;&#36890;&#36807;&#20108;&#32423;&#26041;&#27861;&#65292;&#25506;&#35752;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#31895;&#30053;&#31354;&#38388;&#36824;&#21407;&#19968;&#20123;&#20302;&#39057;&#23618;&#38388;&#20132;&#20114;&#30340;&#26041;&#27861;&#30340;&#21033;&#30410;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20197;&#36825;&#31181;&#26041;&#24335;&#23558;&#23618;&#38388;&#20132;&#20114;&#32467;&#21512;&#36215;&#26469;&#24182;&#19981;&#33021;&#30495;&#27491;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a second-order method, the Natural Gradient Descent (NGD) has the ability to accelerate training of neural networks. However, due to the prohibitive computational and memory costs of computing and inverting the Fisher Information Matrix (FIM), efficient approximations are necessary to make NGD scalable to Deep Neural Networks (DNNs). Many such approximations have been attempted. The most sophisticated of these is KFAC, which approximates the FIM as a block-diagonal matrix, where each block corresponds to a layer of the neural network. By doing so, KFAC ignores the interactions between different layers. In this work, we investigate the interest of restoring some low-frequency interactions between the layers by means of two-level methods. Inspired from domain decomposition, several two-level corrections to KFAC using different coarse spaces are proposed and assessed. The obtained results show that incorporating the layer interactions in this fashion does not really improve the perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17649</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#30340;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;
&lt;/p&gt;
&lt;p&gt;
Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21407;&#26412;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#30340;&#20013;&#31561;&#22823;&#23567;&#33521;&#25991;GPT&#27169;&#22411;&#65292;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#34987;&#31934;&#32454;&#35843;&#25972;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#35757;&#32451;&#21644;&#23454;&#29616;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#22870;&#21169;&#27169;&#22411;&#65289;&#65292;&#20197;&#35780;&#20998;&#24182;&#30830;&#23450;&#31572;&#26696;&#26159;&#21542;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#35813;&#32452;&#20214;&#26377;&#21161;&#20110;&#25913;&#36827;&#31995;&#32479;&#22238;&#31572;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#12290; BLEU&#21644;perplexity&#31561;&#25968;&#23383;&#24230;&#37327;&#26631;&#20934;&#34987;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20351;&#29992;&#20154;&#31867;&#21028;&#26029;&#26469;&#27604;&#36739;&#35299;&#30721;&#25216;&#26415;&#19982;&#20854;&#20182;&#25216;&#26415;&#12290;&#26368;&#32456;&#65292;&#32467;&#26524;&#25903;&#25345;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20351;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#29983;&#25104;&#22238;&#31572;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#30340;&#26234;&#33021;&#19988;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#31995;&#32479;&#25277;&#35937;&#25216;&#26415;&#65292;&#24182;&#19988;&#23450;&#20041;&#20102;&#19968;&#31181;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#24230;&#37327;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17618</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#30340;&#25968;&#25454;&#39537;&#21160;&#25277;&#35937;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version]. (arXiv:2303.17618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17618
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#30340;&#26234;&#33021;&#19988;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#31995;&#32479;&#25277;&#35937;&#25216;&#26415;&#65292;&#24182;&#19988;&#23450;&#20041;&#20102;&#19968;&#31181;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#24230;&#37327;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#19988;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#31995;&#32479;&#25277;&#35937;&#33258;&#36866;&#24212;&#32454;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#26681;&#25454;&#26410;&#26469;&#36755;&#20986;&#30340;&#35266;&#23519;&#23558;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#26159;&#21160;&#24577;&#22320;&#20197;&#19981;&#23545;&#31216;&#30340;&#26041;&#24335;&#26500;&#24314;&#30340;&#12290;&#20026;&#20102;&#23398;&#20064;&#26368;&#20248;&#32467;&#26500;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36866;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#20294;&#19981;&#21463;&#38480;&#20110;&#27492;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#19978;&#36848;&#24230;&#37327;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#21487;&#33021;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#35813;&#24230;&#37327;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#20256;&#32479;&#30340;&#32447;&#24615;&#35268;&#21010;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an adaptive refinement procedure for smart, and scalable abstraction of dynamical systems. Our technique relies on partitioning the state space depending on the observation of future outputs. However, this knowledge is dynamically constructed in an adaptive, asymmetric way. In order to learn the optimal structure, we define a Kantorovich-inspired metric between Markov chains, and we use it as a loss function. Our technique is prone to data-driven frameworks, but not restricted to.  We also study properties of the above mentioned metric between Markov chains, which we believe could be of application for wider purpose. We propose an algorithm to approximate it, and we show that our method yields a much better computational complexity than using classical linear programming techniques.
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17573</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#23478;&#20013;&#27979;&#37327;&#24085;&#37329;&#26862;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#22312;&#32593;&#32476;&#25668;&#20687;&#22836;&#21069;&#23436;&#25104;&#20102;&#36816;&#21160;&#20219;&#21153;&#65288;&#21363;&#28857;&#20987;&#25163;&#25351;&#65289;&#65292;250&#21517;&#20840;&#29699;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#25353;&#29031;&#36816;&#21160;&#38556;&#30861;&#21327;&#20250;&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920; (MDS-UPDRS) &#30340;&#26631;&#20934;&#30001;&#19977;&#21517;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#38752;&#24615;&#65292;&#20869;&#37096;&#19968;&#33268;&#24615;&#31995;&#25968;&#65288;ICC&#65289;&#20026;0.88&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#33719;&#24471;&#19982;MDS-UPDRS&#25351;&#21335;&#19968;&#33268;&#19988;&#19982;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#30340;&#23458;&#35266;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25351;&#26631;&#30340;&#35757;&#32451;&#19979;&#34920;&#29616;&#20248;&#20110;&#19968;&#20010;MDS-UPDRS&#35748;&#35777;&#30340;&#35780;&#20998;&#32773;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.59&#65292;&#32780;&#35780;&#20998;&#32773;&#30340;MAE&#20026;0.79&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#30053;&#36874;&#20110;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#65288;0.53 MAE&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2303.17218</link><description>&lt;p&gt;
HARFLOW3D&#65306;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;
&lt;/p&gt;
&lt;p&gt;
HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#24335;&#26550;&#26500;&#30340;&#24037;&#20855;&#38142;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#26144;&#23556;&#21040;FPGA&#19978;&#65292;&#32771;&#34385;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#21644;&#30446;&#26631;FPGA&#35774;&#22791;&#30340;&#29305;&#24449;&#12290;HARFLOW3D&#24037;&#20855;&#38142;&#20197;ONNX&#26684;&#24335;&#30340;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;FPGA&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#35813;&#24037;&#20855;&#38142;&#30001;&#22810;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#21253;&#25324;i) 3D CNN&#35299;&#26512;&#22120;&#65292;ii) &#24615;&#33021;&#21644;&#36164;&#28304;&#27169;&#22411;&#65292;iii) &#29992;&#20110;&#22312;&#29983;&#25104;&#30340;&#30828;&#20214;&#19978;&#25191;&#34892;3D&#27169;&#22411;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;iv) &#38024;&#23545;3D&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#36164;&#28304;&#24863;&#30693;&#20248;&#21270;&#24341;&#25806;&#65292;v) &#33258;&#21160;&#26144;&#23556;&#21040;&#21487;&#21512;&#25104;&#30340;FPGA&#20195;&#30721;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;3D CNN&#21644;FPGA&#31995;&#32479;&#37197;&#23545;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#24037;&#20855;&#38142;&#25903;&#25345;&#24191;&#27867;&#27169;&#22411;&#21644;&#35774;&#22791;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;3D CNN&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#24037;&#20855;&#38142;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
&lt;/p&gt;</description></item><item><title>DNN&#35757;&#32451;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#23558;&#32473;&#19981;&#21516;&#36755;&#20986;&#31867;&#21035;&#25552;&#20379;&#19981;&#21516;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#39318;&#27425;&#25351;&#20986;&#23548;&#33268;&#33410;&#28857;&#25935;&#24863;&#24615;&#21464;&#21270;&#30340;&#33410;&#28857;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.16589</link><description>&lt;p&gt;
&#35770;&#25991;&#28023;&#25253;&#65306;&#35757;&#32451;DNN&#20013;&#20559;&#24046;&#12289;&#33410;&#28857;&#25935;&#24863;&#24615;&#21644;&#38271;&#23614;&#20998;&#24067;&#20043;&#38388;&#30340;&#38142;&#25509; (arXiv:2303.16589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution in trained DNNs. (arXiv:2303.16589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16589
&lt;/p&gt;
&lt;p&gt;
DNN&#35757;&#32451;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#23558;&#32473;&#19981;&#21516;&#36755;&#20986;&#31867;&#21035;&#25552;&#20379;&#19981;&#21516;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#39318;&#27425;&#25351;&#20986;&#23548;&#33268;&#33410;&#28857;&#25935;&#24863;&#24615;&#21464;&#21270;&#30340;&#33410;&#28857;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#23398;&#20064;(&#21644;&#37325;&#26032;&#23398;&#20064;)&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#19968;&#33324;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;DNNs&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35757;&#32451;&#30340;DNNs&#21487;&#33021;&#23545;&#19981;&#21516;&#30340;&#36755;&#20986;&#31867;&#21035;&#25552;&#20379;&#19981;&#21516;&#31243;&#24230;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#25972;&#20307;&#20559;&#24046;&#65292;&#20294;&#26412;&#25991;&#39318;&#27425;&#25351;&#20986;&#20102;&#23548;&#33268;&#33410;&#28857;&#23545;&#19981;&#21516;&#36755;&#20986;&#31867;&#21035;&#25935;&#24863;&#24615;&#21464;&#21270;&#30340;&#33410;&#28857;&#20559;&#24046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#24378;&#35843;DNNs&#20013;&#36825;&#31181;&#29420;&#29305;&#25361;&#25112;&#30340;&#24037;&#20316;&#65292;&#35752;&#35770;&#20854;&#21487;&#33021;&#30340;&#21407;&#22240;&#65292;&#24182;&#20026;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#24773;&#22659;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#30340;&#23454;&#35777;&#26696;&#20363;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to their remarkable learning (and relearning) capabilities, deep neural networks (DNNs) find use in numerous real-world applications. However, the learning of these data-driven machine learning models is generally as good as the data available to them for training. Hence, training datasets with long-tail distribution pose a challenge for DNNs, since the DNNs trained on them may provide a varying degree of classification performance across different output classes. While the overall bias of such networks is already highlighted in existing works, this work identifies the node bias that leads to a varying sensitivity of the nodes for different output classes. To the best of our knowledge, this is the first work highlighting this unique challenge in DNNs, discussing its probable causes, and providing open challenges for this new research direction. We support our reasoning using an empirical case study of the networks trained on a real-world dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#65292;&#26088;&#22312;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20351;&#29992;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.16458</link><description>&lt;p&gt;
&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65311;&#22522;&#20110;&#25968;&#25454;&#29983;&#25104;&#35270;&#35282;&#30340;&#22238;&#31572;&#65281;
&lt;/p&gt;
&lt;p&gt;
When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!. (arXiv:2303.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#65292;&#26088;&#22312;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20351;&#29992;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#39044;&#35757;&#32451;&#22312;&#23398;&#26415;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#23581;&#35797;&#65292;&#20294;&#36127;&#38754;&#36801;&#31227;&#26159;&#23558;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26102;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#22270;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#20309;&#26102;&#39044;&#35757;&#32451;&#21644;&#22914;&#20309;&#39044;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#20505;&#26080;&#35770;&#31574;&#30053;&#22914;&#20309;&#20808;&#36827;&#65292;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#20173;&#28982;&#26080;&#27861;&#24102;&#26469;&#26126;&#26174;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#26469;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#30340;&#20851;&#38190;&#38382;&#39064;&#65288;&#21363;&#25105;&#20204;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#21487;&#20197;&#21033;&#29992;&#22270;&#39044;&#35757;&#32451;&#65289;&#65292;&#28982;&#21518;&#20877;&#36827;&#34892;&#36153;&#21147;&#30340;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph pre-training has attracted wide research attention, which aims to learn transferable knowledge from unlabeled graph data so as to improve downstream performance. Despite these recent attempts, the negative transfer is a major issue when applying graph pre-trained models to downstream tasks. Existing works made great efforts on the issue of what to pre-train and how to pre-train by designing a number of graph pre-training and fine-tuning strategies. However, there are indeed cases where no matter how advanced the strategy is, the "pre-train and fine-tune" paradigm still cannot achieve clear benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.16296</link><description>&lt;p&gt;
Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65306;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Dice&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#30340;&#35768;&#22810;&#33258;&#21160;&#20998;&#21106;&#26041;&#26696;&#20013;&#65292;&#36719;Dice&#25439;&#22833;&#65288;SDL&#65289;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#25581;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#32972;&#21518;&#30340;&#19968;&#20123;&#21407;&#22240;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20854;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#25903;&#25345;&#30452;&#25509;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#23427;&#30340;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;SDL&#21644;&#30740;&#31350;&#21033;&#29992;&#36719;&#26631;&#31614;&#30340;&#21516;&#26102;&#36827;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21327;&#21516;&#20316;&#29992;&#20173;&#28982;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65288;DMLs&#65289;&#65292;&#23427;&#20204;&#65288;i&#65289;&#22312;&#30828;&#26631;&#31614;&#30340;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;SDL&#30456;&#21516;&#65292;&#20294;&#65288;ii&#65289;&#20063;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#30340;QUBIQ&#12289;LiTS&#21644;KiTS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DMLs&#19982;&#36719;&#26631;&#31614;&#65288;&#22914;&#24179;&#22343;&#12289;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#65289;&#30340;&#28508;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;DMLs&#19982;&#30828;&#26631;&#31614;&#65288;&#22914;&#22823;&#22810;&#25968;&#25237;&#31080;&#21644;&#38543;&#26426;&#36873;&#25321;&#65289;&#30456;&#27604;&#65292;&#20135;&#29983;&#20102;&#26356;&#20248;&#31168;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#24212;&#29992;&#28145;&#24230;&#38598;&#21512;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#20854;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16210</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task. (arXiv:2303.16210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#24212;&#29992;&#28145;&#24230;&#38598;&#21512;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#20854;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#26159;&#36924;&#36817;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#65292;&#24050;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#25506;&#31350;&#35813;&#26041;&#27861;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#39044;&#27979;&#23548;&#24377;&#32467;&#26500;&#30340;&#31354;&#27668;&#21160;&#21147;&#24615;&#33021;&#12290;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;&#38598;&#21512;&#20013;&#31070;&#32463;&#32593;&#32476;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26222;&#36941;&#23384;&#22312;&#20302;&#20272;&#30340;&#36235;&#21183;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20107;&#21518;&#26657;&#20934;&#30340;&#28145;&#24230;&#38598;&#21512;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20854;&#25913;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24615;&#33021;&#12290;&#30452;&#35266;&#22320;&#23558;&#20854;&#19982;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#26159;&#24037;&#31243;&#20013;&#26368;&#24120;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#20063;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensemble is a simple and straightforward approach for approximating Bayesian inference and has been successfully applied to many classification tasks. This study aims to comprehensively investigate this approach in the multi-output regression task to predict the aerodynamic performance of a missile configuration. By scrutinizing the effect of the number of neural networks used in the ensemble, an obvious trend toward underconfidence in estimated uncertainty is observed. In this context, we propose the deep ensemble framework that applies the post-hoc calibration method, and its improved uncertainty quantification performance is demonstrated. It is compared with Gaussian process regression, the most prevalent model for uncertainty quantification in engineering, and is proven to have superior performance in terms of regression accuracy, reliability of estimated uncertainty, and training efficiency. Finally, the impact of the suggested framework on the results of Bayesian optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#26144;&#23556;&#31639;&#27861;&#29992;&#20110;&#32676;&#19981;&#21464;&#27969;&#24418;&#38382;&#39064;&#65292;&#36890;&#36807;&#31215;&#20998;&#22312;&#19981;&#21464;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;&#20986;K-&#19981;&#21464;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#21033;&#29992;K&#20013;&#30340;&#24186;&#27491;&#19981;&#21487;&#32422;&#34920;&#31034;&#30697;&#38453;&#23545;&#20854;&#36827;&#34892;&#23545;&#35282;&#21270;&#65292;&#24182;&#32473;&#20986;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#35745;&#31639;&#20844;&#24335;&#12290;&#21516;&#26102;&#65292;&#23637;&#31034;&#20102;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;L_N&#25910;&#25947;&#20110;Laplace-Beltrami&#31639;&#23376;&#65292;&#25910;&#25947;&#36895;&#24230;&#38543;&#30528;&#23545;&#31216;&#32676;K&#30340;&#32500;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.16169</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#30340;&#32676;&#19981;&#21464;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Maps for Group-Invariant Manifolds. (arXiv:2303.16169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#26144;&#23556;&#31639;&#27861;&#29992;&#20110;&#32676;&#19981;&#21464;&#27969;&#24418;&#38382;&#39064;&#65292;&#36890;&#36807;&#31215;&#20998;&#22312;&#19981;&#21464;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;&#20986;K-&#19981;&#21464;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#21033;&#29992;K&#20013;&#30340;&#24186;&#27491;&#19981;&#21487;&#32422;&#34920;&#31034;&#30697;&#38453;&#23545;&#20854;&#36827;&#34892;&#23545;&#35282;&#21270;&#65292;&#24182;&#32473;&#20986;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#35745;&#31639;&#20844;&#24335;&#12290;&#21516;&#26102;&#65292;&#23637;&#31034;&#20102;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;L_N&#25910;&#25947;&#20110;Laplace-Beltrami&#31639;&#23376;&#65292;&#25910;&#25947;&#36895;&#24230;&#38543;&#30528;&#23545;&#31216;&#32676;K&#30340;&#32500;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24403;&#25968;&#25454;&#38598;&#23545;&#32039;Lie&#32676;K&#30340;&#20316;&#29992;&#20855;&#26377;&#19981;&#21464;&#24615;&#26102;&#65292;&#27969;&#24418;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;&#29616;&#26377;&#25968;&#25454;&#28857;K&#30340;&#36712;&#36947;&#19978;&#31215;&#20998;&#65292;&#23558;&#25968;&#25454;&#35825;&#23548;&#30340;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25193;&#23637;&#21040;K-&#19981;&#21464;&#31639;&#23376;L&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;K&#30340;&#24186;&#27491;&#19981;&#21487;&#32422;&#34920;&#31034;&#30697;&#38453;&#26469;&#23545;&#35282;&#21270;K-&#19981;&#21464;&#31639;&#23376;L&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;L&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#26174;&#24335;&#20844;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;L_N&#25910;&#25947;&#21040;&#25968;&#25454;&#27969;&#24418;&#30340;Laplace-Beltrami&#31639;&#23376;&#65292;&#25910;&#25947;&#36895;&#24230;&#24471;&#21040;&#25913;&#36827;&#65292;&#25913;&#36827;&#38543;&#30528;&#23545;&#31216;&#32676;K&#30340;&#32500;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#26412;&#25991;&#23558;Landa&#21644;Shkolnisky&#30340;&#21487;&#36716;&#21160;&#22270;&#25289;&#26222;&#25289;&#26031;&#26694;&#26550;&#20174;SO&#65288;2&#65289;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#20219;&#24847;&#32039;Lie&#32676;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we consider the manifold learning problem when the data set is invariant under the action of a compact Lie group $K$. Our approach consists in augmenting the data-induced graph Laplacian by integrating over orbits under the action of $K$ of the existing data points. We prove that this $K$-invariant Laplacian operator $L$ can be diagonalized by using the unitary irreducible representation matrices of $K$, and we provide an explicit formula for computing the eigenvalues and eigenvectors of $L$. Moreover, we show that the normalized Laplacian operator $L_N$ converges to the Laplace-Beltrami operator of the data manifold with an improved convergence rate, where the improvement grows with the dimension of the symmetry group $K$. This work extends the steerable graph Laplacian framework of Landa and Shkolnisky from the case of $\operatorname{SO}(2)$ to arbitrary compact Lie groups.
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15799</link><description>&lt;p&gt;
&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Convergence Federated Learning with Aggregated Gradients. (arXiv:2303.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#22810;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#22312;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#30340;&#25968;&#25454;&#26679;&#26412;&#20197;&#21450;&#21442;&#19982;&#32773;&#20043;&#38388;&#39057;&#32321;&#30340;&#36890;&#20449;&#23558;&#20943;&#32531;&#25910;&#25947;&#36895;&#29575;&#24182;&#22686;&#21152;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24120;&#35268;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#20013;&#24341;&#20837;&#32858;&#21512;&#26799;&#24230;&#26469;&#25913;&#21892;&#26412;&#22320;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36827;&#19968;&#27493;&#32771;&#34385;&#26412;&#22320;&#21442;&#25968;&#21644;&#20840;&#23616;&#21442;&#25968;&#30340;&#20559;&#24046;&#12290;&#20197;&#19978;&#31574;&#30053;&#35201;&#27714;&#22312;&#27599;&#20010;&#26412;&#22320;&#36845;&#20195;&#20013;&#25910;&#38598;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#21442;&#25968;&#21644;&#26799;&#24230;&#65292;&#30001;&#20110;&#26412;&#22320;&#26356;&#26032;&#26399;&#38388;&#27809;&#26377;&#36890;&#20449;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22343;&#20540;&#22330;&#26041;&#27861;&#65292;&#24341;&#20837;&#31216;&#20026;&#20840;&#23616;&#22343;&#20540;&#22330;&#21644;&#26412;&#22320;&#22343;&#20540;&#22330;&#30340;&#20004;&#20010;&#22343;&#20540;&#22330;&#26415;&#35821;&#26469;&#23436;&#25104;&#32858;&#21512;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a novel machine learning framework, which enables multiple distributed devices cooperatively training a shared model scheduled by a central server while protecting private data locally. However, the non-independent-and-identically-distributed (Non-IID) data samples and frequent communication among participants will slow down the convergent rate and increase communication costs. To achieve fast convergence, we ameliorate the local gradient descend approach in conventional local update rule by introducing the aggregated gradients at each local update epoch, and propose an adaptive learning rate algorithm that further takes the deviation of local parameter and global parameter into consideration at each iteration. The above strategy requires all clients' local parameters and gradients at each local iteration, which is challenging as there is no communication during local update epochs. Accordingly, we utilize mean field approach by introducing two mean field ter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#26696;&#65292;&#22522;&#20110;&#29109;&#29702;&#35770;&#32531;&#35299;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#23454;&#29616;&#20840;&#23616;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2303.14966</link><description>&lt;p&gt;
&#26032;&#29109;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Learning via New Entropy Approach. (arXiv:2303.14966v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#26696;&#65292;&#22522;&#20110;&#29109;&#29702;&#35770;&#32531;&#35299;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#23454;&#29616;&#20840;&#23616;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26694;&#26550;&#65292;&#23427;&#20801;&#35768;&#36164;&#28304;&#21463;&#38480;&#30340;&#31163;&#25955;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#23384;&#20648;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#20849;&#21516;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#35774;&#22791;&#21644;&#25968;&#25454;&#24046;&#24322;&#20250;&#23548;&#33268;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#30340;&#20559;&#24046;&#65292;&#36827;&#32780;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#20943;&#24930;&#21644;&#31934;&#24230;&#38477;&#20302;&#12290;&#24403;&#21069;&#30340; FL &#31639;&#27861;&#26222;&#36941;&#37319;&#29992;&#38745;&#24577;&#23458;&#25143;&#31471;&#23398;&#20064;&#31574;&#30053;&#24182;&#19981;&#33021;&#36866;&#24212;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#21160;&#24577;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29109;&#29702;&#35770;&#32771;&#34385;&#19981;&#21516;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#25552;&#20986;&#20102;&#22522;&#20110;&#29109;&#29702;&#35770;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#26696;&#65292;&#20197;&#32531;&#35299;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#23454;&#29616;&#20840;&#23616;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;&#20294;&#30001;&#20110;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#21644;&#29305;&#24449;&#20855;&#26377;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#35774;&#35745;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26368;&#20248;&#21160;&#24577;&#23398;&#20064;&#29575;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has recently emerged as a popular framework, which allows resource-constrained discrete clients to cooperatively learn the global model under the orchestration of a central server while storing privacy-sensitive data locally. However, due to the difference in equipment and data divergence of heterogeneous clients, there will be parameter deviation between local models, resulting in a slow convergence rate and a reduction of the accuracy of the global model. The current FL algorithms use the static client learning strategy pervasively and can not adapt to the dynamic training parameters of different clients. In this paper, by considering the deviation between different local model parameters, we propose an adaptive learning rate scheme for each client based on entropy theory to alleviate the deviation between heterogeneous clients and achieve fast convergence of the global model. It's difficult to design the optimal dynamic learning rate for each client as the lo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFP&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#26032;&#29305;&#24449;&#21464;&#25442;&#20026;&#26087;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#26469;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#26032;&#29305;&#24449;&#26041;&#21521;&#30340;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.14595</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#29305;&#24449;&#25237;&#24433;&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;
&lt;/p&gt;
&lt;p&gt;
Preserving Linear Separability in Continual Learning by Backward Feature Projection. (arXiv:2303.14595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFP&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#26032;&#29305;&#24449;&#21464;&#25442;&#20026;&#26087;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#26469;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#26032;&#29305;&#24449;&#26041;&#21521;&#30340;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#38656;&#35201;&#22312;&#26377;&#38480;&#25110;&#27809;&#26377;&#20197;&#21069;&#26597;&#30475;&#20219;&#21153;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#24182;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#30452;&#25509;&#32422;&#26463;&#26032;&#29305;&#24449;&#20197;&#21305;&#37197;&#26087;&#29305;&#24449;&#65292;&#24573;&#35270;&#20102;&#21487;&#22609;&#24615;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Backward Feature Projection&#65288;BFP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#26032;&#29305;&#24449;&#22312;&#26087;&#29305;&#24449;&#30340;&#21487;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#20013;&#21457;&#29983;&#21464;&#21270;&#12290;BFP&#20445;&#30041;&#26087;&#31867;&#21035;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#21516;&#26102;&#20801;&#35768;&#26032;&#30340;&#29305;&#24449;&#26041;&#21521;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#30340;&#31867;&#21035;&#12290;BFP&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;&#38598;&#25104;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;BFP&#26377;&#21161;&#20110;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space,
&lt;/p&gt;</description></item><item><title>TRAK&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#21487;&#24494;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#37327;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2303.14186</link><description>&lt;p&gt;
TRAK: &#21051;&#30011;&#22823;&#35268;&#27169;&#27169;&#22411;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
TRAK: Attributing Model Behavior at Scale. (arXiv:2303.14186v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14186
&lt;/p&gt;
&lt;p&gt;
TRAK&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#21487;&#24494;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#37327;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#30340;&#30446;&#26631;&#26159;&#36861;&#36394;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#27714;&#29992;&#25143;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#36873;&#25321;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#38750;&#20984;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#65289;&#20013;&#65292;&#35745;&#31639;&#37327;&#21487;&#34892;&#30340;&#26041;&#27861;&#21487;&#33021;&#38590;&#20197;&#20934;&#30830;&#22320;&#24402;&#22240;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#22312;&#36825;&#31867;&#22330;&#26223;&#20013;&#26377;&#25928;&#30340;&#26041;&#27861;&#21017;&#38656;&#35201;&#35757;&#32451;&#25968;&#21315;&#20010;&#27169;&#22411;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22823;&#22411;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#20013;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#19981;&#21487;&#34892;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TRAK&#65288;&#38543;&#26426;&#25237;&#24433;&#26680;&#36861;&#36394;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#21487;&#24494;&#27169;&#22411;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#37327;&#21487;&#34892;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#27169;&#22411;&#65292;TRAK &#21487;&#20197;&#21305;&#37197;&#38656;&#35201;&#35757;&#32451;&#25968;&#21315;&#27169;&#22411;&#25165;&#33021;&#24471;&#21040;&#30340;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35770;&#35777;&#20102;TRAK &#22312;&#21508;&#31181;&#27169;&#24335;&#21644;&#35268;&#27169;&#19978;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets.  In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PIPN&#65292;&#23427;&#33021;&#21033;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#21516;&#26102;&#39044;&#27979;&#25152;&#38656;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#20960;&#20309;&#20307;&#19978;&#30340;&#35299;&#65292;&#26377;&#26395;&#22312;&#24037;&#19994;&#30028;&#36827;&#34892;&#24555;&#36895;&#30340;&#20960;&#20309;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.13634</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;PointNet&#65306;&#23427;&#33021;&#21516;&#26102;&#35299;&#20915;&#22810;&#23569;&#19981;&#35268;&#21017;&#20960;&#20309;&#20307;&#30340;&#21453;&#38382;&#39064;&#65311;&#20197;&#32447;&#24377;&#24615;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity. (arXiv:2303.13634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PIPN&#65292;&#23427;&#33021;&#21033;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#21516;&#26102;&#39044;&#27979;&#25152;&#38656;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#20960;&#20309;&#20307;&#19978;&#30340;&#35299;&#65292;&#26377;&#26395;&#22312;&#24037;&#19994;&#30028;&#36827;&#34892;&#24555;&#36895;&#30340;&#20960;&#20309;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35268;&#30340;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21033;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#39044;&#27979;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#20294;&#21482;&#38480;&#20110;&#21333;&#19968;&#30340;&#22495;&#12290;&#30456;&#21453;&#65292;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#39318;&#20808;&#22312;&#24050;&#30693;&#35299;&#65288;&#21363;&#26631;&#35760;&#25968;&#25454;&#65289;&#30340;&#20960;&#21315;&#20010;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#39044;&#27979;&#22312;&#19968;&#20123;&#26410;&#30693;&#22495;&#19978;&#30340;&#35299;&#12290;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;PointNet&#65288;PIPN&#65289;&#20027;&#35201;&#26088;&#22312;&#22635;&#34917;PINN&#65288;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65289;&#21644;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PIPN&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#25152;&#38656;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#25968;&#30334;&#20010;&#22495;&#19978;&#30340;&#35299;&#65292;&#32780;&#21482;&#20351;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20010;&#26694;&#26550;&#26377;&#21161;&#20110;&#22312;&#24037;&#19994;&#30028;&#36827;&#34892;&#24555;&#36895;&#30340;&#20960;&#20309;&#35774;&#35745;&#65292;&#23588;&#20854;&#24403;&#21482;&#26377;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PIPN&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#24179;&#38754;&#24212;&#21147;&#38382;&#39064;&#22312;500&#22810;&#20010;&#19981;&#21516;&#20960;&#20309;&#20307;&#19978;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular physics-informed neural networks (PINNs) predict the solution of partial differential equations using sparse labeled data but only over a single domain. On the other hand, fully supervised learning models are first trained usually over a few thousand domains with known solutions (i.e., labeled data) and then predict the solution over a few hundred unseen domains. Physics-informed PointNet (PIPN) is primarily designed to fill this gap between PINNs (as weakly supervised learning models) and fully supervised learning models. In this article, we demonstrate that PIPN predicts the solution of desired partial differential equations over a few hundred domains simultaneously, while it only uses sparse labeled data. This framework benefits fast geometric designs in the industry when only sparse labeled data are available. Particularly, we show that PIPN predicts the solution of a plane stress problem over more than 500 domains with different geometries, simultaneously. Moreover, we pio
&lt;/p&gt;</description></item><item><title>&#27492;&#25991;&#20171;&#32461;&#20102;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2303.13336</link><description>&lt;p&gt;
&#35821;&#38899;&#21512;&#25104;&#30340;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65306;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI. (arXiv:2303.13336v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13336
&lt;/p&gt;
&lt;p&gt;
&#27492;&#25991;&#20171;&#32461;&#20102;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#35821;&#38899;&#21512;&#25104;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#26041;&#21521;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#26368;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35768;&#22810;&#24037;&#20316;&#24050;&#32463;&#23581;&#35797;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;&#26412;&#25991;&#23545;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#35843;&#26597;&#30340;&#34917;&#20805;&#65292;&#36825;&#20123;&#35843;&#26597;&#35201;&#20040;&#32570;&#20047;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#21512;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35201;&#20040;&#24378;&#35843;&#22312;&#22810;&#20010;&#39046;&#22495;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25972;&#20307;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;&#38899;&#39057;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32972;&#26223;&#12290;&#23545;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#30340;&#38454;&#27573;&#65306;&#22768;&#23398;&#27169;&#22411;&#12289;&#22768;&#30721;&#22120;&#21644;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26576;&#20123;&#20449;&#21495;&#20174;&#36755;&#20837;&#35821;&#38899;&#20013;&#21024;&#38500;&#25110;&#28155;&#21152;&#26469;&#23558;&#21508;&#31181;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#36824;&#28085;&#30422;&#20102;&#23454;&#39564;&#32467;&#26524;&#30340;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#21644;&#37325;&#26500;&#65292;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#38388;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#65292;&#20063;&#19981;&#38656;&#35201;&#20351;&#29992;&#26356;&#22810;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.12848</link><description>&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#30340;&#27979;&#35797;&#26102;&#38388;&#38450;&#24481;&#65306;&#22522;&#20110;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Test-time Defense against Adversarial Attacks: Detection and Reconstruction of Adversarial Examples via Masked Autoencoder. (arXiv:2303.12848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12848
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#21644;&#37325;&#26500;&#65292;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#38388;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#65292;&#20063;&#19981;&#38656;&#35201;&#20351;&#29992;&#26356;&#22810;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#35757;&#32451;&#26102;&#38388;&#21644;&#27979;&#35797;&#26102;&#38388;&#38450;&#24481;&#12290;&#35757;&#32451;&#26102;&#38388;&#38450;&#24481;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#35757;&#32451;&#26102;&#38388;&#65292;&#36890;&#24120;&#26080;&#27861;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25915;&#20987;&#12290;&#32780;&#27979;&#35797;&#26102;&#38388;&#38450;&#24481;&#38656;&#35201;&#35775;&#38382;&#65288;&#37096;&#20998;&#65289;&#27169;&#22411;&#26435;&#37325;&#20197;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#36825;&#23545;&#20110;&#20923;&#32467;&#26435;&#37325;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;DRAM&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26816;&#27979;&#24182;&#37325;&#26500;&#22810;&#31181;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;MAE&#25439;&#22833;&#26500;&#24314;KS&#27979;&#35797;&#26469;&#26816;&#27979;&#23545;&#25239;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;MAE&#25439;&#22833;&#21487;&#20197;&#29992;&#20110;&#20462;&#22797;&#26410;&#35265;&#25915;&#20987;&#31867;&#22411;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;DRAM&#26082;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#38388;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#65292;&#20063;&#19981;&#38656;&#35201;&#20351;&#29992;&#26356;&#22810;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;ImageN&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;DRAM&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#24456;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing defense methods against adversarial attacks can be categorized into training time and test time defenses. Training time defense, i.e., adversarial training, requires a significant amount of extra time for training and is often not able to be generalized to unseen attacks. On the other hand, test time defense by test time weight adaptation requires access to perform gradient descent on (part of) the model weights, which could be infeasible for models with frozen weights. To address these challenges, we propose DRAM, a novel defense method to Detect and Reconstruct multiple types of Adversarial attacks via Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a KS-test to detect adversarial attacks. Moreover, the MAE losses can be used to repair adversarial samples from unseen attack types. In this sense, DRAM neither requires model weight updates in test time nor augments the training set with more adversarial samples. Evaluating DRAM on the large-scale ImageN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;CLIP&#23384;&#22312;&#35823;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.12748</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models. (arXiv:2303.12748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;CLIP&#23384;&#22312;&#35823;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26657;&#20934;&#23545;&#20110;&#20445;&#35777;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#20351;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#27492;&#22312;&#30417;&#30563;&#20998;&#31867;&#27169;&#22411;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#38477;&#20302;&#35823;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#29702;&#26102;&#30340;&#26657;&#20934;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#20363;&#22914;CLIP&#12290;&#26412;&#30740;&#31350;&#34913;&#37327;&#20102;&#36328;&#30456;&#20851;&#21464;&#37327;&#65288;&#22914;&#25552;&#31034;&#65292;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#65289;&#30340;&#26657;&#20934;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;CLIP&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#23384;&#22312;&#35823;&#26657;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#19982;CLIP&#20316;&#20026;&#38646;&#26679;&#26412;&#25512;&#29702;&#27169;&#22411;&#30340;&#24120;&#35265;&#29992;&#20363;&#30456;&#19968;&#33268;&#65292;&#24182;&#23637;&#31034;&#20986;&#21333;&#20010;&#23398;&#20064;&#30340;&#28201;&#24230;&#20540;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#65288;&#30001;&#36873;&#23450;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#23450;&#20041;&#65289;&#65292;&#36328;&#19981;&#21516;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#25552;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration of deep learning models is crucial to their trustworthiness and safe usage, and as such, has been extensively studied in supervised classification models, with methods crafted to decrease miscalibration. However, there has yet to be a comprehensive study of the calibration of vision-language models that are used for zero-shot inference, like CLIP. We measure calibration across relevant variables like prompt, dataset, and architecture, and find that zero-shot inference with CLIP is miscalibrated. Furthermore, we propose a modified version of temperature scaling that is aligned with the common use cases of CLIP as a zero-shot inference model, and show that a single learned temperature generalizes for each specific CLIP model (defined by a chosen pre-training dataset and architecture) across inference dataset and prompt choice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#32852;&#37030;&#32593;&#32476;&#20013;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25903;&#25345;&#21521;&#37327;&#26426;&#23454;&#29616;&#23545;&#32852;&#37030;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#24182;&#25903;&#25345;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38543;&#26426;&#25513;&#30721;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#35299;&#20915;&#24322;&#26500;&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.10254</link><description>&lt;p&gt;
&#24322;&#26500;&#32593;&#32476;&#20013;&#32852;&#37030;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Model Personalization for Federated Supervised SVM in Heterogeneous Networks. (arXiv:2303.10254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#32852;&#37030;&#32593;&#32476;&#20013;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25903;&#25345;&#21521;&#37327;&#26426;&#23454;&#29616;&#23545;&#32852;&#37030;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#24182;&#25903;&#25345;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38543;&#26426;&#25513;&#30721;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#35299;&#20915;&#24322;&#26500;&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#20998;&#31867;&#21644;&#22238;&#24402;&#12290;&#35813;&#26041;&#27861;&#25903;&#25345;&#22312;&#24322;&#26500;&#33410;&#28857;&#32593;&#32476;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#27169;&#22411;&#20132;&#25442;&#65292;&#24182;&#20801;&#35768;&#22312;&#23384;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#12290;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38543;&#26426;&#25513;&#30721;&#36807;&#31243;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#25968;&#25454;&#21453;&#28436;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#38544;&#31169;&#26426;&#21046;&#20197;&#21450;&#21442;&#19982;&#32773;&#30828;&#20214;&#21644;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we design an efficient distributed iterative learning method based on support vector machines (SVMs), which tackles federated classification and regression. The proposed method supports efficient computations and model exchange in a network of heterogeneous nodes and allows personalization of the learning model in the presence of non-i.i.d. data. To further enhance privacy, we introduce a random mask procedure that helps avoid data inversion. Finally, we analyze the impact of the proposed privacy mechanisms and the heterogeneity of participant hardware and data on the system performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.10158</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32508;&#36848;&#65306;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#20135;&#29983;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#21487;&#29992;&#20110;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20016;&#23500;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#22312;AI&#20013;&#30340;&#20316;&#29992;&#24471;&#21040;&#20102;&#26174;&#33879;&#25918;&#22823;&#65292;&#24341;&#21457;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#36825;&#19968;&#26032;&#20852;&#27010;&#24565;&#30340;&#20986;&#29616;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#27880;&#24847;&#21147;&#36880;&#28176;&#20174;&#25512;&#36827;&#27169;&#22411;&#35774;&#35745;&#36716;&#21521;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#30340;&#24517;&#35201;&#24615;&#65292;&#38543;&#21518;&#20174;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#29702;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#20197;&#21450;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#25105;&#20204;&#36824;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#35752;&#35770;&#20102;&#25361;&#25112;&#65292;&#24182;&#21015;&#20986;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#25552;&#20379;&#36328;&#36234;&#21508;&#20010;&#38454;&#27573;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#20840;&#29699;&#35270;&#35282;&#30340;&#32508;&#21512;&#24615;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#22270;&#20687;&#37325;&#24314;&#32593;&#32476;&#35757;&#32451;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#19988;&#22312;&#32570;&#23569;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#30340;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2303.09642</link><description>&lt;p&gt;
SUD$^2$:&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#37325;&#24314;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SUD$^2$: Supervision by Denoising Diffusion Models for Image Reconstruction. (arXiv:2303.09642v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#22270;&#20687;&#37325;&#24314;&#32593;&#32476;&#35757;&#32451;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#19988;&#22312;&#32570;&#23569;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#30340;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22270;&#20687;&#21453;&#38382;&#39064;&#65288;&#22914;&#22270;&#20687;&#20462;&#22797;&#21644;&#21435;&#38654;&#65289;&#37117;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#21069;&#21521;&#27169;&#22411;&#26159;&#26410;&#30693;&#30340;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#26410;&#30693;&#30340;&#28508;&#22312;&#21442;&#25968;&#12290;&#34429;&#28982;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#25104;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#26679;&#30340;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22270;&#20687;&#37325;&#24314;&#32593;&#32476;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#20687;&#21435;&#22122;&#31639;&#27861;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#22312;&#32570;&#23569;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30417;&#30563;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many imaging inverse problems$\unicode{x2014}$such as image-dependent in-painting and dehazing$\unicode{x2014}$are challenging because their forward models are unknown or depend on unknown latent parameters. While one can solve such problems by training a neural network with vast quantities of paired training data, such paired training data is often unavailable. In this paper, we propose a generalized framework for training image reconstruction networks when paired training data is scarce. In particular, we demonstrate the ability of image denoising algorithms and, by extension, denoising diffusion models to supervise network training in the absence of paired training data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#31216;&#20026;BBCT&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#21387;&#32553;&#25972;&#20010;Transformer&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#23454;&#29616;&#23569;&#20110;1&#65285;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2303.09184</link><description>&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#20301;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#31216;&#20026;BBCT&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#21387;&#32553;&#25972;&#20010;Transformer&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#23454;&#29616;&#23569;&#20110;1&#65285;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;BERT&#12289;GPT-3&#21644;ChatGPT&#31561;&#36817;&#26399;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27969;&#34892;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#35745;&#31639;&#37327;&#12289;&#24040;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#39640;&#24310;&#36831;&#26159;&#20113;&#35745;&#31639;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BBCT&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;Transformer&#30340;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#25105;&#20204;&#20197;&#39640;&#25928;BERT&#20026;&#26696;&#20363;&#65292;&#20351;&#29992;BBCT&#26041;&#27861;&#36827;&#34892;&#21387;&#32553;&#12290;&#25105;&#20204;&#22312;General Language Understanding Evaluation(GLUE)&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#23567;&#20110;1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1% accuracy drop in most tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#24182;&#23558;&#20809;&#21051;&#31995;&#32479;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#65292;&#20351;&#29992;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08435</link><description>&lt;p&gt;
&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#22330;&#30340;&#29289;&#29702;&#20449;&#24687;&#20809;&#23398;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields. (arXiv:2303.08435v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#24182;&#23558;&#20809;&#21051;&#31995;&#32479;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#65292;&#20351;&#29992;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#21051;&#26159;&#38598;&#25104;&#30005;&#36335;&#21046;&#36896;&#30340;&#22522;&#30784;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#30340;&#21457;&#23637;&#32531;&#35299;&#20102;&#21046;&#36896;&#36807;&#31243;&#24320;&#38144;&#21644;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;&#26041;&#27861;&#37117;&#23558;&#20809;&#21051;&#31995;&#32479;&#35270;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#40657;&#30418;&#26144;&#23556;&#65292;&#21033;&#29992;&#32593;&#32476;&#21442;&#25968;&#36890;&#36807;&#27515;&#35760;&#30828;&#32972;&#26144;&#23556;&#22823;&#37327;&#30340;&#25513;&#27169;&#21040;&#31354;&#20013;&#25110;&#25513;&#27169;&#21040;&#30005;&#38459;&#22270;&#20687;&#23545;&#65292;&#23548;&#33268;&#25512;&#24191;&#33021;&#21147;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#20005;&#26684;&#30340;&#20809;&#21051;&#27169;&#22411;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#12290;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#20197;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#20809;&#21051;&#31995;&#32479;&#65292;&#21516;&#26102;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#36827;&#34892;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lithography is fundamental to integrated circuit fabrication, necessitating large computation overhead. The advancement of machine learning (ML)-based lithography models alleviates the trade-offs between manufacturing process expense and capability. However, all previous methods regard the lithography system as an image-to-image black box mapping, utilizing network parameters to learn by rote mappings from massive mask-to-aerial or mask-to-resist image pairs, resulting in poor generalization capability. In this paper, we propose a new ML-based paradigm disassembling the rigorous lithographic model into non-parametric mask operations and learned optical kernels containing determinant source, pupil, and lithography information. By optimizing complex-valued neural fields to perform optical kernel regression from coordinates, our method can accurately restore lithography system using a small-scale training dataset with fewer parameters, demonstrating superior generalization capability as w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#29983;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;FlyHash&#27169;&#22411;&#21644;&#20854;&#20182;&#38750;&#31232;&#30095;&#27169;&#22411;&#22312;&#36335;&#32447;&#36319;&#38543;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#21457;&#29616;FlyHash&#27169;&#22411;&#22312;&#25968;&#25454;&#32534;&#30721;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.08109</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#12289;&#20223;&#29983;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#32447;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Vision-based route following by an embodied insect-inspired sparse neural network. (arXiv:2303.08109v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08109
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#29983;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;FlyHash&#27169;&#22411;&#21644;&#20854;&#20182;&#38750;&#31232;&#30095;&#27169;&#22411;&#22312;&#36335;&#32447;&#36319;&#38543;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#21457;&#29616;FlyHash&#27169;&#22411;&#22312;&#25968;&#25454;&#32534;&#30721;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31181;&#21483;&#20570;FlyHash&#27169;&#22411;&#65288;Dasgupta et al., 2017&#65289;&#30340;&#20223;&#29983;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#38750;&#31232;&#30095;&#27169;&#22411;&#22312;&#36335;&#32447;&#36319;&#38543;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#20219;&#21153;&#38656;&#35201;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#24403;&#21069;&#30340;&#35270;&#35273;&#36755;&#20837;&#21644;&#27839;&#36884;&#23384;&#20648;&#30340;&#35760;&#24518;&#26469;&#25511;&#21046;&#36716;&#21521;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;FlyHash&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#32534;&#30721;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network (Dasgupta et al., 2017), to similar but non-sparse models in an embodied navigation task. This requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded the FlyHash model is more efficient than others, especially in terms of data encoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.07909</link><description>&lt;p&gt;
&#29983;&#25104;AI&#20013;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#27969;&#34892;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#24037;&#20316;&#65292;&#26412;&#35843;&#26597;&#20174;&#31616;&#21333;&#20171;&#32461;&#22522;&#26412;&#25193;&#25955;&#27169;&#22411;&#22914;&#20309;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#24320;&#22987;&#65292;&#25509;&#30528;&#26159;&#26465;&#20214;&#25110;&#24341;&#23548;&#22914;&#20309;&#25913;&#36827;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24635;&#32467;&#20102;&#25991;&#26412;&#24341;&#23548;&#21019;&#24847;&#29983;&#25104;&#21644;&#22270;&#20687;&#32534;&#36753;&#30340;&#24212;&#29992;&#12290;&#38500;&#20102;&#36804;&#20170;&#20026;&#27490;&#25152;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#30340;&#20272;&#35745;&#20540;&#36807;&#20110;&#20048;&#35266;&#65292;&#23481;&#26131;&#23548;&#33268;&#26041;&#27861;&#30340;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07272</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#22810;&#37325;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
What is the state of the art? Accounting for multiplicity in machine learning benchmark performance. (arXiv:2303.07272v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07272
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#30340;&#20272;&#35745;&#20540;&#36807;&#20110;&#20048;&#35266;&#65292;&#23481;&#26131;&#23548;&#33268;&#26041;&#27861;&#30340;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26469;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#36825;&#20801;&#35768;&#22810;&#31181;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#24182;&#36328;&#36234;&#26102;&#38388;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#38382;&#39064;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#24615;&#33021;&#34987;&#31216;&#20026;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#65292;&#24182;&#19988;&#34987;&#29992;&#20316;&#26032;&#26041;&#27861;&#20986;&#29256;&#30340;&#21442;&#32771;&#28857;&#12290;&#20294;&#20351;&#29992;&#26368;&#39640;&#25490;&#21517;&#30340;&#24615;&#33021;&#20316;&#20026;SOTA&#30340;&#20272;&#35745;&#20540;&#26159;&#19968;&#31181;&#26377;&#20559;&#30340;&#20272;&#35745;&#22120;&#65292;&#20250;&#32473;&#20986;&#36807;&#20110;&#20048;&#35266;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#22810;&#37325;&#24615;&#30340;&#26426;&#21046;&#26159;&#22810;&#37325;&#27604;&#36739;&#21644;&#22810;&#37325;&#26816;&#39564;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#20294;&#22312;&#20851;&#20110;SOTA&#20272;&#35745;&#30340;&#35752;&#35770;&#20013;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#25552;&#21450;&#12290;&#36807;&#20110;&#20048;&#35266;&#30340;&#26368;&#20808;&#36827;&#20272;&#35745;&#20540;&#34987;&#29992;&#20316;&#35780;&#20272;&#26032;&#26041;&#27861;&#30340;&#26631;&#20934;&#65292;&#32780;&#20855;&#26377;&#26126;&#26174;&#21155;&#21183;&#32467;&#26524;&#30340;&#26041;&#27861;&#24456;&#23481;&#26131;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#21644;&#22810;&#27169;&#24577;&#20223;&#30495;&#26694;&#26550;&#30340;V2X&#36890;&#20449;&#22330;&#26223;DT&#21019;&#24314;&#21644;&#20223;&#30495;&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#31227;&#21160;&#24615;V2X&#36890;&#20449;&#29615;&#22659;&#19979;&#20934;&#30830;&#27169;&#25311;&#20986;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#20449;&#36947;&#12290;</title><link>http://arxiv.org/abs/2303.06947</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#27169;&#24577;&#20223;&#30495;&#26694;&#26550;&#65292;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#22522;&#20110;&#21160;&#24577;&#29615;&#22659;&#30340;V2X&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X Communications in Dynamic Environments. (arXiv:2303.06947v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#21644;&#22810;&#27169;&#24577;&#20223;&#30495;&#26694;&#26550;&#30340;V2X&#36890;&#20449;&#22330;&#26223;DT&#21019;&#24314;&#21644;&#20223;&#30495;&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#31227;&#21160;&#24615;V2X&#36890;&#20449;&#29615;&#22659;&#19979;&#20934;&#30830;&#27169;&#25311;&#20986;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#20449;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#29289;&#29702;&#26080;&#32447;&#29615;&#22659;&#30340;&#31934;&#30830;&#34394;&#25311;&#34920;&#31034;&#65292;&#21487;&#22312;&#29289;&#29702;&#36890;&#20449;&#35774;&#22791;&#19978;&#23454;&#29616;&#22810;&#23618;&#20915;&#31574;&#12290;&#22312;&#39640;&#39057;&#27573;&#65292;DT&#21487;&#24110;&#21161;&#20811;&#26381;V2X&#36890;&#20449;&#39640;&#31227;&#21160;&#24615;&#29615;&#22659;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;V2X&#36890;&#20449;&#22330;&#26223;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#24037;&#20316;&#27969;&#29992;&#20110;&#21019;&#24314;DT&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20223;&#30495;&#26694;&#26550;&#29992;&#20110;&#20135;&#29983;&#36924;&#30495;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#20934;&#30830;&#30340;&#27627;&#31859;&#27874;/&#20122;&#27627;&#31859;&#27874;&#26080;&#32447;&#20449;&#36947;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#34394;&#24187;&#24341;&#25806;&#28216;&#25103;&#24341;&#25806;&#30340;&#27773;&#36710;&#20223;&#30495;&#21644;&#27979;&#35797;&#26694;&#26550;&#20197;&#21450;&#20934;&#30830;&#30340;&#23556;&#32447;&#36319;&#36394;&#20449;&#36947;&#27169;&#25311;&#22120;&#12290;&#22312;&#22478;&#24066;&#22330;&#26223;&#19979;&#30340;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#30784;&#35774;&#26045;&#21644;&#33258;&#36710;&#30340;&#36924;&#30495;&#20256;&#24863;&#22120;&#21644;&#20449;&#36947;&#24314;&#27169;&#22343;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital Twins (DTs) for physical wireless environments have been recently proposed as accurate virtual representations of the propagation environment that can enable multi-layer decisions at the physical communication equipment. At high frequency bands, DTs can help to overcome the challenges emerging in the high mobility conditions featuring vehicular environments. In this paper, we propose a novel data-driven workflow for the creation of the DT of a Vehicle-to-Everything (V2X) communication scenario and a multi-modal simulation framework for the generation of realistic sensor data and accurate mmWave/sub-THz wireless channels. The proposed method leverages an automotive simulation and testing framework based on the Unreal Engine game engine and an accurate ray-tracing channel simulator. Simulations over an urban scenario show the achievable realistic sensor and channel modelling both at the infrastructure and at an ego-vehicle.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#20849;&#36827;&#21270;&#29305;&#24449;&#21644;&#20840;&#23616;&#20851;&#27880;&#26426;&#21046;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#32771;&#34385;&#21040;&#25152;&#26377;&#27531;&#22522;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.06945</link><description>&lt;p&gt;
CoGANPPIS: &#22522;&#20110;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction. (arXiv:2303.06945v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#20849;&#36827;&#21270;&#29305;&#24449;&#21644;&#20840;&#23616;&#20851;&#27880;&#26426;&#21046;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#32771;&#34385;&#21040;&#25152;&#26377;&#27531;&#22522;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#22312;&#29983;&#21270;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#65288;PPIs&#65289;&#21487;&#20197;&#21152;&#28145;&#25105;&#20204;&#23545;&#29983;&#29289;&#26426;&#29702;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;&#26032;&#33647;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;PPI&#39044;&#27979;&#23454;&#39564;&#26041;&#27861;&#25104;&#26412;&#39640;&#26114;&#65292;&#32791;&#26102;&#38271;&#65292;&#22240;&#27492;&#36817;&#24180;&#26469;&#24320;&#21457;&#20102;&#35768;&#22810;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#25366;&#25496;&#20102;&#19968;&#20123;&#26377;&#29992;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#20294;&#26410;&#32771;&#34385;&#21040;&#20849;&#36827;&#21270;&#29305;&#24449;&#65292;&#21518;&#32773;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#32447;&#32034;&#65307;&#65288;2&#65289;attention-based&#27169;&#22411;&#20165;&#20026;&#30456;&#37051;&#27531;&#22522;&#20998;&#37197;&#20851;&#27880;&#26435;&#37325;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#20998;&#37197;&#65292;&#24573;&#30053;&#20102;&#36828;&#31163;&#30446;&#26631;&#27531;&#22522;&#30340;&#19968;&#20123;&#27531;&#22522;&#21487;&#33021;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;PPI&#20301;&#28857;&#39044;&#27979;&#30340;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#20849;&#36827;&#21270;&#29305;&#24449;&#21644;&#20840;&#23616;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#32771;&#34385;&#21040;&#34507;&#30333;&#24207;&#21015;&#20013;&#25152;&#26377;&#27531;&#22522;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein-protein interactions are essential in biochemical processes. Accurate prediction of the protein-protein interaction sites (PPIs) deepens our understanding of biological mechanism and is crucial for new drug design. However, conventional experimental methods for PPIs prediction are costly and time-consuming so that many computational approaches, especially ML-based methods, have been developed recently. Although these approaches have achieved gratifying results, there are still two limitations: (1) Most models have excavated some useful input features, but failed to take coevolutionary features into account, which could provide clues for inter-residue relationships; (2) The attention-based models only allocate attention weights for neighboring residues, instead of doing it globally, neglecting that some residues being far away from the target residues might also matter.  We propose a coevolution-enhanced global attention neural network, a sequence-based deep learning model for P
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;PyTorch&#21644;Firedrake&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23454;&#29616;&#23545;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.06871</link><description>&lt;p&gt;
&#21033;&#29992;PyTorch&#21644;Firedrake&#23454;&#29616;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-driven machine learning models coupling PyTorch and Firedrake. (arXiv:2303.06871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;PyTorch&#21644;Firedrake&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23454;&#29616;&#23545;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#25551;&#36848;&#21644;&#24314;&#27169;&#31185;&#23398;&#21644;&#24037;&#31243;&#23398;&#20013;&#35768;&#22810;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;PDE&#24314;&#27169;&#20165;&#25552;&#20379;&#20102;&#29289;&#29702;&#27169;&#22411;&#30340;&#19981;&#23436;&#25972;&#25551;&#36848;&#12290;&#22522;&#20110;PDE&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26088;&#22312;&#35299;&#20915;&#36825;&#31181;&#38480;&#21046;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;PDE&#29992;&#20316;&#24402;&#32435;&#20559;&#32622;&#65292;&#20351;&#32806;&#21512;&#27169;&#22411;&#33021;&#22815;&#20381;&#36182;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23558;PDE&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#39640;&#24615;&#33021;&#27169;&#25311;&#37096;&#32626;&#21040;&#22797;&#26434;&#38382;&#39064;&#20013;&#38656;&#35201;&#32452;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;PDE&#30340;&#26694;&#26550;&#25552;&#20379;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32806;&#21512;&#26041;&#24335;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;PyTorch&#21644;PDE&#31995;&#32479;Firedrake&#30456;&#32467;&#21512;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#24037;&#31243;&#24072;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25351;&#23450;&#32806;&#21512;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#23545;&#29616;&#26377;&#20195;&#30721;&#36827;&#34892;&#24494;&#19981;&#36275;&#36947;&#30340;&#26356;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are central to describing and modelling complex physical systems that arise in many disciplines across science and engineering. However, in many realistic applications PDE modelling provides an incomplete description of the physics of interest. PDE-based machine learning techniques are designed to address this limitation. In this approach, the PDE is used as an inductive bias enabling the coupled model to rely on fundamental physical laws while requiring less training data. The deployment of high-performance simulations coupling PDEs and machine learning to complex problems necessitates the composition of capabilities provided by machine learning and PDE-based frameworks. We present a simple yet effective coupling between the machine learning framework PyTorch and the PDE system Firedrake that provides researchers, engineers and domain specialists with a high productive way of specifying coupled models while only requiring trivial changes to existi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DLHDMD&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.06289</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;Hankel&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Enhanced Hankel Dynamic-Mode Decomposition. (arXiv:2303.06289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DLHDMD&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a deep learning DMD based method, called DLHDMD, which uses the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics, and is able to generate accurate dynamics for chaotic time series.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#21464;&#24471;&#36234;&#26469;&#36234;&#31616;&#21333;&#21644;&#22797;&#26434;&#65292;&#20294;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#24320;&#21457;&#21160;&#24577;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#38382;&#39064;&#39046;&#22495;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#24050;&#32463;&#19982;&#25152;&#35859;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#36890;&#29992;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#29305;&#21035;&#26377;&#21069;&#36884;&#30340;&#31934;&#23494;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#24320;&#21457;&#36884;&#24452;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#28145;&#24230;&#23398;&#20064;Hankel DMD&#65288;DLHDMD&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DLHDMD&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#65292;&#24182;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#23398;&#20064;&#26144;&#23556;&#65292;&#36825;&#20123;&#26144;&#23556;&#22312;&#25104;&#21151;&#35757;&#32451;&#21518;&#24448;&#24448;&#36235;&#21521;&#20110;&#26174;&#33879;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the acquisition of time series has become increasingly more straightforward and sophisticated, developing dynamical models from time series is still a challenging and ever evolving problem domain. Within the last several years, to address this problem, there has been a merging of machine learning tools with what is called the dynamic mode decomposition (DMD). This general approach has been shown to be an especially promising avenue for sophisticated and accurate model development. Building on this prior body of work, we develop a deep learning DMD based method which makes use of the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). We show that the DLHDMD is able to generate accurate dynamics for chaotic time series, and we likewise explore how our method learns mappings which tend, after successful training, to significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2303.05735</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#24418;&#30340;&#30828;&#20214;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Hardware Acceleration of Neural Graphics. (arXiv:2303.05735v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#28210;&#26579;&#21644;&#21453;&#28210;&#26579;&#31639;&#27861;&#24050;&#34987;&#31070;&#32463;&#34920;&#31034;&#65288;NR&#65289;&#25152;&#21462;&#20195;&#12290;NR&#26368;&#36817;&#34987;&#29992;&#20110;&#23398;&#20064;&#22330;&#26223;&#30340;&#20960;&#20309;&#21644;&#26448;&#36136;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#21512;&#25104;&#30495;&#23454;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#25215;&#35834;&#29992;&#21487;&#20280;&#32553;&#30340;&#36136;&#37327;&#21644;&#21487;&#39044;&#27979;&#30340;&#24615;&#33021;&#26367;&#25442;&#20256;&#32479;&#30340;&#28210;&#26579;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#38382;&#39064;&#65306;&#31070;&#32463;&#22270;&#24418;&#65288;NG&#65289;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#20195;&#34920;&#24615;&#30340;NG&#24212;&#29992;&#31243;&#24207;&#65292;&#21457;&#29616;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#24403;&#21069;&#30340;GPU&#19978;&#20197;60FPS&#28210;&#26579;4K&#20998;&#36776;&#29575;&#65292;&#21017;&#25152;&#38656;&#24615;&#33021;&#19982;&#24403;&#21069;GPU&#30340;&#23454;&#38469;&#24615;&#33021;&#23384;&#22312;1.5&#20493;&#33267;55&#20493;&#30340;&#24046;&#36317;&#12290;&#23545;&#20110;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#65292;&#25152;&#38656;&#24615;&#33021;&#19982;&#25152;&#38656;&#31995;&#32479;&#21151;&#29575;&#20043;&#38388;&#23384;&#22312;&#26356;&#22823;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#65292;&#23545;&#20110;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#12289;&#22810;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#21644;&#20302;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#65292;&#23427;&#20204;&#21344;&#24212;&#29992;&#31243;&#24207;&#26102;&#38388;&#30340;72&#65285;&#12289;60&#65285;&#21644;59&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rendering and inverse-rendering algorithms that drive conventional computer graphics have recently been superseded by neural representations (NR). NRs have recently been used to learn the geometric and the material properties of the scenes and use the information to synthesize photorealistic imagery, thereby promising a replacement for traditional rendering algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (NG) need hardware support? We studied representative NG applications showing that, if we want to render 4k res. at 60FPS there is a gap of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications, there is an even larger gap of 2-4 OOM between the desired performance and the required system power. We identify that the input encoding and the MLP kernels are the performance bottlenecks, consuming 72%,60% and 59% of application time for multi res. hashgrid, multi res. densegrid and low res. densegrid 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20195;&#25968;&#20960;&#20309;&#26041;&#27861;&#32473;&#20986;&#20102;&#24352;&#37327;&#20998;&#35299;&#30340;&#23454;&#23545;&#25968;&#20856;&#33539;&#38408;&#20540;&#30340;&#19978;&#30028;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#29702;&#35770;&#35823;&#24046;&#65292;&#25581;&#31034;&#20102;&#24352;&#37327;&#20998;&#35299;&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2303.05731</link><description>&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#30340;&#23454;&#23545;&#25968;&#20856;&#33539;&#38408;&#20540;&#30340;&#19978;&#30028;&#21450;&#20854;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Upper Bound of Real Log Canonical Threshold of Tensor Decomposition and its Application to Bayesian Inference. (arXiv:2303.05731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20195;&#25968;&#20960;&#20309;&#26041;&#27861;&#32473;&#20986;&#20102;&#24352;&#37327;&#20998;&#35299;&#30340;&#23454;&#23545;&#25968;&#20856;&#33539;&#38408;&#20540;&#30340;&#19978;&#30028;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#29702;&#35770;&#35823;&#24046;&#65292;&#25581;&#31034;&#20102;&#24352;&#37327;&#20998;&#35299;&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#29616;&#22312;&#34987;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#12289;&#20449;&#24687;&#21387;&#32553;&#21644;&#30693;&#35782;&#24674;&#22797;&#12290;&#28982;&#32780;&#65292;&#24352;&#37327;&#20998;&#35299;&#30340;&#25968;&#23398;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#38416;&#26126;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#31181;&#22855;&#24322;&#23398;&#20064;&#26426;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20195;&#25968;&#20960;&#20309;&#26041;&#27861;&#32473;&#20986;&#20102;&#24352;&#37327;&#20998;&#35299;&#30340;&#23454;&#23545;&#25968;&#20856;&#33539;&#38408;&#20540;(RLCT)&#30340;&#19978;&#30028;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20102;&#20854;&#36125;&#21494;&#26031;&#27867;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#32473;&#20986;&#20102;&#20854;&#25968;&#23398;&#24615;&#36136;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor decomposition is now being used for data analysis, information compression, and knowledge recovery. However, the mathematical property of tensor decomposition is not yet fully clarified because it is one of singular learning machines. In this paper, we give the upper bound of its real log canonical threshold (RLCT) of the tensor decomposition by using an algebraic geometrical method and derive its Bayesian generalization error theoretically. We also give considerations about its mathematical property through numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#23450;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#25237;&#24433;&#12289;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#37327;&#35268;&#30340;&#26032;&#29305;&#24449;&#36798;&#21040;&#20102;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#20248;&#21152;&#36895;&#25910;&#25947;&#20445;&#35777; $O(1/T)$&#12289;&#24179;&#28369;&#38382;&#39064;&#30340; $O(1/T^2)$&#65292;&#20197;&#21450;&#20004;&#32773;&#37117;&#28385;&#36275;&#30340;&#21152;&#36895;&#32447;&#24615;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2303.05037</link><description>&lt;p&gt;
&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#30340;&#37327;&#35268;&#21644;&#21152;&#36895;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gauges and Accelerated Optimization over Smooth and/or Strongly Convex Sets. (arXiv:2303.05037v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#23450;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#25237;&#24433;&#12289;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#37327;&#35268;&#30340;&#26032;&#29305;&#24449;&#36798;&#21040;&#20102;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#20248;&#21152;&#36895;&#25910;&#25947;&#20445;&#35777; $O(1/T)$&#12289;&#24179;&#28369;&#38382;&#39064;&#30340; $O(1/T^2)$&#65292;&#20197;&#21450;&#20004;&#32773;&#37117;&#28385;&#36275;&#30340;&#21152;&#36895;&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#23450;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#27010;&#24565;&#19982;&#23427;&#20204;&#21463;&#27426;&#36814;&#30340;&#20989;&#25968;&#23545;&#24212;&#29289;&#30456;&#20284;&#65292;&#20294;&#22312;&#19968;&#38454;&#20248;&#21270;&#25991;&#29486;&#20013;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#25237;&#24433;&#12289;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#32447;&#24615;&#20248;&#21270;&#25110;&#25237;&#24433;&#39044;&#35328;&#26426;&#65292;&#20165;&#20351;&#29992;&#20415;&#23452;&#30340;&#19968;&#32500;&#32447;&#25628;&#32034;&#21644;&#27861;&#21521;&#37327;&#35745;&#31639;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#20248;&#21152;&#36895;&#25910;&#25947;&#20445;&#35777; $O(1/T)$&#12289;&#24179;&#28369;&#38382;&#39064;&#30340; $O(1/T^2)$&#65292;&#20197;&#21450;&#20004;&#32773;&#37117;&#28385;&#36275;&#30340;&#21152;&#36895;&#32447;&#24615;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#22522;&#20110;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#30340;&#38389;&#21487;&#22827;&#26031;&#22522;&#37327;&#30340;&#26032;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#65306;&#23613;&#31649;&#37327;&#35268;&#26082;&#19981;&#26159;&#24179;&#28369;&#30340;&#20063;&#19981;&#26159;&#24378;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#26174;&#31034;&#20102;&#35268;&#27169;&#30340;&#21152;&#24179;&#26041;&#22312;&#38598;&#21512;&#20013;&#32487;&#25215;&#20219;&#20309;&#23384;&#22312;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider feasibility and constrained optimization problems defined over smooth and/or strongly convex sets. These notions mirror their popular function counterparts but are much less explored in the first-order optimization literature. We propose new scalable, projection-free, accelerated first-order methods in these settings. Our methods avoid linear optimization or projection oracles, only using cheap one-dimensional linesearches and normal vector computations. Despite this, we derive optimal accelerated convergence guarantees of $O(1/T)$ for strongly convex problems, $O(1/T^2)$ for smooth problems, and accelerated linear convergence given both. Our algorithms and analysis are based on novel characterizations of the Minkowski gauge of smooth and/or strongly convex sets, which may be of independent interest: although the gauge is neither smooth nor strongly convex, we show the gauge squared inherits any structure present in the set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#21270;&#21464;&#21270;&#26356;&#24555;&#30340;&#21512;&#25104;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2303.04743</link><description>&lt;p&gt;
&#24102;&#26377;&#21452;&#21521;&#20808;&#39564;&#27169;&#22411;&#30340;&#21521;&#37327;&#37327;&#21270;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Vector Quantized Time Series Generation with a Bidirectional Prior Model. (arXiv:2303.04743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#21270;&#21464;&#21270;&#26356;&#24555;&#30340;&#21512;&#25104;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451; GAN &#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;RNN&#26063;&#36890;&#24120;&#22312;&#36828;&#31243;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21463;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986; TimeVQVAE&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#25216;&#26415;&#35299;&#20915; TSG &#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#20351;&#29992;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#22312;&#26102;&#38388; - &#39057;&#29575;&#22495;&#20013;&#36827;&#34892; VQ &#24314;&#27169;&#65292;&#20998;&#20026;&#20302;&#39057;&#65288;LF&#65289;&#21644;&#39640;&#39057;&#65288;HF&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20445;&#30041;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#24615;&#21464;&#21270;&#26356;&#24555;&#30340;&#26032;&#21512;&#25104;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#25513;&#34109;&#22270;&#20687;&#20316;&#20026;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03052</link><description>&lt;p&gt;
&#25513;&#34109;&#22270;&#20687;&#26159;&#40065;&#26834;&#24494;&#35843;&#30340;&#21453;&#20107;&#23454;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Masked Images Are Counterfactual Samples for Robust Fine-tuning. (arXiv:2303.03052v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#25513;&#34109;&#22270;&#20687;&#20316;&#20026;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#32780;&#21463;&#21040;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#23637;&#29616;&#20102;&#31354;&#21069;&#30340;&#40065;&#26834;&#24615;&#26469;&#24212;&#23545;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#20998;&#24067;&#20869;&#24615;&#33021;&#21644;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#22788;&#29702;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#22240;&#26524;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#25513;&#34109;&#22270;&#20687;&#20316;&#20026;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#24494;&#35843;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#31867;&#28608;&#27963;&#22270;&#23545;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20851;&#25110;&#35821;&#20041;&#26080;&#20851;&#34917;&#19969;&#36827;&#34892;&#25513;&#34109;&#65292;&#20197;&#25171;&#30772;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#29992;&#20854;&#20182;&#22270;&#20687;&#30340;&#34917;&#19969;&#26469;&#37325;&#26032;&#22635;&#20805;&#25513;&#34109;&#30340;&#34917;&#19969;&#12290;&#36825;&#20123;&#21453;&#20107;&#23454;&#26679;&#26412;&#21017;&#29992;&#20110;&#29305;&#24449;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are challenged by the distribution shift between the training data and test data. Recently, the large models pre-trained on diverse data demonstrate unprecedented robustness to various distribution shifts. However, fine-tuning on these models can lead to a trade-off between in-distribution (ID) performance and out-of-distribution (OOD) robustness. Existing methods for tackling this trade-off do not explicitly address the OOD robustness problem. In this paper, based on causal analysis on the aforementioned problems, we propose a novel fine-tuning method, which use masked images as counterfactual samples that help improving the robustness of the fine-tuning model. Specifically, we mask either the semantics-related or semantics-unrelated patches of the images based on class activation map to break the spurious correlation, and refill the masked patches with patches from other images. The resulting counterfactual samples are used in feature-based distillation with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25511;&#21046;GPU&#30340;&#33021;&#32791;&#26469;&#38477;&#20302;&#30899;&#25490;&#25918;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#30899;&#24378;&#24230;&#65292;&#23545;&#21508;&#31181;DNN&#24212;&#29992;&#31243;&#24207;&#36866;&#29992;&#65292;&#26080;&#38656;&#39069;&#22806;&#30828;&#20214;&#25110;&#22522;&#30784;&#35774;&#26045;&#12290;</title><link>http://arxiv.org/abs/2303.02508</link><description>&lt;p&gt;
&#36861;&#27714;&#23454;&#29992;&#21487;&#25345;&#32493;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20302;&#30899;&#30005;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training. (arXiv:2303.02508v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25511;&#21046;GPU&#30340;&#33021;&#32791;&#26469;&#38477;&#20302;&#30899;&#25490;&#25918;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#30899;&#24378;&#24230;&#65292;&#23545;&#21508;&#31181;DNN&#24212;&#29992;&#31243;&#24207;&#36866;&#29992;&#65292;&#26080;&#38656;&#39069;&#22806;&#30828;&#20214;&#25110;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#30001;&#20110;&#20351;&#29992;GPU&#36827;&#34892;&#35757;&#32451;&#65292;&#23548;&#33268;&#33021;&#32791;&#21644;&#30899;&#25490;&#25918;&#37327;&#22686;&#21152;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#23581;&#35797;&#23558;&#35757;&#32451;&#24037;&#20316;&#31227;&#21160;&#21040;&#30899;&#24378;&#24230;&#36739;&#20302;&#30340;&#20301;&#32622;&#25110;&#26102;&#38388;&#26694;&#26550;&#65292;&#20197;&#22238;&#24212;&#21487;&#25345;&#32493;&#24615;&#30340;&#21628;&#21505;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#25968;&#25454;&#27861;&#35268;&#31561;&#21407;&#22240;&#65292;&#23558;&#24037;&#20316;&#31227;&#21160;&#21040;&#20854;&#20182;&#22320;&#26041;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25512;&#36831;&#35757;&#32451;&#21487;&#33021;&#20250;&#23545;&#24212;&#29992;&#26381;&#21153;&#36136;&#37327;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#22240;&#20026;&#25903;&#25345;&#26381;&#21153;&#30340;DNN&#27809;&#26377;&#24471;&#21040;&#21450;&#26102;&#26356;&#26032;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#38477;&#20302;DNN&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#65292;&#32780;&#26080;&#38656;&#36801;&#31227;&#25110;&#25512;&#36831;&#24037;&#20316;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#23454;&#26102;&#30340;&#30899;&#24378;&#24230;&#21464;&#21270;&#24182;&#25511;&#21046;GPU&#30340;&#33021;&#32791;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#35757;&#32451;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#30899;&#36275;&#36857;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20027;&#21160;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#30005;&#32593;&#26465;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#26041;&#27861;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#30899;&#24378;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#30899;&#20943;&#25490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;DNN&#24212;&#29992;&#31243;&#24207;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30828;&#20214;&#25110;&#22522;&#30784;&#35774;&#26045;&#12290;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#30899;&#24378;&#24230;&#38477;&#20302;&#39640;&#36798;44%&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has experienced significant growth in recent years, resulting in increased energy consumption and carbon emission from the use of GPUs for training deep neural networks (DNNs). Answering the call for sustainability, conventional solutions have attempted to move training jobs to locations or time frames with lower carbon intensity. However, moving jobs to other locations may not always be feasible due to large dataset sizes or data regulations. Moreover, postponing training can negatively impact application service quality because the DNNs backing the service are not updated in a timely fashion. In this work, we present a practical solution that reduces the carbon footprint of DNN training without migrating or postponing jobs. Specifically, our solution observes real-time carbon intensity shifts during training and controls the energy consumption of GPUs, thereby reducing carbon footprint while maintaining training performance. Furthermore, in order to proactively adapt to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#65292;&#21457;&#29616;&#20351;&#29992;&#20445;&#30495;&#24230;(FoMs)&#30340;&#35780;&#20272;&#19981;&#19968;&#23450;&#19982;&#20219;&#21153;&#20026;&#22522;&#30784;&#30340;&#35780;&#20272;&#19968;&#33268;&#65292;&#32780;&#22522;&#20110;&#20449;&#21495;&#26816;&#27979;&#29702;&#35770;(SDT)&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#23458;&#35266;&#12289;&#26377;&#24847;&#20041;&#30340;&#21435;&#22122;&#25928;&#26524;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#35777;&#26126;&#34394;&#25311;&#20020;&#24202;&#35797;&#39564;&#65288;VCTs&#65289;&#26159;&#35780;&#20272;DL&#26041;&#27861;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.02110</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21435;&#22122;&#26041;&#27861;&#30340;&#23458;&#35266;&#20219;&#21153;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65306;&#20197;&#24515;&#32908;&#28748;&#27880;SPECT&#20026;&#32972;&#26223;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT. (arXiv:2303.02110v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#65292;&#21457;&#29616;&#20351;&#29992;&#20445;&#30495;&#24230;(FoMs)&#30340;&#35780;&#20272;&#19981;&#19968;&#23450;&#19982;&#20219;&#21153;&#20026;&#22522;&#30784;&#30340;&#35780;&#20272;&#19968;&#33268;&#65292;&#32780;&#22522;&#20110;&#20449;&#21495;&#26816;&#27979;&#29702;&#35770;(SDT)&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#23458;&#35266;&#12289;&#26377;&#24847;&#20041;&#30340;&#21435;&#22122;&#25928;&#26524;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#35777;&#26126;&#34394;&#25311;&#20020;&#24202;&#35797;&#39564;&#65288;VCTs&#65289;&#26159;&#35780;&#20272;DL&#26041;&#27861;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#26680;&#21307;&#23398;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26041;&#27861;&#21435;&#22122;&#20302;&#21058;&#37327;&#12289;&#30701;&#37319;&#38598;&#26102;&#38388;&#25110;&#20004;&#32773;&#21516;&#26102;&#33719;&#21462;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23458;&#35266;&#35780;&#20272;&#23545;&#20110;&#20020;&#24202;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;DL&#21435;&#22122;&#26680;&#21307;&#23398;&#22270;&#20687;&#36890;&#24120;&#20351;&#29992;&#31867;&#20284;RMSE&#21644;SSIM&#36825;&#26679;&#30340;&#20445;&#30495;&#24230;&#65288;FoMs&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#26159;&#20026;&#20020;&#24202;&#20219;&#21153;&#32780;&#37319;&#38598;&#30340;&#65292;&#22240;&#27492;&#24212;&#35813;&#26681;&#25454;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;(1)&#35843;&#26597;&#20351;&#29992;&#36825;&#20123;FoMs&#30340;&#35780;&#20272;&#26159;&#21542;&#19982;&#23458;&#35266;&#30340;&#20020;&#24202;&#20219;&#21153;&#35780;&#20272;&#19968;&#33268;; (2)&#25552;&#20379;&#29992;&#20110;&#30830;&#23450;&#21435;&#22122;&#23545;&#20449;&#21495;&#26816;&#27979;&#20219;&#21153;&#24433;&#21709;&#30340;&#29702;&#35770;&#20998;&#26512;; (3)&#23637;&#31034;&#34394;&#25311;&#20020;&#24202;&#35797;&#39564;&#65288;VCTs&#65289;&#29992;&#20110;&#35780;&#20272;DL&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#20351;&#29992;&#36924;&#30495;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#20102;&#19968;&#20010;VCT&#26469;&#35780;&#20272;DL&#21435;&#22122;&#24515;&#32908;&#28748;&#27880;SPECT&#22270;&#20687;&#26041;&#27861;&#12290;&#37319;&#29992;&#23458;&#35266;&#30340;&#24378;&#21046;&#36873;&#25321;&#23454;&#39564;&#65292;&#20351;&#29992;&#20449;&#21495;&#26816;&#27979;&#29702;&#35770;&#65288;SDT&#65289;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#25351;&#26631;&#21644;FoMs&#35780;&#20272;&#20102;&#21435;&#22122;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;FoMs&#35780;&#20272;&#21435;&#22122;&#25928;&#26524;&#19981;&#19968;&#23450;&#19982;&#22522;&#20110;&#20219;&#21153;&#30340;&#35780;&#20272;&#30456;&#20851;&#12290;SDT&#25351;&#26631;&#25552;&#20379;&#20102;&#26356;&#23458;&#35266;&#21644;&#26377;&#24847;&#20041;&#30340;&#21435;&#22122;&#25928;&#26524;&#35780;&#20272;&#26041;&#24335;&#12290;VCTs&#21487;&#20026;&#26680;&#21307;&#23398;&#20013;&#22522;&#20110;DL&#30340;&#21435;&#22122;&#26041;&#27861;&#30340;&#35780;&#20272;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence-based methods have generated substantial interest in nuclear medicine. An area of significant interest has been using deep-learning (DL)-based approaches for denoising images acquired with lower doses, shorter acquisition times, or both. Objective evaluation of these approaches is essential for clinical application. DL-based approaches for denoising nuclear-medicine images have typically been evaluated using fidelity-based figures of merit (FoMs) such as RMSE and SSIM. However, these images are acquired for clinical tasks and thus should be evaluated based on their performance in these tasks. Our objectives were to (1) investigate whether evaluation with these FoMs is consistent with objective clinical-task-based evaluation; (2) provide a theoretical analysis for determining the impact of denoising on signal-detection tasks; (3) demonstrate the utility of virtual clinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a DL-based method for denoisi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#27169;&#22411;&#24494;&#35843;&#21644;&#20462;&#21098;&#31561;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65307;&#36890;&#36807;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30830;&#20445;&#21024;&#38500;&#27700;&#21360;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10296</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21151;&#33021;&#32806;&#21512;&#27700;&#21360;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Function-Coupled Watermarks for Deep Neural Networks. (arXiv:2302.10296v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#27169;&#22411;&#24494;&#35843;&#21644;&#20462;&#21098;&#31561;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65307;&#36890;&#36807;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30830;&#20445;&#21024;&#38500;&#27700;&#21360;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#34920;&#29616;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#28023;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#30693;&#35782;&#20135;&#26435;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#27700;&#21360;&#25216;&#26415;&#65292;&#20854;&#20013;DNN&#25552;&#20379;&#21830;&#23558;&#31192;&#23494;&#20449;&#24687;&#26893;&#20837;&#27169;&#22411;&#20013;&#65292;&#20197;&#20415;&#22312;&#31245;&#21518;&#36890;&#36807;&#19968;&#20123;&#19987;&#29992;&#35302;&#21457;&#36755;&#20837;&#26816;&#32034;&#23884;&#20837;&#30340;&#27700;&#21360;&#32034;&#26435;&#65307;&#34429;&#28982;&#25991;&#29486;&#20013;&#25253;&#21578;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36973;&#21463;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65292;&#20363;&#22914;&#27169;&#22411;&#24494;&#35843;&#21644;&#27169;&#22411;&#20462;&#21098;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#19978;&#36848;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#36825;&#26679;&#21024;&#38500;&#27700;&#21360;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#26469;&#33258;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#31192;&#23494;&#29305;&#24449;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-performed deep neural networks (DNNs) generally require massive labelled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers implant secret information into the model so that they can later claim IP ownership by retrieving their embedded watermarks with some dedicated trigger inputs. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning and model pruning.  In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model's performance on normal inputs. To this end, unlike previous methods relying on secret features learnt from out-of-distribution data, our method only uses features lear
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#22270;&#24418;&#21464;&#25442;&#22120;&#65288;MGT&#65289;&#21644;&#23567;&#27874;&#20301;&#32622;&#32534;&#30721;&#65288;WavePE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#34920;&#31034;&#22823;&#20998;&#23376;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#22312;&#20247;&#22810;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.08647</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#22270;&#24418;&#21464;&#25442;&#22120;&#19982;&#23567;&#27874;&#20301;&#32622;&#32534;&#30721;&#29992;&#20110;&#23398;&#20064;&#20998;&#23618;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures. (arXiv:2302.08647v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#22270;&#24418;&#21464;&#25442;&#22120;&#65288;MGT&#65289;&#21644;&#23567;&#27874;&#20301;&#32622;&#32534;&#30721;&#65288;WavePE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#34920;&#31034;&#22823;&#20998;&#23376;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#22312;&#20247;&#22810;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22270;&#24418;&#23398;&#20064;&#31639;&#27861;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#22823;&#20998;&#23376;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#21407;&#23376;&#20043;&#38388;&#30340;&#20998;&#23618;&#20132;&#20114;&#65292;&#32780;&#36825;&#23545;&#20110;&#30830;&#23450;&#22823;&#20998;&#23376;&#30340;&#23646;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#22270;&#24418;&#21464;&#25442;&#22120;&#65288;MGT&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#23398;&#20064;&#34920;&#31034;&#22810;&#31181;&#23610;&#24230;&#19979;&#22823;&#20998;&#23376;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;MGT&#21487;&#20197;&#23398;&#20064;&#20135;&#29983;&#21407;&#23376;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#32452;&#25104;&#26377;&#24847;&#20041;&#30340;&#21151;&#33021;&#32452;&#25110;&#37325;&#22797;&#21333;&#20803;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#23567;&#27874;&#20301;&#32622;&#32534;&#30721;&#65288;WavePE&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#39057;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#30340;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#30001;&#32858;&#21512;&#29289;&#21644;&#22810;&#32957;&#32452;&#25104;&#30340;&#20004;&#20010;&#22823;&#20998;&#23376;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#20010;&#31867;&#20284;&#33647;&#29289;&#30340;&#20998;&#23376;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#20272;&#31639;&#20998;&#23376;&#24615;&#36136;&#65288;&#20363;&#22914;GAP&#65292;HOMO&#21644;LUMO&#65289;&#26102;&#36798;&#21040;&#20102;&#21270;&#23398;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contemporary graph learning algorithms are not well-defined for large molecules since they do not consider the hierarchical interactions among the atoms, which are essential to determine the molecular properties of macromolecules. In this work, we propose Multiresolution Graph Transformers (MGT), the first graph transformer architecture that can learn to represent large molecules at multiple scales. MGT can learn to produce representations for the atoms and group them into meaningful functional groups or repeating units. We also introduce Wavelet Positional Encoding (WavePE), a new positional encoding method that can guarantee localization in both spectral and spatial domains. Our proposed model achieves competitive results on two macromolecule datasets consisting of polymers and peptides, and one drug-like molecule dataset. Importantly, our model outperforms other state-of-the-art methods and achieves chemical accuracy in estimating molecular properties (e.g., GAP, HOMO and LUMO) calc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#30340;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#30340;&#27491;&#24335;&#20445;&#35777;&#30340;URRLC&#25968;&#25454;&#21253;&#35843;&#24230;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2302.07675</link><description>&lt;p&gt;
&#21033;&#29992;&#20005;&#26684;&#39044;&#27979;&#20445;&#35777;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#27969;&#37327;&#30340;&#21160;&#24577;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Dynamic Scheduling of Ultra-Reliable Low-Latency Traffic via Conformal Prediction. (arXiv:2302.07675v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#30340;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#30340;&#27491;&#24335;&#20445;&#35777;&#30340;URRLC&#25968;&#25454;&#21253;&#35843;&#24230;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#34892;&#30340;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#27969;&#37327;&#65288;URLLC&#65289;&#30340;&#21160;&#24577;&#35843;&#24230;&#21487;&#20197;&#36890;&#36807;&#20165;&#22312;&#24517;&#35201;&#26102;&#20998;&#37197;&#36164;&#28304;&#26469;&#26174;&#33879;&#25552;&#39640;&#22686;&#24378;&#22411;&#31227;&#21160;&#23485;&#24102;&#65288;eMBB&#65289;&#35774;&#22791;&#31561;&#20849;&#23384;&#26381;&#21153;&#30340;&#25928;&#29575;&#12290;&#20027;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;URRLC&#25968;&#25454;&#21253;&#20135;&#29983;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#23601;&#35201;&#27714;&#22312;&#26410;&#26469;&#24103;&#20013;&#20351;&#29992;URRLC&#27969;&#37327;&#39044;&#27979;&#22120;&#12290;&#23454;&#38469;&#19978;&#65292;&#36825;&#31181;&#39044;&#27979;&#21487;&#33021;&#20250;&#39640;&#20272;&#25110;&#20302;&#20272;&#35201;&#20135;&#29983;&#30340;URRLC&#25968;&#25454;&#37327;&#65292;&#23548;&#33268;&#39044;&#20808;&#20998;&#37197;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#36164;&#28304;&#29992;&#20110;URRLC&#25968;&#25454;&#21253;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;URRLC&#25968;&#25454;&#21253;&#35843;&#24230;&#31243;&#24207;&#65292;&#26080;&#35770;URRLC&#27969;&#37327;&#39044;&#27979;&#22120;&#30340;&#36136;&#37327;&#22914;&#20309;&#65292;&#37117;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#30340;&#27491;&#24335;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#36981;&#24490;&#26681;&#25454;&#21160;&#24577;&#35843;&#25972;&#20998;&#37197;&#30340;&#36164;&#28304;&#37327;&#30340;&#21407;&#21017;&#20197;&#28385;&#36275;
&lt;/p&gt;
&lt;p&gt;
The dynamic scheduling of ultra-reliable and low-latency traffic (URLLC) in the uplink can significantly enhance the efficiency of coexisting services, such as enhanced mobile broadband (eMBB) devices, by only allocating resources when necessary. The main challenge is posed by the uncertainty in the process of URLLC packet generation, which mandates the use of predictors for URLLC traffic in the coming frames. In practice, such prediction may overestimate or underestimate the amount of URLLC data to be generated, yielding either an excessive or an insufficient amount of resources to be pre-emptively allocated for URLLC packets. In this paper, we introduce a novel scheduler for URLLC packets that provides formal guarantees on reliability and latency irrespective of the quality of the URLLC traffic predictor. The proposed method leverages recent advances in online conformal prediction (CP), and follows the principle of dynamically adjusting the amount of allocated resources so as to meet
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#27880;&#20876;&#25968;&#25454;&#26469;&#35782;&#21035;&#26032;&#29992;&#25143;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#19988;&#22312;&#20165;&#26377;&#23569;&#37327;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#26356;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2302.07517</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#29305;&#23450;&#36816;&#21160;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Extensible Motion-based Identification of XR Users using Non-Specific Motion Data. (arXiv:2302.07517v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07517
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#27880;&#20876;&#25968;&#25454;&#26469;&#35782;&#21035;&#26032;&#29992;&#25143;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#19988;&#22312;&#20165;&#26377;&#23569;&#37327;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#24335;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#36317;&#31163;&#21644;&#20998;&#31867;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#36890;&#36807;&#29992;&#25143;&#30340;&#36816;&#21160;&#26469;&#35782;&#21035;&#25193;&#23637;&#29616;&#23454;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#8220;&#21322;&#34928;&#26399;&#65306;Alyx&#8221;VR&#28216;&#25103;&#30340;&#29992;&#25143;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#30340;&#22522;&#32447;&#20998;&#31867;&#27169;&#22411;&#20316;&#20026;&#23545;&#27604;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21482;&#20351;&#29992;&#20960;&#20998;&#38047;&#30340;&#27880;&#20876;&#25968;&#25454;&#65292;&#35782;&#21035;&#26032;&#29992;&#25143;&#30340;&#38750;&#29305;&#23450;&#36816;&#21160;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#37325;&#26032;&#35757;&#32451;&#22522;&#32447;&#26041;&#27861;&#38656;&#35201;&#33457;&#36153;&#23558;&#36817;&#19968;&#22825;&#30340;&#26102;&#38388;&#65292;&#24403;&#21482;&#26377;&#24456;&#23569;&#30340;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#21487;&#38752;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#20351;&#29992;&#19981;&#21516;VR&#35774;&#22791;&#35760;&#24405;&#30340;&#26032;&#29992;&#25143;&#25968;&#25454;&#38598;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#26131;&#20110;&#25193;&#23637;&#30340;XR&#29992;&#25143;&#35782;&#21035;&#31995;&#32479;&#22880;&#23450;&#22522;&#30784;&#65292;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we combine the strengths of distance-based and classification-based approaches for the task of identifying extended reality users by their movements. For this we present an embedding-based approach that leverages deep metric learning. We train the model on a dataset of users playing the VR game ``Half-Life: Alyx'' and conduct multiple experiments and analyses using a state of the art classification-based model as baseline. The results show that the embedding-based method 1) is able to identify new users from non-specific movements using only a few minutes of enrollment data, 2) can enroll new users within seconds, while retraining the baseline approach takes almost a day, 3) is more reliable than the baseline approach when only little enrollment data is available, 4) can be used to identify new users from another dataset recorded with different VR devices.  Altogether, our solution is a foundation for easily extensible XR user identification systems, applicable to a wide
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33033;&#20914;&#24418;&#29366;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;PMT&#39281;&#21644;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;&#32447;&#24615;&#21306;&#22495;&#24182;&#25552;&#39640;&#20809;&#23376;&#35745;&#25968;&#21644;&#33021;&#37327;&#37325;&#24314;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.06170</link><description>&lt;p&gt;
&#20351;&#29992;&#33033;&#20914;&#24418;&#29366;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;PMT&#30340;&#39281;&#21644;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Restoring the saturation response of a PMT using pulse-shape and artificial-neural-networks. (arXiv:2302.06170v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06170
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33033;&#20914;&#24418;&#29366;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;PMT&#39281;&#21644;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;&#32447;&#24615;&#21306;&#22495;&#24182;&#25552;&#39640;&#20809;&#23376;&#35745;&#25968;&#21644;&#33021;&#37327;&#37325;&#24314;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#20493;&#22686;&#31649;&#65288;PMT&#65289;&#30340;&#32447;&#24615;&#21709;&#24212;&#26159;&#20809;&#23376;&#35745;&#25968;&#21644;&#20013;&#24494;&#23376;&#33021;&#37327;&#37325;&#24314;&#30340;&#24517;&#35201;&#23646;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#32447;&#24615;&#28919;&#22522;&#33519;&#65288;LAB&#65289;&#30340;&#28082;&#20307;&#38378;&#28865;&#20307;&#30740;&#31350;&#20102;PMT&#30340;&#32447;&#24615;&#26377;&#25928;&#21306;&#22495;&#21644;&#39281;&#21644;&#21709;&#24212;&#12290;&#35266;&#23519;&#21040;&#20102;&#20004;&#31181;&#19981;&#21516;&#39281;&#21644;&#21709;&#24212;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21363;&#33033;&#20914;&#24418;&#29366;&#22833;&#30495;&#21644;&#33033;&#20914;&#38754;&#31215;&#20943;&#23567;&#12290;&#35266;&#23519;&#21040;&#30340;&#33033;&#20914;&#24418;&#29366;&#20026;&#20272;&#35745;&#33033;&#20914;&#38754;&#31215;&#30456;&#23545;&#32447;&#24615;&#21306;&#22495;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#35786;&#26029;&#20801;&#35768;&#21407;&#22320;&#20272;&#35745;&#32447;&#24615;&#33539;&#22260;&#65292;&#36825;&#22312;&#20197;&#21069;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#21033;&#29992;&#27979;&#24471;&#30340;&#20004;&#20010;&#39281;&#21644;&#21709;&#24212;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26469;&#39044;&#27979;&#20174;&#35266;&#23519;&#21040;&#30340;&#33033;&#20914;&#24418;&#29366;&#20013;&#20943;&#23567;&#30340;&#33033;&#20914;&#38754;&#31215;&#12290;ANN&#39044;&#27979;&#30340;&#33033;&#20914;&#38754;&#31215;&#20943;&#23567;&#20351;&#24471;&#21487;&#20197;&#39044;&#27979;&#29420;&#31435;&#20110;&#39281;&#21644;&#34892;&#20026;&#30340;&#29702;&#24819;&#20809;&#30005;&#23376;&#25968;&#12290;&#36825;&#31181;&#22522;&#20110;&#33033;&#20914;&#24418;&#29366;&#30340;&#26041;&#27861;&#20272;&#35745;PMT&#30340;&#32447;&#24615;&#33539;&#22260;&#24182;&#20351;&#29992;ANN&#24674;&#22797;&#39281;&#21644;&#21709;&#24212;&#26159;&#22686;&#24378;&#20013;&#24494;&#23376;&#25506;&#27979;&#22120;&#20013;&#20809;&#23376;&#35745;&#25968;&#21644;&#33021;&#37327;&#37325;&#24314;&#25928;&#29575;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The linear response of a photomultiplier tube (PMT) is a required property for photon counting and reconstruction of the neutrino energy. The linearity valid region and the saturation response of PMT were investigated using a linear-alkyl-benzene (LAB)-based liquid scintillator. A correlation was observed between the two different saturation responses, with pulse-shape distortion and pulse-area decrease. The observed pulse-shape provides useful information for the estimation of the linearity region relative to the pulse-area. This correlation-based diagnosis allows an ${in}$-${situ}$ estimation of the linearity range, which was previously challenging. The measured correlation between the two saturation responses was employed to train an artificial-neural-network (ANN) to predict the decrease in pulse-area from the observed pulse-shape. The ANN-predicted pulse-area decrease enables the prediction of the ideal number of photoelectrons irrelevant to the saturation behavior. This pulse-sha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;I$^2$SB&#65292;&#30452;&#25509;&#23398;&#20064;&#20004;&#20010;&#32473;&#23450;&#20998;&#24067;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#36793;&#30028;&#23545;&#27714;&#35299;&#30340;&#26041;&#27861;&#20351;&#24471;I$^2$SB&#35757;&#32451;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#26356;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.05872</link><description>&lt;p&gt;
I$^2$SB&#65306;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;Schr\"odinger&#26725;
&lt;/p&gt;
&lt;p&gt;
I$^2$SB: Image-to-Image Schr\"odinger Bridge. (arXiv:2302.05872v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05872
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;I$^2$SB&#65292;&#30452;&#25509;&#23398;&#20064;&#20004;&#20010;&#32473;&#23450;&#20998;&#24067;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#36793;&#30028;&#23545;&#27714;&#35299;&#30340;&#26041;&#27861;&#20351;&#24471;I$^2$SB&#35757;&#32451;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#26356;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21363;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;Schr\"odinger&#26725;&#65288;I$^2$SB&#65289;&#65292;&#30452;&#25509;&#23398;&#20064;&#20004;&#20010;&#32473;&#23450;&#20998;&#24067;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#12290;&#36825;&#20123;&#25193;&#25955;&#26725;&#23545;&#20110;&#22270;&#20687;&#24674;&#22797;&#29305;&#21035;&#26377;&#29992;&#65292;&#22240;&#20026;&#36864;&#21270;&#22270;&#20687;&#26159;&#37325;&#26500;&#28165;&#26224;&#22270;&#20687;&#30340;&#32467;&#26500;&#20449;&#24687;&#20808;&#39564;&#12290; I$^2$SB&#23646;&#20110;&#19968;&#31867;&#21487;&#22788;&#29702;&#30340;Schr\"odinger&#26725;&#27169;&#22411;&#65292;&#23427;&#26159;&#24471;&#20998;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#25193;&#23637;&#65292;&#20854;&#36793;&#30028;&#23545;&#30340;&#36793;&#32536;&#20998;&#24067;&#21487;&#20197;&#22312;&#35299;&#26512;&#19978;&#35745;&#31639;&#12290;&#36825;&#31181;&#36890;&#36807;&#36793;&#30028;&#23545;&#27714;&#35299;&#30340;&#26041;&#27861;&#20351;&#24471;I$^2$SB&#35757;&#32451;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#26694;&#26550;&#65292;&#36827;&#32780;&#37319;&#29992;&#22312;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#23454;&#29992;&#25216;&#26415;&#65292;&#20351;&#24471;I$^2$SB&#35757;&#32451;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;ImageNet 256x256&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;I$^2$SB&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20462;&#22797;&#65292;&#36229;&#20998;&#36776;&#29575;&#65292;&#21435;&#27169;&#31946;&#21644;JPEG&#24674;&#22797;&#65292;&#24182;&#34920;&#26126;I$^2$SB&#36229;&#36807;&#20102;&#26631;&#20934;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schr\"odinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05743</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#38752;&#36317;&#31163;&#30697;&#38453;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24120;&#29992;&#20110;&#28041;&#21450;&#22270;&#24418;&#20960;&#20309;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#34429;&#28982;&#20960;&#20309;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#21253;&#21547;&#23436;&#25972;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26080;&#27861;&#23398;&#20064;&#36825;&#31181;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#26032;&#39062;&#30340;&#23545;&#31216;&#20960;&#20309;&#22270;&#30340;&#23478;&#26063;&#65292;&#25193;&#23637;&#20102;MPNN&#26080;&#27861;&#21306;&#20998;&#20854;&#36317;&#31163;&#30697;&#38453;&#30340;&#21453;&#20363;&#23478;&#26063;&#65292;&#24182;&#25552;&#20986;$k$-DisGNNs&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#20960;&#20309;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;$k$-DisGNNs&#30340;&#29305;&#27530;&#24773;&#20917;&#32479;&#19968;&#36215;&#26469;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#37027;&#20123;&#26368;&#21021;&#20026;&#20302;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#35774;&#35745;&#30340;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05666</link><description>&lt;p&gt;
Jaccard&#24230;&#37327;&#25439;&#22833;&#65306;&#20351;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Jaccard&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IoU&#25439;&#22833;&#26159;&#30452;&#25509;&#20248;&#21270;Jaccard&#25351;&#25968;&#30340;&#26367;&#20195;&#21697;&#12290;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#23558;IoU&#25439;&#22833;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#65292;&#19982;&#20165;&#20248;&#21270;&#20687;&#32032;&#25439;&#22833;&#65288;&#22914;&#20132;&#21449;&#29109;&#25439;&#22833;&#65289;&#30456;&#27604;&#65292;&#23545;&#20110;Jaccard&#25351;&#25968;&#27979;&#37327;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#26174;&#30528;&#30340;IoU&#25439;&#22833;&#26159;&#36719;Jaccard&#25439;&#22833;&#21644;Lovasz-Softmax&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25439;&#22833;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#36719;&#26631;&#31614;&#19981;&#20860;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#65292;&#23427;&#20204;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;&#36719;&#26631;&#31614;&#20860;&#23481;&#65292;&#19982;&#36719;Jaccard&#25439;&#22833;&#30456;&#21516;&#12290;&#20351;&#29992;JMLs&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26368;&#27969;&#34892;&#30340;&#36719;&#26631;&#31614;&#29992;&#20363;&#65306;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#19977;&#20010;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;Cityscapes&#12289;PASCAL VOC&#21644;DeepGlobe Land&#65289;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;DeepGlobe Land&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#31354;&#27169;&#22411;&#30340;&#22810;&#30456;&#26426;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#8220;&#36807;&#21435;&#21644;&#26410;&#26469;&#20043;&#38388;&#30340;&#36319;&#36394;&#8221;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#27880;&#24847;&#21147;&#36319;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35937;&#26597;&#35810;&#36830;&#32493;&#22320;&#34920;&#31034;&#36319;&#36394;&#23454;&#20363;&#65292;&#24182;&#25972;&#21512;&#20102;&#36319;&#36394;&#23545;&#35937;&#30340;&#21069;&#21518;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36319;&#36394;&#20934;&#30830;&#24615;&#21644;ID-Switches&#30340;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2302.03802</link><description>&lt;p&gt;
&#36807;&#21435;&#21644;&#26410;&#26469;&#20043;&#38388;&#65306;&#22522;&#20110;&#26102;&#31354;&#27169;&#22411;&#30340;&#22810;&#30456;&#26426;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking. (arXiv:2302.03802v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#31354;&#27169;&#22411;&#30340;&#22810;&#30456;&#26426;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#8220;&#36807;&#21435;&#21644;&#26410;&#26469;&#20043;&#38388;&#30340;&#36319;&#36394;&#8221;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#27880;&#24847;&#21147;&#36319;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35937;&#26597;&#35810;&#36830;&#32493;&#22320;&#34920;&#31034;&#36319;&#36394;&#23454;&#20363;&#65292;&#24182;&#25972;&#21512;&#20102;&#36319;&#36394;&#23545;&#35937;&#30340;&#21069;&#21518;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36319;&#36394;&#20934;&#30830;&#24615;&#21644;ID-Switches&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#30456;&#26426;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#24378;&#35843;&#26102;&#31354;&#36830;&#32493;&#24615;&#65292;&#24182;&#25972;&#21512;&#20102;&#36319;&#36394;&#23545;&#35937;&#30340;&#21069;&#21518;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#8220;&#22522;&#20110;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#36319;&#36394;&#8221;&#65288;PF-Track&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#8220;&#27880;&#24847;&#21147;&#36319;&#36394;&#8221;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#26597;&#35810;&#36830;&#32493;&#22320;&#34920;&#31034;&#36319;&#36394;&#23454;&#20363;&#12290;&#20026;&#20102;&#26126;&#30830;&#20351;&#29992;&#21382;&#21490;&#32447;&#32034;&#65292;&#25105;&#20204;&#30340;&#8220;&#36807;&#21435;&#25512;&#29702;&#8221;&#27169;&#22359;&#23398;&#20064;&#31934;&#32454;&#21270;&#36319;&#36394;&#65292;&#24182;&#36890;&#36807;&#36328;&#21069;&#19968;&#24103;&#21644;&#20854;&#20182;&#23545;&#35937;&#30340;&#26597;&#35810;&#20132;&#21449;&#27880;&#24847;&#26469;&#22686;&#24378;&#23545;&#35937;&#29305;&#24449;&#12290;&#32780;&#8220;&#26410;&#26469;&#25512;&#29702;&#8221;&#27169;&#22359;&#21017;&#28040;&#21270;&#21382;&#21490;&#20449;&#24687;&#24182;&#39044;&#27979;&#24378;&#20581;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#22312;&#38271;&#26102;&#38388;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32500;&#25345;&#23545;&#35937;&#20301;&#32622;&#65292;&#36890;&#36807;&#25972;&#21512;&#36816;&#21160;&#39044;&#27979;&#23454;&#29616;&#37325;&#26032;&#20851;&#32852;&#12290;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;AMOTA&#65292;&#24182;&#23558;ID-Switches&#20943;&#23569;&#20102;90%&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT) framework. It emphasizes spatio-temporal continuity and integrates both past and future reasoning for tracked objects. Thus, we name it "Past-and-Future reasoning for Tracking" (PF-Track). Specifically, our method adapts the "tracking by attention" framework and represents tracked instances coherently over time with object queries. To explicitly use historical cues, our "Past Reasoning" module learns to refine the tracks and enhance the object features by cross-attending to queries from previous frames and other objects. The "Future Reasoning" module digests historical information and predicts robust future trajectories. In the case of long-term occlusions, our method maintains the object positions and enables re-association by integrating motion predictions. On the nuScenes dataset, our method improves AMOTA by a large margin and remarkably reduces ID-Switches by 90% compared to prior approaches, which is an 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#20989;&#25968;&#20316;&#20026;&#20195;&#29702;&#65292;&#21487;&#20197;&#20197;&#19982;&#21407;&#20989;&#25968;&#26799;&#24230;&#19979;&#38477;&#30456;&#21305;&#37197;&#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.03542</link><description>&lt;p&gt;
&#20004;&#31181;&#25439;&#22833;&#27604;&#19968;&#31181;&#26356;&#22909;&#65306;&#20351;&#29992;&#26356;&#20415;&#23452;&#30340;&#20195;&#29702;&#21152;&#24555;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy. (arXiv:2302.03542v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#20989;&#25968;&#20316;&#20026;&#20195;&#29702;&#65292;&#21487;&#20197;&#20197;&#19982;&#21407;&#20989;&#25968;&#26799;&#24230;&#19979;&#38477;&#30456;&#21305;&#37197;&#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#30340;&#12289;&#26131;&#20110;&#35775;&#38382;&#30340;&#20989;&#25968;&#20316;&#20026;&#20195;&#29702;&#65292;&#26469;&#26368;&#23567;&#21270;&#19968;&#20010;&#38590;&#20197;&#35745;&#31639;&#26799;&#24230;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#20195;&#29702;&#30340;&#36817;&#20284;&#36817;&#31471;&#28857;&#36845;&#20195;&#65292;&#32467;&#21512;&#26469;&#33258;&#30446;&#26631;&#20989;&#25968;&#30340;&#30456;&#23545;&#36739;&#23569;&#30340;&#38543;&#26426;&#26799;&#24230;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;$\delta$-&#24179;&#28369;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#20197;&#19982;$\delta$-&#24179;&#28369;&#30446;&#26631;&#20989;&#25968;&#19978;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#21305;&#37197;&#30340;&#36895;&#29575;&#25910;&#25947;&#65292;&#36825;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#35768;&#22810;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#12289;&#29289;&#29702;&#27169;&#25311;&#22120;&#12289;&#28151;&#21512;&#20844;&#20849;&#21644;&#31169;&#20154;&#25968;&#25454;&#31561;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an algorithm for minimizing an objective with hard-to-compute gradients by using a related, easier-to-access function as a proxy. Our algorithm is based on approximate proximal point iterations on the proxy combined with relatively few stochastic gradients from the objective. When the difference between the objective and the proxy is $\delta$-smooth, our algorithm guarantees convergence at a rate matching stochastic gradient descent on a $\delta$-smooth objective, which can lead to substantially better sample efficiency. Our algorithm has many potential applications in machine learning, and provides a principled means of leveraging synthetic data, physics simulators, mixed public and private data, and more.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25209;&#22788;&#29702;&#31574;&#30053;&#65292;&#37319;&#29992;&#22522;&#20110;GPU&#30340;&#25209;&#22788;&#29702;&#26381;&#21153;&#38431;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#24179;&#22343;&#21709;&#24212;&#26102;&#38388;&#21644;&#21151;&#32791;&#12290;</title><link>http://arxiv.org/abs/2301.12865</link><description>&lt;p&gt;
&#22522;&#20110;SMDP&#30340;GPU&#21160;&#24577;&#25209;&#22788;&#29702;&#20248;&#21270;&#25512;&#26029;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SMDP-Based Dynamic Batching for Efficient Inference on GPU-Based Platforms. (arXiv:2301.12865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25209;&#22788;&#29702;&#31574;&#30053;&#65292;&#37319;&#29992;&#22522;&#20110;GPU&#30340;&#25209;&#22788;&#29702;&#26381;&#21153;&#38431;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#24179;&#22343;&#21709;&#24212;&#26102;&#38388;&#21644;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#35745;&#31639;&#25110;&#36793;&#32536;&#35745;&#31639;&#24179;&#21488;&#19978;&#65292;&#25209;&#22788;&#29702;&#26159;&#25552;&#20379;&#39640;&#25928;&#21644;&#32463;&#27982;&#26381;&#21153;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25209;&#22788;&#29702;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#25928;&#29575;&#21644;&#24310;&#36831;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#23558;&#22522;&#20110;GPU&#30340;&#25512;&#26029;&#26381;&#21153;&#24314;&#27169;&#20026;&#25209;&#22788;&#29702;&#26381;&#21153;&#38431;&#21015;&#65292;&#24182;&#23558;&#20854;&#35774;&#35745;&#20026;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#24179;&#22343;&#25104;&#26412;&#38382;&#39064;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#65292;&#24182;&#20197;&#26368;&#23567;&#21270;&#24179;&#22343;&#21709;&#24212;&#26102;&#38388;&#21644;&#24179;&#22343;&#21151;&#32791;&#20043;&#21644;&#20026;&#30446;&#26631;&#12290;&#26368;&#20248;&#31574;&#30053;&#36890;&#36807;&#35299;&#20915;&#30456;&#20851;&#30340;&#31163;&#25955;&#26102;&#38388;&#36125;&#23572;&#26364;&#26041;&#31243;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In up-to-date machine learning (ML) applications on cloud or edge computing platforms, batching is an important technique for providing efficient and economical services at scale. In particular, parallel computing resources on the platforms, such as graphics processing units (GPUs), have higher computational and energy efficiency with larger batch sizes. However, larger batch sizes may also result in longer response time, and thus it requires a judicious design. This paper aims to provide a dynamic batching policy that strikes a balance between efficiency and latency. The GPU-based inference service is modeled as a batch service queue with batch-size dependent processing time. Then, the design of dynamic batching is a continuous-time average-cost problem, and is formulated as a semi-Markov decision process (SMDP) with the objective of minimizing the weighted sum of average response time and average power consumption. The optimal policy is acquired by solving an associated discrete-time
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#30340;&#27425;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#37096;&#20998;&#19981;&#21464;&#24615;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#20174;&#35757;&#32451;&#22495;&#30340;&#19968;&#20010;&#20998;&#21306;&#20013;&#23398;&#20064;&#20197;&#25552;&#39640;&#19981;&#21464;&#24615;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12067</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#19981;&#21464;&#24615;&#23398;&#20064;&#26368;&#20248;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Features via Partial Invariance. (arXiv:2301.12067v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#30340;&#27425;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#37096;&#20998;&#19981;&#21464;&#24615;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#20174;&#35757;&#32451;&#22495;&#30340;&#19968;&#20010;&#20998;&#21306;&#20013;&#23398;&#20064;&#20197;&#25552;&#39640;&#19981;&#21464;&#24615;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23545;&#20110;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#26159;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#37325;&#28857;&#20851;&#27880;&#28857;&#12290;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#22810;&#20010;&#29615;&#22659;&#20013;&#23398;&#20064;&#40065;&#26834;&#27169;&#22411;&#12290;IRM&#30340;&#25104;&#21151;&#38656;&#35201;&#19968;&#20010;&#37325;&#35201;&#30340;&#20551;&#35774;&#65306;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;/&#29305;&#24449;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#24403;&#35813;&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;IRM&#21487;&#33021;&#20250;&#23548;&#33268;&#39044;&#27979;&#22120;&#36807;&#24230;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807; $\textit{&#37096;&#20998;&#19981;&#21464;&#24615;}$ &#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#31361;&#20986;&#20102;IRM&#30340;&#27425;&#20248;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#35757;&#32451;&#22495;&#30340;&#19968;&#20010;&#20998;&#21306;&#20013;&#23398;&#20064;&#20197;&#25552;&#39640;&#19981;&#21464;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#21253;&#25324;&#22312;&#32447;&#24615;&#35774;&#32622;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#28041;&#21450;&#35821;&#35328;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning models that are robust to distribution shifts is a key concern in the context of their real-life applicability. Invariant Risk Minimization (IRM) is a popular framework that aims to learn robust models from multiple environments. The success of IRM requires an important assumption: the underlying causal mechanisms/features remain invariant across environments. When not satisfied, we show that IRM can over-constrain the predictor and to remedy this, we propose a relaxation via $\textit{partial invariance}$. In this work, we theoretically highlight the sub-optimality of IRM and then demonstrate how learning from a partition of training domains can help improve invariant models. Several experiments, conducted both in linear settings as well as with deep neural networks on tasks over both language and image data, allow us to verify our conclusions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.09767</link><description>&lt;p&gt;
Truveta Mapper&#65306;&#19968;&#20010;&#38646;&#26679;&#26412;&#26412;&#20307;&#26144;&#23556;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;(Ontology Matching, OM)&#25110;&#26412;&#20307;&#23545;&#40784;(Ontology Alignment, OA)&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#12290;&#23558;&#26412;&#20307;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#22312;&#28304;&#26412;&#20307;&#22270;&#20013;&#30340;&#33410;&#28857;&#21040;&#30446;&#26631;&#26412;&#20307;&#22270;&#20013;&#30340;&#36335;&#24452;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25152;&#25552;&#20986;&#30340;Truveta Mapper (TM)&#26694;&#26550;&#21033;&#29992;&#22810;&#20219;&#21153;&#24207;&#21015;&#21040;&#24207;&#21015;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#22810;&#20219;&#21153;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#38544;&#21547;&#22320;&#23398;&#20064;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;&#36825;&#20063;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#27169;&#22411;&#20165;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#20869;&#37096;&#26412;&#20307;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#35813;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#26631;&#20934;&#22522;&#20934;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Edit-Similarity&#21644;MINTE+&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;&#8212;&#8212;&#22240;&#26524;&#19977;&#20803;&#32452;&#65292;&#35813;&#22522;&#20934;&#20855;&#26377;&#21487;&#25805;&#20316;&#30340;&#21453;&#20107;&#23454;&#35774;&#32622;&#21644;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#65292;&#23545;&#20998;&#31163;&#21644;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21270;&#65292;&#28982;&#32780;&#22312;&#22240;&#26524;&#20851;&#31995;&#30340;&#35782;&#21035;&#21644;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#27424;&#20339;&#12290;</title><link>http://arxiv.org/abs/2301.05169</link><description>&lt;p&gt;
&#22240;&#26524;&#19977;&#20803;&#32452;&#65306;&#38754;&#21521;&#24178;&#39044;&#20013;&#24515;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#24320;&#25918;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning. (arXiv:2301.05169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;&#8212;&#8212;&#22240;&#26524;&#19977;&#20803;&#32452;&#65292;&#35813;&#22522;&#20934;&#20855;&#26377;&#21487;&#25805;&#20316;&#30340;&#21453;&#20107;&#23454;&#35774;&#32622;&#21644;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#65292;&#23545;&#20998;&#31163;&#21644;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21270;&#65292;&#28982;&#32780;&#22312;&#22240;&#26524;&#20851;&#31995;&#30340;&#35782;&#21035;&#21644;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#27424;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23398;&#32773;&#20204;&#23545;&#20174;&#24178;&#39044;&#19979;&#30340;&#20302;&#32423;&#22270;&#20687;&#23545;&#20013;&#23398;&#20064;&#39640;&#32423;&#22240;&#26524;&#34920;&#31034;&#20135;&#29983;&#20102;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24448;&#24448;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#36825;&#36828;&#31163;&#20102;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#19977;&#20803;&#32452;&#65292;&#36825;&#26159;&#19968;&#20010;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;&#65292;&#19981;&#20165;&#20855;&#26377;&#26356;&#20026;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#20004;&#20010;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#20851;&#38190;&#24895;&#26395;&#65306;(i) &#19968;&#20010;&#21487;&#25805;&#20316;&#30340;&#21453;&#20107;&#23454;&#35774;&#32622;&#65292;&#20854;&#20013;&#21482;&#26377;&#26576;&#20123;&#29289;&#20307;&#32423;&#21464;&#37327;&#20801;&#35768;&#21453;&#20107;&#23454;&#35266;&#23519;&#65292;&#32780;&#20854;&#20182;&#21464;&#37327;&#21017;&#19981;&#20801;&#35768;&#65307;(ii) &#19968;&#20010;&#24178;&#39044;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24378;&#35843;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#21407;&#21017;&#19979;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#20998;&#31163;&#30340;&#25110;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#30340;&#30693;&#35782;&#30340;&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#20998;&#24067;&#24335;&#23545;&#24212;&#29289;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#35782;&#21035;&#21487;&#25805;&#20316;&#30340;&#21453;&#20107;&#23454;&#35774;&#32622;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19988;&#22312;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#27424;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a surge of interest in learning high-level causal representations from low-level image pairs under interventions. Yet, existing efforts are largely limited to simple synthetic settings that are far away from real-world problems. In this paper, we present Causal Triplet, a causal representation learning benchmark featuring not only visually more complex scenes, but also two crucial desiderata commonly overlooked in previous works: (i) an actionable counterfactual setting, where only certain object-level variables allow for counterfactual observations whereas others do not; (ii) an interventional downstream task with an emphasis on out-of-distribution robustness from the independent causal mechanisms principle. Through extensive experiments, we find that models built with the knowledge of disentangled or object-centric representations significantly outperform their distributed counterparts. However, recent causal representation learning methods still struggle to id
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#25968;&#25454;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#24635;&#32467;&#20102;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#26029;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#24635;&#20307;&#20351;&#21629;&#65292;&#25552;&#20379;&#20102;&#23545;DCAI&#20219;&#21153;&#30340;&#35752;&#35770;&#21644;&#35266;&#28857;&#65292;&#24182;&#21015;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.04819</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65306;&#35270;&#35282;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data-centric AI: Perspectives and Challenges. (arXiv:2301.04819v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04819
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#25968;&#25454;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#24635;&#32467;&#20102;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#26029;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#24635;&#20307;&#20351;&#21629;&#65292;&#25552;&#20379;&#20102;&#23545;DCAI&#20219;&#21153;&#30340;&#35752;&#35770;&#21644;&#35266;&#28857;&#65292;&#24182;&#21015;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26041;&#38754;&#30340;&#20316;&#29992;&#36890;&#36807;&#26032;&#20852;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#27010;&#24565;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#24378;&#65292;&#35813;&#27010;&#24565;&#20027;&#24352;&#23558;&#37325;&#28857;&#20174;&#27169;&#22411;&#25913;&#36827;&#36716;&#21521;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#31038;&#21306;&#19968;&#30452;&#22312;&#19981;&#21516;&#26041;&#38754;&#21162;&#21147;&#22686;&#24378;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#23396;&#31435;&#20030;&#25514;&#12290;&#20026;&#20102;&#25512;&#21160;&#31038;&#21306;&#30340;&#38598;&#20307;&#20513;&#35758;&#24182;&#25512;&#21160;DCAI&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24635;&#20307;&#26694;&#26550;&#65292;&#24182;&#38598;&#21512;&#20102;&#19977;&#20010;&#24635;&#20307;&#20351;&#21629;&#65306;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#26029;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#12290;&#25105;&#20204;&#23545;&#20195;&#34920;DCAI&#20219;&#21153;&#36827;&#34892;&#20102;&#39640;&#23618;&#27425;&#35752;&#35770;&#24182;&#20998;&#20139;&#20102;&#35266;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21015;&#20986;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#26356;&#22810;&#36164;&#28304;&#24635;&#32467;&#35814;&#35265;https://github.com/daochenzha/data-centric-AI&#12290;
&lt;/p&gt;
&lt;p&gt;
The role of data in building AI systems has recently been significantly magnified by the emerging concept of data-centric AI (DCAI), which advocates a fundamental shift from model advancements to ensuring data quality and reliability. Although our community has continuously invested efforts into enhancing data in different aspects, they are often isolated initiatives on specific tasks. To facilitate the collective initiative in our community and push forward DCAI, we draw a big picture and bring together three general missions: training data development, inference data development, and data maintenance. We provide a top-level discussion on representative DCAI tasks and share perspectives. Finally, we list open challenges. More resources are summarized at https://github.com/daochenzha/data-centric-AI
&lt;/p&gt;</description></item><item><title>PFF&#31639;&#27861;&#29992;&#20110;&#31070;&#32463;&#31995;&#32479;&#20869;&#30340;&#20449;&#36151;&#20998;&#37197;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#21160;&#24577;&#36882;&#24402;&#31070;&#32463;&#31995;&#32479;&#21644;&#23450;&#21521;&#29983;&#25104;&#30005;&#36335;&#65292;&#26377;&#25928;&#22320;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#23398;&#20064;&#20449;&#21495;&#21644;&#26356;&#26032;&#31361;&#35302;&#65292;&#28040;&#38500;&#20102;&#35745;&#31639;&#21644;&#32467;&#26500;&#19978;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2301.01452</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#21069;&#21521;-&#21069;&#21521;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Predictive Forward-Forward Algorithm. (arXiv:2301.01452v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01452
&lt;/p&gt;
&lt;p&gt;
PFF&#31639;&#27861;&#29992;&#20110;&#31070;&#32463;&#31995;&#32479;&#20869;&#30340;&#20449;&#36151;&#20998;&#37197;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#21160;&#24577;&#36882;&#24402;&#31070;&#32463;&#31995;&#32479;&#21644;&#23450;&#21521;&#29983;&#25104;&#30005;&#36335;&#65292;&#26377;&#25928;&#22320;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#23398;&#20064;&#20449;&#21495;&#21644;&#26356;&#26032;&#31361;&#35302;&#65292;&#28040;&#38500;&#20102;&#35745;&#31639;&#21644;&#32467;&#26500;&#19978;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#27979;&#24615;&#21069;&#21521;-&#21069;&#21521;&#65288;PFF&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#31995;&#32479;&#20013;&#36827;&#34892;&#20449;&#36151;&#20998;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#36882;&#24402;&#31070;&#32463;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#19982;&#34920;&#31034;&#30005;&#36335;&#21516;&#26102;&#23398;&#20064;&#19968;&#20010;&#23450;&#21521;&#29983;&#25104;&#30005;&#36335;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#21487;&#23398;&#20064;&#30340;&#27178;&#21521;&#31454;&#20105;&#65292;&#22122;&#22768;&#27880;&#20837;&#21644;&#39044;&#27979;&#32534;&#30721;&#30340;&#20803;&#32032;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#20852;&#19988;&#26377;&#25928;&#30340;&#30382;&#23618;&#21151;&#33021;&#30340;&#31070;&#32463;&#29983;&#29289;&#23398;&#36807;&#31243;&#29702;&#35770;&#65292;&#19982;&#21069;&#21521;-&#21069;&#21521;&#65288;FF&#65289;&#36866;&#24212;&#26041;&#26696;&#19968;&#36215;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;PFF&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20165;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#20256;&#25773;&#23398;&#20064;&#20449;&#21495;&#24182;&#26356;&#26032;&#31361;&#35302;&#65292;&#28040;&#38500;&#20102;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#26041;&#26696;&#25152;&#26045;&#21152;&#30340;&#20851;&#38190;&#32467;&#26500;&#21644;&#35745;&#31639;&#32422;&#26463;&#12290;&#38500;&#20102;&#35745;&#31639;&#26041;&#38754;&#30340;&#20248;&#21183;&#22806;&#65292;PFF&#36807;&#31243;&#36824;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#29983;&#29289;&#31070;&#32463;&#20803;&#32972;&#26223;&#19979;&#20351;&#29992;&#26412;&#22320;&#20449;&#21495;&#30340;&#23398;&#20064;&#26426;&#21046;&#65292;&#23613;&#31649;&#32570;&#23569;&#21453;&#39304;&#36830;&#25509;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#25968;&#25454;&#19978;&#36816;&#34892;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;PFF&#31243;&#24207;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the predictive forward-forward (PFF) algorithm for conducting credit assignment in neural systems. Specifically, we design a novel, dynamic recurrent neural system that learns a directed generative circuit jointly and simultaneously with a representation circuit. Notably, the system integrates learnable lateral competition, noise injection, and elements of predictive coding, an emerging and viable neurobiological process theory of cortical function, with the forward-forward (FF) adaptation scheme. Furthermore, PFF efficiently learns to propagate learning signals and updates synapses with forward passes only, eliminating key structural and computational constraints imposed by backpropagation-based schemes. Besides computational advantages, the PFF process could prove useful for understanding the learning mechanisms behind biological neurons that use local signals despite missing feedback connections. We run experiments on image data and demonstrate that the PFF procedure work
&lt;/p&gt;</description></item><item><title>FunkNN&#26159;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#37325;&#24314;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;MLP&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.14042</link><description>&lt;p&gt;
FunkNN&#65306;&#21151;&#33021;&#29983;&#25104;&#30340;&#31070;&#32463;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
FunkNN: Neural Interpolation for Functional Generation. (arXiv:2212.14042v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14042
&lt;/p&gt;
&lt;p&gt;
FunkNN&#26159;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#37325;&#24314;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;MLP&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#26500;&#24314;&#36830;&#32493;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36328;&#23610;&#24230;&#27867;&#21270;&#65292;&#24182;&#22312;&#20219;&#20309;&#22352;&#26631;&#22788;&#36827;&#34892;&#35780;&#20272;&#65292;&#21487;&#20197;&#35745;&#31639;&#31934;&#30830;&#30340;&#23548;&#25968;&#65292;&#24182;&#19988;&#22312;&#27010;&#24565;&#19978;&#24456;&#31616;&#21333;&#65311;&#29616;&#26377;&#30340;&#22522;&#20110;MLP&#30340;&#26550;&#26500;&#29983;&#25104;&#30340;&#26679;&#26412;&#27604;&#20855;&#26377;&#26377;&#21033;&#21367;&#31215;&#24402;&#32435;&#20559;&#24046;&#30340;&#32593;&#26684;&#29983;&#25104;&#22120;&#24046;&#12290;&#19987;&#27880;&#20110;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#29983;&#25104;&#22270;&#20687;&#30340;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#37319;&#29992;&#30340;&#26550;&#26500;&#22797;&#26434;&#65292;&#19981;&#36866;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#23548;&#25968;&#36827;&#34892;&#36830;&#32493;&#35780;&#20272;&#12290;&#25105;&#20204;&#37319;&#29992;&#20449;&#21495;&#22788;&#29702;&#30340;&#35270;&#35282;&#65292;&#24182;&#23558;&#36830;&#32493;&#22270;&#20687;&#29983;&#25104;&#35270;&#20026;&#20174;&#26679;&#26412;&#36827;&#34892;&#25554;&#20540;&#12290;&#30830;&#23454;&#65292;&#27491;&#30830;&#37319;&#26679;&#30340;&#31163;&#25955;&#22270;&#20687;&#21253;&#21547;&#26377;&#20851;&#20302;&#31354;&#38388;&#39057;&#29575;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;&#38382;&#39064;&#26159;&#22914;&#20309;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#22806;&#25512;&#39057;&#35889;&#65292;&#21516;&#26102;&#28385;&#36275;&#19978;&#36848;&#35774;&#35745;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;FunkNN--&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#32593;&#32476;&#65292;&#23427;&#23398;&#20064;&#22914;&#20309;&#22312;&#20219;&#24847;&#22352;&#26631;&#37325;&#26500;&#36830;&#32493;&#22270;&#20687;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#32467;&#21512;&#31163;&#25955;&#37319;&#26679;&#27493;&#39588;&#65292;FunkNN&#22312;&#36830;&#32493;&#22495;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;MLP&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we build continuous generative models which generalize across scales, can be evaluated at any coordinate, admit calculation of exact derivatives, and are conceptually simple? Existing MLP-based architectures generate worse samples than the grid-based generators with favorable convolutional inductive biases. Models that focus on generating images at different scales do better, but employ complex architectures not designed for continuous evaluation of images and derivatives. We take a signal-processing perspective and treat continuous image generation as interpolation from samples. Indeed, correctly sampled discrete images contain all information about the low spatial frequencies. The question is then how to extrapolate the spectrum in a data-driven way while meeting the above design criteria. Our answer is FunkNN -- a new convolutional network which learns how to reconstruct continuous images at arbitrary coordinates and can be applied to any image dataset. Combined with a discrete 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#35299;&#20915;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#26080;&#27861;&#37327;&#21270;&#36817;&#20284;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.12474</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#24212;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers. (arXiv:2212.12474v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#35299;&#20915;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#26080;&#27861;&#37327;&#21270;&#36817;&#20284;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#19968;&#31867;&#37325;&#35201;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26426;&#26800;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#29289;&#29702;&#36807;&#31243;&#65292;&#20363;&#22914;&#28909;&#20256;&#23548;&#12289;&#30005;&#30913;&#23398;&#21644;&#27874;&#20256;&#25773;&#31561;&#12290;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#31163;&#25955;&#21270;&#30340;&#19987;&#38376;&#25968;&#20540;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#20123;&#27714;&#35299;&#22120;&#36890;&#24120;&#20351;&#29992;&#26410;&#30693;&#27169;&#22411;&#21442;&#25968;&#30340;&#20272;&#35745;&#20540;&#20197;&#21450;&#22914;&#26524;&#21487;&#29992;&#30340;&#35805;&#65292;&#29289;&#29702;&#27979;&#37327;&#20540;&#29992;&#20110;&#21021;&#22987;&#21270;&#12290;&#36825;&#20123;&#27714;&#35299;&#22120;&#32463;&#24120;&#23884;&#20837;&#21040;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#26356;&#22823;&#30340;&#31185;&#23398;&#27169;&#22411;&#20013;&#65292;&#22240;&#27492;&#35823;&#24046;&#37327;&#21270;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#24573;&#30053;&#21442;&#25968;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;&#19968;&#33268;&#24615;&#30340;&#20272;&#35745;&#20540;&#65292;&#20197;&#29992;&#20110;&#35745;&#31639;&#20854;&#22266;&#26377;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#27714;&#35299;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#37322;&#20026;&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#23450;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25512;&#24191;&#65292;&#35813;&#23450;&#29702;&#36866;&#29992;&#20110;&#36890;&#36807;&#20219;&#24847;&#30028;&#38754;&#36827;&#34892;&#35266;&#23519;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models with a downstream application and thus error quantification plays a key role. However, by ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of the Gaussian process inference theorem to observations made via an arbitrary bou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#28608;&#27963;&#22320;&#22270;&#65288;BAM&#65289;&#30340;&#35786;&#26029;&#26694;&#26550;&#65292;&#20351;&#29992;&#20004;&#20010;U&#24418;&#29983;&#25104;&#22120;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#24847;&#20041;&#26126;&#30830;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#39564;&#35777;&#21644;&#29702;&#35299;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#65292;&#21487;&#24212;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#12290;</title><link>http://arxiv.org/abs/2212.06299</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#28608;&#27963;&#22320;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Interpretable Diabetic Retinopathy Diagnosis based on Biomarker Activation Map. (arXiv:2212.06299v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#28608;&#27963;&#22320;&#22270;&#65288;BAM&#65289;&#30340;&#35786;&#26029;&#26694;&#26550;&#65292;&#20351;&#29992;&#20004;&#20010;U&#24418;&#29983;&#25104;&#22120;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#24847;&#20041;&#26126;&#30830;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#39564;&#35777;&#21644;&#29702;&#35299;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#65292;&#21487;&#24212;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#22522;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#21450;&#20854;&#34880;&#31649;&#24433;&#20687;&#23398;&#65288;OCTA&#65289;&#33258;&#21160;&#35786;&#26029;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#30340;&#26368;&#20934;&#30830;&#30340;&#25163;&#27573;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21183;&#37096;&#20998;&#24402;&#21151;&#20110;&#21253;&#21547;&#30340;&#38544;&#34255;&#23618;&#25152;&#25552;&#20379;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25152;&#38656;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38544;&#34255;&#23618;&#20063;&#20351;&#31639;&#27861;&#36755;&#20986;&#38590;&#20197;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#30340;&#26032;&#22411;&#29983;&#29289;&#26631;&#24535;&#29289;&#28608;&#27963;&#22320;&#22270;&#65288;BAM&#65289;&#26694;&#26550;&#65292;&#20351;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#39564;&#35777;&#21644;&#29702;&#35299;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#19968;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;456&#20010;&#40644;&#26001;&#25195;&#25551;&#26681;&#25454;&#24403;&#21069;&#20020;&#24202;&#26631;&#20934;&#34987;&#20998;&#32423;&#20026;&#38750;&#21487;&#36716;&#35786;&#25110;&#21487;&#36716;&#35786;DR&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#27492;&#25968;&#25454;&#38598;&#20351;&#29992;DR&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;BAM&#12290;BAM&#29983;&#25104;&#26694;&#26550;&#26159;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;U&#24418;&#29983;&#25104;&#22120;&#26469;&#35774;&#35745;&#30340;&#65292;&#20174;&#32780;&#20026;&#27492;&#20998;&#31867;&#22120;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20027;&#35201;&#29983;&#25104;&#22120;&#26159;&#26681;&#25454;&#21487;&#36716;&#35786;&#25195;&#25551;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning classifiers provide the most accurate means of automatically diagnosing diabetic retinopathy (DR) based on optical coherence tomography (OCT) and its angiography (OCTA). The power of these models is attributable in part to the inclusion of hidden layers that provide the complexity required to achieve a desired task. However, hidden layers also render algorithm outputs difficult to interpret. Here we introduce a novel biomarker activation map (BAM) framework based on generative adversarial learning that allows clinicians to verify and understand classifiers decision-making. A data set including 456 macular scans were graded as non-referable or referable DR based on current clinical standards. A DR classifier that was used to evaluate our BAM was first trained based on this data set. The BAM generation framework was designed by combing two U-shaped generators to provide meaningful interpretability to this classifier. The main generator was trained to take referable scans as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#33258;&#25105;&#30417;&#30563;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#24212;&#29992;&#21040;&#30446;&#26631;&#23548;&#33322;&#20013;&#65292;&#22522;&#20110;&#20301;&#32622;&#19968;&#33268;&#24615;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#26631;&#35760;&#26114;&#36149;&#30340;3D&#32593;&#26684;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#21644;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.05923</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#30446;&#26631;&#23548;&#33322;&#20013;&#30340;&#29616;&#22330;&#24494;&#35843;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Object Goal Navigation with In-Situ Finetuning. (arXiv:2212.05923v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#33258;&#25105;&#30417;&#30563;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#24212;&#29992;&#21040;&#30446;&#26631;&#23548;&#33322;&#20013;&#65292;&#22522;&#20110;&#20301;&#32622;&#19968;&#33268;&#24615;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#26631;&#35760;&#26114;&#36149;&#30340;3D&#32593;&#26684;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#21644;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#23478;&#24237;&#26426;&#22120;&#20154;&#24212;&#35813;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29992;&#25143;&#20808;&#27880;&#37322;&#23478;&#20013;&#25152;&#26377;&#29289;&#21697;&#30340;&#24773;&#20917;&#19979;&#23548;&#33322;&#21040;&#30446;&#26631;&#29289;&#21697;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#29289;&#21697;&#23548;&#33322;&#30340;&#26041;&#27861;&#24182;&#26410;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#37325;&#26500;&#30340;&#25151;&#23627;&#25195;&#25551;&#21450;&#26114;&#36149;&#26631;&#35760;&#21322;&#30417;&#30563; 3D &#32593;&#26684;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25506;&#32034;&#26500;&#24314;&#26426;&#22120;&#20154;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;&#65292;&#27491;&#22914;&#20799;&#31461;&#20250;&#20570;&#30340;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25918;&#24323;&#26631;&#35760; 3D &#32593;&#26684;&#30340;&#25104;&#26412;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#21551;&#29992;&#33258;&#25105;&#30417;&#30563;&#29616;&#22330;&#24494;&#35843;&#26426;&#21046;&#12290;&#25105;&#20204;&#30830;&#23450;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#25105;&#30417;&#30563;&#28304;&#65288;&#20301;&#32622;&#19968;&#33268;&#24615; - LocCon&#65289;&#21487;&#20197;&#35757;&#32451; ObjectNav &#20195;&#29702;&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#65292;&#20351;&#29992;&#26410;&#27880;&#37322;&#30340;&#27169;&#25311;&#25151;&#23627;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#26426;&#36523;&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#20301;&#32622;&#19968;&#33268;&#24615;&#20316;&#20026;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#65292;&#25910;&#38598;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282; /&#35282;&#24230;&#30340;&#22270;&#20687;&#65292;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#31243;&#24207;&#21487;&#20197;&#22312;&#30495;&#23454;&#19990;&#30028;&#21644;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A household robot should be able to navigate to target objects without requiring users to first annotate everything in their home. Most current approaches to object navigation do not test on real robots and rely solely on reconstructed scans of houses and their expensively labeled semantic 3D meshes. In this work, our goal is to build an agent that builds self-supervised models of the world via exploration, the same as a child might - thus we (1) eschew the expense of labeled 3D mesh and (2) enable self-supervised in-situ finetuning in the real world. We identify a strong source of self-supervision (Location Consistency - LocCon) that can train all components of an ObjectNav agent, using unannotated simulated houses. Our key insight is that embodied agents can leverage location consistency as a self-supervision signal collecting images from different views/angles and applying contrastive learning. We show that our agent can perform competitively in the real world and simulation. Our 
&lt;/p&gt;</description></item><item><title>Genie&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#24320;&#21457;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#36866;&#21512;&#38646;&#26679;&#26412;&#37327;&#21270;&#30340;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2212.04780</link><description>&lt;p&gt;
Genie: &#23637;&#31034;&#25105;&#37327;&#21270;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Genie: Show Me the Data for Quantization. (arXiv:2212.04780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04780
&lt;/p&gt;
&lt;p&gt;
Genie&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#24320;&#21457;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#36866;&#21512;&#38646;&#26679;&#26412;&#37327;&#21270;&#30340;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25968;&#25454;&#22240;&#20026;&#21508;&#31181;&#21407;&#22240;&#65288;&#21253;&#25324;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#65289;&#26080;&#27861;&#35775;&#38382;&#26102;&#65292;&#38646;&#26679;&#26412;&#37327;&#21270;&#26159;&#24320;&#21457;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;FP32&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#30340;&#23398;&#20064;&#21442;&#25968;&#65288;$\mu$&#21644;$\sigma$&#65289;&#65292;&#38646;&#26679;&#26412;&#37327;&#21270;&#26041;&#26696;&#19987;&#27880;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;&#38543;&#21518;&#65292;&#23427;&#20204;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#20256;&#36882;&#32473;&#37327;&#21270;&#27169;&#22411;&#65288;&#23398;&#29983;&#65289;&#65292;&#20351;&#24471;&#37327;&#21270;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#38646;&#26679;&#26412;&#37327;&#21270;&#20027;&#35201;&#22312;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#30340;&#19978;&#19979;&#25991;&#20013;&#35752;&#35770;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#21644;&#38271;&#26399;&#30340;&#20248;&#21270;&#65292;&#23601;&#20687;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#37327;&#21270;&#65292;&#21487;&#20197;&#22312;&#20960;&#23567;&#26102;&#20869;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37327;&#21270;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Genie&#8221;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36866;&#21512;&#38646;&#26679;&#26412;&#37327;&#21270;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot quantization is a promising approach for developing lightweight deep neural networks when data is inaccessible owing to various reasons, including cost and issues related to privacy. By exploiting the learned parameters ($\mu$ and $\sigma$) of batch normalization layers in an FP32-pre-trained model, zero-shot quantization schemes focus on generating synthetic data. Subsequently, they distill knowledge from the pre-trained model (teacher) to the quantized model (student) such that the quantized model can be optimized with the synthetic dataset. However, thus far, zero-shot quantization has primarily been discussed in the context of quantization-aware training methods, which require task-specific losses and long-term optimization as much as retraining. We thus introduce a post-training quantization scheme for zero-shot quantization that produces high-quality quantized networks within a few hours. Furthermore, we propose a framework called \genie~that generates data suited for q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#38519;&#21644;&#26426;&#36935;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;ReLU&#30340;MLP&#19981;&#33021;&#24418;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#27861;&#20989;&#25968;&#31354;&#38388;&#65292;&#32780;&#20351;&#29992;&#24102;&#26377;&#36755;&#20986;&#23618;&#36229;&#24179;&#38754;&#30340;&#20855;&#26377;$C^n$&#28608;&#27963;&#20989;&#25968;&#30340;MLP&#21487;&#20197;&#20005;&#26684;&#28385;&#36275;&#19968;&#20010;&#32447;&#24615;PDE&#30452;&#21040; $n$ &#38454;&#12290;</title><link>http://arxiv.org/abs/2212.00270</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#29289;&#29702;&#23398;&#20064;&#20013;&#30340;&#20860;&#23481;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Compatibility between Neural Networks and Partial Differential Equations for Physics-informed Learning. (arXiv:2212.00270v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#38519;&#21644;&#26426;&#36935;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;ReLU&#30340;MLP&#19981;&#33021;&#24418;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#27861;&#20989;&#25968;&#31354;&#38388;&#65292;&#32780;&#20351;&#29992;&#24102;&#26377;&#36755;&#20986;&#23618;&#36229;&#24179;&#38754;&#30340;&#20855;&#26377;$C^n$&#28608;&#27963;&#20989;&#25968;&#30340;MLP&#21487;&#20197;&#20005;&#26684;&#28385;&#36275;&#19968;&#20010;&#32447;&#24615;PDE&#30452;&#21040; $n$ &#38454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#38519;&#21644;&#26426;&#36935;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#21482;&#20351;&#29992;ReLU&#65288;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65289;&#25110;&#31867;ReLU&#30340;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23558;&#22987;&#32456;&#23548;&#33268;&#28023;&#26862;&#30697;&#38453;&#28040;&#22833;&#12290;&#36825;&#31181;&#32593;&#32476;&#25152;&#26045;&#21152;&#30340;&#38480;&#21046;&#19982;&#20219;&#20309;&#20108;&#38454;&#25110;&#26356;&#39640;&#38454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30456;&#30683;&#30462;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;ReLU&#30340;MLP&#19981;&#33021;&#24418;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#27861;&#20989;&#25968;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#32570;&#38519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#20855;&#26377;$C^n$&#28608;&#27963;&#20989;&#25968;&#30340;MLP&#65292;&#24403;&#20854;&#36755;&#20986;&#23618;&#30340;&#26435;&#37325;&#20301;&#20110;&#26576;&#20010;&#36229;&#24179;&#38754;&#65288;&#31216;&#20026;&#36755;&#20986;&#23618;&#36229;&#24179;&#38754;&#65289;&#19978;&#26102;&#65292;&#21487;&#20197;&#20005;&#26684;&#28385;&#36275;&#19968;&#20010;&#32447;&#24615;PDE&#30452;&#21040; $n$ &#38454;&#12290;&#37197;&#22791;&#36755;&#20986;&#23618;&#36229;&#24179;&#38754;&#30340;MLP&#21464;&#24471;&#8220;&#29289;&#29702;&#24378;&#21046;&#25191;&#34892;&#8221;&#65292;&#19981;&#20877;&#38656;&#35201;&#38024;&#23545;PDE&#26412;&#36523;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#21482;&#38656;&#35201;&#21021;&#36793;&#20540;&#26465;&#20214;&#30340;&#25439;&#22833;&#20989;&#25968;&#65289;&#12290;&#36825;&#26679;&#30340;&#36229;&#24179;&#38754;&#19981;&#20165;&#23384;&#22312;&#20110;MLP&#20013;&#65292;&#32780;&#19988;&#23384;&#22312;&#20110;&#20219;&#20309;&#32593;&#32476;&#26550;&#26500;&#30340;&#23614;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
We shed light on a pitfall and an opportunity in physics-informed neural networks (PINNs). We prove that a multilayer perceptron (MLP) only with ReLU (Rectified Linear Unit) or ReLU-like Lipschitz activation functions will always lead to a vanished Hessian. Such a network-imposed constraint contradicts any second- or higher-order partial differential equations (PDEs). Therefore, a ReLU-based MLP cannot form a permissible function space for the approximation of their solutions. Inspired by this pitfall, we prove that a linear PDE up to the $n$-th order can be strictly satisfied by an MLP with $C^n$ activation functions when the weights of its output layer lie on a certain hyperplane, as called the out-layer-hyperplane. An MLP equipped with the out-layer-hyperplane becomes "physics-enforced", no longer requiring a loss function for the PDE itself (but only those for the initial and boundary conditions). Such a hyperplane exists not only for MLPs but for any network architecture tailed by
&lt;/p&gt;</description></item><item><title>SinGRAF&#26159;&#19968;&#20010;3D&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21482;&#38656;&#23569;&#37327;&#36755;&#20837;&#19968;&#21333;&#20010;&#22330;&#26223;&#30340;&#22270;&#20687;&#21363;&#21487;&#35757;&#32451;&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#36755;&#20837;&#22806;&#35266;&#30340;&#21516;&#26102;&#29983;&#25104;&#22810;&#26679;&#30340;3D&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2211.17260</link><description>&lt;p&gt;
SinGRAF&#65306;&#23398;&#20064;&#21333;&#20010;&#22330;&#26223;&#30340;3D&#29983;&#25104;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene. (arXiv:2211.17260v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17260
&lt;/p&gt;
&lt;p&gt;
SinGRAF&#26159;&#19968;&#20010;3D&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21482;&#38656;&#23569;&#37327;&#36755;&#20837;&#19968;&#21333;&#20010;&#22330;&#26223;&#30340;&#22270;&#20687;&#21363;&#21487;&#35757;&#32451;&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#36755;&#20837;&#22806;&#35266;&#30340;&#21516;&#26102;&#29983;&#25104;&#22810;&#26679;&#30340;3D&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#36924;&#30495;&#30340;3D&#29289;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SinGRAF&#65292;&#36825;&#26159;&#19968;&#20010;3D&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21482;&#38656;&#23569;&#37327;&#36755;&#20837;&#19968;&#21333;&#20010;&#22330;&#26223;&#30340;&#22270;&#20687;&#21363;&#21487;&#35757;&#32451;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;SinGRAF&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#36755;&#20837;&#22806;&#35266;&#24182;&#21464;&#21270;&#22330;&#26223;&#24067;&#23616;&#30340;&#19981;&#21516;3D&#22330;&#26223;&#23454;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#36817;&#26399;3D GAN&#26550;&#26500;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28176;&#36827;&#24335;&#35268;&#27169;&#34917;&#19969;&#21028;&#21035;&#26041;&#27861;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;SinGRAF&#20135;&#29983;&#30340;&#32467;&#26524;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#37117;&#36828;&#36828;&#20248;&#20110;&#26368;&#25509;&#36817;&#30340;&#30456;&#20851;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have shown great promise in synthesizing photorealistic 3D objects, but they require large amounts of training data. We introduce SinGRAF, a 3D-aware generative model that is trained with a few input images of a single scene. Once trained, SinGRAF generates different realizations of this 3D scene that preserve the appearance of the input while varying scene layout. For this purpose, we build on recent progress in 3D GAN architectures and introduce a novel progressive-scale patch discrimination approach during training. With several experiments, we demonstrate that the results produced by SinGRAF outperform the closest related works in both quality and diversity by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#26469;&#26356;&#24555;&#22320;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2211.16691</link><description>&lt;p&gt;
&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules. (arXiv:2211.16691v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#26469;&#26356;&#24555;&#22320;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#31351;&#20030;&#25506;&#32034;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20197;&#25214;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#32780;&#23548;&#33268;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#22826;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#31995;&#32479;&#30340;&#19987;&#23478;&#30693;&#35782;&#36890;&#24120;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#31616;&#21333;&#35268;&#21017;&#65292;&#25105;&#20204;&#26399;&#26395;&#33391;&#22909;&#30340;&#31574;&#30053;&#22987;&#32456;&#36981;&#24490;&#36825;&#20123;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36830;&#32493;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#20197;&#32435;&#20837;&#36825;&#20123;&#35268;&#21017;&#24182;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#65292;&#20174;&#32780;&#26174;&#30528;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25913;&#36827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#20195;&#29702;&#31243;&#24207;&#36873;&#25321;&#30340;&#21160;&#20316;&#19981;&#31526;&#21512;&#25105;&#20204;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#20250;&#39281;&#21644;&#36825;&#20123;&#21160;&#20316;&#65292;&#20851;&#38190;&#26159;&#20462;&#25913;&#31574;&#30053;&#30340;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#27969;&#31243;&#19981;&#21463;&#39281;&#21644;&#27493;&#39588;&#30340;&#24433;&#21709;&#12290;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#23427;&#20351;&#20195;&#29702;&#31243;&#24207;&#20197;&#27604;&#20256;&#32479;&#20195;&#29702;&#31243;&#24207;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#28040;&#32791;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) generally suffers from poor sample complexity, mostly due to the need to exhaustively explore the state-action space to find well-performing policies. On the other hand, we postulate that expert knowledge of the system often allows us to design simple rules we expect good policies to follow at all times. In this work, we hence propose a simple yet effective modification of continuous actor-critic frameworks to incorporate such rules and avoid regions of the state-action space that are known to be suboptimal, thereby significantly accelerating the convergence of RL agents. Concretely, we saturate the actions chosen by the agent if they do not comply with our intuition and, critically, modify the gradient update step of the policy to ensure the learning process is not affected by the saturation step. On a room temperature control case study, it allows agents to converge to well-performing policies up to 6-7x faster than classical agents without computational o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#26377;&#20107;&#20214;&#30340;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#22522;&#20110;&#25910;&#38598;&#21040;&#30340;&#26102;&#38388;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#21472;&#21152;&#25968;&#25454;&#38598;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#22312;&#21464;&#37327;&#31532;&#19968;&#27425;&#32463;&#21382;&#20302;&#27010;&#29575;&#23454;&#29616;&#26102;&#25165;&#20250;&#26174;&#29616;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.16596</link><description>&lt;p&gt;
&#38754;&#21521;&#31232;&#26377;&#20107;&#20214;&#30340;&#21160;&#24577;&#22240;&#26524;&#21457;&#29616;&#65306;&#19968;&#31181;&#38750;&#21442;&#25968;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16596
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#26377;&#20107;&#20214;&#30340;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#22522;&#20110;&#25910;&#38598;&#21040;&#30340;&#26102;&#38388;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#21472;&#21152;&#25968;&#25454;&#38598;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#22312;&#21464;&#37327;&#31532;&#19968;&#27425;&#32463;&#21382;&#20302;&#27010;&#29575;&#23454;&#29616;&#26102;&#25165;&#20250;&#26174;&#29616;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#31232;&#26377;&#20107;&#20214;&#30456;&#20851;&#32852;&#30340;&#22240;&#26524;&#29616;&#35937;&#22312;&#35768;&#22810;&#24037;&#31243;&#38382;&#39064;&#20013;&#37117;&#23384;&#22312;&#65292;&#20363;&#22914;&#38024;&#23545;&#39118;&#38505;&#30340;&#23433;&#20840;&#20998;&#26512;&#12289;&#20107;&#25925;&#20998;&#26512;&#21644;&#39044;&#38450;&#20197;&#21450;&#26497;&#20540;&#29702;&#35770;&#31561;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#21457;&#29616;&#22312;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#21407;&#22240;&#32852;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#21464;&#21160;&#29615;&#22659;&#19979;&#65292;&#20165;&#22312;&#21464;&#37327;&#31532;&#19968;&#27425;&#32463;&#21382;&#20302;&#27010;&#29575;&#23454;&#29616;&#26102;&#25165;&#20250;&#26174;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21457;&#29983;&#31232;&#26377;&#20294;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#26102;&#38388;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#24213;&#23618;&#25968;&#25454;&#30340;&#26102;&#38388;&#19981;&#21464;&#24615;&#26469;&#26500;&#24314;&#19968;&#20010;&#21472;&#21152;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#20043;&#21069;&#31232;&#26377;&#20107;&#20214;&#21457;&#29983;&#21069;&#31995;&#32479;&#29366;&#24577;&#30340;&#25968;&#25454;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#37325;&#26032;&#32452;&#32455;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#19968;&#33268;&#24615;&#30340;&#38750;&#28176;&#36817;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal phenomena associated with rare events occur across a wide range of engineering problems, such as risk-sensitive safety analysis, accident analysis and prevention, and extreme value theory. However, current methods for causal discovery are often unable to uncover causal links, between random variables in a dynamic setting, that manifest only when the variables first experience low-probability realizations. To address this issue, we introduce a novel statistical independence test on data collected from time-invariant dynamical systems in which rare but consequential events occur. In particular, we exploit the time-invariance of the underlying data to construct a superimposed dataset of the system state before rare events happen at different timesteps. We then design a conditional independence test on the reorganized data. We provide non-asymptotic sample complexity bounds for the consistency of our method, and validate its performance across various simulated and real-world datase
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25910;&#30410;&#20989;&#25968;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#27604;&#36739;&#26469;&#34913;&#37327;&#22522;&#20110;&#27604;&#36739;&#30340;&#20998;&#23618;&#32858;&#31867;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.16459</link><description>&lt;p&gt;
&#22522;&#20110;&#27604;&#36739;&#30340;&#23618;&#27425;&#32858;&#31867;&#30340;&#25910;&#30410;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Revenue Function for Comparison-Based Hierarchical Clustering. (arXiv:2211.16459v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25910;&#30410;&#20989;&#25968;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#27604;&#36739;&#26469;&#34913;&#37327;&#22522;&#20110;&#27604;&#36739;&#30340;&#20998;&#23618;&#32858;&#31867;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27604;&#36739;&#30340;&#23398;&#20064;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#24403;&#25105;&#20204;&#21482;&#26377;&#24418;&#24335;&#20026;&#8220;&#30446;&#26631;A&#19982;B&#30456;&#27604;&#36739;&#27604;C&#26356;&#30456;&#20284;&#8221;&#36825;&#26679;&#30340;&#27604;&#36739;&#65292;&#32780;&#27809;&#26377;&#26174;&#24335;&#29305;&#24449;&#25110;&#25104;&#23545;&#30456;&#20284;&#24230;&#26102;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#23618;&#27425;&#32858;&#31867;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#36825;&#26679;&#30340;&#27604;&#36739;&#23454;&#29616;&#21333;&#19968;&#21644;&#23436;&#20840;&#38142;&#25509;&#65292;&#21516;&#26102;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#26469;&#27169;&#25311;&#24179;&#22343;&#38142;&#25509;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#20165;&#27604;&#36739;&#25214;&#21040;&#23618;&#27425;&#32467;&#26500;&#65288;&#25110;&#26641;&#29366;&#22270;&#65289;&#26159;&#19968;&#20010;&#34987;&#20805;&#20998;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#27809;&#26377;&#22522;&#20934;&#20107;&#23454;&#25110;&#26174;&#24335;&#30456;&#20284;&#24615;&#26102;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#24847;&#20041;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#25910;&#30410;&#20989;&#25968;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#20989;&#25968;&#20801;&#35768;&#25105;&#20204;&#20165;&#20351;&#29992;&#27604;&#36739;&#26469;&#34913;&#37327;&#26641;&#29366;&#22270;&#30340;&#22909;&#22351;&#12290;&#25991;&#31456;&#36824;&#34920;&#26126;&#65292;&#35813;&#20989;&#25968;&#19982;&#20351;&#29992;&#25104;&#23545;&#30456;&#20284;&#24615;&#30340;&#23618;&#27425;&#32858;&#31867;&#30340;Dasgupta&#25104;&#26412;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#25317;&#26377;&#22909;&#30340;&#24615;&#36136;&#30340;&#25910;&#30410;&#20989;&#25968;&#26469;&#20998;&#26512;&#27604;&#36739;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#20123;&#39640;&#26031;&#36807;&#31243;&#30340;&#30456;&#20851;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparison-based learning addresses the problem of learning when, instead of explicit features or pairwise similarities, one only has access to comparisons of the form: \emph{Object $A$ is more similar to $B$ than to $C$.} Recently, it has been shown that, in Hierarchical Clustering, single and complete linkage can be directly implemented using only such comparisons while several algorithms have been proposed to emulate the behaviour of average linkage. Hence, finding hierarchies (or dendrograms) using only comparisons is a well understood problem. However, evaluating their meaningfulness when no ground-truth nor explicit similarities are available remains an open question.  In this paper, we bridge this gap by proposing a new revenue function that allows one to measure the goodness of dendrograms using only comparisons. We show that this function is closely related to Dasgupta's cost for hierarchical clustering that uses pairwise similarities. On the theoretical side, we use the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#35302;&#24335;&#30340;&#26041;&#27861;&#65292;&#24341;&#23548;&#35270;&#35273;Transformer&#23398;&#20064;&#22810;&#35270;&#35282;&#20960;&#20309;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26497;&#32447;&#26469;&#24341;&#23548;Transformer&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#25552;&#20379;&#20219;&#20309;&#25668;&#20687;&#26426;&#23039;&#24577;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#23039;&#24577;&#19981;&#21464;&#30340;&#29289;&#20307;&#23454;&#20363;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2211.15107</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#25945;&#25480;Transformer&#22810;&#35270;&#35282;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
A Light Touch Approach to Teaching Transformers Multi-view Geometry. (arXiv:2211.15107v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#35302;&#24335;&#30340;&#26041;&#27861;&#65292;&#24341;&#23548;&#35270;&#35273;Transformer&#23398;&#20064;&#22810;&#35270;&#35282;&#20960;&#20309;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26497;&#32447;&#26469;&#24341;&#23548;Transformer&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#25552;&#20379;&#20219;&#20309;&#25668;&#20687;&#26426;&#23039;&#24577;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#23039;&#24577;&#19981;&#21464;&#30340;&#29289;&#20307;&#23454;&#20363;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35270;&#35273;&#23398;&#20064;&#20013;&#34920;&#29616;&#24378;&#22823;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#23427;&#20204;&#32570;&#20047;&#25163;&#21160;&#35268;&#23450;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#28789;&#27963;&#24615;&#22312;&#28041;&#21450;&#22810;&#35270;&#35282;&#20960;&#20309;&#30340;&#20219;&#21153;&#20013;&#21487;&#33021;&#20250;&#25104;&#20026;&#38382;&#39064;&#65292;&#22240;&#20026;3D&#24418;&#29366;&#21644;&#35270;&#28857;&#30340;&#36817;&#20046;&#26080;&#38480;&#21487;&#33021;&#30340;&#21464;&#21270;&#38656;&#35201;&#28789;&#27963;&#24615;&#65292;&#32780;&#25237;&#24433;&#20960;&#20309;&#30340;&#31934;&#30830;&#24615;&#21017;&#38656;&#35201;&#20005;&#26684;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36731;&#35302;&#8221;&#26041;&#27861;&#65292;&#24341;&#23548;&#35270;&#35273;Transformer&#23398;&#20064;&#22810;&#35270;&#35282;&#20960;&#20309;&#65292;&#20294;&#22312;&#38656;&#35201;&#26102;&#20801;&#35768;&#23427;&#20204;&#33258;&#30001;&#21457;&#25381;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26497;&#32447;&#26469;&#24341;&#23548;Transformer&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#65292;&#24809;&#32602;&#26497;&#32447;&#20197;&#22806;&#30340;&#27880;&#24847;&#20540;&#65292;&#24182;&#40723;&#21169;&#27839;&#36825;&#20123;&#32447;&#30340;&#26356;&#39640;&#30340;&#27880;&#24847;&#65292;&#22240;&#20026;&#23427;&#20204;&#21253;&#21547;&#20960;&#20309;&#19978;&#21512;&#29702;&#30340;&#21305;&#37197;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#25552;&#20379;&#20219;&#20309;&#25668;&#20687;&#26426;&#23039;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#20851;&#27880;&#20110;&#23039;&#24577;&#19981;&#21464;&#30340;&#29289;&#20307;&#23454;&#20363;&#26816;&#32034;&#65292;&#26631;&#20934;&#30340;Transformer&#32593;&#32476;&#30001;&#20110;&#19981;&#21516;&#23039;&#24577;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#32780;&#38590;&#20197;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are powerful visual learners, in large part due to their conspicuous lack of manually-specified priors. This flexibility can be problematic in tasks that involve multiple-view geometry, due to the near-infinite possible variations in 3D shapes and viewpoints (requiring flexibility), and the precise nature of projective geometry (obeying rigid laws). To resolve this conundrum, we propose a "light touch" approach, guiding visual Transformers to learn multiple-view geometry but allowing them to break free when needed. We achieve this by using epipolar lines to guide the Transformer's cross-attention maps, penalizing attention values outside the epipolar lines and encouraging higher attention along these lines since they contain geometrically plausible matches. Unlike previous methods, our proposal does not require any camera pose information at test-time. We focus on pose-invariant object instance retrieval, where standard Transformer networks struggle, due to the large diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OReX&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22330;&#20316;&#20026;&#25554;&#20540;&#20808;&#39564;&#65292;&#20165;&#20351;&#29992;&#36755;&#20837;&#30340;&#24179;&#38754;&#20999;&#29255;&#65292;&#21363;&#21487;&#39640;&#36136;&#37327;&#22320;&#37325;&#24314;&#19977;&#32500;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2211.12886</link><description>&lt;p&gt;
OReX&#65306;&#20351;&#29992;&#31070;&#32463;&#22330;&#20174;&#24179;&#38754;&#20999;&#29255;&#37325;&#24314;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
OReX: Object Reconstruction from Planar Cross-sections Using Neural Fields. (arXiv:2211.12886v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OReX&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22330;&#20316;&#20026;&#25554;&#20540;&#20808;&#39564;&#65292;&#20165;&#20351;&#29992;&#36755;&#20837;&#30340;&#24179;&#38754;&#20999;&#29255;&#65292;&#21363;&#21487;&#39640;&#36136;&#37327;&#22320;&#37325;&#24314;&#19977;&#32500;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24179;&#38754;&#25130;&#38754;&#37325;&#24314;&#19977;&#32500;&#24418;&#29366;&#26159;&#21307;&#23398;&#24433;&#20687;&#23398;&#21644;&#22320;&#29702;&#20449;&#24687;&#23398;&#31561;&#19979;&#28216;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;&#36755;&#20837;&#26159;&#23436;&#20840;&#23450;&#20041;&#22312;&#31354;&#38388;&#20013;&#31232;&#30095;&#30340;&#24179;&#38754;&#38598;&#21512;&#19978;&#30340;&#20869;/&#22806;&#25351;&#31034;&#20989;&#25968;&#65292;&#36755;&#20986;&#26159;&#25351;&#31034;&#20989;&#25968;&#21040;&#25972;&#20010;&#20307;&#31215;&#30340;&#25554;&#20540;&#12290;&#20197;&#21069;&#35299;&#20915;&#36825;&#20010;&#31232;&#30095;&#21644;&#19981;&#36866;&#23450;&#38382;&#39064;&#30340;&#20316;&#21697;&#35201;&#20040;&#20135;&#29983;&#20302;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22914;&#30446;&#26631;&#25299;&#25169;&#12289;&#22806;&#35266;&#20449;&#24687;&#25110;&#36755;&#20837;&#27861;&#21521;&#26041;&#21521;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OReX&#65292;&#19968;&#31181;&#20165;&#20351;&#29992;&#20999;&#29255;&#21363;&#21487;&#36827;&#34892;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#29305;&#28857;&#26159;&#20197;&#31070;&#32463;&#22330;&#20316;&#20026;&#25554;&#20540;&#20808;&#39564;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#26469;&#35757;&#32451;&#36755;&#20837;&#24179;&#38754;&#65292;&#20197;&#36820;&#22238;&#32473;&#23450;3D&#22352;&#26631;&#30340;&#20869;/&#22806;&#20272;&#35745;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#24378;&#22823;&#30340;&#20808;&#39564;&#65292;&#33021;&#22815;&#35825;&#23548;&#24179;&#28369;&#24615;&#21644;&#33258;&#30456;&#20284;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#39640;&#39057;&#32454;&#33410;&#65292;&#22240;&#20026;&#31070;&#32463;&#20808;&#39564;&#20250;&#36807;&#20110;&#24179;&#28369;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing 3D shapes from planar cross-sections is a challenge inspired by downstream applications like medical imaging and geographic informatics. The input is an in/out indicator function fully defined on a sparse collection of planes in space, and the output is an interpolation of the indicator function to the entire volume. Previous works addressing this sparse and ill-posed problem either produce low quality results, or rely on additional priors such as target topology, appearance information, or input normal directions. In this paper, we present OReX, a method for 3D shape reconstruction from slices alone, featuring a Neural Field as the interpolation prior. A modest neural network is trained on the input planes to return an inside/outside estimate for a given 3D coordinate, yielding a powerful prior that induces smoothness and self-similarities. The main challenge for this approach is high-frequency details, as the neural prior is overly smoothing. To alleviate this, we offe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#22270;&#30340;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;(GraphS4mer)&#65292;&#29992;&#20110;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#26469;&#35299;&#20915;&#38271;&#26102;&#24207;&#21644;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11176</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models. (arXiv:2211.11176v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#22270;&#30340;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;(GraphS4mer)&#65292;&#29992;&#20110;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#26469;&#35299;&#20915;&#38271;&#26102;&#24207;&#21644;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#20013;&#37117;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#33041;&#30005;&#22270;&#12289;&#22810;&#23548;&#30561;&#30496;&#22270;&#21644;&#24515;&#30005;&#22270;&#12290;&#30001;&#20110;&#65288;1&#65289;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#65288;2&#65289;&#30005;&#26497;&#20043;&#38388;&#22797;&#26434;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#34920;&#31034;&#20026;&#26102;&#38388;&#20381;&#36182;&#22270;&#65292;&#24182;&#20171;&#32461;&#20102;GraphS4mer&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32467;&#26500;&#65292;&#36890;&#36807;&#24314;&#31435;&#29983;&#29289;&#20449;&#21495;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26469;&#25552;&#39640;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;1&#65289;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24207;&#21015;&#27169;&#22411;&#65292;&#26469;&#25429;&#25417;&#29983;&#29289;&#20449;&#21495;&#20013;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#65288;2&#65289;&#25105;&#20204;&#24314;&#35758;&#22312;GraphS4mer&#20013;&#28155;&#21152;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#65292;&#20197;&#23398;&#20064;&#25968;&#25454;&#20013;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#24314;&#31435;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate biosignals are prevalent in many medical domains, such as electroencephalography, polysomnography, and electrocardiography. Modeling spatiotemporal dependencies in multivariate biosignals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between the electrodes. To address these challenges, we propose representing multivariate biosignals as time-dependent graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that improves performance on biosignal classification tasks by modeling spatiotemporal dependencies in biosignals. Specifically, (1) we leverage the Structured State Space architecture, a state-of-the-art deep sequence model, to capture long-range temporal dependencies in biosignals and (2) we propose a graph structure learning layer in GraphS4mer to learn dynamically evolving graph structures in the data. We evaluate our proposed model on three distinct biosignal classification tasks and show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DP-FedEmb&#31639;&#27861;&#65292;&#36890;&#36807;&#34394;&#25311;&#23458;&#25143;&#31471;&#12289;&#37096;&#20998;&#32858;&#21512;&#12289;&#31169;&#26377;&#26412;&#22320;&#24494;&#35843;&#21644;&#20844;&#20849;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#12290;&#22312;&#22270;&#20687;&#23884;&#20837;&#27169;&#22411;&#30340;&#23398;&#20064;&#20013;&#65292;DP-FedEmb&#33021;&#22815;&#22312;&#20445;&#25345;&#33391;&#22909;&#27169;&#22411;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#36739;&#24378;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#24471;&#21040;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2211.10844</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#22270;&#20687;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Image Embeddings with User-level Differential Privacy. (arXiv:2211.10844v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DP-FedEmb&#31639;&#27861;&#65292;&#36890;&#36807;&#34394;&#25311;&#23458;&#25143;&#31471;&#12289;&#37096;&#20998;&#32858;&#21512;&#12289;&#31169;&#26377;&#26412;&#22320;&#24494;&#35843;&#21644;&#20844;&#20849;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#12290;&#22312;&#22270;&#20687;&#23884;&#20837;&#27169;&#22411;&#30340;&#23398;&#20064;&#20013;&#65292;DP-FedEmb&#33021;&#22815;&#22312;&#20445;&#25345;&#33391;&#22909;&#27169;&#22411;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#36739;&#24378;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#24471;&#21040;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#65292;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#35757;&#32451;&#23567;&#22411;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#31867;&#31354;&#38388;&#30340;&#21463;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#23884;&#20837;&#24335;&#27169;&#22411;&#26102;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#23454;&#29616;&#22823;&#22411;&#22270;&#20687;&#21040;&#23884;&#20837;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DP-FedEmb&#65292;&#36825;&#26159;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#21464;&#20307;&#65292;&#20855;&#26377;&#27599;&#20010;&#29992;&#25143;&#30340;&#28789;&#25935;&#24230;&#25511;&#21046;&#21644;&#22122;&#22768;&#28155;&#21152;&#65292;&#20197;&#20174;&#22312;&#25968;&#25454;&#20013;&#24515;&#38598;&#20013;&#30340;&#29992;&#25143;&#20998;&#21306;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;DP-FedEmb&#32467;&#21512;&#20102;&#34394;&#25311;&#23458;&#25143;&#31471;&#12289;&#37096;&#20998;&#32858;&#21512;&#12289;&#31169;&#26377;&#26412;&#22320;&#24494;&#35843;&#21644;&#20844;&#20849;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#12290;&#25105;&#20204;&#23558;DP-FedEmb&#24212;&#29992;&#20110;&#20026;&#38754;&#37096;&#12289;&#22320;&#26631;&#21644;&#33258;&#28982;&#29289;&#31181;&#35757;&#32451;&#22270;&#20687;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;DigiFace&#12289;EMNIST&#12289;GLD&#21644;iNaturalist&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#30456;&#21516;&#38544;&#31169;&#39044;&#31639;&#19979;&#30340;&#20248;&#36234;&#25928;&#29992;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#65292;&#22312;&#23398;&#20064;&#22823;&#22411;&#22270;&#20687;&#21040;&#23884;&#20837;&#29305;&#24449;&#25552;&#21462;&#22120;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small on-device models have been successfully trained with user-level differential privacy (DP) for next word prediction and image classification tasks in the past. However, existing methods can fail when directly applied to learn embedding models using supervised training data with a large class space. To achieve user-level DP for large image-to-embedding feature extractors, we propose DP-FedEmb, a variant of federated learning algorithms with per-user sensitivity control and noise addition, to train from user-partitioned data centralized in the datacenter. DP-FedEmb combines virtual clients, partial aggregation, private local fine-tuning, and public pretraining to achieve strong privacy utility trade-offs. We apply DP-FedEmb to train image embedding models for faces, landmarks and natural species, and demonstrate its superior utility under same privacy budget on benchmark datasets DigiFace, EMNIST, GLD and iNaturalist. We further illustrate it is possible to achieve strong user-level
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#35774;&#35745;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22320;&#19979;&#21453;&#38382;&#39064;&#23637;&#31034;&#24212;&#29992;&#20215;&#20540;&#12290;&#30740;&#31350;&#20351;&#29992;Score-CAM&#21644;Deep SHAP&#31561;&#26041;&#27861;&#36873;&#25321;&#36229;&#21442;&#25968;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.08651</link><description>&lt;p&gt;
&#24212;&#29992;&#21487;&#35299;&#37322;&#24615;&#35774;&#35745;&#29289;&#29702;&#24863;&#30693;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22320;&#19979;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Using explainability to design physics-aware CNNs for solving subsurface inverse problems. (arXiv:2211.08651v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#35774;&#35745;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22320;&#19979;&#21453;&#38382;&#39064;&#23637;&#31034;&#24212;&#29992;&#20215;&#20540;&#12290;&#30740;&#31350;&#20351;&#29992;Score-CAM&#21644;Deep SHAP&#31561;&#26041;&#27861;&#36873;&#25321;&#36229;&#21442;&#25968;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#35774;&#35745;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#27714;&#35299;&#27973;&#23618;&#22320;&#19979;&#25104;&#20687;&#21453;&#38382;&#39064;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;CNN&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#36817;&#24180;&#26469;&#21464;&#24471;&#27969;&#34892;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#24320;&#21457;&#20173;&#28982;&#26159;&#19968;&#31181;&#33402;&#26415;&#65292;&#22240;&#20026;&#20851;&#20110;&#36873;&#25321;&#20250;&#20135;&#29983;&#26368;&#20339;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#30340;&#26126;&#30830;&#25351;&#23548;&#24182;&#19981;&#23384;&#22312;&#12290;&#34429;&#28982;&#21487;&#20197;&#20351;&#29992;&#20248;&#21270;&#31639;&#27861;&#33258;&#21160;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#30528;&#37325;&#20110;&#24320;&#21457;&#20855;&#26377;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#32593;&#32476;&#65292;&#32780;&#24573;&#30053;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#65288;&#25551;&#36848;&#20934;&#30830;&#24615;&#65289;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021; (XAI) &#39046;&#22495;&#36890;&#36807;&#25552;&#20379;&#20801;&#35768;&#24320;&#21457;&#32773;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#36923;&#36753;&#30340;&#24037;&#20855;&#26469;&#35299;&#20915;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#32570;&#22833;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861; Score-CAM &#21644; Deep SHAP &#26469;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#20363;&#22914; kern...
&lt;/p&gt;
&lt;p&gt;
We present a novel method of using explainability techniques to design physics-aware neural networks. We demonstrate our approach by developing a convolutional neural network (CNN) for solving an inverse problem for shallow subsurface imaging. Although CNNs have gained popularity in recent years across many fields, the development of CNNs remains an art, as there are no clear guidelines regarding the selection of hyperparameters that will yield the best network. While optimization algorithms may be used to select hyperparameters automatically, these methods focus on developing networks with high predictive accuracy while disregarding model explainability (descriptive accuracy). However, the field of Explainable Artificial Intelligence (XAI) addresses the absence of model explainability by providing tools that allow developers to evaluate the internal logic of neural networks. In this study, we use the explainability methods Score-CAM and Deep SHAP to select hyperparameters, such as ker
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#35299;&#37322;&#34892;&#20026;&#24314;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#29983;&#21487;&#20197;&#29702;&#35299;&#25152;&#23398;&#30340;&#20869;&#23481;&#24182;&#36827;&#34892;&#25512;&#29702;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;</title><link>http://arxiv.org/abs/2211.07882</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Explainable Action Advising for Multi-Agent Reinforcement Learning. (arXiv:2211.07882v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07882
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#35299;&#37322;&#34892;&#20026;&#24314;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#29983;&#21487;&#20197;&#29702;&#35299;&#25152;&#23398;&#30340;&#20869;&#23481;&#24182;&#36827;&#34892;&#25512;&#29702;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#24314;&#35758;&#26159;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33539;&#24335;&#30340;&#24378;&#21270;&#23398;&#20064;&#30693;&#35782;&#36716;&#31227;&#25216;&#26415;&#12290;&#19987;&#23478;&#32769;&#24072;&#22312;&#35757;&#32451;&#26399;&#38388;&#25552;&#20379;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#34920;&#29616;&#12290;&#36825;&#31181;&#24314;&#35758;&#36890;&#24120;&#20197;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24418;&#24335;&#32473;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20351;&#24471;&#23398;&#29983;&#38590;&#20197;&#25512;&#29702;&#21644;&#24212;&#29992;&#20110;&#26032;&#39062;&#29366;&#24577;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#34892;&#20026;&#24314;&#35758;&#65292;&#20854;&#20013;&#32769;&#24072;&#25552;&#20379;&#34892;&#20026;&#24314;&#35758;&#21644;&#30456;&#20851;&#30340;&#35299;&#37322;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36873;&#21462;&#35813;&#34892;&#20026;.&#36825;&#20801;&#35768;&#23398;&#29983;&#33258;&#25105;&#21453;&#24605;&#25152;&#23398;&#30340;&#20869;&#23481;&#65292;&#23454;&#29616;&#24314;&#35758;&#30340;&#27867;&#21270;&#65292;&#24182;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#30340;&#25552;&#39640;&#8212;&#8212;&#21363;&#20351;&#22312;&#32769;&#24072;&#19981;&#29702;&#24819;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#31574;&#30053;&#22238;&#25253;&#21644;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#26631;&#31614;&#22122;&#22768;&#23545;FL&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20250;&#32447;&#24615;&#19979;&#38477;&#65292;&#21516;&#26102;&#20250;&#23548;&#33268;FL&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#20943;&#32531;&#21644;&#20840;&#23616;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2211.07816</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#37327;&#21270;&#26631;&#31614;&#22122;&#22768;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Impact of Label Noise on Federated Learning. (arXiv:2211.07816v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#26631;&#31614;&#22122;&#22768;&#23545;FL&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20250;&#32447;&#24615;&#19979;&#38477;&#65292;&#21516;&#26102;&#20250;&#23548;&#33268;FL&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#20943;&#32531;&#21644;&#20840;&#23616;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#20351;&#29992;&#26412;&#22320;&#65288;&#20154;&#20026;&#29983;&#25104;&#30340;&#65289;&#25968;&#25454;&#38598;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;FL&#31639;&#27861;&#30340;&#24320;&#21457;&#19978;&#20197;&#35299;&#20915;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#32780;&#22312;FL&#20013;&#25968;&#25454;&#36136;&#37327;&#65288;&#22914;&#26631;&#31614;&#22122;&#22768;&#65289;&#36825;&#19968;&#37325;&#35201;&#38382;&#39064;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#26631;&#31614;&#22122;&#22768;&#23545;FL&#30340;&#24433;&#21709;&#36827;&#34892;&#23450;&#37327;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#19978;&#30028;&#26469;&#34913;&#37327;&#23458;&#25143;&#31471;&#26631;&#31614;&#22122;&#22768;&#27700;&#24179;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;FL&#31639;&#27861;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24615;&#20250;&#32447;&#24615;&#19979;&#38477;&#65292;&#36825;&#19982;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#36739;&#39640;&#26102;&#65292;&#26631;&#31614;&#22122;&#22768;&#20250;&#20943;&#32531;FL&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning paradigm where clients collaboratively train a model using their local (human-generated) datasets. While existing studies focus on FL algorithm development to tackle data heterogeneity across clients, the important issue of data quality (e.g., label noise) in FL is overlooked. This paper aims to fill this gap by providing a quantitative study on the impact of label noise on FL. We derive an upper bound for the generalization error that is linear in the clients' label noise level. Then we conduct experiments on MNIST and CIFAR-10 datasets using various FL algorithms. Our empirical results show that the global model accuracy linearly decreases as the noise level increases, which is consistent with our theoretical analysis. We further find that label noise slows down the convergence of FL training, and the global model tends to overfit when the noise level is high.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01201</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Human alignment of neural network representations. (arXiv:2211.01201v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20154;&#31867;&#25110;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#21644;&#23398;&#20064;&#31639;&#27861;&#19982;&#23548;&#33268;&#20154;&#31867;&#35270;&#35273;&#30340;&#26041;&#24335;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#20043;&#22788;&#12290;&#26412;&#25991;&#30740;&#31350;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#36890;&#36807;&#34892;&#20026;&#21453;&#24212;&#25512;&#26029;&#20986;&#30340;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#23545;&#40784;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#19982;&#20154;&#31867;&#34892;&#20026;&#21453;&#24212;&#30340;&#23545;&#40784;&#22522;&#26412;&#19978;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#21017;&#20855;&#26377;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#22312;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#20219;&#21153;&#25910;&#38598;&#30340;&#19977;&#20010;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#20154;&#31867;&#27010;&#24565;...
&lt;/p&gt;
&lt;p&gt;
Today's computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concept
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Pop2Piano&#65292;&#19968;&#20010;&#36890;&#36807;Transformer&#32593;&#32476;&#65292;&#30452;&#25509;&#20174;&#27969;&#34892;&#38899;&#39057;&#29983;&#25104;&#38050;&#29748;&#32763;&#22863;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#37327;&#37197;&#23545;&#21644;&#21516;&#27493;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#29702;&#30340;&#38050;&#29748;&#32763;&#22863;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.00895</link><description>&lt;p&gt;
Pop2Piano: &#22522;&#20110;&#27969;&#34892;&#38899;&#39057;&#30340;&#38050;&#29748;&#32763;&#22863;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pop2Piano : Pop Audio-based Piano Cover Generation. (arXiv:2211.00895v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Pop2Piano&#65292;&#19968;&#20010;&#36890;&#36807;Transformer&#32593;&#32476;&#65292;&#30452;&#25509;&#20174;&#27969;&#34892;&#38899;&#39057;&#29983;&#25104;&#38050;&#29748;&#32763;&#22863;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#37327;&#37197;&#23545;&#21644;&#21516;&#27493;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#29702;&#30340;&#38050;&#29748;&#32763;&#22863;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#38899;&#20048;&#30340;&#38050;&#29748;&#32763;&#22863;&#28145;&#21463;&#24456;&#22810;&#20154;&#30340;&#21916;&#29233;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#29983;&#25104;&#27969;&#34892;&#38899;&#20048;&#30340;&#38050;&#29748;&#32763;&#22863;&#30340;&#20219;&#21153;&#20173;&#28982;&#32570;&#20047;&#30740;&#31350;&#12290;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#23569;&#21516;&#27493;&#30340;{&#27969;&#34892;&#65292;&#38050;&#29748;&#32763;&#22863;}&#25968;&#25454;&#23545;&#65292;&#36825;&#20351;&#24471;&#24212;&#29992;&#26368;&#26032;&#30340;&#25968;&#25454;&#23494;&#38598;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#21457;&#25381;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#23041;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21270;&#27969;&#31243;&#29983;&#25104;&#20102;&#22823;&#37327;&#37197;&#23545;&#21644;&#21516;&#27493;&#30340;{&#27969;&#34892;&#65292;&#38050;&#29748;&#32763;&#22863;}&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pop2Piano&#65292;&#19968;&#20010;&#33021;&#22815;&#26681;&#25454;&#27969;&#34892;&#38899;&#20048;&#30340;&#27874;&#24418;&#29983;&#25104;&#38050;&#29748;&#32763;&#22863;&#30340;Transformer&#32593;&#32476;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30452;&#25509;&#20174;&#27969;&#34892;&#38899;&#39057;&#29983;&#25104;&#38050;&#29748;&#32763;&#22863;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#26059;&#24459;&#21644;&#21644;&#24358;&#25552;&#21462;&#27169;&#22359;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;Pop2Piano&#33021;&#22815;&#20135;&#29983;&#21512;&#29702;&#30340;&#38050;&#29748;&#32763;&#22863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Piano covers of pop music are enjoyed by many people. However, the task of automatically generating piano covers of pop music is still understudied. This is partly due to the lack of synchronized {Pop, Piano Cover} data pairs, which made it challenging to apply the latest data-intensive deep learning-based methods. To leverage the power of the data-driven approach, we make a large amount of paired and synchronized {Pop, Piano Cover} data using an automated pipeline. In this paper, we present Pop2Piano, a Transformer network that generates piano covers given waveforms of pop music. To the best of our knowledge, this is the first model to generate a piano cover directly from pop audio without using melody and chord extraction modules. We show that Pop2Piano, trained with our dataset, is capable of producing plausible piano covers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;Subgraph MPNNs&#30340;GNN&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Subgraph MPNNs&#19981;&#33021;&#22312;&#33410;&#28857;&#32423;&#21035;&#19978;&#35745;&#25968;&#36229;&#36807;4&#20010;&#30340;&#29615;&#65292;&#36825;&#23545;&#20110;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2210.13978</link><description>&lt;p&gt;
&#25552;&#21319;I$^2$-GNN&#22312;&#24490;&#29615;&#35745;&#25968;&#26041;&#38754;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs. (arXiv:2210.13978v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;Subgraph MPNNs&#30340;GNN&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Subgraph MPNNs&#19981;&#33021;&#22312;&#33410;&#28857;&#32423;&#21035;&#19978;&#35745;&#25968;&#36229;&#36807;4&#20010;&#30340;&#29615;&#65292;&#36825;&#23545;&#20110;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;(MPNNs)&#26159;&#19968;&#31867;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#12290;&#28982;&#32780;&#65292;MPNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#21551;&#21457;&#25105;&#20204;&#30740;&#31350;&#21487;&#35777;&#26126;&#20855;&#26377;&#26356;&#24378;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#23376;&#22270;MPNNs&#30340;&#35745;&#25968;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31867;&#26368;&#26032;&#21644;&#24120;&#29992;&#30340;&#24378;&#22823;GNN&#27169;&#22411;&#65292;&#20854;&#20174;&#27599;&#20010;&#33410;&#28857;&#25552;&#21462;&#26681;&#25454;&#23376;&#22270;&#65292;&#22312;&#26681;&#33410;&#28857;&#20998;&#37197;&#21807;&#19968;&#26631;&#35782;&#31526;&#24182;&#22312;&#20854;&#26681;&#25454;&#23376;&#22270;&#20013;&#32534;&#30721;&#26681;&#33410;&#28857;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#23376;&#22270;MPNNs&#19981;&#33021;&#22312;&#33410;&#28857;&#32423;&#21035;&#19978;&#35745;&#25968;&#36229;&#36807;4&#20010;&#30340;&#29615;&#65292;&#36825;&#24847;&#21619;&#30528;&#33410;&#28857;&#34920;&#31034;&#19981;&#33021;&#27491;&#30830;&#22320;&#32534;&#30721;&#21608;&#22260;&#30340;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message Passing Neural Networks (MPNNs) are a widely used class of Graph Neural Networks (GNNs). The limited representational power of MPNNs inspires the study of provably powerful GNN architectures. However, knowing one model is more powerful than another gives little insight about what functions they can or cannot express. It is still unclear whether these models are able to approximate specific functions such as counting certain graph substructures, which is essential for applications in biology, chemistry and social network analysis. Motivated by this, we propose to study the counting power of Subgraph MPNNs, a recent and popular class of powerful GNN models that extract rooted subgraphs for each node, assign the root node a unique identifier and encode the root node's representation within its rooted subgraph. Specifically, we prove that Subgraph MPNNs fail to count more-than-4-cycles at node level, implying that node representations cannot correctly encode the surrounding substru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;RibSeg&#25968;&#25454;&#38598;&#21040;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;RibSeg v2&#65292;&#21152;&#20837;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#65292;&#20849;&#21253;&#21547;660&#20010;CT&#25195;&#25551;&#65288;15,466&#20010;&#29420;&#31435;&#30340;&#32907;&#39592;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32907;&#39592;&#26631;&#35760;&#12289;&#22522;&#20110;&#39592;&#26550;&#21270;&#26041;&#27861;&#29992;&#20110;&#20013;&#24515;&#32447;&#25552;&#21462;&#12289;&#19968;&#31181;&#31232;&#30095;&#28857;&#20113;&#34920;&#31034;CT&#25195;&#25551;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2210.09309</link><description>&lt;p&gt;
RibSeg v2&#65306;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction. (arXiv:2210.09309v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;RibSeg&#25968;&#25454;&#38598;&#21040;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;RibSeg v2&#65292;&#21152;&#20837;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#65292;&#20849;&#21253;&#21547;660&#20010;CT&#25195;&#25551;&#65288;15,466&#20010;&#29420;&#31435;&#30340;&#32907;&#39592;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32907;&#39592;&#26631;&#35760;&#12289;&#22522;&#20110;&#39592;&#26550;&#21270;&#26041;&#27861;&#29992;&#20110;&#20013;&#24515;&#32447;&#25552;&#21462;&#12289;&#19968;&#31181;&#31232;&#30095;&#28857;&#20113;&#34920;&#31034;CT&#25195;&#25551;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#26159;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#30340;&#24120;&#35265;&#21069;&#25552;&#26465;&#20214;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#35201;&#20040;&#20351;&#29992;&#20869;&#37096;&#25968;&#25454;&#38598;&#65292;&#26080;&#27861;&#20026;&#31038;&#32676;&#25152;&#20849;&#20139;&#65292;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#32907;&#39592;&#20998;&#21106;&#32780;&#24573;&#30053;&#20102;&#32907;&#39592;&#26631;&#35760;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#20043;&#21069;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#32907;&#39592;&#20998;&#21106;&#30340;RibSeg&#25968;&#25454;&#38598;&#25193;&#23637;&#20026;&#32508;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;RibSeg v2&#65292;&#24182;&#21152;&#20837;&#20102;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#30340;&#25163;&#21160;&#26631;&#27880;&#65292;&#20849;&#21253;&#21547;&#20102;660&#20010;CT&#25195;&#25551;&#65288;15,466&#20010;&#29420;&#31435;&#30340;&#32907;&#39592;&#65289;&#12290;&#22522;&#20110;RibSeg v2&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21253;&#21547;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32907;&#39592;&#26631;&#35760;&#65292;&#20197;&#21450;&#22522;&#20110;&#39592;&#26550;&#21270;&#26041;&#27861;&#29992;&#20110;&#20013;&#24515;&#32447;&#25552;&#21462;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#28857;&#20113;&#34920;&#31034;CT&#25195;&#25551;&#30340;&#26041;&#27861;&#65292;&#19982;&#26631;&#20934;&#30340;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#21644;&#20998;&#26512;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#20219;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#65292;&#8230;
&lt;/p&gt;
&lt;p&gt;
Automatic rib labeling and anatomical centerline extraction are common prerequisites for various clinical applications. Prior studies either use in-house datasets that are inaccessible to communities, or focus on rib segmentation that neglects the clinical significance of rib labeling. To address these issues, we extend our prior dataset (RibSeg) on the binary rib segmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CT scans (15,466 individual ribs in total) and annotations manually inspected by experts for rib labeling and anatomical centerline extraction. Based on the RibSeg v2, we develop a pipeline including deep learning-based methods for rib labeling, and a skeletonization-based method for centerline extraction. To improve computational efficiency, we propose a sparse point cloud representation of CT scans and compare it with standard dense voxel grids. Moreover, we design and analyze evaluation metrics to address the key challenges of each task. Our dataset,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#39640;&#25928;&#35745;&#21010;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#24037;&#20316;&#65292;&#20854;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#24182;&#19988;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;</title><link>http://arxiv.org/abs/2210.07420</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#25928;&#35745;&#21010;&#31283;&#20581;&#30340;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#39640;&#25928;&#35745;&#21010;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#24037;&#20316;&#65292;&#20854;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#24182;&#19988;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26434;&#20081;&#38382;&#39064;&#65292;&#22810;&#20010;&#21018;&#24615;&#20984;&#22810;&#36793;&#24418;&#29289;&#20307;&#38543;&#26426;&#25918;&#32622;&#22312;&#19968;&#20010;&#24179;&#38754;&#34920;&#38754;&#19978;&#65292;&#24517;&#39035;&#20351;&#29992;&#21333;&#20010;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#25235;&#21462;&#26041;&#24335;&#65292;&#23558;&#23427;&#20204;&#26377;&#25928;&#22320;&#36816;&#36755;&#21040;&#35013;&#31665;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#25705;&#25830;&#26469;&#22686;&#21152;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#65292;&#24182;&#20351;&#29992;&#23454;&#20363;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20197;&#35745;&#21010;&#31283;&#20581;&#30340;&#22810;&#29289;&#20307;&#25235;&#21462;&#12290;&#22312;&#29289;&#29702;&#23454;&#39564;&#20013;&#65292;&#30456;&#27604;&#20110;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#21457;&#29616;&#25104;&#21151;&#29575;&#22686;&#21152;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;&#19982;&#21333;&#20010;&#29289;&#20307;&#25235;&#21462;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;3.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a decluttering problem where multiple rigid convex polygonal objects rest in randomly placed positions and orientations on a planar surface and must be efficiently transported to a packing box using both single and multi-object grasps. Prior work considered frictionless multi-object grasping. In this paper, we introduce friction to increase picks per hour. We train a neural network using real examples to plan robust multi-object grasps. In physical experiments, we find a 13.7% increase in success rate, a 1.6x increase in picks per hour, and a 6.3x decrease in grasp planning time compared to prior work on multi-object grasping. Compared to single object grasping, we find a 3.1x increase in picks per hour.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.07199</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#20960;&#20309;&#23545;&#24212;&#29992;&#20110;&#37326;&#22806;&#31867;&#21035;&#32423;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild. (arXiv:2210.07199v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a self-supervised learning approach for category-level 6D object pose estimation in the wild, which reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. The proposed novel geometrical cycle-consistency losses construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and other tasks.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#65292;&#23427;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#12290;&#24403;&#36716;&#21521;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#26102;&#65292;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#23545;&#26410;&#35265;&#23454;&#20363;&#36827;&#34892;&#27867;&#21270;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#21463;&#21040;&#20174;&#27169;&#25311;&#25110;&#20174;&#20154;&#31867;&#25910;&#38598;&#30340;&#27880;&#37322;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#36825;&#19968;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37325;&#26500;&#20102;&#29289;&#20307;&#31867;&#21035;&#30340;&#35268;&#33539;3D&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;&#23545;&#20110;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#65292;&#23427;&#20204;&#22312;2D-3D&#31354;&#38388;&#12289;&#19981;&#21516;&#23454;&#20363;&#21644;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#26500;&#24314;&#24490;&#29615;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While 6D object pose estimation has wide applications across computer vision and robotics, it remains far from being solved due to the lack of annotations. The problem becomes even more challenging when moving to category-level 6D pose, which requires generalization to unseen instances. Current approaches are restricted by leveraging annotations from simulation or collected from humans. In this paper, we overcome this barrier by introducing a self-supervised learning approach trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. Our framework reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. For training, we propose novel geometrical cycle-consistency losses which construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and othe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.06462</link><description>&lt;p&gt;
&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Guided Diffusion Models. (arXiv:2210.06462v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#25351;&#23548;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#26102;&#12290;&#28982;&#32780;&#65292;&#25351;&#23548;&#38656;&#35201;&#22823;&#37327;&#30340;&#22270;&#20687;-&#27880;&#37322;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;&#20854;&#21487;&#29992;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26080;&#20559;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#27880;&#37322;&#38656;&#27714;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#25552;&#21462;&#20989;&#25968;&#21644;&#33258;&#25105;&#27880;&#37322;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22270;&#20687;&#31890;&#24230;&#19978;&#25552;&#20379;&#25351;&#23548;&#20449;&#21495;&#65306;&#20174;&#25972;&#20307;&#22270;&#20687;&#21040;&#29289;&#20307;&#26694;&#65292;&#29978;&#33267;&#21040;&#20998;&#21106;&#33945;&#29256;&#12290;&#25105;&#20204;&#22312;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability, correctness and unbiasedness. In this paper, we eliminate the need for such annotation by instead leveraging the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels, especially on unbalanced data. When equipped with self-supervised box or m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#21644;&#24102;&#25513;&#33180;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#20110;&#35757;&#32451;&#30340;&#22024;&#26434;&#20294;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;CLIP&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#24102;&#26377;&#25513;&#33180;&#30340;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04150</link><description>&lt;p&gt;
Mask-adapted CLIP&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP. (arXiv:2210.04150v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#21644;&#24102;&#25513;&#33180;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#20110;&#35757;&#32451;&#30340;&#22024;&#26434;&#20294;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;CLIP&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#24102;&#26377;&#25513;&#33180;&#30340;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#30340;&#30446;&#30340;&#26159;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#23558;&#22270;&#20687;&#20998;&#21106;&#20026;&#35821;&#20041;&#21306;&#22495;&#65292;&#36825;&#20123;&#25551;&#36848;&#21487;&#33021;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#34987;&#35266;&#23519;&#21040;&#12290;&#26368;&#36817;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#19981;&#32771;&#34385;&#31867;&#30340;&#25513;&#30721;&#25552;&#35758;&#65292;&#28982;&#21518;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#23545;&#25513;&#30721;&#21306;&#22495;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#19968;&#33539;&#20363;&#30340;&#24615;&#33021;&#29942;&#39048;&#26159;&#39044;&#35757;&#32451;CLIP&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#22312;&#36974;&#34109;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#32452;&#24102;&#26377;&#25513;&#30721;&#22270;&#20687;&#21306;&#22495;&#21450;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;&#25968;&#25454;&#19978;&#23545;CLIP&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;CLIP&#23558;&#25513;&#30721;&#22270;&#20687;&#21306;&#22495;&#19982;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;&#21517;&#35789;&#21305;&#37197;&#26469;&#25366;&#25496;&#29616;&#26377;&#22270;&#20687;-&#26631;&#39064;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;COCO Captions&#65289;&#26469;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;&#19982;&#26356;&#31934;&#30830;&#19988;&#25163;&#21160;&#27880;&#37322;&#30340;&#22266;&#23450;&#31867;&#21035;&#20998;&#21106;&#26631;&#31614;&#65288;&#20363;&#22914;COCO-Stuff&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#22024;&#26434;&#20294;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#33021;&#26356;&#22909;&#22320;&#20445;&#30041;CLIP&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#38500;&#20102;&#24494;&#35843;&#25972;&#20010;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36866;&#24212;&#25513;&#30721;&#30340;CLIP&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#24314;&#27169;&#33945;&#29256;&#36807;&#31243;&#26469;&#26356;&#22909;&#22320;&#22788;&#29702;&#24102;&#26377;&#25513;&#30721;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;OpenImages&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#65292;&#31616;&#21333;&#30340;&#29983;&#29289;&#23398;&#32422;&#26463;&#65288;&#22914;&#22312;&#27963;&#21160;&#21644;&#26435;&#37325;&#26041;&#38754;&#30340;&#38750;&#36127;&#24615;&#21644;&#33021;&#37327;&#25928;&#29575;&#65289;&#21487;&#20197;&#20419;&#36827;&#22823;&#33041;&#31070;&#32463;&#20803;&#30340;&#21333;&#22240;&#32032;&#36873;&#25321;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22823;&#33041;&#20013;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#32463;&#24120;&#34920;&#31034;&#21333;&#20010;&#21487;&#35299;&#37322;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2210.01768</link><description>&lt;p&gt;
&#26377;&#29983;&#29289;&#23398;&#32422;&#26463;&#30340;&#35299;&#32544;&#35770;&#65306;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentanglement with Biological Constraints: A Theory of Functional Cell Types. (arXiv:2210.01768v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#65292;&#31616;&#21333;&#30340;&#29983;&#29289;&#23398;&#32422;&#26463;&#65288;&#22914;&#22312;&#27963;&#21160;&#21644;&#26435;&#37325;&#26041;&#38754;&#30340;&#38750;&#36127;&#24615;&#21644;&#33021;&#37327;&#25928;&#29575;&#65289;&#21487;&#20197;&#20419;&#36827;&#22823;&#33041;&#31070;&#32463;&#20803;&#30340;&#21333;&#22240;&#32032;&#36873;&#25321;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22823;&#33041;&#20013;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#32463;&#24120;&#34920;&#31034;&#21333;&#20010;&#21487;&#35299;&#37322;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#20013;&#30340;&#31070;&#32463;&#20803;&#32463;&#24120;&#23545;&#29305;&#23450;&#20219;&#21153;&#21464;&#37327;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#35299;&#32544;&#30340;&#34920;&#31034;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#22791;&#21463;&#36861;&#25447;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25968;&#23398;&#35777;&#26126;&#20102;&#23545;&#31070;&#32463;&#20803;&#30340;&#31616;&#21333;&#29983;&#29289;&#23398;&#32422;&#26463;&#65288;&#21363;&#22312;&#27963;&#21160;&#21644;&#26435;&#37325;&#26041;&#38754;&#30340;&#38750;&#36127;&#24615;&#21644;&#33021;&#37327;&#25928;&#29575;&#65289;&#36890;&#36807;&#24378;&#21046;&#31070;&#32463;&#20803;&#23545;&#20219;&#21153;&#21464;&#21270;&#30340;&#21333;&#20010;&#22240;&#32032;&#20855;&#26377;&#36873;&#25321;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#36825;&#31181;&#34987;&#36861;&#27714;&#30340;&#35299;&#32544;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#32422;&#26463;&#21487;&#23548;&#33268;&#21508;&#31181;&#20219;&#21153;&#21644;&#26550;&#26500;&#19979;&#30340;&#35299;&#32544;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36825;&#20010;&#29702;&#35770;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22823;&#33041;&#23558;&#20854;&#32454;&#32990;&#20998;&#25104;&#19981;&#21516;&#30340;&#32454;&#32990;&#31867;&#22411;&#65288;&#20363;&#22914;&#32593;&#26684;&#21644;&#23545;&#35937;&#21521;&#37327;&#32454;&#32990;&#65289;&#65292;&#20197;&#21450;&#35299;&#37322;&#20102;&#22823;&#33041;&#20309;&#26102;&#23545;&#32416;&#32544;&#30340;&#20219;&#21153;&#22240;&#32032;&#36827;&#34892;&#32416;&#32544;&#30340;&#34920;&#31034;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#23545;&#20026;&#20160;&#20040;&#22823;&#33041;&#20013;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#32463;&#24120;&#34920;&#31034;&#21333;&#20010;&#21487;&#35299;&#37322;&#22240;&#32032;&#30340;&#25968;&#23398;&#29702;&#35299;&#65292;&#24182;&#36808;&#21521;&#20102;&#35299;&#20915;&#20219;&#21153;&#34920;&#31034;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentanglement in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why single neurons in the brain often represent single human-interpretable factors, and steps towards an understanding task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36719;&#27169;&#26495;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22522;&#31867;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;LASP, &#21516;&#26102;&#36890;&#36807;&#22686;&#21152;&#25552;&#31034;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#26657;&#20934;&#35270;&#35273;-&#35821;&#35328;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292; &#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.01115</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;LASP&#65306;&#38754;&#21521;&#35821;&#35328;&#24863;&#30693;&#30340;&#25991;&#26412;&#20248;&#21270;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision &amp; Language Models. (arXiv:2210.01115v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36719;&#27169;&#26495;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22522;&#31867;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;LASP, &#21516;&#26102;&#36890;&#36807;&#22686;&#21152;&#25552;&#31034;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#26657;&#20934;&#35270;&#35273;-&#35821;&#35328;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292; &#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#27169;&#26495;&#23398;&#20064;&#26368;&#36817;&#24050;&#25104;&#20026;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;V&amp;L&#27169;&#22411;&#30340;&#36873;&#25321;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#32463;&#36807;&#35757;&#32451;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#36739;&#22823;&#32570;&#38519;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#35821;&#35328;&#24863;&#30693;&#30340;&#25991;&#26412;&#25552;&#31034;&#65288;LASP&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#22522;&#31867;&#30340;&#36807;&#25311;&#21512;&#65292;&#22686;&#21152;&#25552;&#31034;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#26657;&#20934;&#35270;&#35273;-&#35821;&#35328;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft prompt learning has recently emerged as one of the methods of choice for adapting V&amp;L models to a downstream task using a few training examples. However, current methods significantly overfit the training data, suffering from large accuracy degradation when tested on unseen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alleviate base class overfitting, we propose a novel Language-Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;PDE-G-CNN&#27169;&#22411;&#20013;&#30340;&#24418;&#24577;&#23398;&#21367;&#31215;&#26680;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#25512;&#33616;&#30340;&#36817;&#20284;&#26680;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.00935</link><description>&lt;p&gt;
(&#23376;)&#40654;&#26364;&#20960;&#20309;PDE-G-CNN&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of (sub-)Riemannian PDE-G-CNNs. (arXiv:2210.00935v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;PDE-G-CNN&#27169;&#22411;&#20013;&#30340;&#24418;&#24577;&#23398;&#21367;&#31215;&#26680;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#25512;&#33616;&#30340;&#36817;&#20284;&#26680;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (G-CNN) &#22312;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;PDE-G-CNN&#26694;&#26550;&#26159;&#23545;G-CNN&#30340;&#25512;&#24191;&#65292;&#20854;&#20027;&#35201;&#20248;&#28857;&#26159;&#21516;&#26102;&#38477;&#20302;&#32593;&#32476;&#22797;&#26434;&#24615;&#12289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#21644;&#25552;&#20379;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#21457;&#29616;&#20808;&#21069;&#25512;&#33616;&#30340;&#36817;&#20284;&#24418;&#24577;&#23398;&#26680;&#19981;&#24635;&#26159;&#20934;&#30830;&#30340;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#21462;&#20915;&#20110;&#40654;&#26364;&#24230;&#37327;&#30340;&#31354;&#38388;&#21508;&#21521;&#24322;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#24517;&#39035;&#20351;&#29992;&#23376;&#40654;&#26364;&#36924;&#36817;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#26032;&#30340;&#36817;&#20284;&#26680;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group equivariant convolutional neural networks (G-CNNs) have been successfully applied in geometric deep learning. Typically, G-CNNs have the advantage over CNNs that they do not waste network capacity on training symmetries that should have been hard-coded in the network. The recently introduced framework of PDE-based G-CNNs (PDE-G-CNNs) generalises G-CNNs. PDE-G-CNNs have the core advantages that they simultaneously 1) reduce network complexity, 2) increase classification performance, and 3) provide geometric interpretability. Their implementations primarily consist of linear and morphological convolutions with kernels.  In this paper we show that the previously suggested approximative morphological kernels do not always accurately approximate the exact kernels accurately. More specifically, depending on the spatial anisotropy of the Riemannian metric, we argue that one must resort to sub-Riemannian approximations. We solve this problem by providing a new approximative kernel that w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21387;&#32553;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.06116</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Patching Weak Convolutional Neural Network Models through Modularization and Composition. (arXiv:2209.06116v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21387;&#32553;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#26469;&#25913;&#36827;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#27169;&#22359;&#21270;&#26041;&#27861;CNNSplitter&#65292;&#23427;&#23558;&#20855;&#26377;$N$&#31867;&#20998;&#31867;&#20219;&#21153;&#30340;&#24378;CNN&#27169;&#22411;&#20998;&#35299;&#20026;$N$&#20010;&#36739;&#23567;&#30340;CNN&#27169;&#22359;&#12290;&#27599;&#20010;&#27169;&#22359;&#26159;&#19968;&#20010;&#23376;&#27169;&#22411;&#65292;&#21253;&#21547;&#24378;&#27169;&#22411;&#30340;&#37096;&#20998;&#21367;&#31215;&#26680;&#12290;&#20026;&#20102;&#20462;&#34917;&#22312;&#30446;&#26631;&#31867;&#21035;&#65288;TC&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#24369;CNN&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20174;&#24378;CNN&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#30456;&#24212;&#27169;&#22359;&#30456;&#32467;&#21512;&#12290;&#36825;&#26679;&#65292;&#24369;CNN&#27169;&#22411;&#35782;&#21035;TC&#30340;&#33021;&#21147;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite great success in many applications, deep neural networks are not always robust in practice. For instance, a convolutional neuron network (CNN) model for classification tasks often performs unsatisfactorily in classifying some particular classes of objects. In this work, we are concerned with patching the weak part of a CNN model instead of improving it through the costly retraining of the entire model. Inspired by the fundamental concepts of modularization and composition in software engineering, we propose a compressed modularization approach, CNNSplitter, which decomposes a strong CNN model for $N$-class classification into $N$ smaller CNN modules. Each module is a sub-model containing a part of the convolution kernels of the strong model. To patch a weak CNN model that performs unsatisfactorily on a target class (TC), we compose the weak CNN model with the corresponding module obtained from a strong CNN model. The ability of the weak CNN model to recognize the TC can thus be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2209.05732</link><description>&lt;p&gt;
R\'{e}nyi&#25955;&#24230;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#8212;&#8212;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#65288;DML&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#36825;&#31181;&#20570;&#27861;&#26356;&#21152;&#28789;&#27963;&#12289;&#21487;&#35843;&#65292;&#20197;&#25913;&#21892;vanilla DML&#12290;&#36825;&#31181;&#20462;&#25913;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#38468;&#21152;&#22797;&#26434;&#24615;&#19979;&#19981;&#26029;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#33539;&#20363;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#19988;&#34920;&#26126;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#38750;&#20984;&#20248;&#21270;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#25910;&#25947;&#30340;&#20559;&#24046;&#20026;$\mathcal{O}(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32447;&#24615;&#21644;&#20840;&#32416;&#32544;&#30005;&#36335;&#20316;&#20026;&#36229;&#21442;&#25968;&#30340;&#37327;&#23376;&#26680;&#20989;&#25968;&#65292;&#22312;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#20013;&#21152;&#24378;&#20102;&#32416;&#32544;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#20840;&#32416;&#32544;&#30005;&#36335;&#22312;&#22823;&#22810;&#25968;&#29305;&#24449;&#19978;&#20248;&#20110;&#20854;&#20182;&#20840;&#32416;&#32544;&#25110;&#32447;&#24615;&#32416;&#32544;&#30005;&#36335;&#20197;&#21450;&#32463;&#20856;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.05142</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#24378;&#37327;&#23376;&#26680;&#20989;&#25968;&#30340;&#25928;&#29575;&#26469;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#32416;&#32544;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The role of entanglement for enhancing the efficiency of quantum kernels towards classification. (arXiv:2209.05142v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32447;&#24615;&#21644;&#20840;&#32416;&#32544;&#30005;&#36335;&#20316;&#20026;&#36229;&#21442;&#25968;&#30340;&#37327;&#23376;&#26680;&#20989;&#25968;&#65292;&#22312;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#20013;&#21152;&#24378;&#20102;&#32416;&#32544;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#20840;&#32416;&#32544;&#30005;&#36335;&#22312;&#22823;&#22810;&#25968;&#29305;&#24449;&#19978;&#20248;&#20110;&#20854;&#20182;&#20840;&#32416;&#32544;&#25110;&#32447;&#24615;&#32416;&#32544;&#30005;&#36335;&#20197;&#21450;&#32463;&#20856;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26680;&#20989;&#25968;&#34987;&#35748;&#20026;&#26159;&#23637;&#31034;&#37327;&#23376;&#35745;&#31639;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20248;&#21183;&#30340;&#28508;&#22312;&#36164;&#28304;&#12290;&#32771;&#34385;&#21040;&#36229;&#21442;&#25968;&#23545;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22522;&#20110;&#37327;&#23376;&#26680;&#30340;&#26041;&#27861;&#35782;&#21035;&#26377;&#21069;&#36884;&#30340;&#36229;&#21442;&#25968;&#20197;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32447;&#24615;&#21644;&#20840;&#32416;&#32544;&#30005;&#36335;&#30340;&#37327;&#23376;&#26680;&#65292;&#20316;&#20026;&#25511;&#21046;&#21333;&#35789;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#20998;&#26512;&#21644;&#20998;&#31867;&#25991;&#26412;&#25968;&#25454;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#32447;&#24615;&#21644;&#20840;&#32416;&#32544;&#30005;&#36335;&#36827;&#19968;&#27493;&#25511;&#21046;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QSVM&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30005;&#36335;&#19982;&#20854;&#20182;&#37327;&#23376;&#30005;&#36335;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20840;&#32416;&#32544;&#30005;&#36335;&#22312;&#22823;&#22810;&#25968;&#29305;&#24449;&#19978;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#20840;&#32416;&#32544;&#25110;&#32447;&#24615;&#32416;&#32544;&#30005;&#36335;&#20197;&#21450;&#32463;&#20856;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum kernels are considered as potential resources to illustrate benefits of quantum computing in machine learning. Considering the impact of hyperparameters on the performance of a classical machine learning model, it is imperative to identify promising hyperparameters using quantum kernel methods in order to achieve quantum advantages. In this work, we analyse and classify sentiments of textual data using a new quantum kernel based on linear and full entangled circuits as hyperparameters for controlling the correlation among words. We also find that the use of linear and full entanglement further controls the expressivity of the Quantum Support Vector Machine (QSVM). In addition, we also compare the efficiency of the proposed circuit with other quantum circuits and classical machine learning algorithms. Our results show that the proposed fully entangled circuit outperforms all other fully or linearly entangled circuits in addition to classical algorithms for most of the features. 
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#26159;&#35270;&#35273;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#20854;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#30340;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#27604;&#36739;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.04747</link><description>&lt;p&gt;
&#35270;&#35273;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models in Vision: A Survey. (arXiv:2209.04747v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04747
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#35270;&#35273;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#20854;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#30340;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#27604;&#36739;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#23637;&#29616;&#20102;&#22312;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#38750;&#20961;&#30340;&#32467;&#26524;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21069;&#21521;&#25193;&#25955;&#21644;&#21453;&#21521;&#25193;&#25955;&#12290;&#22312;&#21069;&#21521;&#25193;&#25955;&#38454;&#27573;&#65292;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#36880;&#28176;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#21453;&#21521;&#38454;&#27573;&#65292;&#27169;&#22411;&#34987;&#20219;&#21153;&#20026;&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#36870;&#36716;&#25193;&#25955;&#36807;&#31243;&#65292;&#36880;&#27493;&#24674;&#22797;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#36739;&#22823;&#65292;&#21363;&#30001;&#20110;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#28041;&#21450;&#30340;&#27493;&#39588;&#25968;&#37327;&#36739;&#22810;&#23548;&#33268;&#30340;&#36895;&#24230;&#36739;&#24930;&#65292;&#20294;&#20854;&#25152;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20173;&#28982;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#22312;&#26412;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#35273;&#20013;&#24212;&#29992;&#30340;&#32508;&#21512;&#24615;&#35780;&#35770;&#65292;&#21253;&#25324;&#35813;&#39046;&#22495;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#65306;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#25193;&#25955;&#27169;&#22411;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#65292;&#22914;&#20351;&#29992; L&#233;vy &#36807;&#31243;&#12289;&#19981;&#21516;&#24418;&#24335;&#30340;&#22122;&#22768;&#12289;&#27169;&#22411;&#26465;&#20214;&#21644;&#27491;&#21017;&#21270;&#12289;&#22810;&#23610;&#24230;&#26550;&#26500;&#21644;&#24182;&#34892;&#21270;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#24182;&#27604;&#36739;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modelin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#32422;&#26463;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#28508;&#22312;&#34920;&#31034;&#36981;&#24490;&#29305;&#23450;&#21160;&#24577;&#35268;&#24459;&#20197;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24847;&#20041;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;DNA&#33639;&#20809;&#30005;&#24433;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#65292;&#34920;&#26126;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#23398;&#20064;&#21160;&#24577;&#35268;&#24459;&#65292;&#24182;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2209.00905</link><description>&lt;p&gt;
&#20174;&#28508;&#22312;&#21160;&#21147;&#23398;&#21040;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
From latent dynamics to meaningful representations. (arXiv:2209.00905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#32422;&#26463;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#28508;&#22312;&#34920;&#31034;&#36981;&#24490;&#29305;&#23450;&#21160;&#24577;&#35268;&#24459;&#20197;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24847;&#20041;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;DNA&#33639;&#20809;&#30005;&#24433;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#65292;&#34920;&#26126;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#23398;&#20064;&#21160;&#24577;&#35268;&#24459;&#65292;&#24182;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#23835;&#36215;&#30340;&#26680;&#24515;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#26159;&#20351;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24847;&#20041;&#12290;&#20026;&#27492;&#65292;&#20856;&#22411;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20808;&#39564;&#27010;&#29575;&#20998;&#24067;&#26469;&#35268;&#33539;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20808;&#39564;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#25110;&#20020;&#26102;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#32422;&#26463;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#27010;&#29575;&#65292;&#32780;&#26159;&#38480;&#21046;&#28508;&#22312;&#34920;&#31034;&#36981;&#24490;&#29305;&#23450;&#30340;&#21160;&#24577;&#35268;&#24459;&#65292;&#36825;&#26159;&#21160;&#24577;&#31995;&#32479;&#34920;&#31034;&#23398;&#20064;&#26356;&#33258;&#28982;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#20449;&#20208;&#28304;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#35266;&#23519;&#65292;&#21363;&#34429;&#28982;&#19981;&#21516;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#19981;&#21516;&#30340;&#36793;&#38469;&#27010;&#29575;&#20998;&#24067;&#65292;&#20294;&#36890;&#24120;&#36981;&#24490;&#30456;&#21516;&#30340;&#21160;&#24577;&#35268;&#24459;&#65292;&#20363;&#22914;&#29275;&#39039;&#21644;&#34203;&#23450;&#35860;&#26041;&#31243;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#31995;&#32479;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#33639;&#20809;DNA&#30005;&#24433;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#23398;&#20064;&#21160;&#24577;&#35268;&#24459;&#65292;&#24182;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
While representation learning has been central to the rise of machine learning and artificial intelligence, a key problem remains in making the learnt representations meaningful. For this the typical approach is to regularize the learned representation through prior probability distributions. However such priors are usually unavailable or ad hoc. To deal with this, we propose a dynamics-constrained representation learning framework. Instead of using predefined probabilities, we restrict the latent representation to follow specific dynamics, which is a more natural constraint for representation learning in dynamical systems. Our belief stems from a fundamental observation in physics that though different systems can have different marginalized probability distributions, they typically obey the same dynamics, such as Newton's and Schrodinger's equations. We validate our framework for different systems including a real-world fluorescent DNA movie dataset. We show that our algorithm can un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35299;&#20915;&#24179;&#28369;&#65288;&#24378;&#65289;&#21333;&#35843;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.13592</link><description>&lt;p&gt;
&#24179;&#28369;&#21333;&#35843;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#19982;&#38797;&#28857;&#38382;&#39064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Smooth Monotone Stochastic Variational Inequalities and Saddle Point Problems: A Survey. (arXiv:2208.13592v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35299;&#20915;&#24179;&#28369;&#65288;&#24378;&#65289;&#21333;&#35843;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35299;&#20915;&#24179;&#28369;&#65288;&#24378;&#65289;&#21333;&#35843;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#38543;&#26426;&#26041;&#27861;&#26368;&#32456;&#28436;&#21464;&#30340;&#30830;&#23450;&#24615;&#22522;&#30784;&#12290;&#28982;&#21518;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#33324;&#38543;&#26426;&#20844;&#24335;&#30340;&#26041;&#27861;&#65292;&#30475;&#30475;&#26377;&#38480;&#21644;&#35774;&#32622;&#12290;&#26368;&#21518;&#37096;&#20998;&#26159;&#33268;&#21147;&#20110;&#21508;&#31181;&#26368;&#36817;&#30340;&#65288;&#19981;&#19968;&#23450;&#26159;&#38543;&#26426;&#30340;&#65289;&#31639;&#27861;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#36827;&#27493;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is a survey of methods for solving smooth (strongly) monotone stochastic variational inequalities. To begin with, we give the deterministic foundation from which the stochastic methods eventually evolved. Then we review methods for the general stochastic formulation, and look at the finite sum setup. The last parts of the paper are devoted to various recent (not necessarily stochastic) advances in algorithms for variational inequalities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#23494;&#35745;&#31639;&#30340;&#32467;&#21512;&#65292;&#24182;&#26803;&#29702;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20445;&#35777;&#26426;&#23494;&#24615;&#21644;&#23436;&#25972;&#24615;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#39640;&#32423;&#29305;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#29616;&#26377;&#30340;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#31995;&#32479;&#22312;&#26426;&#22120;&#23398;&#20064;&#29992;&#20363;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;</title><link>http://arxiv.org/abs/2208.10134</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#26426;&#23494;&#35745;&#31639;&#65306;&#30693;&#35782;&#31995;&#32479;&#21270;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with Confidential Computing: A Systematization of Knowledge. (arXiv:2208.10134v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#23494;&#35745;&#31639;&#30340;&#32467;&#21512;&#65292;&#24182;&#26803;&#29702;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20445;&#35777;&#26426;&#23494;&#24615;&#21644;&#23436;&#25972;&#24615;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#39640;&#32423;&#29305;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#29616;&#26377;&#30340;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#31995;&#32479;&#22312;&#26426;&#22120;&#23398;&#20064;&#29992;&#20363;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#21457;&#23637;&#21644;&#25915;&#20987;&#38754;&#30340;&#25193;&#22823;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#26085;&#30410;&#20005;&#37325;&#12290;&#20316;&#20026;&#19968;&#31181;&#25104;&#29087;&#30340;&#31995;&#32479;&#32423;&#26041;&#27861;&#65292;&#26426;&#23494;&#35745;&#31639;&#24050;&#34987;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#29992;&#20110;&#32531;&#35299;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#23494;&#35745;&#31639;&#20043;&#38388;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#26803;&#29702;&#20102;&#20808;&#21069;&#22522;&#20110;&#26426;&#23494;&#35745;&#31639;&#36741;&#21161;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;i&#65289;&#26426;&#23494;&#24615;&#20445;&#35777;&#21644;ii&#65289;&#23436;&#25972;&#24615;&#20445;&#35777;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#39640;&#32423;&#29305;&#24615;&#21644;&#32570;&#38519;&#12290;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#31995;&#32479;&#22312;&#26426;&#22120;&#23398;&#20064;&#29992;&#20363;&#20013;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#19987;&#38376;&#30340;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#23637;&#26395;&#24615;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#38381;&#29615;&#20445;&#25252;&#30340;&#22522;&#20110;&#22320;&#38754;&#30340;&#38544;&#31169;&#23450;&#20041;&#65292;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#25191;&#34892;&#65292;&#19987;&#38376;&#30340;TEE&#36741;&#21161;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy and security challenges in Machine Learning (ML) have become increasingly severe, along with ML's pervasive development and the recent demonstration of large attack surfaces. As a mature system-oriented approach, Confidential Computing has been utilized in both academia and industry to mitigate privacy and security issues in various ML scenarios. In this paper, the conjunction between ML and Confidential Computing is investigated. We systematize the prior work on Confidential Computing-assisted ML techniques that provide i) confidentiality guarantees and ii) integrity assurances, and discuss their advanced features and drawbacks. Key challenges are further identified, and we provide dedicated analyses of the limitations in existing Trusted Execution Environment (TEE) systems for ML use cases. Finally, prospective works are discussed, including grounded privacy definitions for closed-loop protection, partitioned executions of efficient ML, dedicated TEE-assisted designs for ML, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CycleDance&#30340;&#33310;&#36424;&#39118;&#26684;&#36716;&#25442;&#31995;&#32479;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#35838;&#31243;&#23398;&#20064;&#21644;&#26032;&#25351;&#26631;&#23545;&#33310;&#36424;&#36816;&#21160;&#36827;&#34892;&#36716;&#31227;&#21644;&#21512;&#25104; &#65292;&#33021;&#22815;&#23454;&#29616;&#36924;&#30495;&#30340;&#33310;&#36424;&#39118;&#26684;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2208.09406</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#21464;&#25442;&#22120;&#36827;&#34892;&#33310;&#36424;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Dance Style Transfer with Cross-modal Transformer. (arXiv:2208.09406v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CycleDance&#30340;&#33310;&#36424;&#39118;&#26684;&#36716;&#25442;&#31995;&#32479;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#35838;&#31243;&#23398;&#20064;&#21644;&#26032;&#25351;&#26631;&#23545;&#33310;&#36424;&#36816;&#21160;&#36827;&#34892;&#36716;&#31227;&#21644;&#21512;&#25104; &#65292;&#33021;&#22815;&#23454;&#29616;&#36924;&#30495;&#30340;&#33310;&#36424;&#39118;&#26684;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CycleDance&#30340;&#33310;&#36424;&#39118;&#26684;&#36716;&#25442;&#31995;&#32479;&#65292;&#21487;&#23558;&#19968;&#31181;&#33310;&#36424;&#39118;&#26684;&#20013;&#30340;&#21160;&#20316;&#36716;&#25442;&#20026;&#21478;&#19968;&#31181;&#33310;&#36424;&#39118;&#26684;&#20013;&#30340;&#21160;&#20316;&#65292;&#24182;&#23581;&#35797;&#20445;&#30041;&#33310;&#36424;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;CycleGAN&#26550;&#26500;&#65292;&#29992;&#20110;&#24314;&#27169;&#38899;&#39057;&#24207;&#21015;&#65292;&#24182;&#38598;&#25104;&#20102;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#26469;&#32771;&#34385;&#38899;&#20048;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#35838;&#31243;&#23398;&#20064;&#26469;&#31283;&#23450;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21160;&#24577;&#24103;&#20043;&#38388;&#20016;&#23500;&#32780;&#38271;&#26399;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#36825;&#26159;&#21160;&#24577;&#36716;&#31227;&#21644;&#21512;&#25104;&#24037;&#20316;&#20013;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#22312;&#33310;&#36424;&#36816;&#21160;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#35780;&#20272;&#36716;&#31227;&#24378;&#24230;&#21644;&#20869;&#23481;&#20445;&#30041;&#30340;&#26032;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21066;&#20943;&#30740;&#31350;&#21644;&#20154;&#31867;&#30740;&#31350;&#65292;&#21253;&#25324;30&#21517;&#20855;&#26377;5&#24180;&#25110;&#26356;&#22810;&#33310;&#36424;&#32463;&#39564;&#30340;&#21442;&#19982;&#32773;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CycleDance&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#30446;&#26631;&#39118;&#26684;&#30340;&#36924;&#30495;&#21160;&#20316;&#65292;&#20854;&#33258;&#28982;&#31243;&#24230;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;CycleGAN&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CycleDance, a dance style transfer system to transform an existing motion clip in one dance style to a motion clip in another dance style while attempting to preserve motion context of the dance. Our method extends an existing CycleGAN architecture for modeling audio sequences and integrates multimodal transformer encoders to account for music context. We adopt sequence length-based curriculum learning to stabilize training. Our approach captures rich and long-term intra-relations between motion frames, which is a common challenge in motion transfer and synthesis work. We further introduce new metrics for gauging transfer strength and content preservation in the context of dance movements. We perform an extensive ablation study as well as a human study including 30 participants with 5 or more years of dance experience. The results demonstrate that CycleDance generates realistic movements with the target style, significantly outperforming the baseline CycleGAN on naturalness,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2208.07316</link><description>&lt;p&gt;
MENLI: &#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#34987;&#25552;&#20986;&#30340;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26131;&#21463;&#21040;&#23545;&#20449;&#24687;&#27491;&#30830;&#24615;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#27492;&#31867;&#27169;&#22411;&#26159;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36825;&#31181;&#25351;&#26631;&#26356;&#36866;&#21512;&#24314;&#27169;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#27604;&#26368;&#36817;&#30340;BERT&#22522;&#30784;&#25351;&#26631;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#20302;&#20110;SOTA MT&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#25351;&#26631;&#19982;&#25105;&#20204;&#30340;NLI&#25351;&#26631;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#26082;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65288;15&#65285;-30&#65285;&#65289;&#65292;&#21448;&#33719;&#24471;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#26356;&#39640;&#30340;&#36136;&#37327;&#25351;&#26631;&#65288;+5&#65285;&#33267;30&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20248;&#21270;&#30340;&#20449;&#24687;&#20016;&#23500;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#21033;&#29992;&#38750;&#24179;&#31283;&#24615;&#26469;&#32534;&#30721;&#23545;&#25628;&#32034;&#31354;&#38388;&#20013;&#26576;&#20123;&#21306;&#22495;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#20419;&#36827;&#23616;&#37096;&#25506;&#32034;&#65292;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2208.02704</link><description>&lt;p&gt;
&#24102;&#20449;&#24687;&#21327;&#26041;&#24046;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Informative Covariance. (arXiv:2208.02704v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02704
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20248;&#21270;&#30340;&#20449;&#24687;&#20016;&#23500;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#21033;&#29992;&#38750;&#24179;&#31283;&#24615;&#26469;&#32534;&#30721;&#23545;&#25628;&#32034;&#31354;&#38388;&#20013;&#26576;&#20123;&#21306;&#22495;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#20419;&#36827;&#23616;&#37096;&#25506;&#32034;&#65292;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#22788;&#29702;&#26410;&#30693;&#21644;&#26114;&#36149;&#30446;&#26631;&#30340;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#23558;&#19968;&#20010;&#25311;&#21512;&#36125;&#21494;&#26031;&#22238;&#24402;&#27169;&#22411;&#19982;&#19968;&#20010;&#25910;&#33719;&#20989;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20915;&#23450;&#22312;&#21738;&#37324;&#35780;&#20272;&#30446;&#26631;&#12290;&#20856;&#22411;&#30340;&#22238;&#24402;&#27169;&#22411;&#30001;&#20855;&#26377;&#24179;&#31283;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20989;&#25968;&#26080;&#27861;&#34920;&#36798;&#36755;&#20837;&#30456;&#20851;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#21253;&#25324;&#26368;&#20248;&#28857;&#21487;&#33021;&#20986;&#29616;&#30340;&#20301;&#32622;&#12290;&#24179;&#31283;&#27169;&#22411;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#36890;&#36807;&#20449;&#24687;&#20016;&#23500;&#30340;&#22343;&#20540;&#20989;&#25968;&#21033;&#29992;&#20808;&#39564;&#20449;&#24687;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20248;&#21270;&#30340;&#20449;&#24687;&#20016;&#23500;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#21033;&#29992;&#38750;&#24179;&#31283;&#24615;&#26469;&#32534;&#30721;&#23545;&#25628;&#32034;&#31354;&#38388;&#20013;&#26576;&#20123;&#21306;&#22495;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#20419;&#36827;&#23616;&#37096;&#25506;&#32034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22522;&#20934;&#38382;&#39064;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a methodology for global optimization of unknown and expensive objectives. It combines a surrogate Bayesian regression model with an acquisition function to decide where to evaluate the objective. Typical regression models are given by Gaussian processes with stationary covariance functions. However, these functions are unable to express prior input-dependent information, including possible locations of the optimum. The ubiquity of stationary models has led to the common practice of exploiting prior information via informative mean functions. In this paper, we highlight that these models can perform poorly, especially in high dimensions. We propose novel informative covariance functions for optimization, leveraging nonstationarity to encode preferences for certain regions of the search space and adaptively promote local exploration during optimization. We demonstrate that the proposed functions can increase the sample efficiency of Bayesian optimization in high
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#20854;&#20013;DH-KG&#21253;&#21547;&#36229;&#20851;&#31995;&#23454;&#20363;&#35270;&#22270;&#21644;&#20174;&#23454;&#20307;&#23618;&#27425;&#25277;&#35937;&#20986;&#30340;&#36229;&#20851;&#31995;&#26412;&#20307;&#35270;&#22270;&#65292;&#23450;&#20041;&#20102;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;DH-KG&#25968;&#25454;&#38598;JW44K-6K&#21644;HTDM&#12290;DHGE&#26159;&#19968;&#20010;&#22522;&#20110;GRAN&#32534;&#30721;&#22120;&#12289;HGNN&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;DH-KG&#23884;&#20837;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.08562</link><description>&lt;p&gt;
DHGE&#65306;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#20854;&#20013;DH-KG&#21253;&#21547;&#36229;&#20851;&#31995;&#23454;&#20363;&#35270;&#22270;&#21644;&#20174;&#23454;&#20307;&#23618;&#27425;&#25277;&#35937;&#20986;&#30340;&#36229;&#20851;&#31995;&#26412;&#20307;&#35270;&#22270;&#65292;&#23450;&#20041;&#20102;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;DH-KG&#25968;&#25454;&#38598;JW44K-6K&#21644;HTDM&#12290;DHGE&#26159;&#19968;&#20010;&#22522;&#20110;GRAN&#32534;&#30721;&#22120;&#12289;HGNN&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;DH-KG&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#36229;&#20851;&#31995;&#20107;&#23454;&#30001;&#19968;&#20010;&#20027;&#19977;&#20803;&#32452;&#21644;&#20960;&#20010;&#36741;&#21161;&#30340;&#23646;&#24615;-&#20540;&#25551;&#36848;&#32452;&#25104;&#65292;&#34987;&#35748;&#20026;&#27604;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#20107;&#23454;&#26356;&#20840;&#38754;&#21644;&#20855;&#20307;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21333;&#35270;&#22270;&#30340;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#24369;&#21270;&#20102;&#34920;&#31034;&#23454;&#20307;&#20043;&#38388;&#20146;&#23646;&#20851;&#31995;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#20851;&#31995;&#23454;&#20363;&#35270;&#22270;&#21644;&#20174;&#23454;&#20307;&#23618;&#27425;&#25277;&#35937;&#20986;&#30340;&#36229;&#20851;&#31995;&#26412;&#20307;&#35270;&#22270;&#30340;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#65288;DH-KG&#65289;&#12290;&#26412;&#25991;&#39318;&#27425;&#22312;DH-KG&#19978;&#23450;&#20041;&#20102;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;DH-KG&#25968;&#25454;&#38598;&#65292;JW44K-6K&#65292;&#20174;&#32500;&#22522;&#25968;&#25454;&#20013;&#25552;&#21462;&#65292;&#21644;&#22522;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;HTDM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DHGE&#65292;&#19968;&#20010;&#22522;&#20110;GRAN&#32534;&#30721;&#22120;&#12289;HGNN&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;DH-KG&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of representation learning on knowledge graphs (KGs), a hyper-relational fact consists of a main triple and several auxiliary attribute-value descriptions, which is considered more comprehensive and specific than a triple-based fact. However, currently available hyper-relational KG embedding methods in a single view are limited in application because they weaken the hierarchical structure that represents the affiliation between entities. To overcome this limitation, we propose a dual-view hyper-relational KG structure (DH-KG) that contains a hyper-relational instance view for entities and a hyper-relational ontology view for concepts that are abstracted hierarchically from the entities. This paper defines link prediction and entity typing tasks on DH-KG for the first time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding model based on GRAN encoders, HGNNs, and joint learnin
&lt;/p&gt;</description></item><item><title>AnoShift&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#30340;&#33021;&#21147;&#12290;&#20854;&#20351;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#21464;&#21270;&#20132;&#36890;&#25968;&#25454;&#38598;&#30340;&#38750;&#24179;&#31283;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;AnoShift&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31227;&#24773;&#20917;&#19979;&#26816;&#27979;&#24322;&#24120;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.15476</link><description>&lt;p&gt;
AnoShift: &#19968;&#20010;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection. (arXiv:2206.15476v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15476
&lt;/p&gt;
&lt;p&gt;
AnoShift&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#30340;&#33021;&#21147;&#12290;&#20854;&#20351;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#21464;&#21270;&#20132;&#36890;&#25968;&#25454;&#38598;&#30340;&#38750;&#24179;&#31283;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;AnoShift&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31227;&#24773;&#20917;&#19979;&#26816;&#27979;&#24322;&#24120;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#20998;&#26512;&#26159;&#24403;&#20170;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#22686;&#38271;&#26041;&#21521;&#65292;&#25512;&#21160;&#30528;&#26032;&#20852;&#30340;&#22522;&#20934;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#22522;&#20934;&#20391;&#37325;&#20110;&#25552;&#20379;&#36866;&#21512;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#30340;&#22330;&#26223;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20391;&#37325;&#20110;&#30417;&#30563;&#23398;&#20064;&#65292;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#36824;&#27809;&#26377;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#20934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#25968;&#25454;&#38543;&#26102;&#38388;&#19981;&#26029;&#21464;&#21270;&#65292;&#22522;&#20110;&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#20132;&#36890;&#25968;&#25454;&#38598;&#20140;&#37117;-2006+&#26500;&#24314;&#12290;&#36825;&#31867;&#22411;&#30340;&#25968;&#25454;&#31526;&#21512;&#36755;&#20837;&#20998;&#24067;&#30340;&#21464;&#21270;&#21069;&#25552;&#65306;&#23427;&#35206;&#30422;&#20102;&#24456;&#38271;&#19968;&#27573;&#26102;&#38388;&#65288;10&#24180;&#65289;&#65292;&#38543;&#26102;&#38388;&#33258;&#28982;&#21464;&#21270;&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#20462;&#25913;&#20854;&#34892;&#20026;&#27169;&#24335;&#21644;&#36719;&#20214;&#26356;&#26032;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22522;&#26412;&#30340;&#25353;&#29305;&#24449;&#20998;&#26512;&#12289;t-SNE&#21644;&#26368;&#20248;&#36816;&#36755;&#26041;&#27861;&#26469;&#31361;&#26174;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#27979;&#37327;&#24180;&#20221;&#20043;&#38388;&#25972;&#20307;&#20998;&#24067;&#36317;&#31163;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;AnoShift&#65292;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#32452;&#25351;&#26631;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#20197;&#35780;&#20272;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;AnoShift&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AnoShift&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31227;&#24773;&#20917;&#19979;&#26816;&#27979;&#24322;&#24120;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning (ML), leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning, and to the best of our knowledge, there is none for unsupervised learning. Therefore, we introduce an unsupervised anomaly detection benchmark with data that shifts over time, built over Kyoto-2006+, a traffic dataset for network intrusion detection. This type of data meets the premise of shifting the input distribution: it covers a large time span ($10$ years), with naturally occurring changes over time (eg users modifying their behavior patterns, and software updates). We first highlight the non-stationary nature of the data, using a basic per-feature analysis, t-SNE, and an Optimal Transport approach for measuring the overall distribution distances between years. Next, we propose AnoS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#36817;&#31471;&#20998;&#35010;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#37319;&#29992;&#25193;&#23637;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#22788;&#29702;&#22823;&#37327;&#32422;&#26463;&#19988;&#20135;&#29983;&#26356;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2206.07179</link><description>&lt;p&gt;
&#36817;&#31471;&#20998;&#35010;&#23545;&#25239;&#25915;&#20987;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Proximal Splitting Adversarial Attacks for Semantic Segmentation. (arXiv:2206.07179v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#36817;&#31471;&#20998;&#35010;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#37319;&#29992;&#25193;&#23637;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#22788;&#29702;&#22823;&#37327;&#32422;&#26463;&#19988;&#20135;&#29983;&#26356;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20998;&#31867;&#19978;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#26356;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65288;&#22914;&#35821;&#20041;&#20998;&#21106;&#65289;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#24037;&#20316;&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#33021;&#20934;&#30830;&#22320;&#35299;&#20915;&#23545;&#25239;&#24615;&#20998;&#21106;&#38382;&#39064;&#65292;&#22240;&#27492;&#39640;&#20272;&#20102;&#27450;&#39575;&#27169;&#22411;&#25152;&#38656;&#30340;&#27745;&#26579;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#36817;&#31471;&#20998;&#35010;&#30340;&#30333;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#26356;&#23567;$\ell_\infty$&#33539;&#25968;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#20316;&#32773;&#30340;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#25193;&#23637;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#22788;&#29702;&#22823;&#37327;&#30340;&#32422;&#26463;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212;&#32422;&#26463;&#32553;&#25918;&#21644;&#23631;&#34109;&#31574;&#30053;&#12290;&#20316;&#32773;&#35777;&#26126;&#20102;&#20182;&#20204;&#30340;&#25915;&#20987;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#20197;&#21450;&#20998;&#31867;&#25915;&#20987;&#65288;&#20316;&#32773;&#20026;&#20102;&#20998;&#21106;&#32780;&#25913;&#36827;&#65289;&#65292;&#20026;&#36825;&#39033;&#23494;&#38598;&#20219;&#21153;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification has been the focal point of research on adversarial attacks, but only a few works investigate methods suited to denser prediction tasks, such as semantic segmentation. The methods proposed in these works do not accurately solve the adversarial segmentation problem and, therefore, overestimate the size of the perturbations required to fool models. Here, we propose a white-box attack for these models based on a proximal splitting to produce adversarial perturbations with much smaller $\ell_\infty$ norms. Our attack can handle large numbers of constraints within a nonconvex minimization framework via an Augmented Lagrangian approach, coupled with adaptive constraint scaling and masking strategies. We demonstrate that our attack significantly outperforms previously proposed ones, as well as classification attacks that we adapted for segmentation, providing a first comprehensive benchmark for this dense task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#25552;&#37266;&#26410;&#26469;&#38656;&#35201;&#29702;&#35299;&#27424;&#25311;&#21512;&#21046;&#24230;&#19979;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2206.00501</link><description>&lt;p&gt;
&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65306;&#26356;&#22823;&#27169;&#22411;&#30340;&#21457;&#29616;&#21487;&#35777;&#26126;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models. (arXiv:2206.00501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#25552;&#37266;&#26410;&#26469;&#38656;&#35201;&#29702;&#35299;&#27424;&#25311;&#21512;&#21046;&#24230;&#19979;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#30740;&#31350;&#20026;&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#25311;&#21512;&#26159;&#21542;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#30495;&#30340;&#26159;&#33391;&#24615;&#30340;&#12290;&#25105;&#20204;&#24320;&#22987;&#35266;&#23519;&#21040;&#19968;&#20010; ResNet &#27169;&#22411;&#22312; Cifar10 &#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312; ImageNet &#19978;&#21017;&#19981;&#33391;&#12290;&#20026;&#20102;&#20102;&#35299;&#20026;&#20160;&#20040;&#33391;&#24615;&#36807;&#25311;&#21512;&#22312; ImageNet &#23454;&#39564;&#20013;&#22833;&#36133;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#27604;&#25968;&#25454;&#28857;&#25968;&#37327;&#19981;&#26126;&#26174;&#22823;&#30340;&#38480;&#23450;&#26465;&#20214;&#19979;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#20010;&#36731;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#20102;&#19968;&#20010;&#30456;&#21464;&#65306;&#19982;&#20043;&#21069;&#30340;&#37325;&#36229;&#21442;&#25968;&#21270;&#35774;&#32622;&#19981;&#21516;&#65292;&#24403;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#26102;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#22312;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35299;&#37322;&#20102;&#25105;&#20204;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#24182;&#36890;&#36807;&#19968;&#32452; ResNet &#30340;&#25511;&#21046;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#29702;&#35299;&#27424;&#25311;&#21512;&#21046;&#24230;&#19979;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#65292;&#20316;&#20026;&#26410;&#26469;&#30340;&#19968;&#20010;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies on benign overfitting provide insights for the success of overparameterized deep learning models. In this work, we examine whether overfitting is truly benign in real-world classification tasks. We start with the observation that a ResNet model overfits benignly on Cifar10 but not benignly on ImageNet. To understand why benign overfitting fails in the ImageNet experiment, we theoretically analyze benign overfitting under a more restrictive setup where the number of parameters is not significantly larger than the number of data points. Under this mild overparameterization setup, our analysis identifies a phase change: unlike in the previous heavy overparameterization settings, benign overfitting can now fail in the presence of label noise. Our analysis explains our empirical observations, and is validated by a set of control experiments with ResNets. Our work highlights the importance of understanding implicit bias in underfitting regimes as a future direction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24635;&#32467;&#34913;&#37327;&#26631;&#20934;&#8212;&#8212;&#24179;&#22343;&#35843;&#25972;&#20851;&#32852;&#24230;&#65288;AAA&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#19968;&#20010;&#20855;&#26377;&#28151;&#28102;&#24433;&#21709;&#30340;&#24322;&#36136;&#32676;&#20307;&#20013;&#30340;&#20851;&#32852;&#31243;&#24230;&#12290;&#24182;&#19988;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#37319;&#26679;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2205.14048</link><description>&lt;p&gt;
&#24179;&#22343;&#35843;&#25972;&#20851;&#32852;&#24230;&#65306;&#39640;&#32500;&#28151;&#28102;&#22240;&#32032;&#30340;&#39640;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Average Adjusted Association: Efficient Estimation with High Dimensional Confounders. (arXiv:2205.14048v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24635;&#32467;&#34913;&#37327;&#26631;&#20934;&#8212;&#8212;&#24179;&#22343;&#35843;&#25972;&#20851;&#32852;&#24230;&#65288;AAA&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#19968;&#20010;&#20855;&#26377;&#28151;&#28102;&#24433;&#21709;&#30340;&#24322;&#36136;&#32676;&#20307;&#20013;&#30340;&#20851;&#32852;&#31243;&#24230;&#12290;&#24182;&#19988;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#37319;&#26679;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25968;&#27604;&#29575;&#26159;&#34913;&#37327;&#20108;&#20803;&#32467;&#26524;&#21644;&#26292;&#38706;&#21464;&#37327;&#20851;&#32852;&#24230;&#30340;&#19968;&#31181;&#26082;&#23450;&#25351;&#26631;&#12290;&#23613;&#31649;&#20854;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#26377;&#20851;&#22914;&#20309;&#36890;&#36807;&#24179;&#22343;&#26041;&#24335;&#24635;&#32467;&#20855;&#26377;&#28151;&#28102;&#22240;&#32032;&#30340;&#23545;&#25968;&#27604;&#29575;&#30340;&#35752;&#35770;&#21364;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#22343;&#35843;&#25972;&#20851;&#32852;&#24230;&#65288;AAA&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#35843;&#25972;&#20102;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24322;&#36136;&#32676;&#20307;&#20013;&#30340;&#20851;&#32852;&#24230;&#30340;&#24635;&#32467;&#34913;&#37327;&#26631;&#20934;&#12290;&#20026;&#20102;&#26041;&#20415;&#20351;&#29992;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;AAA&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30340;DML&#20272;&#35745;&#22120;&#20351;&#29992;&#20102;&#20004;&#31181;&#31561;&#25928;&#30340;&#26377;&#25928;&#24433;&#21709;&#20989;&#25968;&#24418;&#24335;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#37319;&#26679;&#22330;&#26223;&#65292;&#21253;&#25324;&#38543;&#26426;&#37319;&#26679;&#65292;&#22522;&#20110;&#32467;&#26524;&#30340;&#37319;&#26679;&#21644;&#22522;&#20110;&#26292;&#38706;&#30340;&#37319;&#26679;&#12290;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#21644;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20272;&#31639;&#26041;&#27861;&#22312;&#27979;&#37327;AAA&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The log odds ratio is a well-established metric for evaluating the association between binary outcome and exposure variables. Despite its widespread use, there has been limited discussion on how to summarize the log odds ratio as a function of confounders through averaging. To address this issue, we propose the Average Adjusted Association (AAA), which is a summary measure of association in a heterogeneous population, adjusted for observed confounders. To facilitate the use of it, we also develop efficient double/debiased machine learning (DML) estimators of the AAA. Our DML estimators use two equivalent forms of the efficient influence function, and are applicable in various sampling scenarios, including random sampling, outcome-based sampling, and exposure-based sampling. Through real data and simulations, we demonstrate the practicality and effectiveness of our proposed estimators in measuring the AAA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#38454;&#27573;&#26041;&#27861;&#30340;&#26032;&#22411;&#23792;&#20540;&#39118;&#26292;&#28526;&#39044;&#27979;&#20195;&#29702;&#27169;&#22411;&#65292;&#21487;&#22312;&#20445;&#35777;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#26377;&#26395;&#22312;&#39118;&#38505;&#35780;&#20272;&#21644;&#24212;&#24613;&#31649;&#29702;&#20915;&#31574;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2204.13168</link><description>&lt;p&gt;
&#19968;&#31181;&#28789;&#27963;&#30340;&#23792;&#20540;&#39118;&#26292;&#28526;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Flexible Peak Storm Surge Prediction. (arXiv:2204.13168v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#38454;&#27573;&#26041;&#27861;&#30340;&#26032;&#22411;&#23792;&#20540;&#39118;&#26292;&#28526;&#39044;&#27979;&#20195;&#29702;&#27169;&#22411;&#65292;&#21487;&#22312;&#20445;&#35777;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#26377;&#26395;&#22312;&#39118;&#38505;&#35780;&#20272;&#21644;&#24212;&#24613;&#31649;&#29702;&#20915;&#31574;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26292;&#28526;&#26159;&#27839;&#28023;&#22320;&#21306;&#30340;&#19968;&#31181;&#37325;&#35201;&#33258;&#28982;&#28798;&#23475;&#65292;&#19981;&#20165;&#20250;&#36896;&#25104;&#20005;&#37325;&#30340;&#36130;&#20135;&#25439;&#22833;&#65292;&#20063;&#20250;&#36896;&#25104;&#20154;&#21592;&#20260;&#20129;&#12290;&#38656;&#35201;&#20934;&#30830;&#39640;&#25928;&#30340;&#39118;&#26292;&#28526;&#27169;&#22411;&#26469;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#21644;&#25351;&#23548;&#24212;&#24613;&#31649;&#29702;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#38454;&#27573;&#26041;&#27861;&#30340;&#26032;&#22411;&#23792;&#20540;&#39118;&#26292;&#28526;&#39044;&#27979;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#28857;&#34987;&#20998;&#31867;&#20026;&#28153;&#27809;&#25110;&#38750;&#28153;&#27809;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#39044;&#27979;&#20102;&#28153;&#27809;&#30340;&#32423;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20195;&#29702;&#38382;&#39064;&#20844;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#28857;&#29420;&#31435;&#39044;&#27979;&#39118;&#26292;&#28526;&#12290;&#36825;&#20801;&#35768;&#30452;&#25509;&#23545;&#19981;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20301;&#32622;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30740;&#31350;&#26696;&#20363;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#24314;&#27169;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storm surge is a major natural hazard in coastal regions, responsible both for significant property damage and loss of life. Accurate, efficient models of storm surge are needed both to assess long-term risk and to guide emergency management decisions. While high-fidelity regional- and global-ocean circulation models such as the ADvanced CIRCulation (ADCIRC) model can accurately predict storm surge, they are very computationally expensive. Here we develop a novel surrogate model for peak storm surge prediction based on a multi-stage approach. In the first stage, points are classified as inundated or not. In the second, the level of inundation is predicted . Additionally, we propose a new formulation of the surrogate problem in which storm surge is predicted independently for each point. This allows for predictions to be made directly for locations not present in the training data, and significantly reduces the number of model parameters. We demonstrate our modeling framework on two stu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#33268;&#21147;&#20110;&#39044;&#27979;&#22312;&#35745;&#21010;&#30340;pump&#26102;&#38388;&#20043;&#21069;&#65292;&#21015;&#22312;&#30446;&#26631;&#20132;&#26131;&#25152;&#20013;&#30340;&#25152;&#26377;&#24065;&#30340;pump&#27010;&#29575;&#65292;&#36890;&#36807;&#23545;P&amp;D&#20107;&#20214;&#30340;&#32463;&#39564;&#20998;&#26512;&#21644;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;pump&#30340;&#30828;&#24065;&#21576;&#29616;&#20986;&#20869;&#37096;&#20449;&#36947;&#30340;&#21516;&#36136;&#24615;&#21644;&#36328;&#20449;&#36947;&#30340;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.12929</link><description>&lt;p&gt;
&#22522;&#20110;&#24207;&#21015;&#30340;&#21152;&#23494;&#36135;&#24065;Pump-and-Dump&#30446;&#26631;&#24065;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump. (arXiv:2204.12929v2 [q-fin.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#39044;&#27979;&#22312;&#35745;&#21010;&#30340;pump&#26102;&#38388;&#20043;&#21069;&#65292;&#21015;&#22312;&#30446;&#26631;&#20132;&#26131;&#25152;&#20013;&#30340;&#25152;&#26377;&#24065;&#30340;pump&#27010;&#29575;&#65292;&#36890;&#36807;&#23545;P&amp;D&#20107;&#20214;&#30340;&#32463;&#39564;&#20998;&#26512;&#21644;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;pump&#30340;&#30828;&#24065;&#21576;&#29616;&#20986;&#20869;&#37096;&#20449;&#36947;&#30340;&#21516;&#36136;&#24615;&#21644;&#36328;&#20449;&#36947;&#30340;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#20013;Pump-and-Dump&#35745;&#21010;&#65288;P&#65286;Ds&#65289;&#30340;&#22686;&#22810;&#65292;&#25552;&#21069;&#26816;&#27979;&#27492;&#31867;&#27450;&#35784;&#27963;&#21160;&#20197;&#35686;&#31034;&#28508;&#22312;&#26131;&#21463;&#24433;&#21709;&#30340;&#25237;&#36164;&#32773;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#37325;&#28857;&#39044;&#27979;&#22312;&#35745;&#21010;&#30340;pump&#26102;&#38388;&#20043;&#21069;&#65292;&#21015;&#22312;&#30446;&#26631;&#20132;&#26131;&#25152;&#20013;&#30340;&#25152;&#26377;&#24065;&#30340;pump&#27010;&#29575;&#65292;&#36825;&#34987;&#25105;&#20204;&#31216;&#20026;&#30446;&#26631;&#24065;&#39044;&#27979;&#20219;&#21153;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;Telegram&#20013;&#33258;2019&#24180;1&#26376;&#33267;2022&#24180;1&#26376;&#32452;&#32455;&#30340;&#26368;&#26032;&#30340;709&#27425;P&#65286;D&#20107;&#20214;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#20998;&#26512;&#25581;&#31034;&#20102;P&#65286;Ds&#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#24335;&#65292;&#20363;&#22914;pump&#30340;&#30828;&#24065;&#21576;&#29616;&#20986;&#20869;&#37096;&#20449;&#36947;&#30340;&#21516;&#36136;&#24615;&#21644;&#36328;&#20449;&#36947;&#30340;&#24322;&#36136;&#24615;&#12290;&#36825;&#20010;&#21457;&#29616;&#21551;&#21457;&#25105;&#20204;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24207;&#21015;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;SNN&#65292;&#36890;&#36807;&#20301;&#32622;&#27880;&#24847;&#26426;&#21046;&#23558;&#20449;&#36947;&#30340;P&#65286;D&#20107;&#20214;&#21382;&#21490;&#32534;&#30721;&#20026;&#24207;&#21015;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of pump-and-dump schemes (P&amp;Ds) in the cryptocurrency market, it becomes imperative to detect such fraudulent activities in advance to alert potentially susceptible investors. In this paper, we focus on predicting the pump probability of all coins listed in the target exchange before a scheduled pump time, which we refer to as the target coin prediction task. Firstly, we conduct a comprehensive study of the latest 709 P&amp;D events organized in Telegram from Jan. 2019 to Jan. 2022. Our empirical analysis reveals some interesting patterns of P&amp;Ds, such as that pumped coins exhibit intra-channel homogeneity and inter-channel heterogeneity. Here channel refers a form of group in Telegram that is frequently used to coordinate P&amp;D events. This observation inspires us to develop a novel sequence-based neural network, dubbed SNN, which encodes a channel's P&amp;D event history into a sequence representation via the positional attention mechanism to enhance the prediction accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#24314;&#27169;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#35786;&#26029;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20813;&#30123;&#21463;&#20307;&#24211;&#20026;&#20363;&#65292;&#38416;&#26126;&#23454;&#39564;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;</title><link>http://arxiv.org/abs/2204.09291</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24314;&#27169;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#37492;&#23450;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#19968;&#39033;&#20851;&#20110;&#20813;&#30123;&#21463;&#20307;&#35786;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving generalization of machine learning-identified biomarkers with causal modeling: an investigation into immune receptor diagnostics. (arXiv:2204.09291v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#24314;&#27169;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#35786;&#26029;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20813;&#30123;&#21463;&#20307;&#24211;&#20026;&#20363;&#65292;&#38416;&#26126;&#23454;&#39564;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36880;&#28176;&#34987;&#24212;&#29992;&#20110;&#39640;&#32500;&#20998;&#23376;&#25968;&#25454;&#20013;&#21457;&#29616;&#35786;&#26029;&#21644;&#39044;&#21518;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#35774;&#35745;&#30456;&#20851;&#30340;&#22810;&#31181;&#22240;&#32032;&#21487;&#33021;&#20250;&#24433;&#21709;&#23398;&#20064;&#21040;&#21487;&#27867;&#21270;&#19988;&#20020;&#24202;&#24212;&#29992;&#20540;&#39640;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#26412;&#25991;&#35748;&#20026;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#26469;&#30475;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#35786;&#26029;&#30340;&#40065;&#26834;&#24615;&#65292;&#27491;&#24335;&#30028;&#23450;&#23427;&#20204;&#19982;&#26426;&#22120;&#23398;&#20064;&#35786;&#26029;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#36866;&#24212;&#24615;&#20813;&#30123;&#21463;&#20307;&#24211;&#65288;AIRR&#65289;&#36825;&#26679;&#19968;&#31181;&#29305;&#23450;&#30340;&#39640;&#32500;&#26631;&#24535;&#29289;&#65292;&#24182;&#20511;&#21161;&#27169;&#25311;&#23454;&#39564;&#38416;&#26126;&#20102;&#19982;AIRR&#39046;&#22495;&#30456;&#20851;&#30340;&#20027;&#35201;&#29983;&#29289;&#21644;&#23454;&#39564;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#21040;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#22240;&#26524;&#24314;&#27169;&#36890;&#36807;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#31283;&#23450;&#20851;&#31995;&#24182;&#25351;&#23548;&#20010;&#20307;&#38388;&#20851;&#31995;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#35843;&#25972;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is increasingly used to discover diagnostic and prognostic biomarkers from high-dimensional molecular data. However, a variety of factors related to experimental design may affect the ability to learn generalizable and clinically applicable diagnostics. Here, we argue that a causal perspective improves the identification of these challenges and formalizes their relation to the robustness and generalization of machine learning-based diagnostics. To make for a concrete discussion, we focus on a specific, recently established high-dimensional biomarker - adaptive immune receptor repertoires (AIRRs). Through simulations, we illustrate how major biological and experimental factors of the AIRR domain may influence the learned biomarkers. In conclusion, we argue that causal modeling improves machine learning-based biomarker robustness by identifying stable relations between variables and by guiding the adjustment of the relations and variables that vary between populations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21487;&#36870;&#27969;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21487;&#36870;&#24615;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2204.07664</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25104;&#20687;&#30340;&#26465;&#20214;&#21487;&#36870;&#27969;
&lt;/p&gt;
&lt;p&gt;
Conditional Injective Flows for Bayesian Imaging. (arXiv:2204.07664v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21487;&#36870;&#27969;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21487;&#36870;&#24615;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29992;&#20110;&#35745;&#31639;&#25104;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22238;&#24402;&#19968;&#20010;&#37325;&#24314;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19981;&#36866;&#23450;&#24615;&#12289;&#38750;&#32447;&#24615;&#12289;&#27169;&#22411;&#19981;&#21305;&#37197;&#21644;&#22122;&#22768;&#24120;&#24120;&#21512;&#35851;&#20351;&#36825;&#20123;&#28857;&#20272;&#35745;&#20855;&#26377;&#35823;&#23548;&#24615;&#25110;&#19981;&#36275;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#22270;&#20687;&#21644;&#65288;&#22024;&#26434;&#30340;&#65289;&#27979;&#37327;&#24314;&#27169;&#20026;&#32852;&#21512;&#20998;&#24067;&#30340;&#38543;&#26426;&#21521;&#37327;&#65292;&#24182;&#26088;&#22312;&#36817;&#20284;&#26410;&#30693;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26465;&#20214;&#26631;&#20934;&#21270;&#27969;&#30340;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#26159;&#20256;&#32479;MCMC&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20855;&#26377;&#32570;&#28857;&#65306;&#23545;&#20110;&#20013;&#31561;&#21040;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#20855;&#26377;&#22312;&#38590;&#20197;&#22788;&#29702;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#20013;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;C-Trumpets--&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25104;&#20687;&#38382;&#39064;&#30340;&#26465;&#20214;&#21487;&#36870;&#27969;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#21487;&#36870;&#24615;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#19982;&#22266;&#23450;&#20307;&#31215;&#30340;&#24314;&#31569;&#21019;&#26032;&#30456;&#32467;&#21512;&#65292;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most deep learning models for computational imaging regress a single reconstructed image. In practice, however, ill-posedness, nonlinearity, model mismatch, and noise often conspire to make such point estimates misleading or insufficient. The Bayesian approach models images and (noisy) measurements as jointly distributed random vectors and aims to approximate the posterior distribution of unknowns. Recent variational inference methods based on conditional normalizing flows are a promising alternative to traditional MCMC methods, but they come with drawbacks: excessive memory and compute demands for moderate to high resolution images and underwhelming performance on hard nonlinear problems. In this work, we propose C-Trumpets -- conditional injective flows specifically designed for imaging problems, which greatly diminish these challenges. Injectivity reduces memory footprint and training time while low-dimensional latent space together with architectural innovations like fixed-volume-c
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#23567;&#26679;&#26412;&#20154;&#31867;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22788;&#29702;&#22810;&#20219;&#21153;&#39044;&#27979;&#24182;&#21487;&#20197;&#20174;&#25972;&#20307;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2203.16309</link><description>&lt;p&gt;
&#38754;&#21521;&#23567;&#26679;&#26412;&#20154;&#31867;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot meta-learning for small-scale data from human subjects. (arXiv:2203.16309v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16309
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#23567;&#26679;&#26412;&#20154;&#31867;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22788;&#29702;&#22810;&#20219;&#21153;&#39044;&#27979;&#24182;&#21487;&#20197;&#20174;&#25972;&#20307;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#22312;&#22823;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#23454;&#38469;&#19978;&#35768;&#22810;&#20154;&#31867;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#19988;&#26631;&#35760;&#31232;&#30095;&#12290;&#24050;&#26377;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#36825;&#26679;&#30340;&#25968;&#25454;&#36890;&#24120;&#19981;&#23481;&#26131;&#27867;&#21270;&#21040;&#26679;&#26412;&#22806;&#30340;&#21463;&#35797;&#32773;&#12290;&#30456;&#21453;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#21487;&#33021;&#26469;&#33258;&#19981;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#20010;&#31216;&#20026;&#8220;&#38646;&#26679;&#26412;&#23398;&#20064;&#8221;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#20110;&#26679;&#26412;&#22806;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#30495;&#23454;&#30340;&#23567;&#35268;&#27169;&#20154;&#31867;&#21463;&#35797;&#32773;&#25968;&#25454;&#38598;&#65288;&#20004;&#20010;&#38543;&#26426;&#23545;&#29031;&#30740;&#31350;&#21644;&#19968;&#20010;&#35266;&#23519;&#24615;&#30740;&#31350;&#65289;&#26469;&#39044;&#27979;&#20445;&#30041;&#30340;&#27835;&#30103;&#32452;&#30340;&#27835;&#30103;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#27599;&#31181;&#24178;&#39044;&#30340;&#28508;&#22312;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#33258;&#28982;&#22320;&#22788;&#29702;&#22810;&#20219;&#21153;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20174;&#25972;&#20307;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
While developments in machine learning led to impressive performance gains on big data, many human subjects data are, in actuality, small and sparsely labeled. Existing methods applied to such data often do not easily generalize to out-of-sample subjects. Instead, models must make predictions on test data that may be drawn from a different distribution, a problem known as \textit{zero-shot learning}. To address this challenge, we develop an end-to-end framework using a meta-learning approach, which enables the model to rapidly adapt to a new prediction task with limited training data for out-of-sample test data. We use three real-world small-scale human subjects datasets (two randomized control studies and one observational study), for which we predict treatment outcomes for held-out treatment groups. Our model learns the latent treatment effects of each intervention and, by design, can naturally handle multi-task predictions. We show that our model performs the best holistically for e
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2203.05711</link><description>&lt;p&gt;
&#30005;&#24433;&#21465;&#36848;&#25688;&#35201;&#65306;&#19968;&#20010;&#29992;&#20110;&#25925;&#20107;&#29702;&#35299;&#30340;&#35270;&#39057;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05711
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;AI&#26377;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#25925;&#20107;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#20854;&#20013;&#21253;&#21547;5,193&#20010;&#27969;&#34892;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#35270;&#39057;&#25688;&#35201;&#12290;SYMON&#25429;&#25417;&#20102;&#30001;&#20154;&#31867;&#21019;&#20316;&#32773;&#21046;&#20316;&#30340;&#38754;&#21521;&#20154;&#31867;&#35266;&#20247;&#30340;&#33258;&#28982;&#25925;&#20107;&#21465;&#36848;&#35270;&#39057;&#12290;&#20316;&#20026;&#19968;&#20010;&#21407;&#22411;&#21644;&#33258;&#28982;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;SYMON&#20855;&#26377;&#39640;&#35206;&#30422;&#30340;&#22810;&#27169;&#24577;&#25925;&#20107;&#20107;&#20214;&#12289;&#20016;&#23500;&#30340;&#24515;&#29702;&#29366;&#24577;&#25551;&#36848;&#21644;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#22823;&#35821;&#20041;&#24046;&#36317;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#21644;&#30005;&#24433;&#25688;&#35201;&#35270;&#39057;&#30340;&#38646;&#26679;&#26412;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#22312;&#25925;&#20107;&#29702;&#35299;&#20013;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;SYMON&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#36827;&#23637;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#24046;&#24322;&#30340;&#38750;&#21512;&#20316;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#22810;&#23610;&#24230;&#12289;&#22810;&#20219;&#21153;CNN&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#35757;&#32451;&#20849;&#20139;&#32534;&#30721;&#22120;&#20197;&#23398;&#20064;&#36890;&#29992;&#29305;&#24449;&#12290;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#22495;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#27169;&#22411;&#30340;&#26631;&#20934;&#21270;&#23618;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2203.04275</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#24046;&#24322;&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#24046;&#24322;&#30340;&#38750;&#21512;&#20316;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#22810;&#23610;&#24230;&#12289;&#22810;&#20219;&#21153;CNN&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#35757;&#32451;&#20849;&#20139;&#32534;&#30721;&#22120;&#20197;&#23398;&#20064;&#36890;&#29992;&#29305;&#24449;&#12290;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#22495;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#27169;&#22411;&#30340;&#26631;&#20934;&#21270;&#23618;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Spacecraft Pose Network v2&#65288;SPNv2&#65289;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#24046;&#24322;&#30340;&#38750;&#21512;&#20316;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#12290;SPNv2&#26159;&#19968;&#20010;&#22810;&#23610;&#24230;&#12289;&#22810;&#20219;&#21153;&#30340;CNN&#65292;&#30001;&#20849;&#20139;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#32534;&#30721;&#22120;&#21644;&#22810;&#20010;&#39044;&#27979;&#22836;&#32452;&#25104;&#65292;&#36825;&#20123;&#39044;&#27979;&#22836;&#22312;&#20849;&#20139;&#29305;&#24449;&#36755;&#20986;&#19978;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#37117;&#19982;&#20174;&#22270;&#20687;&#20013;&#26816;&#27979;&#21644;&#20272;&#35745;&#30446;&#26631;&#33322;&#22825;&#22120;&#30340;&#23039;&#24577;&#26377;&#20851;&#65292;&#20363;&#22914;&#39044;&#27979;&#39044;&#23450;&#20041;&#30340;&#21355;&#26143;&#20851;&#38190;&#28857;&#12289;&#30452;&#25509;&#23039;&#24577;&#22238;&#24402;&#21644;&#21355;&#26143;&#21069;&#26223;&#30340;&#20108;&#20803;&#20998;&#21106;&#31561;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#25968;&#25454;&#22686;&#24378;&#26469;&#20849;&#21516;&#35757;&#32451;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#20849;&#20139;&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#23545;&#20855;&#26377;&#22522;&#26412;&#19981;&#21516;&#35270;&#35273;&#29305;&#24615;&#30340;&#22270;&#20687;&#22495;&#26159;&#36890;&#29992;&#30340;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Online Domain Refinement&#65288;ODR&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30446;&#26631;&#22495;&#19978;&#35843;&#25972;SPNv2&#30340;&#26631;&#20934;&#21270;&#23618;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural Network (CNN) for pose estimation of noncooperative spacecraft across domain gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared multi-scale feature encoder and multiple prediction heads that perform different tasks on a shared feature output. These tasks are all related to detection and pose estimation of a target spacecraft from an image, such as prediction of pre-defined satellite keypoints, direct pose regression, and binary segmentation of the satellite foreground. It is shown that by jointly training on different yet related tasks with extensive data augmentations on synthetic images only, the shared encoder learns features that are common across image domains that have fundamentally different visual characteristics compared to synthetic images. This work also introduces Online Domain Refinement (ODR) which refines the parameters of the normalization layers of SPNv2 on the target doma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25214;&#21040;&#20102;&#24102;&#26435;&#37325;&#34928;&#20943;&#21644;&#38543;&#26426;&#31070;&#32463;&#20803;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#32467;&#26524;&#34920;&#26126;&#26435;&#37325;&#34928;&#20943;&#19982;&#27169;&#22411;&#26550;&#26500;&#30340;&#24378;&#28872;&#20132;&#20114;&#20316;&#29992;&#20250;&#22312;&#22810;&#20110;1&#20010;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#20013;&#21019;&#24314;&#19981;&#33391;&#26497;&#23567;&#20540;&#65292;&#24182;&#34920;&#26126;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#27861;&#26080;&#27861;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.04777</link><description>&lt;p&gt;
&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30340;&#31934;&#30830;&#35299;&#26512;&#35299;
&lt;/p&gt;
&lt;p&gt;
Exact Solutions of a Deep Linear Network. (arXiv:2202.04777v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25214;&#21040;&#20102;&#24102;&#26435;&#37325;&#34928;&#20943;&#21644;&#38543;&#26426;&#31070;&#32463;&#20803;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#32467;&#26524;&#34920;&#26126;&#26435;&#37325;&#34928;&#20943;&#19982;&#27169;&#22411;&#26550;&#26500;&#30340;&#24378;&#28872;&#20132;&#20114;&#20316;&#29992;&#20250;&#22312;&#22810;&#20110;1&#20010;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#20013;&#21019;&#24314;&#19981;&#33391;&#26497;&#23567;&#20540;&#65292;&#24182;&#34920;&#26126;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#27861;&#26080;&#27861;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25214;&#21040;&#20102;&#24102;&#26435;&#37325;&#34928;&#20943;&#21644;&#38543;&#26426;&#31070;&#32463;&#20803;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#36825;&#26159;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#38646;&#26159;&#19968;&#20010;&#29305;&#27530;&#30340;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26435;&#37325;&#34928;&#20943;&#19982;&#27169;&#22411;&#26550;&#26500;&#30340;&#24378;&#28872;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#33021;&#22815;&#22312;&#20855;&#26377;&#36229;&#36807; $1$ &#20010;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#20013;&#21019;&#24314;&#19981;&#33391;&#26497;&#23567;&#20540;&#65292;&#36825;&#19982;&#20165;&#26377; $1$ &#20010;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#26377;&#36136;&#30340;&#19981;&#21516;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#27861;&#26080;&#27861;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20998;&#24067;&#24335;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21305;&#37197;&#22797;&#26434;&#24230;&#19979;&#30028;&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#19981;&#20165;&#22312;&#20998;&#24067;&#24335;&#38543;&#26426;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#22312;&#20998;&#24067;&#24335;&#30830;&#23450;&#24615;&#21644;&#38750;&#20998;&#24067;&#24335;&#38543;&#26426;&#24773;&#20917;&#19979;&#20063;&#26159;&#26368;&#20339;&#30340;&#65292;&#24182;&#24471;&#21040;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2202.02771</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Algorithms for Decentralized Stochastic Variational Inequalities. (arXiv:2202.02771v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20998;&#24067;&#24335;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21305;&#37197;&#22797;&#26434;&#24230;&#19979;&#30028;&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#19981;&#20165;&#22312;&#20998;&#24067;&#24335;&#38543;&#26426;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#22312;&#20998;&#24067;&#24335;&#30830;&#23450;&#24615;&#21644;&#38750;&#20998;&#24067;&#24335;&#38543;&#26426;&#24773;&#20917;&#19979;&#20063;&#26159;&#26368;&#20339;&#30340;&#65292;&#24182;&#24471;&#21040;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#19981;&#31561;&#24335;&#26159;&#19968;&#20010;&#21253;&#25324;&#20102;&#21338;&#24328;&#12289;&#26368;&#23567;&#21270;&#12289;&#38797;&#28857;&#21644;&#24179;&#34913;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#26041;&#27861;&#26159;&#35768;&#22810;&#24212;&#29992;&#20219;&#21153;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#20998;&#24067;&#24335;&#35774;&#32622;&#65292;&#20998;&#24067;&#24335;&#35774;&#32622;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#19981;&#22826;&#34987;&#20102;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#22266;&#23450;&#21644;&#26102;&#21464;&#32593;&#32476;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#38543;&#26426;&#65288;&#24635;&#21644;&#31867;&#22411;&#65289;&#21464;&#20998;&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#20026;&#36890;&#20449;&#21644;&#23616;&#37096;&#36845;&#20195;&#21452;&#26041;&#25552;&#20986;&#20102;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#24182;&#26500;&#24314;&#20102;&#19982;&#36825;&#20123;&#19979;&#30028;&#21305;&#37197;&#30340;&#26368;&#20248;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20165;&#22312;&#20998;&#24067;&#24335;&#38543;&#26426;&#24773;&#20917;&#19979;&#26159;&#26368;&#22909;&#30340;&#65292;&#32780;&#19988;&#22312;&#20998;&#24067;&#24335;&#30830;&#23450;&#24615;&#21644;&#38750;&#20998;&#24067;&#24335;&#38543;&#26426;&#24773;&#20917;&#19979;&#20063;&#26159;&#26368;&#22909;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inequalities are a formalism that includes games, minimization, saddle point, and equilibrium problems as special cases. Methods for variational inequalities are therefore universal approaches for many applied tasks, including machine learning problems. This work concentrates on the decentralized setting, which is increasingly important but not well understood. In particular, we consider decentralized stochastic (sum-type) variational inequalities over fixed and time-varying networks. We present lower complexity bounds for both communication and local iterations and construct optimal algorithms that match these lower bounds. Our algorithms are the best among the available literature not only in the decentralized stochastic case, but also in the decentralized deterministic and non-distributed stochastic cases. Experimental results confirm the effectiveness of the presented algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2202.01752</link><description>&lt;p&gt;
&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#36817;&#20284;&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#35774;&#35745;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#31639;&#27861;&#31995;&#21015;&#65292;&#20165;&#38656;&#35201; $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ &#23616;&#28216;&#25103;&#21363;&#21487;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010; $\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#20854;&#20013; $X,Y$ &#26159;&#20449;&#24687;&#38598;&#30340;&#25968;&#37327;&#65292;$A,B$ &#26159;&#20004;&#21517;&#29609;&#23478;&#30340;&#34892;&#21160;&#25968;&#12290;&#36825;&#27604;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230; $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ &#26377;&#30528; $\widetilde{\mathcal{O}}(\max\{X, Y\})$ &#30340;&#24040;&#22823;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#25968;&#22240;&#23376;&#20869;&#19982;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26032;&#31639;&#27861;&#23454;&#29616;&#20102;&#36825;&#31181;&#26679;&#26412;&#22797;&#26434;&#24230;&#65306;&#24179;&#34913;&#22312;&#32447;&#38236;&#38754;&#19979;&#38477;&#21644;&#24179;&#34913;&#21453;&#20107;&#23454;&#21518;&#24724;&#26368;&#23567;&#21270;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#23558;&#8220;&#24179;&#34913;&#25506;&#32034;&#31574;&#30053;&#8221;&#38598;&#25104;&#21040;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#25163;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;&#25903;&#25345;&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#30340;&#20108;&#20154;&#21338;&#24328;&#21644;&#22810;&#20154;&#21338;&#24328;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts. We also extend our results t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21517;&#20026;&#32039;&#23494;&#24230;&#20998;&#25968;&#65288;CSUFS&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#23616;&#37096;&#32039;&#23494;&#24230;&#26469;&#36873;&#25321;&#25152;&#38656;&#29305;&#24449;&#65292;&#33021;&#22815;&#23454;&#29616;&#38477;&#32500;&#12289;&#27169;&#22411;&#25928;&#26524;&#25552;&#39640;&#21644;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#31561;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.13194</link><description>&lt;p&gt;
&#32039;&#23494;&#24230;&#20998;&#25968;&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#30340;&#24555;&#36895;&#36807;&#28388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compactness Score: A Fast Filter Method for Unsupervised Feature Selection. (arXiv:2201.13194v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21517;&#20026;&#32039;&#23494;&#24230;&#20998;&#25968;&#65288;CSUFS&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#23616;&#37096;&#32039;&#23494;&#24230;&#26469;&#36873;&#25321;&#25152;&#38656;&#29305;&#24449;&#65292;&#33021;&#22815;&#23454;&#29616;&#38477;&#32500;&#12289;&#27169;&#22411;&#25928;&#26524;&#25552;&#39640;&#21644;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20449;&#24687;&#26102;&#20195;&#30340;&#20852;&#36215;&#65292;&#27599;&#22825;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#29305;&#24615;&#65292;&#24448;&#24448;&#38590;&#20197;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#24613;&#38656;&#19968;&#31181;&#39640;&#25928;&#30340;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;&#23545;&#20110;&#29305;&#24449;&#24037;&#31243;&#65292;&#29305;&#24449;&#36873;&#25321;&#20284;&#20046;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#20869;&#23481;&#65292;&#20854;&#20013;&#39044;&#35745;&#20174;&#20505;&#36873;&#29305;&#24449;&#20013;&#36873;&#25321;&#8220;&#20248;&#31168;&#8221;&#30340;&#29305;&#24449;&#12290;&#29305;&#24449;&#36873;&#25321;&#21487;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#21151;&#33021;&#65292;&#22914;&#38477;&#32500;&#12289;&#27169;&#22411;&#25928;&#26524;&#25552;&#39640;&#21644;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#12290;&#22312;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#65292;&#22914;&#26524;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#65292;&#21017;&#36825;&#20123;&#25968;&#25454;&#20284;&#20046;&#36890;&#24120;&#24444;&#27492;&#25509;&#36817;&#65307;&#22240;&#27492;&#65292;&#23616;&#37096;&#32039;&#23494;&#24230;&#23545;&#20110;&#29305;&#24449;&#30340;&#35780;&#20272;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21517;&#20026;&#32039;&#23494;&#24230;&#20998;&#25968;&#65288;CSUFS&#65289;&#65292;&#29992;&#20110;&#36873;&#25321;&#25152;&#38656;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the flourish of the information age, massive amounts of data are generated day by day. Due to the large-scale and high-dimensional characteristics of these data, it is often difficult to achieve better decision-making in practical applications. Therefore, an efficient big data analytics method is urgently needed. For feature engineering, feature selection seems to be an important research content in which is anticipated to select "excellent" features from candidate ones. Different functions can be realized through feature selection, such as dimensionality reduction, model effect improvement, and model performance improvement. In many classification tasks, researchers found that data seem to be usually close to each other if they are from the same class; thus, local compactness is of great importance for the evaluation of a feature. In this manuscript, we propose a fast unsupervised feature selection method, named as, Compactness Score (CSUFS), to select desired features. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27169;&#25311;-&#23454;&#39564;&#23460;-&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#21450;&#27867;&#21270;&#20445;&#35777;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#20445;&#35777;&#30340;&#23433;&#20840;&#24863;&#30693;&#31574;&#30053;&#20998;&#24067;&#26469;&#24357;&#21512;&#29616;&#23454;&#24046;&#36317;&#65292;&#20855;&#26377;&#21452;&#37325;&#31574;&#30053;&#35774;&#32622;&#12289;&#30417;&#30563;&#25511;&#21046;&#31574;&#30053;&#21644;&#8220;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;-Bayes&#8221;&#26694;&#26550;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#22235;&#36724;&#39134;&#34892;&#31561;&#20219;&#21153;&#20013;&#23454;&#29616;&#25913;&#36827;&#30340;&#23433;&#20840;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2201.08355</link><description>&lt;p&gt;
&#27169;&#25311;-&#23454;&#39564;&#23460;-&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#21450;&#27867;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees. (arXiv:2201.08355v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27169;&#25311;-&#23454;&#39564;&#23460;-&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#21450;&#27867;&#21270;&#20445;&#35777;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#20445;&#35777;&#30340;&#23433;&#20840;&#24863;&#30693;&#31574;&#30053;&#20998;&#24067;&#26469;&#24357;&#21512;&#29616;&#23454;&#24046;&#36317;&#65292;&#20855;&#26377;&#21452;&#37325;&#31574;&#30053;&#35774;&#32622;&#12289;&#30417;&#30563;&#25511;&#21046;&#31574;&#30053;&#21644;&#8220;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;-Bayes&#8221;&#26694;&#26550;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#22235;&#36724;&#39134;&#34892;&#31561;&#20219;&#21153;&#20013;&#23454;&#29616;&#25913;&#36827;&#30340;&#23433;&#20840;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#26159;&#33258;&#20027;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#31574;&#30053;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#30340;&#31574;&#30053;&#30001;&#20110;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#32780;&#32463;&#24120;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#29615;&#22659;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#25311;-&#23454;&#39564;&#23460;-&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#20445;&#35777;&#30340;&#23433;&#20840;&#24863;&#30693;&#31574;&#30053;&#20998;&#24067;&#26469;&#24357;&#21512;&#29616;&#23454;&#24046;&#36317;&#12290;&#20026;&#20102;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#37325;&#31574;&#30053;&#35774;&#32622;&#65292;&#19968;&#20010;&#24615;&#33021;&#31574;&#30053;&#20351;&#29992;&#32047;&#31215;&#20219;&#21153;&#22870;&#21169;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#22791;&#20221;&#65288;&#23433;&#20840;&#65289;&#31574;&#30053;&#21017;&#36890;&#36807;&#27714;&#35299;&#22522;&#20110;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#65288;HJ&#65289;&#21487;&#36798;&#24615;&#20998;&#26512;&#30340;&#23433;&#20840;&#36125;&#23572;&#26364;&#26041;&#31243;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#27169;&#25311;-&#23454;&#39564;&#23460;&#36716;&#31227;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#30417;&#30563;&#25511;&#21046;&#31574;&#30053;&#65292;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#20445;&#25252;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#65307;&#22312;&#23454;&#39564;&#23460;-&#30495;&#23454;&#36716;&#31227;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#8220;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;-Bayes&#8221;&#26694;&#26550;&#20026;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#25552;&#20379;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#30340;&#19979;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;HJ&#21487;&#36798;&#24615;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#25105;&#20204;&#31574;&#30053;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#21644;&#22235;&#36724;&#39134;&#34892;&#20219;&#21153;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#25311;-&#23454;&#39564;&#23460;-&#30495;&#23454;&#29615;&#22659;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#23433;&#20840;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#30701;&#26102;&#21051;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;FlowNN&#65292;&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20559;&#24046;&#12289;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#36848;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;17%&#21040;71%&#30340;&#25439;&#22833;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2112.12321</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#27969;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25968;&#25454;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#30701;&#26102;&#21051;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physics Constrained Flow Neural Network for Short-Timescale Predictions in Data Communications Networks. (arXiv:2112.12321v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#30701;&#26102;&#21051;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;FlowNN&#65292;&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20559;&#24046;&#12289;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#36848;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;17%&#21040;71%&#30340;&#25439;&#22833;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25968;&#25454;&#36890;&#20449;&#32593;&#32476;&#20449;&#24687;&#27969;&#21160;&#30340;&#21160;&#24577;&#20998;&#26512;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#32463;&#24120;&#20381;&#38752;&#29616;&#25104;&#30340;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#21382;&#21490;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#26080;&#35270;&#25484;&#25511;&#36825;&#20123;&#20449;&#24687;&#27969;&#29983;&#25104;&#34892;&#20026;&#30340;&#29289;&#29702;&#35268;&#24459;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Flow Neural Network&#65288;FlowNN&#65289;&#65292;&#36890;&#36807;&#23884;&#22871;&#23618;&#21644;&#24402;&#32435;&#23618;&#24341;&#20837;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#20559;&#24046;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#36848;&#65292;&#24182;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#26469;&#26222;&#36941;&#22320;&#23398;&#20064;&#36825;&#20123;&#29289;&#29702;&#35268;&#24459;&#12290;&#22312;&#30701;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#32593;&#32476;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;FlowNN&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;17%&#21040;71%&#30340;&#25439;&#22833;&#19979;&#38477;&#65292;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23041;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is gaining growing momentum in various recent models for the dynamic analysis of information flows in data communications networks. These preliminary models often rely on off-the-shelf learning models to predict from historical statistics while disregarding the physics governing the generating behaviors of these flows. This paper instead introduces Flow Neural Network (FlowNN) to improve the feature representation with learned physical bias. This is implemented by an induction layer, working upon the embedding layer, to impose the physics connected data correlations, and a self-supervised learning strategy with stop-gradient to make the learned physics universal. For the short-timescale network prediction tasks, FlowNN achieves 17% - 71% of loss decrease than the state-of-the-art baselines on both synthetic and real-world networking datasets, which shows the strength of this new approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;DNN&#20013;&#29702;&#35299;&#20135;&#29983;&#30340;&#20132;&#20114;&#27010;&#24565;&#30340;&#27010;&#24565;-emerging&#29616;&#35937;&#65292;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#29992;&#31232;&#30095;&#30340;&#31526;&#21495;&#22240;&#26524;&#22270;&#21644;And-Or&#22270;&#65288;AOG&#65289;&#36827;&#34892;&#37327;&#21270;&#21644;&#31616;&#21270;&#12290;</title><link>http://arxiv.org/abs/2111.06206</link><description>&lt;p&gt;
DNN&#20013;&#31232;&#30095;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Defining and Quantifying the Emergence of Sparse Concepts in DNNs. (arXiv:2111.06206v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;DNN&#20013;&#29702;&#35299;&#20135;&#29983;&#30340;&#20132;&#20114;&#27010;&#24565;&#30340;&#27010;&#24565;-emerging&#29616;&#35937;&#65292;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#29992;&#31232;&#30095;&#30340;&#31526;&#21495;&#22240;&#26524;&#22270;&#21644;And-Or&#22270;&#65288;AOG&#65289;&#36827;&#34892;&#37327;&#21270;&#21644;&#31616;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35828;&#26126;&#22312;&#35757;&#32451;&#36807;&#30340;DNN&#20013;&#27010;&#24565;&#30340;&#20135;&#29983;&#29616;&#35937;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;DNN&#30340;&#25512;&#29702;&#20998;&#25968;&#21487;&#20197;&#20998;&#35299;&#20026;&#23569;&#25968;&#20132;&#20114;&#27010;&#24565;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#31232;&#30095;&#30340;&#31526;&#21495;&#22240;&#26524;&#22270;&#20013;&#30340;&#22240;&#26524;&#27169;&#24335;&#65292;&#35813;&#22270;&#35299;&#37322;&#20102;DNN&#12290;&#20351;&#29992;&#27492;&#31867;&#22240;&#26524;&#22270;&#23545;DNN&#36827;&#34892;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#65292;&#22240;&#20026;&#25105;&#20204;&#35777;&#26126;&#20102;&#22240;&#26524;&#22270;&#21487;&#20197;&#22312;&#25351;&#25968;&#25968;&#37327;&#30340;&#19981;&#21516;&#23631;&#34109;&#26679;&#26412;&#19978;&#24456;&#22909;&#22320;&#27169;&#20223;DNN&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#22240;&#26524;&#22270;&#21487;&#20197;&#36827;&#19968;&#27493;&#31616;&#21270;&#24182;&#37325;&#26032;&#32534;&#20889;&#20026;And-Or&#22270;&#65288;AOG&#65289;&#65292;&#32780;&#19981;&#20250;&#22833;&#21435;&#22826;&#22810;&#30340;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to illustrate the concept-emerging phenomenon in a trained DNN. Specifically, we find that the inference score of a DNN can be disentangled into the effects of a few interactive concepts. These concepts can be understood as causal patterns in a sparse, symbolic causal graph, which explains the DNN. The faithfulness of using such a causal graph to explain the DNN is theoretically guaranteed, because we prove that the causal graph can well mimic the DNN's outputs on an exponential number of different masked samples. Besides, such a causal graph can be further simplified and re-written as an And-Or graph (AOG), without losing much explanation accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#27010;&#36848;&#36817;&#24180;&#26469;&#25552;&#20986;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#65288;FAFL&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.01872</link><description>&lt;p&gt;
&#36861;&#27714;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Fairness-Aware Federated Learning. (arXiv:2111.01872v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#27010;&#36848;&#36817;&#24180;&#26469;&#25552;&#20986;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#65288;FAFL&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#26368;&#36817;&#36827;&#23637;&#20026;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#30340;&#26426;&#20250;&#65292;&#24182;&#20445;&#35777;&#20102;&#24615;&#33021;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20316;&#21697;&#38598;&#20013;&#20110;FL&#20013;&#22830;&#25511;&#21046;&#22120;&#30340;&#21033;&#30410;&#65292;&#32780;&#24573;&#35270;&#20102;FL&#23458;&#25143;&#31471;&#30340;&#21033;&#30410;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#23545;&#24453;&#23458;&#25143;&#31471;&#65292;&#20351;&#20854;&#19981;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#25439;&#23475;FL&#29983;&#24577;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;FL&#30340;&#20844;&#24179;&#24615;&#27491;&#22312;&#21560;&#24341;&#30528;&#22823;&#37327;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#65292;&#19981;&#21516;&#35282;&#24230;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#65288;FAFL&#65289;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#35843;&#26597;&#26469;&#24110;&#21161;&#35835;&#32773;&#28145;&#20837;&#20102;&#35299;&#36825;&#20010;&#36328;&#23398;&#31185;&#39046;&#22495;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#36825;&#26679;&#19968;&#31687;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Federated Learning (FL) have brought large-scale collaborative machine learning opportunities for massively distributed clients with performance and data privacy guarantees. However, most current works focus on the interest of the central controller in FL,and overlook the interests of the FL clients. This may result in unfair treatment of clients that discourages them from actively participating in the learning process and damages the sustainability of the FL ecosystem. Therefore, the topic of ensuring fairness in FL is attracting a great deal of research interest. In recent years, diverse Fairness-Aware FL (FAFL) approaches have been proposed in an effort to achieve fairness in FL from different perspectives. However, there is no comprehensive survey that helps readers gain insight into this interdisciplinary field. This paper aims to provide such a survey. By examining the fundamental and simplifying assumptions, as well as the notions of fairness adopted by existi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#27491;&#21521;-&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#29702;&#35770;&#23545;&#34203;&#23450;&#35860;&#26725;&#36827;&#34892;&#20284;&#28982;&#35757;&#32451;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;SB&#30340;&#20284;&#28982;&#30446;&#26631;&#65292;&#36825;&#21487;&#20197;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.11291</link><description>&lt;p&gt;
&#21033;&#29992;&#27491;&#21521;-&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#29702;&#35770;&#23545;&#34203;&#23450;&#35860;&#26725;&#36827;&#34892;&#20284;&#28982;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Likelihood Training of Schr\"odinger Bridge using Forward-Backward SDEs Theory. (arXiv:2110.11291v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#27491;&#21521;-&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#29702;&#35770;&#23545;&#34203;&#23450;&#35860;&#26725;&#36827;&#34892;&#20284;&#28982;&#35757;&#32451;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;SB&#30340;&#20284;&#28982;&#30446;&#26631;&#65292;&#36825;&#21487;&#20197;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34203;&#23450;&#35860;&#26725;&#65288;SB&#65289;&#26159;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#19982;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30456;&#27604;&#65292;&#22312;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#20013;&#30001;&#20110;&#20854;&#25968;&#23398;&#28789;&#27963;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;SB&#30340;&#20248;&#21270;&#21407;&#21017;&#26159;&#21542;&#19982;&#29616;&#20195;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#30456;&#20851;&#65292;&#21518;&#32773;&#36890;&#24120;&#20381;&#36182;&#20110;&#26500;&#24314;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;SB&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24212;&#29992;&#30340;&#21407;&#21017;&#24615;&#26367;&#20195;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#27491;&#21521;-&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#29702;&#35770;&#23545;SB&#27169;&#22411;&#36827;&#34892;&#20284;&#28982;&#35757;&#32451;&#8212;&#8212;&#36825;&#26159;&#19968;&#31181;&#20986;&#29616;&#22312;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#20013;&#30340;&#25968;&#23398;&#26041;&#27861;&#65292;&#23427;&#23558;SB&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#36716;&#21270;&#20026;&#19968;&#32452;SDE&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#20123;SDE&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;SB&#30340;&#20284;&#28982;&#30446;&#26631;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#24191;&#20041;&#22320;&#25512;&#24191;&#20102;SGM&#30340;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Schr\"odinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory - a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MASHA1&#21644;MASHA2&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#30340;&#36890;&#20449;&#37327;&#65292;&#24182;&#22312;&#33719;&#24471;&#21487;&#27604;&#24615;&#36136;&#37327;&#30340;&#27169;&#22411;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;&#38797;&#28857;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.03313</link><description>&lt;p&gt;
&#21387;&#32553;&#36890;&#35759;&#35299;&#20915;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#21450;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees. (arXiv:2110.03313v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MASHA1&#21644;MASHA2&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#30340;&#36890;&#20449;&#37327;&#65292;&#24182;&#22312;&#33719;&#24471;&#21487;&#27604;&#24615;&#36136;&#37327;&#30340;&#27169;&#22411;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;&#38797;&#28857;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;&#38797;&#28857;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#23398;&#20064;&#12289;GAN&#12289;&#36816;&#36755;&#21644;&#24378;&#21270;&#20248;&#21270;&#31561;&#26041;&#38754;&#12290;&#20026;&#20102;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#65292;&#38656;&#35201;&#20381;&#36182;&#20110;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#65292;&#35745;&#31639;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#25104;&#20026;&#35757;&#32451;&#30340;&#20851;&#38190;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#32500;&#24230;&#21644;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#20351;&#29992;&#21487;&#20197;&#20943;&#23569;&#20256;&#36755;&#20449;&#24687;&#37327;&#30340;&#31574;&#30053;&#26469;&#38477;&#20302;&#35757;&#32451;&#20013;&#30340;&#36890;&#20449;&#37327;&#65292;&#21516;&#26102;&#33719;&#24471;&#20855;&#26377;&#21487;&#27604;&#24615;&#36136;&#37327;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MASHA1&#21644;MASHA2&#31561;&#22522;&#20110;&#21387;&#32553;&#36890;&#35759;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;&#38797;&#28857;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inequalities in general and saddle point problems in particular are increasingly relevant in machine learning applications, including adversarial learning, GANs, transport and robust optimization. With increasing data and problem sizes necessary to train high performing models across various applications, we need to rely on parallel and distributed computing. However, in distributed training, communication among the compute nodes is a key bottleneck during training, and this problem is exacerbated for high dimensional and over-parameterized models. Due to these considerations, it is important to equip existing methods with strategies that would allow to reduce the volume of transmitted information during training while obtaining a model of comparable quality. In this paper, we present the first theoretically grounded distributed methods for solving variational inequalities and saddle point problems using compressed communication: MASHA1 and MASHA2. Our theory and methods al
&lt;/p&gt;</description></item><item><title>OpenFed&#26159;&#19968;&#20010;&#20840;&#38754;&#32780;&#28789;&#27963;&#30340;&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#35757;&#32451;&#28040;&#38500;&#20102;&#25968;&#25454;&#20256;&#36755;&#21644;&#38598;&#20013;&#24335;&#27719;&#32858;&#30340;&#38656;&#27714;&#65292;&#38477;&#20302;&#20102;&#23398;&#20064;&#25104;&#26412;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24212;&#29992;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2109.07852</link><description>&lt;p&gt;
OpenFed&#65306;&#19968;&#20010;&#20840;&#38754;&#32780;&#28789;&#27963;&#30340;&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenFed: A Comprehensive and Versatile Open-Source Federated Learning Framework. (arXiv:2109.07852v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07852
&lt;/p&gt;
&lt;p&gt;
OpenFed&#26159;&#19968;&#20010;&#20840;&#38754;&#32780;&#28789;&#27963;&#30340;&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#35757;&#32451;&#28040;&#38500;&#20102;&#25968;&#25454;&#20256;&#36755;&#21644;&#38598;&#20013;&#24335;&#27719;&#32858;&#30340;&#38656;&#27714;&#65292;&#38477;&#20302;&#20102;&#23398;&#20064;&#25104;&#26412;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24212;&#29992;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#24471;&#20854;&#22312;&#21830;&#19994;&#21644;&#24037;&#19994;&#39046;&#22495;&#24471;&#20197;&#25104;&#21151;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#24335;&#27719;&#32858;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#25968;&#25454;&#25935;&#24863;&#25110;&#25968;&#25454;&#20256;&#36755;&#25104;&#26412;&#39640;&#26114;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#35757;&#32451;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#25968;&#25454;&#20256;&#36755;&#21644;&#27719;&#32858;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#25512;&#21160;&#32852;&#37030;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#20197;&#35299;&#20915;&#19968;&#20123;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenFed&#65292;&#19968;&#20010;&#29992;&#20110;&#31471;&#21040;&#31471;&#32852;&#37030;&#23398;&#20064;&#30340;&#24320;&#28304;&#36719;&#20214;&#26694;&#26550;&#12290; OpenFed&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#22320;&#28040;&#38500;&#29616;&#26377;&#30340;&#30171;&#28857;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#32773;&#38477;&#20302;&#20102;&#20934;&#20837;&#38376;&#27099;&#12290;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#65292;OpenFed&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#26032;&#26041;&#27861;&#30340;&#23454;&#29616;&#21464;&#24471;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in Artificial Intelligence techniques have enabled their successful application across a spectrum of commercial and industrial settings. However, these techniques require large volumes of data to be aggregated in a centralized manner, forestalling their applicability to scenarios wherein the data is sensitive or the cost of data transmission is prohibitive. Federated Learning alleviates these problems by decentralizing model training, thereby removing the need for data transfer and aggregation. To advance the adoption of Federated Learning, more research and development needs to be conducted to address some important open questions. In this work, we propose OpenFed, an open-source software framework for end-to-end Federated Learning. OpenFed reduces the barrier to entry for both researchers and downstream users of Federated Learning by the targeted removal of existing pain points. For researchers, OpenFed provides a framework wherein new methods can be easily implem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#21019;&#24314;&#21160;&#24577;&#27169;&#22411;&#38598;&#21512;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38598;&#21512;&#27169;&#22411;&#32780;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2109.05549</link><description>&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#32852;&#37030;&#28151;&#21512;&#24314;&#27169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Ensemble Model-based Reinforcement Learning in Edge Computing. (arXiv:2109.05549v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.05549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#21019;&#24314;&#21160;&#24577;&#27169;&#22411;&#38598;&#21512;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38598;&#21512;&#27169;&#22411;&#32780;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#19981;&#25910;&#38598;&#35774;&#22791;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20351;&#22320;&#29702;&#19978;&#20998;&#24067;&#21644;&#24322;&#26500;&#30340;&#35774;&#22791;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#12290;&#20026;&#20102;&#23558;FL&#25193;&#23637;&#21040;&#36229;&#20986;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#26469;&#22788;&#29702;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FRL&#31639;&#27861;&#30452;&#25509;&#23558;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;FL&#32467;&#21512;&#36215;&#26469;&#65292;&#22240;&#27492;&#24448;&#24448;&#23548;&#33268;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FRL&#31639;&#27861;&#65292;&#23427;&#26377;&#25928;&#22320;&#23558;&#27169;&#22411;&#24314;&#27169;&#24378;&#21270;&#23398;&#20064;&#21644;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#34701;&#20837;FL&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;FL&#21644;&#30693;&#35782;&#33976;&#39311;&#20026;&#23458;&#25143;&#21019;&#24314;&#21160;&#24577;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#28982;&#21518;&#20165;&#20351;&#29992;&#38598;&#21512;&#27169;&#22411;&#32780;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#35757;&#32451;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#21333;&#35843;&#25913;&#36827;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving distributed machine learning paradigm that enables collaborative training among geographically distributed and heterogeneous devices without gathering their data. Extending FL beyond the supervised learning models, federated reinforcement learning (FRL) was proposed to handle sequential decision-making problems in edge computing systems. However, the existing FRL algorithms directly combine model-free RL with FL, thus often leading to high sample complexity and lacking theoretical guarantees. To address the challenges, we propose a novel FRL algorithm that effectively incorporates model-based RL and ensemble knowledge distillation into FL for the first time. Specifically, we utilise FL and knowledge distillation to create an ensemble of dynamics models for clients, and then train the policy by solely using the ensemble model without interacting with the environment. Furthermore, we theoretically prove that the monotonic improvement of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#30340;&#24322;&#27493;&#36845;&#20195;&#20998;&#26512;&#20013;&#12290;&#20316;&#32773;&#20351;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#20351;&#24471;&#25968;&#31181;&#24182;&#34892;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#35777;&#26126;&#21464;&#24471;&#26356;&#21152;&#31934;&#28860;&#12289;&#31616;&#21333;&#21516;&#26102;&#36824;&#22686;&#24378;&#20102;&#21407;&#26377;&#30340;&#35777;&#26126;&#21487;&#20449;&#24230;&#65292;&#36827;&#32780;&#24314;&#31435;&#20102;&#33267;&#20170;&#32570;&#20047;&#23436;&#25972;&#29702;&#35770;&#29702;&#35299;&#30340;&#27969;&#34892;&#31639;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2109.04522</link><description>&lt;p&gt;
&#20248;&#21270;&#20013;&#30340;&#24322;&#27493;&#36845;&#20195;&#65306;&#26032;&#30340;&#24207;&#21015;&#32467;&#26524;&#21644;&#26356;&#21152;&#31934;&#20934;&#30340;&#31639;&#27861;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees. (arXiv:2109.04522v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#30340;&#24322;&#27493;&#36845;&#20195;&#20998;&#26512;&#20013;&#12290;&#20316;&#32773;&#20351;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#20351;&#24471;&#25968;&#31181;&#24182;&#34892;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#35777;&#26126;&#21464;&#24471;&#26356;&#21152;&#31934;&#28860;&#12289;&#31616;&#21333;&#21516;&#26102;&#36824;&#22686;&#24378;&#20102;&#21407;&#26377;&#30340;&#35777;&#26126;&#21487;&#20449;&#24230;&#65292;&#36827;&#32780;&#24314;&#31435;&#20102;&#33267;&#20170;&#32570;&#20047;&#23436;&#25972;&#29702;&#35770;&#29702;&#35299;&#30340;&#27969;&#34892;&#31639;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#30340;&#24322;&#27493;&#36845;&#20195;&#20998;&#26512;&#20013;&#12290;&#36825;&#20123;&#32467;&#26524;&#26131;&#20110;&#24212;&#29992;&#65292;&#24182;&#32473;&#20986;&#20102;&#36845;&#20195;&#30340;&#24322;&#27493;&#31243;&#24230;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#30340;&#26126;&#30830;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#32553;&#30701;&#12289;&#31616;&#21270;&#21644;&#21152;&#24378;&#20102;&#29616;&#26377;&#30340;&#20960;&#31181;&#24322;&#27493;&#20248;&#21270;&#26041;&#27861;&#30340;&#25910;&#25947;&#35777;&#26126;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#19968;&#20123;&#33267;&#20170;&#32570;&#20047;&#23436;&#25972;&#29702;&#35770;&#29702;&#35299;&#30340;&#27969;&#34892;&#31639;&#27861;&#24314;&#31435;&#25910;&#25947;&#20445;&#35777;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#32467;&#26524;&#26469;&#23548;&#20986;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#36793;&#30028;&#65292;&#20197;&#24212;&#29992;&#20110;&#36817;&#31471;&#22686;&#37327;&#32858;&#21512;&#26799;&#24230;&#26041;&#27861;&#65292;&#38024;&#23545;&#24322;&#27493;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22522;&#20110;&#24179;&#22343;&#32780;&#19981;&#26159;&#26368;&#22823;&#24310;&#36831;&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#20445;&#35777;&#65292;&#20026;Krasnoselskii-Mann&#36845;&#20195;&#30340;&#24322;&#27493;&#22359;&#22352;&#26631;&#23454;&#29616;&#25552;&#20379;&#19981;&#37027;&#20040;&#20445;&#23432;&#30340;&#21152;&#36895;&#26465;&#20214;&#20998;&#26512;&#65292;&#37327;&#21270;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce novel convergence results for asynchronous iterations that appear in the analysis of parallel and distributed optimization algorithms. The results are simple to apply and give explicit estimates for how the degree of asynchrony impacts the convergence rates of the iterates. Our results shorten, streamline and strengthen existing convergence proofs for several asynchronous optimization methods and allow us to establish convergence guarantees for popular algorithms that were thus far lacking a complete theoretical understanding. Specifically, we use our results to derive better iteration complexity bounds for proximal incremental aggregated gradient methods, to obtain tighter guarantees depending on the average rather than maximum delay for the asynchronous stochastic gradient descent method, to provide less conservative analyses of the speedup conditions for asynchronous block-coordinate implementations of Krasnoselskii-Mann iterations, and to quantify the convergence rates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#20197;&#20351;&#29992;&#36739;&#24369;&#20551;&#35774;&#35777;&#26126;&#25910;&#25947;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26041;&#27861;&#35777;&#26126;&#20102;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.03445</link><description>&lt;p&gt;
&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#30340;&#25910;&#25947;&#24615;&#21450;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#20197;&#20351;&#29992;&#36739;&#24369;&#20551;&#35774;&#35777;&#26126;&#25910;&#25947;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26041;&#27861;&#35777;&#26126;&#20102;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#31639;&#27861;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20165;&#21487;&#29992;&#20989;&#25968;&#30340;&#26377;&#22122;&#27979;&#37327;&#24773;&#20917;&#19979;&#25214;&#21040;&#38646;&#28857;&#25110;&#22266;&#23450;&#28857;&#12290;&#30446;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21306;&#20998;&#8220;&#21516;&#27493;&#8221;&#26356;&#26032;&#21644;&#8220;&#24322;&#27493;&#8221;&#26356;&#26032;&#65292;&#22312;&#8220;&#21516;&#27493;&#8221;&#26356;&#26032;&#20013;&#65292;&#27599;&#20010;&#29468;&#27979;&#30340;&#32452;&#20214;&#37117;&#20250;&#22312;&#27599;&#20010;&#26102;&#38388;&#26356;&#26032;&#65292;&#32780;&#22312;&#8220;&#24322;&#27493;&#8221;&#26356;&#26032;&#20013;&#65292;&#20165;&#26356;&#26032;&#19968;&#20010;&#32452;&#20214;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20013;&#38388;&#24773;&#20917;&#65292;&#31216;&#20026;&#8220;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#8221;&#65288;BASA&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#26102;&#38388;&#28857;&#20165;&#26356;&#26032;&#8220;&#24403;&#21069;&#20272;&#35745;&#35299;&#8221;&#30340;&#19968;&#20123;&#20294;&#19981;&#26159;&#20840;&#37096;&#30340;&#32452;&#20214;&#12290;BASA&#20801;&#35768;&#29992;&#25143;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#35777;&#26126;&#27492;&#31867;&#31639;&#27861;&#25910;&#25947;&#20110;&#25152;&#30740;&#31350;&#26144;&#23556;&#30340;&#22266;&#23450;&#28857;&#12290;&#36825;&#20123;&#25910;&#25947;&#35777;&#26126;&#20351;&#29992;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#24369;&#30340;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#30340;&#25910;&#25947;&#35777;&#26126;&#35201;&#27714;&#27493;&#38271;&#21442;&#25968;&#20197;&#36866;&#24403;&#30340;&#36895;&#29575;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20165;&#35201;&#27714;&#27599;&#20010;&#32452;&#20214;&#20855;&#26377;&#36275;&#22815;&#30340;&#26356;&#26032;&#39057;&#29575;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#35777;&#26126;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic approximation (SA) algorithm is a widely used probabilistic method for finding a zero or a fixed point of a vector-valued funtion, when only noisy measurements of the function are available. In the literature to date, one makes a distinction between ``synchronous'' updating, whereby every component of the current guess is updated at each time, and ``asynchronous'' updating, whereby only one component is updated. In this paper, we study an intermediate situation that we call ``batch asynchronous stochastic approximation'' (BASA), in which, at each time instant, \textit{some but not all} components of the current estimated solution are updated. BASA allows the user to trade off memory requirements against time complexity. We develop a general methodology for proving that such algorithms converge to the fixed point of the map under study. These convergence proofs make use of weaker hypotheses than existing results. Specifically, existing convergence proofs require that the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20027;&#21160;&#33033;&#34880;&#31649;&#26641;&#20998;&#21106;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#27835;&#30103;&#20013;&#30340;&#29616;&#29366;&#65292;&#25552;&#39640;&#20102;&#36890;&#36807;CTA&#26816;&#26597;&#26816;&#27979;&#20027;&#21160;&#33033;&#21450;&#20854;&#20998;&#25903;&#34880;&#31649;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2108.02998</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20027;&#21160;&#33033;&#34880;&#31649;&#26641;&#20998;&#21106;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#27835;&#30103;&#20013;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.02998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20027;&#21160;&#33033;&#34880;&#31649;&#26641;&#20998;&#21106;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#27835;&#30103;&#20013;&#30340;&#29616;&#29366;&#65292;&#25552;&#39640;&#20102;&#36890;&#36807;CTA&#26816;&#26597;&#26816;&#27979;&#20027;&#21160;&#33033;&#21450;&#20854;&#20998;&#25903;&#34880;&#31649;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#33033;&#34880;&#31649;&#26641;&#30001;&#20027;&#21160;&#33033;&#21450;&#20854;&#20998;&#25903;&#21160;&#33033;&#32452;&#25104;&#65292;&#22312;&#20379;&#24212;&#20840;&#36523;&#34880;&#28082;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#20027;&#21160;&#33033;&#30142;&#30149;&#65292;&#22914;&#20027;&#21160;&#33033;&#30244;&#25110;&#22841;&#23618;&#65292;&#21487;&#33021;&#23548;&#33268;&#20027;&#21160;&#33033;&#30772;&#35010;&#65292;&#20351;&#29992;&#24320;&#25918;&#25163;&#26415;&#27835;&#30103;&#26497;&#20854;&#21361;&#38505;&#12290;&#22240;&#27492;&#65292;&#24739;&#32773;&#36890;&#24120;&#25509;&#21463;&#33647;&#29289;&#27835;&#30103;&#24182;&#36827;&#34892;&#25345;&#32493;&#30417;&#27979;&#65292;&#38656;&#35201;&#36890;&#36807;&#24433;&#20687;&#26816;&#26597;&#23450;&#26399;&#26816;&#26597;&#34880;&#31649;&#12290;&#29992;&#20110;&#35786;&#26029;&#21644;&#30417;&#27979;&#30340;&#26631;&#20934;&#25104;&#20687;&#27169;&#24335;&#26159;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#65292;&#22914;&#26524;&#21152;&#19978;&#23545;&#27604;&#21058;&#65288;CTA&#65289;&#65292;&#21017;&#21487;&#20197;&#25552;&#20379;&#20027;&#21160;&#33033;&#21450;&#20854;&#20998;&#25903;&#34880;&#31649;&#30340;&#35814;&#32454;&#22270;&#20687;&#12290;&#26368;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#35206;&#30422;&#21644;&#27604;&#36739;&#26469;&#33258;&#36830;&#32493;CTA&#30340;&#25972;&#20010;&#20027;&#21160;&#33033;&#34880;&#31649;&#26641;&#20960;&#20309;&#24418;&#29366;&#12290;&#36825;&#19981;&#20165;&#21487;&#20197;&#26816;&#27979;&#20027;&#21160;&#33033;&#30340;&#21464;&#21270;&#65292;&#36824;&#21487;&#20197;&#26816;&#27979;&#30001;&#20027;&#35201;&#30149;&#29702;&#25110;&#26032;&#21457;&#29983;&#30340;&#20998;&#25903;&#21160;&#33033;&#24341;&#36215;&#30340;&#21464;&#21270;&#12290;&#25163;&#21160;&#37325;&#24314;&#27492;&#36807;&#31243;&#38656;&#35201;&#36880;&#23618;&#21246;&#30011;&#65292;&#21487;&#33021;&#38656;&#35201;&#19968;&#25972;&#22825;&#26102;&#38388;&#25165;&#33021;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aortic vessel tree is composed of the aorta and its branching arteries, and plays a key role in supplying the whole body with blood. Aortic diseases, like aneurysms or dissections, can lead to an aortic rupture, whose treatment with open surgery is highly risky. Therefore, patients commonly undergo drug treatment under constant monitoring, which requires regular inspections of the vessels through imaging. The standard imaging modality for diagnosis and monitoring is computed tomography (CT), which can provide a detailed picture of the aorta and its branching vessels if completed with a contrast agent, called CT angiography (CTA). Optimally, the whole aortic vessel tree geometry from consecutive CTAs is overlaid and compared. This allows not only detection of changes in the aorta, but also of its branches, caused by the primary pathology or newly developed. When performed manually, this reconstruction requires slice by slice contouring, which could easily take a whole day for a sing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#32852;&#37030;&#26041;&#27861;&#65292;&#22312;&#24322;&#26500;&#30340;&#26412;&#22320;&#25968;&#25454;&#20013;&#36827;&#34892;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#31449;&#28857;&#20043;&#38388;&#32858;&#21512;&#36825;&#20123;&#32479;&#35745;&#20449;&#24687;&#65292;&#20197;&#33719;&#24471;&#22810;&#31449;&#28857;&#25968;&#25454;&#30340;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#30340;&#28857;&#20272;&#35745;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#32858;&#21512;&#26041;&#26696;&#38656;&#35201;&#32771;&#34385;&#31449;&#28857;&#20043;&#38388;&#30340;&#27835;&#30103;&#20998;&#37197;&#24322;&#36136;&#24615;&#21644;&#32467;&#26524;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#20351;&#24471;&#20272;&#35745;&#37327;&#26159;&#19968;&#33268;&#30340;&#21644;&#28176;&#36817;&#27491;&#24120;&#30340;&#12290;</title><link>http://arxiv.org/abs/2107.11732</link><description>&lt;p&gt;
&#24322;&#26500;&#35266;&#27979;&#25968;&#25454;&#20013;&#32852;&#37030;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#32852;&#37030;&#26041;&#27861;&#65292;&#22312;&#24322;&#26500;&#30340;&#26412;&#22320;&#25968;&#25454;&#20013;&#36827;&#34892;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#31449;&#28857;&#20043;&#38388;&#32858;&#21512;&#36825;&#20123;&#32479;&#35745;&#20449;&#24687;&#65292;&#20197;&#33719;&#24471;&#22810;&#31449;&#28857;&#25968;&#25454;&#30340;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#30340;&#28857;&#20272;&#35745;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#32858;&#21512;&#26041;&#26696;&#38656;&#35201;&#32771;&#34385;&#31449;&#28857;&#20043;&#38388;&#30340;&#27835;&#30103;&#20998;&#37197;&#24322;&#36136;&#24615;&#21644;&#32467;&#26524;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#20351;&#24471;&#20272;&#35745;&#37327;&#26159;&#19968;&#33268;&#30340;&#21644;&#28176;&#36817;&#27491;&#24120;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20272;&#35745;&#24212;&#29992;&#20110;&#22810;&#20010;&#31449;&#28857;&#20010;&#20307;&#30340;&#27835;&#30103;&#25928;&#24212;&#65292;&#20854;&#20013;&#27599;&#20010;&#31449;&#28857;&#23384;&#20648;&#26412;&#22320;&#25968;&#25454;&#65292;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#20010;&#20307;&#32423;&#25968;&#25454;&#19981;&#33021;&#22312;&#31449;&#28857;&#20043;&#38388;&#20849;&#20139;;&#21516;&#26102;&#65292;&#36825;&#20123;&#31449;&#28857;&#21487;&#33021;&#20855;&#26377;&#24322;&#26500;&#30340;&#20154;&#21475;&#21644;&#27835;&#30103;&#20998;&#37197;&#26426;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32852;&#37030;&#26041;&#27861;&#65292;&#20197;&#23545;&#36328;&#31449;&#28857;&#21512;&#24182;&#25968;&#25454;&#30340;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#20542;&#21521;&#20998;&#25968;&#22312;&#26412;&#22320;&#35745;&#31639;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#65292;&#28982;&#21518;&#22312;&#31449;&#28857;&#20043;&#38388;&#32858;&#21512;&#36825;&#20123;&#32479;&#35745;&#20449;&#24687;&#65292;&#20197;&#33719;&#24471;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#30340;&#28857;&#20272;&#35745;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20272;&#35745;&#37327;&#26159;&#19968;&#33268;&#30340;&#21644;&#28176;&#36817;&#27491;&#24120;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#28176;&#36817;&#24615;&#36136;&#65292;&#25105;&#20204;&#21457;&#29616;&#32858;&#21512;&#26041;&#26696;&#38656;&#35201;&#32771;&#34385;&#31449;&#28857;&#20043;&#38388;&#30340;&#27835;&#30103;&#20998;&#37197;&#24322;&#36136;&#24615;&#21644;&#32467;&#26524;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#22823;&#22411;&#21307;&#23398;&#20020;&#24202;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#26469;&#35777;&#26126;&#25105;&#20204;&#32852;&#37030;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in estimating the effect of a treatment applied to individuals at multiple sites, where data is stored locally for each site. Due to privacy constraints, individual-level data cannot be shared across sites; the sites may also have heterogeneous populations and treatment assignment mechanisms. Motivated by these considerations, we develop federated methods to draw inference on the average treatment effects of combined data across sites. Our methods first compute summary statistics locally using propensity scores and then aggregate these statistics across sites to obtain point and variance estimators of average treatment effects. We show that these estimators are consistent and asymptotically normal. To achieve these asymptotic properties, we find that the aggregation schemes need to account for the heterogeneity in treatment assignments and in outcomes across sites. We demonstrate the validity of our federated methods through a comparative study of two large medical cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#26377;&#30028;&#22495;&#19978;&#38750;IID&#20998;&#24067;&#24335;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#22312;&#20998;&#25955;&#30340;&#35745;&#31639;&#32593;&#32476;&#20013;&#20351;&#29992;&#38543;&#26426;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#24378;&#21333;&#35843;&#12289;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#30340;&#24773;&#20917;&#19979;&#20998;&#21035;&#20998;&#26512;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24320;&#21457;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#25955;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.08315</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#20998;&#24067;&#24335;&#26412;&#22320;&#38543;&#26426;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Local Stochastic Extra-Gradient for Variational Inequalities. (arXiv:2106.08315v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#26377;&#30028;&#22495;&#19978;&#38750;IID&#20998;&#24067;&#24335;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#22312;&#20998;&#25955;&#30340;&#35745;&#31639;&#32593;&#32476;&#20013;&#20351;&#29992;&#38543;&#26426;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#24378;&#21333;&#35843;&#12289;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#30340;&#24773;&#20917;&#19979;&#20998;&#21035;&#20998;&#26512;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24320;&#21457;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#25955;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#26377;&#30028;&#22495;&#19978;&#38750;IID&#20998;&#24067;&#24335;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#35745;&#31639;&#32593;&#32476;&#36827;&#34892;&#20102;&#38750;&#24120;&#19968;&#33324;&#30340;&#20551;&#35774;&#65292;&#21253;&#25324;&#20855;&#26377;&#26102;&#21464;&#32593;&#32476;&#30340;&#23436;&#20840;&#20998;&#25955;&#35745;&#31639;&#21644;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#38598;&#20013;&#25299;&#25169;&#12290;&#21478;&#22806;&#65292;&#21487;&#20197;&#23545;&#33410;&#28857;&#36827;&#34892;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#20943;&#23569;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#39057;&#29575;&#12290;&#25105;&#20204;&#23558;&#38543;&#26426;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#25193;&#23637;&#21040;&#36825;&#20010;&#38750;&#24120;&#26222;&#36941;&#30340;&#35774;&#32622;&#20013;&#65292;&#24182;&#22312;&#24378;&#21333;&#35843;&#12289;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#30340;&#24773;&#20917;&#19979;&#65288;&#24403;Minty&#35299;&#23384;&#22312;&#26102;&#65289;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#25552;&#20379;&#30340;&#36895;&#29575;&#26126;&#30830;&#23637;&#31034;&#20102;&#32593;&#32476;&#29305;&#24449;&#65288;&#20363;&#22914;&#28151;&#21512;&#26102;&#38388;&#65289;&#12289;&#36845;&#20195;&#35745;&#25968;&#22120;&#12289;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#26041;&#24046;&#12289;&#35774;&#22791;&#25968;&#37327;&#21644;&#20854;&#20182;&#26631;&#20934;&#21442;&#25968;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#20998;&#26512;&#21487;&#24212;&#29992;&#20110;&#24320;&#21457;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#25955;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider distributed stochastic variational inequalities (VIs) on unbounded domains with the problem data that is heterogeneous (non-IID) and distributed across many devices. We make a very general assumption on the computational network that, in particular, covers the settings of fully decentralized calculations with time-varying networks and centralized topologies commonly used in Federated Learning. Moreover, multiple local updates on the workers can be made for reducing the communication frequency between the workers. We extend the stochastic extragradient method to this very general setting and theoretically analyze its convergence rate in the strongly-monotone, monotone, and non-monotone (when a Minty solution exists) settings. The provided rates explicitly exhibit the dependence on network characteristics (e.g., mixing time), iteration counter, data heterogeneity, variance, number of devices, and other standard parameters. As a special case, our method and analysis apply to d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;FL-Market&#65292;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#26412;&#22320;&#31169;&#26377;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35299;&#32806;ML&#19982;&#32463;&#32426;&#20154;&#38598;&#20013;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2106.04384</link><description>&lt;p&gt;
FL-Market: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20132;&#26131;&#31169;&#26377;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FL-Market: Trading Private Models in Federated Learning. (arXiv:2106.04384v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;FL-Market&#65292;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#26412;&#22320;&#31169;&#26377;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35299;&#32806;ML&#19982;&#32463;&#32426;&#20154;&#38598;&#20013;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#25968;&#25454;&#20998;&#26512;&#32780;&#35328;&#65292;&#33719;&#21462;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#12290;&#36817;&#24180;&#26469;&#65292;&#25226;ML&#27169;&#22411;&#21830;&#21697;&#21270;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#32463;&#27982;&#19988;&#36866;&#24230;&#30340;&#25968;&#25454;&#33719;&#21462;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;&#20551;&#23450;&#32463;&#32426;&#20154;&#21487;&#20197;&#35775;&#38382;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#24182;&#19981;&#29616;&#23454;&#12290;&#20026;&#20102;&#20419;&#36827;&#21487;&#20449;&#25968;&#25454;&#33719;&#21462;&#65292;&#26412;&#25991;&#25552;&#20986;FL-Market&#65292;&#19968;&#31181;&#26412;&#22320;&#31169;&#26377;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;&#65292;&#19981;&#20165;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#20080;&#23478;&#30340;&#38544;&#31169;&#65292;&#36824;&#21487;&#20197;&#20445;&#25252;&#19981;&#21487;&#20449;&#36182;&#30340;&#32463;&#32426;&#20154;&#12290;FL-Market&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;&#19968;&#31181;&#26032;&#20852;&#30340;&#38544;&#31169;&#20445;&#25252;ML&#33539;&#20363;&#65292;&#20854;&#20013;&#25968;&#25454;&#25152;&#26377;&#32773;&#36890;&#36807;&#19978;&#20256;&#26412;&#22320;&#26799;&#24230;&#65288;&#21363;&#23558;&#32858;&#21512;&#20026;&#29992;&#20110;&#26356;&#26032;&#27169;&#22411;&#30340;&#20840;&#23616;&#26799;&#24230;&#65289;&#26469;&#21327;&#20316;&#22320;&#35757;&#32451;ML&#27169;&#22411;&#65289;&#23558;ML&#19982;&#32463;&#32426;&#20154;&#38598;&#20013;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#35299;&#32806;&#12290;&#28982;&#21518;&#65292;FL-Market&#20351;&#25968;&#25454;&#25152;&#26377;&#32773;&#21487;&#20197;&#26412;&#22320;&#25200;&#21160;&#20854;&#26799;&#24230;&#65292;&#20197;&#36827;&#19968;&#27493;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty in acquiring a sufficient amount of training data is a major bottleneck for machine learning (ML) based data analytics. Recently, commoditizing ML models has been proposed as an economical and moderate solution to ML-oriented data acquisition. However, existing model marketplaces assume that the broker can access data owners' private training data, which may not be realistic in practice. In this paper, to promote trustworthy data acquisition for ML tasks, we propose FL-Market, a locally private model marketplace that protects privacy not only against model buyers but also against the untrusted broker. FL-Market decouples ML from the need to centrally gather training data on the broker's side using federated learning, an emerging privacy-preserving ML paradigm in which data owners collaboratively train an ML model by uploading local gradients (to be aggregated into a global gradient for model updating). Then, FL-Market enables data owners to locally perturb their gradient
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GAN&#22914;&#20309;&#26377;&#25928;&#22320;&#23398;&#20064;&#37027;&#20123;&#25509;&#36817;&#30495;&#23454;&#22270;&#20687;&#20998;&#24067;&#30340;&#20998;&#23618;&#29983;&#25104;&#20998;&#24067;&#65292;&#24403;&#19968;&#20010;&#20998;&#24067;&#20855;&#26377;&#21069;&#21521;&#36229;&#20998;&#36776;&#29575;&#32467;&#26500;&#26102;&#65292;&#36890;&#36807;SGDA&#31616;&#21333;&#22320;&#35757;&#32451;GAN&#23601;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2106.02619</link><description>&lt;p&gt;
&#21069;&#21521;&#36229;&#20998;&#36776;&#29575;&#65306;GAN&#22914;&#20309;&#23398;&#20064;&#36924;&#36817;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions. (arXiv:2106.02619v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GAN&#22914;&#20309;&#26377;&#25928;&#22320;&#23398;&#20064;&#37027;&#20123;&#25509;&#36817;&#30495;&#23454;&#22270;&#20687;&#20998;&#24067;&#30340;&#20998;&#23618;&#29983;&#25104;&#20998;&#24067;&#65292;&#24403;&#19968;&#20010;&#20998;&#24067;&#20855;&#26377;&#21069;&#21521;&#36229;&#20998;&#36776;&#29575;&#32467;&#26500;&#26102;&#65292;&#36890;&#36807;SGDA&#31616;&#21333;&#22320;&#35757;&#32451;GAN&#23601;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#23398;&#20064;&#39640;&#22797;&#26434;&#24230;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#30340;&#26368;&#25104;&#21151;&#27169;&#22411;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26368;&#23567;&#26368;&#22823;&#35757;&#32451;&#30446;&#26631;&#30340;&#39640;&#24230;&#38750;&#20984;&#12289;&#38750;&#20985;&#29305;&#24615;&#65292;GAN&#22312;&#29702;&#35770;&#19978;&#20173;&#28982;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#38590;&#29702;&#35299;&#30340;&#12290;&#26412;&#25991;&#27491;&#24335;&#30740;&#31350;&#20102;GAN&#22914;&#20309;&#26377;&#25928;&#22320;&#23398;&#20064;&#37027;&#20123;&#25509;&#36817;&#30495;&#23454;&#22270;&#20687;&#20998;&#24067;&#30340;&#20998;&#23618;&#29983;&#25104;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#24403;&#19968;&#20010;&#20998;&#24067;&#20855;&#26377;&#25105;&#20204;&#25152;&#31216;&#30340;&#21069;&#21521;&#36229;&#20998;&#36776;&#29575;&#32467;&#26500;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;SGDA&#65289;&#31616;&#21333;&#22320;&#35757;&#32451;GAN&#23601;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#36825;&#20010;&#20998;&#24067;&#65292;&#26080;&#35770;&#26159;&#26679;&#26412;&#36824;&#26159;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65292;&#34920;&#26126;&#25105;&#20204;&#25152;&#20551;&#35774;&#30340;&#8220;&#21069;&#21521;&#36229;&#20998;&#36776;&#29575;&#8221;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#33258;&#28982;&#65292;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#30740;&#31350;&#30340;&#24213;&#23618;&#23398;&#20064;&#26426;&#21046;&#65288;&#36890;&#36807;SGDA&#29702;&#35770;&#19978;&#20801;&#35768;&#25105;&#20204;&#39640;&#25928;&#22320;&#35757;&#32451;GAN&#65289;&#27169;&#25311;&#20102;&#23454;&#38469;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) are among the most successful models for learning high-complexity, real-world distributions. However, in theory, due to the highly non-convex, non-concave landscape of the minmax training objective, GAN remains one of the least understood deep learning models. In this work, we formally study how GANs can efficiently learn certain hierarchically generated distributions that are close to the distribution of real-life images. We prove that when a distribution has a structure that we refer to as Forward Super-Resolution, then simply training generative adversarial networks using stochastic gradient descent ascent (SGDA) can learn this distribution efficiently, both in sample and time complexities. We also provide empirical evidence that our assumption "forward super-resolution" is very natural in practice, and the underlying learning mechanisms that we study in this paper (to allow us efficiently train GAN via SGDA in theory) simulates the actual lear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26410;&#35266;&#27979;&#28151;&#28102;&#65292;&#20294;&#21516;&#26102;&#35266;&#27979;&#21040;&#28151;&#28102;&#20195;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#37117;&#21487;&#20197;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2105.04544</link><description>&lt;p&gt;
&#24102;&#26377;&#26680;&#20989;&#25968;&#30340;&#36817;&#22240;&#26524;&#23398;&#20064;&#65306;&#20004;&#38454;&#27573;&#20272;&#35745;&#19982;&#30697;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction. (arXiv:2105.04544v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.04544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26410;&#35266;&#27979;&#28151;&#28102;&#65292;&#20294;&#21516;&#26102;&#35266;&#27979;&#21040;&#28151;&#28102;&#20195;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#37117;&#21487;&#20197;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#23384;&#22312;&#26410;&#35266;&#27979;&#28151;&#28102;&#65292;&#20294;&#21516;&#26102;&#35266;&#27979;&#21040;&#28508;&#22312;&#28151;&#28102;&#20195;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65306;&#65288;a&#65289;&#20004;&#38454;&#27573;&#22238;&#24402;&#26041;&#27861;&#21644;&#65288;b&#65289;&#26368;&#22823;&#30697;&#38480;&#21046;&#26041;&#27861;&#12290;&#25105;&#20204;&#20851;&#27880;&#36817;&#22240;&#26524;&#23398;&#20064;&#35774;&#32622;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#30001;Fredholm&#31215;&#20998;&#26041;&#31243;&#34920;&#24449;&#30340;&#26356;&#24191;&#27867;&#31867;&#21035;&#30340;&#21453;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20026;&#35299;&#20915;&#38750;&#32447;&#24615;&#35774;&#32622;&#20013;&#30340;&#35813;&#38382;&#39064;&#25552;&#20379;&#20102;&#20004;&#38454;&#27573;&#21644;&#30697;&#38480;&#21046;&#26041;&#27861;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#27599;&#20010;&#31639;&#27861;&#37117;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#37117;&#21487;&#20197;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#26089;&#26399;&#19981;&#33021;&#21033;&#29992;&#20195;&#29702;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of causal effect estimation in the presence of unobserved confounding, but where proxies for the latent confounder(s) are observed. We propose two kernel-based methods for nonlinear causal effect estimation in this setting: (a) a two-stage regression approach, and (b) a maximum moment restriction approach. We focus on the proximal causal learning setting, but our methods can be used to solve a wider class of inverse problems characterised by a Fredholm integral equation. In particular, we provide a unifying view of two-stage and moment restriction approaches for solving this problem in a nonlinear setting. We provide consistency guarantees for each algorithm, and we demonstrate these approaches achieve competitive results on synthetic data and data simulating a real-world task. In particular, our approach outperforms earlier methods that are not suited to leveraging proxy variables.
&lt;/p&gt;</description></item><item><title>DeepEverest&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20540;&#19978;&#25191;&#34892;&#35299;&#37322;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#32034;&#24341;&#25216;&#26415;&#21644;&#26597;&#35810;&#25191;&#34892;&#31639;&#27861;&#65292;DeepEverest&#21487;&#20197;&#23558;&#21333;&#20010;&#26597;&#35810;&#21152;&#36895;&#39640;&#36798;63&#20493;&#65292;&#24182;&#22312;&#22810;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2104.02234</link><description>&lt;p&gt;
DeepEverest&#65306;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#22768;&#26126;&#24615;Top-K&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
DeepEverest: Accelerating Declarative Top-K Queries for Deep Neural Network Interpretation. (arXiv:2104.02234v8 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.02234
&lt;/p&gt;
&lt;p&gt;
DeepEverest&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20540;&#19978;&#25191;&#34892;&#35299;&#37322;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#32034;&#24341;&#25216;&#26415;&#21644;&#26597;&#35810;&#25191;&#34892;&#31639;&#27861;&#65292;DeepEverest&#21487;&#20197;&#23558;&#21333;&#20010;&#26597;&#35810;&#21152;&#36895;&#39640;&#36798;63&#20493;&#65292;&#24182;&#22312;&#22810;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#12289;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;DeepEverest&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#20540;&#19978;&#25191;&#34892;&#35299;&#37322;&#26597;&#35810;&#30340;&#39640;&#25928;&#31995;&#32479;&#12290;DeepEverest&#21253;&#25324;&#19968;&#31181;&#26377;&#25928;&#30340;&#32034;&#24341;&#25216;&#26415;&#21644;&#19968;&#31181;&#20855;&#26377;&#21508;&#31181;&#20248;&#21270;&#30340;&#26597;&#35810;&#25191;&#34892;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26597;&#35810;&#25191;&#34892;&#31639;&#27861;&#26159;&#23454;&#20363;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30340;&#21407;&#22411;&#23454;&#39564;&#34920;&#26126;&#65292;DeepEverest&#20165;&#20351;&#29992;&#19981;&#21040;&#23436;&#20840;&#26448;&#26009;&#21270;20%&#30340;&#23384;&#20648;&#31354;&#38388;&#65292;&#23601;&#33021;&#23558;&#21333;&#20010;&#26597;&#35810;&#21152;&#36895;&#39640;&#36798;63&#20493;&#65292;&#24182;&#22987;&#32456;&#22312;&#27169;&#25311;DNN&#35299;&#37322;&#36807;&#31243;&#30340;&#22810;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design, implement, and evaluate DeepEverest, a system for the efficient execution of interpretation by example queries over the activation values of a deep neural network. DeepEverest consists of an efficient indexing technique and a query execution algorithm with various optimizations. We prove that the proposed query execution algorithm is instance optimal. Experiments with our prototype show that DeepEverest, using less than 20% of the storage of full materialization, significantly accelerates individual queries by up to 63x and consistently outperforms other methods on multi-query workloads that simulate DNN interpretation processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#29702;&#35770;&#26694;&#26550;&#21644;&#25968;&#20540;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26102;&#38388;&#36793;&#32536;&#26679;&#26412;&#25512;&#26029;&#20986;&#38543;&#26426;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#29305;&#21035;&#26159;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#36712;&#36857;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2102.09204</link><description>&lt;p&gt;
&#25506;&#32034;&#36712;&#36857;&#25512;&#26029;&#30340;&#25968;&#23398;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Towards a mathematical theory of trajectory inference. (arXiv:2102.09204v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.09204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#29702;&#35770;&#26694;&#26550;&#21644;&#25968;&#20540;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26102;&#38388;&#36793;&#32536;&#26679;&#26412;&#25512;&#26029;&#20986;&#38543;&#26426;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#29305;&#21035;&#26159;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#36712;&#36857;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#21644;&#25968;&#20540;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38543;&#26426;&#36807;&#31243;&#30340;&#26102;&#38388;&#36793;&#32536;&#26679;&#26412;&#20013;&#25512;&#26029;&#20854;&#36712;&#36857;&#12290;&#36825;&#20010;&#38382;&#39064;&#20986;&#29616;&#22312;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#30340;&#20998;&#26512;&#20013;&#65292;&#23427;&#25552;&#20379;&#20102;&#32454;&#32990;&#29366;&#24577;&#30340;&#39640;&#32500;&#24230;&#27979;&#37327;&#65292;&#20294;&#19981;&#33021;&#36319;&#36394;&#32454;&#32990;&#30340;&#26102;&#38388;&#36712;&#36857;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#31867;&#38543;&#26426;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#26102;&#38388;&#36793;&#32536;&#30340;&#26377;&#38480;&#26679;&#26412;&#20013;&#24674;&#22797;&#20986;&#30495;&#23454;&#36712;&#36857;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#22312;&#23454;&#36341;&#20013;&#39640;&#25928;&#22320;&#25191;&#34892;&#27492;&#25805;&#20316;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#20840;&#23616;Waddington-OT(gWOT)&#65292;&#21487;&#20197;&#36890;&#36807;&#28041;&#21450;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#30340;&#25152;&#26377;&#26102;&#38388;&#28857;&#30340;&#20840;&#23616;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#38382;&#39064;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#39640;&#25928;&#22320;&#35299;&#20915;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#33391;&#22909;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We devise a theoretical framework and a numerical method to infer trajectories of a stochastic process from samples of its temporal marginals. This problem arises in the analysis of single cell RNA-sequencing data, which provide high dimensional measurements of cell states but cannot track the trajectories of the cells over time. We prove that for a class of stochastic processes it is possible to recover the ground truth trajectories from limited samples of the temporal marginals at each time-point, and provide an efficient algorithm to do so in practice. The method we develop, Global Waddington-OT (gWOT), boils down to a smooth convex optimization problem posed globally over all time-points involving entropy-regularized optimal transport. We demonstrate that this problem can be solved efficiently in practice and yields good reconstructions, as we show on several synthetic and real datasets.
&lt;/p&gt;</description></item><item><title>dame-flame&#26159;&#19968;&#20010;&#29992;&#20110;&#35266;&#23519;&#24615;&#22240;&#26524;&#25512;&#26029;&#21305;&#37197;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21305;&#37197;&#31639;&#27861;DAME&#21644;FLAME&#65292;&#29992;&#26426;&#22120;&#23398;&#20064;&#30830;&#23450;&#37325;&#35201;&#21327;&#21464;&#37327;&#36827;&#34892;&#21305;&#37197;&#65292;&#20135;&#29983;&#30340;&#32467;&#26524;&#26159;&#39640;&#36136;&#37327;&#19988;&#21487;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2101.01867</link><description>&lt;p&gt;
dame-flame&#65306;&#25552;&#20379;&#24555;&#36895;&#21487;&#35299;&#37322;&#21305;&#37197;&#30340;&#22240;&#26524;&#25512;&#26029;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
dame-flame: A Python Library Providing Fast Interpretable Matching for Causal Inference. (arXiv:2101.01867v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.01867
&lt;/p&gt;
&lt;p&gt;
dame-flame&#26159;&#19968;&#20010;&#29992;&#20110;&#35266;&#23519;&#24615;&#22240;&#26524;&#25512;&#26029;&#21305;&#37197;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21305;&#37197;&#31639;&#27861;DAME&#21644;FLAME&#65292;&#29992;&#26426;&#22120;&#23398;&#20064;&#30830;&#23450;&#37325;&#35201;&#21327;&#21464;&#37327;&#36827;&#34892;&#21305;&#37197;&#65292;&#20135;&#29983;&#30340;&#32467;&#26524;&#26159;&#39640;&#36136;&#37327;&#19988;&#21487;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
dame-flame&#26159;&#19968;&#20010;Python&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#22312;&#21253;&#21547;&#31163;&#25955;&#21327;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#35266;&#23519;&#24615;&#22240;&#26524;&#25512;&#26029;&#30340;&#21305;&#37197;&#12290;&#35813;&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#21160;&#24577;&#20960;&#20046;&#23436;&#20840;&#21305;&#37197;(DAME)&#21644;&#24555;&#36895;&#22823;&#35268;&#27169;&#20960;&#20046;&#23436;&#20840;&#21305;&#37197;(FLAME)&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#21327;&#21464;&#37327;&#30340;&#23376;&#38598;&#19978;&#21305;&#37197;&#27835;&#30103;&#21644;&#23545;&#29031;&#32452;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21305;&#37197;&#32452;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#22240;&#20026;&#21305;&#37197;&#26159;&#22312;&#21327;&#21464;&#37327;&#19978;&#36827;&#34892;&#30340;&#65292;&#24182;&#19988;&#26159;&#39640;&#36136;&#37327;&#30340;&#65292;&#22240;&#20026;&#26426;&#22120;&#23398;&#20064;&#21487;&#29992;&#20110;&#30830;&#23450;&#38656;&#35201;&#21305;&#37197;&#30340;&#37325;&#35201;&#21327;&#21464;&#37327;&#12290;DAME&#36890;&#36807;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#21482;&#35201;&#22312;&#26368;&#21487;&#33021;&#30340;&#21327;&#21464;&#37327;&#19978;&#21305;&#37197;&#21333;&#20301;&#65292;&#37325;&#35201;&#30340;&#21327;&#21464;&#37327;&#26377;&#20248;&#20808;&#21305;&#37197;&#30340;&#26426;&#20250;&#12290;FLAME&#36890;&#36807;&#26356;&#24555;&#30340;&#21521;&#21518;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#65292;&#36817;&#20284;&#20110;DAME&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#36719;&#20214;&#21253;&#25552;&#20379;&#22810;&#20010;&#21487;&#35843;&#21442;&#25968;&#20197;&#36866;&#24212;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21305;&#37197;&#20043;&#21518;&#35745;&#31639;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20540;&#12290;&#36825;&#20123;&#21442;&#25968;&#30340;&#25551;&#36848;&#20197;&#21450;&#26377;&#20851;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;&#30340;&#35814;&#32454;&#20449;&#24687;&#21487;&#20197;&#22312;&#36719;&#20214;&#21253;&#25991;&#26723;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
dame-flame is a Python package for performing matching for observational causal inference on datasets containing discrete covariates. This package implements the Dynamic Almost Matching Exactly (DAME) and Fast Large-Scale Almost Matching Exactly (FLAME) algorithms, which match treatment and control units on subsets of the covariates. The resulting matched groups are interpretable, because the matches are made on covariates, and high-quality, because machine learning is used to determine which covariates are important to match on. DAME solves an optimization problem that matches units on as many covariates as possible, prioritizing matches on important covariates. FLAME approximates the solution found by DAME via a much faster backward feature selection procedure. The package provides several adjustable parameters to adapt the algorithms to specific applications, and can calculate treatment effect estimates after matching. Descriptions of these parameters, details on estimating treatmen
&lt;/p&gt;</description></item></channel></rss>