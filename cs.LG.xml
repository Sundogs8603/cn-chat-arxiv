<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.20329</link><description>&lt;p&gt;
ReALM: &#21442;&#32771;&#35299;&#26512;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ReALM: Reference Resolution As Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#35299;&#26512;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#25104;&#21151;&#22788;&#29702;&#21508;&#31181;&#19978;&#19979;&#25991;&#33267;&#20851;&#37325;&#35201;&#12290; &#36825;&#31181;&#19978;&#19979;&#25991;&#26082;&#21253;&#25324;&#20808;&#21069;&#30340;&#23545;&#35805;&#65292;&#20063;&#21253;&#25324;&#19982;&#38750;&#23545;&#35805;&#23454;&#20307;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#29992;&#25143;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#25110;&#21518;&#21488;&#36816;&#34892;&#30340;&#23454;&#20307;&#12290; &#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#22312;&#21442;&#32771;&#35299;&#26512;&#20013;&#30340;&#36816;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#23545;&#35805;&#23454;&#20307;&#65292;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290; &#26412;&#25991;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#34987;&#29992;&#26469;&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#20854;&#20013;&#28041;&#21450;&#23631;&#24149;&#19978;&#30340;&#36825;&#31181;&#23454;&#20307;&#31561;&#20256;&#32479;&#19978;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#21442;&#32771;&#35299;&#26512;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#31995;&#32479;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20329v1 Announce Type: cross  Abstract: Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36208;&#34892;&#25805;&#32437;&#36807;&#31243;&#20998;&#35299;&#20026;&#20302;&#23618;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#21644;&#39640;&#23618;&#34892;&#20026;&#20811;&#38534;&#35268;&#21010;&#22120;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#30340;&#25805;&#32437;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.20328</link><description>&lt;p&gt;
&#20174;&#31034;&#33539;&#23398;&#20064;&#35270;&#35273;&#22235;&#36275;&#36208;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning Visual Quadrupedal Loco-Manipulation from Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36208;&#34892;&#25805;&#32437;&#36807;&#31243;&#20998;&#35299;&#20026;&#20302;&#23618;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#21644;&#39640;&#23618;&#34892;&#20026;&#20811;&#38534;&#35268;&#21010;&#22120;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#30340;&#25805;&#32437;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#26426;&#22120;&#20154;&#36880;&#28176;&#34987;&#25972;&#21512;&#36827;&#20154;&#31867;&#29615;&#22659;&#12290;&#23613;&#31649;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#34892;&#36208;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#20294;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#19982;&#29289;&#20307;&#30340;&#20114;&#21160;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#35753;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#30340;&#25805;&#32437;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#36208;&#34892;&#25805;&#32437;&#36807;&#31243;&#20998;&#35299;&#20026;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20302;&#23618;&#25511;&#21046;&#22120;&#21644;&#22522;&#20110;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#30340;&#39640;&#23618;&#35268;&#21010;&#22120;&#12290;&#36890;&#36807;&#21442;&#25968;&#21270;&#25805;&#32437;&#36712;&#36857;&#65292;&#25105;&#20204;&#21516;&#27493;&#19978;&#23618;&#21644;&#19979;&#23618;&#30340;&#21162;&#21147;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;RL&#21644;BC&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#21644;&#29616;&#23454;&#19990;&#30028;&#23454;&#39564;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20328v1 Announce Type: cross  Abstract: Quadruped robots are progressively being integrated into human environments. Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited. While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs. We decompose the loco-manipulation process into a low-level reinforcement learning (RL)-based controller and a high-level Behavior Cloning (BC)-based planner. By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC. Our approach is validated through simulations and real-world experiments, d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Transformer&#27169;&#22411;&#32467;&#21512;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#30340;SOZ&#26412;&#22320;&#21270;&#65292;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.20324</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#21387;&#22120;&#20174;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#20013;&#23450;&#20301;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306;
&lt;/p&gt;
&lt;p&gt;
Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Transformer&#27169;&#22411;&#32467;&#21512;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#30340;SOZ&#26412;&#22320;&#21270;&#65292;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#30142;&#30149;&#20043;&#19968;&#65292;&#35768;&#22810;&#24739;&#32773;&#22312;&#33647;&#29289;&#26080;&#27861;&#25511;&#21046;&#30315;&#30187;&#21457;&#20316;&#26102;&#38656;&#35201;&#25163;&#26415;&#24178;&#39044;&#12290;&#20026;&#20102;&#21462;&#24471;&#26377;&#25928;&#30340;&#25163;&#26415;&#32467;&#26524;&#65292;&#20934;&#30830;&#23450;&#20301;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306; - &#36890;&#24120;&#36817;&#20284;&#20026;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306; (SOZ) - &#33267;&#20851;&#37325;&#35201;&#20294;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#30005;&#21050;&#28608;&#36827;&#34892;&#20027;&#21160;&#25506;&#27979;&#24050;&#32463;&#25104;&#20026;&#35782;&#21035;&#30315;&#30187;&#21457;&#20316;&#21306;&#22495;&#30340;&#26631;&#20934;&#20020;&#24202;&#23454;&#36341;&#12290;&#26412;&#25991;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20351;&#29992;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608; (SPES) &#21709;&#24212;&#36827;&#34892; SOZ &#23450;&#20301;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#20445;&#30041;&#30340;&#24739;&#32773;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20324v1 Announce Type: new  Abstract: Epilepsy is one of the most common neurological disorders, and many patients require surgical intervention when medication fails to control seizures. For effective surgical outcomes, precise localisation of the epileptogenic focus - often approximated through the Seizure Onset Zone (SOZ) - is critical yet remains a challenge. Active probing through electrical stimulation is already standard clinical practice for identifying epileptogenic areas. This paper advances the application of deep learning for SOZ localisation using Single Pulse Electrical Stimulation (SPES) responses. We achieve this by introducing Transformer models that incorporate cross-channel attention. We evaluate these models on held-out patient test sets to assess their generalisability to unseen patients and electrode placements.   Our study makes three key contributions: Firstly, we implement an existing deep learning model to compare two SPES analysis paradigms - namel
&lt;/p&gt;</description></item><item><title>MTLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20302;&#31209;&#36866;&#24212;&#27169;&#22359;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#21442;&#25968;&#31354;&#38388;&#30340;&#20998;&#31163;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#20219;&#21153;&#19987;&#19994;&#21270;&#21644;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;</title><link>https://arxiv.org/abs/2403.20320</link><description>&lt;p&gt;
MTLoRA: &#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20320
&lt;/p&gt;
&lt;p&gt;
MTLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20302;&#31209;&#36866;&#24212;&#27169;&#22359;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#21442;&#25968;&#31354;&#38388;&#30340;&#20998;&#31163;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#20219;&#21153;&#19987;&#19994;&#21270;&#21644;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#19981;&#21516;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#24335;&#65292;&#21516;&#26102;&#20165;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#26159;&#20026;&#21333;&#20219;&#21153;&#36866;&#24212;&#32780;&#35774;&#35745;&#30340;&#65292;&#20294;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26550;&#26500;&#20013;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MTLoRA&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#12290;MTLoRA&#37319;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20302;&#31209;&#36866;&#24212;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#20998;&#24320;&#20102;MTL&#24494;&#35843;&#20013;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#29087;&#32451;&#22788;&#29702;MTL&#19978;&#19979;&#25991;&#20013;&#30340;&#20219;&#21153;&#19987;&#38376;&#21270;&#21644;&#20132;&#20114;&#12290;&#25105;&#20204;&#23558;MTLoRA&#24212;&#29992;&#20110;&#22522;&#20110;&#20998;&#23618;&#21464;&#21387;&#22120;&#30340;MTL&#26550;&#26500;&#65292;&#23558;&#23427;&#20204;&#35843;&#25972;&#21040;&#22810;&#20010;&#19979;&#28216;&#23494;&#38598;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20320v1 Announce Type: cross  Abstract: Adapting models pre-trained on large-scale datasets to a variety of downstream tasks is a common strategy in deep learning. Consequently, parameter-efficient fine-tuning methods have emerged as a promising way to adapt pre-trained models to different tasks while training only a minimal number of parameters. While most of these methods are designed for single-task adaptation, parameter-efficient training in Multi-Task Learning (MTL) architectures is still unexplored. In this paper, we introduce MTLoRA, a novel framework for parameter-efficient training of MTL models. MTLoRA employs Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively disentangle the parameter space in MTL fine-tuning, thereby enabling the model to adeptly handle both task specialization and interaction within MTL contexts. We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting them to multiple downstream dense predictio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;CDR&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#65292;&#36991;&#20813;&#20256;&#32479;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.20298</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#23884;&#20837;&#21644;&#23618;&#27425;&#24863;&#30693;&#22495;&#35299;&#32806;&#30340;&#22522;&#20110;&#35780;&#35770;&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;CDR&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#65292;&#36991;&#20813;&#20256;&#32479;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#23545;&#25512;&#33616;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#30340;&#31639;&#27861;&#65292;&#20197;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#21560;&#24341;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#25429;&#25417;&#21487;&#22312;&#39046;&#22495;&#38388;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20174;&#26356;&#20016;&#23500;&#30340;&#39046;&#22495;&#65288;&#28304;&#39046;&#22495;&#65289;&#36716;&#31227;&#21040;&#26356;&#31232;&#30095;&#30340;&#39046;&#22495;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#27431;&#20960;&#37324;&#24503;&#23884;&#20837;&#31354;&#38388;&#65292;&#22312;&#20934;&#30830;&#34920;&#31034;&#26356;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22788;&#29702;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#20513;&#23548;&#19968;&#31181;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#30340;&#21452;&#26354;CDR&#26041;&#27861;&#26469;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#12290;&#39318;&#20808;&#24378;&#35843;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#21452;&#26354;&#20960;&#20309;&#20013;&#23545;&#23567;&#20462;&#25913;&#36896;&#25104;&#30340;&#24178;&#25200;&#20250;&#34987;&#25918;&#22823;&#65292;&#26368;&#32456;&#23548;&#33268;&#23618;&#27425;&#24615;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20298v1 Announce Type: cross  Abstract: The issue of data sparsity poses a significant challenge to recommender systems. In response to this, algorithms that leverage side information such as review texts have been proposed. Furthermore, Cross-Domain Recommendation (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention. Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items. This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships. We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic geometry result in magnified perturbations, ultimately leading to the collapse of hierarchical 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20287</link><description>&lt;p&gt;
&#22522;&#20934;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Counterfactual Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#22312;&#29702;&#35299;&#21464;&#37327;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#29983;&#25104;&#26080;&#20559;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#22270;&#20687;&#29983;&#25104;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#35780;&#20272;&#23545;&#29031;&#29983;&#25104;&#30340;&#38656;&#27714;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#23545;&#29031;&#24773;&#26223;&#26159;&#27809;&#26377;&#21487;&#35266;&#27979;&#22522;&#20934;&#20107;&#23454;&#30340;&#20551;&#35774;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#39062;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20391;&#37325;&#20110;&#35780;&#20272;&#23545;&#29031;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20363;&#22914;&#32452;&#25104;&#12289;&#26377;&#25928;&#24615;&#12289;&#24178;&#39044;&#30340;&#26368;&#23567;&#24615;&#21644;&#22270;&#20687;&#36924;&#30495;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#33539;&#24335;&#30340;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#31867;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#37197;&#22791;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20287v1 Announce Type: cross  Abstract: Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further eval
&lt;/p&gt;</description></item><item><title>LayerNorm&#22312;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#65292;&#24494;&#35843;output LayerNorm&#32780;&#20445;&#25345;&#20854;&#20182;&#37096;&#20998;&#19981;&#21464;&#36275;&#20197;&#22312;&#22810;&#20010;GLUE&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#21147;</title><link>https://arxiv.org/abs/2403.20284</link><description>&lt;p&gt;
LayerNorm&#65306;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
LayerNorm: A key component in parameter-efficient fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20284
&lt;/p&gt;
&lt;p&gt;
LayerNorm&#22312;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#65292;&#24494;&#35843;output LayerNorm&#32780;&#20445;&#25345;&#20854;&#20182;&#37096;&#20998;&#19981;&#21464;&#36275;&#20197;&#22312;&#22810;&#20010;GLUE&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#65288;&#21253;&#25324;BERT&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#65292;&#24494;&#35843;&#36807;&#31243;&#32791;&#36153;&#20102;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#21560;&#24341;&#20154;&#26041;&#27861;&#26159;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#21363;&#20165;&#20462;&#25913;&#27169;&#22411;&#30340;&#26368;&#23567;&#37096;&#20998;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20313;&#37096;&#20998;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;BERT&#27169;&#22411;&#30340;&#21738;&#20010;&#37096;&#20998;&#23545;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;BERT&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#32452;&#20214;&#65292;&#20197;&#26597;&#26126;&#22312;&#24494;&#35843;&#21518;&#21738;&#20123;&#32452;&#20214;&#21457;&#29983;&#20102;&#26368;&#26174;&#33879;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#38024;&#23545;&#19981;&#21516;General Language Understanding Evaluation&#65288;GLUE&#65289;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#36755;&#20986;&#30340;LayerNorm&#21457;&#29983;&#30340;&#21464;&#21270;&#27604;&#20854;&#20182;&#32452;&#20214;&#37117;&#35201;&#22823;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20165;&#24494;&#35843;output LayerNorm&#32780;&#20445;&#25345;&#20854;&#20182;&#37096;&#20998;&#19981;&#21464;&#23601;&#36275;&#20197;&#22312;&#22810;&#20010;GLUE&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20284v1 Announce Type: new  Abstract: Fine-tuning a pre-trained model, such as Bidirectional Encoder Representations from Transformers (BERT), has been proven to be an effective method for solving many natural language processing (NLP) tasks. However, due to the large number of parameters in many state-of-the-art NLP models, including BERT, the process of fine-tuning is computationally expensive. One attractive solution to this issue is parameter-efficient fine-tuning, which involves modifying only a minimal segment of the model while keeping the remainder unchanged. Yet, it remains unclear which segment of the BERT model is crucial for fine-tuning. In this paper, we first analyze different components in the BERT model to pinpoint which one undergoes the most significant changes after fine-tuning. We find that output LayerNorm changes more than any other components when fine-tuned for different General Language Understanding Evaluation (GLUE) tasks. Then we show that only fi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20280</link><description>&lt;p&gt;
&#24102;&#26377;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#31232;&#30095;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Sparse multimodal fusion with modal channel attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20280
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#37327;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#20316;&#20026;&#27169;&#24577;&#31232;&#30095;&#24230;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#20013;&#24341;&#20837;&#20102;&#27169;&#24577;&#19981;&#23436;&#20840;&#36890;&#36947;&#65292;&#31216;&#20026;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#12290;&#20351;&#29992;&#20102;&#21253;&#21547;4&#31181;&#27169;&#24577;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;CMU-MOSEI&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;TCGA&#29992;&#20110;&#22810;&#32452;&#23398;&#12290;&#27169;&#22411;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#26679;&#26412;&#20013;&#21482;&#29992;&#20102;&#22235;&#31181;&#27169;&#24577;&#20013;&#30340;&#20004;&#31181;&#23601;&#23398;&#20064;&#20986;&#32479;&#19968;&#19988;&#23545;&#40784;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#21457;&#29616;&#65292;&#21363;&#20351;&#27809;&#26377;&#27169;&#24577;&#31232;&#30095;&#65292;&#25152;&#25552;&#20986;&#30340;MCA&#26426;&#21046;&#20063;&#33021;&#25552;&#39640;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#65292;&#21484;&#22238;&#25351;&#26631;&#65292;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20280v1 Announce Type: cross  Abstract: The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.
&lt;/p&gt;</description></item><item><title>Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.20266</link><description>&lt;p&gt;
Latxa: &#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Latxa: An Open Language Model and Evaluation Suite for Basque
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20266
&lt;/p&gt;
&lt;p&gt;
Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Latxa&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Llama 2&#30340;&#22823;&#22411;&#24052;&#26031;&#20811;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7&#21040;700&#20159;&#12290;Latxa&#22522;&#20110;&#26032;&#30340;&#24052;&#26031;&#20811;&#35821;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;430&#19975;&#20010;&#25991;&#26723;&#21644;42&#20159;&#20010;&#26631;&#35760;&#12290;&#38024;&#23545;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;4&#20010;&#22810;&#39033;&#36873;&#25321;&#35780;&#20272;&#25968;&#25454;&#38598;&#65306;EusProficiency&#65292;&#21253;&#25324;&#26469;&#33258;&#23448;&#26041;&#35821;&#35328;&#33021;&#21147;&#32771;&#35797;&#30340;5169&#20010;&#38382;&#39064;&#65307;EusReading&#65292;&#21253;&#25324;352&#20010;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65307;EusTrivia&#65292;&#21253;&#25324;&#26469;&#33258;5&#20010;&#30693;&#35782;&#39046;&#22495;&#30340;1715&#20010;&#29712;&#20107;&#38382;&#39064;&#65307;&#20197;&#21450;EusExams&#65292;&#21253;&#25324;&#26469;&#33258;&#20844;&#20849;&#32771;&#35797;&#30340;16774&#20010;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#20013;&#65292;Latxa&#22312;&#19982;&#25105;&#20204;&#27604;&#36739;&#30340;&#25152;&#26377;&#20808;&#21069;&#24320;&#25918;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#38405;&#35835;&#29702;&#35299;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26041;&#38754;&#33853;&#21518;&#65292;&#20294;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;&#23427;&#19982;GPT-4 Turbo&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;Latxa&#27169;&#22411;&#31995;&#21015;&#65292;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20266v1 Announce Type: cross  Abstract: We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.20262</link><description>&lt;p&gt;
ELITR-Bench: &#38754;&#21521;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#20250;&#35758;&#21161;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20027;&#35201;&#33268;&#21147;&#20110;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#38271;&#25991;&#26723;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#38271;&#36317;&#31163;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#20294;&#29616;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#32771;&#34385;&#30340;&#26159;&#19981;&#19968;&#23450;&#19982;&#29616;&#23454;&#24212;&#29992;&#30456;&#20851;&#30340;&#36890;&#29992;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#30340;&#26032;&#22522;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#65292;&#38271;&#19978;&#19979;&#25991;&#30001;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#33719;&#24471;&#30340;&#36716;&#24405;&#32452;&#25104;&#65292;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#22266;&#26377;&#22024;&#26434;&#24615;&#21644;&#21475;&#35821;&#29305;&#24615;&#65292;&#36825;&#20026;LLMs&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;ELITR-Bench&#65292;&#36890;&#36807;271&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21450;&#20854;&#30495;&#23454;&#31572;&#26696;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;ELITR&#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#12290;&#25105;&#20204;&#22312;ELITR-Bench&#19978;&#23545;&#26368;&#26032;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20262v1 Announce Type: cross  Abstract: Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, e
&lt;/p&gt;</description></item><item><title>FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.20261</link><description>&lt;p&gt;
FABind+: &#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#22686;&#24378;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20261
&lt;/p&gt;
&lt;p&gt;
FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#25216;&#26415;&#20381;&#36182;&#20110;&#21463;&#29289;&#29702;&#21407;&#29702;&#25903;&#37197;&#30340;&#24191;&#27867;&#37319;&#26679;&#21644;&#27169;&#25311;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21069;&#26223;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#22686;&#38271;&#12290;&#24314;&#31435;&#22312;FABind&#30340;&#22522;&#30784;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind+&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22823;&#25552;&#21319;&#20854;&#21069;&#36523;&#24615;&#33021;&#30340;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30830;&#23450;&#21475;&#34955;&#39044;&#27979;&#26159;&#20998;&#23376;&#23545;&#25509;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23545;&#25509;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23545;&#25509;&#27169;&#22359;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#20854;&#23039;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#20256;&#32479;&#37319;&#26679;/&#29983;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MedCLIP-SAM&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CLIP&#21644;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20020;&#24202;&#22270;&#20687;&#20998;&#21106;</title><link>https://arxiv.org/abs/2403.20253</link><description>&lt;p&gt;
MedCLIP-SAM&#65306;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#34892;&#26725;&#25509;&#65292;&#23454;&#29616;&#36890;&#29992;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MedCLIP-SAM&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CLIP&#21644;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20020;&#24202;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#35299;&#21078;&#32467;&#26500;&#21644;&#30149;&#21464;&#30340;&#20998;&#21106;&#22312;&#29616;&#20195;&#20020;&#24202;&#35786;&#26029;&#12289;&#30142;&#30149;&#30740;&#31350;&#21644;&#27835;&#30103;&#35268;&#21010;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;MedCLIP-SAM&#65292;&#32467;&#21512;&#20102;CLIP&#21644;SAM&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20020;&#24202;&#25195;&#25551;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20253v1 Announce Type: cross  Abstract: Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLMs&#24314;&#27169;&#30446;&#26631;&#20154;&#32676;&#30340;&#20449;&#24565;&#21644;&#20559;&#22909;&#65292;&#26088;&#22312;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#65292;&#35780;&#20272;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#26816;&#39564;&#20854;&#22312;&#21305;&#37197;&#30495;&#23454;&#20154;&#31867;&#21463;&#35775;&#32773;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.20252</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#24314;&#27169;&#30446;&#26631;&#20154;&#32676;&#30340;&#20449;&#24565;&#21644;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to Model the Beliefs and Preferences of Targeted Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20252
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLMs&#24314;&#27169;&#30446;&#26631;&#20154;&#32676;&#30340;&#20449;&#24565;&#21644;&#20559;&#22909;&#65292;&#26088;&#22312;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#65292;&#35780;&#20272;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#26816;&#39564;&#20854;&#22312;&#21305;&#37197;&#30495;&#23454;&#20154;&#31867;&#21463;&#35775;&#32773;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#19982;&#20154;&#32676;&#30340;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#24314;&#27169;&#29305;&#23450;&#20154;&#32676;&#30340;&#20449;&#24565;&#12289;&#20559;&#22909;&#21644;&#34892;&#20026;&#23545;&#20110;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#21487;&#33021;&#24456;&#26377;&#29992;&#65292;&#27604;&#22914;&#20026;&#26032;&#20135;&#21697;&#24320;&#23637;&#27169;&#25311;&#28966;&#28857;&#23567;&#32452;&#12289;&#36827;&#34892;&#34394;&#25311;&#35843;&#26597;&#20197;&#21450;&#27979;&#35797;&#34892;&#20026;&#24178;&#39044;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26114;&#36149;&#12289;&#19981;&#20999;&#23454;&#38469;&#25110;&#19981;&#36947;&#24503;&#30340;&#24178;&#39044;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#20351;&#29992;LLMs&#20934;&#30830;&#24314;&#27169;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#65292;&#35780;&#20272;&#24471;&#21040;&#30340;&#20154;&#32676;&#22312;&#21305;&#37197;&#23545;&#30005;&#27744;&#30005;&#21160;&#27773;&#36710;(BEVs)&#20559;&#22909;&#35843;&#26597;&#20013;&#30495;&#23454;&#20154;&#31867;&#21463;&#35775;&#32773;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#19982;&#25972;&#20307;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20197;&#21450;&#20010;&#20307;&#22238;&#24212;&#30456;&#21305;&#37197;&#65292;&#24182;&#30740;&#31350;LLMs&#22312;&#24314;&#27169;&#20154;&#32676;&#20449;&#24565;&#21644;&#20559;&#22909;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20252v1 Announce Type: cross  Abstract: We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22810;&#21160;&#20316;&#22330;&#26223;&#20013;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#20272;&#35745;&#12289;&#39118;&#38505;&#20559;&#22909;&#21644;&#28508;&#22312;&#25925;&#38556;&#19977;&#20010;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.20250</link><description>&lt;p&gt;
&#22810;&#21160;&#20316;&#22330;&#26223;&#20013;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#65306;&#20272;&#35745;&#12289;&#39118;&#38505;&#20559;&#22909;&#21644;&#28508;&#22312;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
Optimal Policy Learning with Observational Data in Multi-Action Scenarios: Estimation, Risk Preference, and Potential Failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22810;&#21160;&#20316;&#22330;&#26223;&#20013;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#20272;&#35745;&#12289;&#39118;&#38505;&#20559;&#22909;&#21644;&#28508;&#22312;&#25925;&#38556;&#19977;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#65288;OPL&#65289;&#65292;&#21363;&#25968;&#25454;&#39537;&#21160;&#30340;&#26368;&#20248;&#20915;&#31574;&#65292;&#22312;&#22810;&#21160;&#20316;&#65288;&#25110;&#22810;&#33218;&#65289;&#35774;&#32622;&#20013;&#65292;&#26377;&#38480;&#30340;&#20915;&#31574;&#36873;&#39033;&#21487;&#20379;&#36873;&#25321;&#12290;&#25991;&#31456;&#20998;&#20026;&#19977;&#20010;&#37096;&#20998;&#65292;&#20998;&#21035;&#35752;&#35770;&#65306;&#20272;&#35745;&#12289;&#39118;&#38505;&#20559;&#22909;&#21644;&#28508;&#22312;&#25925;&#38556;&#12290;&#31532;&#19968;&#37096;&#20998;&#31616;&#35201;&#22238;&#39038;&#20102;&#22312;&#36825;&#31181;&#20998;&#26512;&#32972;&#26223;&#19979;&#20272;&#35745;&#22870;&#21169;&#65288;&#25110;&#20540;&#65289;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#31532;&#20108;&#37096;&#20998;&#28145;&#20837;&#20998;&#26512;&#20102;&#20915;&#31574;&#39118;&#38505;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#20915;&#31574;&#32773;&#23545;&#39118;&#38505;&#30340;&#24577;&#24230;&#21487;&#20197;&#24433;&#21709;&#26368;&#20248;&#36873;&#25321;&#65292;&#20855;&#20307;&#20307;&#29616;&#22312;&#22870;&#21169;&#26465;&#20214;&#22343;&#20540;&#19982;&#26465;&#20214;&#26041;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#36825;&#37324;&#65292;&#20316;&#32773;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20250v1 Announce Type: cross  Abstract: This paper deals with optimal policy learning (OPL) with observational data, i.e. data-driven optimal decision-making, in multi-action (or multi-arm) settings, where a finite set of decision options is available. It is organized in three parts, where I discuss respectively: estimation, risk preference, and potential failures. The first part provides a brief review of the key approaches to estimating the reward (or value) function and optimal policy within this context of analysis. Here, I delineate the identification assumptions and statistical properties related to offline optimal policy learning estimators. In the second part, I delve into the analysis of decision risk. This analysis reveals that the optimal choice can be influenced by the decision maker's attitude towards risks, specifically in terms of the trade-off between reward conditional mean and conditional variance. Here, I present an application of the proposed model to rea
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31867;&#21644;&#29305;&#24449;&#36136;&#24515;&#22686;&#24378;&#38477;&#32500;&#25955;&#28857;&#22270;&#65292;&#25552;&#39640;&#25955;&#28857;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.20246</link><description>&lt;p&gt;
&#20511;&#21161;&#31867;&#21644;&#29305;&#24449;&#36136;&#24515;&#22686;&#24378;&#38477;&#32500;&#25955;&#28857;&#22270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Dimension-Reduced Scatter Plots with Class and Feature Centroids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20246
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31867;&#21644;&#29305;&#24449;&#36136;&#24515;&#22686;&#24378;&#38477;&#32500;&#25955;&#28857;&#22270;&#65292;&#25552;&#39640;&#25955;&#28857;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20246v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22312;&#39640;&#32500;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#65292;&#38477;&#32500;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20197;&#25552;&#39640;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#24403;&#25968;&#25454;&#38598;&#34987;&#38477;&#20302;&#21040;&#20004;&#20010;&#32500;&#24230;&#26102;&#65292;&#27599;&#20010;&#35266;&#27979;&#20540;&#34987;&#20998;&#37197;&#19968;&#20010;x&#21644;y&#22352;&#26631;&#65292;&#24182;&#34920;&#31034;&#20026;&#25955;&#28857;&#22270;&#19978;&#30340;&#19968;&#20010;&#28857;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#35299;&#37322;x&#21644;y&#36724;&#30340;&#21547;&#20041;&#65292;&#36825;&#26159;&#30001;&#20110;&#38477;&#32500;&#26412;&#36523;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#25152;&#33268;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20174;&#38477;&#32500;&#24471;&#21040;&#30340;x&#21644;y&#22352;&#26631;&#26469;&#35745;&#31639;&#31867;&#21644;&#29305;&#24449;&#36136;&#24515;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36825;&#20123;&#36136;&#24515;&#21487;&#20197;&#21472;&#21152;&#21040;&#25955;&#28857;&#22270;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#20302;&#32500;&#31354;&#38388;&#19982;&#21407;&#22987;&#39640;&#32500;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#19977;&#31181;&#31070;&#32463;&#36951;&#20256;&#30142;&#30149;&#34920;&#22411;&#30340;&#25968;&#25454;&#38416;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#28155;&#21152;&#31867;&#21644;&#29305;&#24449;&#36136;&#24515;&#22914;&#20309;&#22686;&#21152;&#25955;&#28857;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20246v1 Announce Type: new  Abstract: Dimension reduction is increasingly applied to high-dimensional biomedical data to improve its interpretability. When datasets are reduced to two dimensions, each observation is assigned an x and y coordinates and is represented as a point on a scatter plot. A significant challenge lies in interpreting the meaning of the x and y axes due to the complexities inherent in dimension reduction. This study addresses this challenge by using the x and y coordinates derived from dimension reduction to calculate class and feature centroids, which can be overlaid onto the scatter plots. This method connects the low-dimension space to the original high-dimensional space. We illustrate the utility of this approach with data derived from the phenotypes of three neurogenetic diseases and demonstrate how the addition of class and feature centroids increases the interpretability of scatter plots.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.20233</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Bilevel Optimization for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20233
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#26032;&#30340;&#20989;&#25968;&#35270;&#35282;&#65292;&#20854;&#20013;&#20869;&#37096;&#30446;&#26631;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#34987;&#26368;&#23567;&#21270;&#12290;&#36825;&#20123;&#31867;&#22411;&#30340;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#22312;&#21442;&#25968;&#35774;&#32622;&#19979;&#24320;&#21457;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#20869;&#37096;&#30446;&#26631;&#23545;&#20110;&#39044;&#27979;&#20989;&#25968;&#30340;&#21442;&#25968;&#24378;&#20984;&#12290;&#20989;&#25968;&#35270;&#35282;&#19981;&#20381;&#36182;&#20110;&#27492;&#20551;&#35774;&#65292;&#29305;&#21035;&#20801;&#35768;&#20351;&#29992;&#36229;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20869;&#37096;&#39044;&#27979;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#36866;&#21512;&#33258;&#28982;&#20989;&#25968;&#21452;&#23618;&#32467;&#26500;&#30340;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20233v1 Announce Type: cross  Abstract: In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;&#21487;&#37325;&#26500;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;EfficientViT&#65292;&#22312;&#30828;&#20214;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2403.20230</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;FPGA&#30340;&#21487;&#37325;&#26500;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#21367;&#31215;-Transformer&#28151;&#21512;EfficientViT
&lt;/p&gt;
&lt;p&gt;
An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;&#21487;&#37325;&#26500;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;EfficientViT&#65292;&#22312;&#30828;&#20214;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViT&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23494;&#38598;&#30340;&#35745;&#31639;&#21644;&#24040;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#25361;&#25112;&#20102;ViTs&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;ViTs&#12290;&#22312;&#20854;&#20013;&#65292;EfficientViT&#26159;&#26368;&#20808;&#36827;&#30340;&#19968;&#31181;&#65292;&#20855;&#26377;&#21367;&#31215;-Transformer&#28151;&#21512;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#26550;&#26500;&#65292;&#29616;&#26377;&#30340;&#21152;&#36895;&#22120;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;EfficientViT&#30340;&#30828;&#20214;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;EfficientViT&#21152;&#36895;&#22120;&#65292;&#20197;&#25512;&#36827;ViTs&#30340;&#30828;&#20214;&#25928;&#29575;&#21069;&#27839;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#37325;&#26500;&#26550;&#26500;&#65292;&#26377;&#25928;&#25903;&#25345;&#21508;&#31181;&#25805;&#20316;&#31867;&#22411;&#65292;&#21253;&#25324;&#36731;&#37327;&#32423;&#21367;&#31215;&#21644;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;&#30828;&#20214;&#21033;&#29992;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#20998;&#22797;&#29992;&#21644;&#27969;&#27700;&#32447;&#25968;&#25454;&#27969;&#65292;&#20419;&#36827;&#20102;&#23618;&#20869;&#21644;&#23618;&#38388;&#34701;&#21512;&#65292;&#20943;&#23569;&#20102;&#29255;&#22806;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20230v1 Announce Type: cross  Abstract: Vision Transformers (ViTs) have achieved significant success in computer vision. However, their intensive computations and massive memory footprint challenge ViTs' deployment on embedded devices, calling for efficient ViTs. Among them, EfficientViT, the state-of-the-art one, features a Convolution-Transformer hybrid architecture, enhancing both accuracy and hardware efficiency. Unfortunately, existing accelerators cannot fully exploit the hardware benefits of EfficientViT due to its unique architecture. In this paper, we propose an FPGA-based accelerator for EfficientViT to advance the hardware efficiency frontier of ViTs. Specifically, we design a reconfigurable architecture to efficiently support various operation types, including lightweight convolutions and attention, boosting hardware utilization. Additionally, we present a time-multiplexed and pipelined dataflow to facilitate both intra- and inter-layer fusions, reducing off-chip
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;GRADE&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32858;&#21512;-&#25193;&#25955;&#26041;&#31243;&#23454;&#29616;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#20122;&#31283;&#23450;&#24615;&#65292;&#33021;&#22815;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.20221</link><description>&lt;p&gt;
&#24102;&#38544;&#34109;&#29366;&#24577;&#30340;&#22270;&#31070;&#32463;&#32858;&#21512;-&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Aggregation-diffusion with Metastability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20221
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;GRADE&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32858;&#21512;-&#25193;&#25955;&#26041;&#31243;&#23454;&#29616;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#20122;&#31283;&#23450;&#24615;&#65292;&#33021;&#22815;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24494;&#20998;&#26041;&#31243;&#30340;&#36830;&#32493;&#22270;&#31070;&#32463;&#27169;&#22411;&#25193;&#23637;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26550;&#26500;&#12290;&#30001;&#20110;&#22270;&#25193;&#25955;&#19982;&#20449;&#24687;&#20256;&#36882;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#33258;&#28982;&#22320;&#23558;&#31995;&#32479;&#25512;&#21521;&#24179;&#34913;&#29366;&#24577;&#65292;&#23548;&#33268;&#38382;&#39064;&#65292;&#22914;&#36807;&#24230;&#24179;&#28369;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#22270;&#32858;&#21512;-&#25193;&#25955;&#26041;&#31243;&#21551;&#21457;&#30340;GRADE&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#32447;&#24615;&#25193;&#25955;&#21644;&#30456;&#20114;&#20316;&#29992;&#21183;&#24341;&#36215;&#30340;&#32858;&#21512;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#36890;&#36807;&#32858;&#21512;-&#25193;&#25955;&#26041;&#31243;&#33719;&#24471;&#30340;&#33410;&#28857;&#34920;&#31034;&#34920;&#29616;&#20986;&#20122;&#31283;&#24577;&#65292;&#34920;&#26126;&#29305;&#24449;&#21487;&#20197;&#32858;&#21512;&#25104;&#22810;&#20010;&#31751;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31751;&#20869;&#30340;&#21160;&#24577;&#21487;&#20197;&#25345;&#32493;&#24456;&#38271;&#26102;&#38388;&#65292;&#26377;&#26395;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27169;&#22411;&#20013;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#25512;&#24191;&#20102;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#24182;&#30830;&#31435;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20221v1 Announce Type: cross  Abstract: Continuous graph neural models based on differential equations have expanded the architecture of graph neural networks (GNNs). Due to the connection between graph diffusion and message passing, diffusion-based models have been widely studied. However, diffusion naturally drives the system towards an equilibrium state, leading to issues like over-smoothing. To this end, we propose GRADE inspired by graph aggregation-diffusion equations, which includes the delicate balance between nonlinear diffusion and aggregation induced by interaction potentials. The node representations obtained through aggregation-diffusion equations exhibit metastability, indicating that features can aggregate into multiple clusters. In addition, the dynamics within these clusters can persist for long time periods, offering the potential to alleviate over-smoothing effects. This nonlinear diffusion in our model generalizes existing diffusion-based models and estab
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.20212</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#23610;&#23544;&#21644;&#38590;&#24230;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20212
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29992;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#29983;&#25104;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23884;&#20837;&#26469;&#26500;&#24314;&#19968;&#20010;&#28909;&#22270;&#65292;&#25351;&#31034;&#27599;&#26465;&#36793;&#25104;&#20026;&#26368;&#20339;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#24212;&#29992;&#23616;&#37096;&#25628;&#32034;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#35757;&#32451;&#23454;&#20363;&#22823;&#23567;&#12289;&#23884;&#20837;&#32500;&#25968;&#21644;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;&#19981;&#21516;&#20998;&#24067;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#21508;&#31181;&#20998;&#24067;&#30340;&#38590;&#24230;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#38590;&#24230;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20212v1 Announce Type: new  Abstract: We study the generalization capability of Unsupervised Learning in solving the Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN) trained with a surrogate loss function to generate an embedding for each node. We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route. We then apply local search to generate our final predictions. Our investigation explores how different training instance sizes, embedding dimensions, and distributions influence the outcomes of Unsupervised Learning methods. Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model's ability to solve TSP. Furthermore, in evaluating generalization across different distributions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.20208</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#39044;&#27979;&#34920;&#26684;&#20219;&#21153;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#32570;&#22833;&#20540;&#22635;&#20805;&#31561;&#39044;&#27979;&#20219;&#21153;&#26159;&#19982;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#20219;&#21153;&#12290;&#23613;&#31649;LLMs&#25797;&#38271;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#24102;&#26377;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;Llama-2&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;-shot&#39044;&#27979;&#12289;&#23569;-shot&#39044;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#26368;&#24120;&#29992;&#20316;&#38899;&#39057;&#22788;&#29702;&#20219;&#21153;&#20013;&#20449;&#21495;&#20998;&#35299;&#26041;&#27861;&#30340;&#20613;&#37324;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#30340;&#31616;&#27905;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#21487; intelligibility &#30340;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.20202</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35821;&#38899;&#20449;&#21495;&#22788;&#29702;&#12290;&#20197;&#35828;&#35805;&#32773;&#38548;&#31163;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Voice Signal Processing for Machine Learning. The Case of Speaker Isolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#26368;&#24120;&#29992;&#20316;&#38899;&#39057;&#22788;&#29702;&#20219;&#21153;&#20013;&#20449;&#21495;&#20998;&#35299;&#26041;&#27861;&#30340;&#20613;&#37324;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#30340;&#31616;&#27905;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#21487; intelligibility &#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#21161;&#25163;&#30340;&#24191;&#27867;&#20351;&#29992;&#20197;&#21450;&#20854;&#20182;&#26368;&#36817;&#30340;&#25216;&#26415;&#21457;&#23637;&#22686;&#21152;&#20102;&#23545;&#22788;&#29702;&#38899;&#39057;&#20449;&#21495;&#21644;&#29305;&#21035;&#26159;&#20154;&#31867;&#35821;&#38899;&#30340;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25191;&#34892;&#12290;&#23613;&#31649;&#23384;&#22312;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20294;&#36866;&#24403;&#39044;&#22788;&#29702;&#20449;&#21495;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#20854;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;ML&#27169;&#22411;&#21644;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#20174;&#20107;&#36825;&#31867;&#20219;&#21153;&#30340;ML&#24037;&#31243;&#24072;&#21487;&#33021;&#27809;&#26377;&#20449;&#21495;&#22788;&#29702;&#32972;&#26223;&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#19981;&#21516;&#30340;&#19987;&#19994;&#39046;&#22495;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#23545;&#20613;&#37324;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#36827;&#34892;&#31616;&#27905;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#36825;&#26159;&#29992;&#20110;&#38899;&#39057;&#22788;&#29702;&#20219;&#21153;&#30340;&#26368;&#24120;&#29992;&#30340;&#20449;&#21495;&#20998;&#35299;&#26041;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#21487; intelligibility &#30340;&#25351;&#26631;&#65292;&#21363;&#19981;&#21464;&#23610;&#24230;&#30340;&#20449;&#21495;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20202v1 Announce Type: cross  Abstract: The widespread use of automated voice assistants along with other recent technological developments have increased the demand for applications that process audio signals and human voice in particular. Voice recognition tasks are typically performed using artificial intelligence and machine learning models. Even though end-to-end models exist, properly pre-processing the signal can greatly reduce the complexity of the task and allow it to be solved with a simpler ML model and fewer computational resources. However, ML engineers who work on such tasks might not have a background in signal processing which is an entirely different area of expertise.   The objective of this work is to provide a concise comparative analysis of Fourier and Wavelet transforms that are most commonly used as signal decomposition methods for audio processing tasks. Metrics for evaluating speech intelligibility are also discussed, namely Scale-Invariant Signal-to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;/&#26497;&#24615;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23545;&#20598;&#20307;&#31215;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21333;&#32431;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;SSMF&#31639;&#27861;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.20197</link><description>&lt;p&gt;
&#21452;&#23545;&#20598;&#20307;&#31215;&#26368;&#22823;&#21270;&#29992;&#20110;&#21333;&#32431;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20197
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;/&#26497;&#24615;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23545;&#20598;&#20307;&#31215;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21333;&#32431;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;SSMF&#31639;&#27861;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Simplex-structured matrix factorization&#65288;SSMF&#65289;&#26159;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#27867;&#21270;&#65292;&#26159;&#19968;&#31181;&#22522;&#30784;&#30340;&#21487;&#35299;&#37322;&#25968;&#25454;&#20998;&#26512;&#27169;&#22411;&#65292;&#22312;&#39640;&#20809;&#35889;&#35299;&#28151;&#21644;&#21644;&#20027;&#39064;&#24314;&#27169;&#20013;&#26377;&#24212;&#29992;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#35782;&#21035;&#30340;&#35299;&#65292;&#26631;&#20934;&#26041;&#27861;&#26159;&#23547;&#25214;&#26368;&#23567;&#20307;&#31215;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#38754;&#20307;&#30340;&#23545;&#20598;/&#26497;&#24615;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#20307;&#31215;SSMF&#36716;&#25442;&#20026;&#23545;&#20598;&#31354;&#38388;&#20013;&#30340;&#26368;&#22823;&#20307;&#31215;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#36825;&#20010;&#26368;&#22823;&#20307;&#31215;&#23545;&#20598;&#38382;&#39064;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#23545;&#20598;&#20844;&#24335;&#25552;&#20379;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;SSMF&#30340;&#20004;&#20010;&#29616;&#26377;&#31639;&#27861;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21363;&#20307;&#31215;&#26368;&#23567;&#21270;&#21644;&#38754;&#35782;&#21035;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;SSMF&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20197v1 Announce Type: cross  Abstract: Simplex-structured matrix factorization (SSMF) is a generalization of nonnegative matrix factorization, a fundamental interpretable data analysis model, and has applications in hyperspectral unmixing and topic modeling. To obtain identifiable solutions, a standard approach is to find minimum-volume solutions. By taking advantage of the duality/polarity concept for polytopes, we convert minimum-volume SSMF in the primal space to a maximum-volume problem in the dual space. We first prove the identifiability of this maximum-volume dual problem. Then, we use this dual formulation to provide a novel optimization approach which bridges the gap between two existing families of algorithms for SSMF, namely volume minimization and facet identification. Numerical experiments show that the proposed approach performs favorably compared to the state-of-the-art SSMF algorithms.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#31354;&#38388;&#32422;&#26463;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;SCB-Net&#65289;&#30340;&#26032;&#26550;&#26500;&#26088;&#22312;&#26377;&#25928;&#21033;&#29992;&#36741;&#21161;&#21464;&#37327;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#23721;&#24615;&#21046;&#22270;&#21644;&#35299;&#20915;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#25216;&#26415;&#22312;&#22788;&#29702;&#22810;&#21464;&#37327;&#21644;&#31354;&#38388;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.20195</link><description>&lt;p&gt;
&#21033;&#29992;&#31354;&#38388;&#32422;&#26463;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;SCB-Net&#65289;&#22686;&#24378;&#23721;&#24615;&#21046;&#22270;&#65306;&#19968;&#31181;&#22522;&#20110;&#29616;&#22330;&#25968;&#25454;&#32422;&#26463;&#39044;&#27979;&#30340;&#26041;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Enhancing Lithological Mapping with Spatially Constrained Bayesian Network (SCB-Net): An Approach for Field Data-Constrained Predictions with Uncertainty Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20195
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#32422;&#26463;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;SCB-Net&#65289;&#30340;&#26032;&#26550;&#26500;&#26088;&#22312;&#26377;&#25928;&#21033;&#29992;&#36741;&#21161;&#21464;&#37327;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#23721;&#24615;&#21046;&#22270;&#21644;&#35299;&#20915;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#25216;&#26415;&#22312;&#22788;&#29702;&#22810;&#21464;&#37327;&#21644;&#31354;&#38388;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#36136;&#22320;&#22270;&#26159;&#22320;&#29699;&#31185;&#23398;&#20013;&#19968;&#20221;&#26497;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#20026;&#30719;&#20135;&#21208;&#25506;&#12289;&#33258;&#28982;&#28798;&#23475;&#26131;&#25439;&#24615;&#31561;&#25552;&#20379;&#35265;&#35299;&#12290;&#36825;&#20123;&#22320;&#22270;&#20351;&#29992;&#22320;&#36136;&#35266;&#27979;&#25968;&#25454;&#26469;&#25512;&#26029;&#25968;&#25454;&#30340;&#25968;&#20540;&#25110;&#27010;&#24565;&#27169;&#22411;&#21019;&#24314;&#32780;&#25104;&#12290;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#25216;&#26415;&#34987;&#29992;&#20110;&#29983;&#25104;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#31354;&#38388;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36741;&#21161;&#21464;&#37327;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;&#26041;&#27861;&#21464;&#24471;&#26356;&#21152;&#32321;&#37325;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#38590;&#20197;&#22788;&#29702;&#31354;&#38388;&#30456;&#20851;&#25968;&#25454;&#65292;&#38590;&#20197;&#20174;&#22320;&#23398;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#31216;&#20026;&#31354;&#38388;&#32422;&#26463;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;SCB-Net&#65289;&#12290;SCB-Net&#26088;&#22312;&#26377;&#25928;&#21033;&#29992;&#36741;&#21161;&#21464;&#37327;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20195v1 Announce Type: cross  Abstract: Geological maps are an extremely valuable source of information for the Earth sciences. They provide insights into mineral exploration, vulnerability to natural hazards, and many other applications. These maps are created using numerical or conceptual models that use geological observations to extrapolate data. Geostatistical techniques have traditionally been used to generate reliable predictions that take into account the spatial patterns inherent in the data. However, as the number of auxiliary variables increases, these methods become more labor-intensive. Additionally, traditional machine learning methods often struggle with spatially correlated data and extracting valuable non-linear information from geoscientific datasets. To address these limitations, a new architecture called the Spatially Constrained Bayesian Network (SCB-Net) has been developed. The SCB-Net aims to effectively exploit the information from auxiliary variables
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;Wilkie, Stonham&#21644;Aleksander&#30340;Recognition Device (WiSARD)&#36827;&#34892;&#21516;&#24577;&#35780;&#20272;&#65292;&#28982;&#21518;&#29992;&#20110;&#21152;&#23494;&#25968;&#25454;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;Weightless Neural Networks (WNNs)&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#36739;&#20256;&#32479;&#30340;CNNs&#65292;WNNs&#22312;&#24615;&#33021;&#19978;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.20190</link><description>&lt;p&gt;
Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data
&lt;/p&gt;
&lt;p&gt;
Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;Wilkie, Stonham&#21644;Aleksander&#30340;Recognition Device (WiSARD)&#36827;&#34892;&#21516;&#24577;&#35780;&#20272;&#65292;&#28982;&#21518;&#29992;&#20110;&#21152;&#23494;&#25968;&#25454;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;Weightless Neural Networks (WNNs)&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#36739;&#20256;&#32479;&#30340;CNNs&#65292;WNNs&#22312;&#24615;&#33021;&#19978;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24191;&#27867;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#21040;&#25968;&#25454;&#38544;&#31169;&#30740;&#31350;&#30028;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#20154;&#35797;&#22270;&#20026;&#20854;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;&#25216;&#26415;&#12290;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#23545;&#21152;&#23494;&#25968;&#25454;&#30452;&#25509;&#25191;&#34892;&#25805;&#20316;&#30340;&#21516;&#24577;&#35780;&#20272;&#20855;&#26377;&#31361;&#20986;&#20248;&#28857;&#65292;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#20445;&#23494;&#20445;&#38556;&#12290;&#25512;&#26029;&#31639;&#27861;&#30340;&#21516;&#24577;&#35780;&#20272;&#21363;&#20351;&#23545;&#20110;&#30456;&#23545;&#28145;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#20063;&#26159;&#23454;&#29992;&#30340;&#12290;&#28982;&#32780;, &#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20542;&#21521;&#20110;&#20351;&#29992;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#36825;&#21487;&#33021;&#19981;&#36866;&#21512;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#22914;&#22270;&#20687;&#35782;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;Wilkie, Stonham&#21644;Aleksander&#30340;Recognition Device (WiSARD)&#21644;&#38543;&#21518;&#30340;Weightless Neural Networks (WNNs)&#36827;&#34892;&#21516;&#24577;&#35780;&#20272;&#65292;&#29992;&#20110;&#21152;&#23494;&#25968;&#25454;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#19982;CNNs&#30456;&#27604;&#65292;WNNs&#22312;&#24615;&#33021;&#19978;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20190v1 Announce Type: cross  Abstract: The widespread application of machine learning algorithms is a matter of increasing concern for the data privacy research community, and many have sought to develop privacy-preserving techniques for it. Among existing approaches, the homomorphic evaluation of ML algorithms stands out by performing operations directly over encrypted data, enabling strong guarantees of confidentiality. The homomorphic evaluation of inference algorithms is practical even for relatively deep Convolution Neural Networks (CNNs). However, training is still a major challenge, with current solutions often resorting to lightweight algorithms that can be unfit for solving more complex problems, such as image recognition. This work introduces the homomorphic evaluation of Wilkie, Stonham, and Aleksander's Recognition Device (WiSARD) and subsequent Weightless Neural Networks (WNNs) for training and inference on encrypted data. Compared to CNNs, WNNs offer better pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#24335;&#32676;&#26234;&#33021;&#23398;&#20064;&#65288;DSL&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#21644;&#29983;&#29289;&#32676;&#26234;&#33021;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20026;&#22823;&#35268;&#27169;IoT&#22312;&#26080;&#32447;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#31283;&#20581;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.20188</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#32676;&#26234;&#33021;&#23398;&#20064;&#29992;&#20110;&#36793;&#32536;&#29289;&#32852;&#32593;
&lt;/p&gt;
&lt;p&gt;
Distributed Swarm Learning for Edge Internet of Things
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#24335;&#32676;&#26234;&#33021;&#23398;&#20064;&#65288;DSL&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#21644;&#29983;&#29289;&#32676;&#26234;&#33021;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20026;&#22823;&#35268;&#27169;IoT&#22312;&#26080;&#32447;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#31283;&#20581;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20188v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#23548;&#33268;&#26234;&#33021;IoT&#35774;&#22791;&#22312;&#26080;&#32447;&#36793;&#32536;&#36827;&#34892;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24320;&#21551;&#20102;&#36793;&#32536;&#23398;&#20064;&#30340;&#26032;&#26102;&#20195;&#12290;&#38754;&#23545;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#36816;&#34892;&#30340;&#22823;&#37327;&#30828;&#20214;&#21463;&#38480;&#21046;&#30340;IoT&#35774;&#22791;&#65292;&#36793;&#32536;&#23398;&#20064;&#38754;&#20020;&#30528;&#36890;&#20449;&#21644;&#35745;&#31639;&#29942;&#39048;&#12289;&#35774;&#22791;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#23433;&#20840;&#39118;&#38505;&#12289;&#38544;&#31169;&#27844;&#28431;&#12289;&#38750;&#20984;&#20248;&#21270;&#21644;&#22797;&#26434;&#30340;&#26080;&#32447;&#29615;&#22659;&#31561;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#24067;&#24335;&#32676;&#26234;&#33021;&#23398;&#20064;&#65288;DSL&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#21644;&#29983;&#29289;&#32676;&#26234;&#33021;&#22312;&#25972;&#20307;&#19978;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#36816;&#29992;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#36890;&#20449;&#25216;&#26415;&#65292;DSL&#20026;&#26080;&#32447;&#32593;&#32476;&#36793;&#32536;&#30340;&#22823;&#35268;&#27169;IoT&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#31283;&#20581;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20188v1 Announce Type: cross  Abstract: The rapid growth of Internet of Things (IoT) has led to the widespread deployment of smart IoT devices at wireless edge for collaborative machine learning tasks, ushering in a new era of edge learning. With a huge number of hardware-constrained IoT devices operating in resource-limited wireless networks, edge learning encounters substantial challenges, including communication and computation bottlenecks, device and data heterogeneity, security risks, privacy leakages, non-convex optimization, and complex wireless environments. To address these issues, this article explores a novel framework known as distributed swarm learning (DSL), which combines artificial intelligence and biological swarm intelligence in a holistic manner. By harnessing advanced signal processing and communications, DSL provides efficient solutions and robust tools for large-scale IoT at the edge of wireless networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31995;&#32479;&#22312;&#25972;&#20307;&#38899;&#39057;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#21033;&#29992;ASR&#39537;&#21160;&#30340;Wav2Vec2&#26550;&#26500;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#22312;&#35821;&#38899;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.20184</link><description>&lt;p&gt;
&#21033;&#29992;ASR&#39537;&#21160;&#30340;Wav2Vec2&#22312;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#20013;&#25506;&#31350;&#30149;&#24577;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31995;&#32479;&#22312;&#25972;&#20307;&#38899;&#39057;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#21033;&#29992;ASR&#39537;&#21160;&#30340;Wav2Vec2&#26550;&#26500;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#22312;&#35821;&#38899;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#20316;&#20026;&#20256;&#32479;&#24863;&#30693;&#20020;&#24202;&#35780;&#20272;&#30340;&#26367;&#20195;&#25110;&#25903;&#25345;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#22823;&#37096;&#20998;&#30740;&#31350;&#20165;&#22312;&#31616;&#21333;&#20219;&#21153;&#65288;&#22914;&#20108;&#20803;&#20998;&#31867;&#65289;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#23558;&#24739;&#32773;&#30340;&#38899;&#39057;&#25991;&#20214;&#20998;&#21106;&#25104;&#22810;&#20010;&#26679;&#26412;&#20197;&#22686;&#21152;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#23558;&#25972;&#20307;&#38899;&#39057;&#24471;&#20998;&#38388;&#25509;&#22320;&#19982;&#20010;&#21035;&#27573;&#30456;&#20851;&#32852;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#31995;&#32479;&#22312;&#25972;&#20010;&#38899;&#39057;&#32423;&#21035;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#22312;&#29255;&#27573;&#32423;&#21035;&#23398;&#20064;&#65292;&#23613;&#31649;&#23384;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#35821;&#38899;&#35780;&#20272;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Wav2Vec2&#26550;&#26500;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;ASR&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#22312;HNC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;ASR&#39537;&#21160;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#24314;&#31435;&#20102;&#26032;&#30340;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343;$MSE=0.73$&#21644;$MSE=1.15$&#26469;&#39044;&#27979;&#26234;&#21830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20184v1 Announce Type: cross  Abstract: Automatic speech quality assessment has raised more attention as an alternative or support to traditional perceptual clinical evaluation. However, most research so far only gains good results on simple tasks such as binary classification, largely due to data scarcity. To deal with this challenge, current works tend to segment patients' audio files into many samples to augment the datasets. Nevertheless, this approach has limitations, as it indirectly relates overall audio scores to individual segments. This paper introduces a novel approach where the system learns at the audio level instead of segments despite data scarcity. This paper proposes to use the pre-trained Wav2Vec2 architecture for both SSL, and ASR as feature extractor in speech assessment. Carried out on the HNC dataset, our ASR-driven approach established a new baseline compared with other approaches, obtaining average $MSE=0.73$ and $MSE=1.15$ for the prediction of intel
&lt;/p&gt;</description></item><item><title>CAESAR&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#21644;&#31579;&#36873;&#26426;&#21046;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#20010;&#20307;&#20195;&#29702;&#22312;&#19981;&#21516;MDPs&#19978;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.20156</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;MDPs&#20013;&#36890;&#36807;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#19982;&#31579;&#36873;&#22686;&#24378;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;CAESAR&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20156
&lt;/p&gt;
&lt;p&gt;
CAESAR&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#21644;&#31579;&#36873;&#26426;&#21046;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#20010;&#20307;&#20195;&#29702;&#22312;&#19981;&#21516;MDPs&#19978;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20540;&#20026;&#22522;&#30784;&#20195;&#29702;&#22312;&#19981;&#21516;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20043;&#38388;&#36816;&#34892;&#26102;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#65288;FedRL&#65289;&#12290;&#29616;&#26377;&#30340;FedRL&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#20195;&#29702;&#30340;&#20540;&#20989;&#25968;&#36827;&#34892;&#24179;&#22343;&#26469;&#25913;&#21892;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#32858;&#21512;&#31574;&#30053;&#22312;&#20195;&#29702;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#26102;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#20010;&#20307;&#20195;&#29702;&#36328;&#21508;&#31181;MDPs&#23398;&#20064;&#30340;Convergence-AwarE SAmpling with scReening&#65288;CAESAR&#65289;&#32858;&#21512;&#26041;&#26696;&#12290;CAESAR&#26159;&#26381;&#21153;&#22120;&#20351;&#29992;&#30340;&#19968;&#31181;&#32858;&#21512;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#21644;&#31579;&#36873;&#26426;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#30456;&#21516;MDP&#20013;&#20195;&#29702;&#25910;&#25947;&#21040;&#30456;&#21516;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#20107;&#23454;&#65292;CAESAR&#20351;&#24471;&#33021;&#22815;&#20174;&#26356;&#29087;&#32451;&#30340;&#21516;&#34892;&#37027;&#37324;&#26377;&#36873;&#25321;&#22320;&#21560;&#25910;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20156v1 Announce Type: cross  Abstract: In this study, we delve into Federated Reinforcement Learning (FedRL) in the context of value-based agents operating across diverse Markov Decision Processes (MDPs). Existing FedRL methods typically aggregate agents' learning by averaging the value functions across them to improve their performance. However, this aggregation strategy is suboptimal in heterogeneous environments where agents converge to diverse optimal value functions. To address this problem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR) aggregation scheme designed to enhance the learning of individual agents across varied MDPs. CAESAR is an aggregation strategy used by the server that combines convergence-aware sampling with a screening mechanism. By exploiting the fact that agents learning in identical MDPs are converging to the same optimal value function, CAESAR enables the selective assimilation of knowledge from more proficient counterparts, 
&lt;/p&gt;</description></item><item><title>TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20150</link><description>&lt;p&gt;
TFB&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20840;&#38754;&#19988;&#20844;&#24179;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20150
&lt;/p&gt;
&lt;p&gt;
TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20250;&#22312;&#32463;&#27982;&#12289;&#20132;&#36890;&#12289;&#20581;&#24247;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#39046;&#22495;&#20013;&#20135;&#29983;&#65292;&#23545;&#26410;&#26469;&#25968;&#20540;&#30340;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#35768;&#22810;&#39044;&#27979;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#20026;&#20102;&#30830;&#20445;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#33021;&#22815;&#20197;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#26041;&#24335;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TFB&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;TFB&#36890;&#36807;&#35299;&#20915;&#19982;&#25968;&#25454;&#38598;&#12289;&#27604;&#36739;&#26041;&#27861;&#21644;&#35780;&#20272;&#31649;&#36947;&#30456;&#20851;&#30340;&#32570;&#28857;&#65292;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#65306;1&#65289;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#65292;2&#65289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;3&#65289;&#19981;&#19968;&#33268;&#21644;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#39046;&#22495;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#26469;&#33258;10&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#12289;&#30005;&#21147;&#12289;&#33021;&#28304;&#12289;&#29615;&#22659;&#12289;&#33258;&#28982;&#12289;&#32463;&#27982;&#12289;&#32929;&#31080;&#24066;&#22330;&#12289;&#38134;&#34892;&#12289;&#20581;&#24247;&#21644;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#30005;&#21147;&#24066;&#22330;&#20013;&#20351;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#23545;&#20809;&#20239;&#30005;&#21147;&#26085;&#21069;&#39044;&#27979;&#36827;&#34892;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#32467;&#21512;&#29305;&#23450;&#20986;&#20215;&#31574;&#30053;&#21487;&#20197;&#22312;&#20445;&#25345;&#33021;&#37327;&#24179;&#34913;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#39640;&#21033;&#28070;&#12290;</title><link>https://arxiv.org/abs/2403.20149</link><description>&lt;p&gt;
&#30005;&#21147;&#24066;&#22330;&#20013;&#20809;&#20239;&#30005;&#21147;&#38543;&#26426;&#20915;&#31574;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20149
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#30005;&#21147;&#24066;&#22330;&#20013;&#20351;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#23545;&#20809;&#20239;&#30005;&#21147;&#26085;&#21069;&#39044;&#27979;&#36827;&#34892;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#32467;&#21512;&#29305;&#23450;&#20986;&#20215;&#31574;&#30053;&#21487;&#20197;&#22312;&#20445;&#25345;&#33021;&#37327;&#24179;&#34913;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#39640;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#65292;&#19968;&#31181;&#26032;&#20852;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#24378;&#22312;&#30005;&#21147;&#24066;&#22330;&#20013;&#26085;&#21069;&#20809;&#20239;&#30005;&#21147;&#39044;&#27979;&#30340;&#21442;&#19982;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#28857;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#23454;&#26045;&#20102;&#20960;&#31181;CP&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#21019;&#24314;CP&#21306;&#38388;&#21644;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#26469;&#37327;&#21270;&#36825;&#20123;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#20272;&#35745;&#20102;&#22810;&#31181;&#20986;&#20215;&#31574;&#30053;&#19979;&#30340;&#30005;&#21147;&#24066;&#22330;&#30340;&#26368;&#20248;&#25968;&#37327;&#20986;&#20215;&#65292;&#21363;&#65306;&#20449;&#20219;&#39044;&#27979;&#12289;&#26368;&#22351;&#24773;&#20917;&#12289;Newsvendor&#21644;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#65288;EUM&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;k&#26368;&#36817;&#37051;&#21644;/&#25110;Mondrian&#20998;&#31665;&#30340;CP&#32988;&#36807;&#20854;&#23545;&#24212;&#30340;&#32447;&#24615;&#20998;&#20301;&#25968;&#22238;&#24402;&#22120;&#12290;&#20351;&#29992;CP&#32467;&#21512;&#26576;&#20123;&#20986;&#20215;&#31574;&#30053;&#21487;&#20197;&#22312;&#20445;&#25345;&#33021;&#37327;&#24179;&#34913;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#39640;&#21033;&#28070;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#20855;&#26377;k&#36817;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20149v1 Announce Type: new  Abstract: This paper studies the use of conformal prediction (CP), an emerging probabilistic forecasting method, for day-ahead photovoltaic power predictions to enhance participation in electricity markets. First, machine learning models are used to construct point predictions. Thereafter, several variants of CP are implemented to quantify the uncertainty of those predictions by creating CP intervals and cumulative distribution functions. Optimal quantity bids for the electricity market are estimated using several bidding strategies under uncertainty, namely: trust-the-forecast, worst-case, Newsvendor and expected utility maximization (EUM). Results show that CP in combination with k-nearest neighbors and/or Mondrian binning outperforms its corresponding linear quantile regressors. Using CP in combination with certain bidding strategies can yield high profit with minimal energy imbalance. In concrete, using conformal predictive systems with k-near
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27850;&#26494;&#31215;&#20998;&#22120;&#30340;&#35774;&#35745;&#29702;&#35299;&#20026;&#35299;&#20915;&#29305;&#23450;PDE&#38382;&#39064;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#23545;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;PDE&#30340;&#31616;&#21333;&#36817;&#20284;&#65292;&#32467;&#21512;&#20102;&#29289;&#29702;&#24314;&#27169;&#21644;&#25968;&#25454;&#30340;&#35774;&#35745;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.20139</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#27850;&#26494;&#31215;&#20998;&#22120;
&lt;/p&gt;
&lt;p&gt;
Designing Poisson Integrators Through Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27850;&#26494;&#31215;&#20998;&#22120;&#30340;&#35774;&#35745;&#29702;&#35299;&#20026;&#35299;&#20915;&#29305;&#23450;PDE&#38382;&#39064;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#23545;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;PDE&#30340;&#31616;&#21333;&#36817;&#20284;&#65292;&#32467;&#21512;&#20102;&#29289;&#29702;&#24314;&#27169;&#21644;&#25968;&#25454;&#30340;&#35774;&#35745;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#27850;&#26494;&#31215;&#20998;&#22120;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21363;&#20445;&#30041;&#22522;&#30784;&#27850;&#26494;&#20960;&#20309;&#30340;&#31215;&#20998;&#22120;&#12290;&#25105;&#20204;&#20551;&#35774;&#27850;&#26494;&#27969;&#24418;&#26159;&#21487;&#31215;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23384;&#22312;&#24050;&#30693;&#30340;&#23616;&#37096;&#36763;&#32676;&#20307;&#65292;&#27850;&#26494;&#27969;&#24418;&#20316;&#20026;&#21333;&#20301;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#24314;&#31435;&#22312;&#27850;&#26494;&#24494;&#20998;&#21516;&#32986;&#21644;&#25289;&#26684;&#26391;&#26085;&#20999;&#20998;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#20043;&#19978;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#27850;&#26494;&#31215;&#20998;&#22120;&#30340;&#35774;&#35745;&#37325;&#26032;&#34920;&#36848;&#20026;&#23545;&#26576;&#20010;&#29305;&#23450;PDE&#65288;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#65289;&#30340;&#35299;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23558;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;PDE&#29702;&#35299;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#35299;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#25216;&#26415;&#36731;&#26494;&#36817;&#20284;&#12290;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#19982;&#24403;&#21069;PDE&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#36235;&#21183;&#30456;&#19968;&#33268;&#65292;&#22914;&#30001;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#25152;&#21457;&#36215;&#65292;&#20513;&#23548;&#32467;&#21512;&#29289;&#29702;&#24314;&#27169;&#65288;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;PDE&#65289;&#21644;&#25968;&#25454;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20139v1 Announce Type: cross  Abstract: This paper presents a general method to construct Poisson integrators, i.e., integrators that preserve the underlying Poisson geometry. We assume the Poisson manifold is integrable, meaning there is a known local symplectic groupoid for which the Poisson manifold serves as the set of units. Our constructions build upon the correspondence between Poisson diffeomorphisms and Lagrangian bisections, which allows us to reformulate the design of Poisson integrators as solutions to a certain PDE (Hamilton-Jacobi). The main novelty of this work is to understand the Hamilton-Jacobi PDE as an optimization problem, whose solution can be easily approximated using machine learning related techniques. This research direction aligns with the current trend in the PDE and machine learning communities, as initiated by Physics- Informed Neural Networks, advocating for designs that combine both physical modeling (the Hamilton-Jacobi PDE) and data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#31181;&#29305;&#24449;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#20272;&#35745;&#22768;&#28304;&#30340;&#20301;&#32622;&#21644;&#31867;&#21035;&#65292;&#21253;&#25324;&#24341;&#20837;"&#22768;&#38899;&#22320;&#22270;"&#29305;&#24449;&#12289;&#20351;&#29992;Gammatone&#28388;&#27874;&#22120;&#29983;&#25104;&#26356;&#36866;&#21512;&#23460;&#22806;&#29615;&#22659;&#30340;&#22768;&#23398;&#29305;&#24449;&#65292;&#20197;&#21450;&#38598;&#25104;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#36890;&#36947;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.20130</link><description>&lt;p&gt;
&#22312;&#23460;&#22806;&#29615;&#22659;&#20013;&#20351;&#29992;WASN&#36827;&#34892;&#22768;&#20107;&#20214;&#23450;&#20301;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Sound event localization and classification using WASN in Outdoor Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#31181;&#29305;&#24449;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#20272;&#35745;&#22768;&#28304;&#30340;&#20301;&#32622;&#21644;&#31867;&#21035;&#65292;&#21253;&#25324;&#24341;&#20837;"&#22768;&#38899;&#22320;&#22270;"&#29305;&#24449;&#12289;&#20351;&#29992;Gammatone&#28388;&#27874;&#22120;&#29983;&#25104;&#26356;&#36866;&#21512;&#23460;&#22806;&#29615;&#22659;&#30340;&#22768;&#23398;&#29305;&#24449;&#65292;&#20197;&#21450;&#38598;&#25104;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#36890;&#36947;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22768;&#20107;&#20214;&#23450;&#20301;&#21644;&#20998;&#31867;&#26159;&#26080;&#32447;&#22768;&#23398;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22768;&#20107;&#20214;&#23450;&#20301;&#21644;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#20010;&#40614;&#20811;&#39118;&#38453;&#21015;&#65292;&#23481;&#26131;&#21463;&#21040;&#20449;&#21495;&#34928;&#20943;&#21644;&#29615;&#22659;&#22122;&#38899;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#30417;&#27979;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22810;&#20010;&#40614;&#20811;&#39118;&#38453;&#21015;&#30340;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#28304;&#23450;&#20301;&#65292;&#24573;&#30053;&#20102;&#22768;&#20107;&#20214;&#20998;&#31867;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#31181;&#29305;&#24449;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#20272;&#35745;&#22768;&#28304;&#30340;&#20301;&#32622;&#21644;&#31867;&#21035;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#22768;&#38899;&#22320;&#22270;"&#29305;&#24449;&#65292;&#20197;&#25429;&#33719;&#22810;&#20010;&#39057;&#27573;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;Gammatone&#28388;&#27874;&#22120;&#29983;&#25104;&#26356;&#36866;&#21512;&#23460;&#22806;&#29615;&#22659;&#30340;&#22768;&#23398;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38598;&#25104;&#20102;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#36890;&#36947;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20130v1 Announce Type: cross  Abstract: Deep learning-based sound event localization and classification is an emerging research area within wireless acoustic sensor networks. However, current methods for sound event localization and classification typically rely on a single microphone array, making them susceptible to signal attenuation and environmental noise, which limits their monitoring range. Moreover, methods using multiple microphone arrays often focus solely on source localization, neglecting the aspect of sound event classification. In this paper, we propose a deep learning-based method that employs multiple features and attention mechanisms to estimate the location and class of sound source. We introduce a Soundmap feature to capture spatial information across multiple frequency bands. We also use the Gammatone filter to generate acoustic features more suitable for outdoor environments. Furthermore, we integrate attention mechanisms to learn channel-wise relationsh
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#35874;&#24615;&#20943;&#37325;&#25163;&#26415;&#24739;&#32773;&#30340;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27169;&#22411;&#21644;&#21464;&#37327;&#31867;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#20248;&#21270;&#27835;&#30103;&#31574;&#30053;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.20124</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20195;&#35874;&#24615;&#20943;&#37325;&#25163;&#26415;&#26415;&#21518;&#25104;&#21151;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Application of Machine Learning Algorithms in Classifying Postoperative Success in Metabolic Bariatric Surgery: A Comprehensive Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20124
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#35874;&#24615;&#20943;&#37325;&#25163;&#26415;&#24739;&#32773;&#30340;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27169;&#22411;&#21644;&#21464;&#37327;&#31867;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#20248;&#21270;&#27835;&#30103;&#31574;&#30053;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20195;&#35874;&#24615;&#20943;&#37325;&#25163;&#26415;(MBS)&#26159;&#23545;&#24739;&#26377;&#32933;&#32982;&#21450;&#30456;&#20851;&#20581;&#24247;&#38382;&#39064;&#30340;&#24739;&#32773;&#36827;&#34892;&#37325;&#35201;&#24178;&#39044;&#12290;&#31934;&#30830;&#20998;&#31867;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#23545;&#20248;&#21270;&#27835;&#30103;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20195;&#35874;&#24615;&#20943;&#37325;&#25163;&#26415;&#29615;&#22659;&#20013;&#23545;&#24739;&#32773;&#36827;&#34892;&#20998;&#31867;&#65292;&#20026;&#19981;&#21516;&#27169;&#22411;&#21644;&#21464;&#37327;&#31867;&#22411;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#35265;&#35299;&#12290;&#26041;&#27861;&#65306;&#23545;73&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#24212;&#29992;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;GaussianNB&#12289;ComplementNB&#12289;KNN&#12289;&#20915;&#31574;&#26641;&#12289;&#24102;&#26377;RandomOverSampler&#30340;KNN&#21644;&#24102;&#26377;SMOTE&#30340;KNN&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#24515;&#29702;&#27979;&#37327;&#12289;&#31038;&#20250;&#32463;&#27982;&#21644;&#20998;&#26512;&#21464;&#37327;&#65292;&#36890;&#36807;&#20998;&#26512;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#21464;&#37327;&#20998;&#32452;&#21644;&#36807;&#37319;&#26679;&#25216;&#26415;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#65306;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#39640;&#36798;66.7%&#65292;&#34920;&#26126;&#20102;&#19968;&#20123;&#27169;&#22411;&#22312;&#20195;&#35874;&#24615;&#20943;&#37325;&#25163;&#26415;&#20013;&#20855;&#26377;&#33391;&#22909;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20124v1 Announce Type: new  Abstract: Objectives: Metabolic Bariatric Surgery (MBS) is a critical intervention for patients living with obesity and related health issues. Accurate classification and prediction of patient outcomes are vital for optimizing treatment strategies. This study presents a novel machine learning approach to classify patients in the context of metabolic bariatric surgery, providing insights into the efficacy of different models and variable types. Methods: Various machine learning models, including GaussianNB, ComplementNB, KNN, Decision Tree, KNN with RandomOverSampler, and KNN with SMOTE, were applied to a dataset of 73 patients. The dataset, comprising psychometric, socioeconomic, and analytical variables, was analyzed to determine the most efficient predictive model. The study also explored the impact of different variable groupings and oversampling techniques. Results: Experimental results indicate average accuracy values as high as 66.7% for the
&lt;/p&gt;</description></item><item><title>LUGSI&#26159;&#31532;&#19968;&#27425;&#24341;&#20837;&#31890;&#24230;&#32479;&#35745;&#19981;&#21464;&#37327;&#65292;&#30456;&#36739;&#20110;LUSI&#65292;&#23427;&#36890;&#36807;&#24378;&#24369;&#20004;&#31181;&#25910;&#25947;&#26426;&#21046;&#24182;&#26368;&#23567;&#21270;&#39044;&#26399;&#39118;&#38505;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#32467;&#26500;&#20449;&#24687;&#24182;&#25104;&#21151;&#38477;&#32500;&#12290;</title><link>https://arxiv.org/abs/2403.20122</link><description>&lt;p&gt;
&#20351;&#29992;&#31890;&#24230;&#32479;&#35745;&#19981;&#21464;&#37327;&#36827;&#34892;&#20998;&#31867;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning using granularity statistical invariants for classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20122
&lt;/p&gt;
&lt;p&gt;
LUGSI&#26159;&#31532;&#19968;&#27425;&#24341;&#20837;&#31890;&#24230;&#32479;&#35745;&#19981;&#21464;&#37327;&#65292;&#30456;&#36739;&#20110;LUSI&#65292;&#23427;&#36890;&#36807;&#24378;&#24369;&#20004;&#31181;&#25910;&#25947;&#26426;&#21046;&#24182;&#26368;&#23567;&#21270;&#39044;&#26399;&#39118;&#38505;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#32467;&#26500;&#20449;&#24687;&#24182;&#25104;&#21151;&#38477;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20122v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23398;&#20064;&#20351;&#29992;&#32479;&#35745;&#19981;&#21464;&#37327;&#65288;Learning using statistical invariants&#65292;LUSI&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#37319;&#29992;&#20102;&#24369;&#25910;&#25947;&#26426;&#21046;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;LUSI&#20013;&#19981;&#21464;&#30697;&#38453;&#30340;&#35745;&#31639;&#25104;&#26412;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35828;&#36807;&#39640;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31890;&#24230;&#32479;&#35745;&#19981;&#21464;&#37327;&#29992;&#20110;LUSI&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#20351;&#29992;&#31890;&#24230;&#32479;&#35745;&#19981;&#21464;&#37327;&#65288;Learning using granularity statistical invariants&#65292;LUGSI&#65289;&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#12290;LUGSI&#21516;&#26102;&#37319;&#29992;&#20102;&#24378;&#21644;&#24369;&#25910;&#25947;&#26426;&#21046;&#65292;&#20197;&#26368;&#23567;&#21270;&#39044;&#26399;&#39118;&#38505;&#30340;&#35270;&#35282;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#26500;&#24314;&#31890;&#24230;&#32479;&#35745;&#19981;&#21464;&#37327;&#12290;&#19982;LUSI&#30456;&#27604;&#65292;&#24341;&#20837;&#36825;&#31181;&#26032;&#30340;&#32479;&#35745;&#19981;&#21464;&#37327;&#24102;&#26469;&#20004;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#22686;&#24378;&#20102;&#25968;&#25454;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;LUGSI&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#21035;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#23558;&#22823;&#30340;&#19981;&#21464;&#24615;&#30697;&#38453;&#36716;&#21270;&#20026;&#23567;&#30697;&#38453;&#65292;&#36798;&#21040;&#20102;&#38477;&#32500;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20122v1 Announce Type: new  Abstract: Learning using statistical invariants (LUSI) is a new learning paradigm, which adopts weak convergence mechanism, and can be applied to a wider range of classification problems. However, the computation cost of invariant matrices in LUSI is high for large-scale datasets during training. To settle this issue, this paper introduces a granularity statistical invariant for LUSI, and develops a new learning paradigm called learning using granularity statistical invariants (LUGSI). LUGSI employs both strong and weak convergence mechanisms, taking a perspective of minimizing expected risk. As far as we know, it is the first time to construct granularity statistical invariants. Compared to LUSI, the introduction of this new statistical invariant brings two advantages. Firstly, it enhances the structural information of the data. Secondly, LUGSI transforms a large invariant matrix into a smaller one by maximizing the distance between classes, achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#31574;&#30053;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20855;&#20307;&#28041;&#21450;&#20083;&#33146;&#30284;&#22522;&#22240;&#32452;&#25968;&#25454;&#21644;&#20840;&#20999;&#29255;&#25104;&#20687;&#20998;&#26512;&#65292;&#34429;&#28982;&#30149;&#29702;&#23398;&#23478;&#30340;&#21442;&#19982;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#20294;&#20998;&#31867;&#32467;&#26524;&#19981;&#29702;&#24819;&#65292;&#25351;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.20112</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#26426;&#21327;&#21516;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20083;&#33146;&#30284;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12289;&#20998;&#31867;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#31574;&#30053;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20855;&#20307;&#28041;&#21450;&#20083;&#33146;&#30284;&#22522;&#22240;&#32452;&#25968;&#25454;&#21644;&#20840;&#20999;&#29255;&#25104;&#20687;&#20998;&#26512;&#65292;&#34429;&#28982;&#30149;&#29702;&#23398;&#23478;&#30340;&#21442;&#19982;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#20294;&#20998;&#31867;&#32467;&#26524;&#19981;&#29702;&#24819;&#65292;&#25351;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#24212;&#29992;&#20154;&#26426;&#21327;&#21516;&#65288;HITL&#65289;&#31574;&#30053;&#12290;&#22312;&#26412;&#26696;&#20363;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#29983;&#21442;&#19982;&#30340;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#20154;&#31867;&#19987;&#23478;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#22797;&#26434;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#28041;&#21450;&#22522;&#22240;&#32452;&#25968;&#25454;&#30340;&#25972;&#21512;&#20197;&#21450;&#20083;&#33146;&#30284;&#30340;&#20840;&#20999;&#29255;&#25104;&#20687;&#65288;WSI&#65289;&#20998;&#26512;&#12290;&#24320;&#21457;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#30340;&#20998;&#21106;&#65292;&#38024;&#23545;&#30284;&#30151;&#22522;&#22240;&#32452;&#20122;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#65292;&#26368;&#21518;&#26159;&#23545;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#30340;&#35299;&#37322;&#12290;&#30149;&#29702;&#23398;&#23478;&#30340;&#21442;&#19982;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#20294;&#20998;&#31867;&#32467;&#26524;&#19981;&#29702;&#24819;&#65292;&#31361;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65306;&#23613;&#31649;&#28041;&#21450;&#20102;&#20154;&#31867;&#19987;&#23478;&#65292;&#20294;&#22797;&#26434;&#39046;&#22495;&#20173;&#28982;&#21487;&#33021;&#24102;&#26469;&#25361;&#25112;&#65292;&#20154;&#26426;&#21327;&#21516;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20112v1 Announce Type: cross  Abstract: This paper explores the application of Human-in-the-Loop (HITL) strategies in training machine learning models in the medical domain. In this case a doctor-in-the-loop approach is proposed to leverage human expertise in dealing with large and complex data. Specifically, the paper deals with the integration of genomic data and Whole Slide Imaging (WSI) analysis of breast cancer. Three different tasks were developed: segmentation of histopathological images, classification of this images regarding the genomic subtype of the cancer and, finally, interpretation of the machine learning results. The involvement of a pathologist helped us to develop a better segmentation model and to enhance the explainatory capabilities of the models, but the classification results were suboptimal, highlighting the limitations of this approach: despite involving human experts, complex domains can still pose challenges, and a HITL approach may not always be e
&lt;/p&gt;</description></item><item><title>Mol-AIR &#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#20869;&#22312;&#22870;&#21169;&#30340;&#20998;&#23376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#21644;&#23398;&#20064;&#30340;&#20869;&#22312;&#22870;&#21169;&#20248;&#21183;&#65292;&#22312;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20109</link><description>&lt;p&gt;
Mol-AIR&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#20869;&#22312;&#22870;&#21169;&#30340;&#20998;&#23376;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#30446;&#26631;&#23548;&#21521;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20109
&lt;/p&gt;
&lt;p&gt;
Mol-AIR &#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#20869;&#22312;&#22870;&#21169;&#30340;&#20998;&#23376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#21644;&#23398;&#20064;&#30340;&#20869;&#22312;&#22870;&#21169;&#20248;&#21183;&#65292;&#22312;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#21457;&#29616;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#30340;&#25216;&#26415;&#22312;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#20998;&#23376;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#25506;&#32034;&#24222;&#22823;&#30340;&#21270;&#23398;&#31354;&#38388;&#21644;&#20248;&#21270;&#29305;&#23450;&#21270;&#23398;&#24615;&#36136;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mol-AIR&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#20869;&#22312;&#22870;&#21169;&#36827;&#34892;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#20998;&#23376;&#29983;&#25104;&#12290;Mol-AIR&#21033;&#29992;&#22522;&#20110;&#21382;&#21490;&#30340;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#20869;&#22312;&#22870;&#21169;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#33976;&#39311;&#32593;&#32476;&#21644;&#22522;&#20110;&#35745;&#25968;&#30340;&#31574;&#30053;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Mol-AIR&#23637;&#29616;&#20102;&#36229;&#20986;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#20998;&#23376;&#26041;&#38754;&#65292;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#65292;&#21253;&#25324;pena
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20109v1 Announce Type: cross  Abstract: Optimizing techniques for discovering molecular structures with desired properties is crucial in artificial intelligence(AI)-based drug discovery. Combining deep generative models with reinforcement learning has emerged as an effective strategy for generating molecules with specific properties. Despite its potential, this approach is ineffective in exploring the vast chemical space and optimizing particular chemical properties. To overcome these limitations, we present Mol-AIR, a reinforcement learning-based framework using adaptive intrinsic rewards for effective goal-directed molecular generation. Mol-AIR leverages the strengths of both history-based and learning-based intrinsic rewards by exploiting random distillation network and counting-based strategies. In benchmark tests, Mol-AIR demonstrates superior performance over existing approaches in generating molecules with desired properties without any prior knowledge, including pena
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22270;&#20687;&#21435;&#27169;&#31946;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#32858;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#28040;&#38500;&#38271;&#36317;&#31163;&#27169;&#31946;&#38477;&#22122;&#25200;&#21160;&#21644;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#38590;&#39064;</title><link>https://arxiv.org/abs/2403.20106</link><description>&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#32858;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20197;&#23454;&#29616;&#39640;&#25928;&#22270;&#20687;&#21435;&#27169;&#31946;
&lt;/p&gt;
&lt;p&gt;
Aggregating Local and Global Features via Selective State Spaces Model for Efficient Image Deblurring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20106
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22270;&#20687;&#21435;&#27169;&#31946;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#32858;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#28040;&#38500;&#38271;&#36317;&#31163;&#27169;&#31946;&#38477;&#22122;&#25200;&#21160;&#21644;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21435;&#27169;&#31946;&#26159;&#25351;&#20174;&#23545;&#24212;&#30340;&#27169;&#31946;&#22270;&#20687;&#20013;&#24674;&#22797;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;CNN&#21644;Transformer&#30340;&#20986;&#29616;&#20351;&#24471;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#30528;&#22312;&#28040;&#38500;&#36828;&#31243;&#27169;&#31946;&#38477;&#35299;&#25200;&#21160;&#21644;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#20004;&#38590;&#22659;&#22320;&#65292;&#36825;&#21046;&#32422;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#20687;&#21435;&#27169;&#31946;&#32593;&#32476;&#65292;&#21033;&#29992;&#36873;&#25321;&#24615;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#32858;&#21512;&#20016;&#23500;&#20934;&#30830;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32858;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#22359;&#65288;ALGBlock&#65289;&#26469;&#25429;&#33719;&#21644;&#34701;&#21512;&#23616;&#37096;&#19981;&#21464;&#29305;&#24615;&#21644;&#38750;&#23616;&#37096;&#20449;&#24687;&#12290;ALGBlock&#30001;&#20004;&#20010;&#22359;&#32452;&#25104;&#65306;&#65288;1&#65289;&#23616;&#37096;&#22359;&#20351;&#29992;&#31616;&#21270;&#30340;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22411;&#23616;&#37096;&#36830;&#36890;&#24615;&#12290;&#65288;2&#65289;&#20840;&#23616;&#22359;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20106v1 Announce Type: cross  Abstract: Image deblurring is a process of restoring a high quality image from the corresponding blurred image. Significant progress in this field has been made possible by the emergence of various effective deep learning models, including CNNs and Transformers. However, these methods often face the dilemma between eliminating long-range blur degradation perturbations and maintaining computational efficiency, which hinders their practical application. To address this issue, we propose an efficient image deblurring network that leverages selective structured state spaces model to aggregate enriched and accurate features. Specifically, we design an aggregate local and global block (ALGBlock) to capture and fuse both local invariant properties and non-local information. The ALGBlock consists of two blocks: (1) The local block models local connectivity using simplified channel attention. (2) The global block captures long-range dependency features w
&lt;/p&gt;</description></item><item><title>RealKIE&#25552;&#20379;&#20102;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20225;&#19994;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#20026;&#25237;&#36164;&#20998;&#26512;&#21644;&#27861;&#24459;&#25968;&#25454;&#22788;&#29702;&#31561;&#20219;&#21153;&#25552;&#20379;&#20102;&#29616;&#23454;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#24182;&#20026;NLP&#27169;&#22411;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.20101</link><description>&lt;p&gt;
RealKIE: &#20116;&#20010;&#26032;&#39062;&#30340;&#20225;&#19994;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RealKIE: Five Novel Datasets for Enterprise Key Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20101
&lt;/p&gt;
&lt;p&gt;
RealKIE&#25552;&#20379;&#20102;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20225;&#19994;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#20026;&#25237;&#36164;&#20998;&#26512;&#21644;&#27861;&#24459;&#25968;&#25454;&#22788;&#29702;&#31561;&#20219;&#21153;&#25552;&#20379;&#20102;&#29616;&#23454;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#24182;&#20026;NLP&#27169;&#22411;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;RealKIE&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25512;&#21160;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#21457;&#23637;&#30340;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#22522;&#20934;&#65292;&#37325;&#28857;&#26159;&#20225;&#19994;&#24212;&#29992;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#25324;&#32654;&#22269;SEC S1&#25991;&#20214;&#12289;&#32654;&#22269;&#20445;&#23494;&#21327;&#35758;&#12289;&#33521;&#22269;&#24904;&#21892;&#25253;&#21578;&#12289;FCC&#21457;&#31080;&#21644;&#36164;&#28304;&#21512;&#21516;&#31561;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26723;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#25991;&#26412;&#24207;&#21015;&#21270;&#19981;&#20339;&#12289;&#38271;&#25991;&#26723;&#20013;&#31232;&#30095;&#30340;&#27880;&#37322;&#21644;&#22797;&#26434;&#30340;&#34920;&#26684;&#24067;&#23616;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20026;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65288;&#22914;&#25237;&#36164;&#20998;&#26512;&#21644;&#27861;&#24459;&#25968;&#25454;&#22788;&#29702;&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#29616;&#23454;&#30340;&#27979;&#35797;&#22522;&#22320;&#12290;&#38500;&#20102;&#20171;&#32461;&#36825;&#20123;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#27880;&#37322;&#36807;&#31243;&#12289;&#25991;&#26723;&#22788;&#29702;&#25216;&#26415;&#21644;&#22522;&#32447;&#24314;&#27169;&#26041;&#27861;&#30340;&#28145;&#20837;&#25551;&#36848;&#12290;&#36825;&#19968;&#36129;&#29486;&#20419;&#36827;&#20102;&#33021;&#22815;&#22788;&#29702;&#23454;&#38469;&#25361;&#25112;&#30340;NLP&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#25903;&#25345;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#24212;&#29992;&#20110;&#24037;&#19994;&#30340;&#20449;&#24687;&#25552;&#21462;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20101v1 Announce Type: new  Abstract: We introduce RealKIE, a benchmark of five challenging datasets aimed at advancing key information extraction methods, with an emphasis on enterprise applications. The datasets include a diverse range of documents including SEC S1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and Resource Contracts. Each presents unique challenges: poor text serialization, sparse annotations in long documents, and complex tabular layouts. These datasets provide a realistic testing ground for key information extraction tasks like investment analysis and legal data processing.   In addition to presenting these datasets, we offer an in-depth description of the annotation process, document processing techniques, and baseline modeling approaches. This contribution facilitates the development of NLP models capable of handling practical challenges and supports further research into information extraction technologies applicable to indu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#39564;OOD&#26816;&#27979;&#26041;&#27861;NegLabel&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#36127;&#26631;&#31614;&#20449;&#24687;&#65292;&#35774;&#35745;&#20102;&#19982;&#36127;&#26631;&#31614;&#21327;&#20316;&#30340;OOD&#20998;&#25968;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;VLM&#26550;&#26500;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20078</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36127;&#26631;&#31614;&#24341;&#23548;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Negative Label Guided OOD Detection with Pretrained Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#39564;OOD&#26816;&#27979;&#26041;&#27861;NegLabel&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#36127;&#26631;&#31614;&#20449;&#24687;&#65292;&#35774;&#35745;&#20102;&#19982;&#36127;&#26631;&#31614;&#21327;&#20316;&#30340;OOD&#20998;&#25968;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;VLM&#26550;&#26500;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OOD&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#26469;&#33258;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#22312;&#20449;&#20219;&#27169;&#22411;&#25269;&#24481;&#24847;&#22806;&#36755;&#20837;&#38169;&#35823;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#39564;OOD&#26816;&#27979;&#26041;&#27861;NegLabel&#65292;&#35813;&#26041;&#27861;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#20102;&#22823;&#37327;&#36127;&#26631;&#31614;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19982;&#36127;&#26631;&#31614;&#21327;&#20316;&#30340;OOD&#20998;&#25968;&#30340;&#26032;&#26041;&#26696;&#12290;&#29702;&#35770;&#20998;&#26512;&#26377;&#21161;&#20110;&#29702;&#35299;&#36127;&#26631;&#31614;&#30340;&#26426;&#21046;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;NegLabel&#22312;&#21508;&#31181;OOD&#26816;&#27979;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;VLM&#26550;&#26500;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20078v1 Announce Type: cross  Abstract: Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels. Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM arch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32771;&#34385;&#33021;&#28304;&#21644;&#24310;&#36831;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20248;&#21270;&#19981;&#21516;&#36164;&#28304;&#39044;&#31639;&#30340;&#35774;&#22791;&#20043;&#38388;&#30340;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#25968;&#37327;&#65292;&#20197;&#26368;&#23567;&#21270;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.20075</link><description>&lt;p&gt;
&#33021;&#28304;&#21644;&#24310;&#36831;&#21463;&#38480;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32771;&#34385;&#33021;&#28304;&#21644;&#24310;&#36831;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20248;&#21270;&#19981;&#21516;&#36164;&#28304;&#39044;&#31639;&#30340;&#35774;&#22791;&#20043;&#38388;&#30340;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#25968;&#37327;&#65292;&#20197;&#26368;&#23567;&#21270;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#30001;&#20013;&#24515;&#33410;&#28857;&#32858;&#21512;&#21442;&#25968;&#65292;&#36890;&#20449;&#24320;&#38144;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35268;&#36991;&#36825;&#19968;&#38480;&#21046;&#24182;&#20943;&#36731;FL&#26694;&#26550;&#20869;&#30340;&#21333;&#28857;&#25925;&#38556;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#20316;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#32771;&#34385;&#21040;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#65292;&#20197;&#21450;&#19982;&#21442;&#25968;&#32858;&#21512;&#30456;&#20851;&#30340;&#33021;&#37327;&#25104;&#26412;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#26377;&#38480;&#36164;&#28304;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#32771;&#34385;&#33021;&#28304;&#21644;&#24310;&#36831;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;DFL&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#20248;&#21270;&#22312;&#19981;&#21516;&#36164;&#28304;&#39044;&#31639;&#30340;&#22810;&#26679;&#21270;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#30340;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#20351;&#36825;&#20010;&#38382;&#39064;&#21487;&#34892;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#20855;&#26377;&#19981;&#21516;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#30340;&#36793;&#32536;&#35774;&#22791;&#30340;DFL&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20075v1 Announce Type: new  Abstract: In Federated Learning (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern. To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized Federated Learning (DFL) as a viable alternative. Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated. Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints. The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets. To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training. The derive
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20058</link><description>&lt;p&gt;
&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#21644;&#28145;&#24230;&#25972;&#21512;&#30340;&#33041;&#20195;&#35874;&#12289;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#28748;&#27880;&#32593;&#32476;&#24443;&#24213;&#25913;&#21464;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#21151;&#33021;PET/MR&#65288;sf-PET/MR&#65289;&#26159;&#19968;&#31181;&#23574;&#31471;&#30340;&#22810;&#27169;&#24335;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#21516;&#26102;&#30417;&#27979;&#21644;&#25972;&#21512;&#30001;&#26102;&#31354;&#21327;&#21464;&#20195;&#35874;&#27963;&#21160;&#12289;&#31070;&#32463;&#27963;&#21160;&#21644;&#33041;&#34880;&#27969;&#65288;&#28748;&#27880;&#65289;&#26500;&#24314;&#30340;&#22810;&#26041;&#38754;&#22823;&#33041;&#32593;&#32476;&#12290;&#34429;&#28982;&#22312;&#31185;&#23398;/&#20020;&#24202;&#20215;&#20540;&#19978;&#24456;&#39640;&#65292;&#20294;PET/MR&#30828;&#20214;&#30340;&#21487;&#21450;&#24615;&#19981;&#36275;&#38459;&#30861;&#20102;&#20854;&#24212;&#29992;&#65292;&#26356;&#19981;&#29992;&#35828;&#29616;&#20195;&#22522;&#20110;AI&#30340;PET/MR&#34701;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#21487;&#34892;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#30340;sf-PET/MR&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20801;&#35768;&#21333;&#27169;&#24577;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20165;PET&#65289;&#20197;&#21450;&#24378;&#21046;&#22810;&#27169;&#24577;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#23545;&#40784;&#21644;&#37325;&#26500;&#27169;&#22411;&#12290;&#23427;&#26159;&#27169;&#24577;&#21487;&#20998;&#31163;&#21644;&#21487;&#20132;&#25442;&#30340;&#65292;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;"&#28151;&#21512;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20058v1 Announce Type: cross  Abstract: Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#35757;&#32451;&#22312;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#26816;&#27979;&#20013;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26410;&#30693;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#35757;&#32451;&#21152;&#21095;&#26410;&#30693;&#25968;&#25454;&#19981;&#21487;&#38752;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.20047</link><description>&lt;p&gt;
&#19968;&#27493;&#27493;&#25317;&#25265;&#26410;&#30693;&#65306;&#36208;&#21521;&#30495;&#23454;&#19990;&#30028;&#20013;&#21487;&#38752;&#30340;&#31232;&#30095;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#35757;&#32451;&#22312;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#26816;&#27979;&#20013;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26410;&#30693;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#35757;&#32451;&#21152;&#21095;&#26410;&#30693;&#25968;&#25454;&#19981;&#21487;&#38752;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36164;&#28304;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26041;&#27861;&#65292;&#28982;&#32780;&#31232;&#30095;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26816;&#27979;&#26410;&#30693;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#26102;&#12290;&#26412;&#30740;&#31350;&#20174;OOD&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#31232;&#30095;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#31232;&#30095;&#35757;&#32451;&#21152;&#21095;&#20102;OOD&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;&#32570;&#20047;&#26410;&#30693;&#20449;&#24687;&#21644;&#31232;&#30095;&#32422;&#26463;&#38459;&#30861;&#20102;&#23545;&#26435;&#37325;&#31354;&#38388;&#30340;&#26377;&#25928;&#25506;&#32034;&#20197;&#21450;&#20934;&#30830;&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26410;&#30693;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25439;&#22833;&#20462;&#25913;&#12289;&#33258;&#21160;&#35843;&#25972;&#31574;&#30053;&#21644;&#25237;&#31080;&#26041;&#26696;&#65292;&#20197;&#24341;&#23548;&#26435;&#37325;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#24182;&#20943;&#36731;&#24050;&#30693;&#21644;&#26410;&#30693;&#20449;&#24687;&#20043;&#38388;&#30340;&#28151;&#28102;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#26174;&#33879;&#30340;&#39069;&#22806;&#25104;&#26412;&#25110;&#38656;&#35201;&#39069;&#22806;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20047v1 Announce Type: new  Abstract: Sparse training has emerged as a promising method for resource-efficient deep neural networks (DNNs) in real-world applications. However, the reliability of sparse models remains a crucial concern, particularly in detecting unknown out-of-distribution (OOD) data. This study addresses the knowledge gap by investigating the reliability of sparse training from an OOD perspective and reveals that sparse training exacerbates OOD unreliability. The lack of unknown information and the sparse constraints hinder the effective exploration of weight space and accurate differentiation between known and unknown knowledge. To tackle these challenges, we propose a new unknown-aware sparse training method, which incorporates a loss modification, auto-tuning strategy, and a voting scheme to guide weight space exploration and mitigate confusion between known and unknown information without incurring significant additional costs or requiring access to addi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20915;&#31574;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20449;&#24687;&#24615;&#21464;&#37327;&#26469;&#25552;&#39640;&#20215;&#26684;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.20033</link><description>&lt;p&gt;
&#20351;&#29992;&#24377;&#24615;&#32593;&#32476;&#21644;MOPSO&#36827;&#34892;&#38144;&#21806;&#20215;&#26684;&#39044;&#27979;&#30340;&#26032;&#22411;&#20915;&#31574;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20915;&#31574;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20449;&#24687;&#24615;&#21464;&#37327;&#26469;&#25552;&#39640;&#20215;&#26684;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#26684;&#39044;&#27979;&#31639;&#27861;&#26681;&#25454;&#24066;&#22330;&#36235;&#21183;&#12289;&#39044;&#26399;&#38656;&#27714;&#20197;&#21450;&#20854;&#20182;&#29305;&#24449;&#65288;&#21253;&#25324;&#25919;&#24220;&#35268;&#23450;&#12289;&#22269;&#38469;&#20132;&#26131;&#12289;&#25237;&#26426;&#21644;&#26399;&#26395;&#65289;&#20026;&#27599;&#31181;&#20135;&#21697;&#25110;&#26381;&#21153;&#25552;&#20986;&#20215;&#26684;&#12290;&#20316;&#20026;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#22240;&#21464;&#37327;&#65292;&#20215;&#26684;&#21463;&#21040;&#22810;&#20010;&#29420;&#31435;&#21644;&#30456;&#20851;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23545;&#20215;&#26684;&#39044;&#27979;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20801;&#35768;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#20215;&#26684;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#23545;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36755;&#20837;&#21464;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#25928;&#26524;&#19978;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#32423;&#34701;&#21512;&#26041;&#27861;&#26469;&#36873;&#25321;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#20449;&#24687;&#24615;&#21464;&#37327;&#12290;&#24314;&#35758;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#24179;&#34913;&#20102;&#20004;&#20010;&#31454;&#20105;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26088;&#22312;&#25913;&#21892;&#21033;&#29992;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20033v1 Announce Type: cross  Abstract: Price prediction algorithms propose prices for every product or service according to market trends, projected demand, and other characteristics, including government rules, international transactions, and speculation and expectation. As the dependent variable in price prediction, it is affected by several independent and correlated variables which may challenge the price prediction. To overcome this challenge, machine learning algorithms allow more accurate price prediction without explicitly modeling the relatedness between variables. However, as inputs increase, it challenges the existing machine learning approaches regarding computing efficiency and prediction effectiveness. Hence, this study introduces a novel decision level fusion approach to select informative variables in price prediction. The suggested metaheuristic algorithm balances two competitive objective functions, which are defined to improve the prediction utilized vari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#22312;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26080;&#21442;&#25968;&#36125;&#23572;&#26364;&#26144;&#23556;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;&#36924;&#36817;&#29305;&#24615;&#65292;&#26080;&#38656;&#23545;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#20063;&#19981;&#38656;&#35201;&#20102;&#35299;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#25805;&#20316;&#21644;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.20020</link><description>&lt;p&gt;
&#26080;&#21442;&#25968;&#36125;&#23572;&#26364;&#26144;&#23556;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65306;&#22312;&#40065;&#26834;&#33258;&#36866;&#24212;&#28388;&#27874;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Bellman Mappings for Reinforcement Learning: Application to Robust Adaptive Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#22312;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26080;&#21442;&#25968;&#36125;&#23572;&#26364;&#26144;&#23556;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;&#36924;&#36817;&#29305;&#24615;&#65292;&#26080;&#38656;&#23545;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#20063;&#19981;&#38656;&#35201;&#20102;&#35299;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#25805;&#20316;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#22312;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHSs&#65289;&#20013;&#30340;&#26080;&#21442;&#25968;&#36125;&#23572;&#26364;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26144;&#23556;&#21033;&#29992;RKHSs&#30340;&#20016;&#23500;&#36924;&#36817;&#29305;&#24615;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#26080;&#21442;&#25968;&#29305;&#24615;&#65292;&#19981;&#23545;&#25968;&#25454;&#30340;&#32479;&#35745;&#24615;&#36136;&#20316;&#20219;&#20309;&#20551;&#35774;&#65292;&#20063;&#19981;&#38656;&#35201;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36716;&#31227;&#27010;&#29575;&#26377;&#20219;&#20309;&#20102;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20801;&#35768;&#36890;&#36807;&#35774;&#35745;&#36712;&#36857;&#26679;&#26412;&#36827;&#34892;&#21363;&#26102;&#25277;&#26679;&#65292;&#36890;&#36807;&#32463;&#39564;&#37325;&#25918;&#37325;&#22797;&#20351;&#29992;&#36807;&#21435;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36890;&#36807;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#23454;&#29616;&#32500;&#24230;&#38477;&#20302;&#65292;&#24182;&#23454;&#29616;&#35745;&#31639;&#36731;&#37327;&#32423;&#25805;&#20316;&#20197;&#36866;&#24212;&#39640;&#25928;&#30340;&#22312;&#32447;&#25110;&#26102;&#38388;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21464;&#20998;&#26694;&#26550;&#26469;&#35774;&#35745;&#25152;&#25552;&#20986;&#30340;&#36125;&#23572;&#26364;&#26144;&#23556;&#30340;&#33258;&#30001;&#21442;&#25968;&#65292;&#24182;&#23637;&#31034;&#36866;&#24403;&#36873;&#25321;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#20135;&#29983;&#20960;&#31181;&#27969;&#34892;&#30340;&#36125;&#23572;&#26364;&#26144;&#23556;&#35774;&#35745;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20020v1 Announce Type: cross  Abstract: This paper designs novel nonparametric Bellman mappings in reproducing kernel Hilbert spaces (RKHSs) for reinforcement learning (RL). The proposed mappings benefit from the rich approximating properties of RKHSs, adopt no assumptions on the statistics of the data owing to their nonparametric nature, require no knowledge on transition probabilities of Markov decision processes, and may operate without any training data. Moreover, they allow for sampling on-the-fly via the design of trajectory samples, re-use past test data via experience replay, effect dimensionality reduction by random Fourier features, and enable computationally lightweight operations to fit into efficient online or time-adaptive learning. The paper offers also a variational framework to design the free parameters of the proposed Bellman mappings, and shows that appropriate choices of those parameters yield several popular Bellman-mapping designs. As an application, t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#21033;&#29992;LiDAR&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25513;&#25252;&#22320;&#22270;&#21644;&#28508;&#22312;&#23041;&#32961;&#22320;&#22270;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#25513;&#25252;&#21033;&#29992;&#12289;&#26368;&#23567;&#21270;&#26292;&#38706;&#20110;&#23041;&#32961;&#24182;&#39640;&#25928;&#21040;&#36798;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.20016</link><description>&lt;p&gt;
EnCoMP: &#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#30340;&#38544;&#34109;&#26426;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20016
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#21033;&#29992;LiDAR&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25513;&#25252;&#22320;&#22270;&#21644;&#28508;&#22312;&#23041;&#32961;&#22320;&#22270;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#25513;&#25252;&#21033;&#29992;&#12289;&#26368;&#23567;&#21270;&#26292;&#38706;&#20110;&#23041;&#32961;&#24182;&#39640;&#25928;&#21040;&#36798;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#38544;&#34109;&#23548;&#33322;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#38656;&#35201;&#35782;&#21035;&#21644;&#21033;&#29992;&#29615;&#22659;&#25513;&#25252;&#21516;&#26102;&#20445;&#25345;&#26377;&#25928;&#23548;&#33322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#35782;&#21035;&#21644;&#21033;&#29992;&#33258;&#28982;&#21644;&#20154;&#24037;&#29615;&#22659;&#29305;&#24449;&#20316;&#20026;&#25513;&#25252;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#26292;&#38706;&#20110;&#28508;&#22312;&#23041;&#32961;&#20043;&#19979;&#12290;&#25105;&#20204;&#30340;&#24863;&#30693;&#31649;&#36947;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#25513;&#25252;&#22320;&#22270;&#21644;&#28508;&#22312;&#23041;&#32961;&#22320;&#22270;&#65292;&#25552;&#20379;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#23454;&#38469;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#23398;&#20064;&#19968;&#20010;&#35780;&#20272;&#20505;&#36873;&#34892;&#21160;&#36136;&#37327;&#30340;&#24378;&#20581;&#31574;&#30053;&#65292;&#22522;&#20110;&#23427;&#20204;&#26368;&#22823;&#21270;&#25513;&#25252;&#21033;&#29992;&#12289;&#26368;&#23567;&#21270;&#26292;&#38706;&#20110;&#23041;&#32961;&#21644;&#39640;&#25928;&#21040;&#36798;&#30446;&#26631;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#30340;&#23454;&#38469;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20016v1 Announce Type: cross  Abstract: Cover navigation in complex environments is a critical challenge for autonomous robots, requiring the identification and utilization of environmental cover while maintaining efficient navigation. We propose an enhanced navigation system that enables robots to identify and utilize natural and artificial environmental features as cover, thereby minimizing exposure to potential threats. Our perception pipeline leverages LiDAR data to generate high-fidelity cover maps and potential threat maps, providing a comprehensive understanding of the surrounding environment. We train an offline reinforcement learning model using a diverse dataset collected from real-world environments, learning a robust policy that evaluates the quality of candidate actions based on their ability to maximize cover utilization, minimize exposure to threats, and reach the goal efficiently. Extensive real-world experiments demonstrate the superiority of our approach in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#21644;&#36755;&#20986; token &#27010;&#29575;&#21160;&#24577;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.20009</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Large Language Models' Hallucination with Regard to Known Facts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20009
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#21644;&#36755;&#20986; token &#27010;&#29575;&#21160;&#24577;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#20107;&#23454;&#31867;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#20063;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;LLMs&#20855;&#26377;&#27491;&#30830;&#31572;&#26696;&#30693;&#35782;&#21364;&#20173;&#28982;&#20135;&#29983;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#36825;&#26159;&#20197;&#24448;&#20851;&#20110;&#24187;&#35273;&#30740;&#31350;&#23578;&#26410;&#28085;&#30422;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#24605;&#36335;&#36827;&#34892;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26597;&#35810;&#30456;&#21516;&#19977;&#20803;&#30693;&#35782;&#20294;&#23548;&#33268;&#19981;&#21516;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#12290;&#27169;&#22411;&#22312;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#36755;&#20986;&#19978;&#30340;&#34892;&#20026;&#24046;&#24322;&#22240;&#27492;&#26263;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#27169;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#21097;&#20313;&#27969;&#21040;&#35789;&#27719;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#36755;&#20986;&#20196;&#29260;&#27010;&#29575;&#22312;&#27491;&#30830;&#21644;&#24187;&#35273;&#24773;&#20917;&#19979;&#22312;&#23618;&#28145;&#24230;&#19978;&#30340;&#19981;&#21516;&#21160;&#24577;&#12290;&#22312;&#24187;&#35273;&#26696;&#20363;&#20013;&#65292;&#36755;&#20986;&#20196;&#29260;&#30340;&#20449;&#24687;&#24456;&#23569;&#34920;&#29616;&#20986;&#31361;&#22686;&#21644;&#25345;&#32493;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20009v1 Announce Type: new  Abstract: Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen. Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space. We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token's information rarely demonstrates abrupt increases and consistent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24322;&#26500;IoT&#20256;&#24863;&#22120;&#25968;&#25454;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19996</link><description>&lt;p&gt;
DeepHeteroIoT&#65306;&#22522;&#20110;&#24322;&#26500;IoT&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#28145;&#24230;&#23616;&#37096;&#21644;&#20840;&#23616;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24322;&#26500;IoT&#20256;&#24863;&#22120;&#25968;&#25454;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#20256;&#24863;&#22120;&#25968;&#25454;&#25110;&#35835;&#25968;&#22312;&#26102;&#38388;&#25139;&#33539;&#22260;&#12289;&#37319;&#26679;&#39057;&#29575;&#12289;&#22320;&#29702;&#20301;&#32622;&#12289;&#27979;&#37327;&#21333;&#20301;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#24046;&#24322;&#12290;&#36825;&#31181;&#21576;&#29616;&#30340;&#24207;&#21015;&#25968;&#25454;&#24322;&#36136;&#24615;&#20351;&#20256;&#32479;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#38590;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#24322;&#36136;&#24615;&#25361;&#25112;&#38656;&#35201;&#23398;&#20064;&#19981;&#20165;&#23376;&#27169;&#24335;&#65288;&#23616;&#37096;&#29305;&#24449;&#65289;&#32780;&#19988;&#24635;&#20307;&#27169;&#24335;&#65288;&#20840;&#23616;&#29305;&#24449;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#31867;&#24322;&#26500;IoT&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#31867;&#22411;&#22914;&#28201;&#24230;&#21644;&#28287;&#24230;&#36827;&#34892;&#20998;&#31867;&#65289;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65292;&#20998;&#21035;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#12290;&#36890;&#36807;&#23545;&#24322;&#26500;IoT&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19996v1 Announce Type: new  Abstract: Internet of Things (IoT) sensor data or readings evince variations in timestamp range, sampling frequency, geographical location, unit of measurement, etc. Such presented sequence data heterogeneity makes it difficult for traditional time series classification algorithms to perform well. Therefore, addressing the heterogeneity challenge demands learning not only the sub-patterns (local features) but also the overall pattern (global feature). To address the challenge of classifying heterogeneous IoT sensor data (e.g., categorizing sensor data types like temperature and humidity), we propose a novel deep learning model that incorporates both Convolutional Neural Network and Bi-directional Gated Recurrent Unit to learn local and global features respectively, in an end-to-end manner. Through rigorous experimentation on heterogeneous IoT sensor datasets, we validate the effectiveness of our proposed model, which outperforms recent state-of-th
&lt;/p&gt;</description></item><item><title>&#36866;&#37197;&#22120;&#35843;&#25972;&#26041;&#27861;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#36739;&#20248;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#35843;&#25972;&#20849;&#20139;&#36866;&#37197;&#22120;&#21644;&#21033;&#29992;&#23384;&#20648;&#21407;&#22411;&#36827;&#34892;&#29305;&#24449;&#37319;&#26679;&#21644;&#26356;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19979</link><description>&lt;p&gt;
&#35821;&#20041;&#36716;&#31227;&#22686;&#37327;&#36866;&#37197;&#22120;&#35843;&#25972;&#26159;&#19968;&#31181;&#25345;&#32493;&#30340; ViTransformer
&lt;/p&gt;
&lt;p&gt;
Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19979
&lt;/p&gt;
&lt;p&gt;
&#36866;&#37197;&#22120;&#35843;&#25972;&#26041;&#27861;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#36739;&#20248;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#35843;&#25972;&#20849;&#20139;&#36866;&#37197;&#22120;&#21644;&#21033;&#29992;&#23384;&#20648;&#21407;&#22411;&#36827;&#34892;&#29305;&#24449;&#37319;&#26679;&#21644;&#26356;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26088;&#22312;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#25345;&#32493;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#19981;&#21516;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36866;&#37197;&#22120;&#35843;&#25972;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#27599;&#20010;&#23398;&#20064;&#20250;&#35805;&#20013;&#27809;&#26377;&#21442;&#25968;&#25193;&#23637;&#30340;&#24773;&#20917;&#19979;&#20063;&#22914;&#27492;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#35843;&#25972;&#20849;&#20139;&#36866;&#37197;&#22120;&#32780;&#19981;&#26045;&#21152;&#21442;&#25968;&#26356;&#26032;&#32422;&#26463;&#65292;&#22686;&#24378;&#39592;&#24178;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#23384;&#20648;&#30340;&#21407;&#22411;&#20013;&#25277;&#21462;&#29305;&#24449;&#26679;&#26412;&#26469;&#37325;&#26032;&#35757;&#32451;&#32479;&#19968;&#30340;&#20998;&#31867;&#22120;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#20272;&#35745;&#26087;&#21407;&#22411;&#30340;&#35821;&#20041;&#36716;&#31227;&#65292;&#32780;&#26080;&#27861;&#35775;&#38382;&#36807;&#21435;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#20010;&#20250;&#35805;&#26356;&#26032;&#23384;&#20648;&#30340;&#21407;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#27169;&#22411;&#30340;&#25193;&#23637;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19979v1 Announce Type: cross  Abstract: Class-incremental learning (CIL) aims to enable models to continuously learn new classes while overcoming catastrophic forgetting. The introduction of pre-trained models has brought new tuning paradigms to CIL. In this paper, we revisit different parameter-efficient tuning (PET) methods within the context of continual learning. We observe that adapter tuning demonstrates superiority over prompt-based methods, even without parameter expansion in each learning session. Motivated by this, we propose incrementally tuning the shared adapter without imposing parameter update constraints, enhancing the learning capacity of the backbone. Additionally, we employ feature sampling from stored prototypes to retrain a unified classifier, further improving its performance. We estimate the semantic shift of old prototypes without access to past samples and update stored prototypes session by session. Our proposed method eliminates model expansion and
&lt;/p&gt;</description></item><item><title>SMART&#21098;&#26525;&#22120;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#25513;&#30721;&#12289;&#21487;&#24494;&#30340;Top k&#36816;&#31639;&#31526;&#21644;&#21160;&#24577;&#28201;&#24230;&#21442;&#25968;&#25216;&#24039;&#65292;&#22312;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19969</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#20998;&#31163;&#12289;&#21160;&#24577;&#21644;&#21487;&#24494;&#65288;SMART&#65289;&#21098;&#26525;&#22120;
&lt;/p&gt;
&lt;p&gt;
Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19969
&lt;/p&gt;
&lt;p&gt;
SMART&#21098;&#26525;&#22120;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#25513;&#30721;&#12289;&#21487;&#24494;&#30340;Top k&#36816;&#31639;&#31526;&#21644;&#21160;&#24577;&#28201;&#24230;&#21442;&#25968;&#25216;&#24039;&#65292;&#22312;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21098;&#26525;&#24050;&#34987;&#35270;&#20026;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12289;&#25913;&#21892;&#25512;&#29702;&#24310;&#36831;&#20197;&#21450;&#38477;&#20302;DNN&#21152;&#36895;&#22120;&#21151;&#32791;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#22312;&#21508;&#31181;&#21098;&#26525;&#25216;&#26415;&#20013;&#65292;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#22312;&#21152;&#36895;&#30828;&#20214;&#24615;&#33021;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#36890;&#24120;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#31163;&#12289;&#21160;&#24577;&#21644;&#21487;&#24494;&#65288;SMART&#65289;&#21098;&#26525;&#22120;&#12290;&#35813;&#21098;&#26525;&#22120;&#21033;&#29992;&#19968;&#20010;&#29420;&#31435;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#25513;&#30721;&#36827;&#34892;&#26435;&#37325;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#37319;&#29992;&#21487;&#24494;&#30340;Top k&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24615;&#65292;&#24182;&#21033;&#29992;&#21160;&#24577;&#28201;&#24230;&#21442;&#25968;&#25216;&#24039;&#26469;&#36867;&#31163;&#38750;&#31232;&#30095;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;SMART&#21098;&#26525;&#22120;&#22312;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#30340;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#19978;&#19968;&#30452;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19969v1 Announce Type: cross  Abstract: Deep Neural Network (DNN) pruning has emerged as a key strategy to reduce model size, improve inference latency, and lower power consumption on DNN accelerators. Among various pruning techniques, block and output channel pruning have shown significant potential in accelerating hardware performance. However, their accuracy often requires further improvement. In response to this challenge, we introduce a separate, dynamic and differentiable (SMART) pruner. This pruner stands out by utilizing a separate, learnable probability mask for weight importance ranking, employing a differentiable Top k operator to achieve target sparsity, and leveraging a dynamic temperature parameter trick to escape from non-sparse local minima. In our experiments, the SMART pruner consistently demonstrated its superiority over existing pruning methods across a wide range of tasks and models on block and output channel pruning. Additionally, we extend our testing
&lt;/p&gt;</description></item><item><title>FairRAG&#26694;&#26550;&#36890;&#36807;&#22312;&#22806;&#37096;&#22270;&#20687;&#25968;&#25454;&#24211;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#20687;&#26469;&#25552;&#39640;&#20154;&#31867;&#29983;&#25104;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#24212;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21435;&#20559;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.19964</link><description>&lt;p&gt;
FairRAG: &#20844;&#24179;&#20154;&#31867;&#29983;&#25104;&#30340;&#20844;&#24179;&#26816;&#32034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
FairRAG: Fair Human Generation via Fair Retrieval Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19964
&lt;/p&gt;
&lt;p&gt;
FairRAG&#26694;&#26550;&#36890;&#36807;&#22312;&#22806;&#37096;&#22270;&#20687;&#25968;&#25454;&#24211;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#20687;&#26469;&#25552;&#39640;&#20154;&#31867;&#29983;&#25104;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#24212;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21435;&#20559;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21453;&#26144;&#29978;&#33267;&#25918;&#22823;&#20102;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#26681;&#28145;&#33922;&#22266;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#23545;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#27169;&#22411;&#20559;&#21521;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#32452;&#12290;&#29616;&#26377;&#30340;&#32416;&#27491;&#27492;&#38382;&#39064;&#30340;&#23581;&#35797;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#22266;&#26377;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#26410;&#33021;&#22312;&#26681;&#26412;&#19978;&#25913;&#21892;&#20154;&#21475;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20844;&#24179;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;FairRAG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#26469;&#33258;&#22806;&#37096;&#22270;&#20687;&#25968;&#25454;&#24211;&#30340;&#21442;&#32771;&#22270;&#20687;&#19978;&#36827;&#34892;&#26465;&#20214;&#21270;&#26469;&#25552;&#39640;&#20154;&#31867;&#29983;&#25104;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;FairRAG&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#32447;&#24615;&#27169;&#22359;&#23454;&#29616;&#26465;&#20214;&#21270;&#65292;&#23558;&#21442;&#32771;&#22270;&#20687;&#25237;&#23556;&#21040;&#25991;&#26412;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;FairRAG&#24212;&#29992;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#22270;&#20687;&#12290;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19964v1 Announce Type: cross  Abstract: Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#30340;&#25968;&#25454;&#24182;&#32454;&#35843;&#27169;&#22411;&#20197;&#21450;&#35774;&#35745;&#33021;&#22815;&#26377;&#25928;&#28608;&#27963;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#20302;&#21442;&#25968;LLMs&#30340;&#36890;&#29992;&#20195;&#29702;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19962</link><description>&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;&#21644;&#22810;&#25903;&#36335;&#25512;&#29702;&#22686;&#24378;&#20302;&#21442;&#25968;LLMs&#30340;&#36890;&#29992;&#20195;&#29702;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19962
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#30340;&#25968;&#25454;&#24182;&#32454;&#35843;&#27169;&#22411;&#20197;&#21450;&#35774;&#35745;&#33021;&#22815;&#26377;&#25928;&#28608;&#27963;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#20302;&#21442;&#25968;LLMs&#30340;&#36890;&#29992;&#20195;&#29702;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19962v1 &#22768;&#26126;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#24320;&#28304;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#23427;&#20204;&#29992;&#20316;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#38382;&#39064;&#30340;&#20195;&#29702;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#36828;&#36828;&#19981;&#21450;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#21830;&#29992;&#27169;&#22411;&#12290;&#20316;&#20026;&#26234;&#33021;&#20195;&#29702;&#65292;LLMs&#38656;&#35201;&#20855;&#22791;&#20219;&#21153;&#35268;&#21010;&#12289;&#38271;&#26399;&#35760;&#24518;&#20197;&#21450;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#21508;&#31181;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#12290;&#19968;&#26041;&#38754;&#65292;&#26377;&#20123;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#30340;&#25968;&#25454;&#21644;&#24494;&#35843;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#26041;&#27861;&#38598;&#20013;&#20110;&#35774;&#35745;&#33021;&#26377;&#25928;&#28608;&#27963;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;7B&#21644;13B&#27169;&#22411;&#19978;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-4&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#25968;&#25454;&#30340;&#20840;&#38754;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19962v1 Announce Type: cross  Abstract: Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. T
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;OOD&#27867;&#21270;&#35774;&#32622;&#20013;&#30340;&#32622;&#20449;&#38598;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24418;&#25104;&#26377;&#20449;&#24515;&#30340;&#39044;&#27979;&#38598;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.19950</link><description>&lt;p&gt;
&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#39044;&#27979;&#38598;&#21512;&#29992;&#20110;OOD&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19950
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;OOD&#27867;&#21270;&#35774;&#32622;&#20013;&#30340;&#32622;&#20449;&#38598;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24418;&#25104;&#26377;&#20449;&#24515;&#30340;&#39044;&#27979;&#38598;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#36817;&#24180;&#26469;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;OOD&#27867;&#21270;&#35774;&#32622;&#20013;&#30340;&#32622;&#20449;&#38598;&#39044;&#27979;&#38382;&#39064;&#12290; &#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;SCP&#65289;&#26159;&#22788;&#29702;&#32622;&#20449;&#38598;&#39044;&#27979;&#38382;&#39064;&#30340;&#26377;&#25928;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#39046;&#22495;&#19982;&#28304;&#39046;&#22495;&#19981;&#21516;&#26102;&#65292;&#31616;&#21333;&#24212;&#29992;SCP&#23548;&#33268;&#26080;&#27861;&#32500;&#25345;&#36793;&#38469;&#35206;&#30422;&#30340;&#22833;&#36133;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;OOD&#35774;&#32622;&#20013;&#24418;&#25104;&#26377;&#20449;&#24515;&#30340;&#39044;&#27979;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#29702;&#35770;&#30340;&#27491;&#30830;&#24615;&#21644;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19950v1 Announce Type: new  Abstract: Out-of-distribution (OOD) generalization has attracted increasing research attention in recent years, due to its promising experimental results in real-world applications. In this paper,we study the confidence set prediction problem in the OOD generalization setting. Split conformal prediction (SCP) is an efficient framework for handling the confidence set prediction problem. However, the validity of SCP requires the examples to be exchangeable, which is violated in the OOD setting. Empirically, we show that trivially applying SCP results in a failure to maintain the marginal coverage when the unseen target domain is different from the source domain. To address this issue, we develop a method for forming confident prediction sets in the OOD setting and theoretically prove the validity of our method. Finally, we conduct experiments on simulated data to empirically verify the correctness of our theory and the validity of our proposed metho
&lt;/p&gt;</description></item><item><title>TDANet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#21435;&#22122;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#21892;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19943</link><description>&lt;p&gt;
TDANet&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#21435;&#22122;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TDANet: A Novel Temporal Denoise Convolutional Neural Network With Attention for Fault Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19943
&lt;/p&gt;
&lt;p&gt;
TDANet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#21435;&#22122;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#21892;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#22312;&#32500;&#25252;&#26426;&#26800;&#31995;&#32479;&#30340;&#36816;&#34892;&#23436;&#25972;&#24615;&#65292;&#36991;&#20813;&#30001;&#20110;&#24847;&#22806;&#25925;&#38556;&#36896;&#25104;&#30340;&#37325;&#22823;&#25439;&#22833;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDANet&#30340;&#26102;&#38388;&#21435;&#22122;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#21892;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#36136;&#23558;&#19968;&#32500;&#20449;&#21495;&#36716;&#25442;&#20026;&#20108;&#32500;&#24352;&#37327;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;2D&#21367;&#31215;&#26680;&#20174;&#21608;&#26399;&#20869;&#37096;&#21644;&#36328;&#21608;&#26399;&#25552;&#21462;&#20449;&#21495;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#35782;&#21035;&#20449;&#21495;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19943v1 Announce Type: cross  Abstract: Fault diagnosis plays a crucial role in maintaining the operational integrity of mechanical systems, preventing significant losses due to unexpected failures. As intelligent manufacturing and data-driven approaches evolve, Deep Learning (DL) has emerged as a pivotal technique in fault diagnosis research, recognized for its ability to autonomously extract complex features. However, the practical application of current fault diagnosis methods is challenged by the complexity of industrial environments. This paper proposed the Temporal Denoise Convolutional Neural Network With Attention (TDANet), designed to improve fault diagnosis performance in noise environments. This model transforms one-dimensional signals into two-dimensional tensors based on their periodic properties, employing multi-scale 2D convolution kernels to extract signal information both within and across periods. This method enables effective identification of signal chara
&lt;/p&gt;</description></item><item><title>DiJiang&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19928</link><description>&lt;p&gt;
DiJiang&#65306;&#36890;&#36807;&#32039;&#20945;&#30340;&#26680;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiJiang: Efficient Large Language Models through Compact Kernelization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19928
&lt;/p&gt;
&lt;p&gt;
DiJiang&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;Transformers&#30340;&#35745;&#31639;&#36127;&#33655;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#26426;&#21046;&#30340;&#25913;&#36827;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#32463;&#36807;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DiJiang&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#36716;&#21270;&#20026;&#20855;&#26377;&#36739;&#23567;&#35757;&#32451;&#25104;&#26412;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#25311;&#38543;&#26426;&#37319;&#26679;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#35757;&#32451;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#30340;&#26680;&#26041;&#27861;&#22522;&#20110;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#25805;&#20316;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#19982;&#21407;&#22987;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#35757;&#32451;&#26102;&#38388;&#22823;&#22823;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19928v1 Announce Type: new  Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;Mamba&#26694;&#26550;&#25972;&#21512;&#21040;Decision Transformer&#26550;&#26500;&#20013;&#65292;&#25552;&#20986;&#20102;Decision Mamba&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#20915;&#31574;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#34920;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;</title><link>https://arxiv.org/abs/2403.19925</link><description>&lt;p&gt;
&#20915;&#31574;&#24040;&#34770;&#65306;&#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;Mamba&#26694;&#26550;&#25972;&#21512;&#21040;Decision Transformer&#26550;&#26500;&#20013;&#65292;&#25552;&#20986;&#20102;Decision Mamba&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#20915;&#31574;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#34920;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Decision Transformer&#26159;&#19968;&#31181;&#23558;Transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#24207;&#21015;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Mamba&#26694;&#26550;&#30340;&#25972;&#21512;&#65292;&#35813;&#26694;&#26550;&#20197;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#23558;&#20854;&#25972;&#21512;&#21040;Decision Transformer&#26550;&#26500;&#20013;&#65292;&#20851;&#27880;&#22312;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#28508;&#22312;&#30340;&#24615;&#33021;&#22686;&#24378;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21508;&#31181;&#20915;&#31574;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#31995;&#32479;&#35780;&#20272;&#36825;&#31181;&#25972;&#21512;&#65292;&#23558;&#20462;&#25913;&#21518;&#30340;Decision Transformer&#65292;Decision Mamba&#65292;&#19982;&#20256;&#32479;&#23545;&#24212;&#29289;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#39033;&#24037;&#20316;&#20419;&#36827;&#20102;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20854;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19925v1 Announce Type: cross  Abstract: Decision Transformer, a promising approach that applies Transformer architectures to reinforcement learning, relies on causal self-attention to model sequences of states, actions, and rewards. While this method has shown competitive results, this paper investigates the integration of the Mamba framework, known for its advanced capabilities in efficient and effective sequence modeling, into the Decision Transformer architecture, focusing on the potential performance enhancements in sequential decision-making tasks. Our study systematically evaluates this integration by conducting a series of experiments across various decision-making environments, comparing the modified Decision Transformer, Decision Mamba, with its traditional counterpart. This work contributes to the advancement of sequential decision-making models, suggesting that the architecture and training methodology of neural networks can significantly impact their performance 
&lt;/p&gt;</description></item><item><title>CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.19918</link><description>&lt;p&gt;
CtRL-Sim&#65306;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#24212;&#24615;&#21487;&#25511;&#39550;&#39542;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19918
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CtRL-Sim&#65292;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#22686;&#24378;&#30340;Nocturne&#27169;&#25311;&#22120;&#20013;&#30340;&#22238;&#25253;&#26465;&#20214;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#39640;&#25928;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;Nocturne&#27169;&#25311;&#22120;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19918v1 Announce Type: cross  Abstract: Evaluating autonomous vehicle stacks (AVs) in simulation typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from offline data do not react to the actions of the AV, and their behaviour cannot be easily controlled to simulate counterfactual scenarios. Existing approaches have attempted to address these shortcomings by proposing methods that rely on heuristics or learned generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned offline reinforcement learning within a physics-enhanced Nocturne simulator to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through the Nocturne simulator to generate a diverse offli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.19913</link><description>&lt;p&gt;
MANGO&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MANGO&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#25191;&#34892;&#22522;&#20110;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;&#19968;&#22871;&#25991;&#26412;&#28216;&#25103;&#30340;53&#20010;&#36855;&#23467;&#65306;&#27599;&#20010;&#36855;&#23467;&#37117;&#19982;&#19968;&#20010;&#28216;&#35272;&#35828;&#26126;&#37197;&#23545;&#65292;&#20854;&#20013;&#21253;&#21547;&#27599;&#20010;&#20301;&#32622;&#30340;&#35775;&#38382;&#20294;&#19981;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#36335;&#24452;&#12290;&#20219;&#21153;&#26159;&#38382;&#31572;&#65306;&#23545;&#20110;&#27599;&#20010;&#36855;&#23467;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35835;&#21462;&#28216;&#35272;&#35828;&#26126;&#24182;&#22238;&#31572;&#25968;&#30334;&#20010;&#26144;&#23556;&#21644;&#23548;&#33322;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#20320;&#24212;&#35813;&#20174;&#25151;&#23376;&#35199;&#37096;&#22914;&#20309;&#21435;&#38401;&#27004;&#65311;&#8221;&#21644;&#8220;&#22914;&#26524;&#25105;&#20204;&#20174;&#22320;&#19979;&#23460;&#21521;&#21271;&#21644;&#19996;&#36208;&#65292;&#25105;&#20204;&#20250;&#22312;&#21738;&#37324;&#65311;&#8221;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#65292;&#20294;&#20107;&#23454;&#35777;&#26126;&#65292;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#29978;&#33267;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#23558;&#26377;&#21033;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19913v1 Announce Type: cross  Abstract: Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as "How should you go to Attic from West of House?" and "Where are we if we go north and east from Cellar?". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ORAL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#19990;&#30028;&#22270;&#23398;&#20064;&#20013;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#21407;&#22411;&#23398;&#20064;&#21644;&#21407;&#22411;&#27880;&#24847;&#21147;&#32593;&#32476;&#35299;&#20915;&#20102;&#26032;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19907</link><description>&lt;p&gt;
&#36229;&#36234;&#24050;&#30693;&#65306;&#24320;&#25918;&#19990;&#30028;&#22270;&#23398;&#20064;&#20013;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Beyond the Known: Novel Class Discovery for Open-world Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ORAL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#19990;&#30028;&#22270;&#23398;&#20064;&#20013;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#21407;&#22411;&#23398;&#20064;&#21644;&#21407;&#22411;&#27880;&#24847;&#21147;&#32593;&#32476;&#35299;&#20915;&#20102;&#26032;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#24320;&#25918;&#22330;&#26223;&#20013;&#26631;&#35760;&#33021;&#21147;&#26377;&#38480;&#19988;&#21457;&#23637;&#36805;&#36895;&#65292;&#26410;&#26631;&#35760;&#27979;&#35797;&#33410;&#28857;&#19978;&#21487;&#33021;&#20250;&#20986;&#29616;&#26032;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#22270;&#19978;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;&#20851;&#27880;&#36739;&#23569;&#12290;&#30001;&#20110;&#22270;&#20013;&#30340;&#26032;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#33410;&#28857;&#30001;&#36793;&#30456;&#20851;&#32852;&#65292;&#22240;&#27492;&#24403;&#24212;&#29992;&#28040;&#24687;&#20256;&#36882;GNN&#26102;&#23427;&#20204;&#30340;&#34920;&#31034;&#26159;&#19981;&#21487;&#21306;&#20998;&#30340;&#12290;&#27492;&#22806;&#65292;&#26032;&#31867;&#21035;&#32570;&#20047;&#26631;&#31614;&#20449;&#24687;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Open-world gRAph neuraL network (ORAL)&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;ORAL&#39318;&#20808;&#36890;&#36807;&#21322;&#30417;&#30563;&#21407;&#22411;&#23398;&#20064;&#26816;&#27979;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#36890;&#36807;&#21407;&#22411;&#27880;&#24847;&#21147;&#32593;&#32476;&#28040;&#38500;&#31867;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#31867;&#21035;&#25552;&#20379;&#29420;&#29305;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20805;&#20998;&#25506;&#32034;mu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19907v1 Announce Type: cross  Abstract: Node classification on graphs is of great importance in many applications. Due to the limited labeling capability and evolution in real-world open scenarios, novel classes can emerge on unlabeled testing nodes. However, little attention has been paid to novel class discovery on graphs. Discovering novel classes is challenging as novel and known class nodes are correlated by edges, which makes their representations indistinguishable when applying message passing GNNs. Furthermore, the novel classes lack labeling information to guide the learning process. In this paper, we propose a novel method Open-world gRAph neuraL network (ORAL) to tackle these challenges. ORAL first detects correlations between classes through semi-supervised prototypical learning. Inter-class correlations are subsequently eliminated by the prototypical attention network, leading to distinctive representations for different classes. Furthermore, to fully explore mu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#38754;&#37096;&#22270;&#20687;&#20010;&#20307;&#31181;&#26063;&#30456;&#20851;&#34920;&#22411;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#65292;&#20174;&#32780;&#20998;&#31163;&#31181;&#26063;&#30456;&#20851;&#30340;&#34920;&#22411;&#29305;&#24449;&#65292;&#20026;&#25968;&#25454;&#39537;&#21160;&#30340;&#31181;&#26063;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#25552;&#20379;&#20851;&#38190;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.19897</link><description>&lt;p&gt;
&#20998;&#31163;&#31181;&#26063;&#34920;&#22411;&#65306;&#23545;&#31181;&#26063;&#30456;&#20851;&#38754;&#37096;&#34920;&#22411;&#29305;&#24449;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Disentangling Racial Phenotypes: Fine-Grained Control of Race-related Facial Phenotype Characteristics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#38754;&#37096;&#22270;&#20687;&#20010;&#20307;&#31181;&#26063;&#30456;&#20851;&#34920;&#22411;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#65292;&#20174;&#32780;&#20998;&#31163;&#31181;&#26063;&#30456;&#20851;&#30340;&#34920;&#22411;&#29305;&#24449;&#65292;&#20026;&#25968;&#25454;&#39537;&#21160;&#30340;&#31181;&#26063;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#25552;&#20379;&#20851;&#38190;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#29616;&#23545;2D&#38754;&#37096;&#22270;&#20687;&#30340;&#26377;&#25928;&#30340;&#32454;&#31890;&#24230;&#22806;&#35266;&#21464;&#21270;&#65292;&#21516;&#26102;&#20445;&#30041;&#38754;&#37096;&#36523;&#20221;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#24120;&#35265;&#30340;2D&#38754;&#37096;&#29305;&#24449;&#32534;&#30721;&#31354;&#38388;&#20855;&#26377;&#24456;&#39640;&#30340;&#22797;&#26434;&#24615;&#21644;&#32416;&#32544;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#35299;&#26469;&#23454;&#29616;&#36825;&#31181;&#32454;&#31890;&#24230;&#25511;&#21046;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#31181;&#26063;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#20998;&#26512;&#12289;&#34920;&#24449;&#21644;&#21512;&#25104;&#20154;&#31867;&#38754;&#23380;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;GAN&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#38754;&#37096;&#22270;&#20687;&#20010;&#20307;&#31181;&#26063;&#30456;&#20851;&#34920;&#22411;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#28508;&#22312;&#65288;&#29305;&#24449;&#65289;&#31354;&#38388;&#20998;&#35299;&#20026;&#30456;&#24212;&#20110;&#31181;&#26063;&#30456;&#20851;&#38754;&#37096;&#34920;&#22411;&#34920;&#31034;&#30340;&#20803;&#32032;&#65292;&#20174;&#32780;&#20998;&#31163;&#34920;&#22411;&#26041;&#38754;&#65288;&#20363;&#22914;&#30382;&#32932;&#12289;&#22836;&#21457;&#39068;&#33394;&#12289;&#40763;&#23376;&#12289;&#30524;&#30555;&#12289;&#22068;&#24052;&#24418;&#29366;&#65289;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38754;&#37096;&#25968;&#25454;&#20013;&#26159;&#26497;&#20854;&#38590;&#20197;&#40065;&#26834;&#22320;&#26631;&#27880;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19897v1 Announce Type: cross  Abstract: Achieving an effective fine-grained appearance variation over 2D facial images, whilst preserving facial identity, is a challenging task due to the high complexity and entanglement of common 2D facial feature encoding spaces. Despite these challenges, such fine-grained control, by way of disentanglement is a crucial enabler for data-driven racial bias mitigation strategies across multiple automated facial analysis tasks, as it allows to analyse, characterise and synthesise human facial diversity. In this paper, we propose a novel GAN framework to enable fine-grained control over individual race-related phenotype attributes of the facial images. Our framework factors the latent (feature) space into elements that correspond to race-related facial phenotype representations, thereby separating phenotype aspects (e.g. skin, hair colour, nose, eye, mouth shapes), which are notoriously difficult to annotate robustly in real-world facial data.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#24102;&#26377;&#29978;&#33267;&#31435;&#26041;&#38750;&#32447;&#24615;&#30340;&#31616;&#21333;&#23454;&#29616;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#20248;&#21270;&#21442;&#25968;&#20351;&#24471;&#28608;&#27963;&#20989;&#25968;&#20855;&#26377;&#26356;&#22823;&#30340;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22826;&#22810;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.19896</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#22686;&#24378;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Nonlinearity Enhanced Adaptive Activation Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19896
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#24102;&#26377;&#29978;&#33267;&#31435;&#26041;&#38750;&#32447;&#24615;&#30340;&#31616;&#21333;&#23454;&#29616;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#20248;&#21270;&#21442;&#25968;&#20351;&#24471;&#28608;&#27963;&#20989;&#25968;&#20855;&#26377;&#26356;&#22823;&#30340;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22826;&#22810;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#23454;&#29616;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20855;&#26377;&#29978;&#33267;&#31435;&#26041;&#38750;&#32447;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#22826;&#22810;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#19968;&#31181;&#26126;&#26174;&#30340;&#25910;&#25947;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#26469;&#23454;&#29616;&#12290;&#35813;&#28608;&#27963;&#20989;&#25968;&#36890;&#36807;&#24341;&#20837;&#21487;&#20248;&#21270;&#21442;&#25968;&#26469;&#27867;&#21270;&#26631;&#20934;RELU&#20989;&#25968;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#33258;&#30001;&#24230;&#65292;&#20351;&#24471;&#38750;&#32447;&#24615;&#31243;&#24230;&#21487;&#20197;&#34987;&#35843;&#25972;&#12290;&#36890;&#36807;&#19982;&#26631;&#20934;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#23558;&#22312;MNIST&#25968;&#23383;&#25968;&#25454;&#38598;&#30340;&#32972;&#26223;&#19979;&#37327;&#21270;&#30456;&#20851;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19896v1 Announce Type: new  Abstract: A simply implemented activation function with even cubic nonlinearity is introduced that increases the accuracy of neural networks without substantial additional computational resources. This is partially enabled through an apparent tradeoff between convergence and accuracy. The activation function generalizes the standard RELU function by introducing additional degrees of freedom through optimizable parameters that enable the degree of nonlinearity to be adjusted. The associated accuracy enhancement is quantified in the context of the MNIST digit data set through a comparison with standard techniques.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#21487;&#20197;&#33258;&#30001;&#25554;&#20540;&#24182;&#20135;&#29983;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21516;&#26102;&#20855;&#26377;&#26368;&#20248;&#36755;&#36816;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.19895</link><description>&lt;p&gt;
&#19968;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Information-Theoretic Framework for Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19895
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#21487;&#20197;&#33258;&#30001;&#25554;&#20540;&#24182;&#20135;&#29983;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21516;&#26102;&#20855;&#26377;&#26368;&#20248;&#36755;&#36816;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;Integral Probability Metric&#65288;IPM&#65289;&#21644;$f$-divergence&#20043;&#38388;&#33258;&#30001;&#25554;&#20540;&#65292;&#33258;&#28982;&#22320;&#24674;&#22797;&#20102;&#19968;&#20123;&#24050;&#30693;&#32467;&#26524;&#65288;&#21253;&#25324;Wasserstein&#21644;KL-bound&#65289;&#65292;&#24182;&#20135;&#29983;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#26368;&#20248;&#36755;&#36816;&#35299;&#37322;&#12290;&#22312;&#20004;&#20010;&#20855;&#20307;&#31034;&#20363;&#20013;&#35780;&#20272;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#30028;&#38480;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20005;&#26684;&#25913;&#36827;&#20102;&#29616;&#26377;&#30028;&#38480;&#65292;&#25110;&#32773;&#24674;&#22797;&#20102;&#29616;&#26377;OOD&#27867;&#21270;&#30028;&#38480;&#20013;&#30340;&#26368;&#20339;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19895v1 Announce Type: cross  Abstract: We study the Out-of-Distribution (OOD) generalization in machine learning and propose a general framework that provides information-theoretic generalization bounds. Our framework interpolates freely between Integral Probability Metric (IPM) and $f$-divergence, which naturally recovers some known results (including Wasserstein- and KL-bounds), as well as yields new generalization bounds. Moreover, we show that our framework admits an optimal transport interpretation. When evaluated in two concrete examples, the proposed bounds either strictly improve upon existing bounds in some cases or recover the best among existing OOD generalization bounds.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#20581;&#22766;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19889</link><description>&lt;p&gt;
&#26397;&#21521;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#25688;&#35201;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Robust Retrieval-Based Summarization System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#30340;&#35843;&#26597;&#12290;&#34429;&#28982;LLMs&#25552;&#20379;&#20102;&#25688;&#35201;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;LogicSumm&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29616;&#23454;&#22330;&#26223;&#65292;&#29992;&#26469;&#35780;&#20272;LLMs&#22312;RAG&#22522;&#30784;&#25688;&#35201;&#36807;&#31243;&#20013;&#30340;&#20581;&#22766;&#24615;&#12290;&#26681;&#25454;LogicSumm&#35782;&#21035;&#20986;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SummRAG&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#21019;&#24314;&#35757;&#32451;&#23545;&#35805;&#24182;&#24494;&#35843;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#22312;LogicSumm&#22330;&#26223;&#20013;&#30340;&#20581;&#22766;&#24615;&#12290;SummRAG&#26159;&#25105;&#20204;&#23450;&#20041;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#27979;&#35797;LLM&#33021;&#21147;&#30340;&#30446;&#26631;&#30340;&#19968;&#20010;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#19968;&#21171;&#27704;&#36920;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;SummRAG&#30340;&#24378;&#22823;&#65292;&#23637;&#31034;&#20102;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#25688;&#35201;&#36136;&#37327;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19889v1 Announce Type: cross  Abstract: This paper describes an investigation of the robustness of large language models (LLMs) for retrieval augmented generation (RAG)-based summarization tasks. While LLMs provide summarization capabilities, their performance in complex, real-world scenarios remains under-explored. Our first contribution is LogicSumm, an innovative evaluation framework incorporating realistic scenarios to assess LLM robustness during RAG-based summarization. Based on limitations identified by LogiSumm, we then developed SummRAG, a comprehensive system to create training dialogues and fine-tune a model to enhance robustness within LogicSumm's scenarios. SummRAG is an example of our goal of defining structured methods to test the capabilities of an LLM, rather than addressing issues in a one-off fashion. Experimental results confirm the power of SummRAG, showcasing improved logical coherence and summarization quality. Data, corresponding model weights, and Py
&lt;/p&gt;</description></item><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>Jamba&#26159;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;80GB GPU&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23545;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21644;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#20855;&#26377;state-of-the-art&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.19887</link><description>&lt;p&gt;
Jamba: &#19968;&#20010;&#28151;&#21512;Transformer-Mamba&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jamba: A Hybrid Transformer-Mamba Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19887
&lt;/p&gt;
&lt;p&gt;
Jamba&#26159;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;80GB GPU&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23545;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21644;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#20855;&#26377;state-of-the-art&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Jamba&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26032;&#39062;&#30340;&#28151;&#21512;Transformer-Mamba&#28151;&#21512;&#19987;&#23478;(MoE)&#26550;&#26500;&#30340;&#26032;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Jamba&#20132;&#38169;&#20351;&#29992;Transformer&#21644;Mamba&#23618;&#65292;&#20174;&#20004;&#31181;&#27169;&#22411;&#23478;&#26063;&#20013;&#33719;&#30410;&#12290;MoE&#34987;&#28155;&#21152;&#22312;&#20854;&#20013;&#19968;&#20123;&#23618;&#20013;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27963;&#36291;&#21442;&#25968;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#26550;&#26500;&#20801;&#35768;&#29305;&#23450;&#36164;&#28304;&#21644;&#30446;&#26631;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19887v1 Announce Type: new  Abstract: We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;Vision Transformer&#65288;ViT&#65289;&#32593;&#32476;&#20013;&#37325;&#26032;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25216;&#26415;&#21644;&#35265;&#35299;&#65292;&#26088;&#22312;&#25552;&#21319;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19882</link><description>&lt;p&gt;
&#25552;&#21319;&#35270;&#35273;Transformer&#32593;&#32476;&#25928;&#29575;&#30340;&#35774;&#35745;&#25216;&#26415;&#21644;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;Vision Transformer&#65288;ViT&#65289;&#32593;&#32476;&#20013;&#37325;&#26032;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25216;&#26415;&#21644;&#35265;&#35299;&#65292;&#26088;&#22312;&#25552;&#21319;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#35782;&#21035;&#26174;&#33879;&#21306;&#22495;&#30340;&#22266;&#26377;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#24050;&#32463;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20219;&#21153;&#20013;&#12290;&#26500;&#24314;&#22312;&#36825;&#19968;&#33539;&#24335;&#20043;&#19978;&#65292;Vision Transformer&#65288;ViT&#65289;&#32593;&#32476;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#25928;&#29575;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;ViTs&#20013;&#37325;&#26032;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#25552;&#21319;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#35774;&#35745;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25216;&#26415;&#21644;&#35265;&#35299;&#65292;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#25991;&#29486;&#12290;&#35813;&#35843;&#26597;&#20174;&#20171;&#32461;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#22522;&#26412;&#27010;&#24565;&#24320;&#22987;&#12290;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102;ViTs&#20869;&#21508;&#31181;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#37319;&#29992;&#20102;&#37325;&#26032;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#23427;&#20204;&#30340;&#24212;&#29992;&#12289;&#30446;&#26631;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19882v1 Announce Type: cross  Abstract: Intrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer Vision (CV) tasks. Building upon this paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;</title><link>https://arxiv.org/abs/2403.19871</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#24930;&#21464;&#21270;&#30340;&#24207;&#21015;&#23454;&#29616;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19871
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#36138;&#23146;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#32771;&#34385;&#36890;&#36807;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#28436;&#21464;&#26469;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36890;&#36807;&#19981;&#21516;&#30340;&#25968;&#25454;&#25209;&#27425;&#26356;&#26032;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20445;&#30041;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265; - &#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#23454;&#26045;&#31616;&#26131;&#24615;&#21644;&#19982;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201; - &#36890;&#36807;&#20351;&#29992;&#21487;&#20197;&#30452;&#25509;&#32435;&#20837;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#23450;&#20041;&#23450;&#20041;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#27604;&#36138;&#23146;&#35757;&#32451;&#27169;&#22411;&#26356;&#24378;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;</title><link>https://arxiv.org/abs/2403.19867</link><description>&lt;p&gt;
&#22312;&#27969;&#24335;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#25214;&#21040;&#20915;&#31574;&#26641;&#20998;&#21106;&#28857;
&lt;/p&gt;
&lt;p&gt;
Finding Decision Tree Splits in Streaming and Massively Parallel Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;&#20998;&#21106;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#35266;&#27979;&#25968;&#25454;&#27969;$x_i$&#21450;&#20854;&#26631;&#31614;$y_i$&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#30340;&#26368;&#20339;&#20998;&#21106;&#28857;$j$&#65292;&#20351;&#24471;&#22343;&#26041;&#35823;&#24046;&#65288;&#22238;&#24402;&#38382;&#39064;&#65289;&#25110;&#35823;&#20998;&#31867;&#29575;&#65288;&#20998;&#31867;&#38382;&#39064;&#65289;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#24555;&#36895;&#30340;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#20351;&#29992;&#20122;&#32447;&#24615;&#31354;&#38388;&#21644;&#23569;&#37327;&#27425;&#25968;&#30340;&#36941;&#21382;&#12290;&#36825;&#20123;&#31639;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#27169;&#22411;&#20013;&#12290;&#23613;&#31649;&#19981;&#33021;&#30452;&#25509;&#27604;&#36739;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;Domingos&#21644;Hulten&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65288;KDD 2000&#65289;&#30456;&#20114;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19867v1 Announce Type: cross  Abstract: In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate (for classification) is minimized. We provide various fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos and Hulten (KDD 2000).
&lt;/p&gt;</description></item><item><title>DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19863</link><description>&lt;p&gt;
DeNetDM: &#36890;&#36807;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#26469;&#28040;&#38500;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
DeNetDM: Debiasing by Network Depth Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19863
&lt;/p&gt;
&lt;p&gt;
DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#26080;&#24847;&#38388;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#36890;&#24120;&#21253;&#25324;&#21033;&#29992;&#20559;&#35265;&#27880;&#37322;&#12289;&#26681;&#25454;&#20266;&#20559;&#35265;&#26631;&#31614;&#36827;&#34892;&#21152;&#26435;&#37325;&#12289;&#25110;&#36890;&#36807;&#22686;&#24378;&#25216;&#26415;&#22686;&#21152;&#20559;&#35265;&#20914;&#31361;&#25968;&#25454;&#28857;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeNetDM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35266;&#23519;&#32467;&#26524;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#26680;&#24515;&#23646;&#24615;&#65292;&#32780;&#26356;&#28145;&#23618;&#27425;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#33719;&#21462;&#19981;&#21516;&#20449;&#24687;&#26102;&#24378;&#35843;&#20559;&#35265;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#19987;&#23478;&#20056;&#31215;&#20013;&#25512;&#23548;&#20986;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21019;&#24314;&#20102;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#65292;&#28982;&#21518;&#29992;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#24037;&#20855;&#22791;&#21463;&#20851;&#27880;&#65292;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;GNN&#22312;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.19852</link><description>&lt;p&gt;
&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Graph Neural Networks in Epidemic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19852
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#24037;&#20855;&#22791;&#21463;&#20851;&#27880;&#65292;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;GNN&#22312;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26032;&#20896;&#30123;&#24773;&#29190;&#21457;&#20197;&#26469;&#65292;&#20154;&#20204;&#23545;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20256;&#32479;&#30340;&#26426;&#26800;&#27169;&#22411;&#25968;&#23398;&#25551;&#36848;&#20102;&#20256;&#26579;&#30149;&#30340;&#20256;&#25773;&#26426;&#21046;&#65292;&#20294;&#22312;&#38754;&#23545;&#24403;&#20170;&#19981;&#26029;&#22686;&#38271;&#30340;&#25361;&#25112;&#26102;&#24448;&#24448;&#21147;&#19981;&#20174;&#24515;&#12290;&#22240;&#27492;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#35797;&#22270;&#20840;&#38754;&#22238;&#39038;GNN&#22312;&#27969;&#34892;&#30149;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#28508;&#22312;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20026;&#27969;&#34892;&#30149;&#20219;&#21153;&#21644;&#26041;&#27861;&#35770;&#21508;&#24341;&#20837;&#20102;&#20998;&#23618;&#20998;&#31867;&#27861;&#65292;&#20026;&#35813;&#39046;&#22495;&#20869;&#30340;&#21457;&#23637;&#36712;&#36857;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#23545;&#20110;&#27969;&#34892;&#30149;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#27969;&#34892;&#30149;&#39046;&#22495;&#36890;&#24120;&#24212;&#29992;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;&#23545;&#20110;&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#20998;&#20026;&#8220;&#31070;&#32463;&#27169;&#22411;&#8221;&#21644;&#8220;&#28151;&#21512;&#27169;&#22411;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19852v1 Announce Type: new  Abstract: Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into \textit{Neural Models} and \textit{Hybrid Models}. Following this, we perform an exha
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27573;&#33853;&#35760;&#24518;&#30340;&#26799;&#24230;&#20855;&#26377;&#21487;&#21306;&#20998;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#36890;&#36807;&#24494;&#35843;&#39640;&#26799;&#24230;&#26435;&#37325;&#21487;&#20197;&#21462;&#28040;&#23398;&#20064;&#65292;&#23450;&#20301;&#20102;&#29305;&#21035;&#21442;&#19982;&#27573;&#33853;&#35760;&#24518;&#30340;&#20302;&#23618;&#27880;&#24847;&#22836;&#65292;&#24182;&#30740;&#31350;&#20102;&#35760;&#24518;&#22312;&#21069;&#32512;&#20013;&#30340;&#26412;&#22320;&#21270;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.19851</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27573;&#33853;&#35760;&#24518;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localizing Paragraph Memorization in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19851
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27573;&#33853;&#35760;&#24518;&#30340;&#26799;&#24230;&#20855;&#26377;&#21487;&#21306;&#20998;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#36890;&#36807;&#24494;&#35843;&#39640;&#26799;&#24230;&#26435;&#37325;&#21487;&#20197;&#21462;&#28040;&#23398;&#20064;&#65292;&#23450;&#20301;&#20102;&#29305;&#21035;&#21442;&#19982;&#27573;&#33853;&#35760;&#24518;&#30340;&#20302;&#23618;&#27880;&#24847;&#22836;&#65292;&#24182;&#30740;&#31350;&#20102;&#35760;&#24518;&#22312;&#21069;&#32512;&#20013;&#30340;&#26412;&#22320;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35760;&#24518;&#21644;&#32972;&#35829;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#27573;&#30340;&#26435;&#37325;&#21644;&#26426;&#21046;&#26412;&#22320;&#21270;&#65311;&#26412;&#25991;&#34920;&#26126;&#65292;&#34429;&#28982;&#35760;&#24518;&#20998;&#24067;&#22312;&#22810;&#20010;&#23618;&#27425;&#21644;&#27169;&#22411;&#32452;&#20214;&#20013;&#65292;&#20294;&#35760;&#24518;&#27573;&#33853;&#30340;&#26799;&#24230;&#20855;&#26377;&#21487;&#21306;&#20998;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#36739;&#20302;&#27169;&#22411;&#23618;&#27425;&#20013;&#30340;&#26799;&#24230;&#27604;&#38750;&#35760;&#24518;&#31034;&#20363;&#30340;&#26799;&#24230;&#26356;&#22823;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35760;&#24518;&#31034;&#20363;&#21487;&#20197;&#36890;&#36807;&#20165;&#24494;&#35843;&#39640;&#26799;&#24230;&#26435;&#37325;&#26469;&#21462;&#28040;&#23398;&#20064;&#12290;&#25105;&#20204;&#23450;&#20301;&#20102;&#19968;&#20010;&#20284;&#20046;&#29305;&#21035;&#21442;&#19982;&#27573;&#33853;&#35760;&#24518;&#30340;&#20302;&#23618;&#27880;&#24847;&#22836;&#12290;&#36825;&#20010;&#22836;&#37096;&#20027;&#35201;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#21333;&#35821;&#20998;&#24067;&#20013;&#26368;&#19981;&#39057;&#32321;&#30340;&#29420;&#29305;&#12289;&#32597;&#35265;&#30340;&#20196;&#29260;&#19978;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#25200;&#21160;&#20196;&#29260;&#24182;&#27979;&#37327;&#23545;&#35299;&#30721;&#36896;&#25104;&#30340;&#25913;&#21464;&#26469;&#30740;&#31350;&#35760;&#24518;&#22312;&#21069;&#32512;&#20013;&#30340;&#26412;&#22320;&#21270;&#31243;&#24230;&#12290;&#21069;&#32512;&#20013;&#30340;&#19968;&#20123;&#29420;&#29305;&#20196;&#29260;&#32463;&#24120;&#20250;&#20351;&#25972;&#20010;&#20869;&#23481;&#21463;&#25439;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19851v1 Announce Type: new  Abstract: Can we localize the weights and mechanisms used by a language model to memorize and recite entire paragraphs of its training data? In this paper, we show that while memorization is spread across multiple layers and model components, gradients of memorized paragraphs have a distinguishable spatial pattern, being larger in lower model layers than gradients of non-memorized examples. Moreover, the memorized examples can be unlearned by fine-tuning only the high-gradient weights. We localize a low-layer attention head that appears to be especially involved in paragraph memorization. This head is predominantly focusing its attention on distinctive, rare tokens that are least frequent in a corpus-level unigram distribution. Next, we study how localized memorization is across the tokens in the prefix by perturbing tokens and measuring the caused change in the decoding. A few distinctive tokens early in a prefix can often corrupt the entire cont
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#26080;&#32447;&#24322;&#26500;&#29615;&#22659;&#19979;&#30340;&#26377;&#20559;Over-the-Air&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#20559;&#24046;&#21644;&#26041;&#24046;&#20043;&#38388;&#26435;&#34913;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.19849</link><description>&lt;p&gt;
&#22312;&#26080;&#32447;&#24322;&#26500;&#29615;&#22659;&#19979;&#30340;&#26377;&#20559;&#31354;&#20013;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Biased Over-the-Air Federated Learning under Wireless Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19849
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#26080;&#32447;&#24322;&#26500;&#29615;&#22659;&#19979;&#30340;&#26377;&#20559;Over-the-Air&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#20559;&#24046;&#21644;&#26041;&#24046;&#20043;&#38388;&#26435;&#34913;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;OTA&#65288;Over-the-Air&#65289;&#35745;&#31639;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#33539;&#24335;&#23853;&#38706;&#22836;&#35282;&#65292;&#21033;&#29992;&#26080;&#32447;&#20449;&#36947;&#30340;&#27874;&#24418;&#21472;&#21152;&#29305;&#24615;&#23454;&#29616;&#24555;&#36895;&#27169;&#22411;&#26356;&#26032;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;\emph{&#21516;&#36136;}&#26080;&#32447;&#26465;&#20214;&#19979;&#30340;OTA&#35774;&#22791;&#8220;&#39044;&#32553;&#25918;&#22120;&#8221;&#35774;&#35745;&#65292;&#21363;&#35774;&#22791;&#32463;&#21382;&#30456;&#21516;&#30340;&#24179;&#22343;&#36335;&#24452;&#25439;&#32791;&#65292;&#23548;&#33268;&#38646;&#20559;&#24046;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38646;&#20559;&#24046;&#35774;&#35745;&#21463;&#21040;&#36335;&#24452;&#25439;&#32791;&#26368;&#20005;&#37325;&#30340;&#35774;&#22791;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#22312;\emph{&#24322;&#26500;}&#26080;&#32447;&#29615;&#22659;&#20013;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35774;&#35745;\emph{&#26377;&#20559;}&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#26377;&#30410;&#65292;&#20197;&#25442;&#21462;&#27169;&#22411;&#26356;&#26032;&#30340;&#26041;&#24046;&#38477;&#20302;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;OTA&#35774;&#22791;&#39044;&#32553;&#25918;&#22120;&#30340;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;OTA-FL&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#27169;&#22411;&#8220;&#26368;&#20248;&#24615;&#35823;&#24046;&#8221;&#30340;&#19978;&#30028;&#65292;&#26126;&#30830;&#25429;&#25417;&#20102;&#20559;&#24046;&#21644;&#26041;&#24046;&#23545;&#20110;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19849v1 Announce Type: new  Abstract: Recently, Over-the-Air (OTA) computation has emerged as a promising federated learning (FL) paradigm that leverages the waveform superposition properties of the wireless channel to realize fast model updates. Prior work focused on the OTA device ``pre-scaler" design under \emph{homogeneous} wireless conditions, in which devices experience the same average path loss, resulting in zero-bias solutions. Yet, zero-bias designs are limited by the device with the worst average path loss and hence may perform poorly in \emph{heterogeneous} wireless settings. In this scenario, there may be a benefit in designing \emph{biased} solutions, in exchange for a lower variance in the model updates. To optimize this trade-off, we study the design of OTA device pre-scalers by focusing on the OTA-FL convergence. We derive an upper bound on the model ``optimality error", which explicitly captures the effect of bias and variance in terms of the choice of the 
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#26799;&#24230;&#19979;&#38477;&#30456;&#23545;&#20110;Cartesian reverse derivative categories (CRDCs)&#30340;&#36890;&#29992;&#23458;&#35266;&#20989;&#25968;&#35825;&#23548;&#20986;&#19968;&#20010;&#36229;&#22270;&#20989;&#23376;&#65292;&#23558;&#20248;&#21270;&#38382;&#39064;&#26144;&#23556;&#21040;&#21160;&#21147;&#31995;&#32479;&#65292;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.19845</link><description>&lt;p&gt;
&#24191;&#20041;&#26799;&#24230;&#19979;&#38477;&#26159;&#19968;&#20010;&#36229;&#22270;&#20989;&#23376;
&lt;/p&gt;
&lt;p&gt;
Generalized Gradient Descent is a Hypergraph Functor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19845
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#26799;&#24230;&#19979;&#38477;&#30456;&#23545;&#20110;Cartesian reverse derivative categories (CRDCs)&#30340;&#36890;&#29992;&#23458;&#35266;&#20989;&#25968;&#35825;&#23548;&#20986;&#19968;&#20010;&#36229;&#22270;&#20989;&#23376;&#65292;&#23558;&#20248;&#21270;&#38382;&#39064;&#26144;&#23556;&#21040;&#21160;&#21147;&#31995;&#32479;&#65292;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cartesian reverse derivative categories (CRDCs)&#25552;&#20379;&#20102;&#23545;&#21453;&#21521;&#23548;&#25968;&#30340;&#20844;&#29702;&#21270;&#27867;&#21270;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#23558;&#30456;&#23545;&#20110;&#24191;&#27867;&#31867;&#38382;&#39064;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#24191;&#20041;&#31867;&#27604;&#24212;&#29992;&#20110;&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#32473;&#23450;CRDC&#30340;&#24191;&#20041;&#26799;&#24230;&#19979;&#38477;&#35825;&#23548;&#20986;&#19968;&#20010;&#20174;&#20248;&#21270;&#38382;&#39064;&#30340;&#36229;&#22270;&#33539;&#30068;&#21040;&#21160;&#21147;&#31995;&#32479;&#30340;&#36229;&#22270;&#20989;&#23376;&#12290;&#35813;&#20989;&#23376;&#30340;&#23450;&#20041;&#22495;&#30001;&#23458;&#35266;&#20989;&#25968;&#32452;&#25104;&#65292;&#36825;&#20123;&#23458;&#35266;&#20989;&#25968;&#22312;&#20219;&#24847;CRDC&#19979;&#37117;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#26159;&#24320;&#25918;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#21464;&#37327;&#20849;&#20139;&#19982;&#20854;&#20182;&#36825;&#26679;&#30340;&#23458;&#35266;&#20989;&#25968;&#32452;&#21512;&#12290;&#23545;&#26144;&#22495;&#31867;&#20284;&#22320;&#34987;&#25351;&#23450;&#20026;&#22522;&#30784;CRDC&#30340;&#36890;&#29992;&#21644;&#24320;&#25918;&#21160;&#24577;&#31995;&#32479;&#31867;&#21035;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#36229;&#22270;&#20989;&#23376;&#22914;&#20309;&#35825;&#23548;&#20986;&#19968;&#20010;&#38024;&#23545;&#20219;&#24847;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19845v1 Announce Type: cross  Abstract: Cartesian reverse derivative categories (CRDCs) provide an axiomatic generalization of the reverse derivative, which allows generalized analogues of classic optimization algorithms such as gradient descent to be applied to a broad class of problems. In this paper, we show that generalized gradient descent with respect to a given CRDC induces a hypergraph functor from a hypergraph category of optimization problems to a hypergraph category of dynamical systems. The domain of this functor consists of objective functions that are 1) general in the sense that they are defined with respect to an arbitrary CRDC, and 2) open in that they are decorated spans that can be composed with other such objective functions via variable sharing. The codomain is specified analogously as a category of general and open dynamical systems for the underlying CRDC. We describe how the hypergraph functor induces a distributed optimization algorithm for arbitrary
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20122;&#32467;&#26500;&#35745;&#25968;&#12289;k-mers&#21644;Daylight-like&#25351;&#32441;&#65292;&#25193;&#23637;&#20102;&#21270;&#23398;&#32467;&#26500;&#30340;&#34920;&#31034;&#65292;&#25552;&#21319;&#20102;&#20998;&#23376;&#25351;&#32441;&#30340;&#20449;&#24687;&#20869;&#23481;&#21644;&#21306;&#20998;&#33021;&#21147;&#65292;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#20026;&#20998;&#23376;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#21270;&#23398;&#32467;&#26500;&#35774;&#35745;&#25552;&#20379;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.19844</link><description>&lt;p&gt;
&#20351;&#29992;k-mers&#21644;&#22522;&#20110;&#29255;&#27573;&#30340;&#25351;&#32441;&#25193;&#23637;&#21270;&#23398;&#34920;&#31034;&#36827;&#34892;&#20998;&#23376;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Expanding Chemical Representation with k-mers and Fragment-based Fingerprints for Molecular Fingerprinting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19844
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20122;&#32467;&#26500;&#35745;&#25968;&#12289;k-mers&#21644;Daylight-like&#25351;&#32441;&#65292;&#25193;&#23637;&#20102;&#21270;&#23398;&#32467;&#26500;&#30340;&#34920;&#31034;&#65292;&#25552;&#21319;&#20102;&#20998;&#23376;&#25351;&#32441;&#30340;&#20449;&#24687;&#20869;&#23481;&#21644;&#21306;&#20998;&#33021;&#21147;&#65292;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#20026;&#20998;&#23376;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#21270;&#23398;&#32467;&#26500;&#35774;&#35745;&#25552;&#20379;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#20122;&#32467;&#26500;&#35745;&#25968;&#12289;k-mers&#21644;&#31867;&#20284;Daylight&#30340;&#25351;&#32441;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25193;&#23637;SMILES&#23383;&#31526;&#20018;&#20013;&#21270;&#23398;&#32467;&#26500;&#30340;&#34920;&#31034;&#12290;&#38598;&#25104;&#26041;&#27861;&#29983;&#25104;&#20102;&#22686;&#24378;&#21306;&#20998;&#33021;&#21147;&#21644;&#20449;&#24687;&#20869;&#23481;&#30340;&#32508;&#21512;&#20998;&#23376;&#23884;&#20837;&#12290;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;Morgan&#25351;&#32441;&#12289;MACCS&#21644;&#21333;&#29420;Daylight&#25351;&#32441;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#33647;&#29289;&#20998;&#31867;&#31561;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20855;&#20449;&#24687;&#30340;&#21270;&#23398;&#32467;&#26500;&#34920;&#31034;&#65292;&#25512;&#36827;&#20102;&#20998;&#23376;&#30456;&#20284;&#24615;&#20998;&#26512;&#65292;&#24182;&#20419;&#36827;&#20102;&#22312;&#20998;&#23376;&#35774;&#35745;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#20026;&#20998;&#23376;&#32467;&#26500;&#20998;&#26512;&#21644;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20855;&#26377;&#23454;&#38469;&#23454;&#26045;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19844v1 Announce Type: cross  Abstract: This study introduces a novel approach, combining substruct counting, $k$-mers, and Daylight-like fingerprints, to expand the representation of chemical structures in SMILES strings. The integrated method generates comprehensive molecular embeddings that enhance discriminative power and information content. Experimental evaluations demonstrate its superiority over traditional Morgan fingerprinting, MACCS, and Daylight fingerprint alone, improving chemoinformatics tasks such as drug classification. The proposed method offers a more informative representation of chemical structures, advancing molecular similarity analysis and facilitating applications in molecular design and drug discovery. It presents a promising avenue for molecular structure analysis and design, with significant potential for practical implementation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26356;&#20808;&#36827;&#30340;&#26234;&#33021;&#20316;&#29289;&#31649;&#29702;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#20248;&#21270;&#20316;&#29289;&#31649;&#29702;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.19839</link><description>&lt;p&gt;
&#26032;&#30340;&#20892;&#23398;&#23478;&#65306;&#35821;&#35328;&#27169;&#22411;&#26159;&#20316;&#29289;&#31649;&#29702;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
The New Agronomists: Language Models are Experts in Crop Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26356;&#20808;&#36827;&#30340;&#26234;&#33021;&#20316;&#29289;&#31649;&#29702;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#20248;&#21270;&#20316;&#29289;&#31649;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#31649;&#29702;&#22312;&#20915;&#23450;&#20316;&#29289;&#20135;&#37327;&#12289;&#32463;&#27982;&#30408;&#21033;&#21644;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#22312;&#20197;&#24448;&#30740;&#31350;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#20808;&#36827;&#30340;&#26234;&#33021;&#20316;&#29289;&#31649;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#29420;&#29305;&#22320;&#23558;&#24378;&#21270;&#23398;&#20064;&#12289;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21644;&#30001;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20026;&#20892;&#19994;&#25216;&#26415;&#36716;&#31227;&#65288;DSSAT&#65289;&#23454;&#29616;&#30340;&#20316;&#29289;&#27169;&#25311;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;Q&#32593;&#32476;&#65292;&#26469;&#35757;&#32451;&#22788;&#29702;&#27169;&#25311;&#22120;&#20013;&#20247;&#22810;&#29366;&#24577;&#21464;&#37327;&#20316;&#20026;&#35266;&#27979;&#30340;&#31649;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19839v1 Announce Type: cross  Abstract: Crop management plays a crucial role in determining crop yield, economic profitability, and environmental sustainability. Despite the availability of management guidelines, optimizing these practices remains a complex and multifaceted challenge. In response, previous studies have explored using reinforcement learning with crop simulators, typically employing simple neural-network-based reinforcement learning (RL) agents. Building on this foundation, this paper introduces a more advanced intelligent crop management system. This system uniquely combines RL, a language model (LM), and crop simulations facilitated by the Decision Support System for Agrotechnology Transfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train management policies that process numerous state variables from the simulator as observations. A novel aspect of our approach is the conversion of these state variables into more informative language, fac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19837</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Concept-based Analysis of Neural Networks via Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#38750;&#24120;&#21487;&#21462;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#34920;&#36798;&#35270;&#35273;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#20197;&#21450;&#32570;&#20047;&#39640;&#25928;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#36825;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#12289;&#35270;&#35273;&#35821;&#35328;&#12289;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20854;&#21487;&#20197;&#25512;&#29702;&#35270;&#35273;&#27169;&#22411;&#30340;&#36879;&#38236;&#12290;VLMs&#24050;&#32463;&#22312;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#27492;&#38544;&#24335;&#22320;&#20102;&#35299;&#25551;&#36848;&#36825;&#20123;&#22270;&#20687;&#30340;&#39640;&#23618;&#27425;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Con}_{\texttt{spec}}$&#30340;&#36923;&#36753;&#35268;&#33539;&#35821;&#35328;&#65292;&#26088;&#22312;&#20415;&#20110;&#25353;&#29031;&#36825;&#20123;&#27010;&#24565;&#32534;&#20889;&#35268;&#33539;&#12290;&#20026;&#20102;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#26816;&#26597;$\texttt{Con}_{\texttt{spec}}$&#35268;&#33539;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;VLM&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#21644;&#39640;&#25928;&#26816;&#26597;&#35270;&#35273;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#26041;&#27861;&#65292;&#35780;&#20272;&#21307;&#30103;&#35786;&#26029;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21307;&#23398;&#25351;&#21335;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#30830;&#23450;&#33008;&#33146;&#30284;&#27835;&#30103;&#20851;&#38190;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#20351;&#29992;&#30456;&#20284;&#24230;&#34913;&#37327;&#30340;&#35299;&#37322;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19820</link><description>&lt;p&gt;
&#35780;&#20272;&#21307;&#30103;&#35786;&#26029;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65306;&#19968;&#31181;&#20154;&#26426;&#21327;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19820
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#26041;&#27861;&#65292;&#35780;&#20272;&#21307;&#30103;&#35786;&#26029;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21307;&#23398;&#25351;&#21335;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#30830;&#23450;&#33008;&#33146;&#30284;&#27835;&#30103;&#20851;&#38190;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#20351;&#29992;&#30456;&#20284;&#24230;&#34913;&#37327;&#30340;&#35299;&#37322;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#27169;&#22411;&#22312;&#33008;&#33146;&#30284;&#25968;&#25454;&#38598;&#19978;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;&#20154;&#26426;&#21327;&#20316;&#25216;&#26415;&#21644;&#21307;&#23398;&#25351;&#21335;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#26469;&#28304;&#65292;&#20197;&#30830;&#23450;&#19982;&#21046;&#23450;&#33008;&#33146;&#30284;&#27835;&#30103;&#30456;&#20851;&#30340;&#19981;&#21516;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#19981;&#20165;&#29992;&#20316;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#36824;&#29992;&#20316;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#30340;&#26041;&#24335;&#65292;&#20351;&#29992;&#26080;&#30693;&#21644;&#38750;&#26080;&#30693;&#30340;&#35299;&#37322;&#24615;&#25216;&#26415;&#12290;&#20026;&#20102;&#20415;&#20110;&#35299;&#37322;&#32467;&#26524;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35832;&#22914;&#21152;&#26435;&#26480;&#21345;&#30456;&#20284;&#31995;&#25968;&#20043;&#31867;&#30340;&#30456;&#20284;&#24230;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#20165;&#26159;&#36873;&#25321;&#24615;&#33021;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#26368;&#22909;&#35299;&#37322;&#20854;&#32467;&#35770;&#24182;&#19982;&#20043;&#20445;&#25345;&#19968;&#33268;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19820v1 Announce Type: cross  Abstract: This paper presents a comprehensive study on the evaluation of explanatory capabilities of machine learning models, with a focus on Decision Trees, Random Forest and XGBoost models using a pancreatic cancer dataset. We use Human-in-the-Loop related techniques and medical guidelines as a source of domain knowledge to establish the importance of the different features that are relevant to establish a pancreatic cancer treatment. These features are not only used as a dimensionality reduction approach for the machine learning models, but also as way to evaluate the explainability capabilities of the different models using agnostic and non-agnostic explainability techniques. To facilitate interpretation of explanatory results, we propose the use of similarity measures such as the Weighted Jaccard Similarity coefficient. The goal is to not only select the best performing model but also the one that can best explain its conclusions and aligns
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#38146;&#31163;&#23376;&#30005;&#27744;&#20013;&#25972;&#21512;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#20110;&#21097;&#20313;&#26377;&#25928;&#23551;&#21629;&#65288;RUL&#65289;&#30340;&#27010;&#24565;&#20197;&#21450;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.19816</link><description>&lt;p&gt;
&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#39044;&#27979;&#22312;CPS&#26102;&#20195;&#30340;&#29366;&#20917;
&lt;/p&gt;
&lt;p&gt;
The State of Lithium-Ion Battery Health Prognostics in the CPS Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#38146;&#31163;&#23376;&#30005;&#27744;&#20013;&#25972;&#21512;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#20110;&#21097;&#20313;&#26377;&#25928;&#23551;&#21629;&#65288;RUL&#65289;&#30340;&#27010;&#24565;&#20197;&#21450;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#31163;&#23376;&#30005;&#27744;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33021;&#28304;&#23384;&#20648;&#25216;&#26415;&#65292;&#36890;&#36807;&#20026;&#21508;&#31181;&#35774;&#22791;&#21644;&#24212;&#29992;&#25552;&#20379;&#21160;&#21147;&#65292;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#27744;&#20869;&#23454;&#29616;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#30340;&#26080;&#32541;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#36825;&#20123;&#21160;&#21147;&#28304;&#30340;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#12290;&#21097;&#20313;&#26377;&#25928;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#22312;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#27010;&#24565;&#65292;&#34987;&#28145;&#20837;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#39044;&#27979;&#32452;&#20214;&#25925;&#38556;&#20043;&#21069;&#30340;&#20316;&#29992;&#12290;&#35813;&#35770;&#25991;&#23457;&#26597;&#20102;&#21508;&#31181;RUL&#39044;&#27979;&#26041;&#27861;&#65292;&#20174;&#20256;&#32479;&#27169;&#22411;&#21040;&#23574;&#31471;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#31361;&#20986;&#20102;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#39044;&#27979;&#39046;&#22495;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36716;&#21464;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19816v1 Announce Type: new  Abstract: Lithium-ion batteries (Li-ion) have revolutionized energy storage technology, becoming integral to our daily lives by powering a diverse range of devices and applications. Their high energy density, fast power response, recyclability, and mobility advantages have made them the preferred choice for numerous sectors. This paper explores the seamless integration of Prognostics and Health Management within batteries, presenting a multidisciplinary approach that enhances the reliability, safety, and performance of these powerhouses. Remaining useful life (RUL), a critical concept in prognostics, is examined in depth, emphasizing its role in predicting component failure before it occurs. The paper reviews various RUL prediction methods, from traditional models to cutting-edge data-driven techniques. Furthermore, it highlights the paradigm shift toward deep learning architectures within the field of Li-ion battery health prognostics, elucidatin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#24182;&#34892;&#20648;&#27700;&#27744;&#21644;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21160;&#24577;&#31995;&#32479;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19806</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#30340;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65306;&#22312;&#20648;&#27700;&#27744;&#35745;&#31639;&#20013;&#36808;&#21521;&#21487;&#35299;&#37322;&#24615;&#21644;&#26497;&#31616;&#20027;&#20041;&#30340;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#24182;&#34892;&#20648;&#27700;&#27744;&#21644;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21160;&#24577;&#31995;&#32479;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#33539;&#24335;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26032;&#39062;&#19988;&#21487;&#35299;&#37322;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#20256;&#32479;&#30340;ESN&#22312;&#21160;&#24577;&#31995;&#32479;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#22823;&#21160;&#24577;&#20648;&#30406;&#12290;&#23427;&#20063;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#38590;&#20197;&#21306;&#20998;&#19981;&#21516;&#36755;&#20837;&#32452;&#21512;&#23545;&#36755;&#20986;&#30340;&#36129;&#29486;&#12290;&#22312;&#36825;&#37324;&#65292;&#36890;&#36807;&#20351;&#29992;&#30001;&#19981;&#21516;&#36755;&#20837;&#32452;&#21512;&#39537;&#21160;&#30340;&#36739;&#23567;&#24182;&#34892;&#20648;&#27700;&#27744;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20648;&#27700;&#27744;&#26550;&#26500;&#65292;&#31216;&#20026;&#29305;&#24449;ESN&#65288;Feat-ESN&#65289;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#36827;&#34892;&#38750;&#32447;&#24615;&#32452;&#21512;&#20197;&#20135;&#29983;&#36755;&#20986;&#12290;&#25152;&#24471;&#30340;&#22522;&#20110;&#29305;&#24449;&#30340;ESN&#22312;&#23569;&#37327;&#20648;&#27700;&#27744;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#20256;&#32479;&#30340;&#21333;&#19968;&#20648;&#27700;&#27744;ESN&#12290;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#19977;&#20010;&#31995;&#32479;&#19978;&#24471;&#21040;&#20102;&#23637;&#31034;&#65306;&#20004;&#20010;&#26469;&#33258;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#19968;&#32452;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19806v1 Announce Type: new  Abstract: This paper proposes a novel and interpretable recurrent neural-network structure using the echo-state network (ESN) paradigm for time-series prediction. While the traditional ESNs perform well for dynamical systems prediction, it needs a large dynamic reservoir with increased computational complexity. It also lacks interpretability to discern contributions from different input combinations to the output. Here, a systematic reservoir architecture is developed using smaller parallel reservoirs driven by different input combinations, known as features, and then they are nonlinearly combined to produce the output. The resultant feature-based ESN (Feat-ESN) outperforms the traditional single-reservoir ESN with less reservoir nodes. The predictive capability of the proposed architecture is demonstrated on three systems: two synthetic datasets from chaotic dynamical systems and a set of real-time traffic data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Gegenbauer-based graph convolutional (GegenConv)&#31639;&#23376;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.19800</link><description>&lt;p&gt;
Gegenbauer&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19800
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Gegenbauer-based graph convolutional (GegenConv)&#31639;&#23376;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#26102;&#21464;&#22270;&#20449;&#21495;&#65288;&#25110;&#22270;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20174;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#21040;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20934;&#30830;&#25429;&#25417;&#36825;&#20123;&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20449;&#24687;&#23545;&#20110;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#26102;&#38388;&#24046;&#30340;&#24179;&#28369;&#24615;&#20551;&#35774;&#21644;&#31616;&#21333;&#30340;&#20984;&#20248;&#21270;&#25216;&#26415;&#65292;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23398;&#20064;&#27169;&#22359;&#20197;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;Gegenbauer&#22810;&#39033;&#24335;&#29702;&#35770;&#30340;Gegenbauer-based graph convolutional&#65288;GegenConv&#65289;&#31639;&#23376;&#65292;&#36825;&#26159;&#20256;&#32479;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#30340;&#25512; generalization&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19800v1 Announce Type: cross  Abstract: Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand t
&lt;/p&gt;</description></item><item><title>MAPL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;Model Agnostic Peer-to-peer Learning&#65292;&#36890;&#36807;&#28857;&#23545;&#28857;&#36890;&#20449;&#22312;&#37051;&#36817;&#23458;&#25143;&#31471;&#20043;&#38388;&#21516;&#26102;&#23398;&#20064;&#24322;&#36136;&#20010;&#24615;&#21270;&#27169;&#22411;&#21644;&#21327;&#20316;&#22270;&#65292;&#22312;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;MAPL&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19792</link><description>&lt;p&gt;
MAPL: &#27169;&#22411;&#26080;&#20851;&#30340;&#28857;&#23545;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MAPL: Model Agnostic Peer-to-peer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19792
&lt;/p&gt;
&lt;p&gt;
MAPL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;Model Agnostic Peer-to-peer Learning&#65292;&#36890;&#36807;&#28857;&#23545;&#28857;&#36890;&#20449;&#22312;&#37051;&#36817;&#23458;&#25143;&#31471;&#20043;&#38388;&#21516;&#26102;&#23398;&#20064;&#24322;&#36136;&#20010;&#24615;&#21270;&#27169;&#22411;&#21644;&#21327;&#20316;&#22270;&#65292;&#22312;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;MAPL&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#65292;&#24322;&#36136;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#22312;&#25991;&#29486;&#20013;&#26159;&#19968;&#20010;&#30456;&#24403;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#20174;&#32467;&#26500;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#28857;&#23545;&#28857;&#23398;&#20064;&#65288;&#31616;&#31216;MAPL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#37051;&#36817;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#21516;&#26102;&#23398;&#20064;&#24322;&#36136;&#20010;&#24615;&#21270;&#27169;&#22411;&#21644;&#21327;&#20316;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;MAPL&#30001;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#65288;i&#65289;&#26412;&#22320;&#32423;&#21035;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#23398;&#20064;&#65288;PML&#65289;&#65292;&#21033;&#29992;&#23458;&#25143;&#31471;&#20869;&#37096;&#21644;&#23458;&#25143;&#31471;&#38388;&#23545;&#27604;&#25439;&#22833;&#30340;&#32452;&#21512;&#65307;&#65288;ii&#65289;&#32593;&#32476;&#33539;&#22260;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#65292;&#26681;&#25454;&#26412;&#22320;&#20219;&#21153;&#30456;&#20284;&#24615;&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#21160;&#24577;&#22320;&#20248;&#21270;&#21327;&#20316;&#26435;&#37325;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;MAPL&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#19982;&#20854;&#38598;&#20013;&#24335;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;MAPL&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65288;&#25110;&#32773;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26356;&#20248;&#65289;&#65292;&#32780;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20013;&#24515;&#26381;&#21153;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19792v1 Announce Type: cross  Abstract: Effective collaboration among heterogeneous clients in a decentralized setting is a rather unexplored avenue in the literature. To structurally address this, we introduce Model Agnostic Peer-to-peer Learning (coined as MAPL) a novel approach to simultaneously learn heterogeneous personalized models as well as a collaboration graph through peer-to-peer communication among neighboring clients. MAPL is comprised of two main modules: (i) local-level Personalized Model Learning (PML), leveraging a combination of intra- and inter-client contrastive losses; (ii) network-wide decentralized Collaborative Graph Learning (CGL) dynamically refining collaboration weights in a privacy-preserving manner based on local task similarities. Our extensive experimentation demonstrates the efficacy of MAPL and its competitive (or, in most cases, superior) performance compared to its centralized model-agnostic counterparts, without relying on any central ser
&lt;/p&gt;</description></item><item><title>AlloyBERT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#25991;&#26412;&#36755;&#20837;&#39044;&#27979;&#21512;&#37329;&#30340;&#24377;&#24615;&#27169;&#37327;&#21644;&#23624;&#26381;&#24378;&#24230;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;RoBERTa&#32534;&#30721;&#22120;&#21644;&#35757;&#32451;&#26377;&#32032;&#30340;&#20998;&#35789;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;MPEA&#25968;&#25454;&#38598;&#19978;&#30340;&#20302;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.19783</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#37329;&#24615;&#33021;&#39044;&#27979;&#65306;AlloyBERT
&lt;/p&gt;
&lt;p&gt;
AlloyBERT: Alloy Property Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19783
&lt;/p&gt;
&lt;p&gt;
AlloyBERT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#25991;&#26412;&#36755;&#20837;&#39044;&#27979;&#21512;&#37329;&#30340;&#24377;&#24615;&#27169;&#37327;&#21644;&#23624;&#26381;&#24378;&#24230;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;RoBERTa&#32534;&#30721;&#22120;&#21644;&#35757;&#32451;&#26377;&#32032;&#30340;&#20998;&#35789;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;MPEA&#25968;&#25454;&#38598;&#19978;&#30340;&#20302;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21270;&#23398;&#25104;&#20998;&#21644;&#22788;&#29702;&#21442;&#25968;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;AlloyBERT&#27169;&#22411;&#26469;&#39044;&#27979;&#21512;&#37329;&#30340;&#24377;&#24615;&#27169;&#37327;&#21644;&#23624;&#26381;&#24378;&#24230;&#31561;&#24615;&#33021;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;RoBERTa&#32534;&#30721;&#22120;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#33258;&#27880;&#24847;&#26426;&#21046;&#24314;&#31435;&#21333;&#35789;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#20851;&#31995;&#65292;&#20174;&#32780;&#35299;&#37322;&#21487;&#35835;&#30340;&#36755;&#20837;&#24182;&#39044;&#27979;&#21512;&#37329;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19783v1 Announce Type: cross  Abstract: The pursuit of novel alloys tailored to specific requirements poses significant challenges for researchers in the field. This underscores the importance of developing predictive techniques for essential physical properties of alloys based on their chemical composition and processing parameters. This study introduces AlloyBERT, a transformer encoder-based model designed to predict properties such as elastic modulus and yield strength of alloys using textual inputs. Leveraging the pre-trained RoBERTa encoder model as its foundation, AlloyBERT employs self-attention mechanisms to establish meaningful relationships between words, enabling it to interpret human-readable input and predict target alloy properties. By combining a tokenizer trained on our textual data and a RoBERTa encoder pre-trained and fine-tuned for this specific task, we achieved a mean squared error (MSE) of 0.00015 on the Multi Principal Elemental Alloys (MPEA) data set 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#20195;&#29702;&#27169;&#25311;&#20013;&#35266;&#23519;&#21040;&#30340;&#30495;&#23454;&#24066;&#22330;&#39118;&#26684;&#21270;&#20107;&#23454;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;RL&#20195;&#29702;&#22312;&#38754;&#23545;&#22806;&#37096;&#24066;&#22330;&#20914;&#20987;&#26102;&#30340;&#34892;&#20026;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.19781</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22522;&#20110;&#20195;&#29702;&#30340;&#24066;&#22330;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#65306;&#25581;&#31034;&#30495;&#23454;&#30340;&#39118;&#26684;&#21270;&#20107;&#23454;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19781
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#20195;&#29702;&#27169;&#25311;&#20013;&#35266;&#23519;&#21040;&#30340;&#30495;&#23454;&#24066;&#22330;&#39118;&#26684;&#21270;&#20107;&#23454;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;RL&#20195;&#29702;&#22312;&#38754;&#23545;&#22806;&#37096;&#24066;&#22330;&#20914;&#20987;&#26102;&#30340;&#34892;&#20026;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#36164;&#32773;&#21644;&#30417;&#31649;&#26426;&#26500;&#21487;&#20197;&#20174;&#19968;&#20010;&#29616;&#23454;&#30340;&#24066;&#22330;&#27169;&#25311;&#22120;&#20013;&#21463;&#30410;&#65292;&#36825;&#20010;&#27169;&#25311;&#22120;&#35753;&#20182;&#20204;&#33021;&#22815;&#39044;&#27979;&#20182;&#20204;&#22312;&#30495;&#23454;&#24066;&#22330;&#20013;&#30340;&#20915;&#31574;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#24066;&#22330;&#27169;&#25311;&#22120;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#25429;&#25417;&#24066;&#22330;&#21442;&#19982;&#32773;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#22312;&#22806;&#37096;&#24066;&#22330;&#24433;&#21709;&#20107;&#20214;&#25110;&#20854;&#20182;&#21442;&#19982;&#32773;&#34892;&#20026;&#21464;&#21270;&#26102;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20195;&#29702;&#27169;&#25311;&#26694;&#26550;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#35777;&#26126;&#20102;&#27169;&#25311;&#24066;&#22330;&#23637;&#31034;&#20102;&#35266;&#23519;&#21040;&#30340;&#30495;&#23454;&#19990;&#30028;&#24066;&#22330;&#30340;&#39118;&#26684;&#21270;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;RL&#20195;&#29702;&#38754;&#20020;&#22806;&#37096;&#24066;&#22330;&#20914;&#20987;&#26102;&#30340;&#34892;&#20026;&#65292;&#27604;&#22914;&#38378;&#23849;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;RL&#20195;&#29702;&#22312;&#27169;&#25311;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20204;&#24212;&#23545;&#22806;&#37096;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19781v1 Announce Type: cross  Abstract: Investors and regulators can greatly benefit from a realistic market simulator that enables them to anticipate the consequences of their decisions in real markets. However, traditional rule-based market simulators often fall short in accurately capturing the dynamic behavior of market participants, particularly in response to external market impact events or changes in the behavior of other participants. In this study, we explore an agent-based simulation framework employing reinforcement learning (RL) agents. We present the implementation details of these RL agents and demonstrate that the simulated market exhibits realistic stylized facts observed in real-world markets. Furthermore, we investigate the behavior of RL agents when confronted with external market impacts, such as a flash crash. Our findings shed light on the effectiveness and adaptability of RL-based agents within the simulation, offering insights into their response to 
&lt;/p&gt;</description></item><item><title>CLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#26041;&#27861;&#65292;&#29992;&#20110;&#32452;&#21512;&#22810;&#20010;LoRA&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#23558;&#19981;&#21516;&#27010;&#24565;LoRA&#27169;&#22411;&#26080;&#32541;&#28151;&#21512;&#21040;&#19968;&#20010;&#22270;&#20687;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19776</link><description>&lt;p&gt;
CLoRA: &#19968;&#31181;&#23545;&#27604;&#26041;&#27861;&#26469;&#32452;&#21512;&#22810;&#20010; LoRA &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLoRA: A Contrastive Approach to Compose Multiple LoRA Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19776
&lt;/p&gt;
&lt;p&gt;
CLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#26041;&#27861;&#65292;&#29992;&#20110;&#32452;&#21512;&#22810;&#20010;LoRA&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#23558;&#19981;&#21516;&#27010;&#24565;LoRA&#27169;&#22411;&#26080;&#32541;&#28151;&#21512;&#21040;&#19968;&#20010;&#22270;&#20687;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#20013;&#19968;&#31181;&#24378;&#22823;&#19988;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#26469;&#35843;&#25972;&#21644;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20840;&#38754;&#22320;&#37325;&#26032;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340; LoRA &#27169;&#22411;&#65292;&#20363;&#22914;&#20195;&#34920;&#29305;&#23450;&#29483;&#21644;&#29305;&#23450;&#29399;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#22270;&#20687;&#65292;&#35813;&#22270;&#20687;&#30495;&#23454;&#22320;&#20307;&#29616;&#20102; LoRA &#25152;&#23450;&#20041;&#30340;&#20004;&#31181;&#21160;&#29289;&#12290;&#28982;&#32780;&#65292;&#26080;&#32541;&#22320;&#28151;&#21512;&#22810;&#20010;&#27010;&#24565; LoRA &#27169;&#22411;&#20197;&#25429;&#33719;&#19968;&#20010;&#22270;&#20687;&#20013;&#30340;&#21508;&#31181;&#27010;&#24565;&#30340;&#20219;&#21153;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#24120;&#35265;&#26041;&#27861;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#19981;&#21516; LoRA &#27169;&#22411;&#20869;&#30340;&#27880;&#24847;&#26426;&#21046;&#37325;&#21472;&#65292;&#23548;&#33268;&#19968;&#20010;&#27010;&#24565;&#21487;&#33021;&#34987;&#23436;&#20840;&#24573;&#30053;&#65288;&#20363;&#22914;&#28431;&#25481;&#20102;&#29399;&#65289;&#65292;&#25110;&#32773;&#27010;&#24565;&#34987;&#38169;&#35823;&#22320;&#32452;&#21512;&#22312;&#19968;&#36215;&#65288;&#20363;&#22914;&#29983;&#25104;&#20004;&#21482;&#29483;&#30340;&#22270;&#20687;&#32780;&#19981;&#26159;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19776v1 Announce Type: cross  Abstract: Low-Rank Adaptations (LoRAs) have emerged as a powerful and popular technique in the field of image generation, offering a highly effective way to adapt and refine pre-trained deep learning models for specific tasks without the need for comprehensive retraining. By employing pre-trained LoRA models, such as those representing a specific cat and a particular dog, the objective is to generate an image that faithfully embodies both animals as defined by the LoRAs. However, the task of seamlessly blending multiple concept LoRAs to capture a variety of concepts in one image proves to be a significant challenge. Common approaches often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog). To overcome th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22810;&#23610;&#24230;&#23618;&#27425;&#20449;&#24687;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#23618;&#20381;&#36182;&#25439;&#22833;&#26469;&#25552;&#39640;&#24847;&#22270;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35013;&#37197;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#24847;&#22270;&#35782;&#21035;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19770</link><description>&lt;p&gt;
&#20998;&#23618;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#35013;&#37197;&#20219;&#21153;&#20013;&#36828;&#31243;&#25805;&#32437;&#25805;&#20316;&#24847;&#22270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22810;&#23610;&#24230;&#23618;&#27425;&#20449;&#24687;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#23618;&#20381;&#36182;&#25439;&#22833;&#26469;&#25552;&#39640;&#24847;&#22270;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35013;&#37197;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#24847;&#22270;&#35782;&#21035;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#21327;&#20316;&#20013;&#65292;&#20849;&#20139;&#25511;&#21046;&#20026;&#36828;&#31243;&#25805;&#20316;&#26426;&#22120;&#20154;&#25805;&#32437;&#25552;&#20379;&#20102;&#25552;&#21319;&#21046;&#36896;&#21644;&#35013;&#37197;&#24037;&#33402;&#25928;&#29575;&#30340;&#26426;&#20250;&#12290;&#22312;&#25191;&#34892;&#29992;&#25143;&#24847;&#22270;&#19978;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#36741;&#21161;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#40065;&#26834;&#21644;&#21363;&#26102;&#30340;&#24847;&#22270;&#20272;&#35745;&#65292;&#20381;&#36182;&#20110;&#34892;&#20026;&#35266;&#23519;&#12290;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32423;&#21035;&#30340;&#24847;&#22270;&#20272;&#35745;&#25216;&#26415;&#65292;&#21363;&#20302;&#32423;&#21160;&#20316;&#21644;&#39640;&#32423;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#25972;&#21512;&#22810;&#23610;&#24230;&#23618;&#27425;&#20449;&#24687;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#23618;&#20381;&#36182;&#25439;&#22833;&#26469;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31383;&#21475;&#26041;&#27861;&#65292;&#20026;&#36755;&#20837;&#25968;&#25454;&#20998;&#37197;&#36866;&#24403;&#30340;&#20998;&#23618;&#39044;&#27979;&#31383;&#21475;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#36755;&#20837;&#30340;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#28145;&#24230;&#20998;&#23618;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24230;&#21644;&#25552;&#21069;&#24847;&#22270;&#35782;&#21035;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#23454;&#29616;&#22312;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19770v1 Announce Type: cross  Abstract: In human-robot collaboration, shared control presents an opportunity to teleoperate robotic manipulation to improve the efficiency of manufacturing and assembly processes. Robots are expected to assist in executing the user's intentions. To this end, robust and prompt intention estimation is needed, relying on behavioral observations. The framework presents an intention estimation technique at hierarchical levels i.e., low-level actions and high-level tasks, by incorporating multi-scale hierarchical information in neural networks. Technically, we employ hierarchical dependency loss to boost overall accuracy. Furthermore, we propose a multi-window method that assigns proper hierarchical prediction windows of input data. An analysis of the predictive power with various inputs demonstrates the predominance of the deep hierarchical model in the sense of prediction accuracy and early intention identification. We implement the algorithm on a
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20026;&#21355;&#26143;&#20013;&#19968;&#20123;&#38590;&#20197;&#24314;&#27169;&#30340;&#24322;&#24120;&#21152;&#36895;&#24230;&#24773;&#20917;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;</title><link>https://arxiv.org/abs/2403.19736</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21355;&#26143;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for Satellite State Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19736
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20026;&#21355;&#26143;&#20013;&#19968;&#20123;&#38590;&#20197;&#24314;&#27169;&#30340;&#24322;&#24120;&#21152;&#36895;&#24230;&#24773;&#20917;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21355;&#26143;&#29366;&#24577;&#20272;&#35745;&#12299;&#35770;&#25991;&#32763;&#35793;&#25688;&#35201;&#65306;&#22826;&#31354;&#39046;&#22495;&#24847;&#35782;&#65288;SDA&#65289;&#32676;&#20307;&#36890;&#36807;&#23558;&#36712;&#36947;&#29366;&#24577;&#25311;&#21512;&#21040;&#22826;&#31354;&#30417;&#35270;&#32593;&#32476;&#65288;SSN&#65289;&#35266;&#27979;&#21040;&#30340;&#21355;&#26143;&#26469;&#23450;&#26399;&#36319;&#36394;&#21457;&#23556;&#21355;&#26143;&#12290;&#20026;&#20102;&#25311;&#21512;&#36825;&#26679;&#30340;&#36712;&#36947;&#65292;&#38656;&#35201;&#20934;&#30830;&#30340;&#20316;&#29992;&#20110;&#21355;&#26143;&#30340;&#21147;&#27169;&#22411;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#65292;&#20026;&#21355;&#26143;&#29366;&#24577;&#20272;&#35745;&#21644;&#20256;&#25773;&#24320;&#21457;&#20102;&#39640;&#36136;&#37327;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20272;&#31639;&#21644;&#20256;&#25773;&#38750;&#26426;&#21160;&#21355;&#26143;&#30340;&#36712;&#36947;&#29366;&#24577;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65307;&#28982;&#32780;&#65292;&#21355;&#26143;&#21487;&#33021;&#36935;&#21040;&#20960;&#31867;&#19981;&#22826;&#22909;&#24314;&#27169;&#30340;&#24322;&#24120;&#21152;&#36895;&#24230;&#65292;&#27604;&#22914;&#20351;&#29992;&#20302;&#25512;&#21147;&#30005;&#25512;&#36827;&#26469;&#20462;&#25913;&#36712;&#36947;&#30340;&#21355;&#26143;&#12290;&#23545;&#20110;&#36825;&#20123;&#31867;&#21035;&#30340;&#21355;&#26143;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26159;&#19968;&#20010;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#29289;&#29702;&#27169;&#22411;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;DNN&#26159;&#39640;&#24230;&#34920;&#29616;&#21147;&#21644;&#22810;&#21151;&#33021;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19736v1 Announce Type: cross  Abstract: The Space Domain Awareness (SDA) community routinely tracks satellites in orbit by fitting an orbital state to observations made by the Space Surveillance Network (SSN). In order to fit such orbits, an accurate model of the forces that are acting on the satellite is required. Over the past several decades, high-quality, physics-based models have been developed for satellite state estimation and propagation. These models are exceedingly good at estimating and propagating orbital states for non-maneuvering satellites; however, there are several classes of anomalous accelerations that a satellite might experience which are not well-modeled, such as satellites that use low-thrust electric propulsion to modify their orbit. Physics-Informed Neural Networks (PINNs) are a valuable tool for these classes of satellites as they combine physics models with Deep Neural Networks (DNNs), which are highly expressive and versatile function approximator
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#32599;&#39532;&#21270;&#20711;&#20285;&#32599;&#35821;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#24335;&#12289;&#24773;&#32490;&#21644;&#34892;&#20026;&#32447;&#32034;&#21487;&#39640;&#20934;&#30830;&#29575;&#33258;&#21160;&#31579;&#26597;&#25233;&#37057;&#30151;&#30151;&#29366;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#24403;&#21069;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#30830;&#23450;&#38656;&#31215;&#26497;&#24178;&#39044;&#21644;&#25903;&#25345;&#30340;&#20010;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.19728</link><description>&lt;p&gt;
EmoScan&#65306;&#32599;&#39532;&#21270;&#20711;&#20285;&#32599;&#35821;&#25512;&#25991;&#20013;&#25233;&#37057;&#30151;&#30151;&#29366;&#30340;&#33258;&#21160;&#31579;&#26597;
&lt;/p&gt;
&lt;p&gt;
EmoScan: Automatic Screening of Depression Symptoms in Romanized Sinhala Tweets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19728
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#32599;&#39532;&#21270;&#20711;&#20285;&#32599;&#35821;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#24335;&#12289;&#24773;&#32490;&#21644;&#34892;&#20026;&#32447;&#32034;&#21487;&#39640;&#20934;&#30830;&#29575;&#33258;&#21160;&#31579;&#26597;&#25233;&#37057;&#30151;&#30151;&#29366;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#24403;&#21069;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#30830;&#23450;&#38656;&#31215;&#26497;&#24178;&#39044;&#21644;&#25903;&#25345;&#30340;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#32599;&#39532;&#21270;&#20711;&#20285;&#32599;&#35821;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#35782;&#21035;&#23384;&#22312;&#25233;&#37057;&#39118;&#38505;&#30340;&#20010;&#20307;&#12290;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#35821;&#35328;&#27169;&#24335;&#12289;&#24773;&#32490;&#21644;&#34892;&#20026;&#32447;&#32034;&#65292;&#22312;&#19968;&#22871;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#31579;&#26597;&#25233;&#37057;&#30151;&#30151;&#29366;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#27880;&#24847;&#21147;&#23618;&#65292;&#33021;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#65292;&#22312;&#26816;&#27979;&#25233;&#37057;&#30151;&#29366;&#26041;&#38754;&#36798;&#21040;&#20102;93.25%&#30340;&#26174;&#33879;&#20934;&#30830;&#29575;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#22312;&#30830;&#23450;&#38656;&#35201;&#31215;&#26497;&#24178;&#39044;&#21644;&#25903;&#25345;&#30340;&#20010;&#20307;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#24515;&#29702;&#20581;&#24247;&#19987;&#19994;&#20154;&#22763;&#12289;&#20915;&#31574;&#32773;&#21644;&#31038;&#20132;&#23186;&#20307;&#20844;&#21496;&#21487;&#20197;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19728v1 Announce Type: new  Abstract: This work explores the utilization of Romanized Sinhala social media data to identify individuals at risk of depression. A machine learning-based framework is presented for the automatic screening of depression symptoms by analyzing language patterns, sentiment, and behavioural cues within a comprehensive dataset of social media posts. The research has been carried out to compare the suitability of Neural Networks over the classical machine learning techniques. The proposed Neural Network with an attention layer which is capable of handling long sequence data, attains a remarkable accuracy of 93.25% in detecting depression symptoms, surpassing current state-of-the-art methods. These findings underscore the efficacy of this approach in pinpointing individuals in need of proactive interventions and support. Mental health professionals, policymakers, and social media companies can gain valuable insights through the proposed model. Leveragin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20843;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21478;&#22806;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#26356;&#30701;&#12289;&#35789;&#27719;&#21464;&#21270;&#26356;&#23569;&#65292;&#32780;&#20154;&#31867;&#29983;&#25104;&#20869;&#23481;&#21017;&#20351;&#29992;&#20102;&#19968;&#20123;&#29305;&#23450;&#39046;&#22495;&#30456;&#20851;&#20851;&#38190;&#35789;&#34987;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24573;&#30053;&#20102;&#12290;</title><link>https://arxiv.org/abs/2403.19725</link><description>&lt;p&gt;
MUGC&#65306;&#26426;&#22120;&#29983;&#25104;&#19982;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MUGC: Machine Generated versus User Generated Content Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20843;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21478;&#22806;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#26356;&#30701;&#12289;&#35789;&#27719;&#21464;&#21270;&#26356;&#23569;&#65292;&#32780;&#20154;&#31867;&#29983;&#25104;&#20869;&#23481;&#21017;&#20351;&#29992;&#20102;&#19968;&#20123;&#29305;&#23450;&#39046;&#22495;&#30456;&#20851;&#20851;&#38190;&#35789;&#34987;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24573;&#30053;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#30340;&#29616;&#20195;&#31995;&#32479;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#22686;&#24378;&#20854;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#21644;&#36924;&#30495;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21306;&#20998;&#29992;&#25143;&#29983;&#25104;&#19982;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20843;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#65292;&#20197;&#21306;&#20998;&#36328;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;&#35799;&#27468;&#12289;&#25688;&#35201;&#21644;&#35770;&#25991;&#65289;&#20013;&#30340;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#21453;&#26144;&#20102;&#20687;RoBERT&#36825;&#26679;&#30340;&#28909;&#38376;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#19982;&#20154;&#31867;&#29983;&#25104;&#20869;&#23481;&#30456;&#27604;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#26356;&#30701;&#65292;&#35789;&#27719;&#21464;&#21270;&#26356;&#23569;&#12290;&#34429;&#28982;&#20154;&#31867;&#24120;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;&#30456;&#20851;&#20851;&#38190;&#35789;&#34987;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24573;&#30053;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19725v1 Announce Type: cross  Abstract: As advanced modern systems like deep neural networks (DNNs) and generative AI continue to enhance their capabilities in producing convincing and realistic content, the need to distinguish between user-generated and machine generated content is becoming increasingly evident. In this research, we undertake a comparative evaluation of eight traditional machine-learning algorithms to distinguish between machine-generated and human-generated data across three diverse datasets: Poems, Abstracts, and Essays. Our results indicate that traditional methods demonstrate a high level of accuracy in identifying machine-generated data, reflecting the documented effectiveness of popular pre-trained models like RoBERT. We note that machine-generated texts tend to be shorter and exhibit less word variety compared to human-generated content. While specific domain-related keywords commonly utilized by humans, albeit disregarded by current LLMs (Large Lang
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31283;&#20581;&#20027;&#25104;&#20998;&#20998;&#26512;(RPCA)&#36827;&#34892;&#22122;&#22768;&#38477;&#20302;&#21644;&#24322;&#24120;&#20540;&#25490;&#38500;&#65292;&#20197;&#21450;&#20248;&#21270;&#20256;&#24863;&#22120;&#25918;&#32622;(OSP)&#36827;&#34892;&#26377;&#25928;&#25968;&#25454;&#21387;&#32553;&#21644;&#23384;&#20648;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20248;&#21270;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#30340;&#22823;&#25968;&#25454;&#39044;&#27979;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19721</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#31283;&#20581;&#39044;&#27979;&#20998;&#26512;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31283;&#20581;&#20027;&#25104;&#20998;&#20998;&#26512;(RPCA)&#36827;&#34892;&#22122;&#22768;&#38477;&#20302;&#21644;&#24322;&#24120;&#20540;&#25490;&#38500;&#65292;&#20197;&#21450;&#20248;&#21270;&#20256;&#24863;&#22120;&#25918;&#32622;(OSP)&#36827;&#34892;&#26377;&#25928;&#25968;&#25454;&#21387;&#32553;&#21644;&#23384;&#20648;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20248;&#21270;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#30340;&#22823;&#25968;&#25454;&#39044;&#27979;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#23494;&#38598;&#30340;&#26102;&#20195;&#65292;&#22823;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#37325;&#35201;&#36164;&#20135;&#65292;&#20026;&#24320;&#21457;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#24182;&#28145;&#20837;&#25506;&#32034;&#21508;&#31181;&#26410;&#30693;&#39046;&#22495;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#12289;&#23384;&#20648;&#38480;&#21046;&#21644;&#39044;&#27979;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21033;&#29992;&#31283;&#20581;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;RPCA&#65289;&#26377;&#25928;&#38477;&#22122;&#21644;&#21435;&#38500;&#31163;&#32676;&#20540;&#65292;&#20197;&#21450;&#20248;&#21270;&#20256;&#24863;&#22120;&#25918;&#32622;&#65288;OSP&#65289;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#21387;&#32553;&#21644;&#23384;&#20648;&#12290;&#25152;&#25552;&#20986;&#30340;OSP&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#25968;&#25454;&#21387;&#32553;&#65292;&#21516;&#26102;&#20943;&#23569;&#23384;&#20648;&#38656;&#27714;&#65292;&#19988;&#20449;&#24687;&#25439;&#22833;&#19981;&#22823;&#12290;&#34429;&#28982;RPCA&#20026;&#39640;&#32500;&#25968;&#25454;&#31649;&#29702;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26356;&#22909;&#30340;&#36873;&#25321;&#65292;&#20294;&#26412;&#30740;&#31350;&#30340;&#33539;&#22260;&#21017;&#25193;&#23637;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#65292;&#19987;&#27880;&#20110;&#36866;&#29992;&#20110;&#23454;&#26102;&#28023;&#37327;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19721v1 Announce Type: cross  Abstract: In the current data-intensive era, big data has become a significant asset for Artificial Intelligence (AI), serving as a foundation for developing data-driven models and providing insight into various unknown fields. This study navigates through the challenges of data uncertainties, storage limitations, and predictive data-driven modeling using big data. We utilize Robust Principal Component Analysis (RPCA) for effective noise reduction and outlier elimination, and Optimal Sensor Placement (OSP) for efficient data compression and storage. The proposed OSP technique enables data compression without substantial information loss while simultaneously reducing storage needs. While RPCA offers an enhanced alternative to traditional Principal Component Analysis (PCA) for high-dimensional data management, the scope of this work extends its utilization, focusing on robust, data-driven modeling applicable to huge data sets in real-time. For tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#39640;&#32500;&#22810;&#20803;&#38543;&#26426;&#25928;&#24212;&#32447;&#24615;&#27169;&#22411;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#20803;&#23398;&#20064;&#65292;&#22312;&#20351;&#29992;&#24191;&#20041;&#23725;&#22238;&#24402;&#36827;&#34892;&#39044;&#27979;&#26102;&#21457;&#29616;&#65292;&#21033;&#29992;&#38543;&#26426;&#22238;&#24402;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#21487;&#20197;&#22312;&#26032;&#20219;&#21153;&#19978;&#20570;&#20986;&#26356;&#22909;&#30340;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#30340;&#26435;&#37325;&#30697;&#38453;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19720</link><description>&lt;p&gt;
&#20855;&#26377;&#24191;&#20041;&#23725;&#22238;&#24402;&#30340;&#20803;&#23398;&#20064;&#65306;&#39640;&#32500;&#28176;&#36817;&#24615;&#12289;&#26368;&#20248;&#24615;&#21644;&#36229;&#21327;&#26041;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#39640;&#32500;&#22810;&#20803;&#38543;&#26426;&#25928;&#24212;&#32447;&#24615;&#27169;&#22411;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#20803;&#23398;&#20064;&#65292;&#22312;&#20351;&#29992;&#24191;&#20041;&#23725;&#22238;&#24402;&#36827;&#34892;&#39044;&#27979;&#26102;&#21457;&#29616;&#65292;&#21033;&#29992;&#38543;&#26426;&#22238;&#24402;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#21487;&#20197;&#22312;&#26032;&#20219;&#21153;&#19978;&#20570;&#20986;&#26356;&#22909;&#30340;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#30340;&#26435;&#37325;&#30697;&#38453;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Meta-learning&#21363;&#20803;&#23398;&#20064;&#65292;&#25351;&#30340;&#26159;&#20197;&#19968;&#31181;&#26041;&#24335;&#22312;&#22810;&#20010;&#35757;&#32451;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20043;&#33021;&#22815;&#22312;&#26032;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#27979;&#35797;&#20219;&#21153;&#19978;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#23558;&#20803;&#23398;&#20064;&#32435;&#20837;&#39640;&#32500;&#22810;&#20803;&#38543;&#26426;&#25928;&#24212;&#32447;&#24615;&#27169;&#22411;&#26694;&#26550;&#20013;&#65292;&#24182;&#30740;&#31350;&#22522;&#20110;&#24191;&#20041;&#23725;&#22238;&#24402;&#30340;&#39044;&#27979;&#12290;&#22312;&#35813;&#35774;&#23450;&#19979;&#20351;&#29992;&#24191;&#20041;&#23725;&#22238;&#24402;&#30340;&#32479;&#35745;&#30452;&#35273;&#26159;&#65292;&#38543;&#26426;&#22238;&#24402;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#22312;&#26032;&#20219;&#21153;&#19978;&#20570;&#20986;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#35814;&#32454;&#25551;&#36848;&#20102;&#22312;&#25968;&#25454;&#32500;&#24230;&#19982;&#27599;&#20010;&#20219;&#21153;&#26679;&#26412;&#25968;&#25104;&#27604;&#20363;&#22686;&#38271;&#26102;&#65292;&#23545;&#20110;&#26032;&#27979;&#35797;&#20219;&#21153;&#30340;&#39044;&#27979;&#39118;&#38505;&#30340;&#31934;&#30830;&#28176;&#36817;&#34892;&#20026;&#12290;&#25509;&#30528;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#24191;&#20041;&#23725;&#22238;&#24402;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#36873;&#25321;&#20026;&#38543;&#26426;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#36870;&#26102;&#65292;&#36825;&#31181;&#39044;&#27979;&#39118;&#38505;&#26159;&#26368;&#20248;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19720v1 Announce Type: cross  Abstract: Meta-learning involves training models on a variety of training tasks in a way that enables them to generalize well on new, unseen test tasks. In this work, we consider meta-learning within the framework of high-dimensional multivariate random-effects linear models and study generalized ridge-regression based predictions. The statistical intuition of using generalized ridge regression in this setting is that the covariance structure of the random regression coefficients could be leveraged to make better predictions on new tasks. Accordingly, we first characterize the precise asymptotic behavior of the predictive risk for a new test task when the data dimension grows proportionally to the number of samples per task. We next show that this predictive risk is optimal when the weight matrix in generalized ridge regression is chosen to be the inverse of the covariance matrix of random coefficients. Finally, we propose and analyze an estimat
&lt;/p&gt;</description></item><item><title>&#21019;&#24314;&#20102;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#20998;&#23376;&#25351;&#32441;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25509;&#21475;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#23558;&#35813;&#24211;&#25972;&#21512;&#21040;&#20854;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.19718</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#35745;&#31639;&#20998;&#23376;&#25351;&#32441;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
A Python library for efficient computation of molecular fingerprints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19718
&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#20102;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#20998;&#23376;&#25351;&#32441;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25509;&#21475;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#23558;&#35813;&#24211;&#25972;&#21512;&#21040;&#20854;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#38750;&#24120;&#27969;&#34892;&#65292;&#20855;&#26377;&#35832;&#22810;&#24212;&#29992;&#65292;&#22914;&#26032;&#33647;&#21457;&#29616;&#25110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#12290;&#20998;&#23376;&#25351;&#32441;&#26159;&#19968;&#31181;&#24120;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#21270;&#23398;&#20998;&#23376;&#21521;&#37327;&#21270;&#65292;&#20316;&#20026;&#39044;&#22788;&#29702;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#24211;&#33021;&#22815;&#39640;&#25928;&#22320;&#20026;&#22823;&#22411;&#25968;&#25454;&#38598;&#23454;&#29616;&#23427;&#20204;&#65292;&#21033;&#29992;&#29616;&#20195;&#30340;&#22810;&#26680;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#22823;&#37096;&#20998;&#24211;&#24182;&#27809;&#26377;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#65292;&#25110;&#32773;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20860;&#23481;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19718v1 Announce Type: cross  Abstract: Machine learning solutions are very popular in the field of chemoinformatics, where they have numerous applications, such as novel drug discovery or molecular property prediction. Molecular fingerprints are algorithms commonly used for vectorizing chemical molecules as a part of preprocessing in this kind of solution. However, despite their popularity, there are no libraries that implement them efficiently for large datasets, utilizing modern, multicore architectures. On top of that, most of them do not provide the user with an intuitive interface, or one that would be compatible with other machine learning tools.   In this project, we created a Python library that computes molecular fingerprints efficiently and delivers an interface that is comprehensive and enables the user to easily incorporate the library into their existing machine learning workflow. The library enables the user to perform computation on large datasets using paral
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Instagram&#21644;TikTok&#20004;&#20010;&#27969;&#34892;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#20013;&#35270;&#35273;&#27169;&#22411;&#25512;&#26029;&#29992;&#25143;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20026;&#30830;&#20445;&#29992;&#25143;&#33719;&#24471;&#20844;&#24179;&#20934;&#30830;&#30340;&#26381;&#21153;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2403.19717</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#20215;&#20540;500&#20010;&#26631;&#31614;&#65306;Instagram&#21644;TikTok&#26412;&#22320;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#24046;&#24322;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Picture is Worth 500 Labels: A Case Study of Demographic Disparities in Local Machine Learning Models for Instagram and TikTok
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Instagram&#21644;TikTok&#20004;&#20010;&#27969;&#34892;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#20013;&#35270;&#35273;&#27169;&#22411;&#25512;&#26029;&#29992;&#25143;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20026;&#30830;&#20445;&#29992;&#25143;&#33719;&#24471;&#20844;&#24179;&#20934;&#30830;&#30340;&#26381;&#21153;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#24212;&#29992;&#36890;&#36807;&#23558;&#25968;&#25454;&#22788;&#29702;&#31227;&#33267;&#29992;&#25143;&#30340;&#26234;&#33021;&#25163;&#26426;&#26469;&#25903;&#25345;&#29992;&#25143;&#38544;&#31169;&#12290;&#29616;&#22312;&#65292;&#35832;&#22914;&#35270;&#35273;&#27169;&#22411;&#20043;&#31867;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#33021;&#22815;&#22312;&#26412;&#22320;&#20998;&#26512;&#29992;&#25143;&#22270;&#20687;&#65292;&#20197;&#25552;&#21462;&#39537;&#21160;&#22810;&#31181;&#21151;&#33021;&#30340;&#35265;&#35299;&#12290;&#21033;&#29992;&#36825;&#31181;&#26032;&#30340;&#22788;&#29702;&#27169;&#22411;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;TikTok&#21644;Instagram&#65292;&#25581;&#31034;&#36825;&#20004;&#20010;&#24212;&#29992;&#20013;&#30340;&#35270;&#35273;&#27169;&#22411;&#20174;&#29992;&#25143;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#26041;&#38754;&#26159;&#21542;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#12290;&#30001;&#20110;&#35270;&#35273;&#27169;&#22411;&#20026;&#35832;&#22914;&#24180;&#40836;&#39564;&#35777;&#21644;&#20154;&#33080;&#35782;&#21035;&#31561;&#25935;&#24863;&#25216;&#26415;&#25552;&#20379;&#20449;&#21495;&#65292;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#20559;&#35265;&#23545;&#20110;&#30830;&#20445;&#29992;&#25143;&#33719;&#24471;&#20844;&#24179;&#20934;&#30830;&#30340;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25429;&#33719;&#21644;&#35780;&#20272;&#31227;&#21160;&#24212;&#29992;&#20013;ML&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20195;&#30721;&#28151;&#28102;&#12289;&#21407;&#29983;&#24212;&#29992;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19717v1 Announce Type: new  Abstract: Mobile apps have embraced user privacy by moving their data processing to the user's smartphone. Advanced machine learning (ML) models, such as vision models, can now locally analyze user images to extract insights that drive several functionalities. Capitalizing on this new processing model of locally analyzing user images, we analyze two popular social media apps, TikTok and Instagram, to reveal (1) what insights vision models in both apps infer about users from their image and video data and (2) whether these models exhibit performance disparities with respect to demographics. As vision models provide signals for sensitive technologies like age verification and facial recognition, understanding potential biases in these models is crucial for ensuring that users receive equitable and accurate services.   We develop a novel method for capturing and evaluating ML tasks in mobile apps, overcoming challenges like code obfuscation, native c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#26631;&#27880;&#30340;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#36187;&#36947;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.19713</link><description>&lt;p&gt;
NJUST-KMG&#21442;&#21152;TRAC-2024&#20219;&#21153;1&#21644;&#20219;&#21153;2&#65306;&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#26631;&#27880;&#30340;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#36187;&#36947;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#27604;&#36187;&#21253;&#21547;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#35780;&#20998;&#26631;&#27880;&#65292;&#20197;&#25429;&#25417;&#31163;&#32447;&#29615;&#22659;&#21361;&#23475;&#30340;&#24494;&#22937;&#21547;&#20041;&#12290;&#21442;&#19982;&#32773;&#30340;&#20219;&#21153;&#26159;&#35774;&#35745;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#29305;&#23450;&#24773;&#20917;&#19979;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#31163;&#32447;&#21361;&#23475;&#26368;&#21487;&#33021;&#30340;&#30446;&#26631;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#36187;&#36947;&#20013;&#25490;&#21517;&#31532;&#20108;&#65292;F1&#20540;&#20998;&#21035;&#20026;0.73&#21644;0.96&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#36873;&#25321;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25972;&#21512;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19713v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we proposed in the TRAC-2024 Offline Harm Potential dentification which encloses two sub-tasks. The investigation utilized a rich dataset comprised of social media comments in several Indian languages, annotated with precision by expert judges to capture the nuanced implications for offline context harm. The objective assigned to the participants was to design algorithms capable of accurately assessing the likelihood of harm in given situations and identifying the most likely target(s) of offline harm. Our approach ranked second in two separate tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally involved selecting pretrained models for finetuning, incorporating contrastive learning techniques, and culminating in an ensemble approach for the test set.
&lt;/p&gt;</description></item><item><title>STRUM-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23646;&#24615;&#21270;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#24182;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#65292;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#23567;&#20307;&#31215;&#12290;</title><link>https://arxiv.org/abs/2403.19710</link><description>&lt;p&gt;
STRUM-LLM: &#23646;&#24615;&#21270;&#21644;&#32467;&#26500;&#21270;&#23545;&#27604;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
STRUM-LLM: Attributed and Structured Contrastive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19710
&lt;/p&gt;
&lt;p&gt;
STRUM-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23646;&#24615;&#21270;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#24182;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#65292;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#23567;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#32463;&#24120;&#22312;&#20004;&#20010;&#36873;&#39033;&#65288;A vs B&#65289;&#20043;&#38388;&#20570;&#20915;&#31574;&#26102;&#24863;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#36890;&#24120;&#38656;&#35201;&#22312;&#22810;&#20010;&#32593;&#39029;&#19978;&#36827;&#34892;&#32791;&#26102;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STRUM-LLM&#65292;&#36890;&#36807;&#29983;&#25104;&#24102;&#23646;&#24615;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#65292;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;STRUM-LLM&#35782;&#21035;&#20102;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#65306;&#20004;&#20010;&#36873;&#39033;&#22312;&#21738;&#20123;&#29305;&#23450;&#23646;&#24615;&#19978;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#26368;&#26377;&#21487;&#33021;&#24433;&#21709;&#29992;&#25143;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#65292;&#24182;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#20316;&#20026;&#30417;&#30563;&#12290;STRUM-LLM&#23558;&#25152;&#26377;&#25552;&#21462;&#30340;&#20869;&#23481;&#23646;&#24615;&#21270;&#65292;&#20197;&#21450;&#25991;&#26412;&#35777;&#25454;&#65292;&#19988;&#19981;&#38480;&#21046;&#20854;&#22788;&#29702;&#30340;&#36755;&#20837;&#26469;&#28304;&#30340;&#38271;&#24230;&#12290;STRUM-LLM Distilled&#30340;&#21534;&#21520;&#37327;&#27604;&#20855;&#26377;&#30456;&#20284;&#24615;&#33021;&#30340;&#27169;&#22411;&#39640;100&#20493;&#65292;&#21516;&#26102;&#20307;&#31215;&#23567;10&#20493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19710v1 Announce Type: cross  Abstract: Users often struggle with decision-making between two options (A vs B), as it usually requires time-consuming research across multiple web pages. We propose STRUM-LLM that addresses this challenge by generating attributed, structured, and helpful contrastive summaries that highlight key differences between the two options. STRUM-LLM identifies helpful contrast: the specific attributes along which the two options differ significantly and which are most likely to influence the user's decision. Our technique is domain-agnostic, and does not require any human-labeled data or fixed attribute list as supervision. STRUM-LLM attributes all extractions back to the input sources along with textual evidence, and it does not have a limit on the length of input sources that it can process. STRUM-LLM Distilled has 100x more throughput than the models with comparable performance while being 10x smaller. In this paper, we provide extensive evaluations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#36866;&#37197;&#22330;&#26223;&#19979;&#38477;&#20302;&#27599;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#21644;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#22522;&#32447;</title><link>https://arxiv.org/abs/2403.19709</link><description>&lt;p&gt;
&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#22823;&#22411;&#35821;&#38899;&#27169;&#22411;&#30340;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#36866;&#37197;&#22330;&#26223;&#19979;&#38477;&#20302;&#27599;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#21644;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#37197;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#26426;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#36866;&#37197;&#22330;&#26223;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#36866;&#37197;&#22120;&#22312;&#36866;&#37197;&#22120;&#21442;&#25968;&#20998;&#37197;&#26041;&#38754;&#26159;&#20998;&#23618;&#30340;&#12290;&#36866;&#37197;&#22120;&#30001;&#19968;&#20010;&#20849;&#20139;&#30340;&#25511;&#21046;&#32593;&#32476;&#21644;&#22810;&#20010;&#20219;&#21153;&#32423;&#36866;&#37197;&#22120;&#22836;&#32452;&#25104;&#65292;&#20197;&#20943;&#23569;&#27599;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#24320;&#38144;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#36824;&#26159;&#36882;&#24402;&#30340;&#65292;&#22240;&#27492;&#25972;&#20010;&#36866;&#37197;&#22120;&#21442;&#25968;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#20043;&#38388;&#34987;&#37325;&#29992;&#12290;&#25105;&#20204;&#30340;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;&#65288;HRA&#65289;&#22312;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#36866;&#37197;&#35774;&#32622;&#20013;&#37117;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#20197;&#21450;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19709v1 Announce Type: cross  Abstract: Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task parameter overhead is considered still high when the number of downstream tasks to adapt for is large. We introduce an adapter module that has a better efficiency in large scale multi-task adaptation scenario. Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks. The adapter is also recurrent so the entire adapter parameters are reused across different layers of the pre-trained model. Our Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based approaches as well as full model fine-tuning baseline in both single and multi-task adaptation settings when evalua
&lt;/p&gt;</description></item><item><title>AttentionStore&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23454;&#29616;KV&#32531;&#23384;&#30340;&#22797;&#29992;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#37325;&#22797;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.19708</link><description>&lt;p&gt;
AttentionStore: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#23454;&#29616;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#25104;&#26412;&#25928;&#30410;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19708
&lt;/p&gt;
&lt;p&gt;
AttentionStore&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23454;&#29616;KV&#32531;&#23384;&#30340;&#22797;&#29992;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#37325;&#22797;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37325;&#22797;&#35745;&#31639;&#21382;&#21490;&#35760;&#21495;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#65292;&#23548;&#33268;&#29616;&#26377;&#29992;&#20110;&#25191;&#34892;&#22810;&#36718;&#23545;&#35805;&#30340;LLM&#26381;&#21153;&#24341;&#25806;&#25928;&#29575;&#20302;&#19979;&#65292;&#20135;&#29983;&#39640;&#26114;&#30340;&#26381;&#21153;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AttentionStore&#65292;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36328;&#22810;&#36718;&#23545;&#35805;&#30340;KV&#32531;&#23384;&#22797;&#29992;&#65288;&#21363; &#27880;&#24847;&#21147;&#22797;&#29992;&#65289;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#37325;&#22797;&#35745;&#31639;&#24320;&#38144;&#12290;AttentionStore&#32500;&#25252;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;KV&#32531;&#23384;&#31995;&#32479;&#65292;&#21033;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#20869;&#23384;/&#23384;&#20648;&#20171;&#36136;&#20026;&#25152;&#26377;&#35831;&#27714;&#20445;&#23384;KV&#32531;&#23384;&#12290;&#20026;&#20102;&#20943;&#23569;&#24930;&#36895;&#20171;&#36136;&#30340;KV&#32531;&#23384;&#35775;&#38382;&#24320;&#38144;&#65292;AttentionStore&#37319;&#29992;&#36880;&#23618;&#39044;&#21152;&#36733;&#21644;&#24322;&#27493;&#20445;&#23384;&#26041;&#26696;&#65292;&#23558;KV&#32531;&#23384;&#35775;&#38382;&#19982;GPU&#35745;&#31639;&#37325;&#21472;&#12290;&#20026;&#30830;&#20445;&#35201;&#35775;&#38382;&#30340;KV&#32531;&#23384;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19708v1 Announce Type: new  Abstract: Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches (i.e., attention reuse) across multi-turn conversations, significantly reducing the repetitive computation overheads. AttentionStore maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, AttentionStore employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are pl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#19968;&#20010;&#36335;&#24452;&#20449;&#21495;&#25104;&#20998;&#21151;&#29575;&#30340;NLOS&#25233;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;NLOS&#26465;&#20214;&#19979;&#30340;&#27979;&#37327;&#32467;&#26524;&#24182;&#38477;&#20302;&#20854;&#22312;&#26631;&#31614;&#20301;&#32622;&#20272;&#35745;&#20013;&#30340;&#37325;&#35201;&#24615;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.19706</link><description>&lt;p&gt;
UWB&#23450;&#20301;&#31995;&#32479;&#20013;&#22522;&#20110;&#31532;&#19968;&#20010;&#36335;&#24452;&#25104;&#20998;&#21151;&#29575;&#30340;NLOS&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
First path component power based NLOS mitigation in UWB positioning system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#19968;&#20010;&#36335;&#24452;&#20449;&#21495;&#25104;&#20998;&#21151;&#29575;&#30340;NLOS&#25233;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;NLOS&#26465;&#20214;&#19979;&#30340;&#27979;&#37327;&#32467;&#26524;&#24182;&#38477;&#20302;&#20854;&#22312;&#26631;&#31614;&#20301;&#32622;&#20272;&#35745;&#20013;&#30340;&#37325;&#35201;&#24615;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;UWB&#23450;&#20301;&#31995;&#32479;&#30340;NLOS&#65288;&#38750;&#35270;&#36317;&#65289;&#25233;&#21046;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#23616;&#37096;&#23545;&#35937;&#19982;&#24418;&#25104;&#31995;&#32479;&#22522;&#30784;&#35774;&#26045;&#30340;&#38170;&#28857;&#20043;&#38388;&#30340;&#20256;&#25773;&#26465;&#20214;&#20998;&#31867;&#20026;LOS&#65288;&#35270;&#36317;&#65289;&#12289;NLOS&#21644;&#20005;&#37325;NLOS&#20013;&#30340;&#19968;&#31181;&#12290;&#26681;&#25454;&#31532;&#19968;&#20010;&#36335;&#24452;&#20449;&#21495;&#25104;&#20998;&#21151;&#29575;&#27979;&#37327;&#36827;&#34892;&#38750;&#35270;&#36317;&#26816;&#27979;&#12290;&#38024;&#23545;&#27599;&#20010;&#31867;&#21035;&#65292;&#22522;&#20110;&#22312;&#37197;&#22791;&#40784;&#20840;&#30340;&#20844;&#23507;&#36827;&#34892;&#30340;&#27979;&#37327;&#27963;&#21160;&#26399;&#38388;&#25910;&#38598;&#21040;&#30340;&#32467;&#26524;&#65292;&#20272;&#35745;&#20102;&#24179;&#22343;NLOS&#24341;&#20837;&#21040;&#36798;&#26102;&#38388;&#20559;&#24046;&#21644;&#20559;&#24046;&#26631;&#20934;&#24046;&#12290;&#20026;&#20102;&#23450;&#20301;&#26631;&#31614;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;EKF&#65288;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65289;&#30340;&#31639;&#27861;&#12290;&#25552;&#20986;&#30340;NLOS&#25233;&#21046;&#26041;&#27861;&#22312;&#32416;&#27491;&#22312;NLOS&#26465;&#20214;&#19979;&#33719;&#24471;&#30340;&#27979;&#37327;&#32467;&#26524;&#24182;&#38477;&#20302;&#20854;&#22312;&#26631;&#31614;&#20301;&#32622;&#20272;&#35745;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#12290;&#35770;&#25991;&#21253;&#25324;&#35813;&#26041;&#27861;&#21644;r
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19706v1 Announce Type: cross  Abstract: The paper describes an NLOS (Non-Line-of-Sight) mitigation method intended for use in a UWB positioning system. In the proposed method propagation conditions between the localized objects and the anchors forming system infrastructure are classified into one of three categories: LOS (Line-of-Sight), NLOS and severe NLOS. Non-Line-of-Sight detection is conducted based on first path signal component power measurements. For each of the categories, average NLOS inducted time of arrival bias and bias standard deviation have been estimated based on results gathered during a measurement campaign conducted in a fully furnished apartment. To locate a tag, an EKF (Extended Kalman Filter) based algorithm is used. The proposed method of NLOS mitigation consists in correcting measurement results obtained in NLOS conditions and lowering their significance in a tag position estimation process. The paper includes the description of the method and the r
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;</title><link>https://arxiv.org/abs/2403.19669</link><description>&lt;p&gt;
&#20998;&#26512;&#35821;&#35328;&#21644;&#35270;&#35273;&#22312;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Roles of Language and Vision in Learning from Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19669
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#35821;&#35328;&#26159;&#21542;&#26377;&#21161;&#20110;&#29702;&#35299;&#35270;&#35273;&#19990;&#30028;&#65311;&#23454;&#38469;&#35266;&#23519;&#19990;&#30028;&#38656;&#35201;&#30475;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#29992;&#25991;&#23383;&#25551;&#36848;&#21527;&#65311;&#20851;&#20110;&#26234;&#33021;&#26412;&#36136;&#30340;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#24456;&#38590;&#22238;&#31572;&#65292;&#22240;&#20026;&#25105;&#20204;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#30340;&#20363;&#23376;&#8212;&#8212;&#20154;&#31867;&#8212;&#8212;&#20197;&#21450;&#26377;&#38480;&#30340;&#29420;&#31435;&#35821;&#35328;&#25110;&#35270;&#35273;&#30340;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#22797;&#26434;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#25506;&#32034;&#35821;&#35328;&#21644;&#35270;&#35273;&#23545;&#20110;&#23398;&#20064;&#19990;&#30028;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#20174;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#20013;&#20999;&#38500;&#32452;&#20214;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#24674;&#22797;&#20102;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23427;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#32780;&#35821;&#35328;&#20284;&#20046;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 Announce Type: cross  Abstract: Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;GQA-LUT&#31639;&#27861;&#22312;&#21464;&#21387;&#22120;&#20013;&#30340;&#38750;&#32447;&#24615;&#25805;&#20316;&#20013;&#20855;&#26377;&#37327;&#21270;&#24863;&#30693;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;INT8-based LUT&#36924;&#36817;&#30340;&#24212;&#29992;&#65292;&#33410;&#32422;&#20102;&#22823;&#37327;&#30828;&#20214;&#21644;&#21151;&#32791;</title><link>https://arxiv.org/abs/2403.19591</link><description>&lt;p&gt;
&#22522;&#22240;&#37327;&#21270;&#24863;&#30693;&#36924;&#36817;&#29992;&#20110;&#21464;&#21387;&#22120;&#20013;&#30340;&#38750;&#32447;&#24615;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;GQA-LUT&#31639;&#27861;&#22312;&#21464;&#21387;&#22120;&#20013;&#30340;&#38750;&#32447;&#24615;&#25805;&#20316;&#20013;&#20855;&#26377;&#37327;&#21270;&#24863;&#30693;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;INT8-based LUT&#36924;&#36817;&#30340;&#24212;&#29992;&#65292;&#33410;&#32422;&#20102;&#22823;&#37327;&#30828;&#20214;&#21644;&#21151;&#32791;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#21387;&#22120;&#21450;&#20854;&#36731;&#37327;&#32423;&#21464;&#20307;&#20013;&#65292;&#38750;&#32447;&#24615;&#20989;&#25968;&#26222;&#36941;&#23384;&#22312;&#65292;&#23548;&#33268;&#30828;&#20214;&#25104;&#26412;&#26174;&#33879;&#19988;&#32463;&#24120;&#34987;&#20302;&#20272;&#12290;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#20316;&#21697;&#36890;&#36807;&#20998;&#27573;&#32447;&#24615;&#36924;&#36817;&#26469;&#20248;&#21270;&#36825;&#20123;&#25805;&#20316;&#65292;&#24182;&#23558;&#21442;&#25968;&#23384;&#20648;&#22312;&#26597;&#25214;&#34920;&#65288;LUT&#65289;&#20013;&#65292;&#20294;&#22823;&#22810;&#25968;&#38656;&#35201;&#19981;&#21451;&#22909;&#30340;&#39640;&#31934;&#24230;&#31639;&#26415;&#65292;&#22914;FP/INT 32&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#32431;&#25972;&#25968;INT&#37327;&#21270;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36951;&#20256;LUT-&#36924;&#36817;&#31639;&#27861;&#65292;&#21363;GQA-LUT&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#30830;&#23450;&#20855;&#26377;&#37327;&#21270;&#24847;&#35782;&#30340;&#21442;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#26222;&#36890;&#21644;&#32447;&#24615;Transformer&#27169;&#22411;&#30340;&#25361;&#25112;&#24615;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;GQA-LUT&#23454;&#29616;&#20102;&#21487;&#24573;&#30053;&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;GQA-LUT&#20351;&#24471;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;INT8&#30340;LUT&#36924;&#36817;&#65292;&#30456;&#27604;&#39640;&#31934;&#24230;FP/INT 32&#65292;&#21487;&#20197;&#23454;&#29616;81.3~81.7%&#30340;&#38754;&#31215;&#33410;&#32422;&#21644;79.3~80.2%&#30340;&#21151;&#32791;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19591v1 Announce Type: new  Abstract: Non-linear functions are prevalent in Transformers and their lightweight variants, incurring substantial and frequently underestimated hardware costs. Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT quantization. This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19060</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#26045;&#24037;&#26426;&#22120;&#20154;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24314;&#31569;&#34892;&#19994;&#20013;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#38598;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#21270;&#29305;&#23450;&#20219;&#21153;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#24314;&#31569;&#24037;&#20316;&#27969;&#31243;&#20013;&#20154;&#31867;&#22240;&#32032;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#26412;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#8220;&#24037;&#20316;&#20276;&#20387;&#28459;&#28216;&#22120;&#8221;&#65292;&#26088;&#22312;&#21327;&#21161;&#24314;&#31569;&#24037;&#20154;&#23436;&#25104;&#20854;&#29616;&#26377;&#23454;&#36341;&#65292;&#26088;&#22312;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#27969;&#30021;&#24615;&#65292;&#21516;&#26102;&#23562;&#37325;&#24314;&#31569;&#21171;&#21160;&#30340;&#25216;&#26415;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#22312;&#26408;&#24037;&#27169;&#26495;&#24037;&#31243;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#36890;&#36807;&#29615;&#22659;&#30456;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26426;&#21160;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#30340;&#24037;&#20154;-&#26426;&#22120;&#20154;&#21327;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#65292;&#20513;&#23548;&#21327;&#20316;&#27169;&#22411;&#65292;&#20854;&#20013;&#33258;&#36866;&#24212;&#26426;&#22120;&#20154;&#25903;&#25345;&#32780;&#19981;&#26159;&#21462;&#20195;&#20154;&#31867;&#65292;&#24378;&#35843;&#20102;&#20132;&#20114;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19060v1 Announce Type: cross  Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive a
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#22914;Deep Sets&#21644;Transformers&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#30340;&#36827;&#23637;</title><link>https://arxiv.org/abs/2403.17410</link><description>&lt;p&gt;
&#35770;&#25490;&#21015;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On permutation-invariant neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17410
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22914;Deep Sets&#21644;Transformers&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#22312;&#20551;&#35774;&#36755;&#20837;&#25968;&#25454;&#36981;&#24490;&#22522;&#20110;&#21521;&#37327;&#30340;&#26684;&#24335;&#30340;&#21069;&#25552;&#19979;&#35774;&#35745;&#65292;&#30528;&#37325;&#20110;&#22522;&#20110;&#21521;&#37327;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38656;&#27714;&#28041;&#21450;&#22522;&#20110;&#38598;&#21512;&#30340;&#20219;&#21153;&#30340;&#22686;&#38271;&#65292;&#30740;&#31350;&#30028;&#23545;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#20852;&#36259;&#21457;&#29983;&#20102;&#33539;&#24335;&#36716;&#21464;&#12290;&#36817;&#24180;&#26469;&#65292;Deep Sets&#21644;Transformers&#31561;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20986;&#29616;&#22312;&#22788;&#29702;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#26550;&#26500;&#19987;&#38376;&#35774;&#35745;&#20026;&#33258;&#28982;&#23481;&#32435;&#38598;&#21512;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#22788;&#29702;&#38598;&#21512;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22823;&#37327;&#33268;&#21147;&#20110;&#25506;&#32034;&#21644;&#21033;&#29992;&#36825;&#20123;&#26550;&#26500;&#33021;&#21147;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#20197;&#36924;&#36817;&#38598;&#21512;&#20989;&#25968;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#26088;&#22312;&#27010;&#36848;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17410v1 Announce Type: cross  Abstract: Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17343</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#30340;&#20813;&#36153;&#21161;&#25512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models are Free Boosters for Biomedical Imaging Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#20256;&#32479;&#19978;&#32570;&#20047;&#35821;&#35328;&#25110;&#25991;&#26412;&#25968;&#25454;&#30340;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#25552;&#21462;&#30340;&#20923;&#32467;&#21464;&#21387;&#22120;&#22359;&#20316;&#20026;&#21019;&#26032;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#12290;&#36825;&#31181;&#31574;&#30053;&#19982;&#36890;&#24120;&#20381;&#36182;&#20110;&#35821;&#35328;&#39537;&#21160;&#25552;&#31034;&#21644;&#36755;&#20837;&#30340;&#26631;&#20934;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#26694;&#26550;&#26377;&#30528;&#26174;&#33879;&#30340;&#19981;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;LLMs&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;2D&#21644;3D&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#65292;&#20805;&#24403;&#21363;&#25554;&#21363;&#29992;&#30340;&#21161;&#25512;&#22120;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;M&#30340;&#24191;&#27867;&#12289;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16970</link><description>&lt;p&gt;
&#32852;&#21512;&#33016;&#37096;X&#20809;&#35786;&#26029;&#21644;&#20020;&#24202;&#35270;&#35273;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20020;&#24202;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#39046;&#22495;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#22312;&#25918;&#23556;&#23398;&#31579;&#26597;&#36807;&#31243;&#20013;&#20020;&#24202;&#21307;&#29983;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#22270;&#20026;&#25552;&#20379;&#37325;&#35201;&#27934;&#23519;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#36164;&#20135;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#36825;&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#65292;&#21033;&#29992;&#20102;DenseNet201&#20027;&#24178;&#21644;&#22522;&#20110;&#27531;&#24046;&#21644;&#33192;&#32960;&#28608;&#21169;&#22359;&#30340;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#29992;&#20110;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#36827;&#34892;&#30142;&#30149;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
&lt;/p&gt;</description></item><item><title>FedAC&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#25552;&#20379;&#20840;&#23616;&#30693;&#35782;&#65292;&#24341;&#20837;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21450;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16460</link><description>&lt;p&gt;
FedAC&#65306;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20998;&#31751;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16460
&lt;/p&gt;
&lt;p&gt;
FedAC&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#25552;&#20379;&#20840;&#23616;&#30693;&#35782;&#65292;&#24341;&#20837;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21450;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20998;&#31751;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedAC&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#24182;&#21033;&#29992;&#19981;&#21516;&#30340;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#26377;&#25928;&#22320;&#23558;&#20840;&#23616;&#30693;&#35782;&#25972;&#21512;&#21040;&#31751;&#20869;&#23398;&#20064;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65307;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38477;&#32500;&#30340;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65307;&#24182;&#19988;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#22797;&#26434;&#24615;&#30340;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36866;&#24212;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16460v1 Announce Type: cross  Abstract: Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#30446;&#26631;&#22359;&#24494;&#35843;&#65292;&#26681;&#25454;&#25968;&#25454;&#28418;&#31227;&#31867;&#22411;&#24494;&#35843;&#19981;&#21516;&#27169;&#22359;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21644;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.15905</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#37327;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#30446;&#26631;&#22359;&#24494;&#35843;&#65292;&#26681;&#25454;&#25968;&#25454;&#28418;&#31227;&#31867;&#22411;&#24494;&#35843;&#19981;&#21516;&#27169;&#22359;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21644;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20010;&#24615;&#21270;&#20197;&#35299;&#20915;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20391;&#37325;&#20110;&#24494;&#35843;&#23436;&#25972;&#22522;&#30784;&#27169;&#22411;&#25110;&#20854;&#26368;&#21518;&#20960;&#23618;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#33021;&#28304;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#8212;&#8212;&#30446;&#26631;&#22359;&#24494;&#35843;&#65288;TBFT&#65289;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#28418;&#31227;&#21644;&#20010;&#24615;&#21270;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#36755;&#20837;&#32423;&#21035;&#12289;&#29305;&#24449;&#32423;&#21035;&#21644;&#36755;&#20986;&#32423;&#21035;&#12290;&#38024;&#23545;&#27599;&#31181;&#31867;&#22411;&#65292;&#25105;&#20204;&#24494;&#35843;&#19981;&#21516;&#27169;&#22411;&#22359;&#20197;&#23454;&#29616;&#22312;&#38477;&#20302;&#33021;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36755;&#20837;&#32423;&#12289;&#29305;&#24449;&#32423;&#21644;&#36755;&#20986;&#32423;&#23545;&#24212;&#20110;&#24494;&#35843;&#27169;&#22411;&#30340;&#21069;&#31471;&#12289;&#20013;&#27573;&#21644;&#21518;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15905v1 Announce Type: new  Abstract: The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#35270;&#39057;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24773;&#32490;&#27010;&#29575;&#65292;&#24182;&#22522;&#20110;&#39044;&#23450;&#20041;&#35268;&#21017;&#36827;&#34892;&#20915;&#31574;&#65292;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;&#28508;&#21147;&#20026;&#20154;&#31867;&#22522;&#26412;&#21644;&#22797;&#21512;&#24773;&#24863;&#24773;&#22659;&#19979;&#30340;&#38899;&#35270;&#39057;&#25968;&#25454;&#27880;&#37322;&#25552;&#20379;&#26234;&#33021;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.12687</link><description>&lt;p&gt;
&#22522;&#20110;&#36831;&#26469;&#30340;&#27169;&#24577;&#34701;&#21512;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#20915;&#31574;&#30340;&#38899;&#35270;&#39057;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12687
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#35270;&#39057;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24773;&#32490;&#27010;&#29575;&#65292;&#24182;&#22522;&#20110;&#39044;&#23450;&#20041;&#35268;&#21017;&#36827;&#34892;&#20915;&#31574;&#65292;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;&#28508;&#21147;&#20026;&#20154;&#31867;&#22522;&#26412;&#21644;&#22797;&#21512;&#24773;&#24863;&#24773;&#22659;&#19979;&#30340;&#38899;&#35270;&#39057;&#25968;&#25454;&#27880;&#37322;&#25552;&#20379;&#26234;&#33021;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#20845;&#23626;ABAW&#27604;&#36187;&#30340;SUN&#22242;&#38431;&#38024;&#23545;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#25361;&#25112;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#35270;&#39057;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#34701;&#21512;&#24773;&#32490;&#27010;&#29575;&#27700;&#24179;&#30340;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#32780;&#20851;&#20110;&#22797;&#21512;&#34920;&#36798;&#39044;&#27979;&#30340;&#20915;&#31574;&#22522;&#20110;&#39044;&#23450;&#20041;&#35268;&#21017;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20351;&#29992;&#20219;&#20309;&#29305;&#23450;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#35821;&#26009;&#35757;&#32451;&#21644;&#36328;&#35821;&#26009;&#39564;&#35777;&#35774;&#32622;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#20174;&#25361;&#25112;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#20026;&#22312;&#20154;&#31867;&#22522;&#26412;&#21644;&#22797;&#21512;&#24773;&#24863;&#24773;&#22659;&#19979;&#27880;&#37322;&#38899;&#35270;&#39057;&#25968;&#25454;&#30340;&#26234;&#33021;&#24037;&#20855;&#24320;&#21457;&#22880;&#23450;&#22522;&#30784;&#12290;&#28304;&#20195;&#30721;&#24050;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12687v1 Announce Type: cross  Abstract: This paper presents the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition. We propose a novel audio-visual method for compound expression recognition. Our method relies on emotion recognition models that fuse modalities at the emotion probability level, while decisions regarding the prediction of compound expressions are based on predefined rules. Notably, our method does not use any training data specific to the target task. The method is evaluated in multi-corpus training and cross-corpus validation setups. Our findings from the challenge demonstrate that the proposed method can potentially form a basis for development of intelligent tools for annotating audio-visual data in the context of human's basic and compound emotions. The source code is publicly available.
&lt;/p&gt;</description></item><item><title>&#22312;&#23398;&#20064;&#30340;&#37325;&#21152;&#26435;(LRW)&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20013;&#20351;&#29992;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#20316;&#20026;&#39564;&#35777;&#38598;&#65292;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.12236</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#22256;&#38590;&#26679;&#26412;&#19978;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization via Meta-Learning on Hard Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12236
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#30340;&#37325;&#21152;&#26435;(LRW)&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20013;&#20351;&#29992;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#20316;&#20026;&#39564;&#35777;&#38598;&#65292;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#37325;&#21152;&#26435;(LRW)&#26041;&#27861;&#29992;&#19968;&#20010;&#20248;&#21270;&#20934;&#21017;&#20026;&#35757;&#32451;&#23454;&#20363;&#20998;&#37197;&#26435;&#37325;&#65292;&#20197;&#20415;&#22312;&#19968;&#20010;&#20195;&#34920;&#24615;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#24418;&#24335;&#21270;&#20102;LRW&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#39564;&#35777;&#38598;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39564;&#35777;&#38598;&#20013;&#20351;&#29992;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#26082;&#19982;&#29702;&#35770;&#30456;&#20851;&#65292;&#21448;&#26377;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#36825;&#20010;&#20803;&#20248;&#21270;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#27425;&#35757;&#32451;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#36827;&#34892;&#35880;&#24910;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#26131;&#39564;&#35777;&#25968;&#25454;&#19968;&#36215;&#30340;LRW&#34920;&#29616;&#22987;&#32456;&#27604;&#38590;&#39564;&#35777;&#25968;&#25454;&#19968;&#36215;&#30340;LRW&#34920;&#29616;&#24046;&#65292;&#20174;&#32780;&#30830;&#31435;&#20102;&#25105;&#20204;&#30340;&#20803;&#20248;&#21270;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#36716;&#31227;&#25361;&#25112;&#19978;&#32988;&#36807;&#20102;&#21508;&#31181;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12236v1 Announce Type: new  Abstract: Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Ima
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11624</link><description>&lt;p&gt;
&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Channel Multiplex Graph Neural Networks for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#25429;&#25417;&#21453;&#26144;&#20010;&#20154;&#20559;&#22909;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23646;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#25512;&#33616;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#19978;&#30340;&#28857;&#20987;&#12289;&#26631;&#35760;&#25910;&#34255;&#21644;&#36141;&#20080;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#65306;(1) &#19981;&#36275;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#22810;&#36890;&#36335;&#20851;&#31995;&#24418;&#25104;&#30340;&#21508;&#31181;&#34892;&#20026;&#27169;&#24335;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;(2) &#24573;&#30053;&#20102;&#34892;&#20026;&#27169;&#24335;&#20013;&#19981;&#21516;&#20851;&#31995;&#23545;&#25512;&#33616;&#31995;&#32479;&#22330;&#26223;&#20013;&#30446;&#26631;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#19968;&#25968;&#23398;&#30446;&#26631;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;&#21512;&#25104;&#25968;&#25454;&#24212;&#35813;&#20174;&#19982;&#35266;&#27979;&#25968;&#25454;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25552;&#21462;&#65292;&#24182;&#19988;&#25512;&#29702;&#20102;&#20219;&#20309;&#19968;&#32452;&#25351;&#26631;&#30340;&#23436;&#25972;&#24615;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#40723;&#21169;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#22522;&#32447;&#21644;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.10424</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#35780;&#20272;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Structured Evaluation of Synthetic Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#19968;&#25968;&#23398;&#30446;&#26631;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;&#21512;&#25104;&#25968;&#25454;&#24212;&#35813;&#20174;&#19982;&#35266;&#27979;&#25968;&#25454;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25552;&#21462;&#65292;&#24182;&#19988;&#25512;&#29702;&#20102;&#20219;&#20309;&#19968;&#32452;&#25351;&#26631;&#30340;&#23436;&#25972;&#24615;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#40723;&#21169;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#22522;&#32447;&#21644;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#36890;&#24120;&#23384;&#22312;&#20294;&#24448;&#24448;&#19981;&#23436;&#25972;&#65292;&#25968;&#25454;&#37327;&#36739;&#23567;&#65292;&#24182;&#19988;&#30001;&#20110;&#38544;&#31169;&#21407;&#22240;&#21463;&#38480;&#20110;&#35775;&#38382;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23384;&#22312;&#35768;&#22810;&#29992;&#20110;&#35780;&#20272;&#21512;&#25104;&#34920;&#26684;&#24335;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#36825;&#20123;&#25351;&#26631;&#30340;&#23458;&#35266;&#12289;&#36830;&#36143;&#30340;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#19968;&#25968;&#23398;&#30446;&#26631;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35748;&#20026;&#21512;&#25104;&#25968;&#25454;&#24212;&#35813;&#20174;&#19982;&#35266;&#27979;&#25968;&#25454;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25552;&#21462;&#12290;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#21508;&#31181;&#32467;&#26500;&#20998;&#35299;&#65292;&#35813;&#26694;&#26550;&#39318;&#27425;&#20801;&#35768;&#25105;&#20204;&#25512;&#29702;&#20219;&#20309;&#19968;&#32452;&#25351;&#26631;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#32479;&#19968;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#21253;&#25324;&#28304;&#33258;&#24544;&#23454;&#24615;&#32771;&#34385;&#12289;&#19979;&#28216;&#24212;&#29992;&#21644;&#22522;&#20110;&#27169;&#22411;&#26041;&#27861;&#30340;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#28608;&#21169;&#20102;&#26080;&#27169;&#22411;&#22522;&#32447;&#21644;&#19968;&#31995;&#21015;&#26032;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#32467;&#26500;&#21270;&#20449;&#24687;&#21512;&#25104;&#22120;&#21644;&#21512;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10424v1 Announce Type: new  Abstract: Tabular data is common yet typically incomplete, small in volume, and access-restricted due to privacy concerns. Synthetic data generation offers potential solutions. Many metrics exist for evaluating the quality of synthetic tabular data; however, we lack an objective, coherent interpretation of the many metrics. To address this issue, we propose an evaluation framework with a single, mathematical objective that posits that the synthetic data should be drawn from the same distribution as the observed data. Through various structural decomposition of the objective, this framework allows us to reason for the first time the completeness of any set of metrics, as well as unifies existing metrics, including those that stem from fidelity considerations, downstream application, and model-based approaches. Moreover, the framework motivates model-free baselines and a new spectrum of metrics. We evaluate structurally informed synthesizers and syn
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#65292;&#30740;&#31350;&#20102;SGD&#22312;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#27604;&#20102;&#39044;&#26465;&#20214;&#21270;SGD&#21644;&#65288;&#26631;&#20934;&#21644;&#39044;&#26465;&#20214;&#21270;&#65289;&#23725;&#22238;&#24402;&#65292;&#20026;&#25913;&#21892;SGD&#29702;&#35299;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#20851;&#38190;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.08585</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#25913;&#21892;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08585
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#65292;&#30740;&#31350;&#20102;SGD&#22312;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#27604;&#20102;&#39044;&#26465;&#20214;&#21270;SGD&#21644;&#65288;&#26631;&#20934;&#21644;&#39044;&#26465;&#20214;&#21270;&#65289;&#23725;&#22238;&#24402;&#65292;&#20026;&#25913;&#21892;SGD&#29702;&#35299;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#20851;&#38190;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31639;&#27861;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#27867;&#21270;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;SGD&#30340;&#27867;&#21270;&#24615;&#33021;&#26377;&#26102;&#20250;&#27604;&#23725;&#22238;&#24402;&#24046;&#65292;&#36825;&#26159;&#30001;&#20110;&#27839;&#19981;&#21516;&#32500;&#24230;&#30340;&#20248;&#21270;&#19981;&#22343;&#21248;&#36896;&#25104;&#30340;&#12290;&#39044;&#26465;&#20214;&#21270;&#36890;&#36807;&#37325;&#26032;&#24179;&#34913;&#27839;&#19981;&#21516;&#26041;&#21521;&#30340;&#20248;&#21270;&#26469;&#25552;&#20379;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#33258;&#28982;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#26465;&#20214;&#21270;&#33021;&#22815;&#25552;&#21319;SGD&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#31243;&#24230;&#20197;&#21450;&#23427;&#26159;&#21542;&#33021;&#22815;&#22635;&#34917;&#29616;&#26377;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#24046;&#36317;&#20173;&#19981;&#30830;&#23450;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#26465;&#20214;&#21270;&#23545;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;SGD&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20840;&#38754;&#27604;&#36739;&#20102;&#39044;&#26465;&#20214;&#21270;SGD&#19982;&#65288;&#26631;&#20934;&#21644;&#39044;&#26465;&#20214;&#21270;&#65289;&#23725;&#22238;&#24402;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20102;&#35299;&#21644;&#25913;&#21892;SGD&#20570;&#20986;&#20102;&#20960;&#39033;&#20851;&#38190;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08585v1 Announce Type: new  Abstract: Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \&amp; preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#28436;&#31034;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#37197;&#32622;&#30340;&#27169;&#31946;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10189</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#28436;&#31034;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#37197;&#32622;&#30340;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#28436;&#31034;&#26469;&#24443;&#24213;&#25913;&#21464;&#20102;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;LLM&#21709;&#24212;&#20013;&#30340;&#21487;&#20449;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#65292;&#20063;&#34987;&#31215;&#26497;&#35752;&#35770;&#12290;&#29616;&#26377;&#24037;&#20316;&#33268;&#21147;&#20110;&#37327;&#21270;LLM&#21709;&#24212;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;LLM&#30340;&#22797;&#26434;&#24615;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29420;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#20851;&#30340;LLM&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#26469;&#33258;&#20110;&#25552;&#20379;&#30340;&#28436;&#31034;&#65288;aleatoric&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#19982;&#27169;&#22411;&#37197;&#32622;&#30456;&#20851;&#30340;&#27169;&#31946;&#24615;&#65288;epistemic&#19981;&#30830;&#23450;&#24615;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#21644;&#30456;&#24212;&#30340;&#20272;&#35745;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20026;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#37324;&#30340;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10189v1 Announce Type: new  Abstract: In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-cont
&lt;/p&gt;</description></item><item><title>"Lens"&#26159;&#19968;&#20010;&#22522;&#20110;T5&#26550;&#26500;&#30340;&#22522;&#30784;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#27969;&#37327;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.03646</link><description>&lt;p&gt;
Lens: &#32593;&#32476;&#27969;&#37327;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lens: A Foundation Model for Network Traffic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03646
&lt;/p&gt;
&lt;p&gt;
"Lens"&#26159;&#19968;&#20010;&#22522;&#20110;T5&#26550;&#26500;&#30340;&#22522;&#30784;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#27969;&#37327;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#37327;&#26159;&#25351;&#36890;&#36807;&#20114;&#32852;&#32593;&#25110;&#36830;&#25509;&#35745;&#31639;&#26426;&#30340;&#20219;&#20309;&#31995;&#32479;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#20449;&#24687;&#37327;&#12290;&#20998;&#26512;&#21644;&#29702;&#35299;&#32593;&#32476;&#27969;&#37327;&#23545;&#20110;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#21253;&#30340;&#29305;&#27530;&#29305;&#24615;&#65292;&#22914;&#24322;&#26500;&#26631;&#22836;&#21644;&#32570;&#20047;&#35821;&#20041;&#30340;&#21152;&#23494;&#36127;&#36733;&#65292;&#32593;&#32476;&#27969;&#37327;&#30340;&#20998;&#26512;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25429;&#25417;&#27969;&#37327;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#19968;&#20123;&#30740;&#31350;&#37319;&#29992;&#20102;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#25110;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#20174;&#22823;&#35268;&#27169;&#30340;&#27969;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21482;&#22312;&#27969;&#37327;&#29702;&#35299;&#65288;&#20998;&#31867;&#65289;&#25110;&#27969;&#37327;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Lens&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#21033;&#29992;T5&#26550;&#26500;&#20174;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#39044;&#35757;&#32451;&#34920;&#31034;&#12290;&#20511;&#21161;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#23454;&#29616;&#31934;&#30830;&#30340;&#27969;&#37327;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the glob
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02977</link><description>&lt;p&gt;
&#21464;&#20998;&#27969;&#27169;&#22411;&#65306;&#20197;&#20320;&#30340;&#39118;&#26684;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Variational Flow Models: Flowing in Your Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;"&#21518;&#39564;&#27969;"&#27169;&#22411;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#35299;&#37322;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20197;&#23558;"&#27010;&#29575;&#27969;"&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#38543;&#26426;&#36807;&#31243;&#31867;&#21035;&#65292;&#19981;&#24517;&#23616;&#38480;&#20110;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32467;&#26524;&#31216;&#20026;"&#21464;&#20998;&#27969;&#27169;&#22411;"&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#30001;&#26041;&#31243;Xt = at * X0 + st * X1&#25152;&#25551;&#36848;&#30340;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#36716;&#21270;&#20026;&#30452;&#32447;&#24658;&#36895;(SC)&#27969;&#65292;&#31867;&#20284;&#20110;&#30699;&#27491;&#27969;&#12290;&#36825;&#31181;&#36716;&#21270;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#27839;&#30528;&#21407;&#22987;&#30340;&#21518;&#39564;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;SC&#27969;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36716;&#25442;&#25193;&#23637;&#21040;&#20004;&#20010;&#19981;&#21516;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#36716;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23558;&#39640;&#38454;&#25968;&#20540;&#35299;&#27861;&#36731;&#26494;&#38598;&#25104;&#21040;&#36716;&#25442;&#21518;&#30340;SC&#27969;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
&lt;/p&gt;</description></item><item><title>CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;</title><link>https://arxiv.org/abs/2402.00786</link><description>&lt;p&gt;
CroissantLLM: &#19968;&#20010;&#30495;&#27491;&#30340;&#21452;&#35821;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CroissantLLM: A Truly Bilingual French-English Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00786
&lt;/p&gt;
&lt;p&gt;
CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CroissantLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;3T&#20010;&#33521;&#35821;&#21644;&#27861;&#35821;&#26631;&#35760;&#19978;&#39044;&#35757;&#32451;&#30340;13&#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#30740;&#31350;&#21644;&#24037;&#19994;&#31038;&#21306;&#24102;&#26469;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#12289;&#23436;&#20840;&#24320;&#28304;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#26412;&#22320;&#30828;&#20214;&#19978;&#24555;&#36895;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#31181;&#20869;&#22312;&#21452;&#35821;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#27861;&#35821;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#24037;&#31574;&#21010;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; FrenchBench&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#22312;&#27861;&#35821;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#36879;&#26126;&#24230;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20195;&#30721;&#24211;&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#20960;&#21313;&#20010;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;DXAI&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#31867;&#21035;&#19981;&#21487;&#30693;&#21644;&#31867;&#21035;&#26126;&#26174;&#37096;&#20998;&#26469;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#65292;&#20026;&#35299;&#37322;&#20998;&#31867;&#25552;&#20379;&#20102;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#24335;</title><link>https://arxiv.org/abs/2401.00320</link><description>&lt;p&gt;
DXAI&#65306;&#36890;&#36807;&#22270;&#20687;&#20998;&#35299;&#35299;&#37322;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DXAI: Explaining Classification by Image Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00320
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;DXAI&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#31867;&#21035;&#19981;&#21487;&#30693;&#21644;&#31867;&#21035;&#26126;&#26174;&#37096;&#20998;&#26469;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#65292;&#20026;&#35299;&#37322;&#20998;&#31867;&#25552;&#20379;&#20102;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#20998;&#35299;&#30340;&#21487;&#35299;&#37322;AI&#65288;DXAI&#65289;&#26469;&#35299;&#37322;&#21644;&#21487;&#35270;&#21270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26159;&#25552;&#20379;&#35299;&#37322;&#28909;&#22270;&#65292;&#32780;&#26159;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#19982;&#25968;&#25454;&#21644;&#25152;&#36873;&#20998;&#31867;&#22120;&#30456;&#20851;&#30340;&#31867;&#21035;&#19981;&#21487;&#30693;&#21644;&#31867;&#21035;&#26126;&#26174;&#37096;&#20998;&#12290;&#36981;&#24490;&#20998;&#26512;&#21644;&#32508;&#21512;&#30340;&#22522;&#26412;&#20449;&#21495;&#22788;&#29702;&#33539;&#24335;&#65292;&#21407;&#22987;&#22270;&#20687;&#26159;&#20998;&#35299;&#37096;&#20998;&#30340;&#24635;&#21644;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#35299;&#37322;&#20998;&#31867;&#26041;&#24335;&#12290;&#31867;&#21035;&#19981;&#21487;&#30693;&#37096;&#20998;&#29702;&#24819;&#22320;&#30001;&#19981;&#20855;&#26377;&#31867;&#21035;&#20449;&#24687;&#30340;&#25152;&#26377;&#22270;&#20687;&#29305;&#24449;&#32452;&#25104;&#65292;&#32780;&#31867;&#21035;&#26126;&#26174;&#37096;&#20998;&#21017;&#26159;&#20854;&#34917;&#20805;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#24403;&#23646;&#24615;&#23494;&#38598;&#12289;&#20840;&#23616;&#19988;&#20855;&#26377;&#32047;&#31215;&#24615;&#36136;&#26102;&#65292;&#27492;&#26032;&#21487;&#35270;&#21270;&#21487;&#33021;&#26356;&#26377;&#24110;&#21161;&#21644;&#20449;&#24687;&#24615;&#65292;&#20363;&#22914;&#65292;&#24403;&#39068;&#33394;&#25110;&#32441;&#29702;&#23545;&#20110;&#31867;&#21035;&#21306;&#20998;&#33267;&#20851;&#37325;&#35201;&#26102;&#12290;&#20195;&#30721;&#21487;&#22312;https://gith&#19978;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00320v2 Announce Type: replace-cross  Abstract: We propose a new way to explain and to visualize neural network classification through a decomposition-based explainable AI (DXAI). Instead of providing an explanation heatmap, our method yields a decomposition of the image into class-agnostic and class-distinct parts, with respect to the data and chosen classifier. Following a fundamental signal processing paradigm of analysis and synthesis, the original image is the sum of the decomposed parts. We thus obtain a radically different way of explaining classification. The class-agnostic part ideally is composed of all image features which do not posses class information, where the class-distinct part is its complementary. This new visualization can be more helpful and informative in certain scenarios, especially when the attributes are dense, global and additive in nature, for instance, when colors or textures are essential for class distinction. Code is available at https://gith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36138;&#23146;&#31639;&#27861;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33218;&#29305;&#24449;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#36866;&#29992;&#31867;&#21035;&#21644;&#28151;&#21512;&#20998;&#24067;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2312.12400</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#20013;&#36138;&#23146;&#36866;&#29992;&#33218;&#29305;&#24449;&#20998;&#24067;&#30340;&#26032;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
New Classes of the Greedy-Applicable Arm Feature Distributions in the Sparse Linear Bandit Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36138;&#23146;&#31639;&#27861;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33218;&#29305;&#24449;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#36866;&#29992;&#31867;&#21035;&#21644;&#28151;&#21512;&#20998;&#24067;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#31232;&#30095;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#33218;&#29305;&#24449;&#36890;&#36807;&#31232;&#30095;&#21442;&#25968;&#30340;&#20869;&#31215;&#24433;&#21709;&#22870;&#21169;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#36138;&#23146;&#33218;&#36873;&#25321;&#31574;&#30053;&#30340;&#19981;&#32771;&#34385;&#31232;&#30095;&#24615;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#20998;&#26512;&#38656;&#35201;&#23545;&#33218;&#29305;&#24449;&#20998;&#24067;&#20570;&#20986;&#24378;&#20551;&#35774;&#65292;&#20197;&#30830;&#20445;&#36138;&#23146;&#36873;&#25321;&#30340;&#26679;&#26412;&#36275;&#22815;&#22810;&#26679;&#21270;&#65307;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#20551;&#35774;&#20043;&#19968;&#26159;&#25918;&#26494;&#23545;&#31216;&#24615;&#65292;&#23545;&#20998;&#24067;&#26045;&#21152;&#20102;&#36817;&#20284;&#21407;&#28857;&#23545;&#31216;&#24615;&#65292;&#36825;&#19981;&#20801;&#35768;&#20855;&#26377;&#21407;&#28857;&#19981;&#23545;&#31216;&#25903;&#25345;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36138;&#23146;&#31639;&#27861;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33218;&#29305;&#24449;&#20998;&#24067;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;&#20855;&#26377;&#19968;&#20010;&#36138;&#23146;&#36866;&#29992;&#32452;&#20214;&#30340;&#28151;&#21512;&#20998;&#24067;&#20063;&#26159;&#36138;&#23146;&#36866;&#29992;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#24067;&#31867;&#21035;&#65292;&#19982;&#39640;&#26031;&#28151;&#21512;&#12289;&#31163;&#25955;&#21644;&#24452;&#21521;&#20998;&#24067;&#30456;&#20851;&#65292;&#36866;&#29992;&#20110;&#35813;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12400v2 Announce Type: replace  Abstract: We consider the sparse contextual bandit problem where arm feature affects reward through the inner product of sparse parameters. Recent studies have developed sparsity-agnostic algorithms based on the greedy arm selection policy. However, the analysis of these algorithms requires strong assumptions on the arm feature distribution to ensure that the greedily selected samples are sufficiently diverse; One of the most common assumptions, relaxed symmetry, imposes approximate origin-symmetry on the distribution, which cannot allow distributions that has origin-asymmetric support. In this paper, we show that the greedy algorithm is applicable to a wider range of the arm feature distributions from two aspects. Firstly, we show that a mixture distribution that has a greedy-applicable component is also greedy-applicable. Second, we propose new distribution classes, related to Gaussian mixture, discrete, and radial distribution, for which th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21943;&#27922;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#21160;&#24577;&#30340;&#21270;&#36523;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#26684;&#25110;NeRF&#34920;&#31034;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#21407;&#22987;&#30340;3D&#39640;&#26031;&#34920;&#31034;&#21644;&#31070;&#32463;&#38544;&#24335;&#22330;&#26469;&#31283;&#23450;&#21644;&#25913;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2312.11461</link><description>&lt;p&gt;
GAvatar&#65306;&#20855;&#26377;&#38544;&#24335;&#32593;&#26684;&#23398;&#20064;&#30340;&#21487;&#21160;&#24577;&#19977;&#32500;&#39640;&#26031;&#21270;&#36523;
&lt;/p&gt;
&lt;p&gt;
GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21943;&#27922;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#21160;&#24577;&#30340;&#21270;&#36523;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#26684;&#25110;NeRF&#34920;&#31034;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#21407;&#22987;&#30340;3D&#39640;&#26031;&#34920;&#31034;&#21644;&#31070;&#32463;&#38544;&#24335;&#22330;&#26469;&#31283;&#23450;&#21644;&#25913;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#21943;&#27922;&#24050;&#32463;&#24418;&#25104;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;3D&#34920;&#31034;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#26174;&#24335;&#65288;&#32593;&#26684;&#65289;&#21644;&#38544;&#24335;&#65288;NeRF&#65289;3D&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#39640;&#26031;&#21943;&#27922;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#21160;&#24577;&#30340;&#21270;&#36523;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#26684;&#25110;NeRF&#34920;&#31034;&#25152;&#26045;&#21152;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#31561;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#24212;&#29992;&#39640;&#26031;&#21943;&#27922;&#19981;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21487;&#21160;&#24577;&#21270;&#36523;&#65292;&#23481;&#26131;&#20135;&#29983;&#23398;&#20064;&#19981;&#31283;&#23450;&#24615;&#65307;&#23427;&#20063;&#19981;&#33021;&#25429;&#25417;&#21040;&#31934;&#32454;&#30340;&#21270;&#36523;&#20960;&#20309;&#24418;&#29366;&#24182;&#32463;&#24120;&#23548;&#33268;&#36864;&#21270;&#30340;&#36523;&#20307;&#37096;&#20998;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#22522;&#20110;&#21407;&#22987;&#30340;3D&#39640;&#26031;&#34920;&#31034;&#65292;&#20854;&#20013;&#39640;&#26031;&#23450;&#20041;&#22312;&#23039;&#21183;&#39537;&#21160;&#30340;&#22522;&#26412;&#20803;&#32032;&#20869;&#20197;&#20415;&#20419;&#36827;&#21160;&#30011;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#31283;&#23450;&#21644;&#25674;&#38144;&#25968;&#30334;&#19975;&#39640;&#26031;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#38544;&#24335;&#22330;&#26469;&#39044;&#27979;&#39640;&#26031;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11461v2 Announce Type: replace-cross  Abstract: Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27969;&#24335;ASR&#31995;&#32479;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#26550;&#26500;&#36866;&#37197;&#12289;&#31070;&#32463;&#32593;&#32476;&#22270;&#21464;&#25442;&#21644;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5.26&#20493;&#23454;&#26102;&#36895;&#24230;&#30340;&#35821;&#38899;&#35782;&#21035;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#20943;&#23569;&#33021;&#32791;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.10359</link><description>&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#26497;&#31471;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#19978;&#30340;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Conformer-Based Speech Recognition On Extreme Edge-Computing Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27969;&#24335;ASR&#31995;&#32479;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#26550;&#26500;&#36866;&#37197;&#12289;&#31070;&#32463;&#32593;&#32476;&#22270;&#21464;&#25442;&#21644;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5.26&#20493;&#23454;&#26102;&#36895;&#24230;&#30340;&#35821;&#38899;&#35782;&#21035;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#20943;&#23569;&#33021;&#32791;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20170;&#22825;&#35774;&#22791;&#20013;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#36164;&#28304;&#65292;&#20256;&#32479;&#19978;&#22312;&#20113;&#31471;&#25191;&#34892;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#27491;&#20174;&#20113;&#31471;&#36716;&#31227;&#21040;&#35774;&#22791;&#19978;&#20197;&#26356;&#22909;&#22320;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;&#26412;&#22320;ASR&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20363;&#22914;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#20854;&#20182;&#23567;&#22411;&#23478;&#23621;&#33258;&#21160;&#21270;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#26550;&#26500;&#35843;&#25972;&#12289;&#31070;&#32463;&#32593;&#32476;&#22270;&#21464;&#25442;&#21644;&#25968;&#20540;&#20248;&#21270;&#65292;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#36866;&#37197;&#20808;&#36827;&#30340;&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27969;&#24335;ASR&#31995;&#32479;&#65292;&#32780;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#23567;&#22411;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;&#23454;&#26102;5.26&#20493;&#24555;&#65288;0.19 RTF&#65289;&#30340;&#35821;&#38899;&#35782;&#21035;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#33021;&#32791;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24191;&#27867;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26080;&#26381;&#21153;&#22120;AI&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10359v2 Announce Type: replace  Abstract: With increasingly more powerful compute capabilities and resources in today's devices, traditionally compute-intensive automatic speech recognition (ASR) has been moving from the cloud to devices to better protect user privacy. However, it is still challenging to implement on-device ASR on resource-constrained devices, such as smartphones, smart wearables, and other small home automation devices. In this paper, we propose a series of model architecture adaptions, neural network graph transformations, and numerical optimizations to fit an advanced Conformer based end-to-end streaming ASR system on resource-constrained devices without accuracy degradation. We achieve over 5.26 times faster than realtime (0.19 RTF) speech recognition on small wearables while minimizing energy consumption and achieving state-of-the-art accuracy. The proposed methods are widely applicable to other transformer-based server-free AI applications. In addition
&lt;/p&gt;</description></item><item><title>&#32456;&#36523;&#35760;&#24518;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#21644;&#26816;&#32034;&#26041;&#24335;&#35775;&#38382;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65292;&#21033;&#29992;&#38646;-shot&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#65292;&#20351;&#29992;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#27169;&#22359;&#20135;&#29983;&#33258;&#20449;&#12289;&#39640;&#36136;&#37327;&#21644;&#21487;&#35299;&#37322;&#30340;&#31572;&#26696;&#65292;&#22312;EgoSchema&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#22312;Ego4D&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;</title><link>https://arxiv.org/abs/2312.05269</link><description>&lt;p&gt;
&#32456;&#36523;&#35760;&#24518;&#65306;&#21033;&#29992;LLMs&#22238;&#31572;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#20013;&#30340;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05269
&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#35760;&#24518;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#21644;&#26816;&#32034;&#26041;&#24335;&#35775;&#38382;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65292;&#21033;&#29992;&#38646;-shot&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#65292;&#20351;&#29992;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#27169;&#22359;&#20135;&#29983;&#33258;&#20449;&#12289;&#39640;&#36136;&#37327;&#21644;&#21487;&#35299;&#37322;&#30340;&#31572;&#26696;&#65292;&#22312;EgoSchema&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#22312;Ego4D&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32456;&#36523;&#35760;&#24518;(LifelongMemory)&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#21644;&#26816;&#32034;&#26469;&#35775;&#38382;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#23384;&#20648;&#12290;&#32456;&#36523;&#35760;&#24518;&#29983;&#25104;&#25668;&#20687;&#26426;&#20329;&#25140;&#32773;&#30340;&#31616;&#27905;&#35270;&#39057;&#27963;&#21160;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25512;&#29702;&#38271;&#31687;&#35270;&#39057;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#32456;&#36523;&#35760;&#24518;&#20351;&#29992;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#27169;&#22359;&#26469;&#20135;&#29983;&#33258;&#20449;&#12289;&#39640;&#36136;&#37327;&#21644;&#21487;&#35299;&#37322;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;EgoSchema&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;Ego4D&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;(NLQ)&#25361;&#25112;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Agentic-Learning-AI-Lab/lifelong-memory &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05269v2 Announce Type: replace-cross  Abstract: In this paper we introduce LifelongMemory, a new framework for accessing long-form egocentric videographic memory through natural language question answering and retrieval. LifelongMemory generates concise video activity descriptions of the camera wearer and leverages the zero-shot capabilities of pretrained large language models to perform reasoning over long-form video context. Furthermore, Lifelong Memory uses a confidence and explanation module to produce confident, high-quality, and interpretable answers. Our approach achieves state-of-the-art performance on the EgoSchema benchmark for question answering and is highly competitive on the natural language query (NLQ) challenge of Ego4D. Code is available at https://github.com/Agentic-Learning-AI-Lab/lifelong-memory.
&lt;/p&gt;</description></item><item><title>&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;&#65288;RMA&#65289;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#27867;&#21270;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#28145;&#24230;&#24863;&#30693;&#24320;&#21457;&#20102;&#38024;&#23545;&#21508;&#31181;&#25805;&#32437;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2312.04670</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#32437;&#33218;&#30340;&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rapid Motor Adaptation for Robotic Manipulator Arms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04670
&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;&#65288;RMA&#65289;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#27867;&#21270;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#28145;&#24230;&#24863;&#30693;&#24320;&#21457;&#20102;&#38024;&#23545;&#21508;&#31181;&#25805;&#32437;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#26159;&#20307;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#36825;&#21253;&#25324;&#36328;&#36234;&#21508;&#31181;&#20219;&#21153;&#37197;&#32622;&#30340;&#27867;&#21270;&#65292;&#28085;&#30422;&#20102;&#23545;&#35937;&#24418;&#29366;&#12289;&#23494;&#24230;&#12289;&#25705;&#25830;&#31995;&#25968;&#30340;&#21464;&#21270;&#20197;&#21450;&#22806;&#37096;&#24178;&#25200;&#65292;&#22914;&#26045;&#21152;&#22312;&#26426;&#22120;&#20154;&#36523;&#19978;&#30340;&#21147;&#12290;&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;&#65288;Rapid Motor Adaptation&#65292;RMA&#65289;&#20026;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#35748;&#20026;&#65292;&#24433;&#21709;&#26234;&#33021;&#20307;&#20219;&#21153;&#34920;&#29616;&#30340;&#22522;&#26412;&#38544;&#21464;&#37327;&#65292;&#22914;&#23545;&#35937;&#36136;&#37327;&#21644;&#24418;&#29366;&#65292;&#21487;&#20197;&#20174;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#21644;&#26412;&#20307;&#24863;&#24615;&#21382;&#21490;&#20013;&#26377;&#25928;&#22320;&#25512;&#26029;&#20986;&#26469;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#22312;&#31227;&#21160;&#21644;&#25163;&#20869;&#26059;&#36716;&#20013;&#30340;RMA&#65292;&#21033;&#29992;&#28145;&#24230;&#24863;&#30693;&#26469;&#24320;&#21457;&#38024;&#23545;&#21508;&#31181;&#25805;&#32437;&#20219;&#21153;&#30340;&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;Maniskill2&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#25105;&#20204;&#30340;&#20195;&#29702;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#20174;YCB&#21644;EGAD&#25968;&#25454;&#38598;&#20013;&#25968;&#30334;&#20010;&#23545;&#35937;&#30340;&#21462;&#25918;&#25805;&#20316;&#65292;&#20197;&#21450;&#31934;&#30830;&#23450;&#20301;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04670v2 Announce Type: replace-cross  Abstract: Developing generalizable manipulation skills is a core challenge in embodied AI. This includes generalization across diverse task configurations, encompassing variations in object shape, density, friction coefficient, and external disturbances such as forces applied to the robot. Rapid Motor Adaptation (RMA) offers a promising solution to this challenge. It posits that essential hidden variables influencing an agent's task performance, such as object mass and shape, can be effectively inferred from the agent's action and proprioceptive history. Drawing inspiration from RMA in locomotion and in-hand rotation, we use depth perception to develop agents tailored for rapid motor adaptation in a variety of manipulation tasks. We evaluated our agents on four challenging tasks from the Maniskill2 benchmark, namely pick-and-place operations with hundreds of objects from the YCB and EGAD datasets, peg insertion with precise position and 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#20013;&#36866;&#24212;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#22806;&#31185;&#25163;&#26415;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2311.17693</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22806;&#31185;&#21307;&#29983;&#21442;&#19982;&#30524;&#31185;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17693
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#20013;&#36866;&#24212;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#22806;&#31185;&#25163;&#26415;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#31995;&#32479;&#22312;&#25552;&#39640;&#25163;&#26415;&#31934;&#30830;&#24230;&#21644;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#32570;&#20047;&#36866;&#24212;&#20010;&#21035;&#22806;&#31185;&#21307;&#29983;&#30340;&#29420;&#29305;&#20559;&#22909;&#21644;&#35201;&#27714;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#26222;&#36890;&#25163;&#26415;&#65288;&#22914;&#33145;&#33108;&#38236;&#25163;&#26415;&#65289;&#65292;&#19981;&#36866;&#29992;&#20110;&#38750;&#24120;&#31934;&#23494;&#30340;&#24494;&#21019;&#25163;&#26415;&#65292;&#22914;&#30524;&#31185;&#25163;&#26415;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#21487;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#36807;&#31243;&#20013;&#36866;&#24212;&#20010;&#21035;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#39318;&#36873;&#22806;&#31185;&#25163;&#26415;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#27169;&#25311;&#29615;&#22659;&#26469;&#35757;&#32451;&#20197;&#22270;&#20687;&#25968;&#25454;&#20026;&#25351;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#65292;&#20197;&#25191;&#34892;&#30333;&#20869;&#38556;&#25163;&#26415;&#30340;&#20999;&#21475;&#38454;&#27573;&#25152;&#26377;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22806;&#31185;&#21307;&#29983;&#30340;&#21160;&#20316;&#21644;&#20559;&#22909;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35753;&#22806;&#31185;&#21307;&#29983;&#21442;&#19982;&#20854;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17693v2 Announce Type: replace-cross  Abstract: Robotic-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems lack the ability to accommodate the unique preferences and requirements of individual surgeons. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are not suitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose a simulation-based image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon's skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach utilizes a simulated environment to train reinforcement and imitation learning agents guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon's actions and preferences into the training process with the surgeon-in-the-loop, our ap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.17076</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32452;&#21512;&#24335;&#24605;&#32500;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compositional Chain-of-Thought Prompting for Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#35270;&#35273;&#39592;&#24178;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#29702;&#30340;&#32467;&#21512;&#24050;&#32463;&#23548;&#33268;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#25104;&#20026;&#24403;&#21069;&#24191;&#27867;&#35270;&#35273;&#21644;&#35821;&#35328;(VL)&#20219;&#21153;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LMM&#20173;&#28982;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#65292;&#27604;&#22914;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22330;&#26223;&#22270;(SGs)&#8212;&#8212;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#21644;&#23646;&#24615;&#30340;&#24418;&#24335;&#21270;&#34920;&#36798;&#65292;&#23427;&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#28982;&#32780;&#65292;&#22330;&#26223;&#22270;&#25968;&#25454;&#38656;&#35201;&#22330;&#26223;&#22270;&#27880;&#37322;&#65292;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#38590;&#20197;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22330;&#26223;&#22270;&#25968;&#25454;&#24494;&#35843;LMM&#21487;&#33021;&#23548;&#33268;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21463;&#21040;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;Shift-NMF&#21644;Nearly-NMF&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#36127;&#20540;&#30340;&#22024;&#26434;&#25968;&#25454;&#65292;&#22312;&#19981;&#24341;&#20837;&#27491;&#30340;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#24674;&#22797;&#38750;&#36127;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.04855</link><description>&lt;p&gt;
&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#36127;&#20540;&#30340;&#22024;&#26434;&#25968;&#25454;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;Shift-NMF&#21644;Nearly-NMF&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#36127;&#20540;&#30340;&#22024;&#26434;&#25968;&#25454;&#65292;&#22312;&#19981;&#24341;&#20837;&#27491;&#30340;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#24674;&#22797;&#38750;&#36127;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#38477;&#32500;&#25216;&#26415;&#65292;&#22312;&#20998;&#26512;&#22024;&#26434;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22825;&#25991;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#30001;&#20110;&#22122;&#22768;&#65292;&#35266;&#27979;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#36127;&#20540;&#65292;&#21363;&#20351;&#30495;&#23454;&#30340;&#29289;&#29702;&#20449;&#21495;&#20005;&#26684;&#20026;&#27491;&#12290;&#20197;&#24448;&#30340;NMF&#24037;&#20316;&#26410;&#20197;&#32479;&#35745;&#19968;&#33268;&#30340;&#26041;&#24335;&#22788;&#29702;&#36127;&#25968;&#25454;&#65292;&#36825;&#22312;&#20302;&#20449;&#22122;&#27604;&#25968;&#25454;&#20013;&#20986;&#29616;&#35768;&#22810;&#36127;&#20540;&#26102;&#20250;&#21464;&#24471;&#26840;&#25163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;Shift-NMF&#21644;Nearly-NMF&#65292;&#21487;&#20197;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#22024;&#26434;&#24615;&#65292;&#24182;&#28040;&#38500;&#20219;&#20309;&#24341;&#20837;&#30340;&#36127;&#20540;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20351;&#29992;&#36127;&#25968;&#25454;&#31354;&#38388;&#32780;&#19981;&#36827;&#34892;&#25130;&#21462;&#65292;&#24182;&#19988;&#22312;&#28040;&#38500;&#36127;&#25968;&#25454;&#26102;&#19981;&#20250;&#24341;&#20837;&#27491;&#30340;&#20559;&#31227;&#37327;&#12290;&#25105;&#20204;&#22312;&#31616;&#21333;&#21644;&#26356;&#29616;&#23454;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#28436;&#31034;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#20855;&#26377;&#21333;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04855v2 Announce Type: replace-cross  Abstract: Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotoni
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#28151;&#21512;&#37327;&#23376;&#22270;&#20687;&#20998;&#31867;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#32925;&#33026;&#32938;&#21464;&#24615;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#22240;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.02402</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;&#22270;&#20687;&#20998;&#31867;&#21644;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#32925;&#33026;&#32938;&#21464;&#24615;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Hybrid quantum image classification and federated learning for hepatic steatosis diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#28151;&#21512;&#37327;&#23376;&#22270;&#20687;&#20998;&#31867;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#32925;&#33026;&#32938;&#21464;&#24615;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#22240;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32925;&#31227;&#26893;&#39046;&#22495;&#20013;&#65292;&#20934;&#30830;&#30830;&#23450;&#32925;&#33026;&#32938;&#21464;&#24615;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#25552;&#39640;&#35786;&#26029;&#31934;&#24230;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#36805;&#36895;&#22788;&#29702;&#26131;&#20110;&#35299;&#20915;&#30340;&#30149;&#20363;&#65292;&#35753;&#19987;&#23478;&#26377;&#26356;&#22810;&#26102;&#38388;&#20851;&#27880;&#22797;&#26434;&#30149;&#20363;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#20248;&#21270;&#32925;&#27963;&#26816;&#22270;&#20687;&#20998;&#31867;&#30340;&#23574;&#31471;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#24403;&#21019;&#24314;&#33258;&#21160;&#21270;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21464;&#25104;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#21307;&#38498;&#20043;&#38388;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#21463;&#21040;&#38480;&#21046;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#24320;&#21457;&#21644;&#39564;&#35777;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#20511;&#21161;&#20110;&#20174;&#24555;&#36895;&#21457;&#23637;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24341;&#20837;&#30340;&#26032;&#25216;&#26415;&#26469;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#25216;&#26415;&#20197;&#20854;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23454;&#26045;&#38544;&#31169;&#24847;&#35782;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65292;&#36824;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02402v2 Announce Type: replace  Abstract: In the realm of liver transplantation, accurately determining hepatic steatosis levels is crucial. Recognizing the essential need for improved diagnostic precision, particularly for optimizing diagnosis time by swiftly handling easy-to-solve cases and allowing the expert time to focus on more complex cases, this study aims to develop cutting-edge algorithms that enhance the classification of liver biopsy images. Additionally, the challenge of maintaining data privacy arises when creating automated algorithmic solutions, as sharing patient data between hospitals is restricted, further complicating the development and validation process. This research tackles diagnostic accuracy by leveraging novel techniques from the rapidly evolving field of quantum machine learning, known for their superior generalization abilities. Concurrently, it addresses privacy concerns through the implementation of privacy-conscious collaborative machine lear
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#22312;&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#35774;&#35745;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.05764</link><description>&lt;p&gt;
&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#30340;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#22312;&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#35774;&#35745;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#34507;&#30333;&#36136;&#21151;&#33021;&#38656;&#35201;&#19982;&#23567;&#20998;&#23376;&#32467;&#21512;&#65292;&#21253;&#25324;&#37238;&#20652;&#21270;&#12290;&#22240;&#27492;&#65292;&#20026;&#23567;&#20998;&#23376;&#35774;&#35745;&#32467;&#21512;&#21475;&#34955;&#20855;&#26377;&#20174;&#33647;&#29289;&#21512;&#25104;&#21040;&#33021;&#37327;&#23384;&#20648;&#31561;&#22810;&#31181;&#24433;&#21709;&#28145;&#36828;&#30340;&#24212;&#29992;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;HarmonicFlow&#65292;&#36825;&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20110;&#33258;&#35843;&#27969;&#21305;&#37197;&#30446;&#26631;&#30340;3D&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#32467;&#26500;&#29983;&#25104;&#36807;&#31243;&#12290;FlowSite&#23558;&#36825;&#31181;&#27969;&#27169;&#22411;&#25193;&#23637;&#21040;&#32852;&#21512;&#29983;&#25104;&#34507;&#30333;&#36136;&#21475;&#34955;&#30340;&#31163;&#25955;&#27531;&#22522;&#31867;&#22411;&#21644;&#20998;&#23376;&#30340;&#32467;&#21512;3D&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HarmonicFlow&#22312;&#21475;&#34955;&#32423;&#23545;&#25509;&#20013;&#22312;&#31616;&#21333;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#24179;&#22343;&#26679;&#26412;&#36136;&#37327;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#20511;&#21161;&#20110;&#36825;&#31181;&#32467;&#26500;&#24314;&#27169;&#65292;FlowSite&#35774;&#35745;&#30340;&#32467;&#21512;&#20301;&#28857;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05764v3 Announce Type: replace-cross  Abstract: A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon state-of-the-art generative processes for docking in simplicity, generality, and average sample quality in pocket-level docking. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Differentiable Frank-Wolfe Layer&#65288;DFWLayer&#65289;&#30340;&#21487;&#24494;&#23618;&#65292;&#36890;&#36807;&#25512;&#20986;Frank-Wolfe&#26041;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#33539;&#25968;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#35299;&#21644;&#26799;&#24230;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.10806</link><description>&lt;p&gt;
DFWLayer: &#21487;&#24494;&#21270;&#30340;Frank-Wolfe&#20248;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
DFWLayer: Differentiable Frank-Wolfe Optimization Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Differentiable Frank-Wolfe Layer&#65288;DFWLayer&#65289;&#30340;&#21487;&#24494;&#23618;&#65292;&#36890;&#36807;&#25512;&#20986;Frank-Wolfe&#26041;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#33539;&#25968;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#35299;&#21644;&#26799;&#24230;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;iable&#20248;&#21270;&#22240;&#20854;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#20986;Frank-Wolfe&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#23618;&#65292;&#21629;&#21517;&#20026;Differentiable Frank-Wolfe Layer&#65288;DFWLayer&#65289;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#25237;&#24433;&#21644;Hessian&#30697;&#38453;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#33879;&#21517;&#20248;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#22788;&#29702;&#20855;&#26377;&#33539;&#25968;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DFWLayer&#19981;&#20165;&#22312;&#35299;&#21644;&#26799;&#24230;&#31934;&#24230;&#19978;&#34920;&#29616;&#31454;&#20105;&#24615;&#65292;&#32780;&#19988;&#22987;&#32456;&#36981;&#23432;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10806v2 Announce Type: replace-cross  Abstract: Differentiable optimization has received a significant amount of attention due to its foundational role in the domain of machine learning based on neural networks. This paper proposes a differentiable layer, named Differentiable Frank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a well-known optimization algorithm which can solve constrained optimization problems without projections and Hessian matrix computations, thus leading to an efficient way of dealing with large-scale convex optimization problems with norm constraints. Experimental results demonstrate that the DFWLayer not only attains competitive accuracy in solutions and gradients but also consistently adheres to constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#31995;&#32479;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#21033;&#29992;&#29615;&#22659;&#21160;&#24577;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#20248;&#28857;</title><link>https://arxiv.org/abs/2303.01563</link><description>&lt;p&gt;
&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#21644;&#23433;&#20840;&#30340;&#31665;&#23376;&#25805;&#32437;&#65306;&#23637;&#31034;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#29289;&#29702;&#20808;&#39564;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Data-efficient, Explainable and Safe Box Manipulation: Illustrating the Advantages of Physical Priors in Model-Predictive Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#31995;&#32479;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#21033;&#29992;&#29615;&#22659;&#21160;&#24577;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;/&#25511;&#21046;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#32570;&#20047;&#25163;&#24037;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#38590;&#20197;&#35843;&#35797;/&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#31995;&#32479;&#20013;&#65292;&#29615;&#22659;&#36816;&#21160;&#23398;/&#21160;&#21147;&#23398;&#30340;&#20808;&#39564;&#30693;&#35782;&#26159;&#21487;&#29992;&#30340;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#25506;&#32034;&#38656;&#27714;&#65292;&#21516;&#26102;&#20419;&#36827;&#20195;&#29702;&#25152;&#20570;&#20915;&#31574;&#20197;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#23454;&#20307;&#26469;&#34920;&#36798;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#38416;&#26126;&#21644;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#31995;&#32479;&#27169;&#25311;&#20102;&#19968;&#20010;&#36733;&#33655;&#25805;&#32437;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312; MPC &#26694;&#26550;&#20013;&#21033;&#29992;&#20851;&#20110;&#29615;&#22659;&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#25913;&#21892;&#21487;&#35299;&#37322;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#23433;&#20840;&#12289;&#26356;&#39640;&#25928;&#30340;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.01563v2 Announce Type: replace-cross  Abstract: Model-based RL/control have gained significant traction in robotics. Yet, these approaches often remain data-inefficient and lack the explainability of hand-engineered solutions. This makes them difficult to debug/integrate in safety-critical settings. However, in many systems, prior knowledge of environment kinematics/dynamics is available. Incorporating such priors can help address the aforementioned problems by reducing problem complexity and the need for exploration, while also facilitating the expression of the decisions taken by the agent in terms of physically meaningful entities. Our aim with this paper is to illustrate and support this point of view via a case-study. We model a payload manipulation problem based on a real robotic system, and show that leveraging prior knowledge about the dynamics of the environment in an MPC framework can lead to improvements in explainability, safety and data-efficiency, leading to sa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20998;&#31867;&#27861;&#65292;&#27010;&#36848;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#35843;&#26597;&#20102;&#32467;&#26500;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;&#37325;&#35201;&#22270;&#31867;&#30340;&#25193;&#23637;&#65292;&#22914;3D&#20998;&#23376;&#22270;&#12290;</title><link>https://arxiv.org/abs/2302.04181</link><description>&lt;p&gt;
&#20851;&#27880;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Attending to Graph Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.04181
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20998;&#31867;&#27861;&#65292;&#27010;&#36848;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#35843;&#26597;&#20102;&#32467;&#26500;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;&#37325;&#35201;&#22270;&#31867;&#30340;&#25193;&#23637;&#65292;&#22914;3D&#20998;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#22270;&#24418;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#26367;&#20195;&#25216;&#26415;&#20986;&#29616;&#65292;&#20363;&#22914;&#65288;&#28040;&#24687;&#20256;&#36882;&#65289;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23427;&#20204;&#24050;&#32463;&#23637;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20363;&#22914;&#22312;&#20998;&#23376;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#65292;&#36890;&#24120;&#24402;&#22240;&#20110;&#20854;&#32469;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#28857;&#65292;&#22914;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20998;&#31867;&#27861;&#65292;&#20026;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#24102;&#26469;&#20102;&#19968;&#20123;&#31209;&#24207;&#12290;&#25105;&#20204;&#27010;&#36848;&#23427;&#20204;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#35843;&#26597;&#32467;&#26500;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#37325;&#35201;&#22270;&#31867;&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;3D&#20998;&#23376;&#22270;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#30740;&#31350;&#22270;&#36716;&#25442;&#22120;&#22914;&#20309;&#24674;&#22797;&#21508;&#31181;&#22270;&#23646;&#24615;&#65292;&#22914;&#20309;&#22788;&#29702;&#24322;&#24615;&#22270;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#38450;&#27490;&#36807;&#24230;&#21387;&#32553;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.04181v3 Announce Type: replace-cross  Abstract: Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as (message-passing) graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets, often attributed to their ability to circumvent graph neural networks' shortcomings, such as over-smoothing and over-squashing. Here, we derive a taxonomy of graph transformer architectures, bringing some order to this emerging field. We overview their theoretical properties, survey structural and positional encodings, and discuss extensions for important graph classes, e.g., 3D molecular graphs. Empirically, we probe how well graph transformers can recover various graph properties, how well they can deal with heterophilic graphs, and to what extent they prevent over-squashing. Further, we outline open challenges and research direction to stimulate future wo
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;3-WL&#22270;&#21516;&#26500;&#27979;&#35797;&#20110;&#28857;&#20113;&#30340;Gram&#30697;&#38453;&#65292;&#25110;&#32773;&#24212;&#29992;&#27431;&#20960;&#37324;&#24471;2-WL&#27979;&#35797;&#65292;&#22312;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#20869;&#23454;&#29616;&#23545;&#23436;&#20840;&#27431;&#20960;&#37324;&#24471;&#22270;&#30340;&#23436;&#20840;&#30830;&#23450;&#24615;</title><link>https://arxiv.org/abs/2301.13821</link><description>&lt;p&gt;
&#23436;&#20840;&#27431;&#20960;&#37324;&#24471;&#22270;&#30340;&#23436;&#20840;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Complete Neural Networks for Complete Euclidean Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13821
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;3-WL&#22270;&#21516;&#26500;&#27979;&#35797;&#20110;&#28857;&#20113;&#30340;Gram&#30697;&#38453;&#65292;&#25110;&#32773;&#24212;&#29992;&#27431;&#20960;&#37324;&#24471;2-WL&#27979;&#35797;&#65292;&#22312;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#20869;&#23454;&#29616;&#23545;&#23436;&#20840;&#27431;&#20960;&#37324;&#24471;&#22270;&#30340;&#23436;&#20840;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#23545;&#28857;&#20113;&#24314;&#27169;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22240;&#20026;&#23427;&#20204;&#23562;&#37325;&#25490;&#21015;&#21644;&#21018;&#24615;&#36816;&#21160;&#30340;&#33258;&#28982;&#19981;&#21464;&#24615;&#65292;&#20174;&#20998;&#23376;&#21160;&#21147;&#23398;&#21040;&#25512;&#33616;&#31995;&#32479;&#65292;&#20174;&#20960;&#20309;&#29616;&#35937;&#21040;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#26410;&#30693;&#20855;&#26377;&#22810;&#39033;&#24335;&#22797;&#26434;&#24615;&#30340;&#27169;&#22411;&#26159;&#23436;&#22791;&#30340;&#65292;&#21363;&#33021;&#22815;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#28857;&#20113;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#21487;&#20197;&#23558;&#28857;&#20113;&#23436;&#20840;&#30830;&#23450;&#65292;&#30452;&#21040;&#25490;&#21015;&#21644;&#21018;&#24615;&#36816;&#21160;&#65292;&#36890;&#36807;&#23558;3-WL&#22270;&#21516;&#26500;&#27979;&#35797;&#24212;&#29992;&#20110;&#28857;&#20113;&#30340;&#38598;&#20013;Gram&#30697;&#38453;&#26469;&#22635;&#34917;&#36825;&#19968;&#29702;&#35770;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;2-WL&#27979;&#35797;&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#23427;&#20063;&#36275;&#20197;&#23454;&#29616;&#23436;&#25972;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20013;&#31561;&#22823;&#23567;&#30340;&#27431;&#20960;&#37324;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#25105;&#20204;&#30340;&#23436;&#20840;&#27431;&#20960;&#37324;&#24471;WL&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#39640;&#24230;&#23545;&#31216;&#30340;&#28857;&#20113;&#19978;&#30340;&#20998;&#31163;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13821v3 Announce Type: replace  Abstract: Neural networks for point clouds, which respect their natural invariance to permutation and rigid motion, have enjoyed recent success in modeling geometric phenomena, from molecular dynamics to recommender systems. Yet, to date, no model with polynomial complexity is known to be complete, that is, able to distinguish between any pair of non-isomorphic point clouds. We fill this theoretical gap by showing that point clouds can be completely determined, up to permutation and rigid motion, by applying the 3-WL graph isomorphism test to the point cloud's centralized Gram matrix. Moreover, we formulate an Euclidean variant of the 2-WL test and show that it is also sufficient to achieve completeness. We then show how our complete Euclidean WL tests can be simulated by an Euclidean graph neural network of moderate size and demonstrate their separation capability on highly symmetrical point clouds.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#23376;&#33455;&#29255;&#26550;&#26500;&#19978;&#34920;&#29616;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#25104;&#26412;&#20989;&#25968;&#24448;&#24448;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#24179;&#22343;&#20540;&#12290;</title><link>https://arxiv.org/abs/2212.14426</link><description>&lt;p&gt;
&#38480;&#21046;&#22312;&#33455;&#29255;&#26550;&#26500;&#19978;&#20445;&#25345;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Restricting to the chip architecture maintains the quantum neural network accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.14426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#23376;&#33455;&#29255;&#26550;&#26500;&#19978;&#34920;&#29616;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#25104;&#26412;&#20989;&#25968;&#24448;&#24448;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#30340;&#26102;&#20195;&#65292;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQAs&#65289;&#20316;&#20026;&#26500;&#24314;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26174;&#33879;&#31574;&#30053;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;&#37327;&#23376;&#37096;&#20998;&#21644;&#32463;&#20856;&#37096;&#20998;&#12290;&#37327;&#23376;&#37096;&#20998;&#36890;&#36807;&#21442;&#25968;&#21270; $U$ &#26469;&#34920;&#24449;&#65292;&#36890;&#24120;&#30001;&#21508;&#31181;&#37327;&#23376;&#38376;&#30340;&#32452;&#21512;&#24471;&#21040;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32463;&#20856;&#37096;&#20998;&#28041;&#21450;&#19968;&#20010;&#20248;&#21270;&#22120;&#65292;&#35843;&#33410; $U$ &#30340;&#21442;&#25968;&#20197;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968; $C$&#12290;&#23613;&#31649;VQAs&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#27604;&#22914;&#30830;&#23450;&#26368;&#20339;&#38376;&#24207;&#21015;&#12289;&#35774;&#35745;&#39640;&#25928;&#30340;&#21442;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#36873;&#25321;&#21512;&#36866;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#20102;&#35299;&#37327;&#23376;&#33455;&#29255;&#26550;&#26500;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#25104;&#26412;&#20989;&#25968;&#20542;&#21521;&#20110;&#25910;&#25947;&#21040;&#19968;&#20010;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.14426v2 Announce Type: replace-cross  Abstract: In the era of noisy intermediate-scale quantum devices, variational quantum algorithms (VQAs) stand as a prominent strategy for constructing quantum machine learning models. These models comprise both a quantum and a classical component. The quantum facet is characterized by a parametrization $U$, typically derived from the composition of various quantum gates. On the other hand, the classical component involves an optimizer that adjusts the parameters of $U$ to minimize a cost function $C$. Despite the extensive applications of VQAs, several critical questions persist, such as determining the optimal gate sequence, devising efficient parameter optimization strategies, selecting appropriate cost functions, and understanding the influence of quantum chip architectures on the final results. This article aims to address the last question, emphasizing that, in general, the cost function tends to converge towards an average value as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#28176;&#36827;&#27491;&#24577;&#20998;&#24067;&#25915;&#20987;&#65288;MultiANDA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#20998;&#24067;&#26126;&#30830;&#21051;&#30011;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#30340;&#28176;&#36827;&#27491;&#24577;&#24615;&#36136;&#26469;&#36817;&#20284;&#25200;&#21160;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#38598;&#25104;&#31574;&#30053;&#20316;&#20026;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#26377;&#25928;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2209.11964</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#28176;&#36827;&#27491;&#24577;&#20998;&#24067;&#23398;&#20064;&#36827;&#34892;&#24378;&#21487;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.11964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#28176;&#36827;&#27491;&#24577;&#20998;&#24067;&#25915;&#20987;&#65288;MultiANDA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#20998;&#24067;&#26126;&#30830;&#21051;&#30011;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#30340;&#28176;&#36827;&#27491;&#24577;&#24615;&#36136;&#26469;&#36817;&#20284;&#25200;&#21160;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#38598;&#25104;&#31574;&#30053;&#20316;&#20026;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#26377;&#25928;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21487;&#36716;&#31227;&#23545;&#25239;&#26679;&#26412;&#23545;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#25915;&#20987;&#30340;&#24615;&#33021;&#36890;&#24120;&#23545;&#24102;&#26377;&#38480;&#20449;&#24687;&#30340;&#24494;&#23567;&#22270;&#20687;&#36716;&#25442;&#25935;&#24863;&#65292;&#20856;&#22411;&#22320;&#21482;&#26377;&#19968;&#20010;&#36755;&#20837;&#31034;&#20363;&#12289;&#20960;&#20010;&#30333;&#30418;&#28304;&#27169;&#22411;&#21644;&#26410;&#23450;&#20041;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#38459;&#30861;&#23427;&#20204;&#23545;&#26410;&#30693;&#26550;&#26500;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#28176;&#36827;&#27491;&#24577;&#20998;&#24067;&#25915;&#20987;&#65288;MultiANDA&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#26126;&#30830;&#22320;&#20174;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#20013;&#34920;&#24449;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#65288;SGA&#65289;&#30340;&#28176;&#36827;&#27491;&#24577;&#24615;&#36136;&#26469;&#36817;&#20284;&#28041;&#21450;&#25200;&#21160;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31574;&#30053;&#20316;&#20026;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#26377;&#25928;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.11964v2 Announce Type: replace  Abstract: Strong adversarial examples are crucial for evaluating and enhancing the robustness of deep neural networks. However, the performance of popular attacks is usually sensitive, for instance, to minor image transformations, stemming from limited information -- typically only one input example, a handful of white-box source models, and undefined defense strategies. Hence, the crafted adversarial examples are prone to overfit the source model, which hampers their transferability to unknown architectures. In this paper, we propose an approach named Multiple Asymptotically Normal Distribution Attacks (MultiANDA) which explicitly characterize adversarial perturbations from a learned distribution. Specifically, we approximate the posterior distribution over the perturbations by taking advantage of the asymptotic normality property of stochastic gradient ascent (SGA), then employ the deep ensemble strategy as an effective proxy for Bayesian ma
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#20219;&#21153;&#20998;&#24067;&#65292;&#20174;&#32780;&#23545;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#25152;&#38656;&#35757;&#32451;&#20219;&#21153;&#25968;&#25552;&#20986;&#20102;&#26032;&#30340;&#30028;&#38480;&#20998;&#26512;&#26041;&#27861;</title><link>https://arxiv.org/abs/2206.10716</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#20219;&#21153;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;--&#19968;&#31181;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.10716
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#20219;&#21153;&#20998;&#24067;&#65292;&#20174;&#32780;&#23545;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#25152;&#38656;&#35757;&#32451;&#20219;&#21153;&#25968;&#25552;&#20986;&#20102;&#26032;&#30340;&#30028;&#38480;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;(meta RL)&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#19968;&#32452;&#35757;&#32451;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#24555;&#36895;&#35299;&#20915;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#26032;&#20219;&#21153;&#26469;&#33258;&#30456;&#21516;&#30340;&#20219;&#21153;&#20998;&#24067;&#12290;&#20248;&#21270;&#30340;&#20803;RL&#31574;&#30053;&#65292;&#21363;&#36125;&#21494;&#26031;&#26368;&#20248;&#34892;&#20026;&#65292;&#26159;&#26126;&#30830;&#23450;&#20041;&#30340;&#65292;&#24182;&#20445;&#35777;&#26399;&#26395;&#19979;&#30456;&#23545;&#20110;&#20219;&#21153;&#20998;&#24067;&#30340;&#26368;&#20248;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25506;&#35752;&#30340;&#38382;&#39064;&#26159;&#38656;&#35201;&#22810;&#23569;&#20010;&#35757;&#32451;&#20219;&#21153;&#25165;&#33021;&#20445;&#35777;&#20197;&#39640;&#27010;&#29575;&#36817;&#20284;&#22320;&#33719;&#24471;&#26368;&#20248;&#34892;&#20026;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20026;&#27169;&#22411;&#26080;&#20851;&#35774;&#32622;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;PAC&#20998;&#26512;&#65292;&#20854;&#20013;&#20174;&#35757;&#32451;&#20219;&#21153;&#20013;&#23398;&#20064;&#20102;&#19968;&#31181;&#21382;&#21490;&#30456;&#20851;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#30452;&#25509;&#23398;&#20064;&#20219;&#21153;&#20998;&#24067;&#65292;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#65292;&#28982;&#21518;&#22312;&#23398;&#20064;&#30340;&#20219;&#21153;&#20998;&#24067;&#19978;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20381;&#36182;&#20110;&#20219;&#21153;&#20998;&#24067;&#32500;&#24230;&#30340;&#30028;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20219;&#21153;&#20998;&#24067;&#32500;&#24230;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.10716v2 Announce Type: replace-cross  Abstract: In meta reinforcement learning (meta RL), an agent learns from a set of training tasks how to quickly solve a new task, drawn from the same task distribution. The optimal meta RL policy, a.k.a. the Bayes-optimal behavior, is well defined, and guarantees optimal reward in expectation, taken with respect to the task distribution. The question we explore in this work is how many training tasks are required to guarantee approximately optimal behavior with high probability. Recent work provided the first such PAC analysis for a model-free setting, where a history-dependent policy was learned from the training tasks. In this work, we propose a different approach: directly learn the task distribution, using density estimation techniques, and then train a policy on the learned task distribution. We show that our approach leads to bounds that depend on the dimension of the task distribution. In particular, in settings where the task dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; QAGCN &#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;&#36827;&#34892;&#24863;&#30693;&#26469;&#23454;&#29616;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#65292;&#20174;&#32780;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#37319;&#29992;&#12290;</title><link>https://arxiv.org/abs/2206.01818</link><description>&lt;p&gt;
QAGCN&#65306;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.01818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; QAGCN &#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;&#36827;&#34892;&#24863;&#30693;&#26469;&#23454;&#29616;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#65292;&#20174;&#32780;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20851;&#31995;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#30001;&#22810;&#20010;&#20851;&#31995;&#32452;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#38271;&#26102;&#38388;&#25512;&#29702;&#38142;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#26126;&#26174;&#20351;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#36880;&#27493;&#26631;&#31614;&#20256;&#25773;&#30340;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27983;&#35272;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#23427;&#20204;&#30340;&#25512;&#29702;&#26426;&#21046;&#36890;&#24120;&#22797;&#26434;&#19988;&#38590;&#20197;&#23454;&#29616;&#25110;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#21487;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#23454;&#29616;&#22810;&#20851;&#31995;QA&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#26356;&#39640;&#25928;&#19988;&#26356;&#26131;&#20110;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; QAGCN -- &#19968;&#31181;&#22522;&#20110;&#38382;&#39064;&#24847;&#35782;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#21463;&#25511;&#38382;&#39064;&#30456;&#20851;&#20449;&#24687;&#20256;&#25773;&#30340;GCN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.01818v3 Announce Type: replace  Abstract: Multi-relation question answering (QA) is a challenging task, where given questions usually require long reasoning chains in KGs that consist of multiple relations. Recently, methods with explicit multi-step reasoning over KGs have been prominently used in this task and have demonstrated promising performance. Examples include methods that perform stepwise label propagation through KG triples and methods that navigate over KG triples based on reinforcement learning. A main weakness of these methods is that their reasoning mechanisms are usually complex and difficult to implement or train. In this paper, we argue that multi-relation QA can be achieved via end-to-end single-step implicit reasoning, which is simpler, more efficient, and easier to adopt. We propose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based method that includes a novel GCN architecture with controlled question-dependent message propagation for the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25910;&#38598;&#21508;&#31181;&#22270;&#20381;&#36182;&#24615;&#30340;&#38598;&#20013;&#30028;&#38480;&#65292;&#35813;&#30740;&#31350;&#25512;&#23548;&#20986;&#20102;&#29992;&#20110;&#22522;&#20110;&#22270;&#20381;&#36182;&#24615;&#25968;&#25454;&#23398;&#20064;&#30340;Rademacher&#22797;&#26434;&#24230;&#21644;&#31283;&#23450;&#24615;&#30340;&#23398;&#20064;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2203.13534</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20381;&#36182;&#24615;&#30340;&#23398;&#20064;&#27867;&#21270;&#30028;&#38480;&#65306;&#19968;&#20221;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generalization bounds for learning under graph-dependence: A survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.13534
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#21508;&#31181;&#22270;&#20381;&#36182;&#24615;&#30340;&#38598;&#20013;&#30028;&#38480;&#65292;&#35813;&#30740;&#31350;&#25512;&#23548;&#20986;&#20102;&#29992;&#20110;&#22522;&#20110;&#22270;&#20381;&#36182;&#24615;&#25968;&#25454;&#23398;&#20064;&#30340;Rademacher&#22797;&#26434;&#24230;&#21644;&#31283;&#23450;&#24615;&#30340;&#23398;&#20064;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20381;&#36182;&#20110;&#25968;&#25454;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#24448;&#24448;&#19981;&#25104;&#31435;&#12290;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23398;&#20064;&#22330;&#26223;&#20013;&#31034;&#20363;&#20043;&#38388;&#23384;&#22312;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#30001;&#20381;&#36182;&#22270;&#25551;&#36848;&#65292;&#36825;&#26159;&#27010;&#29575;&#35770;&#21644;&#32452;&#21512;&#25968;&#23398;&#20013;&#24120;&#29992;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#21508;&#31181;&#22270;&#20381;&#36182;&#24615;&#30340;&#38598;&#20013;&#30028;&#38480;&#65292;&#28982;&#21518;&#29992;&#23427;&#20204;&#26469;&#25512;&#23548;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#21644;&#31283;&#23450;&#24615;&#30340;&#23398;&#20064;&#27867;&#21270;&#30028;&#38480;&#65292;&#20197;&#36866;&#29992;&#20110;&#22522;&#20110;&#22270;&#20381;&#36182;&#24615;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#38469;&#23398;&#20064;&#20219;&#21153;&#26469;&#35828;&#26126;&#36825;&#19968;&#33539;&#24335;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#19968;&#20123;&#24314;&#35758;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20221;&#35843;&#26597;&#26159;&#35813;&#20027;&#39064;&#19978;&#31532;&#19968;&#20221;&#27492;&#31867;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.13534v2 Announce Type: replace  Abstract: Traditional statistical learning theory relies on the assumption that data are identically and independently distributed (i.i.d.). However, this assumption often does not hold in many real-life applications. In this survey, we explore learning scenarios where examples are dependent and their dependence relationship is described by a dependency graph, a commonly utilized model in probability and combinatorics. We collect various graph-dependent concentration bounds, which are then used to derive Rademacher complexity and stability generalization bounds for learning from graph-dependent data. We illustrate this paradigm through practical learning tasks and provide some research directions for future work. To our knowledge, this survey is the first of this kind on this subject.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#20855;&#26377;&#25671;&#33218;&#21453;&#39304;&#30340;&#21452;&#37325;&#26368;&#20248;&#26080;&#24724;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#20248;&#21518;&#24724;&#21644;&#32852;&#21512;&#21160;&#20316;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#28857;&#30340;&#36895;&#29575;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2112.02856</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#25671;&#33218;&#21453;&#39304;&#30340;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#30340;&#21452;&#37325;&#26368;&#20248;&#26080;&#24724;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Doubly Optimal No-Regret Online Learning in Strongly Monotone Games with Bandit Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.02856
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#20855;&#26377;&#25671;&#33218;&#21453;&#39304;&#30340;&#21452;&#37325;&#26368;&#20248;&#26080;&#24724;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#20248;&#21518;&#24724;&#21644;&#32852;&#21512;&#21160;&#20316;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#28857;&#30340;&#36895;&#29575;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#25671;&#33218;&#21453;&#39304;&#30340;&#26410;&#30693;&#21338;&#24328;&#20013;&#30340;&#22312;&#32447;&#26080;&#24724;&#23398;&#20064;&#65292;&#20854;&#20013;&#27599;&#20010;&#29609;&#23478;&#21482;&#33021;&#35266;&#23519;&#21040;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#22870;&#21169; -- &#30001;&#25152;&#26377;&#29609;&#23478;&#24403;&#21069;&#30340;&#32852;&#21512;&#21160;&#20316;&#30830;&#23450; -- &#32780;&#19981;&#26159;&#26799;&#24230;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;\textit{&#20809;&#28369;&#19988;&#24378;&#21333;&#35843;}&#21338;&#24328;&#31867;&#65292;&#24182;&#30740;&#31350;&#20854;&#20013;&#30340;&#26368;&#20248;&#26080;&#24724;&#23398;&#20064;&#12290;&#21033;&#29992;&#33258;&#20849;&#36717;&#38556;&#30861;&#20989;&#25968;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25671;&#33218;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#20809;&#28369;&#21644;&#24378;&#20985;&#24615;&#22870;&#21169;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;$\tilde{\Theta}(n\sqrt{T})$&#30340;&#21333;&#19968;&#20195;&#29702;&#26368;&#20248;&#21518;&#24724;&#65288;$n \geq 1$&#26159;&#38382;&#39064;&#32500;&#24230;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#65292;&#22914;&#26524;&#27599;&#20010;&#29609;&#23478;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#24212;&#29992;&#36825;&#20010;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#65292;&#32852;&#21512;&#21160;&#20316;&#22312;\textit{&#26368;&#21518;&#36845;&#20195;}&#20013;&#20197;$\tilde{\Theta}(nT^{-1/2})$&#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#28857;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20043;&#21069;&#65292;&#22312;&#30456;&#21516;&#31867;&#21035;&#30340;&#21338;&#24328;&#20013;&#65292;&#26368;&#20339;&#24050;&#30693;&#30340;&#25910;&#25947;&#36895;&#29575;&#20026;$\tilde{O}(n^{2/3}T^{-1/3})&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.02856v4 Announce Type: replace  Abstract: We consider online no-regret learning in unknown games with bandit feedback, where each player can only observe its reward at each time -- determined by all players' current joint action -- rather than its gradient. We focus on the class of \textit{smooth and strongly monotone} games and study optimal no-regret learning therein. Leveraging self-concordant barrier functions, we first construct a new bandit learning algorithm and show that it achieves the single-agent optimal regret of $\tilde{\Theta}(n\sqrt{T})$ under smooth and strongly concave reward functions ($n \geq 1$ is the problem dimension). We then show that if each player applies this no-regret learning algorithm in strongly monotone games, the joint action converges in the \textit{last iterate} to the unique Nash equilibrium at a rate of $\tilde{\Theta}(nT^{-1/2})$. Prior to our work, the best-known convergence rate in the same class of games is $\tilde{O}(n^{2/3}T^{-1/3})
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25237;&#24433;Wasserstein&#36317;&#31163;&#36827;&#34892;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#27979;&#35797;&#33021;&#21147;&#20943;&#24369;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#20248;&#25237;&#24433;&#21644;&#20302;&#32500;&#32447;&#24615;&#26144;&#23556;&#26368;&#22823;&#21270;Wasserstein&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2010.11970</link><description>&lt;p&gt;
&#20351;&#29992;&#25237;&#24433;Wasserstein&#36317;&#31163;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Two-sample Test using Projected Wasserstein Distance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2010.11970
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25237;&#24433;Wasserstein&#36317;&#31163;&#36827;&#34892;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#27979;&#35797;&#33021;&#21147;&#20943;&#24369;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#20248;&#25237;&#24433;&#21644;&#20302;&#32500;&#32447;&#24615;&#26144;&#23556;&#26368;&#22823;&#21270;Wasserstein&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#25237;&#24433;Wasserstein&#36317;&#31163;&#36827;&#34892;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#32473;&#23450;&#20004;&#32452;&#26679;&#26412;&#65292;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36991;&#20813;Wasserstein&#36317;&#31163;&#20013;&#30340;&#32500;&#24230;&#28798;&#38590;&#65306;&#24403;&#32500;&#24230;&#24456;&#39640;&#26102;&#65292;&#23427;&#30340;&#27979;&#35797;&#33021;&#21147;&#20250;&#26174;&#33879;&#20943;&#24369;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#32500;&#31354;&#38388;&#20013;Wasserstein&#24230;&#37327;&#30340;&#32531;&#24930;&#38598;&#20013;&#29305;&#24615;&#25152;&#33268;&#12290;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#23558;&#26368;&#20248;&#25237;&#24433;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#25214;&#21040;&#20302;&#32500;&#32447;&#24615;&#26144;&#23556;&#20197;&#26368;&#22823;&#21270;&#25237;&#24433;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#12290;&#25105;&#20204;&#21051;&#30011;&#20102;IPM&#20013;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#36895;&#29575;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#35813;&#24230;&#37327;&#30340;&#23454;&#38469;&#31639;&#27861;&#12290;&#25968;&#20540;&#23454;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2010.11970v4 Announce Type: replace-cross  Abstract: We develop a projected Wasserstein distance for the two-sample test, a fundamental problem in statistics and machine learning: given two sets of samples, to determine whether they are from the same distribution. In particular, we aim to circumvent the curse of dimensionality in Wasserstein distance: when the dimension is high, it has diminishing testing power, which is inherently due to the slow concentration property of Wasserstein metrics in the high dimension space. A key contribution is to couple optimal projection to find the low dimensional linear mapping to maximize the Wasserstein distance between projected probability distributions. We characterize the theoretical property of the finite-sample convergence rate on IPMs and present practical algorithms for computing this metric. Numerical examples validate our theoretical results.
&lt;/p&gt;</description></item><item><title>MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13460</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#35786;&#26029;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13460
&lt;/p&gt;
&lt;p&gt;
MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;Agent&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#22312;&#38476;&#29983;&#21644;&#25932;&#23545;&#29615;&#22659;&#20013;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#22312;&#29087;&#24713;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26032;&#24773;&#20917;&#19979;&#24448;&#24448;&#20250;&#22240;&#20026;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#32780;&#22833;&#36133;&#12290;&#22312;&#26082;&#21253;&#21547;&#21512;&#20316;&#21448;&#21253;&#21547;&#31454;&#20105;&#34892;&#20026;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#65292;&#20307;&#29616;&#20102;&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#25361;&#25112;&#30340;&#21452;&#37325;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#31283;&#20581;&#24615;&#35786;&#26029;&#65288;MADRID&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#22810;Agent&#31574;&#30053;&#20013;&#26292;&#38706;&#25112;&#30053;&#28431;&#27934;&#30340;&#22810;&#26679;&#21270;&#23545;&#25239;&#22330;&#26223;&#30340;&#26032;&#26041;&#27861;&#12290;MADRID&#21033;&#29992;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#23548;&#33322;&#23545;&#25239;&#29615;&#22659;&#30340;&#24191;&#38420;&#31354;&#38388;&#65292;&#20351;&#29992;&#30446;&#26631;&#31574;&#30053;&#30340;&#36951;&#25022;&#20540;&#26469;&#34913;&#37327;&#36825;&#20123;&#29615;&#22659;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#22312;11vs11&#29256;&#30340;Google Research Football&#19978;&#35780;&#20272;&#20102;MADRID&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2312.10144</link><description>&lt;p&gt;
&#21333;&#19968;GPU&#19978;&#30340;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20849;&#20139;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#21333;&#19968;&#28508;&#22312;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#29616;&#26377;&#30340;&#22312;&#22823;&#37327;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#24212;&#35813;&#33021;&#22815;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#20174;&#21333;&#27169;&#24577;&#27169;&#22411;&#20013;&#21019;&#24314;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuseMix&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#20219;&#24847;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;FuseMix&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21644;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#22312;Flickr30K&#30340;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#27604;CLIP&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#32422;600&#20493;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \! 600\times$ fewer GP
&lt;/p&gt;</description></item><item><title>LipSim&#26159;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;&#25552;&#20379;&#20102;&#22260;&#32469;&#24230;&#37327;&#26041;&#27861;&#30340;&#38450;&#25252;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.18274</link><description>&lt;p&gt;
LipSim: &#19968;&#31181;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LipSim: A Provably Robust Perceptual Similarity Metric. (arXiv:2310.18274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18274
&lt;/p&gt;
&lt;p&gt;
LipSim&#26159;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;&#25552;&#20379;&#20102;&#22260;&#32469;&#24230;&#37327;&#26041;&#27861;&#30340;&#38450;&#25252;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#24320;&#21457;&#21644;&#24212;&#29992;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#20687;&#32032;&#32423;&#24230;&#37327;&#26041;&#27861;&#65292;&#30693;&#35273;&#24230;&#37327;&#26041;&#27861;&#22312;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#21644;&#20316;&#20026;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#20195;&#29702;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30693;&#35273;&#24230;&#37327;&#26041;&#27861;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#25915;&#20987;&#33030;&#24369;&#24615;&#30340;&#20851;&#27880;&#20063;&#26085;&#30410;&#22686;&#38271;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;ViT&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#26368;&#26032;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LipSim&#65288;Lipschitz&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65289;&#30340;&#40065;&#26834;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;LipSim&#25552;&#20379;&#20102;&#22260;&#32469;&#30528;&#24230;&#37327;&#26041;&#27861;&#30340;&#38450;&#25252;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen growing interest in developing and applying perceptual similarity metrics. Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system. On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks. It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks. In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks. We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas aroun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;OGD&#31639;&#27861;\textsf{AdaOGD}&#65292;&#22312;&#24378;&#20984;&#24615;&#19979;&#23454;&#29616;&#20102;$ O(\log^2(T)) $&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#20351;&#24471;&#32852;&#21512;&#34892;&#21160;&#26368;&#21518;&#19968;&#27425;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.14085</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#26799;&#24230;&#21453;&#39304;&#30340;&#24378;&#21333;&#35843;&#21644;&#25351;&#25968;&#20984;&#21338;&#24328;&#20013;&#30340;&#33258;&#36866;&#24212;&#12289;&#21452;&#37325;&#26368;&#20248;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback. (arXiv:2310.14085v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;OGD&#31639;&#27861;\textsf{AdaOGD}&#65292;&#22312;&#24378;&#20984;&#24615;&#19979;&#23454;&#29616;&#20102;$ O(\log^2(T)) $&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#20351;&#24471;&#32852;&#21512;&#34892;&#21160;&#26368;&#21518;&#19968;&#27425;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#20984;&#24615;&#25110;&#21333;&#35843;&#24615;&#20551;&#35774;&#19979;&#65292;&#32593;&#19978;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#21452;&#37325;&#26368;&#20248;&#30340;&#65306;&#65288;1&#65289;&#22312;&#21333;&#20010;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#24378;&#20984;&#25104;&#26412;&#20989;&#25968;&#65292;&#23427;&#23454;&#29616;&#20102;$ \Theta(\log T) $&#30340;&#26368;&#20248;&#21518;&#24724;&#65307;&#65288;2&#65289;&#22312;&#20855;&#26377;&#24378;&#21333;&#35843;&#24615;&#30340;&#22810;&#20195;&#29702;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#20195;&#29702;&#20351;&#29992;OGD&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20851;&#20110;&#32852;&#21512;&#34892;&#21160;&#30340;&#26368;&#21518;&#19968;&#27425;&#25910;&#25947;&#21040;&#21807;&#19968;&#32435;&#20160;&#22343;&#34913;&#30340;&#26368;&#20248;&#36895;&#29575;$ \Theta(\frac{1}{T}) $&#12290;&#23613;&#31649;&#36825;&#20123;&#26377;&#38480;&#26102;&#38388;&#30340;&#20445;&#35777;&#31361;&#20986;&#20102;&#20854;&#20248;&#28857;&#65292;&#20294;OGD&#30340;&#32570;&#28857;&#26159;&#38656;&#35201;&#30693;&#36947;&#24378;&#20984;&#24615;/&#21333;&#35843;&#24615;&#30340;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;OGD&#31639;&#27861;\textsf{AdaOGD}&#65292;&#23427;&#19981;&#38656;&#35201;&#20808;&#39564;&#30340;&#30693;&#35782;&#36825;&#20123;&#21442;&#25968;&#12290;&#22312;&#21333;&#20010;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24378;&#20984;&#24615;&#19979;&#23454;&#29616;&#20102;$ O(\log^2(T)) $&#30340;&#21518;&#24724;&#65292;&#36825;&#26159;&#26368;&#20248;&#30340;&#38500;&#20102;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#22312;&#24378;&#21333;&#35843;&#21338;&#24328;&#20013;&#27599;&#20010;&#20195;&#29702;&#37117;&#20351;&#29992;\textsf{AdaOGD}&#65292;&#21017;&#32852;&#21512;&#34892;&#21160;&#25910;&#25947;&#21040;&#26368;&#21518;&#19968;&#20010;&#36845;&#20195;&#26102;&#30340;&#19968;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\Theta(\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate s
&lt;/p&gt;</description></item><item><title>&#36879;&#26126;&#24230;&#38382;&#39064;&#26159;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#25919;&#31574;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#40657;&#30418;&#23376;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;&#21644;&#38382;&#36131;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21644;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13240</link><description>&lt;p&gt;
&#36879;&#26126;&#24230;&#25361;&#25112;&#19982;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#35780;&#20272; - &#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#38382;&#36131;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability. (arXiv:2310.13240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13240
&lt;/p&gt;
&lt;p&gt;
&#36879;&#26126;&#24230;&#38382;&#39064;&#26159;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#25919;&#31574;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#40657;&#30418;&#23376;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;&#21644;&#38382;&#36131;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21644;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#24320;&#22987;&#22312;&#23454;&#38469;&#25919;&#31574;&#35780;&#20272;&#20219;&#21153;&#20013;&#20351;&#29992;&#65292;&#28789;&#27963;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#25152;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#40657;&#30418;&#23376;&#65292;&#21363;&#27809;&#26377;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#20272;&#35745;&#12290;&#36825;&#22312;&#25919;&#31574;&#35780;&#20272;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25919;&#24220;&#39046;&#22495;&#65292;&#22240;&#20026;&#24456;&#38590;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#25353;&#29031;&#20844;&#27491;&#30340;&#26041;&#24335;&#36816;&#34892;&#65292;&#22522;&#20110;&#27491;&#30830;&#30340;&#35777;&#25454;&#35299;&#37322;&#65292;&#24182;&#19988;&#36879;&#26126;&#21040;&#36275;&#20197;&#20801;&#35768;&#22312;&#20986;&#29616;&#38382;&#39064;&#26102;&#36827;&#34892;&#38382;&#36131;&#12290;&#28982;&#32780;&#65292;&#22312;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24456;&#23569;&#35752;&#35770;&#36879;&#26126;&#24230;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20026;&#20160;&#20040;&#36879;&#26126;&#24230;&#38382;&#39064;&#26159;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#20844;&#20849;&#25919;&#31574;&#35780;&#20272;&#24212;&#29992;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21644;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal machine learning tools are beginning to see use in real-world policy evaluation tasks to flexibly estimate treatment effects. One issue with these methods is that the machine learning models used are generally black boxes, i.e., there is no globally interpretable way to understand how a model makes estimates. This is a clear problem in policy evaluation applications, particularly in government, because it is difficult to understand whether such models are functioning in ways that are fair, based on the correct interpretation of evidence and transparent enough to allow for accountability if things go wrong. However, there has been little discussion of transparency problems in the causal machine learning literature and how these might be overcome. This paper explores why transparency issues are a problem for causal machine learning in public policy evaluation applications and considers ways these problems might be addressed through explainable AI tools and by simplifying models in
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#65292;&#20998;&#21035;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#25512;&#23548;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.11256</link><description>&lt;p&gt;
&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#31867;&#20284;&#20110;Gromov-Wassertein&#30340;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space. (arXiv:2310.11256v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#65292;&#20998;&#21035;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#25512;&#23548;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38598;&#21512;&#19978;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#12290;&#31532;&#19968;&#31181;&#36317;&#31163;&#26159;&#22312;&#39640;&#26031;&#27979;&#24230;&#31354;&#38388;&#19978;&#20004;&#20010;&#31163;&#25955;&#20998;&#24067;&#30340;Gromov-Wasserstein&#36317;&#31163;&#12290;&#35813;&#36317;&#31163;&#21487;&#20197;&#20316;&#20026;Gromov-Wasserstein&#30340;&#26367;&#20195;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20294;&#19981;&#33021;&#30452;&#25509;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#36816;&#36755;&#26041;&#26696;&#12290;&#20026;&#20102;&#35774;&#35745;&#20986;&#36825;&#26679;&#30340;&#36816;&#36755;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21478;&#19968;&#31181;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#36317;&#31163;&#19982;Gromov-Wasserstein&#23494;&#20999;&#30456;&#20851;&#12290;&#24403;&#23558;&#20801;&#35768;&#30340;&#36816;&#36755;&#32806;&#21512;&#38480;&#21046;&#20026;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26102;&#65292;&#36825;&#23450;&#20041;&#20102;&#21478;&#19968;&#31181;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21487;&#20197;&#20316;&#20026;Gromov-Wasserstein&#30340;&#21478;&#19968;&#31181;&#26367;&#20195;&#65292;&#24182;&#20801;&#35768;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce two Gromov-Wasserstein-type distances on the set of Gaussian mixture models. The first one takes the form of a Gromov-Wasserstein distance between two discrete distributionson the space of Gaussian measures. This distance can be used as an alternative to Gromov-Wasserstein for applications which only require to evaluate how far the distributions are from each other but does not allow to derive directly an optimal transportation plan between clouds of points. To design a way to define such a transportation plan, we introduce another distance between measures living in incomparable spaces that turns out to be closely related to Gromov-Wasserstein. When restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein and which allows to derive an optimal assignment between points. Finally,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GTA&#65289;&#65292;&#29992;&#20110;&#23558;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30456;&#23545;&#21464;&#25442;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#35270;&#22270;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10375</link><description>&lt;p&gt;
GTA&#65306;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#22810;&#35270;&#22270;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers. (arXiv:2310.10375v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10375
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GTA&#65289;&#65292;&#29992;&#20110;&#23558;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30456;&#23545;&#21464;&#25442;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#35270;&#22270;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#23545;&#36755;&#20837;&#26631;&#35760;&#30340;&#25490;&#21015;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#23545;&#26631;&#35760;&#30340;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#23545;&#35768;&#22810;&#20219;&#21153;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#23545;&#20110;&#36890;&#24120;&#22312;&#20854;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#32467;&#26500;&#29305;&#24615;&#30340;&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#65292;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20540;&#24471;&#24576;&#30097;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#23545;&#20110;3D&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#23562;&#37325;&#20854;&#24213;&#23618;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#23558;&#26631;&#35760;&#30340;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30001;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#25152;&#30830;&#23450;&#30340;&#30456;&#23545;&#21464;&#25442;&#12290;&#36890;&#36807;&#22312;&#31232;&#30095;&#23485;&#22522;&#32447;&#22810;&#35270;&#22270;&#35774;&#32622;&#20013;&#35780;&#20272;&#22810;&#20010;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#65288;NVS&#65289;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#8212;&#8212;&#20960;&#20309;&#21464;&#25442;&#27880;&#24847;&#21147;&#65288;GTA&#65289;&#22914;&#20309;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03223</link><description>&lt;p&gt;
TacoGFN: &#38024;&#23545;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet
&lt;/p&gt;
&lt;p&gt;
TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#26159;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#20013;&#30340;&#34507;&#30333;&#36136;-&#20998;&#23376;&#20998;&#24067;&#36827;&#34892;&#36817;&#20284;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#30340;&#20998;&#23376;&#20013;&#24456;&#38590;&#23454;&#29616;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#32467;&#21512;&#25913;&#21892;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#23558;&#21475;&#34955;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#23450;&#20041;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;TacoGFN&#65292;&#19968;&#31181;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#32780;&#19981;&#26159;&#36866;&#24212;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#26041;&#27861;&#26469;&#21152;&#24555;&#23545;&#25509;&#24471;&#20998;&#35745;&#31639;&#65292;&#24182;&#25552;&#20986;&#20102;TacoGFN&#26469;&#39640;&#25928;&#22320;&#25506;&#32034;&#20998;&#23376;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20960;&#36718;&#20027;&#21160;&#23398;&#20064;&#65292;&#20351;&#29992;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#23545;&#29983;&#25104;&#30340;&#26679;&#26412;&#36827;&#34892;&#26597;&#35810;&#65292;&#20197;&#25913;&#21892;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#25506;&#32034;&#26356;&#22810;&#30340;&#20998;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule land
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MRA-FNO&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#26469;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#24182;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16971</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Multi-Resolution Active Learning of Fourier Neural Operators. (arXiv:2309.16971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MRA-FNO&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#26469;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#24182;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20165;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#38754;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;FNO&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#22240;&#20026;&#23427;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#29289;&#29702;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;FNO&#65288;MRA-FNO&#65289;&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#65292;&#23613;&#37327;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#65292;&#21516;&#26102;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#22810;&#20998;&#36776;&#29575;FNO&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21518;&#39564;&#25512;&#29702;&#31639;&#27861;&#12290;&#20026;&#20102;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#26368;&#22823;&#21270;&#25928;&#29992;&#25104;&#26412;&#27604;&#20316;&#20026;&#33719;&#21462;&#20989;&#25968;&#65292;&#22312;&#27599;&#19968;&#27493;&#33719;&#21462;&#26032;&#30340;&#26679;&#26412;&#21644;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#30697;&#21305;&#37197;&#21644;&#30697;&#38453;&#34892;&#21015;&#24335;&#24341;&#29702;&#23454;&#29616;&#20102;&#21487;&#34892;&#65292;&#39640;&#25928;&#30340;&#25928;&#29992;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operator (FNO) is a popular operator learning framework, which not only achieves the state-of-the-art performance in many tasks, but also is highly efficient in training and prediction. However, collecting training data for the FNO is a costly bottleneck in practice, because it often demands expensive physical simulations. To overcome this problem, we propose Multi-Resolution Active learning of FNO (MRA-FNO), which can dynamically select the input functions and resolutions to lower the data cost as much as possible while optimizing the learning efficiency. Specifically, we propose a probabilistic multi-resolution FNO and use ensemble Monte-Carlo to develop an effective posterior inference algorithm. To conduct active learning, we maximize a utility-cost ratio as the acquisition function to acquire new examples and resolutions at each step. We use moment matching and the matrix determinant lemma to enable tractable, efficient utility computation. Furthermore, we develop a
&lt;/p&gt;</description></item><item><title>RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.02671</link><description>&lt;p&gt;
RLSynC: &#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#26041;&#27861;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02671
&lt;/p&gt;
&lt;p&gt;
RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26159;&#30830;&#23450;&#33021;&#22815;&#21453;&#24212;&#24418;&#25104;&#25152;&#38656;&#20135;&#29289;&#30340;&#19968;&#32452;&#21453;&#24212;&#29289;&#20998;&#23376;&#30340;&#36807;&#31243;&#12290;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#39318;&#20808;&#39044;&#27979;&#20135;&#29289;&#20013;&#30340;&#21453;&#24212;&#20013;&#24515;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#21512;&#25104;&#29289;&#37325;&#26032;&#34917;&#20840;&#25104;&#21453;&#24212;&#29289;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#24517;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#23454;&#29992;&#24615;&#65292;&#20197;&#25351;&#23548;&#21512;&#25104;&#35268;&#21010;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;RLSynC&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#26041;&#27861;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;RLSynC&#20026;&#27599;&#20010;&#21512;&#25104;&#29289;&#20998;&#37197;&#19968;&#20010;&#20195;&#29702;&#65292;&#25152;&#26377;&#20195;&#29702;&#37117;&#36890;&#36807;&#21516;&#27493;&#36827;&#34892;&#36880;&#27493;&#34892;&#21160;&#65292;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#12290;RLSynC&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#25506;&#32034;&#26032;&#30340;&#21453;&#24212;&#31354;&#38388;&#12290;RLSynC&#20351;&#29992;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#21453;&#24212;&#29289;&#22312;&#21512;&#25104;&#20135;&#29289;&#26102;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09591</link><description>&lt;p&gt;
&#26799;&#24230;&#21453;&#20987;&#65306;&#22914;&#20309;&#28388;&#38500;&#39640;&#39057;&#29575;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#22411;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#36880;&#28176;&#21462;&#20195;&#20102;&#26087;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20026;&#20309;&#20248;&#20110;&#26799;&#24230;&#22411;&#26041;&#27861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20174;&#32463;&#39564;&#35266;&#23519;&#24320;&#22987;&#65306;&#36825;&#20004;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#21151;&#29575;&#35889;&#65292;&#26799;&#24230;&#22411;&#26041;&#27861;&#25581;&#31034;&#20102;&#27604;&#39044;&#27979;&#22411;&#26041;&#27861;&#26356;&#22810;&#30340;&#39640;&#39057;&#20869;&#23481;&#12290;&#36825;&#19968;&#35266;&#23519;&#24341;&#21457;&#20102;&#22810;&#20010;&#38382;&#39064;&#65306;&#36825;&#31181;&#39640;&#39057;&#20449;&#24687;&#30340;&#26469;&#28304;&#26159;&#20160;&#20040;&#65292;&#23427;&#26159;&#21542;&#30495;&#27491;&#21453;&#26144;&#20102;&#31995;&#32479;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#65311;&#26368;&#21518;&#65292;&#20026;&#20160;&#20040;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19979;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#39057;&#20449;&#24687;&#23558;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#25968;&#65311;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#30340;&#26799;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#21253;&#21547;&#26469;&#33258;&#39640;&#39057;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;NILM&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#22312;&#34394;&#25311;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09244</link><description>&lt;p&gt;
&#38754;&#21521;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#21487;&#25345;&#32493;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Sustainable Deep Learning for Multi-Label Classification on NILM. (arXiv:2307.09244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;NILM&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#22312;&#34394;&#25311;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;&#26159;&#20174;&#21333;&#20010;&#35745;&#37327;&#28857;&#33719;&#21462;&#23478;&#24237;&#25110;&#20225;&#19994;&#24635;&#30005;&#21147;&#28040;&#32791;&#30340;&#30005;&#22120;&#32423;&#25968;&#25454;&#30340;&#36807;&#31243;&#12290;&#30005;&#22120;&#32423;&#25968;&#25454;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#38656;&#27714;&#21709;&#24212;&#24212;&#29992;&#12289;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20197;&#21450;&#25552;&#39640;&#33021;&#25928;&#21644;&#20943;&#23569;&#30899;&#36275;&#36857;&#30340;&#24847;&#35782;&#25552;&#39640;&#21644;&#28608;&#21169;&#12290;&#26368;&#36817;&#65292;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#22312;NILM&#20998;&#31867;&#20013;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#65292;&#24182;&#35777;&#26126;&#22312;&#22686;&#38271;&#30340;&#22797;&#26434;&#24615;&#19979;&#23545;NILM&#20998;&#31867;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#38543;&#30528;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#25805;&#20316;&#36807;&#31243;&#20013;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;DL&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#26469;&#22686;&#24378;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20351;&#29992;&#20174;&#27979;&#37327;&#25968;&#25454;&#38598;&#21512;&#25104;&#30340;&#25968;&#25454;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230;&#36339;&#36291;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#24046;&#23398;&#20064;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#31283;&#20581;&#24615;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#24418;&#24335;&#21644;&#24378;&#24230;&#36339;&#36291;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02766</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#36339;&#36291;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#24046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference Learning for High-Dimensional PIDEs with Jumps. (arXiv:2307.02766v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230;&#36339;&#36291;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#24046;&#23398;&#20064;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#31283;&#20581;&#24615;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#24418;&#24335;&#21644;&#24378;&#24230;&#36339;&#36291;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#24046;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#65288;PIDEs&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;Levy&#36807;&#31243;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#30456;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#27169;&#25311;&#25972;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#26041;&#31243;&#30340;&#35299;&#21644;&#38750;&#23616;&#37096;&#39033;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26102;&#24046;&#35823;&#24046;&#12289;&#32456;&#27490;&#26465;&#20214;&#21644;&#38750;&#23616;&#37096;&#39033;&#30340;&#23646;&#24615;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22312;100&#32500;&#30340;&#23454;&#39564;&#20013;&#30456;&#23545;&#35823;&#24046;&#36798;&#21040;&#20102;O(10^{-3})&#65292;&#22312;&#19968;&#32500;&#32431;&#36339;&#36291;&#38382;&#39064;&#20013;&#36798;&#21040;&#20102;O(10^{-4})&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#31283;&#20581;&#24615;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#24418;&#24335;&#21644;&#24378;&#24230;&#36339;&#36291;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a deep learning framework for solving high-dimensional partial integro-differential equations (PIDEs) based on the temporal difference learning. We introduce a set of Levy processes and construct a corresponding reinforcement learning model. To simulate the entire process, we use deep neural networks to represent the solutions and non-local terms of the equations. Subsequently, we train the networks using the temporal difference error, termination condition, and properties of the non-local terms as the loss function. The relative error of the method reaches O(10^{-3}) in 100-dimensional experiments and O(10^{-4}) in one-dimensional pure jump problems. Additionally, our method demonstrates the advantages of low computational cost and robustness, making it well-suited for addressing problems with different forms and intensities of jumps.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20135;&#21697;&#37327;&#21270;&#65288;PQ&#65289;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26367;&#20195;&#20256;&#32479;&#20056;&#21152;&#65288;MAC&#65289;&#36816;&#31639;&#30340;&#25928;&#26524;&#12290;&#20316;&#32773;&#21457;&#29616;FLOP&#21644;&#21442;&#25968;&#25968;&#37327;&#31561;&#25351;&#26631;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;PQ&#23450;&#21046;&#30828;&#20214;&#21152;&#36895;&#22120;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18334</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;&#65306;&#25105;&#20204;&#21040;&#20102;&#21527;&#65311;&#20135;&#21697;&#37327;&#21270;&#21450;&#20854;&#30828;&#20214;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are We There Yet? Product Quantization and its Hardware Acceleration. (arXiv:2305.18334v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20135;&#21697;&#37327;&#21270;&#65288;PQ&#65289;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26367;&#20195;&#20256;&#32479;&#20056;&#21152;&#65288;MAC&#65289;&#36816;&#31639;&#30340;&#25928;&#26524;&#12290;&#20316;&#32773;&#21457;&#29616;FLOP&#21644;&#21442;&#25968;&#25968;&#37327;&#31561;&#25351;&#26631;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;PQ&#23450;&#21046;&#30828;&#20214;&#21152;&#36895;&#22120;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20056;&#21152;&#65288;MAC&#65289;&#36816;&#31639;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#20027;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#26368;&#36817;&#65292;&#20135;&#21697;&#37327;&#21270;&#65288;PQ&#65289;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#36825;&#20123;&#24037;&#20316;&#36127;&#36733;&#65292;&#29992;&#39044;&#20808;&#35745;&#31639;&#30340;&#28857;&#31215;&#30340;&#20869;&#23384;&#26597;&#25214;&#26367;&#25442;&#20102;MAC&#12290;&#34429;&#28982;&#36825;&#20010;&#23646;&#24615;&#20351;PQ&#25104;&#20026;&#27169;&#22411;&#21152;&#36895;&#30340;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20154;&#20204;&#24456;&#23569;&#20102;&#35299;&#19982;&#35745;&#31639;&#21644;&#23384;&#20648;&#22120;&#21344;&#29992;&#30456;&#20851;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;PQ&#35774;&#32622;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#36880;&#23618;&#37325;&#24314;&#35823;&#24046;&#21644;&#31471;&#21040;&#31471;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#30740;&#31350;&#37096;&#32626;PQ DNN&#30340;&#25928;&#29575;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;FLOP&#12289;&#21442;&#25968;&#25968;&#37327;&#29978;&#33267;CPU/GPU&#24615;&#33021;&#31561;&#25351;&#26631;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26356;&#20844;&#24179;&#22320;&#35780;&#20272;PQ&#30340;&#30828;&#20214;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#23450;&#21046;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#36816;&#34892;PQ&#27169;&#22411;&#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;PQ&#37197;&#32622;&#30340;&#30828;&#20214;&#24615;&#33021;&#21644;&#23384;&#20648;&#35201;&#27714;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional multiply-accumulate (MAC) operations have long dominated computation time for deep neural networks (DNNs). Recently, product quantization (PQ) has been successfully applied to these workloads, replacing MACs with memory lookups to pre-computed dot products. While this property makes PQ an attractive solution for model acceleration, little is understood about the associated trade-offs in terms of compute and memory footprint, and the impact on accuracy. Our empirical study investigates the impact of different PQ settings and training methods on layerwise reconstruction error and end-to-end model accuracy. When studying the efficiency of deploying PQ DNNs, we find that metrics such as FLOPs, number of parameters, and even CPU/GPU performance, can be misleading. To address this issue, and to more fairly assess PQ in terms of hardware efficiency, we design the first custom hardware accelerator to evaluate the speed and efficiency of running PQ models. We identify PQ configurat
&lt;/p&gt;</description></item><item><title>MaxViT-UNet&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#22312;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.08396</link><description>&lt;p&gt;
MaxViT-UNet: &#22810;&#36724;&#27880;&#24847;&#21147;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08396
&lt;/p&gt;
&lt;p&gt;
MaxViT-UNet&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#22312;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21367;&#31215;&#31639;&#23376;&#30340;&#23616;&#37096;&#24615;&#36136;&#25233;&#21046;&#20102;CNNs&#25429;&#25417;&#20840;&#23616;&#21644;&#38271;&#31243;&#20132;&#20114;&#12290;&#26368;&#36817;&#65292;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#20294;&#26159;&#65292;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;CNN&#31867;&#24402;&#32435;&#20559;&#24046;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MaxViT-UNet&#65292;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#25552;&#20986;&#30340;&#28151;&#21512;&#35299;&#30721;&#22120;&#65292;&#36824;&#22522;&#20110;MaxViT-block&#65292;&#26088;&#22312;&#22312;&#27599;&#20010;&#35299;&#30721;&#38454;&#27573;&#26368;&#23567;&#21270;&#35745;&#31639;&#36127;&#25285;&#19979;&#21033;&#29992;&#21367;&#31215;&#21644;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#21147;&#37327;&#12290;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#30340;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;&#28151;&#21512;&#35299;&#30721;&#22120;&#22359;&#26368;&#21021;&#36890;&#36807;&#19978;&#37319;&#26679;&#20256;&#36755;&#20302;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have made significant strides in medical image analysis in recent years. However, the local nature of the convolution operator inhibits the CNNs from capturing global and long-range interactions. Recently, Transformers have gained popularity in the computer vision community and also medical image segmentation. But scalability issues of self-attention mechanism and lack of the CNN like inductive bias have limited their adoption. In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision transformer for medical image segmentation. The proposed hybrid decoder, also based on MaxViT-block, is designed to harness the power of convolution and self-attention mechanism at each decoding stage with minimal computational burden. The multi-axis self-attention in each decoder stage helps in differentiating between the object and background regions much more efficiently. The hybrid decoder block initially fuses the lower level features upsampled via tra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;159&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#21450;&#20854;&#29992;&#25143;&#21453;&#39304;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#35745;&#31639;&#31508;&#35760;&#26412;&#20013;&#35774;&#35745;&#21487;&#35270;&#20998;&#26512;&#24037;&#20855;&#30340;&#29420;&#29305;&#26426;&#20250;&#21644;&#32771;&#34385;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.03039</link><description>&lt;p&gt;
SuperNOVA: &#35745;&#31639;&#31508;&#35760;&#26412;&#20013;&#20114;&#21160;&#21487;&#35270;&#21270;&#30340;&#35774;&#35745;&#31574;&#30053;&#19982;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks. (arXiv:2305.03039v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03039
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;159&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#21450;&#20854;&#29992;&#25143;&#21453;&#39304;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#35745;&#31639;&#31508;&#35760;&#26412;&#20013;&#35774;&#35745;&#21487;&#35270;&#20998;&#26512;&#24037;&#20855;&#30340;&#29420;&#29305;&#26426;&#20250;&#21644;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#31508;&#35760;&#26412;&#65292;&#22914; Jupyter Notebook&#65292;&#24050;&#25104;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20107;&#23454;&#32534;&#31243;&#29615;&#22659;&#12290;&#35768;&#22810;&#21487;&#35270;&#21270;&#30740;&#31350;&#32773;&#21644;&#23454;&#36341;&#32773;&#24050;&#24320;&#21457;&#20986;&#25903;&#25345;&#31508;&#35760;&#26412;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22312;&#31508;&#35760;&#26412;&#20013;&#35774;&#35745; VA &#24037;&#20855;&#30340;&#36866;&#24403;&#26041;&#27861;&#30340;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512; 159 &#20010;&#31508;&#35760;&#26412; VA &#24037;&#20855;&#21450;&#20854;&#29992;&#25143;&#21453;&#39304;&#26469;&#30740;&#31350;&#36825;&#19968;&#39046;&#22495;&#30340;&#35774;&#35745;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;&#26469;&#33258;&#23398;&#26415;&#35770;&#25991;&#30340; 62 &#20010;&#31995;&#32479;&#21644;&#26469;&#33258;&#36890;&#36807;&#22312; GitHub &#19978;&#25235;&#21462; 860 &#19975;&#20010;&#31508;&#35760;&#26412;&#32780;&#33719;&#24471;&#30340;&#21253;&#21547;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#30340; 55k &#20010;&#31508;&#35760;&#26412;&#27744;&#20013;&#30340; 103 &#20010;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102; 15 &#20010;&#29992;&#25143;&#30740;&#31350;&#30340;&#21457;&#29616;&#21644; 379 &#20010; GitHub &#38382;&#39064;&#20013;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#36890;&#36807;&#36825;&#39033;&#24037;&#20316;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26410;&#26469;&#31508;&#35760;&#26412; VA &#24037;&#20855;&#30340;&#29420;&#29305;&#35774;&#35745;&#26426;&#20250;&#21644;&#32771;&#34385;&#22240;&#32032;&#65292;&#20363;&#22914;&#22312;&#31508;&#35760;&#26412;&#20013;&#20351;&#29992;&#21644;&#25805;&#20316;&#22810;&#27169;&#24577;&#25968;&#25454;&#20197;&#21450;&#24179;&#34913;&#21487;&#35270;&#21270;&#31508;&#35760;&#26412;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational notebooks such as Jupyter Notebook have become data scientists' de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 159 notebook VA tools and their users' feedback. Our analysis encompasses 62 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-noteb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.14178</link><description>&lt;p&gt;
mPLUG-Owl: &#27169;&#22359;&#21270;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#24320;&#25918;&#24335;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;LLMs&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;mPLUG-Owl&#65292;&#36890;&#36807;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#30340;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#20351;LLMs&#20855;&#22791;&#20102;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#21327;&#20316;&#20419;&#36827;&#20102;&#22810;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#21253;&#25324;&#29992;&#20110;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#36741;&#21161;&#23398;&#20064;&#35270;&#35273;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25913;&#36827;&#20102;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20923;&#32467;&#30340;LLM&#27169;&#22359;&#23545;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#25277;&#35937;&#22120;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#20197;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20165;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30417;&#30563;&#25968;&#25454;&#38598;&#20849;&#21516;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23545;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#20248;&#21270;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#65292;&#20854;&#20013;&#20195;&#29702;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#20248;&#21270;&#26356;&#21152;&#24555;&#36895;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2304.12420</link><description>&lt;p&gt;
&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#30340;&#26679;&#26412;&#39640;&#25928;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls. (arXiv:2304.12420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#20248;&#21270;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#65292;&#20854;&#20013;&#20195;&#29702;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#20248;&#21270;&#26356;&#21152;&#24555;&#36895;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#27169;&#25311;&#26159;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;(CAD)&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#35745;&#31639;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20351;&#31934;&#30830;(&#35745;&#31639;&#26114;&#36149;)&#30340;&#27169;&#25311;&#21487;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#20248;&#21270;&#26694;&#26550;&#25110;&#24555;&#36895;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;(&#20195;&#29702;&#27169;&#22411;)&#26469;&#20195;&#26367;&#38271;&#26102;&#38388;&#36816;&#34892;&#30340;&#27169;&#25311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#20248;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#36827;&#23637;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35774;&#35745;&#19968;&#20010;&#26368;&#20339;&#30340;&#26080;&#20154;&#27700;&#19979;&#33322;&#34892;&#22120;(UUV)&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#20248;&#21270;&#25216;&#26415;&#22312;&#20248;&#21270;&#24490;&#29615;&#20013;&#19982;&#26631;&#20934;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;(CFD)&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#26102;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25910;&#25947;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#36924;&#36817;&#21542;&#21017;&#36890;&#36807;CFD&#27714;&#35299;&#22120;&#36827;&#34892;&#35745;&#31639;&#30340;&#38459;&#21147;&#12290;&#20195;&#29702;&#27169;&#22411;&#36827;&#32780;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics simulations are a computational bottleneck in computer-aided design (CAD) optimization processes. Hence, in order to make accurate (computationally expensive) simulations feasible for use in design optimization, one requires either an optimization framework that is highly sample-efficient or fast data-driven proxies (surrogate models) for long running simulations. In this work, we leverage recent advances in optimization and artificial intelligence (AI) to address both of these potential solutions, in the context of designing an optimal unmanned underwater vehicle (UUV). We first investigate and compare the sample efficiency and convergence behavior of different optimization techniques with a standard computational fluid dynamics (CFD) solver in the optimization loop. We then develop a deep neural network (DNN) based surrogate model to approximate drag forces that would otherwise be computed via direct numerical simulation with the CFD solver. The surrogate model is in turn use
&lt;/p&gt;</description></item><item><title>TACOS &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#21512;&#25104;&#20219;&#24847;&#36755;&#20837;&#32593;&#32476;&#25299;&#25169;&#30340;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#21512;&#25104;&#22120;&#12290;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;TACOS &#21512;&#25104;&#30340; All-Reduce &#31639;&#27861;&#36895;&#24230;&#25552;&#39640;&#20102; 3.73 &#20493;&#65292;&#20026; 512-NPU &#31995;&#32479;&#21512;&#25104;&#38598;&#20307;&#31639;&#27861;&#21482;&#38656; 6.1 &#20998;&#38047;&#12290;</title><link>http://arxiv.org/abs/2304.05301</link><description>&lt;p&gt;
TACOS: &#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#38598;&#21512;&#31639;&#27861;&#21512;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Training. (arXiv:2304.05301v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05301
&lt;/p&gt;
&lt;p&gt;
TACOS &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#21512;&#25104;&#20219;&#24847;&#36755;&#20837;&#32593;&#32476;&#25299;&#25169;&#30340;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#21512;&#25104;&#22120;&#12290;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;TACOS &#21512;&#25104;&#30340; All-Reduce &#31639;&#27861;&#36895;&#24230;&#25552;&#39640;&#20102; 3.73 &#20493;&#65292;&#20026; 512-NPU &#31995;&#32479;&#21512;&#25104;&#38598;&#20307;&#31639;&#27861;&#21482;&#38656; 6.1 &#20998;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#32676;&#20043;&#38388;&#30340;&#38598;&#21512;&#36890;&#35759;&#26159;&#20998;&#24067;&#24335;&#35757;&#32451;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#36816;&#34892;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#31639;&#27861;&#23545;&#20110;&#20248;&#21270;&#36890;&#35759;&#24615;&#33021;&#20197;&#26368;&#23567;&#21270;&#25317;&#22622;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#30446;&#21069;&#65292;&#27492;&#31867;&#31639;&#27861;&#20165;&#36866;&#29992;&#20110;&#19968;&#23567;&#37096;&#20998;&#31616;&#21333;&#25299;&#25169;&#32467;&#26500;&#65292;&#38480;&#21046;&#20102;&#35757;&#32451;&#38598;&#32676;&#20013;&#37319;&#29992;&#30340;&#25299;&#25169;&#32467;&#26500;&#24182;&#22788;&#29702;&#30001;&#20110;&#32593;&#32476;&#25925;&#38556;&#32780;&#20135;&#29983;&#30340;&#19981;&#35268;&#21017;&#25299;&#25169;&#32467;&#26500;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102; TACOS&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#33258;&#21160;&#21512;&#25104;&#20219;&#24847;&#36755;&#20837;&#32593;&#32476;&#25299;&#25169;&#30340;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#21512;&#25104;&#22120;&#12290;TACOS &#21512;&#25104;&#30340; All-Reduce &#31639;&#27861;&#27604;&#22522;&#32447;&#31639;&#27861;&#24555;&#20102; 3.73 &#20493;&#65292;&#24182;&#20026; 512-NPU &#31995;&#32479;&#21512;&#25104;&#38598;&#20307;&#31639;&#27861;&#20165;&#38656; 6.1 &#20998;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collective communications are an indispensable part of distributed training. Running a topology-aware collective algorithm is crucial for optimizing communication performance by minimizing congestion. Today such algorithms only exist for a small set of simple topologies, limiting the topologies employed in training clusters and handling irregular topologies due to network failures. In this paper, we propose TACOS, an automated topology-aware collective synthesizer for arbitrary input network topologies. TACOS synthesized 3.73x faster All-Reduce algorithm over baselines, and synthesized collective algorithms for 512-NPU system in just 6.1 minutes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06471</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#20195;&#30340;&#32959;&#30244;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review. (arXiv:2303.06471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review article analyzes the application of deep neural networks in multimodal data integration to improve the accuracy and reliability of cancer diagnosis and treatment.
&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#22312;&#19981;&#21516;&#23610;&#24230;&#12289;&#27169;&#24577;&#21644;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#25968;&#25454;&#20013;&#20855;&#26377;&#20851;&#31995;&#20449;&#24687;&#65292;&#20363;&#22914;&#25918;&#23556;&#23398;&#12289;&#30149;&#29702;&#23398;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#21644;&#20020;&#24202;&#35760;&#24405;&#12290;&#25972;&#21512;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#21487;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#21487;&#33021;&#23384;&#22312;&#20154;&#31867;&#25110;&#29616;&#26377;&#25216;&#26415;&#24037;&#20855;&#26080;&#27861;&#35270;&#35273;&#19978;&#21306;&#20998;&#30340;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#21333;&#20010;&#23610;&#24230;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#37096;&#20998;&#25110;&#21333;&#19968;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#26410;&#28085;&#30422;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#23436;&#25972;&#20809;&#35889;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20419;&#36827;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#21644;&#25972;&#21512;&#30456;&#20851;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22914;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21464;&#21387;&#22120;&#65292;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#27861;&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer has relational information residing at varying scales, modalities, and resolutions of the acquired data, such as radiology, pathology, genomics, proteomics, and clinical records. Integrating diverse data types can improve the accuracy and reliability of cancer diagnosis and treatment. There can be disease-related information that is too subtle for humans or existing technological tools to discern visually. Traditional methods typically focus on partial or unimodal information about biological systems at individual scales and fail to encapsulate the complete spectrum of the heterogeneous nature of data. Deep neural networks have facilitated the development of sophisticated multimodal data fusion approaches that can extract and integrate relevant information from multiple sources. Recent deep learning frameworks such as Graph Neural Networks (GNNs) and Transformers have shown remarkable success in multimodal learning. This review article provides an in-depth analysis of the state-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#32467;&#26500;&#21270;&#22122;&#22768;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#21644;&#36203;&#24067;&#29983;&#35268;&#21017;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13417</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#22122;&#22768;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20998;&#31867;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks with structured noise improves classification and generalization. (arXiv:2302.13417v3 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13417
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#32467;&#26500;&#21270;&#22122;&#22768;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#21644;&#36203;&#24067;&#29983;&#35268;&#21017;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#22312;&#23398;&#20064;&#20013;&#30340;&#31215;&#26497;&#20316;&#29992;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#20013;&#19968;&#20010;&#24050;&#32463;&#34987;&#30830;&#35748;&#30340;&#27010;&#24565;&#65292;&#36825;&#34920;&#26126;&#29978;&#33267;&#29983;&#29289;&#31995;&#32479;&#21487;&#33021;&#21033;&#29992;&#31867;&#20284;&#30340;&#26426;&#21046;&#26469;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;Gardner&#21644;&#21512;&#20316;&#32773;&#25552;&#20986;&#30340;&#22122;&#22768;&#35757;&#32451;&#31639;&#27861;&#26159;&#22312;&#24490;&#29615;&#32593;&#32476;&#20013;&#27880;&#20837;&#22122;&#22768;&#30340;&#20856;&#22411;&#31034;&#20363;&#65292;&#24490;&#29615;&#32593;&#32476;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#30495;&#23454;&#31070;&#32463;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#32467;&#26500;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#65292;&#20351;&#24471;&#21487;&#20197;&#25509;&#36817;&#23436;&#32654;&#20998;&#31867;&#21644;&#26368;&#22823;&#21560;&#24341;&#22495;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25152;&#35859;&#30340;&#36203;&#24067;&#29983;&#35268;&#21017;&#22312;&#22122;&#22768;&#36798;&#21040;&#26368;&#22823;&#19988;&#25968;&#25454;&#26159;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#22266;&#23450;&#28857;&#26102;&#19982;&#22122;&#22768;&#35757;&#32451;&#31639;&#27861;&#19968;&#33268;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#26045;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#20339;&#22122;&#22768;&#25968;&#25454;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#26469;&#36229;&#36234;&#22122;&#22768;&#35757;&#32451;&#21644;&#36203;&#24067;&#29983;&#35268;&#21017;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The beneficial role of noise in learning is nowadays a consolidated concept in the field of artificial neural networks, suggesting that even biological systems might take advantage of similar mechanisms to maximize their performance. The training-with-noise algorithm proposed by Gardner and collaborators is an emblematic example of a noise injection procedure in recurrent networks, which are usually employed to model real neural systems. We show how adding structure into noisy training data can substantially improve the algorithm performance, allowing to approach perfect classification and maximal basins of attraction. We also prove that the so-called Hebbian unlearning rule coincides with the training-with-noise algorithm when noise is maximal and data are fixed points of the network dynamics. A sampling scheme for optimal noisy data is eventually proposed and implemented to outperform both the training-with-noise and the Hebbian unlearning procedures.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#30149;&#29702;&#22270;&#20687;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;FCL&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26412;&#22320;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#38388;&#30340;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#22312;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#65292;FCL&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06089</link><description>&lt;p&gt;
&#38754;&#21521;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated contrastive learning models for prostate cancer diagnosis and Gleason grading. (arXiv:2302.06089v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#30149;&#29702;&#22270;&#20687;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;FCL&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26412;&#22320;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#38388;&#30340;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#22312;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#65292;FCL&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#25928;&#26524;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#31283;&#20581;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#25968;&#25454;&#25910;&#38598;&#38754;&#20020;&#27807;&#36890;&#12289;&#20262;&#29702;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#38480;&#21046;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#21327;&#35843;&#22810;&#20010;&#23458;&#25143;&#31471;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#30149;&#29702;&#22270;&#20687;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;FCL&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26412;&#22320;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#38388;&#30340;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#32531;&#35299;&#21442;&#25968;&#20256;&#36755;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#24182;&#39564;&#35777;FCL&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36890;&#36807;&#28155;&#21152;&#22122;&#38899;&#36827;&#19968;&#27493;&#20445;&#25252;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;19,635&#20010;&#26469;&#33258;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#21069;&#21015;&#33146;&#30284;WSI&#19978;&#35780;&#20272;&#20102;FCL&#22312;&#30284;&#30151;&#35786;&#26029;&#20219;&#21153;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#35786;&#26029;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21306;&#20998;&#30284;&#24615;&#21644;&#38750;&#30284;&#24615;WSI&#30340;0.99&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12290;&#22312;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;FCL&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#20010;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20026;0.143&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#38477;&#20302;&#20102;21.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application effect of artificial intelligence (AI) in the field of medical imaging is remarkable. Robust AI model training requires large datasets, but data collection faces communication, ethics, and privacy protection constraints. Fortunately, federated learning can solve the above problems by coordinating multiple clients to train the model without sharing the original data. In this study, we design a federated contrastive learning framework (FCL) for large-scale pathology images and the heterogeneity challenges. It enhances the model's generalization ability by maximizing the attention consistency between the local client and server models. To alleviate the privacy leakage problem when transferring parameters and verify the robustness of FCL, we use differential privacy to further protect the model by adding noise. We evaluate the effectiveness of FCL on the cancer diagnosis task and Gleason grading task on 19,635 prostate cancer WSIs from multiple clients. In the diagnosis tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Sim2Real&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CPPF++&#26041;&#27861;&#65292;&#36890;&#36807;&#25237;&#31080;&#32858;&#21512;&#21644;&#27010;&#29575;&#24314;&#27169;&#26469;&#32771;&#34385;&#25237;&#31080;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#36807;&#28388;&#26469;&#25552;&#39640;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13398</link><description>&lt;p&gt;
CPPF++&#65306;&#22522;&#20110;&#25237;&#31080;&#32858;&#21512;&#30340;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;Sim2Real&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote Aggregation. (arXiv:2211.13398v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Sim2Real&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CPPF++&#26041;&#27861;&#65292;&#36890;&#36807;&#25237;&#31080;&#32858;&#21512;&#21644;&#27010;&#29575;&#24314;&#27169;&#26469;&#32771;&#34385;&#25237;&#31080;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#36807;&#28388;&#26469;&#25552;&#39640;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26159;&#19977;&#32500;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#26412;&#25991;&#38024;&#23545;&#21482;&#21033;&#29992;&#19977;&#32500;CAD&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CPPF++&#26041;&#27861;&#65292;&#29992;&#20110;Sim2Real&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27010;&#29575;&#24615;&#35270;&#35282;&#37325;&#26032;&#26500;&#24605;&#20102;CPPF&#30340;&#22522;&#30784;&#28857;&#23545;&#25237;&#31080;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#25237;&#31080;&#30896;&#25758;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#35268;&#33539;&#31354;&#38388;&#20013;&#27599;&#20010;&#28857;&#23545;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#24314;&#27169;&#25237;&#31080;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#36807;&#28388;&#26469;&#28040;&#38500;&#19982;&#32972;&#26223;&#25110;&#26434;&#27874;&#26377;&#20851;&#30340;&#25237;&#31080;&#65292;&#24182;&#22686;&#24378;&#20102;&#27599;&#20010;&#25237;&#31080;&#25152;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object pose estimation constitutes a critical area within the domain of 3D vision. While contemporary state-of-the-art methods that leverage real-world pose annotations have demonstrated commendable performance, the procurement of such real-world training data incurs substantial costs. This paper focuses on a specific setting wherein only 3D CAD models are utilized as a priori knowledge, devoid of any background or clutter information. We introduce a novel method, CPPF++, designed for sim-to-real pose estimation. This method builds upon the foundational point-pair voting scheme of CPPF, reconceptualizing it through a probabilistic lens. To address the challenge of voting collision, we model voting uncertainty by estimating the probabilistic distribution of each point pair within the canonical space. This approach is further augmented by iterative noise filtering, employed to eradicate votes associated with backgrounds or clutters. Additionally, we enhance the context provided by each v
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#27604;&#25439;&#22833;&#21644;&#20351;&#29992;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#22312;PU&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.01206</link><description>&lt;p&gt;
&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Positive Unlabeled Contrastive Learning. (arXiv:2206.01206v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01206
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#27604;&#25439;&#22833;&#21644;&#20351;&#29992;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#22312;PU&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20174;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#25193;&#23637;&#21040;&#32463;&#20856;&#30340;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#65288;PU&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#30340;&#20219;&#21153;&#26159;&#20165;&#36890;&#36807;&#19968;&#20123;&#26631;&#35760;&#20026;&#27491;&#26679;&#26412;&#21644;&#65288;&#36890;&#24120;&#65289;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#65288;&#21487;&#20197;&#26159;&#27491;&#26679;&#26412;&#25110;&#36127;&#26679;&#26412;&#65289;&#26469;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#26631;&#20934;infoNCE&#23545;&#27604;&#25439;&#22833;&#30340;&#23478;&#26063;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;PU&#35774;&#32622;&#65307;&#24182;&#19988;&#35777;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26032;&#30340;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#20026;&#26410;&#26631;&#35760;&#26679;&#26412;&#26500;&#24314;&#20266;&#26631;&#31614;&#65307;&#36825;&#20123;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#26368;&#32456;&#30340;&#65288;&#27491;&#26679;&#26412; vs. &#36127;&#26679;&#26412;&#65289;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#26631;&#20934;PU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;PU&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#31867;&#21035;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pretraining on unlabeled data followed by supervised fine-tuning on labeled data is a popular paradigm for learning from limited labeled examples. We extend this paradigm to the classical positive unlabeled (PU) setting, where the task is to learn a binary classifier given only a few labeled positive samples, and (often) a large amount of unlabeled samples (which could be positive or negative).  We first propose a simple extension of standard infoNCE family of contrastive losses, to the PU setting; and show that this learns superior representations, as compared to existing unsupervised and supervised approaches. We then develop a simple methodology to pseudo-label the unlabeled samples using a new PU-specific clustering scheme; these pseudo-labels can then be used to train the final (positive vs. negative) classifier. Our method handily outperforms state-of-the-art PU methods over several standard PU benchmark datasets, while not requiring a-priori knowledge of any clas
&lt;/p&gt;</description></item></channel></rss>