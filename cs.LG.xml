<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>TRAK&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#21487;&#24494;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#37327;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2303.14186</link><description>&lt;p&gt;
TRAK: &#21051;&#30011;&#22823;&#35268;&#27169;&#27169;&#22411;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
TRAK: Attributing Model Behavior at Scale. (arXiv:2303.14186v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14186
&lt;/p&gt;
&lt;p&gt;
TRAK&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#21487;&#24494;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#37327;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#30340;&#30446;&#26631;&#26159;&#36861;&#36394;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#27714;&#29992;&#25143;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#36873;&#25321;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#38750;&#20984;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#65289;&#20013;&#65292;&#35745;&#31639;&#37327;&#21487;&#34892;&#30340;&#26041;&#27861;&#21487;&#33021;&#38590;&#20197;&#20934;&#30830;&#22320;&#24402;&#22240;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#22312;&#36825;&#31867;&#22330;&#26223;&#20013;&#26377;&#25928;&#30340;&#26041;&#27861;&#21017;&#38656;&#35201;&#35757;&#32451;&#25968;&#21315;&#20010;&#27169;&#22411;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22823;&#22411;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#20013;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#19981;&#21487;&#34892;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TRAK&#65288;&#38543;&#26426;&#25237;&#24433;&#26680;&#36861;&#36394;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#21487;&#24494;&#27169;&#22411;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#37327;&#21487;&#34892;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#27169;&#22411;&#65292;TRAK &#21487;&#20197;&#21305;&#37197;&#38656;&#35201;&#35757;&#32451;&#25968;&#21315;&#27169;&#22411;&#25165;&#33021;&#24471;&#21040;&#30340;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35770;&#35777;&#20102;TRAK &#22312;&#21508;&#31181;&#27169;&#24335;&#21644;&#35268;&#27169;&#19978;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets.  In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#28431;&#27934;&#22914;&#20309;&#21462;&#20915;&#20110;&#21463;&#38480;&#20110;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23376;&#31354;&#38388;&#32500;&#25968;&#65292;&#21516;&#26102;&#38024;&#23545;&#26631;&#20934;PGD&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#25104;&#21151;&#29575;&#25552;&#20986;&#20102;&#21333;&#35843;&#36882;&#22686;&#20989;&#25968;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.14173</link><description>&lt;p&gt;
&#25214;&#21040;&#23545;&#25239;&#26679;&#26412;&#38656;&#35201;&#22810;&#23569;&#32500;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many dimensions are required to find an adversarial example?. (arXiv:2303.14173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#28431;&#27934;&#22914;&#20309;&#21462;&#20915;&#20110;&#21463;&#38480;&#20110;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23376;&#31354;&#38388;&#32500;&#25968;&#65292;&#21516;&#26102;&#38024;&#23545;&#26631;&#20934;PGD&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#25104;&#21151;&#29575;&#25552;&#20986;&#20102;&#21333;&#35843;&#36882;&#22686;&#20989;&#25968;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#25506;&#32034;&#23545;&#25239;&#24615;&#28431;&#27934;&#30340;&#30740;&#31350;&#37117;&#30528;&#30524;&#20110;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#27169;&#22411;&#36755;&#20837;&#30340;&#25152;&#26377;&#32500;&#24230;&#30340;&#24773;&#20917;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#32771;&#34385;&#20197;&#19979;&#24773;&#20917;&#65306;&#65288;i&#65289;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;&#21442;&#25968;&#25110;&#65288;ii&#65289;&#22810;&#27169;&#24577;&#38382;&#39064;&#20013;&#30340;&#27169;&#24577;&#23376;&#38598;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#24615;&#26679;&#26412;&#26377;&#25928;&#22320;&#21463;&#38480;&#20110;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23376;&#31354;&#38388;$V$&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#28431;&#27934;&#22914;&#20309;&#21462;&#20915;&#20110;$V$&#30340;&#32500;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;PGD&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#25104;&#21151;&#29575;&#22914;&#20309;&#34920;&#29616;&#20026;$\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$&#30340;&#21333;&#35843;&#36882;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;$\epsilon$&#26159;&#25200;&#21160;&#39044;&#31639;&#65292;$\frac{1}{p}+\frac{q}{q}=1$&#65292;&#21482;&#35201;$p&gt;1$&#65288;&#24403;$p=1$&#26102;&#20250;&#20986;&#29616;&#39069;&#22806;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#25105;&#20204;&#23545;&#27492;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#65289;&#12290;&#36825;&#20010;&#20989;&#25968;&#24418;&#24335;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work exploring adversarial vulnerability have focused on situations where an adversary can perturb all dimensions of model input. On the other hand, a range of recent works consider the case where either (i) an adversary can perturb a limited number of input parameters or (ii) a subset of modalities in a multimodal problem. In both of these cases, adversarial examples are effectively constrained to a subspace $V$ in the ambient input space $\mathcal{X}$. Motivated by this, in this work we investigate how adversarial vulnerability depends on $\dim(V)$. In particular, we show that the adversarial success of standard PGD attacks with $\ell^p$ norm constraints behaves like a monotonically increasing function of $\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$ where $\epsilon$ is the perturbation budget and $\frac{1}{p} + \frac{1}{q} =1$, provided $p &gt; 1$ (the case $p=1$ presents additional subtleties which we analyze in some detail). This functional form can be easily deriv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23610;&#24230;&#19981;&#21464;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#20351;&#29992;&#31354;&#38388;&#21367;&#31215;&#25110;&#20998;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23618;&#20869;&#29305;&#24449;&#22270;&#20998;&#35299;&#20026;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#24182;&#36890;&#36807;&#25554;&#20540;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#22270;&#20687;&#65292;&#20351;&#24471;&#27169;&#22411;&#20855;&#26377;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.14157</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#34892;&#21015;&#20132;&#38169;&#20687;&#32032;&#21512;&#25104;&#30340;&#39640;&#25928;&#23610;&#24230;&#19981;&#21464;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis. (arXiv:2303.14157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14157
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23610;&#24230;&#19981;&#21464;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#20351;&#29992;&#31354;&#38388;&#21367;&#31215;&#25110;&#20998;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23618;&#20869;&#29305;&#24449;&#22270;&#20998;&#35299;&#20026;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#24182;&#36890;&#36807;&#25554;&#20540;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#22270;&#20687;&#65292;&#20351;&#24471;&#27169;&#22411;&#20855;&#26377;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#27604;&#20363;&#22270;&#20687;&#21512;&#25104;&#20026;&#21512;&#25104;&#22312;&#20219;&#24847;&#27604;&#20363;&#19979;&#21512;&#25104;&#36924;&#30495;&#22270;&#20687;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#36229;&#20986;&#20102;2K&#20998;&#36776;&#29575;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;&#35299;&#20915;&#26041;&#26696;&#36807;&#24230;&#20381;&#36182;&#20110;&#21367;&#31215;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#22312;&#32553;&#25918;&#36755;&#20986;&#20998;&#36776;&#29575;&#26102;&#20250;&#24341;&#20837;&#19981;&#19968;&#33268;&#24615;&#21644;&#8220;&#32441;&#29702;&#31896;&#36830;&#8221;&#38382;&#39064;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#22522;&#20110;INR&#30340;&#29983;&#25104;&#22120;&#20174;&#35774;&#35745;&#19978;&#26159;&#23610;&#24230;&#31561;&#21464;&#30340;&#65292;&#20294;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#32531;&#24930;&#30340;&#25512;&#29702;&#22952;&#30861;&#20102;&#36825;&#20123;&#32593;&#32476;&#22312;&#22823;&#35268;&#27169;&#25110;&#23454;&#26102;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;&#20855;&#26377;&#34892;&#21015;&#20132;&#38169;&#20687;&#32032;&#21512;&#25104;&#30340;&#21015;-&#34892;&#20132;&#38169;&#20687;&#32032;&#21512;&#25104;&#65288;$\textbf{CREPS}$&#65289;&#12290;&#19981;&#20351;&#29992;&#20219;&#20309;&#31354;&#38388;&#21367;&#31215;&#25110;&#20174;&#31895;&#21040;&#32454;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#33410;&#30465;&#20869;&#23384;&#21344;&#29992;&#24182;&#20351;&#31995;&#32479;&#21487;&#25193;&#23637;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32447;&#34920;&#31034;&#27861;&#65292;&#23558;&#23618;&#20869;&#29305;&#24449;&#22270;&#20998;&#35299;&#20026;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#29420;&#31435;&#8220;&#21402;&#26465;&#8221;&#25554;&#20540;&#36825;&#20123;&#26465;&#24102;&#30340;&#34701;&#21512;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;$\textbf{CREPS}$&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#20445;&#25345;&#26356;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#30340;&#21516;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any-scale image synthesis offers an efficient and scalable solution to synthesize photo-realistic images at any scale, even going beyond 2K resolution. However, existing GAN-based solutions depend excessively on convolutions and a hierarchical architecture, which introduce inconsistency and the $``$texture sticking$"$ issue when scaling the output resolution. From another perspective, INR-based generators are scale-equivariant by design, but their huge memory footprint and slow inference hinder these networks from being adopted in large-scale or real-time systems. In this work, we propose $\textbf{C}$olumn-$\textbf{R}$ow $\textbf{E}$ntangled $\textbf{P}$ixel $\textbf{S}$ynthesis ($\textbf{CREPS}$), a new generative model that is both efficient and scale-equivariant without using any spatial convolutions or coarse-to-fine design. To save memory footprint and make the system scalable, we employ a novel bi-line representation that decomposes layer-wise feature maps into separate $``$thick
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#23545;&#27604;&#23398;&#20064;&#30340;&#24494;&#35843;&#26694;&#26550;LRCLR&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#21307;&#23398;&#22270;&#20687;&#20013;&#26174;&#33879;&#30340;&#23616;&#37096;&#22270;&#20687;&#21306;&#22495;&#65292;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#35299;&#37322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.14153</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#35782;&#21035;&#30340;&#23616;&#37096;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Local Contrastive Learning for Medical Image Recognition. (arXiv:2303.14153v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#23545;&#27604;&#23398;&#20064;&#30340;&#24494;&#35843;&#26694;&#26550;LRCLR&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#21307;&#23398;&#22270;&#20687;&#20013;&#26174;&#33879;&#30340;&#23616;&#37096;&#22270;&#20687;&#21306;&#22495;&#65292;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#35299;&#37322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#30340;&#26222;&#21450;&#24050;&#32463;&#21019;&#36896;&#20102;&#23545;&#19987;&#23478;&#26631;&#35760;&#30340;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#24040;&#22823;&#38656;&#27714;&#12290;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#36890;&#36807;&#20174;&#30456;&#20851;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#33719;&#21462;&#30417;&#30563;&#26469;&#32531;&#35299;&#23545;&#19987;&#23478;&#26631;&#27880;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26694;&#26550;&#24448;&#24448;&#38590;&#20197;&#21306;&#20998;&#21307;&#23398;&#22270;&#20687;&#20013;&#19981;&#21516;&#30149;&#29702;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20013;&#30340;&#35768;&#22810;&#19981;&#25552;&#20379;&#22270;&#20687;&#21306;&#22495;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35299;&#37322;&#65292;&#20351;&#35786;&#26029;&#21307;&#24072;&#24456;&#38590;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#37096;&#23545;&#27604;&#23398;&#20064;&#65288;LRCLR&#65289;&#30340;&#28789;&#27963;&#24494;&#35843;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#28155;&#21152;&#20102;&#26174;&#30528;&#30340;&#22270;&#20687;&#21306;&#22495;&#36873;&#25321;&#23618;&#20197;&#21450;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#19968;&#20010;&#22806;&#37096;&#30340;&#33016;&#36879;&#39564;&#35777;&#38598;&#34920;&#26126;&#65292;LRCLR&#35782;&#21035;&#20102;&#26174;&#33879;&#30340;&#23616;&#37096;&#22270;&#20687;&#21306;&#22495;&#65292;&#24182;&#22312;&#25913;&#21892;&#20102;&#20960;&#39033;&#33016;&#36879;&#20998;&#31867;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#19982;&#25918;&#23556;&#23398;&#25991;&#26412;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Deep Learning (DL)-based methods for radiographic image analysis has created a great demand for expert-labeled radiology data. Recent self-supervised frameworks have alleviated the need for expert labeling by obtaining supervision from associated radiology reports. These frameworks, however, struggle to distinguish the subtle differences between different pathologies in medical images. Additionally, many of them do not provide interpretation between image regions and text, making it difficult for radiologists to assess model predictions. In this work, we propose Local Region Contrastive Learning (LRCLR), a flexible fine-tuning framework that adds layers for significant image region selection as well as cross-modality interaction. Our results on an external validation set of chest x-rays suggest that LRCLR identifies significant local image regions and provides meaningful interpretation against radiology text while improving zero-shot performance on several chest x-
&lt;/p&gt;</description></item><item><title>&#21452;&#37325;&#19979;&#38477;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#29616;&#35937;&#65292;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32500;&#24230;&#21644;&#27169;&#22411;&#21442;&#25968;&#26159;&#24433;&#21709;&#21452;&#37325;&#19979;&#38477;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#32773;&#25214;&#21040;&#20102;&#21046;&#36896;&#21452;&#37325;&#19979;&#38477;&#30340;&#19977;&#20010;&#35299;&#37322;&#24615;&#22240;&#32032;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#22240;&#32032;&#21487;&#20197;&#22312;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#38381;&#12290;</title><link>http://arxiv.org/abs/2303.14151</link><description>&lt;p&gt;
&#21452;&#37325;&#19979;&#38477;&#30340;&#35868;&#22242;&#65306;&#36776;&#35782;&#12289;&#35299;&#37322;&#21644;&#28040;&#35299;&#28145;&#24230;&#23398;&#20064;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Double Descent Demystified: Identifying, Interpreting &amp; Ablating the Sources of a Deep Learning Puzzle. (arXiv:2303.14151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14151
&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;&#19979;&#38477;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#29616;&#35937;&#65292;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32500;&#24230;&#21644;&#27169;&#22411;&#21442;&#25968;&#26159;&#24433;&#21709;&#21452;&#37325;&#19979;&#38477;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#32773;&#25214;&#21040;&#20102;&#21046;&#36896;&#21452;&#37325;&#19979;&#38477;&#30340;&#19977;&#20010;&#35299;&#37322;&#24615;&#22240;&#32032;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#22240;&#32032;&#21487;&#20197;&#22312;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#38381;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;&#19979;&#38477;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#29616;&#35937;&#65292;&#21363;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30456;&#23545;&#20110;&#25968;&#25454;&#37327;&#30340;&#22686;&#38271;&#65292;&#27979;&#35797;&#35823;&#24046;&#22312;&#27169;&#22411;&#19981;&#26029;&#25193;&#22823;&#32780;&#36827;&#20837;&#39640;&#24230;&#36229;&#21442;&#25968;&#21270;&#65288;&#25968;&#25454;&#26410;&#20805;&#20998;&#37319;&#26679;&#65289;&#38454;&#27573;&#26102;&#19979;&#38477;&#12290;&#27979;&#35797;&#35823;&#24046;&#19979;&#38477;&#30340;&#36825;&#31181;&#24773;&#20917;&#19982;&#20256;&#32479;&#30340;&#20851;&#20110;&#36807;&#24230;&#25311;&#21512;&#30340;&#23398;&#20064;&#29702;&#35770;&#30456;&#24726;&#65292;&#21487;&#35859;&#25215;&#36733;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#22411;&#27169;&#22411;&#30340;&#25104;&#21151;&#12290;&#36825;&#31181;&#38750;&#21333;&#35843;&#30340;&#27979;&#35797;&#35823;&#24046;&#21464;&#21270;&#34892;&#20026;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25968;&#25454;&#30340;&#32500;&#24230;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31616;&#35201;&#25551;&#36848;&#20102;&#21452;&#37325;&#19979;&#38477;&#29616;&#35937;&#65292;&#28982;&#21518;&#29992;&#26131;&#20110;&#29702;&#35299;&#21644;&#25509;&#21463;&#30340;&#26041;&#24335;&#23545;&#20026;&#20309;&#20986;&#29616;&#21452;&#37325;&#19979;&#38477;&#36827;&#34892;&#20102;&#35299;&#37322;&#65292;&#21482;&#38656;&#35201;&#20102;&#35299;&#32447;&#24615;&#20195;&#25968;&#21644;&#27010;&#29575;&#35770;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#39033;&#24335;&#22238;&#24402;&#25552;&#20379;&#21487;&#35270;&#21270;&#30340;&#30452;&#35266;&#24863;&#21463;&#65292;&#28982;&#21518;&#36890;&#36807;&#26222;&#36890;&#32447;&#24615;&#22238;&#24402;&#25968;&#23398;&#20998;&#26512;&#21452;&#37325;&#19979;&#38477;&#65292;&#30830;&#23450;&#20102;&#19977;&#20010;&#35299;&#37322;&#24615;&#22240;&#32032;&#65292;&#24403;&#21516;&#26102;&#23384;&#22312;&#26102;&#65292;&#21487;&#20197;&#20849;&#21516;&#21046;&#36896;&#21452;&#37325;&#19979;&#38477;&#29616;&#35937;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#34987;&#20851;&#38381;&#22312;&#36755;&#20986;&#21333;&#20010;&#26631;&#37327;&#30340;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#28040;&#34701;&#23454;&#39564;&#27979;&#35797;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#21452;&#37325;&#19979;&#38477;&#29616;&#35937;&#19981;&#20165;&#20165;&#26159;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#25152;&#29305;&#26377;&#30340;&#65292;&#20063;&#20986;&#29616;&#22312;&#27424;&#21442;&#25968;&#21270;&#27169;&#22411;&#21644;&#25554;&#20540;&#38408;&#20540;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double des
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CIFAKE&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#23558;&#30495;&#23454;&#29031;&#29255;&#19982;AI&#29983;&#25104;&#22270;&#20687;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.14126</link><description>&lt;p&gt;
CIFAKE: AI&#29983;&#25104;&#22270;&#20687;&#30340;&#20998;&#31867;&#21644;&#21487;&#35299;&#37322;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. (arXiv:2303.14126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CIFAKE&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#23558;&#30495;&#23454;&#29031;&#29255;&#19982;AI&#29983;&#25104;&#22270;&#20687;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21512;&#25104;&#25968;&#25454;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;&#29983;&#25104;&#30340;&#22270;&#20687;&#36136;&#37327;&#22914;&#27492;&#20043;&#39640;&#65292;&#20197;&#33267;&#20110;&#20154;&#31867;&#26080;&#27861;&#21306;&#20998;&#30495;&#23454;&#29031;&#29255;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#37492;&#20110;&#25968;&#25454;&#21487;&#38752;&#24615;&#21644;&#35748;&#35777;&#30340;&#33267;&#20851;&#37325;&#35201;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#22686;&#24378;&#25105;&#20204;&#35782;&#21035;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19982;&#24050;&#26377;&#30340;CIFAR-10&#25968;&#25454;&#38598;&#20013;&#30340;&#21313;&#20010;&#31867;&#21035;&#30456;&#20284;&#65292;&#25552;&#20379;&#19982;&#30495;&#23454;&#29031;&#29255;&#23545;&#27604;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#30340;&#35270;&#35273;&#23646;&#24615;&#65292;&#20363;&#22914;&#27700;&#20013;&#36924;&#30495;&#30340;&#21453;&#23556;&#12290;&#36825;&#20004;&#32452;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#21363;&#29031;&#29255;&#26159;&#30495;&#23454;&#30340;&#36824;&#26159;&#30001;AI&#29983;&#25104;&#30340;&#12290;&#26412;&#30740;&#31350;&#38543;&#21518;&#25552;&#20986;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23558;&#22270;&#20687;&#20998;&#31867;&#20026;&#20004;&#20010;&#31867;&#21035;&#65306;&#30495;&#23454;&#25110;&#20266;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent technological advances in synthetic data have enabled the generation of images with such high quality that human beings cannot tell the difference between real-life photographs and Artificial Intelligence (AI) generated images. Given the critical necessity of data reliability and authentication, this article proposes to enhance our ability to recognise AI-generated images through computer vision. Initially, a synthetic dataset is generated that mirrors the ten classes of the already available CIFAR-10 dataset with latent diffusion which provides a contrasting set of images for comparison to real photographs. The model is capable of generating complex visual attributes, such as photorealistic reflections in water. The two sets of data present as a binary classification problem with regard to whether the photograph is real or generated by AI. This study then proposes the use of a Convolutional Neural Network (CNN) to classify the images into two categories; Real or Fake. Following
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20316;&#32773;&#20851;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20013;&#30340;&#28508;&#21147;&#12290;&#36825;&#26159;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#32780;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14116</link><description>&lt;p&gt;
&#20174;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#35270;&#35282;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65306;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives. (arXiv:2303.14116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14116
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20316;&#32773;&#20851;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20013;&#30340;&#28508;&#21147;&#12290;&#36825;&#26159;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#32780;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20851;&#27880;&#20110;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#65292;&#28085;&#30422;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#20855;&#20307;&#39044;&#27979;&#36807;&#31243;&#20173;&#38590;&#20197;&#35299;&#37322;&#21644;&#35828;&#26126;&#65292;&#36825;&#34987;&#31216;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#21270;&#65292;&#24182;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;&#19968;&#20010;&#29305;&#21035;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#21046;&#36896;&#19994;&#12289;&#21830;&#19994;&#12289;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#34892;&#19994;&#31561;&#26222;&#36941;&#20351;&#29992;&#35813;&#25216;&#26415;&#65292;&#20197;&#21450;&#21307;&#23398;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26412;&#25991;&#22522;&#20110;&#20316;&#32773;&#35770;&#25991;&#30340;&#25688;&#35201;&#65292;&#35813;&#35770;&#25991;&#30340;&#26680;&#24515;&#30740;&#31350;&#20851;&#27880;&#20110;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22522;&#30784;&#30740;&#31350;&#21644;&#24212;&#29992;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the dramatic advances in deep learning technology, machine learning research is focusing on improving the interpretability of model predictions as well as prediction performance in both basic and applied research. While deep learning models have much higher prediction performance than traditional machine learning models, the specific prediction process is still difficult to interpret and/or explain. This is known as the black-boxing of machine learning models and is recognized as a particularly important problem in a wide range of research fields, including manufacturing, commerce, robotics, and other industries where the use of such technology has become commonplace, as well as the medical field, where mistakes are not tolerated. This bulletin is based on the summary of the author's dissertation. The research summarized in the dissertation focuses on the attention mechanism, which has been the focus of much attention in recent years, and discusses its potential for both basic res
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23398;&#20064;&#26377;&#38480;&#33258;&#21160;&#26426;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#21644;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14111</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#20248;&#21270;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Anomaly Detection via Discrete Optimization. (arXiv:2303.14111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23398;&#20064;&#26377;&#38480;&#33258;&#21160;&#26426;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#21644;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20363;&#22914;&#32593;&#32476;&#23433;&#20840;&#12289;&#25191;&#27861;&#12289;&#21307;&#23398;&#21644;&#27450;&#35784;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#24448;&#24448;&#38590;&#20197;&#29702;&#35299;&#65292;&#36825;&#36890;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#32473;&#23450;&#30340;&#26410;&#26631;&#35760;&#24207;&#21015;&#22810;&#37325;&#38598;&#20013;&#23398;&#20064;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426; &#65288;DFA&#65289;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;&#35745;&#31639;&#38590;&#39064;&#65292;&#24182;&#22522;&#20110;&#32422;&#26463;&#20248;&#21270;&#24320;&#21457;&#20102;&#20004;&#20010;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20248;&#21270;&#38382;&#39064;&#24341;&#20837;&#20102;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#25105;&#20204;&#30340;DFA&#30340;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21407;&#22411;&#23454;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is essential in many application domains, such as cyber security, law enforcement, medicine, and fraud protection. However, the decision-making of current deep learning approaches is notoriously hard to understand, which often limits their practical applicability. To overcome this limitation, we propose a framework for learning inherently interpretable anomaly detectors from sequential data. More specifically, we consider the task of learning a deterministic finite automaton (DFA) from a given multi-set of unlabeled sequences. We show that this problem is computationally hard and develop two learning algorithms based on constraint optimization. Moreover, we introduce novel regularization schemes for our optimization problems that improve the overall interpretability of our DFAs. Using a prototype implementation, we demonstrate that our approach shows promising results in terms of accuracy and F1 score.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#25239;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#38024;&#23545;&#25968;&#25454;&#22312;&#33719;&#21462;&#26041;&#24335;&#19978;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#24046;&#65292;&#25193;&#23637;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#31181;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#21367;&#31215;&#21644;&#21464;&#21387;&#22120;&#30340;&#28151;&#21512;&#21028;&#21035;-&#29983;&#25104;&#35757;&#32451;&#65292;&#22312;&#19981;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14096</link><description>&lt;p&gt;
&#36890;&#36807;&#25200;&#21160;&#22686;&#24191;&#20449;&#24687;&#29942;&#39048;&#65292;&#25552;&#21319;&#22810;&#37325;&#21487;&#38752;&#24615;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multiple Reliability Measures via Nuisance-extended Information Bottleneck. (arXiv:2303.14096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#25239;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#38024;&#23545;&#25968;&#25454;&#22312;&#33719;&#21462;&#26041;&#24335;&#19978;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#24046;&#65292;&#25193;&#23637;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#31181;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#21367;&#31215;&#21644;&#21464;&#21387;&#22120;&#30340;&#28151;&#21512;&#21028;&#21035;-&#29983;&#25104;&#35757;&#32451;&#65292;&#22312;&#19981;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#35757;&#32451;&#26377;&#38480;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#35768;&#22810;&#39044;&#27979;&#20449;&#21495;&#24448;&#24448;&#26469;&#33258;&#20110;&#25968;&#25454;&#33719;&#21462;&#30340;&#26576;&#20123;&#20559;&#24046;&#65288;&#21363;&#19981;&#22815;&#20855;&#26377;&#19968;&#33324;&#24615;&#65289;&#65292;&#22240;&#27492;&#19981;&#33021;&#38450;&#27490;&#27169;&#22411;&#22312;&#36825;&#20123;&#65288;&#25152;&#35859;&#30340;&#65289;&#8220;&#24555;&#25463;&#8221;&#20449;&#21495;&#19978;&#36827;&#34892;&#20849;&#36866;&#24212;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#22312;&#21508;&#31181;&#20998;&#24067;&#36716;&#31227;&#26041;&#38754;&#21464;&#24471;&#33030;&#24369;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#22833;&#25928;&#27169;&#24335;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#25239;&#23041;&#32961;&#27169;&#22411;&#65292;&#20197;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35757;&#32451;&#25200;&#21160;&#31867;&#21035;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#26631;&#20934;&#20449;&#24687;&#29942;&#39048;&#25193;&#23637;&#21040;&#21478;&#22806;&#27169;&#25311;&#26377;&#23475;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#35757;&#32451;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20197;&#21450;&#23454;&#29992;&#30340;&#32534;&#30721;&#22120;&#35774;&#35745;&#65292;&#20197;&#20415;&#20110;&#22522;&#20110;&#21367;&#31215;&#21644;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#30340;&#28151;&#21512;&#21028;&#21035;-&#29983;&#25104;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#23398;&#34920;&#24449;&#30340;&#40065;&#26834;&#24615;&#65288;&#26174;&#33879;&#22320;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical scenarios where training data is limited, many predictive signals in the data can be rather from some biases in data acquisition (i.e., less generalizable), so that one cannot prevent a model from co-adapting on such (so-called) "shortcut" signals: this makes the model fragile in various distribution shifts. To bypass such failure modes, we consider an adversarial threat model under a mutual information constraint to cover a wider class of perturbations in training. This motivates us to extend the standard information bottleneck to additionally model the nuisance information. We propose an autoencoder-based training to implement the objective, as well as practical encoder designs to facilitate the proposed hybrid discriminative-generative training concerning both convolutional- and Transformer-based architectures. Our experimental results show that the proposed scheme improves robustness of learned representations (remarkably without using any domain-specific knowledge), w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#23558;&#29702;&#35770;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#24182;&#32467;&#21512;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#37325;&#23376;&#25955;&#23556;&#30340;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.14090</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#21033;&#29992;&#26263;&#29289;&#36136;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter. (arXiv:2303.14090v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#23558;&#29702;&#35770;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#24182;&#32467;&#21512;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#37325;&#23376;&#25955;&#23556;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#21512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23558;&#32479;&#35745;&#27169;&#24335;&#19982;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#20854;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#24050;&#30693;&#20851;&#31995;&#26469;&#20016;&#23500;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#20197;&#38480;&#21046;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#12290;&#27700;&#21160;&#21147;&#23398;&#27169;&#25311;&#26159;&#29616;&#20195;&#23431;&#23449;&#23398;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#32780;&#25152;&#38656;&#30340;&#35745;&#31639;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24555;&#36895;&#27169;&#25311;&#26263;&#29289;&#36136;&#38656;&#35201;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25104;&#20026;&#30740;&#31350;&#30340;&#19968;&#20010;&#27963;&#36291;&#39046;&#22495;;&#22312;&#36825;&#37324;&#65292;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#21457;&#29616;&#30340;&#25955;&#23556;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#21644;&#29289;&#29702;&#32422;&#26463;&#65292;&#23558;&#20851;&#20110;&#37325;&#23376;&#36716;&#21270;&#25928;&#29575;&#30340;&#29702;&#35770;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#22522;&#20110;&#32467;&#26524;&#22270;&#20687;&#20013;&#21160;&#21147;&#23398;&#21151;&#29575;&#35889;&#20013;&#30340;&#35823;&#24046;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#37327;&#21270;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks have emerged as a coherent framework for building predictive models that combine statistical patterns with domain knowledge. The underlying notion is to enrich the optimization loss function with known relationships to constrain the space of possible solutions. Hydrodynamic simulations are a core constituent of modern cosmology, while the required computations are both expensive and time-consuming. At the same time, the comparatively fast simulation of dark matter requires fewer resources, which has led to the emergence of machine learning algorithms for baryon inpainting as an active area of research; here, recreating the scatter found in hydrodynamic simulations is an ongoing challenge. This paper presents the first application of physics-informed neural networks to baryon inpainting by combining advances in neural network architectures with physical constraints, injecting theory on baryon conversion efficiency into the model loss function. We also in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#20855;&#26377;&#26174;&#24335;&#35823;&#24046;&#30028;&#38480;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25511;&#21046;&#31639;&#27861;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.14084</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Control. (arXiv:2303.14084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#20855;&#26377;&#26174;&#24335;&#35823;&#24046;&#30028;&#38480;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25511;&#21046;&#31639;&#27861;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25511;&#21046;&#26159;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#24037;&#20855;&#65292;&#29992;&#20110;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#23545;&#29031;&#25968;&#25454;&#26469;&#20272;&#35745;&#24178;&#39044;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#26469;&#33258;&#20854;&#20182;&#30456;&#20284;&#35266;&#23519;&#65288;&#21363;&#65292;&#25424;&#36192;&#32773;&#27744;&#65289;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#36890;&#36807;&#20998;&#26512;&#24178;&#39044;&#21069;&#30446;&#26631;&#21644;&#25424;&#36192;&#32773;&#27744;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#39044;&#27979;&#24863;&#20852;&#36259;&#30340;&#21453;&#20107;&#23454;&#26102;&#38388;&#24207;&#21015;&#65288;&#21363;&#65292;&#30446;&#26631;&#21333;&#20803;&#65289;&#12290;&#38543;&#30528;&#21512;&#25104;&#25511;&#21046;&#24037;&#20855;&#34987;&#36234;&#26469;&#36234;&#24212;&#29992;&#20110;&#25935;&#24863;&#25110;&#19987;&#26377;&#25968;&#25454;&#65292;&#24418;&#24335;&#21270;&#30340;&#38544;&#31169;&#20445;&#25252;&#36890;&#24120;&#26159;&#24517;&#38656;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#20855;&#26377;&#26174;&#24335;&#35823;&#24046;&#30028;&#38480;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25511;&#21046;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38750;&#31169;&#26377;&#21512;&#25104;&#25511;&#21046;&#21644;&#24046;&#20998;&#38544;&#31169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#20851;&#20110;&#21512;&#25104;&#25511;&#21046;&#26597;&#35810;&#25935;&#24863;&#24615;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#25105;&#20204;&#30340;&#31169;&#26377;&#21512;&#25104;&#25511;&#21046;&#31639;&#27861;&#20934;&#30830;&#24615;&#30340;&#26174;&#24335;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic control is a causal inference tool used to estimate the treatment effects of an intervention by creating synthetic counterfactual data. This approach combines measurements from other similar observations (i.e., donor pool ) to predict a counterfactual time series of interest (i.e., target unit) by analyzing the relationship between the target and the donor pool before the intervention. As synthetic control tools are increasingly applied to sensitive or proprietary data, formal privacy protections are often required. In this work, we provide the first algorithms for differentially private synthetic control with explicit error bounds. Our approach builds upon tools from non-private synthetic control and differentially private empirical risk minimization. We provide upper and lower bounds on the sensitivity of the synthetic control query and provide explicit error bounds on the accuracy of our private synthetic control algorithms. We show that our algorithms produce accurate pre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#23436;&#32654;&#27867;&#21270;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.14083</link><description>&lt;p&gt;
&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#19979;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning for the Random Feature Model in the Student-Teacher Framework. (arXiv:2303.14083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#23436;&#32654;&#27867;&#21270;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#38543;&#30528;&#26435;&#37325;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20854;&#24615;&#33021;&#36890;&#24120;&#20250;&#25552;&#39640;&#65292;&#23548;&#33268;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31532;&#19968;&#23618;&#26159;&#20923;&#32467;&#30340;&#65292;&#32780;&#26368;&#21518;&#19968;&#23618;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#31216;&#20026;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#36890;&#36807;&#23548;&#20986;&#19968;&#32452;&#23398;&#20064;&#21160;&#24577;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#65292;&#24182;&#35745;&#31639;&#38750;&#38646;&#28176;&#36817;&#27867;&#21270;&#35823;&#24046;&#12290;&#21482;&#26377;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#25165;&#26377;&#21487;&#33021;&#23454;&#29616;&#23436;&#32654;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are widely used prediction algorithms whose performance often improves as the number of weights increases, leading to over-parametrization. We consider a two-layered neural network whose first layer is frozen while the last layer is trainable, known as the random feature model. We study over-parametrization in the context of a student-teacher framework by deriving a set of differential equations for the learning dynamics. For any finite ratio of hidden layer size and input dimension, the student cannot generalize perfectly, and we compute the non-zero asymptotic generalization error. Only when the student's hidden layer size is exponentially larger than the input dimension, an approach to perfect generalization is possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;(Instance-adaptive Adversarial Training, IAAT)&#36890;&#36807;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#30340;&#26368;&#26032;&#12289;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14077</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#20363;&#32423;&#25439;&#22833;&#24179;&#28369;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (arXiv:2303.14077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;(Instance-adaptive Adversarial Training, IAAT)&#36890;&#36807;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#30340;&#26368;&#26032;&#12289;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#23545;&#25239;&#25200;&#21160;&#65306;&#21363;&#20154;&#31867;&#38590;&#20197;&#23519;&#35273;&#30340;&#20154;&#36896;&#22122;&#22768;&#65292;&#21487;&#20197;&#36731;&#26131;&#22320;&#36855;&#24785;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#32780;&#20570;&#20986;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#30446;&#21069;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#26368;&#25104;&#21151;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#20197;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#20174;&#23454;&#20363;&#32423;&#21035;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#23545;&#25239;&#35757;&#32451;&#26399;&#38388;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#28436;&#21464;&#12290;&#21457;&#29616;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#36890;&#36807;&#29306;&#29298;&#30456;&#24403;&#27604;&#20363;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#24615;&#25439;&#22833;&#30340;&#25972;&#20307;&#38477;&#20302;&#65292;&#36825;&#23548;&#33268;&#20102;&#19981;&#21516;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#20998;&#24067;&#19981;&#22343;&#34913;&#12290;&#36825;&#31181;&#8220;&#19981;&#22343;&#34913;&#33030;&#24369;&#24615;&#8221;&#22312;&#20960;&#31181;&#27969;&#34892;&#30340;&#40065;&#26834;&#24615;&#35757;&#32451;&#26041;&#27861;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#19982;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#36807;&#25311;&#21512;&#30456;&#20851;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65306;Instance-adaptive Adversarial Training (IAAT)&#12290;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#12290;&#26412;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks can be easily fooled into making incorrect predictions through corruption of the input by adversarial perturbations: human-imperceptible artificial noise. So far adversarial training has been the most successful defense against such adversarial attacks. This work focuses on improving adversarial training to boost adversarial robustness. We first analyze, from an instance-wise perspective, how adversarial vulnerability evolves during adversarial training. We find that during training an overall reduction of adversarial loss is achieved by sacrificing a considerable proportion of training samples to be more vulnerable to adversarial attack, which results in an uneven distribution of adversarial vulnerability among data. Such "uneven vulnerability", is prevalent across several popular robust training methods and, more importantly, relates to overfitting in adversarial training. Motivated by this observation, we propose a new adversarial training method: Instance-adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;1D CNN-LSTM&#32467;&#26500;&#30340;&#33337;&#33334;&#36712;&#36857;&#20851;&#32852;&#31639;&#27861;&#65292;&#37319;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#26469;&#35299;&#20915;</title><link>http://arxiv.org/abs/2303.14068</link><description>&lt;p&gt;
&#22522;&#20110;CNN-LSTM&#26550;&#26500;&#30340; AIS &#25968;&#25454;&#33337;&#33334;&#36712;&#36857;&#20851;&#32852;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A CNN-LSTM Architecture for Marine Vessel Track Association Using Automatic Identification System (AIS) Data. (arXiv:2303.14068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;1D CNN-LSTM&#32467;&#26500;&#30340;&#33337;&#33334;&#36712;&#36857;&#20851;&#32852;&#31639;&#27861;&#65292;&#37319;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#26469;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28023;&#27915;&#30417;&#27979;&#20013;&#65292;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#33337;&#33334;&#36816;&#21160;&#27169;&#24335;&#23545;&#20110;&#21450;&#26102;&#35782;&#21035;&#28508;&#22312;&#23041;&#32961;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#35813;&#30446;&#26631;&#65292;&#38656;&#35201;&#20351;&#29992;&#36712;&#36857;&#20851;&#32852;&#31639;&#27861;&#65292;&#23558;&#30001;&#36816;&#21160;&#21442;&#25968;&#32452;&#25104;&#30340;&#26102;&#24207;&#35266;&#27979;&#32467;&#26524;&#19982;&#30456;&#24212;&#30340;&#33337;&#21482;&#20851;&#32852;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;1D CNN-LSTM&#32467;&#26500;&#30340;&#36712;&#36857;&#20851;&#32852;&#26694;&#26550;&#65292;&#23558;&#36825;&#19968;&#36861;&#36394;&#20219;&#21153;&#35270;&#20026;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
In marine surveillance, distinguishing between normal and anomalous vessel movement patterns is critical for identifying potential threats in a timely manner. Once detected, it is important to monitor and track these vessels until a necessary intervention occurs. To achieve this, track association algorithms are used, which take sequential observations comprising geological and motion parameters of the vessels and associate them with respective vessels. The spatial and temporal variations inherent in these sequential observations make the association task challenging for traditional multi-object tracking algorithms. Additionally, the presence of overlapping tracks and missing data can further complicate the trajectory tracking process. To address these challenges, in this study, we approach this tracking task as a multivariate time series problem and introduce a 1D CNN-LSTM architecture-based framework for track association. This special neural network architecture can capture the spat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#28857;&#36816;&#31639;&#27431;&#25289;&#29305;&#24449;&#24471;&#21040;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#65292;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#27431;&#25289;&#36718;&#24275;&#21450;&#20854;&#28151;&#21512;&#21464;&#25442;&#25429;&#25417;&#20449;&#24687;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14040</link><description>&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#27431;&#25289;&#29305;&#24449;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Euler Characteristic Tools For Topological Data Analysis. (arXiv:2303.14040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#28857;&#36816;&#31639;&#27431;&#25289;&#29305;&#24449;&#24471;&#21040;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#65292;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#27431;&#25289;&#36718;&#24275;&#21450;&#20854;&#28151;&#21512;&#21464;&#25442;&#25429;&#25417;&#20449;&#24687;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#12290;&#20174;&#25968;&#25454;&#26500;&#24314;&#30340;&#19968;&#26063;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#28857;&#36816;&#31639;&#27431;&#25289;&#29305;&#24449;&#65292;&#24471;&#21040;&#25152;&#35859;&#30340;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#25551;&#36848;&#31526;&#20197;&#26497;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#22312;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#21463;&#20449;&#21495;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35745;&#31639;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#30340;&#28151;&#21512;&#21464;&#25442;&#12290;&#36825;&#20123;&#31215;&#20998;&#21464;&#25442;&#23558;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#19982;&#21202;&#36125;&#26684;&#31215;&#20998;&#28151;&#21512;&#65292;&#25552;&#20379;&#39640;&#25928;&#21387;&#32553;&#25299;&#25169;&#20449;&#21495;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#23450;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27431;&#25289;&#36718;&#24275;&#21450;&#20854;&#28151;&#21512;&#21464;&#25442;&#25152;&#25429;&#25417;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#30340;&#20247;&#22810;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#25551;&#36848;&#31526;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#20197;&#21450;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we study Euler characteristic techniques in topological data analysis. Pointwise computing the Euler characteristic of a family of simplicial complexes built from data gives rise to the so-called Euler characteristic profile. We show that this simple descriptor achieve state-of-the-art performance in supervised tasks at a very low computational cost. Inspired by signal analysis, we compute hybrid transforms of Euler characteristic profiles. These integral transforms mix Euler characteristic techniques with Lebesgue integration to provide highly efficient compressors of topological signals. As a consequence, they show remarkable performances in unsupervised settings. On the qualitative side, we provide numerous heuristics on the topological and geometric information captured by Euler profiles and their hybrid transforms. Finally, we prove stability results for these descriptors as well as asymptotic guarantees in random settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PENTACET&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;2300&#19975;&#20010;&#19978;&#19979;&#25991;&#20195;&#30721;&#27880;&#37322;&#21644;50&#19975;&#20010;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#27880;&#37322;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#23558;&#36827;&#19968;&#27493;&#25512;&#21160;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;SATD&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.14029</link><description>&lt;p&gt;
PENTACET&#25968;&#25454;&#8212;&#8212;2300&#19975;&#20010;&#19978;&#19979;&#25991;&#20195;&#30721;&#27880;&#37322;&#21644;50&#19975;&#20010;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
PENTACET data -- 23 Million Contextual Code Comments and 500,000 SATD comments. (arXiv:2303.14029v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PENTACET&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;2300&#19975;&#20010;&#19978;&#19979;&#25991;&#20195;&#30721;&#27880;&#37322;&#21644;50&#19975;&#20010;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#27880;&#37322;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#23558;&#36827;&#19968;&#27493;&#25512;&#21160;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;SATD&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#65288;SATD&#65289;&#30740;&#31350;&#20351;&#29992;&#22914;&#8220;TODO&#8221;&#21644;&#8220;FIXME&#8221;&#20043;&#31867;&#30340;&#26174;&#24335;SATD&#29305;&#24449;&#36827;&#34892;SATD&#26816;&#27979;&#12290;&#26356;&#20180;&#32454;&#22320;&#35266;&#23519;&#21457;&#29616;&#65292;&#19968;&#20123;SATD&#30740;&#31350;&#20351;&#29992;&#31616;&#21333;&#30340;SATD&#65288;&#8220;&#26131;&#20110;&#21457;&#29616;&#30340;&#8221;&#65289;&#20195;&#30721;&#27880;&#37322;&#32780;&#27809;&#26377;&#19978;&#19979;&#25991;&#25968;&#25454;&#65288;&#21069;&#25991;&#21644;&#21518;&#25991;&#28304;&#20195;&#30721;&#19978;&#19979;&#25991;&#65289;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;PENTACET&#65288;&#25110;5C&#25968;&#25454;&#38598;&#65289;&#25968;&#25454;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;PENTACET&#26159;&#19968;&#20010;&#30001;&#36129;&#29486;&#32773;&#36827;&#34892;&#31579;&#36873;&#30340;&#22823;&#22411;&#19978;&#19979;&#25991;&#20195;&#30721;&#27880;&#37322;&#25968;&#25454;&#24211;&#65292;&#26159;&#26368;&#20840;&#38754;&#30340;SATD&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;9,096&#20010;&#24320;&#28304;&#36719;&#20214;Java&#39033;&#30446;&#20013;&#25366;&#25496;&#20102;&#24635;&#20849;435&#30334;&#19975;&#34892;&#20195;&#30721;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;2300&#19975;&#20010;&#20195;&#30721;&#27880;&#37322;&#65292;&#24182;&#20026;&#27599;&#20010;&#27880;&#37322;&#25552;&#20379;&#20102;&#21069;&#21518;&#28304;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;50&#19975;&#20010;&#34987;&#26631;&#35760;&#20026;SATD&#30340;&#27880;&#37322;&#65292;&#21253;&#25324;&#8220;&#26131;&#20110;&#21457;&#29616;&#30340;&#8221;&#21644;&#8220;&#38590;&#20110;&#21457;&#29616;&#30340;&#8221; SATD&#12290;&#25105;&#20204;&#30456;&#20449;PENTACET&#25968;&#25454;&#38598;&#23558;&#36827;&#19968;&#27493;&#25512;&#21160;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;SATD&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Self-Admitted Technical Debt (SATD) research utilizes explicit SATD features such as 'TODO' and 'FIXME' for SATD detection. A closer look reveals several SATD research uses simple SATD ('Easy to Find') code comments without the contextual data (preceding and succeeding source code context). This work addresses this gap through PENTACET (or 5C dataset) data. PENTACET is a large Curated Contextual Code Comments per Contributor and the most extensive SATD data. We mine 9,096 Open Source Software Java projects with a total of 435 million LOC. The outcome is a dataset with 23 million code comments, preceding and succeeding source code context for each comment, and more than 500,000 comments labeled as SATD, including both 'Easy to Find' and 'Hard to Find' SATD. We believe PENTACET data will further SATD research using Artificial Intelligence techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ASTRA-sim2.0&#65292;&#19968;&#31181;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#25311;&#22522;&#30784;&#35774;&#26045;&#65292;&#21487;&#27169;&#25311;&#24403;&#20195;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#22411;&#21644;&#24179;&#21488;&#12290;ASTRA-sim2.0&#26131;&#20110;&#20351;&#29992;&#12289;&#28789;&#27963;&#65292;&#25903;&#25345;&#20219;&#24847;&#27169;&#22411;&#24182;&#34892;&#21270;&#31574;&#30053;&#12289;&#22810;&#32500;&#24322;&#26500;&#25299;&#25169;&#12289;&#20219;&#24847;&#20998;&#35299;&#20869;&#23384;&#31995;&#32479;&#21644;&#31934;&#32454;&#30340;&#20202;&#22120;&#21644;&#20107;&#20214;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2303.14006</link><description>&lt;p&gt;
ASTRA-sim2.0: &#27169;&#25311;&#20998;&#23618;&#32593;&#32476;&#21644;&#20998;&#35299;&#31995;&#32479;&#65292;&#23454;&#29616;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#30340;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale. (arXiv:2303.14006v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ASTRA-sim2.0&#65292;&#19968;&#31181;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#25311;&#22522;&#30784;&#35774;&#26045;&#65292;&#21487;&#27169;&#25311;&#24403;&#20195;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#22411;&#21644;&#24179;&#21488;&#12290;ASTRA-sim2.0&#26131;&#20110;&#20351;&#29992;&#12289;&#28789;&#27963;&#65292;&#25903;&#25345;&#20219;&#24847;&#27169;&#22411;&#24182;&#34892;&#21270;&#31574;&#30053;&#12289;&#22810;&#32500;&#24322;&#26500;&#25299;&#25169;&#12289;&#20219;&#24847;&#20998;&#35299;&#20869;&#23384;&#31995;&#32479;&#21644;&#31934;&#32454;&#30340;&#20202;&#22120;&#21644;&#20107;&#20214;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#36755;&#20837;&#25968;&#25454;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#25193;&#23637;&#65292;&#37319;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#24179;&#21488;&#26469;&#36866;&#24212;&#27169;&#22411;&#24182;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#22914;&#26230;&#22278;&#32423;&#33410;&#28857;&#12289;&#22810;&#32500;&#32593;&#32476;&#25299;&#25169;&#12289;&#20998;&#35299;&#24335;&#20869;&#23384;&#31995;&#32479;&#21644;&#24182;&#34892;&#21270;&#31574;&#30053;&#65292;&#24050;&#32463;&#34987;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#31995;&#32479;&#31215;&#26497;&#37319;&#29992;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#36719;&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#22534;&#26632;&#65292;&#24182;&#38656;&#35201;&#24314;&#31435;&#19968;&#20010;&#27169;&#25311;&#22522;&#30784;&#35774;&#26045;&#20197;&#36827;&#34892;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;&#24320;&#28304;&#30340; ASTRA-sim &#22522;&#30784;&#35774;&#26045;&#19978;&#25193;&#23637;&#20102;&#21151;&#33021;&#65292;&#24182;&#36171;&#20104;&#20854;&#27169;&#25311;&#24403;&#20195;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#22411;&#21644;&#24179;&#21488;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;(i) &#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#22270;&#30340;&#35757;&#32451;&#24490;&#29615;&#23454;&#29616;&#20102;&#23545;&#20219;&#24847;&#27169;&#22411;&#24182;&#34892;&#21270;&#31574;&#30053;&#30340;&#25903;&#25345;&#65292;(ii) &#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#22810;&#32500;&#24322;&#26500;&#25299;&#25169;&#29983;&#25104;&#27169;&#22359;&#65292;(iii) &#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#23618;&#20869;&#23384;&#25277;&#35937;&#25509;&#21475;&#65292;&#25903;&#25345;&#20219;&#24847;&#20998;&#35299;&#20869;&#23384;&#31995;&#32479;&#65292;(iv) &#25105;&#20204;&#21551;&#29992;&#20102;&#31934;&#32454;&#30340;&#20202;&#22120;&#21644;&#20107;&#20214;&#36319;&#36394;&#12290;&#24471;&#21040;&#30340;&#24314;&#27169;&#22522;&#30784;&#35774;&#26045;&#65292;&#31216;&#20026; ASTRA-sim2.0&#65292;&#26131;&#20110;&#20351;&#29992;&#19988;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#21508;&#31181;&#20998;&#24067;&#24335;&#35757;&#32451;&#31995;&#32479;&#65292;&#21253;&#25324;&#21033;&#29992;&#26032;&#20852;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;&#22914;NVM&#35774;&#22791;&#21644;&#31232;&#30095;&#30697;&#38453;&#21152;&#36895;&#22120;&#65289;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models and input data are scaling at an unprecedented rate, it is inevitable to move towards distributed training platforms to fit the model and increase training throughput. State-of-the-art approaches and techniques, such as wafer-scale nodes, multi-dimensional network topologies, disaggregated memory systems, and parallelization strategies, have been actively adopted by emerging distributed training systems. This results in a complex SW/HW co-design stack of distributed training, necessitating a modeling/simulation infrastructure for design-space exploration. In this paper, we extend the open-source ASTRA-sim infrastructure and endow it with the capabilities to model state-of-the-art and emerging distributed training models and platforms. More specifically, (i) we enable ASTRA-sim to support arbitrary model parallelization strategies via a graph-based training-loop implementation, (ii) we implement a parameterizable multi-dimensional heterogeneous topology generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#23558;&#22797;&#26434;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#27169;&#22411;&#19978;&#65292;&#20174;&#32780;&#20351;&#20302;&#20869;&#23384;&#35774;&#22791;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#32570;&#38519;&#20998;&#31867;&#65292;&#19988;&#26080;&#38656;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.13974</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#20302;&#20869;&#23384;&#35774;&#22791;&#30340;&#28151;&#21512;&#30789;&#29255;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation. (arXiv:2303.13974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#23558;&#22797;&#26434;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#27169;&#22411;&#19978;&#65292;&#20174;&#32780;&#20351;&#20302;&#20869;&#23384;&#35774;&#22791;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#32570;&#38519;&#20998;&#31867;&#65292;&#19988;&#26080;&#38656;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#30789;&#29255;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#25968;&#21315;&#20010;&#27493;&#39588;&#12290;&#30789;&#29255;&#22320;&#22270;&#30340;&#32570;&#38519;&#27169;&#24335;&#35782;&#21035;&#23545;&#20110;&#30830;&#23450;&#29983;&#20135;&#32570;&#38519;&#30340;&#26681;&#26412;&#21407;&#22240;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#21487;&#33021;&#36827;&#19968;&#27493;&#20026;&#30789;&#29255;&#24037;&#21378;&#30340;&#20135;&#37327;&#25552;&#39640;&#25552;&#20379;&#35265;&#35299;&#12290;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#65292;&#21508;&#31181;&#32570;&#38519;&#21487;&#33021;&#21333;&#29420;&#20986;&#29616;&#22312;&#30789;&#29255;&#20013;&#65292;&#20063;&#21487;&#33021;&#20197;&#19981;&#21516;&#30340;&#32452;&#21512;&#24418;&#24335;&#20986;&#29616;&#12290;&#35782;&#21035;&#30789;&#29255;&#20013;&#30340;&#22810;&#20010;&#32570;&#38519;&#36890;&#24120;&#27604;&#35782;&#21035;&#21333;&#20010;&#32570;&#38519;&#26356;&#38590;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#28151;&#21512;&#31867;&#22411;DPR&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32570;&#38519;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#22823;&#22411;&#22797;&#26434;&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#24456;&#38590;&#22312;&#36890;&#24120;&#29992;&#20110;&#21046;&#36896;&#23454;&#39564;&#23460;&#30340;&#20302;&#20869;&#23384;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#21478;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#26469;&#35757;&#32451;&#22797;&#26434;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#35757;&#32451;&#31243;&#24207;&#65292;&#23558;&#22797;&#26434;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#36731;&#37327;&#32423;&#21487;&#37096;&#32626;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20998;&#31867;&#27169;&#22411;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#36275;&#22815;&#39640;&#25928;&#65292;&#21487;&#22312;&#20302;&#20869;&#23384;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing wafers is an intricate task involving thousands of steps. Defect Pattern Recognition (DPR) of wafer maps is crucial for determining the root cause of production defects, which may further provide insight for yield improvement in wafer foundry. During manufacturing, various defects may appear standalone in the wafer or may appear as different combinations. Identifying multiple defects in a wafer is generally harder compared to identifying a single defect. Recently, deep learning methods have gained significant traction in mixed-type DPR. However, the complexity of defects requires complex and large models making them very difficult to operate on low-memory embedded devices typically used in fabrication labs. Another common issue is the unavailability of labeled data to train complex networks. In this work, we propose an unsupervised training routine to distill the knowledge of complex pre-trained models to lightweight deployment-ready models. We empirically show that this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20174;&#21487;&#25345;&#32493;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#36229;&#21442;&#25968;&#35843;&#25972;&#31574;&#30053;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#26469;&#38477;&#20302;&#33021;&#32791;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#20102;&#21367;&#31215;&#12289;&#32447;&#24615;&#23618;&#21644;&#27744;&#21270;&#23618;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2303.13972</link><description>&lt;p&gt;
&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#30340;&#33410;&#33021;&#23454;&#36341;&#65306;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncovering Energy-Efficient Practices in Deep Learning Training: Preliminary Steps Towards Green AI. (arXiv:2303.13972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20174;&#21487;&#25345;&#32493;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#36229;&#21442;&#25968;&#35843;&#25972;&#31574;&#30053;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#26469;&#38477;&#20302;&#33021;&#32791;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#20102;&#21367;&#31215;&#12289;&#32447;&#24615;&#23618;&#21644;&#27744;&#21270;&#23618;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;AI&#23454;&#36341;&#30340;&#30446;&#26631;&#37117;&#26159;&#30456;&#21516;&#30340;&#65306;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#8220;&#32467;&#26524;&#8221;&#36890;&#24120;&#25351;&#23436;&#25104;&#26576;&#20010;&#31454;&#20105;&#24615;&#38382;&#39064;&#38598;&#26102;&#36798;&#21040;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#23558;&#33021;&#28304;&#28040;&#32791;&#20316;&#20026;&#19968;&#20010;&#21516;&#31561;&#37325;&#35201;&#30340;&#25351;&#26631;&#26469;&#32771;&#34385;&#65292;&#24182;&#20943;&#23569;&#20219;&#20309;&#26080;&#20851;&#30340;&#20219;&#21153;&#25110;&#33021;&#37327;&#20351;&#29992;&#12290;&#25105;&#20204;&#20174;&#21487;&#25345;&#32493;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#36890;&#36807;&#30740;&#31350;&#36229;&#21442;&#25968;&#35843;&#25972;&#31574;&#30053;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#36825;&#20004;&#20010;&#23545;&#25972;&#20010;&#31649;&#36947;&#33021;&#28304;&#28040;&#32791;&#26377;&#24040;&#22823;&#24433;&#21709;&#30340;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#26399;&#38388;&#30340;&#32593;&#26684;&#25628;&#32034;&#65292;&#38543;&#26426;&#25628;&#32034;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#36125;&#21494;&#26031;&#20248;&#21270;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#24182;&#32771;&#34385;&#20102;&#19977;&#31181;&#20027;&#35201;&#23618;&#31867;&#22411;&#65306;&#21367;&#31215;&#65292;&#32447;&#24615;&#23618;&#21644;&#27744;&#21270;&#23618;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern AI practices all strive towards the same goal: better results. In the context of deep learning, the term "results" often refers to the achieved accuracy on a competitive problem set. In this paper, we adopt an idea from the emerging field of Green AI to consider energy consumption as a metric of equal importance to accuracy and to reduce any irrelevant tasks or energy usage. We examine the training stage of the deep learning pipeline from a sustainability perspective, through the study of hyperparameter tuning strategies and the model complexity, two factors vastly impacting the overall pipeline's energy consumption. First, we investigate the effectiveness of grid search, random search and Bayesian optimisation during hyperparameter tuning, and we find that Bayesian optimisation significantly dominates the other strategies. Furthermore, we analyse the architecture of convolutional neural networks with the energy consumption of three prominent layer types: convolutional, linear a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26368;&#20248;&#36755;&#36816;&#22870;&#21169;&#26631;&#35760;&#65288;OTR&#65289;&#8221;&#30340;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#20026;&#26080;&#26631;&#35760;&#30340;&#36712;&#36857;&#20998;&#37197;&#22870;&#21169;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#35745;&#31639;&#25968;&#25454;&#38598;&#20013;&#26410;&#27880;&#37322;&#30340;&#36712;&#36857;&#21644;&#19987;&#23478;&#28436;&#31034;&#20043;&#38388;&#30340;&#26368;&#20339;&#23545;&#40784;&#65292;&#24471;&#20986;&#21487;&#35299;&#37322;&#20026;&#22870;&#21169;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;OTR&#21482;&#20351;&#29992;&#19968;&#20010;&#28436;&#31034;&#26102;&#21487;&#20197;&#25345;&#32493;&#21305;&#37197;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13971</link><description>&lt;p&gt;
&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#30340;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Offline Imitation Learning. (arXiv:2303.13971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26368;&#20248;&#36755;&#36816;&#22870;&#21169;&#26631;&#35760;&#65288;OTR&#65289;&#8221;&#30340;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#20026;&#26080;&#26631;&#35760;&#30340;&#36712;&#36857;&#20998;&#37197;&#22870;&#21169;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#35745;&#31639;&#25968;&#25454;&#38598;&#20013;&#26410;&#27880;&#37322;&#30340;&#36712;&#36857;&#21644;&#19987;&#23478;&#28436;&#31034;&#20043;&#38388;&#30340;&#26368;&#20339;&#23545;&#40784;&#65292;&#24471;&#20986;&#21487;&#35299;&#37322;&#20026;&#22870;&#21169;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;OTR&#21482;&#20351;&#29992;&#19968;&#20010;&#28436;&#31034;&#26102;&#21487;&#20197;&#25345;&#32493;&#21305;&#37197;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#23398;&#20064;&#33391;&#22909;&#20915;&#31574;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#19982;&#30495;&#23454;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22312;&#32447;&#19979;RL&#20013;&#38656;&#35201;&#25968;&#25454;&#38598;&#36827;&#34892;&#22870;&#21169;&#27880;&#37322;&#65292;&#36825;&#22312;&#22870;&#21169;&#24037;&#31243;&#22256;&#38590;&#25110;&#33719;&#24471;&#22870;&#21169;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#21171;&#21160;&#21147;&#26102;&#65292;&#20250;&#24102;&#26469;&#23454;&#38469;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26368;&#20248;&#36755;&#36816;&#22870;&#21169;&#26631;&#35760;&#65288;OTR&#65289;&#8221;&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#35745;&#31639;&#25968;&#25454;&#38598;&#20013;&#26410;&#27880;&#37322;&#30340;&#36712;&#36857;&#21644;&#19987;&#23478;&#28436;&#31034;&#20043;&#38388;&#30340;&#26368;&#20339;&#23545;&#40784;&#65292;&#20174;&#32780;&#24471;&#20986;&#19968;&#31181;&#21487;&#20197;&#35299;&#37322;&#20026;&#22870;&#21169;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#35813;&#24230;&#37327;&#21487;&#20197;&#30001;&#31163;&#32447;RL&#31639;&#27861;&#29992;&#20110;&#23398;&#20064;&#31574;&#30053;&#12290;OTR&#26131;&#20110;&#23454;&#29616;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;OTR&#21482;&#20351;&#29992;&#19968;&#20010;&#28436;&#31034;&#26102;&#21487;&#20197;&#25345;&#32493;&#21305;&#37197;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of large datasets, offline reinforcement learning (RL) is a promising framework for learning good decision-making policies without the need to interact with the real environment. However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive. In this paper, we introduce Optimal Transport Reward labeling (OTR), an algorithm that assigns rewards to offline trajectories, with a few high-quality demonstrations. OTR's key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we show that OTR with a single demonstration can consistently match the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20108;&#23618;&#20248;&#21270;&#31639;&#27861;&#22312;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#26799;&#24230;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#28508;&#22312;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13964</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#23618;&#20248;&#21270;&#30340;&#26799;&#24230;&#31232;&#32570;&#38382;&#39064;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Gradient scarcity with Bilevel Optimization for Graph Learning. (arXiv:2303.13964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20108;&#23618;&#20248;&#21270;&#31639;&#27861;&#22312;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#26799;&#24230;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#28508;&#22312;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#19979;&#65292;&#22270;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#26799;&#24230;&#31232;&#32570;&#29616;&#35937;&#12290;&#21363;&#23398;&#20064;&#19968;&#37096;&#20998;&#33410;&#28857;&#30340;&#25439;&#22833;&#20250;&#23548;&#33268;&#26410;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#36793;&#32536;&#25910;&#21040;&#38646;&#26799;&#24230;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#29616;&#35937;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#25968;&#23398;&#21051;&#30011;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20063;&#23384;&#22312;&#20110;&#20108;&#23618;&#20248;&#21270;&#20013;&#65292;&#20854;&#20013;&#38382;&#39064;&#30340;&#21442;&#25968;&#23384;&#22312;&#39069;&#22806;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#22270;&#21040;&#22270;&#27169;&#22411;&#65288;G2G&#65289;&#36827;&#34892;&#28508;&#22312;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common issue in graph learning under the semi-supervised setting is referred to as gradient scarcity. That is, learning graphs by minimizing a loss on a subset of nodes causes edges between unlabelled nodes that are far from labelled ones to receive zero gradients. The phenomenon was first described when optimizing the graph and the weights of a Graph Neural Network (GCN) with a joint optimization algorithm. In this work, we give a precise mathematical characterization of this phenomenon, and prove that it also emerges in bilevel optimization, where additional dependency exists between the parameters of the problem. While for GCNs gradient scarcity occurs due to their finite receptive field, we show that it also occurs with the Laplacian regularization model, in the sense that gradients amplitude decreases exponentially with distance to labelled nodes. To alleviate this issue, we study several solutions: we propose to resort to latent graph learning using a Graph-to-Graph model (G2G)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#35299;&#20998;&#24067;&#24335;&#31232;&#30095;&#22359;&#32534;&#30721;&#65288;SBC&#65289;&#30340;GSBC&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#22522;&#20110;&#38408;&#20540;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#12289;&#26465;&#20214;&#38543;&#26426;&#37319;&#26679;&#21644;$\ell_\infty$&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#24182;&#33021;&#22815;&#20998;&#26512;&#30830;&#23450;&#39044;&#26399;&#30340;&#35299;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#36817;&#20284;&#32780;&#25918;&#26494;&#30340;&#22122;&#22768;SBC&#20013;&#31526;&#21495;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.13957</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31232;&#30095;&#22359;&#32534;&#30721;&#30340;&#20998;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Factorizers for Distributed Sparse Block Codes. (arXiv:2303.13957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#35299;&#20998;&#24067;&#24335;&#31232;&#30095;&#22359;&#32534;&#30721;&#65288;SBC&#65289;&#30340;GSBC&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#22522;&#20110;&#38408;&#20540;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#12289;&#26465;&#20214;&#38543;&#26426;&#37319;&#26679;&#21644;$\ell_\infty$&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#24182;&#33021;&#22815;&#20998;&#26512;&#30830;&#23450;&#39044;&#26399;&#30340;&#35299;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#36817;&#20284;&#32780;&#25918;&#26494;&#30340;&#22122;&#22768;SBC&#20013;&#31526;&#21495;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#31232;&#30095;&#22359;&#32534;&#30721;&#65288;SBC&#65289;&#21033;&#29992;&#22266;&#23450;&#23485;&#24230;&#30340;&#21521;&#37327;&#23545;&#31526;&#21495;&#25968;&#25454;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#21644;&#25805;&#20316;&#65292;&#20855;&#26377;&#32039;&#20945;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#22312;&#19981;&#24517;&#25628;&#23547;&#25152;&#26377;&#21487;&#33021;&#30340;&#32452;&#21512;&#30340;&#24773;&#20917;&#19979;&#23558;&#36825;&#20123;&#25968;&#25454;&#32467;&#26500;&#25286;&#20998;&#25104;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;&#24403;&#20351;&#29992;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#26597;&#35810;&#21521;&#37327;&#26102;&#65292;&#22122;&#22768;SBC&#20013;&#30340;&#31526;&#21495;&#34920;&#31034;&#30001;&#20110;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#36817;&#20284;&#32780;&#25918;&#26494;&#65292;&#36825;&#20351;&#24471;&#36825;&#31181;&#20998;&#35299;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#31934;&#24230;&#30340;&#26041;&#27861;&#26469;&#20998;&#35299;&#19968;&#31181;&#26356;&#28789;&#27963;&#12289;&#22240;&#27492;&#26356;&#26222;&#36941;&#30340;SBC&#24418;&#24335;&#65292;&#31216;&#20026;GSBC&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#22240;&#23376;&#24341;&#20837;&#20102;&#22522;&#20110;&#38408;&#20540;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#12289;&#26465;&#20214;&#38543;&#26426;&#37319;&#26679;&#21644;$\ell_\infty$&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#23427;&#30340;&#38543;&#26426;&#37319;&#26679;&#26426;&#21046;&#19982;&#21472;&#21152;&#25628;&#32034;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20998;&#26512;&#30830;&#23450;&#39044;&#26399;&#30340;&#35299;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed sparse block codes (SBCs) exhibit compact representations for encoding and manipulating symbolic data structures using fixed-with vectors. One major challenge however is to disentangle, or factorize, such data structures into their constituent elements without having to search through all possible combinations. This factorization becomes more challenging when queried by noisy SBCs wherein symbol representations are relaxed due to perceptual uncertainty and approximations made when modern neural networks are used to generate the query vectors. To address these challenges, we first propose a fast and highly accurate method for factorizing a more flexible and hence generalized form of SBCs, dubbed GSBCs. Our iterative factorizer introduces a threshold-based nonlinear activation, a conditional random sampling, and an $\ell_\infty$-based similarity metric. Its random sampling mechanism in combination with the search in superposition allows to analytically determine the expected 
&lt;/p&gt;</description></item><item><title>Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.13937</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#31890;&#23376;&#29289;&#29702;&#36807;&#31243;&#30340;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Topological Reconstruction of Particle Physics Processes using Graph Neural Networks. (arXiv:2303.13937v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13937
&lt;/p&gt;
&lt;p&gt;
Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Topograph&#65292;&#23427;&#21033;&#29992;&#31890;&#23376;&#29289;&#29702;&#34928;&#21464;&#30340;&#26412;&#36136;&#21644;&#20449;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#65292;&#37325;&#24314;&#20102;&#21253;&#25324;&#20013;&#20171;&#31890;&#23376;&#22312;&#20869;&#30340;&#24213;&#23618;&#29289;&#29702;&#36807;&#31243;&#12290;Topograph&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#30340;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#23558;&#23427;&#20204;&#19982;&#23427;&#20204;&#21407;&#26469;&#30340;&#27597;&#31890;&#23376;&#20851;&#32852;&#36215;&#26469;&#65292;&#32780;&#19988;&#30452;&#25509;&#39044;&#27979;&#20102;&#30828;&#25955;&#23556;&#36807;&#31243;&#20013;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#12290;&#19982;&#26631;&#20934;&#30340;&#32452;&#21512;&#26041;&#27861;&#25110;&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#30340;&#22797;&#26434;&#24230;&#19982;&#37325;&#26500;&#23545;&#35937;&#30340;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#24212;&#29992;Topograph&#20110;&#20840;&#24378;&#23376;&#34928;&#21464;&#27169;&#24335;&#19979;&#30340;&#39030;&#22840;&#20811;&#23545;&#20135;&#29983;&#38382;&#39064;&#65292;&#30456;&#23545;&#26631;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new approach, the Topograph, which reconstructs underlying physics processes, including the intermediary particles, by leveraging underlying priors from the nature of particle physics decays and the flexibility of message passing graph neural networks. The Topograph not only solves the combinatoric assignment of observed final state objects, associating them to their original mother particles, but directly predicts the properties of intermediate particles in hard scatter processes and their subsequent decays. In comparison to standard combinatoric approaches or modern approaches using graph neural networks, which scale exponentially or quadratically, the complexity of Topographs scales linearly with the number of reconstructed objects.  We apply Topographs to top quark pair production in the all hadronic decay channel, where we outperform the standard approach and match the performance of the state-of-the-art machine learning technique.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;MFVB&#26041;&#27861;&#65292;&#26377;&#25928;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#33539;&#22260;&#65292;&#21487;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#27874;&#21160;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2303.13930</link><description>&lt;p&gt;
&#31890;&#23376;&#24179;&#22343;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Particle Mean Field Variational Bayes. (arXiv:2303.13930v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;MFVB&#26041;&#27861;&#65292;&#26377;&#25928;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#33539;&#22260;&#65292;&#21487;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#27874;&#21160;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#26041;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26368;&#39640;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#25216;&#26415;&#20043;&#19968;&#65292;&#28982;&#32780;&#20854;&#20351;&#29992;&#20165;&#38480;&#20110;&#20855;&#26377;&#20849;&#36717;&#20808;&#39564;&#25110;&#38656;&#35201;&#35299;&#26512;&#35745;&#31639;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;MFVB&#26041;&#27861;&#65292;&#22823;&#22823;&#25193;&#23637;&#20102;MFVB&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;Wasserstein&#26799;&#24230;&#27969;&#19982;Langevin&#25193;&#25955;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#26500;&#24314;&#20102;&#26032;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#27874;&#21160;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mean Field Variational Bayes (MFVB) method is one of the most computationally efficient techniques for Bayesian inference. However, its use has been restricted to models with conjugate priors or those that require analytical calculations. This paper proposes a novel particle-based MFVB approach that greatly expands the applicability of the MFVB method. We establish the theoretical basis of the new method by leveraging the connection between Wasserstein gradient flows and Langevin diffusion dynamics, and demonstrate the effectiveness of this approach using Bayesian logistic regression, stochastic volatility, and deep neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#38024;&#23545;&#32974;&#20799;&#31579;&#26597;&#36229;&#22768;&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#21644;&#21345;&#23610;&#21435;&#38500;&#28151;&#28102;&#20449;&#24687;&#65292;&#20197;&#35757;&#32451;&#26356;&#21152;&#20934;&#30830;&#30340;&#36229;&#22768;&#35786;&#26029;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13918</link><description>&lt;p&gt;
&#21435;&#38500;&#32974;&#20799;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#28151;&#28102;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Removing confounding information from fetal ultrasound images. (arXiv:2303.13918v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#38024;&#23545;&#32974;&#20799;&#31579;&#26597;&#36229;&#22768;&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#21644;&#21345;&#23610;&#21435;&#38500;&#28151;&#28102;&#20449;&#24687;&#65292;&#20197;&#35757;&#32451;&#26356;&#21152;&#20934;&#30830;&#30340;&#36229;&#22768;&#35786;&#26029;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#23884;&#20837;&#30340;&#25991;&#26412;&#25110;&#26631;&#35760;&#31561;&#28151;&#28102;&#20449;&#24687;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#35786;&#26029;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20026;&#20020;&#24202;&#30446;&#30340;&#25910;&#38598;&#30340;&#25968;&#25454;&#36890;&#24120;&#25317;&#26377;&#36825;&#26679;&#30340;&#26631;&#35760;&#12290;&#22312;&#30382;&#32932;&#23398;&#20013;&#65292;&#24050;&#30693;&#23384;&#22312;&#23558;&#22270;&#20687;&#20013;&#24694;&#24615;&#30149;&#21464;&#36807;&#24230;&#23637;&#31034;&#30340;&#32472;&#30011;&#25110;&#26631;&#23610;&#31561;&#26631;&#35760;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23884;&#20837;&#22312;&#22269;&#23478;&#25968;&#25454;&#24211;&#20013;&#21253;&#21547;&#32974;&#20799;&#31579;&#26597;&#36229;&#22768;&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#21644;&#21345;&#23610;&#65292;&#36825;&#20123;&#22270;&#20687;&#19982;&#38656;&#35201;&#39044;&#27979;&#30340;&#26631;&#20934;&#24179;&#38754;&#30456;&#20851;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#24211;&#20013;&#21487;&#29992;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#39564;&#35777;&#20102;&#19968;&#31995;&#21015;&#26368;&#23567;&#21270;&#23884;&#20837;&#25991;&#26412;&#21644;&#21345;&#23610;&#23545;&#36229;&#22768;&#35786;&#26029;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#20197;&#26631;&#20934;&#24179;&#38754;&#20998;&#31867;&#20316;&#20026;&#27979;&#35797;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Confounding information in the form of text or markings embedded in medical images can severely affect the training of diagnostic deep learning algorithms. However, data collected for clinical purposes often have such markings embedded in them. In dermatology, known examples include drawings or rulers that are overrepresented in images of malignant lesions. In this paper, we encounter text and calipers placed on the images found in national databases containing fetal screening ultrasound scans, which correlate with standard planes to be predicted. In order to utilize the vast amounts of data available in these databases, we develop and validate a series of methods for minimizing the confounding effects of embedded text and calipers on deep learning algorithms designed for ultrasound, using standard plane classification as a test case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#24341;&#21147;&#27874;&#25968;&#25454;&#27969;&#20013;&#30340;&#27611;&#21050;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25506;&#32034;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;Fine-tune&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#26368;&#20339;&#27169;&#22411;F1&#20998;&#25968;&#36798;&#21040;97.18&#65285;&#65292;&#34920;&#26126;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20998;&#31867;&#24341;&#21147;&#27874;&#25968;&#25454;&#27969;&#20013;&#30340;&#27611;&#21050;&#20449;&#21495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.13917</link><description>&lt;p&gt;
&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#24341;&#21147;&#27874;&#25968;&#25454;&#27969;&#20013;&#30340;&#27611;&#21050;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks for the classification of glitches in gravitational-wave data streams. (arXiv:2303.13917v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#24341;&#21147;&#27874;&#25968;&#25454;&#27969;&#20013;&#30340;&#27611;&#21050;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25506;&#32034;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;Fine-tune&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#26368;&#20339;&#27169;&#22411;F1&#20998;&#25968;&#36798;&#21040;97.18&#65285;&#65292;&#34920;&#26126;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20998;&#31867;&#24341;&#21147;&#27874;&#25968;&#25454;&#27969;&#20013;&#30340;&#27611;&#21050;&#20449;&#21495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;&#21253;&#25324;&#29616;&#20195;&#30340;ConvNeXt&#32593;&#32476;&#31995;&#21015;&#65289;&#26469;&#20998;&#31867;&#26469;&#33258;Advanced LIGO&#25506;&#27979;&#22120;&#30340;&#25968;&#25454;&#20013;&#30340;&#30701;&#26242;&#22122;&#22768;&#20449;&#21495;&#65288;&#21363;&#27611;&#21050;&#20449;&#21495;&#65289;&#21644;&#24341;&#21147;&#27874;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;Gravity Spy&#25968;&#25454;&#38598;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;Fine-tune&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#29616;&#26377;&#32467;&#26524;&#38750;&#24120;&#25509;&#36817;&#65292;&#26368;&#20339;&#30340;&#30417;&#30563;&#65288;&#33258;&#30417;&#30563;&#65289;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#36798;&#21040;&#20102;97.18&#65285;&#65288;94.15&#65285;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;LIGO-Virgo O3&#36816;&#34892;&#30340;&#23454;&#38469;&#24341;&#21147;&#27874;&#20449;&#21495;&#27979;&#35797;&#20102;&#27169;&#22411;&#12290;&#34429;&#28982;&#26159;&#20351;&#29992;&#21069;&#20960;&#27425;&#36816;&#34892; &#65288;O1&#21644;O2&#65289;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#20998;&#25968;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#24341;&#21147;&#27874;&#25968;&#25454;&#27969;&#20013;&#30340;&#27611;&#21050;&#20449;&#21495;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the use of Convolutional Neural Networks (including the modern ConvNeXt network family) to classify transient noise signals (i.e.~glitches) and gravitational waves in data from the Advanced LIGO detectors. First, we use models with a supervised learning approach, both trained from scratch using the Gravity Spy dataset and employing transfer learning by fine-tuning pre-trained models in this dataset. Second, we also explore a self-supervised approach, pre-training models with automatically generated pseudo-labels. Our findings are very close to existing results for the same dataset, reaching values for the F1 score of 97.18% (94.15%) for the best supervised (self-supervised) model. We further test the models using actual gravitational-wave signals from LIGO-Virgo's O3 run. Although trained using data from previous runs (O1 and O2), the models show good performance, in particular when using transfer learning. We find that transfer learning improves the scores without the n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25151;&#39076;&#20998;&#31867;&#26041;&#27861;&#22312;12&#23548;&#32852;&#24515;&#30005;&#22270;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#35782;&#21035;&#25151;&#39076;&#24182;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.13915</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25151;&#39076;&#20998;&#31867;&#23545;&#22122;&#22768;&#30340;&#24433;&#21709;&#35780;&#20272;&#65306;&#19968;&#20010;12&#23548;&#32852;&#24515;&#30005;&#22270;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Impact of Noise on Deep Learning-based Classification of Atrial Fibrillation in 12-Lead ECG. (arXiv:2303.13915v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25151;&#39076;&#20998;&#31867;&#26041;&#27861;&#22312;12&#23548;&#32852;&#24515;&#30005;&#22270;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#35782;&#21035;&#25151;&#39076;&#24182;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#20998;&#26512;&#22312;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30446;&#21069;&#26159;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#30001;&#20110;&#20854;&#25968;&#25454;&#39537;&#21160;&#30340;&#29305;&#24615;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#20449;&#21495;&#22122;&#22768;&#65292;&#20294;&#20854;&#23545;&#36825;&#20123;&#26041;&#27861;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#65288;PTBXL&#65289;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#25151;&#39076;&#26816;&#27979;&#31934;&#24230;&#22312;&#22235;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#19979;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#20154;&#24037;&#19987;&#23478;&#25552;&#20379;&#30340;&#20851;&#20110;&#22122;&#22768;&#30340;&#20803;&#25968;&#25454;&#26469;&#20026;&#27599;&#20010;&#24515;&#30005;&#22270;&#20998;&#37197;&#20449;&#21495;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#24515;&#30005;&#22270;&#35745;&#31639;&#20102;&#23450;&#37327;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19982;&#36825;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#30340;&#20851;&#31995;&#65292;&#24182;&#35266;&#23519;&#21040;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#26576;&#20123;&#20449;&#21495;&#34987;&#26631;&#35760;&#20026;&#22810;&#23548;&#22122;&#22768;&#24773;&#20917;&#19979;&#31283;&#20581;&#22320;&#35782;&#21035;&#25151;&#39076;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20449;&#21495;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiography analysis is widely used in various clinical applications and Deep Learning models for classification tasks are currently in the focus of research. Due to their data-driven character, they bear the potential to handle signal noise efficiently, but its influence on the accuracy of these methods is still unclear. Therefore, we benchmark the influence of four types of noise on the accuracy of a Deep Learning-based method for atrial fibrillation detection in 12-lead electrocardiograms. We use a subset of a publicly available dataset (PTBXL) and use the metadata provided by human experts regarding noise for assigning a signal quality to each electrocardiogram. Furthermore, we compute a quantitative signal-to-noise ratio for each electrocardiogram. We analyze the accuracy of the Deep Learning model with respect to both metrics and observe that the method can robustly identify atrial fibrillation, even in cases signals are labelled by human experts as being noisy on multipl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Wave-U-Net&#37492;&#21035;&#22120;&#65292;&#23427;&#26159;&#19968;&#20010;&#21333;&#29420;&#20294;&#34920;&#36798;&#21147;&#24378;&#30340;&#37492;&#21035;&#22120;&#65292;&#21487;&#20197;&#20197;&#36880;&#20010;&#26679;&#26412;&#30340;&#26041;&#24335;&#35780;&#20272;&#27874;&#24418;&#65292;&#21516;&#26102;&#25552;&#21462;&#22810;&#32423;&#29305;&#24449;&#65292;&#20026;&#29983;&#25104;&#22120;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#19981;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35821;&#38899;&#21512;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.13909</link><description>&lt;p&gt;
Wave-U-Net&#37492;&#21035;&#22120;&#65306;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35821;&#38899;&#21512;&#25104;&#21028;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
Wave-U-Net Discriminator: Fast and Lightweight Discriminator for Generative Adversarial Network-Based Speech Synthesis. (arXiv:2303.13909v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Wave-U-Net&#37492;&#21035;&#22120;&#65292;&#23427;&#26159;&#19968;&#20010;&#21333;&#29420;&#20294;&#34920;&#36798;&#21147;&#24378;&#30340;&#37492;&#21035;&#22120;&#65292;&#21487;&#20197;&#20197;&#36880;&#20010;&#26679;&#26412;&#30340;&#26041;&#24335;&#35780;&#20272;&#27874;&#24418;&#65292;&#21516;&#26102;&#25552;&#21462;&#22810;&#32423;&#29305;&#24449;&#65292;&#20026;&#29983;&#25104;&#22120;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#19981;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35821;&#38899;&#21512;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25552;&#39640;&#35821;&#38899;&#36136;&#37327;&#65292;&#35813;&#32593;&#32476;&#35757;&#32451;&#29983;&#25104;&#22120;&#65288;&#35821;&#38899;&#21512;&#25104;&#22120;&#65289;&#21644;&#37492;&#21035;&#22120;&#65292;&#20351;&#23427;&#20204;&#22312;&#19968;&#20010;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#20013;&#21327;&#21516;&#23545;&#25239;&#12290;&#36817;&#26399;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;HiFi-GAN&#65289;&#21644;&#31471;&#21040;&#31471;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65288;&#20363;&#22914;VITS&#65289;&#36890;&#24120;&#20351;&#29992;&#22810;&#20010;&#37492;&#21035;&#22120;&#30340;&#38598;&#21512;&#26469;&#20174;&#22810;&#20010;&#35282;&#24230;&#20180;&#32454;&#22320;&#26816;&#26597;&#27874;&#24418;&#12290;&#36825;&#31181;&#37492;&#21035;&#22120;&#21487;&#20197;&#20351;&#21512;&#25104;&#35821;&#38899;&#20805;&#20998;&#25509;&#36817;&#23454;&#38469;&#35821;&#38899;&#65292;&#20294;&#26159;&#38543;&#30528;&#37492;&#21035;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#38656;&#35201;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Wave-U-Net&#37492;&#21035;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#21333;&#29420;&#30340;&#20294;&#20855;&#26377;Wave-U-Net&#26550;&#26500;&#30340;&#34920;&#36798;&#21147;&#24378;&#30340;&#37492;&#21035;&#22120;&#12290;&#36825;&#20010;&#37492;&#21035;&#22120;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#23427;&#21487;&#20197;&#20197;&#19982;&#36755;&#20837;&#20449;&#21495;&#30456;&#21516;&#30340;&#20998;&#36776;&#29575;&#20197;&#36880;&#20010;&#26679;&#26412;&#30340;&#26041;&#24335;&#35780;&#20272;&#27874;&#24418;&#65292;&#21516;&#26102;&#36890;&#36807;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#25552;&#21462;&#22810;&#32423;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#12290;&#36825;&#31181;&#26550;&#26500;&#20026;&#29983;&#25104;&#22120;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#65292;
&lt;/p&gt;
&lt;p&gt;
In speech synthesis, a generative adversarial network (GAN), training a generator (speech synthesizer) and a discriminator in a min-max game, is widely used to improve speech quality. An ensemble of discriminators is commonly used in recent neural vocoders (e.g., HiFi-GAN) and end-to-end text-to-speech (TTS) systems (e.g., VITS) to scrutinize waveforms from multiple perspectives. Such discriminators allow synthesized speech to adequately approach real speech; however, they require an increase in the model size and computation time according to the increase in the number of discriminators. Alternatively, this study proposes a Wave-U-Net discriminator, which is a single but expressive discriminator with Wave-U-Net architecture. This discriminator is unique; it can assess a waveform in a sample-wise manner with the same resolution as the input signal, while extracting multilevel features via an encoder and decoder with skip connections. This architecture provides a generator with sufficie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31867;&#27604;&#21046;&#20316;&#26426;&#21046;&#23558;&#26032;&#25968;&#25454;&#37325;&#26032;&#26144;&#23556;&#21040;&#26087;&#31867;&#65292;&#24182;&#21033;&#29992;&#25152;&#23398;&#25552;&#31034;&#20272;&#35745;&#21644;&#23545;&#25239;&#34920;&#31034;&#20559;&#31227;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#22686;&#37327;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13898</link><description>&lt;p&gt;
&#36807;&#21435;&#30340;&#25552;&#37266;: &#24102;&#31867;&#27604;&#25552;&#31034;&#30340;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Remind of the Past: Incremental Learning with Analogical Prompts. (arXiv:2303.13898v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31867;&#27604;&#21046;&#20316;&#26426;&#21046;&#23558;&#26032;&#25968;&#25454;&#37325;&#26032;&#26144;&#23556;&#21040;&#26087;&#31867;&#65292;&#24182;&#21033;&#29992;&#25152;&#23398;&#25552;&#31034;&#20272;&#35745;&#21644;&#23545;&#25239;&#34920;&#31034;&#20559;&#31227;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#22686;&#37327;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26080;&#25968;&#25454;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#23545;&#23384;&#20648;&#21451;&#22909;&#65292;&#20294;&#22312;&#32570;&#20047;&#21382;&#21490;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#21644;&#23545;&#25239;&#34920;&#31034;&#20559;&#31227;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#20154;&#31867;&#31867;&#27604;&#33021;&#21147;&#30340;&#21551;&#21457;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#27604;&#21046;&#20316;&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23558;&#26032;&#25968;&#25454;&#37325;&#26032;&#26144;&#23556;&#21040;&#26087;&#31867;&#12290;&#23427;&#20165;&#20351;&#29992;&#26032;&#31867;&#30340;&#26679;&#26412;&#27169;&#25311;&#20102;&#26087;&#27169;&#22411;&#19978;&#30446;&#26631;&#26087;&#31867;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#25152;&#23398;&#20064;&#30340;&#25552;&#31034;&#36827;&#19968;&#27493;&#29992;&#20110;&#20272;&#35745;&#21644;&#23545;&#25239;&#30001;&#20110;&#23545;&#21382;&#21490;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#32780;&#23548;&#33268;&#30340;&#34920;&#31034;&#20559;&#31227;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35838;&#31243;&#21644;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#35774;&#32622;&#19979;&#22312;&#22235;&#20010;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#20165;&#20445;&#23384;&#27599;&#20010;&#31867;&#21035;&#30340;&#29305;&#24449;&#21407;&#22411;&#19981;&#26029;&#20248;&#20110;&#25968;&#25454;&#37325;&#25918;&#26041;&#27861;&#12290;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#65292;&#23427;&#24050;&#32463;&#20960;&#20046;&#36798;&#21040;&#20102;&#23454;&#35777;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although data-free incremental learning methods are memory-friendly, accurately estimating and counteracting representation shifts is challenging in the absence of historical data. This paper addresses this thorny problem by proposing a novel incremental learning method inspired by human analogy capabilities. Specifically, we design an analogy-making mechanism to remap the new data into the old class by prompt tuning. It mimics the feature distribution of the target old class on the old model using only samples of new classes. The learnt prompts are further used to estimate and counteract the representation shift caused by fine-tuning for the historical prototypes. The proposed method sets up new state-of-the-art performance on four incremental learning benchmarks under both the class and domain incremental learning settings. It consistently outperforms data-replay methods by only saving feature prototypes for each class. It has almost hit the empirical upper bound by joint training on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#39033;&#24335;&#32593;&#32476;&#32467;&#26500;&#21450;&#20854;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20845;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040; ResNet &#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#25552;&#20986;&#20102; D-PolyNets &#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#27491;&#21017;&#21270;&#26041;&#26696;&#21644;&#25552;&#39640;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.13896</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;&#22810;&#39033;&#24335;&#32593;&#32476;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regularization of polynomial networks for image recognition. (arXiv:2303.13896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#39033;&#24335;&#32593;&#32476;&#32467;&#26500;&#21450;&#20854;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20845;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040; ResNet &#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#25552;&#20986;&#20102; D-PolyNets &#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#27491;&#21017;&#21270;&#26041;&#26696;&#21644;&#25552;&#39640;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#26159;&#40657;&#30418;&#23376;&#65292;&#20363;&#22914;&#38590;&#20197;&#29702;&#35770;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#22810;&#39033;&#24335;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#20986;&#29616;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#20173;&#26410;&#36798;&#21040;&#24378;&#22823;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#24179;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#32553;&#23567;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31867;&#22810;&#39033;&#24335;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#20845;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040; ResNet &#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#35777;&#26126;&#24378;&#27491;&#21017;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23545;&#30830;&#20999;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#21160;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; D-PolyNets&#65292;&#36825;&#20123;&#32593;&#32476;&#27604;&#20197;&#21069;&#25552;&#20986;&#30340;&#22810;&#39033;&#24335;&#32593;&#32476;&#26377;&#26356;&#39640;&#30340;&#25193;&#23637;&#24230;&#12290;D-PolyNets &#22312;&#23454;&#29616;&#31867;&#20284;&#20854;&#20182;&#22810;&#39033;&#24335;&#32593;&#32476;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#33021;&#22815;&#24102;&#26469;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#27491;&#21017;&#21270;&#30340;&#22810;&#39033;&#24335;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#26356;&#22909;&#22320;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have obtained impressive performance across tasks, however they still remain as black boxes, e.g., hard to theoretically analyze. At the same time, Polynomial Networks (PNs) have emerged as an alternative method with a promising performance and improved interpretability but have yet to reach the performance of the powerful DNN baselines. In this work, we aim to close this performance gap. We introduce a class of PNs, which are able to reach the performance of ResNet across a range of six benchmarks. We demonstrate that strong regularization is critical and conduct an extensive study of the exact regularization schemes required to match performance. To further motivate the regularization schemes, we introduce D-PolyNets that achieve a higher-degree of expansion than previously proposed polynomial networks. D-PolyNets are more parameter-efficient while achieving a similar performance as other polynomial networks. We expect that our new models can lead to an un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2303.13850</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22240;&#26524;&#24402;&#22240;&#23398;&#20064;&#65306;&#36229;&#36234;&#30452;&#25509;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25429;&#25417;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#22240;&#26524;&#26041;&#27861;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20551;&#35774;&#36755;&#20837;&#21464;&#37327;&#29420;&#31435;&#65288;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65289;&#65292;&#22240;&#27492;&#20165;&#30740;&#31350;&#30452;&#25509;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#22312;&#36755;&#20837;&#29305;&#24449;&#20013;&#24341;&#20837;&#36793;&#32536;&#20197;&#25429;&#25417;&#21644;&#32500;&#25252;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#26377;&#25928;&#30340;&#36817;&#20284;&#31574;&#30053;&#26469;&#37327;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#22240;&#26524;&#24402;&#22240;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#25509;&#36817;&#22522;&#26412;&#20107;&#23454;&#25928;&#26524;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#19981;&#21516;&#31038;&#20132;&#29305;&#24449;&#30340;&#20132;&#20114;&#20132;&#36890;&#22330;&#26223;&#19979;&#37319;&#21462;&#23433;&#20840;&#26377;&#25928;&#30340;&#26426;&#21160;&#26041;&#24335;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#21487;&#25511;&#34892;&#20026;&#29983;&#25104;&#65288;SCBG&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#23454;&#29616;&#20102;&#36924;&#30495;&#32780;&#31867;&#20154;&#30340;&#36712;&#36857;&#29983;&#25104;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#36712;&#36857;&#30340;&#31036;&#35980;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.13830</link><description>&lt;p&gt;
&#20132;&#36890;&#27169;&#25311;&#20013;&#30340;&#21487;&#32534;&#36753;&#39550;&#39542;&#35282;&#33394;&#65306;&#31038;&#20132;&#21487;&#25511;&#34892;&#20026;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Editing Driver Character: Socially-Controllable Behavior Generation for Interactive Traffic Simulation. (arXiv:2303.13830v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13830
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#19981;&#21516;&#31038;&#20132;&#29305;&#24449;&#30340;&#20132;&#20114;&#20132;&#36890;&#22330;&#26223;&#19979;&#37319;&#21462;&#23433;&#20840;&#26377;&#25928;&#30340;&#26426;&#21160;&#26041;&#24335;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#21487;&#25511;&#34892;&#20026;&#29983;&#25104;&#65288;SCBG&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#23454;&#29616;&#20102;&#36924;&#30495;&#32780;&#31867;&#20154;&#30340;&#36712;&#36857;&#29983;&#25104;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#36712;&#36857;&#30340;&#31036;&#35980;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27169;&#25311;&#22312;&#35780;&#20272;&#21644;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#20844;&#20849;&#36947;&#36335;&#19978;&#37096;&#32626;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21518;&#65292;&#38656;&#35201;&#19982;&#20855;&#26377;&#19981;&#21516;&#31038;&#20132;&#20559;&#22909;&#65288;&#20363;&#22914;&#65292;&#33258;&#31169;&#25110;&#24428;&#24428;&#26377;&#31036;&#30340;&#20154;&#31867;&#39550;&#39542;&#21592;&#65289;&#30340;&#20154;&#31867;&#36947;&#36335;&#21442;&#19982;&#32773;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#19981;&#21516;&#30340;&#20132;&#20114;&#20132;&#36890;&#22330;&#26223;&#20013;&#37319;&#21462;&#23433;&#20840;&#26377;&#25928;&#30340;&#26426;&#21160;&#26041;&#24335;&#65292;&#25105;&#20204;&#24212;&#35813;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#19982;&#24102;&#26377;&#19981;&#21516;&#31038;&#20132;&#29305;&#24449;&#30340;&#21453;&#24212;&#20195;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#21487;&#25511;&#34892;&#20026;&#29983;&#25104;&#65288;SCBG&#65289;&#27169;&#22411;&#65292;&#23427;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#29983;&#25104;&#36712;&#36857;&#30340;&#31036;&#35980;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#20174;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#20013;&#23398;&#20064;&#26469;&#30830;&#20445;&#36924;&#30495;&#21644;&#31867;&#20154;&#30340;&#36712;&#36857;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#39550;&#39542;&#34892;&#20026;&#30340;&#31036;&#35980;&#31243;&#24230;&#65292;&#24182;&#21033;&#29992;&#36793;&#38469;&#21644;&#26465;&#20214;&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic simulation plays a crucial role in evaluating and improving autonomous driving planning systems. After being deployed on public roads, autonomous vehicles need to interact with human road participants with different social preferences (e.g., selfish or courteous human drivers). To ensure that autonomous vehicles take safe and efficient maneuvers in different interactive traffic scenarios, we should be able to evaluate autonomous vehicles against reactive agents with different social characteristics in the simulation environment. We propose a socially-controllable behavior generation (SCBG) model for this purpose, which allows the users to specify the level of courtesy of the generated trajectory while ensuring realistic and human-like trajectory generation through learning from real-world driving data. Specifically, we define a novel and differentiable measure to quantify the level of courtesy of driving behavior, leveraging marginal and conditional behavior prediction models t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#22411;&#21487;&#24418;&#21464;&#21367;&#31215;&#21464;&#25442;&#22120;&#65288;DC Transformer&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35782;&#21035;&#21333;&#19968;&#21644;&#28151;&#21512;&#22411;&#26230;&#22278;&#32570;&#38519;&#65292;&#32858;&#28966;&#20110;&#20840;&#23616;&#29305;&#24449;&#65292;&#20934;&#30830;&#39044;&#27979;&#32570;&#38519;&#30340;&#25968;&#30446;&#21644;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13827</link><description>&lt;p&gt;
&#39640;&#25928;&#28151;&#21512;&#22411;&#26230;&#22278;&#32570;&#38519;&#22270;&#26696;&#35782;&#21035;&#20351;&#29992;&#32039;&#20945;&#22411;&#21487;&#24418;&#21464;&#21367;&#31215;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact Deformable Convolutional Transformers. (arXiv:2303.13827v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#22411;&#21487;&#24418;&#21464;&#21367;&#31215;&#21464;&#25442;&#22120;&#65288;DC Transformer&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35782;&#21035;&#21333;&#19968;&#21644;&#28151;&#21512;&#22411;&#26230;&#22278;&#32570;&#38519;&#65292;&#32858;&#28966;&#20110;&#20840;&#23616;&#29305;&#24449;&#65292;&#20934;&#30830;&#39044;&#27979;&#32570;&#38519;&#30340;&#25968;&#30446;&#21644;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#22278;&#21046;&#36896;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#25968;&#21315;&#20010;&#27493;&#39588;&#12290;&#26230;&#22278;&#32570;&#38519;&#22270;&#26696;&#35782;&#21035;&#65288;DPR&#65289;&#23545;&#20110;&#25214;&#21040;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#26230;&#22278;&#38136;&#36896;&#30340;&#20135;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#21333;&#19968;&#31867;&#22411;DPR&#30456;&#27604;&#65292;&#28151;&#21512;&#31867;&#22411;DPR&#30001;&#20110;&#31354;&#38388;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#32570;&#38519;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23384;&#22312;&#25968;&#30446;&#31561;&#21407;&#22240;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#20934;&#30830;&#39044;&#27979;&#32570;&#38519;&#30340;&#25968;&#30446;&#21644;&#31867;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32039;&#20945;&#22411;&#21487;&#24418;&#21464;&#21367;&#31215;&#21464;&#25442;&#22120;&#65288;DC Transformer&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DC Transformer&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#24418;&#21464;&#20869;&#26680;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#32858;&#28966;&#20110;&#26230;&#22278;&#22270;&#20013;&#23384;&#22312;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31616;&#27905;&#22320;&#27169;&#25311;&#20102;&#26230;&#22278;&#22270;&#21644;&#32570;&#38519;&#20043;&#38388;&#30340;&#20869;&#37096;&#20851;&#31995;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;38&#31181;&#32570;&#38519;&#27169;&#24335;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DC Transformer&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DC Transformer&#22312;&#35782;&#21035;&#21333;&#19968;&#20197;&#21450;&#28151;&#21512;&#22411;&#26230;&#22278;&#32570;&#38519;&#26041;&#38754;&#34920;&#29616;&#24471;&#24322;&#24120;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing wafers is an intricate task involving thousands of steps. Defect Pattern Recognition (DPR) of wafer maps is crucial to find the root cause of the issue and further improving the yield in the wafer foundry. Mixed-type DPR is much more complicated compared to single-type DPR due to varied spatial features, the uncertainty of defects, and the number of defects present. To accurately predict the number of defects as well as the types of defects, we propose a novel compact deformable convolutional transformer (DC Transformer). Specifically, DC Transformer focuses on the global features present in the wafer map by virtue of learnable deformable kernels and multi-head attention to the global features. The proposed method succinctly models the internal relationship between the wafer maps and the defects. DC Transformer is evaluated on a real dataset containing 38 defect patterns. Experimental results show that DC Transformer performs exceptionally well in recognizing both single 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#23376;&#20998;&#35299;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FDGAN&#65289;&#29992;&#20110;&#25991;&#26412;&#36716;&#22270;&#20687;&#21512;&#25104;&#65292;&#33021;&#22815;&#23558;&#21477;&#23376;&#23884;&#20837;&#21644;&#22122;&#22768;&#21521;&#37327;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#22240;&#23376;&#65292;&#24182;&#36890;&#36807;&#21152;&#24615;&#35268;&#33539;&#23618;&#26469;&#23545;&#40784;&#21644;&#34701;&#21512;&#25991;&#26412;-&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FDGAN&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.13821</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#23376;&#20998;&#35299;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#25991;&#26412;&#36716;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Factor Decomposed Generative Adversarial Networks for Text-to-Image Synthesis. (arXiv:2303.13821v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#23376;&#20998;&#35299;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FDGAN&#65289;&#29992;&#20110;&#25991;&#26412;&#36716;&#22270;&#20687;&#21512;&#25104;&#65292;&#33021;&#22815;&#23558;&#21477;&#23376;&#23884;&#20837;&#21644;&#22122;&#22768;&#21521;&#37327;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#22240;&#23376;&#65292;&#24182;&#36890;&#36807;&#21152;&#24615;&#35268;&#33539;&#23618;&#26469;&#23545;&#40784;&#21644;&#34701;&#21512;&#25991;&#26412;-&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FDGAN&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#25991;&#26412;&#36716;&#22270;&#20687;&#21512;&#25104;&#24037;&#20316;&#36890;&#24120;&#26159;&#23558;&#21477;&#23376;&#23884;&#20837;&#19982;&#22122;&#22768;&#21521;&#37327;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#32780;&#21477;&#23376;&#23884;&#20837;&#21644;&#22122;&#22768;&#21521;&#37327;&#26159;&#25511;&#21046;&#29983;&#25104;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#23376;&#12290;&#31616;&#21333;&#22320;&#23558;&#23427;&#20204;&#25340;&#25509;&#22312;&#19968;&#36215;&#20250;&#20351;&#28508;&#22312;&#30340;&#22240;&#23376;&#32416;&#32544;&#22312;&#19968;&#36215;&#65292;&#38459;&#30861;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20998;&#35299;&#36825;&#20004;&#20010;&#22240;&#23376;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#20998;&#35299;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FDGAN&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#22270;&#20687;&#65292;&#28982;&#21518;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#24402;&#19968;&#21270;&#23618;&#20013;&#24212;&#29992;&#21477;&#23376;&#23884;&#20837;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#21152;&#24615;&#35268;&#33539;&#23618;&#26469;&#23545;&#40784;&#21644;&#34701;&#21512;&#25991;&#26412;-&#22270;&#20687;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#36716;&#22270;&#20687;&#21512;&#25104;&#20013;&#20998;&#35299;&#22122;&#22768;&#21644;&#21477;&#23376;&#23884;&#20837;&#21487;&#20197;&#35299;&#24320;&#28508;&#22312;&#30340;&#22240;&#23376;&#65292;&#24182;&#20351;&#29983;&#25104;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;FDGAN&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works about text-to-image synthesis typically concatenated the sentence embedding with the noise vector, while the sentence embedding and the noise vector are two different factors, which control the different aspects of the generation. Simply concatenating them will entangle the latent factors and encumber the generative model.  In this paper, we attempt to decompose these two factors and propose Factor Decomposed Generative Adversarial Networks~(FDGAN). To achieve this, we firstly generate images from the noise vector and then apply the sentence embedding in the normalization layer for both generator and discriminators. We also design an additive norm layer to align and fuse the text-image features. The experimental results show that decomposing the noise and the sentence embedding can disentangle latent factors in text-to-image synthesis, and make the generative model more efficient. Compared with the baseline, FDGAN can achieve better performance, while fewer parameters are u
&lt;/p&gt;</description></item><item><title>marl-jax&#26159;&#19968;&#20010;&#22522;&#20110;DeepMind&#30340;JAX&#29983;&#24577;&#31995;&#21644;RL&#29983;&#24577;&#31995;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#22312;&#22810;&#26679;&#24615;&#32972;&#26223;&#19979;&#30340;&#31038;&#20250;&#26222;&#36866;&#24615;&#65292;&#25552;&#20379;&#21629;&#20196;&#34892;&#30028;&#38754;&#65292;&#36866;&#29992;&#20110;&#21512;&#20316;&#19982;&#31454;&#20105;&#28216;&#25103;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2303.13808</link><description>&lt;p&gt;
marl-jax&#65306;&#29992;&#20110;&#31038;&#20250;&#26222;&#36866;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
marl-jax: Multi-agent Reinforcement Leaning framework for Social Generalization. (arXiv:2303.13808v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13808
&lt;/p&gt;
&lt;p&gt;
marl-jax&#26159;&#19968;&#20010;&#22522;&#20110;DeepMind&#30340;JAX&#29983;&#24577;&#31995;&#21644;RL&#29983;&#24577;&#31995;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#22312;&#22810;&#26679;&#24615;&#32972;&#26223;&#19979;&#30340;&#31038;&#20250;&#26222;&#36866;&#24615;&#65292;&#25552;&#20379;&#21629;&#20196;&#34892;&#30028;&#38754;&#65292;&#36866;&#29992;&#20110;&#21512;&#20316;&#19982;&#31454;&#20105;&#28216;&#25103;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#35768;&#22810;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#36827;&#23637;&#26159;&#30001;&#31639;&#27861;&#21644;&#24037;&#31243;&#26041;&#38754;&#30340;&#25913;&#36827;&#39537;&#21160;&#30340;&#65292;&#23548;&#33268;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;marl-jax&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#30340;&#31038;&#20250;&#26222;&#36866;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36719;&#20214;&#21253;&#12290;&#35813;&#21253;&#26088;&#22312;&#35757;&#32451;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#19968;&#32452;&#20195;&#29702;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22810;&#26679;&#21270;&#32972;&#26223;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#24314;&#31435;&#22312;DeepMind&#30340;JAX&#29983;&#24577;&#31995;&#32479;&#19978;&#65292;&#24182;&#21033;&#29992;&#30001;DeepMind&#24320;&#21457;&#30340;RL&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;marl-jax&#26694;&#26550;&#33021;&#22815;&#22312;&#22810;&#20010;&#20195;&#29702;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#12289;&#21516;&#26102;&#34892;&#21160;&#30340;&#29615;&#22659;&#20013;&#24037;&#20316;&#12290;&#35813;&#21253;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#21629;&#20196;&#34892;&#30028;&#38754;&#65292;&#29992;&#20110;&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#24182;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#24635;&#20043;&#65292;marl-jax&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Reinforcement Learning (RL) have led to many exciting applications. These advancements have been driven by improvements in both algorithms and engineering, which have resulted in faster training of RL agents. We present marl-jax, a multi-agent reinforcement learning software package for training and evaluating social generalization of the agents. The package is designed for training a population of agents in multi-agent environments and evaluating their ability to generalize to diverse background agents. It is built on top of DeepMind's JAX ecosystem~\cite{deepmind2020jax} and leverages the RL ecosystem developed by DeepMind. Our framework marl-jax is capable of working in cooperative and competitive, simultaneous-acting environments with multiple agents. The package offers an intuitive and user-friendly command-line interface for training a population and evaluating its generalization capabilities. In conclusion, marl-jax provides a valuable resource for researchers
&lt;/p&gt;</description></item><item><title>UniTS&#26159;&#19968;&#20010;&#24102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#37096;&#20998;&#26631;&#35760;&#21644;&#39046;&#22495;&#36716;&#31227;&#31561;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13804</link><description>&lt;p&gt;
UniTS: &#19968;&#31181;&#24102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniTS: A Universal Time Series Analysis Framework with Self-supervised Representation Learning. (arXiv:2303.13804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13804
&lt;/p&gt;
&lt;p&gt;
UniTS&#26159;&#19968;&#20010;&#24102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#37096;&#20998;&#26631;&#35760;&#21644;&#39046;&#22495;&#36716;&#31227;&#31561;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#19981;&#21516;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#38754;&#20020;&#30528;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#21644;&#39046;&#22495;&#36716;&#31227;&#31561;&#23454;&#38469;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#20998;&#26512;&#24182;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;UniTS&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;&#25110;&#39044;&#35757;&#32451;&#65289;&#12290; UniTS&#30340;&#32452;&#20214;&#20351;&#29992;&#31867;&#20284;&#20110;sklearn&#30340;API&#36827;&#34892;&#35774;&#35745;&#65292;&#20197;&#20801;&#35768;&#28789;&#27963;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#29992;&#25143;&#22914;&#20309;&#20351;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;GUI&#25191;&#34892;&#20998;&#26512;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;UniTS&#22312;&#20116;&#20010;&#20027;&#27969;&#20219;&#21153;&#21644;&#20004;&#20010;&#23454;&#38469;&#35774;&#32622;&#20013;&#30456;&#36739;&#20110;&#20256;&#32479;&#29305;&#23450;&#20219;&#21153;&#26041;&#27861;&#27809;&#26377;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has emerged as a powerful tool for time series analysis. Existing methods are usually customized for different analysis tasks and face challenges in tackling practical problems such as partial labeling and domain shift. To achieve universal analysis and address the aforementioned problems, we develop UniTS, a novel framework that incorporates self-supervised representation learning (or pre-training). The components of UniTS are designed using sklearn-like APIs to allow flexible extensions. We demonstrate how users can easily perform an analysis task using the user-friendly GUIs, and show the superior performance of UniTS over the traditional task-specific methods without self-supervised pre-training on five mainstream tasks and two practical settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCot&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#21644;&#20266;&#26631;&#31614;&#65292;&#23454;&#29616;&#24320;&#25918;&#22495;&#27133;&#22635;&#20805;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13801</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#21327;&#21516;&#35757;&#32451;&#23454;&#29616;&#24320;&#25918;&#22495;&#27133;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
Toward Open-domain Slot Filling via Self-supervised Co-training. (arXiv:2303.13801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCot&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#21644;&#20266;&#26631;&#31614;&#65292;&#23454;&#29616;&#24320;&#25918;&#22495;&#27133;&#22635;&#20805;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27133;&#22635;&#20805;&#26159;&#29616;&#20195;&#20250;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#20043;&#19968;&#12290;&#29616;&#26377;&#22823;&#37096;&#20998;&#25991;&#29486;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#38656;&#35201;&#27599;&#20010;&#26032;&#22495;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#31561;&#24050;&#34920;&#29616;&#20986;&#26367;&#20195;&#25163;&#21160;&#26631;&#27880;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#36825;&#20123;&#23398;&#20064;&#33539;&#20363;&#22312;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#36874;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#24182;&#23637;&#31034;&#24320;&#25918;&#22495;&#27133;&#22635;&#20805;&#30340;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;SCot&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#20869;&#25163;&#21160;&#26631;&#35760;&#35757;&#32451;&#31034;&#20363;&#24182;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called SCot, that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised cotraining mechanism, where both
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26032;&#26694;&#26550;P-ToD&#65292;&#23427;&#20351;&#29992;&#38646;-shot&#27867;&#21270;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.13797</link><description>&lt;p&gt;
&#38646;-shot&#27867;&#21270;&#22870;&#21169;&#20989;&#25968;&#20010;&#24615;&#21270;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function. (arXiv:2303.13797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26032;&#26694;&#26550;P-ToD&#65292;&#23427;&#20351;&#29992;&#38646;-shot&#27867;&#21270;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20351;&#29992;&#25143;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23436;&#25104;&#20219;&#21153;&#12290;&#29616;&#26377;&#25216;&#26415;&#30340;&#31995;&#32479;&#26080;&#35770;&#29992;&#25143;&#22914;&#20309;&#65292;&#37117;&#20250;&#29992;&#30456;&#21516;&#30340;&#26041;&#24335;&#22238;&#24212;&#65292;&#20294;&#26159;&#33258;&#23450;&#20041;&#23545;&#35805;&#21487;&#33021;&#20250;&#25552;&#39640;&#37319;&#29992;&#29575;&#21644;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#26500;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#24037;&#20316;&#38754;&#23545;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#24037;&#20316;&#20381;&#36182;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38656;&#35201;&#27599;&#20010;&#29992;&#25143;&#36164;&#26009;&#36827;&#34892;&#32321;&#29712;&#21644;&#26114;&#36149;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#20026;&#27599;&#20010;&#29992;&#25143;&#26723;&#26696;&#25910;&#38598;&#21644;&#26631;&#35760;&#25968;&#25454;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;P-ToD&#65292;&#36890;&#36807;&#38646;-shot&#27867;&#21270;&#22870;&#21169;&#20989;&#25968;&#65292;&#20010;&#24615;&#21270;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#36866;&#24212;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#36164;&#26009;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;P-ToD&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialog systems enable users to accomplish tasks using natural language. State-of-the-art systems respond to users in the same way regardless of their personalities, although personalizing dialogues can lead to higher levels of adoption and better user experiences. Building personalized dialog systems is an important, yet challenging endeavor and only a handful of works took on the challenge. Most existing works rely on supervised learning approaches and require laborious and expensive labeled training data for each user profile. Additionally, collecting and labeling data for each user profile is virtually impossible. In this work, we propose a novel framework, P-ToD, to personalize task-oriented dialog systems capable of adapting to a wide range of user profiles in an unsupervised fashion using a zero-shot generalizable reward function. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three phases. Phase one performs task-specific training. Phase two kicks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#28857;&#35009;&#21098;&#31639;&#27861;(MKPC)&#26469;&#23450;&#20301;&#12289;&#25552;&#21462;&#21644;&#35009;&#21098;&#21253;&#21547;&#37325;&#35201;&#20449;&#24687;&#30340;&#20849;&#35270;&#21306;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20108;&#38454;&#27573;&#31649;&#32447;&#29992;&#20110;&#22270;&#20687;&#21305;&#37197;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#25143;&#22806;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#21305;&#37197;&#24615;&#33021;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#20026;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2303.13794</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#28857;&#35009;&#21098;&#30340;&#39640;&#25928;&#19982;&#20934;&#30830;&#20849;&#35270;&#21306;&#22495;&#23450;&#20301;&#31639;&#27861;(MKPC)&#65306;&#19968;&#20010;&#20108;&#38454;&#27573;&#31649;&#32447;&#29992;&#20110;&#25552;&#39640;&#22270;&#20687;&#21305;&#37197;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient and Accurate Co-Visible Region Localization with Matching Key-Points Crop (MKPC): A Two-Stage Pipeline for Enhancing Image Matching Performance. (arXiv:2303.13794v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#28857;&#35009;&#21098;&#31639;&#27861;(MKPC)&#26469;&#23450;&#20301;&#12289;&#25552;&#21462;&#21644;&#35009;&#21098;&#21253;&#21547;&#37325;&#35201;&#20449;&#24687;&#30340;&#20849;&#35270;&#21306;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20108;&#38454;&#27573;&#31649;&#32447;&#29992;&#20110;&#22270;&#20687;&#21305;&#37197;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#25143;&#22806;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#21305;&#37197;&#24615;&#33021;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#20026;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21305;&#37197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#26159;&#19968;&#20010;&#32463;&#20856;&#19988;&#22522;&#30784;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20551;&#35774;&#20849;&#35270;&#21306;&#22495;&#22806;&#37096;&#30340;&#21306;&#22495;&#20960;&#20046;&#19981;&#21253;&#21547;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#29305;&#24449;&#28857;&#35009;&#21098;&#31639;&#27861;(MKPC)&#65292;&#23450;&#20301;&#12289;&#25552;&#21462;&#21644;&#35009;&#21098;&#21253;&#21547;&#37325;&#35201;&#20449;&#24687;&#30340;&#20849;&#35270;&#21306;&#22495;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;MKPC&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20108;&#38454;&#27573;&#31649;&#32447;&#29992;&#20110;&#22270;&#20687;&#21305;&#37197;&#65292;&#23427;&#23545;&#20110;&#20219;&#20309;&#22270;&#20687;&#21305;&#37197;&#27169;&#22411;&#25110;&#32452;&#21512;&#37117;&#36866;&#29992;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#23558;SuperPoint + SuperGlue&#20316;&#20026;&#22270;&#20687;&#21305;&#37197;&#27169;&#22411;&#65292;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25143;&#22806;&#20301;&#23039;&#20272;&#35745;&#26041;&#38754;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#20844;&#24179;&#30340;&#27604;&#36739;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;2022&#24180;&#22270;&#20687;&#21305;&#37197;&#25361;&#25112;&#36187;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#20195;&#34920;&#30446;&#21069;&#26368;&#38590;&#30340;&#25143;&#22806;&#22270;&#20687;&#21305;&#37197;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image matching is a classic and fundamental task in computer vision. In this paper, under the hypothesis that the areas outside the co-visible regions carry little information, we propose a matching key-points crop (MKPC) algorithm. The MKPC locates, proposes and crops the critical regions, which are the co-visible areas with great efficiency and accuracy. Furthermore, building upon MKPC, we propose a general two-stage pipeline for image matching, which is compatible to any image matching models or combinations. We experimented with plugging SuperPoint + SuperGlue into the two-stage pipeline, whose results show that our method enhances the performance for outdoor pose estimations. What's more, in a fair comparative condition, our method outperforms the SOTA on Image Matching Challenge 2022 Benchmark, which represents the hardest outdoor benchmark of image matching currently.
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#30456;&#20851;&#20107;&#20214;&#30340;&#39044;&#27979;&#31454;&#36187;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22359;&#30456;&#20851;&#30340;&#27010;&#24565;&#12290;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#30456;&#20851;&#24615;&#30340;&#20998;&#24067;&#19979;&#65292;&#22522;&#20110;follow-the-regularized-leader(FTRL)&#30340;&#31454;&#36187;&#26426;&#21046;&#20173;&#28982;&#20445;&#30041;&#20102;&#23427;&#30340;$\epsilon$-&#26368;&#20248;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.13793</link><description>&lt;p&gt;
&#24102;&#26377;&#30456;&#20851;&#20107;&#20214;&#30340;&#39044;&#27979;&#31454;&#36187;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting Competitions with Correlated Events. (arXiv:2303.13793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13793
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#30456;&#20851;&#20107;&#20214;&#30340;&#39044;&#27979;&#31454;&#36187;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22359;&#30456;&#20851;&#30340;&#27010;&#24565;&#12290;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#30456;&#20851;&#24615;&#30340;&#20998;&#24067;&#19979;&#65292;&#22522;&#20110;follow-the-regularized-leader(FTRL)&#30340;&#31454;&#36187;&#26426;&#21046;&#20173;&#28982;&#20445;&#30041;&#20102;&#23427;&#30340;$\epsilon$-&#26368;&#20248;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24120;&#35265;&#30340;&#36194;&#23478;&#36890;&#21507;&#26426;&#21046;&#20013;&#23384;&#22312;&#30340;&#28608;&#21169;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#39044;&#27979;&#31454;&#36187;&#30740;&#31350;&#20174;Witkowski&#31561;&#20154;[2022]&#24320;&#22987;&#12290;Frongillo&#31561;&#20154;[2021]&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;follow-the-regularized-leader(FTRL)&#30340;&#31454;&#36187;&#26426;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#12290;&#20182;&#20204;&#35777;&#26126;&#20182;&#20204;&#30340;&#26426;&#21046;&#20165;&#20351;&#29992;$O(\log(n)/\epsilon^2)$&#20010;&#20107;&#20214;&#23601;&#33021;&#39640;&#27010;&#29575;&#36873;&#25321;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#30340;&#39044;&#27979;&#32773;&#12290;&#36825;&#20123;&#24037;&#20316;&#20197;&#21450;&#20043;&#21069;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#25152;&#26377;&#24037;&#20316;&#37117;&#20551;&#35774;&#20107;&#20214;&#26159;&#29420;&#31435;&#30340;&#12290;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#24102;&#26377;&#30456;&#20851;&#20107;&#20214;&#30340;&#39044;&#27979;&#31454;&#36187;&#12290;&#20026;&#20102;&#37327;&#21270;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22359;&#30456;&#20851;&#30340;&#27010;&#24565;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#20107;&#20214;&#19982;&#26368;&#22810;$b$&#20010;&#20107;&#20214;&#24378;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20855;&#26377;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20998;&#24067;&#19979;&#65292;FTRL&#26426;&#21046;&#20173;&#28982;&#20351;&#29992;$O(b^2 \log(n)/\epsilon^2)$&#20010;&#20107;&#20214;&#20445;&#30041;&#20102;&#23427;&#30340;$\epsilon$-&#26368;&#20248;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#28041;&#21450;&#21040;&#19968;&#31181;&#26032;&#30340;&#30456;&#20851;&#38543;&#26426;&#21464;&#37327;&#27987;&#24230;&#30028;&#65292;&#36825;&#21487;&#33021;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beginning with Witkowski et al. [2022], recent work on forecasting competitions has addressed incentive problems with the common winner-take-all mechanism. Frongillo et al. [2021] propose a competition mechanism based on follow-the-regularized-leader (FTRL), an online learning framework. They show that their mechanism selects an $\epsilon$-optimal forecaster with high probability using only $O(\log(n)/\epsilon^2)$ events. These works, together with all prior work on this problem thus far, assume that events are independent. We initiate the study of forecasting competitions for correlated events. To quantify correlation, we introduce a notion of block correlation, which allows each event to be strongly correlated with up to $b$ others. We show that under distributions with this correlation, the FTRL mechanism retains its $\epsilon$-optimal guarantee using $O(b^2 \log(n)/\epsilon^2)$ events. Our proof involves a novel concentration bound for correlated random variables which may be of br
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#30149;&#20154;&#20934;&#21017;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#32771;&#34385;&#20102;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#30340;&#23884;&#20837;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13790</link><description>&lt;p&gt;
&#36890;&#36807;&#30149;&#20154;&#20934;&#21017;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#23454;&#29616;&#20844;&#24179;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint. (arXiv:2303.13790v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#30149;&#20154;&#20934;&#21017;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#32771;&#34385;&#20102;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#30340;&#23884;&#20837;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#26032;&#22411;&#27835;&#30103;&#26041;&#27861;&#30340;&#24320;&#21457;&#20013;&#19981;&#21487;&#25110;&#32570;&#65292;&#20294;&#30001;&#20110;&#25307;&#21215;&#21644;&#30041;&#23384;&#30149;&#20154;&#30340;&#38590;&#24230;&#65292;&#24448;&#24448;&#38590;&#20197;&#25307;&#21215;&#36275;&#22815;&#25968;&#37327;&#30340;&#21442;&#19982;&#32773;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24050;&#32463;&#21019;&#24314;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#26041;&#27861;&#12290;&#36825;&#20123;&#26694;&#26550;&#20250;&#35745;&#31639;&#30149;&#20154;&#21644;&#20020;&#24202;&#35797;&#39564;&#30340;&#30456;&#20284;&#24230;&#65292;&#32771;&#34385;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26694;&#26550;&#30340;&#24615;&#33021;&#20248;&#20110;&#26089;&#26399;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24403;&#26576;&#20123;&#25935;&#24863;&#20154;&#32676;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#34987;&#20302;&#20272;&#26102;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#25968;&#25454;&#19981;&#23436;&#25972;&#25110;&#19981;&#20934;&#30830;&#65292;&#20174;&#32780;&#23545;&#24739;&#32773;&#36896;&#25104;&#28508;&#22312;&#21361;&#23475;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#30149;&#20154;&#20934;&#21017;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#26469;&#35299;&#20915;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#30340;&#23884;&#20837;&#24046;&#24322;&#65292;&#22522;&#20110;&#23884;&#20837;&#24046;&#24322;&#21046;&#23450;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are indispensable in developing new treatments, but they face obstacles in patient recruitment and retention, hindering the enrollment of necessary participants. To tackle these challenges, deep learning frameworks have been created to match patients to trials. These frameworks calculate the similarity between patients and clinical trial eligibility criteria, considering the discrepancy between inclusion and exclusion criteria. Recent studies have shown that these frameworks outperform earlier approaches. However, deep learning models may raise fairness issues in patient-trial matching when certain sensitive groups of individuals are underrepresented in clinical trials, leading to incomplete or inaccurate data and potential harm. To tackle the issue of fairness, this work proposes a fair patient-trial matching framework by generating a patient-criterion level fairness constraint. The proposed framework considers the inconsistency between the embedding of inclusion and e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20998;&#35010;&#24182;&#34892;&#65292;&#24212;&#29992;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#19978;&#65292;&#33021;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#24182;&#34892;&#26041;&#27861;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#21516;&#26102;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2303.13775</link><description>&lt;p&gt;
GSplit: &#36890;&#36807;&#20998;&#35010;&#24182;&#34892;&#23454;&#29616;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism. (arXiv:2303.13775v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20998;&#35010;&#24182;&#34892;&#65292;&#24212;&#29992;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#19978;&#65292;&#33021;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#24182;&#34892;&#26041;&#27861;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#21516;&#26102;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#34892;&#19994;&#12289;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#12289;&#31038;&#20132;&#22270;&#20998;&#26512;&#12289;&#30693;&#35782;&#24211;&#12289;&#26448;&#26009;&#31185;&#23398;&#21644;&#29983;&#29289;&#23398;&#65289;&#20013;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#36793;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30001;&#20110;&#22312;&#21508;&#31181;&#22270;&#20998;&#26512;&#20219;&#21153;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#26469;&#23398;&#20064;&#36825;&#20123;&#22270;&#24418;&#12290;&#22312;&#22823;&#22411;&#22270;&#24418;&#19978;&#35757;&#32451;&#36890;&#24120;&#37319;&#29992;&#23567;&#25209;&#37327;&#35757;&#32451;&#65292;&#24182;&#19988;&#25968;&#25454;&#24182;&#34892;&#26159;&#23558;&#23567;&#25209;&#37327;&#35757;&#32451;&#25193;&#23637;&#21040;&#22810;&#20010; GPU &#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;GNN &#35757;&#32451;&#31995;&#32479;&#30340;&#20960;&#20010;&#22522;&#26412;&#24615;&#33021;&#29942;&#39048;&#19982;&#25968;&#25454;&#24182;&#34892;&#26041;&#27861;&#30340;&#22266;&#26377;&#38480;&#21046;&#26377;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#33539;&#24335;- &#20998;&#35010;&#24182;&#34892;&#65292;&#24182;&#23558;&#20854;&#23454;&#29616;&#22312;&#19968;&#20010;&#21517;&#20026;gsplit&#30340;&#26032;&#31995;&#32479;&#20013;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;gsplit &#30340;&#24615;&#33021;&#20248;&#20110;DGL&#12289;Quiver&#21644;PaGraph&#31561;&#29616;&#26377;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale graphs with billions of edges are ubiquitous in many industries, science, and engineering fields such as recommendation systems, social graph analysis, knowledge base, material science, and biology. Graph neural networks (GNN), an emerging class of machine learning models, are increasingly adopted to learn on these graphs due to their superior performance in various graph analytics tasks. Mini-batch training is commonly adopted to train on large graphs, and data parallelism is the standard approach to scale mini-batch training to multiple GPUs. In this paper, we argue that several fundamental performance bottlenecks of GNN training systems have to do with inherent limitations of the data parallel approach. We then propose split parallelism, a novel parallel mini-batch training paradigm. We implement split parallelism in a novel system called gsplit and show that it outperforms state-of-the-art systems such as DGL, Quiver, and PaGraph.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13773</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65306;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#27169;&#22411;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models. (arXiv:2303.13773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26356;&#26377;&#25928;&#22320;&#35843;&#24230;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#12290;&#22312;&#31163;&#32447;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#65288;ONTS&#65289;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#22312;&#36712;&#36947;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26368;&#20339;&#23433;&#25490;&#65292;&#21516;&#26102;&#32771;&#34385;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#20248;&#20808;&#32423;&#65292;&#26368;&#23567;&#21644;&#26368;&#22823;&#28608;&#27963;&#20107;&#20214;&#65292;&#25191;&#34892;&#26102;&#38388;&#26694;&#26550;&#65292;&#21608;&#26399;&#21644;&#25191;&#34892;&#31383;&#21475;&#65292;&#20197;&#21450;&#21355;&#26143;&#30005;&#21147;&#36164;&#28304;&#21644;&#33021;&#37327;&#25910;&#38598;&#21644;&#31649;&#29702;&#30340;&#22797;&#26434;&#24615;&#30340;&#32422;&#26463;&#12290;ONTS&#38382;&#39064;&#24050;&#32463;&#20351;&#29992;&#20256;&#32479;&#30340;&#25968;&#23398;&#20844;&#24335;&#21644;&#31934;&#30830;&#26041;&#27861;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;GNN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#35843;&#24230;&#38382;&#39064;&#21644;&#35774;&#26045;&#25918;&#32622;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ONTS&#38382;&#39064;&#30340;MILP&#23454;&#20363;&#23436;&#20840;&#34920;&#31034;&#25104;&#20108;&#20998;&#22270;&#32593;&#32476;&#32467;&#26500;&#26469;&#24212;&#29992;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates how to schedule nanosatellite tasks more efficiently using Graph Neural Networks (GNN). In the Offline Nanosatellite Task Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks to be carried out in orbit while taking into account Quality-of-Service (QoS) considerations such as priority, minimum and maximum activation events, execution time-frames, periods, and execution windows, as well as constraints on the satellite's power resources and the complexity of energy harvesting and management. The ONTS problem has been approached using conventional mathematical formulations and precise methods, but their applicability to challenging cases of the problem is limited. This study examines the use of GNNs in this context, which has been effectively applied to many optimization problems, including traveling salesman problems, scheduling problems, and facility placement problems. Here, we fully represent MILP instances of the ONTS problem in biparti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13769</link><description>&lt;p&gt;
&#26410;&#30693;&#21957;&#25506;&#22120;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#65306;&#19981;&#35201;&#23545;&#26410;&#30693;&#23545;&#35937;&#35270;&#32780;&#19981;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25552;&#20986;&#30340;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#21644;&#24320;&#25918;&#38598;&#26816;&#27979;&#22312;&#23547;&#25214;&#20174;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24182;&#23558;&#20854;&#19982;&#24050;&#30693;&#31867;&#21035;&#21306;&#20998;&#24320;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#23545;&#20174;&#24050;&#30693;&#31867;&#21035;&#21521;&#26410;&#30693;&#31867;&#21035;&#30340;&#30693;&#35782;&#20256;&#36882;&#30340;&#30740;&#31350;&#38656;&#35201;&#26356;&#28145;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#25506;&#27979;&#38544;&#34255;&#22312;&#32972;&#26223;&#20013;&#30340;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#26469;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#65292;&#20165;&#20351;&#29992;&#24050;&#30693;&#31867;&#21035;&#26679;&#26412;&#36827;&#34892;&#30417;&#30563;&#21644;&#36991;&#20813;&#22312;&#32972;&#26223;&#20013;&#19981;&#36866;&#24403;&#22320;&#21387;&#21046;&#26410;&#30693;&#29289;&#20307;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24050;&#30693;&#29289;&#20307;&#23398;&#20064;&#21040;&#30340;&#36825;&#31181;&#32622;&#20449;&#24230;&#20998;&#25968;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#29289;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#36827;&#19968;&#27493;&#38480;&#21046;&#32972;&#26223;&#20013;&#38750;&#29289;&#20307;&#26679;&#26412;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30001;&#20110;&#32570;&#20047;&#23427;&#20204;&#22312;&#35757;&#32451;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#30340;&#26368;&#20339;&#26694;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;</title><link>http://arxiv.org/abs/2303.13763</link><description>&lt;p&gt;
&#26080;&#38656;&#36793;&#32536;&#20294;&#20855;&#26377;&#32467;&#26500;&#24863;&#30693;&#24615;&#65306;&#20174;GNN&#21040;MLP&#30340;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#31934;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20219;&#21153;&#20013;&#21387;&#32553;&#25104;&#20302;&#24310;&#36831;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20250;&#23558;&#22270;&#30340;&#36793;&#32536;&#22788;&#29702;&#25104;&#39069;&#22806;&#30340;&#36755;&#20837;&#32473;MLP&#65292;&#20294;&#36825;&#26679;&#30340;&#22270;&#32467;&#26500;&#23545;&#20110;&#21508;&#31181;&#22330;&#26223;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GNN&#25945;&#24072;&#20013;&#30340;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21407;&#22411;&#22312;&#26080;&#36793;&#32536;&#35774;&#32622;&#20013;&#20174;GNN&#21040;MLP&#36827;&#34892;&#20102;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#27969;&#34892;&#30340;&#22270;&#24418;&#22522;&#20934;&#23454;&#39564;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PGKD&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. However, MLPs rely exclusively on the node features and fail to capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22270;&#22686;&#24378;&#26041;&#27861; (SAug)&#65292;&#36890;&#36807;&#35782;&#21035;&#22270;&#20013;&#30340;&#20013;&#24515;&#33410;&#28857;&#21644;&#23614;&#33410;&#28857;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#23614;&#33410;&#28857;&#26102;&#12290;</title><link>http://arxiv.org/abs/2303.13757</link><description>&lt;p&gt;
&#32467;&#26500;&#19981;&#24179;&#34913;&#24863;&#30693;&#30340;&#22270;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structural Imbalance Aware Graph Augmentation Learning. (arXiv:2303.13757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22270;&#22686;&#24378;&#26041;&#27861; (SAug)&#65292;&#36890;&#36807;&#35782;&#21035;&#22270;&#20013;&#30340;&#20013;&#24515;&#33410;&#28857;&#21644;&#23614;&#33410;&#28857;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#23614;&#33410;&#28857;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064; (GML) &#22312;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#12289;&#22270;&#20998;&#31867;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#30340;&#22270;&#24448;&#24448;&#20855;&#26377;&#32467;&#26500;&#19981;&#24179;&#34913;&#24615;&#65292;&#21363;&#21482;&#26377;&#23569;&#25968;&#20960;&#20010;&#20013;&#24515;&#33410;&#28857;&#20855;&#26377;&#26356;&#23494;&#38598;&#30340;&#23616;&#37096;&#32467;&#26500;&#21644;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#21487;&#33021;&#20250;&#25439;&#23475;&#29616;&#26377; GML &#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#23614;&#33410;&#28857;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22270;&#22686;&#24378;&#26041;&#27861; (SAug) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;Pagerank&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#35782;&#21035;&#22270;&#20013;&#30340;&#20013;&#24515;&#33410;&#28857;&#21644;&#23614;&#33410;&#28857;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;&#20013;&#24515;&#33410;&#28857;&#30340;&#22122;&#22768;&#37051;&#23621;&#25918;&#24323;&#22312;&#19968;&#20391;&#65292;&#24182;&#22312;&#21478;&#19968;&#20391;&#21457;&#29616;&#28508;&#22312;&#30340;&#37051;&#23621;&#21644;&#20026;&#23614;&#33410;&#28857;&#29983;&#25104;&#20266;&#37051;&#23621;&#12290;&#23427;&#36824;&#21487;&#20197;&#20943;&#36731;&#20004;&#31181;&#31867;&#22411;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#22312;&#22686;&#24378;&#21518;&#30340;&#22270;&#19978;&#37325;&#26032;&#35757;&#32451; GNN &#27169;&#22411;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SAug &#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph machine learning (GML) has made great progress in node classification, link prediction, graph classification and so on. However, graphs in reality are often structurally imbalanced, that is, only a few hub nodes have a denser local structure and higher influence. The imbalance may compromise the robustness of existing GML models, especially in learning tail nodes. This paper proposes a selective graph augmentation method (SAug) to solve this problem. Firstly, a Pagerank-based sampling strategy is designed to identify hub nodes and tail nodes in the graph. Secondly, a selective augmentation strategy is proposed, which drops the noisy neighbors of hub nodes on one side, and discovers the latent neighbors and generates pseudo neighbors for tail nodes on the other side. It can also alleviate the structural imbalance between two types of nodes. Finally, a GNN model will be retrained on the augmented graph. Extensive experiments demonstrate that SAug can significantly improve the backb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ViT&#31639;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;</title><link>http://arxiv.org/abs/2303.13755</link><description>&lt;p&gt;
Sparsifiner&#65306;&#23398;&#20064;&#31232;&#30095;&#30340;&#23454;&#20363;&#30456;&#20851;&#27880;&#24847;&#21147;&#29992;&#20110;&#39640;&#25928;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers. (arXiv:2303.13755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ViT&#31639;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#30456;&#27604;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#20294;&#24448;&#24448;&#20276;&#38543;&#30528;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#19968;&#23450;&#25968;&#37327;&#30340;&#31354;&#38388;&#30456;&#37051;&#20196;&#29260;&#26469;&#25506;&#32034;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20197;&#21152;&#36895;ViT&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MHSA&#65289;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#32467;&#26500;&#21270;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#23558;&#20196;&#29260;&#19982;&#20854;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#20196;&#29260;&#20043;&#38388;&#30340;&#20196;&#29260; - &#20196;&#29260;&#36830;&#25509;&#38480;&#21046;&#22312;&#20102;&#19968;&#23450;&#33539;&#22260;&#20869;&#65292;&#36825;&#19981;&#32771;&#34385;&#20174;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#20013;&#23398;&#20064;&#30340;&#35821;&#20041;&#36830;&#25509;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#36830;&#25509;&#24615;&#39044;&#27979;&#27169;&#22359;&#26469;&#23398;&#20064;&#23454;&#20363;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#30452;&#35266;&#30340;&#35828;&#65292;&#22914;&#26524;&#35748;&#20026;&#29305;&#24449;&#22312;&#31354;&#38388;&#25110;&#35821;&#20041;&#19978;&#26159;&#30456;&#20851;&#30340;&#65292;&#21017;&#20004;&#20010;&#26631;&#35760;&#20855;&#26377;&#39640;&#30340;&#36830;&#25509;&#24471;&#20998;&#12290;&#30001;&#20110;&#27599;&#20010;&#26631;&#35760;&#21482;&#19982;&#23569;&#37327;&#20854;&#20182;&#26631;&#35760;&#30456;&#20851;&#65292;&#22240;&#27492;&#20108;&#20803;&#21270;&#36830;&#25509;&#25513;&#30721;&#36890;&#24120;&#26159;&#26377;&#25928;&#30340; &#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViT) have shown their competitive advantages performance-wise compared to convolutional neural networks (CNNs) though they often come with high computational costs. To this end, previous methods explore different attention patterns by limiting a fixed number of spatially nearby tokens to accelerate the ViT's multi-head self-attention (MHSA) operations. However, such structured attention patterns limit the token-to-token connections to their spatial relevance, which disregards learned semantic connections from a full attention mask. In this work, we propose a novel approach to learn instance-dependent attention patterns, by devising a lightweight connectivity predictor module to estimate the connectivity score of each pair of tokens. Intuitively, two tokens have high connectivity scores if the features are considered relevant either spatially or semantically. As each token only attends to a small number of other tokens, the binarized connectivity masks are often ver
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#21160;&#24577;&#26550;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#20132;&#26367;&#20004;&#20010;&#30446;&#26631;&#30340;&#26041;&#24335;&#65292;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#26102;&#19981;&#20250;&#28798;&#38590;&#24615;&#22320;&#36951;&#24536;&#24050;&#32463;&#23398;&#20064;&#30340;&#20869;&#23481;&#12290;&#35813;&#30740;&#31350;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13752</link><description>&lt;p&gt;
&#21033;&#29992;&#26087;&#30693;&#35782;&#22312;&#21307;&#30103;&#24433;&#20687;&#20013;&#25345;&#32493;&#23398;&#20064;&#26032;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Old Knowledge to Continually Learn New Classes in Medical Images. (arXiv:2303.13752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#21160;&#24577;&#26550;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#20132;&#26367;&#20004;&#20010;&#30446;&#26631;&#30340;&#26041;&#24335;&#65292;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#26102;&#19981;&#20250;&#28798;&#38590;&#24615;&#22320;&#36951;&#24536;&#24050;&#32463;&#23398;&#20064;&#30340;&#20869;&#23481;&#12290;&#35813;&#30740;&#31350;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#22686;&#37327;&#25345;&#32493;&#23398;&#20064;&#26159;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26680;&#24515;&#27493;&#39588;&#65292;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#26032;&#27010;&#24565;&#32780;&#19981;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#19981;&#26029;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#23588;&#20854;&#38656;&#35201;&#19981;&#26029;&#20174;&#26032;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20197;&#23545;&#25193;&#23637;&#30340;&#30142;&#30149;&#38598;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#26087;&#30693;&#35782;&#26469;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#19968;&#20010;&#20855;&#26377;&#25193;&#23637;&#34920;&#31034;&#30340;&#21160;&#24577;&#26550;&#26500;&#65292;&#20197;&#20445;&#30041;&#20808;&#21069;&#23398;&#20064;&#30340;&#29305;&#24449;&#24182;&#23481;&#32435;&#26032;&#29305;&#24449;&#65307; &#65288;2&#65289;&#19968;&#20010;&#20132;&#26367;&#20004;&#20010;&#30446;&#26631;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#22312;&#32500;&#25252;&#27169;&#22411;&#23545;&#26087;&#31867;&#21035;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#24179;&#34913;&#23398;&#20064;&#26032;&#29305;&#24449;&#12290;&#22810;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental continual learning is a core step towards developing artificial intelligence systems that can continuously adapt to changes in the environment by learning new concepts without forgetting those previously learned. This is especially needed in the medical domain where continually learning from new incoming data is required to classify an expanded set of diseases. In this work, we focus on how old knowledge can be leveraged to learn new classes without catastrophic forgetting. We propose a framework that comprises of two main components: (1) a dynamic architecture with expanding representations to preserve previously learned features and accommodate new features; and (2) a training procedure alternating between two objectives to balance the learning of new features while maintaining the model's performance on old classes. Experiment results on multiple medical datasets show that our solution is able to achieve superior performance over state-of-the-art baselines in terms
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;LONGNN&#65292;&#23427;&#37319;&#29992;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#22810;&#39033;&#24335;&#22522;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#25152;&#24102;&#26469;&#30340;&#32570;&#38519;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13750</link><description>&lt;p&gt;
LONGNN: &#20855;&#26377;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LONGNN: Spectral GNNs with Learnable Orthonormal Basis. (arXiv:2303.13750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;LONGNN&#65292;&#23427;&#37319;&#29992;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#22810;&#39033;&#24335;&#22522;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#25152;&#24102;&#26469;&#30340;&#32570;&#38519;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#37327;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#31995;&#25968;&#30340;&#22810;&#39033;&#24335;&#22522;&#22312;&#35768;&#22810;&#33410;&#28857;&#32423;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39030;&#32423;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#22810;&#39033;&#24335;&#22522;&#65292;&#20294;&#26159;&#27599;&#31181;&#26041;&#27861;&#37117;&#37319;&#29992;&#20102;&#22266;&#23450;&#30340;&#22810;&#39033;&#24335;&#22522;&#65292;&#21487;&#33021;&#19981;&#26159;&#32473;&#23450;&#22270;&#24418;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#26041;&#27861;&#25152;&#35859;&#30340;&#36234;&#30028;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#36825;&#22312;&#23427;&#20204;&#19981;&#22826;&#31995;&#32479;&#21270;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#19978;&#26377;&#25152;&#26681;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;&#38597;&#21508;&#27604;&#22810;&#39033;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#30340;&#35889;GNN&#65292;LON-GNN&#65292;&#24182;&#35777;&#26126;&#20102;&#27491;&#21017;&#21270;&#31995;&#25968;&#29616;&#22312;&#31561;&#25928;&#20110;&#27491;&#21017;&#21270;&#25152;&#23398;&#28388;&#27874;&#20989;&#25968;&#30340;&#33539;&#25968;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;LON-GNN&#30340;&#25311;&#21512;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a plethora of spectral graph neural networks (GNN) methods have utilized polynomial basis with learnable coefficients to achieve top-tier performances on many node-level tasks. Although various kinds of polynomial bases have been explored, each such method adopts a fixed polynomial basis which might not be the optimal choice for the given graph. Besides, we identify the so-called over-passing issue of these methods and show that it is somewhat rooted in their less-principled regularization strategy and unnormalized basis. In this paper, we make the first attempts to address these two issues. Leveraging Jacobi polynomials, we design a novel spectral GNN, LON-GNN, with Learnable OrthoNormal bases and prove that regularizing coefficients becomes equivalent to regularizing the norm of learned filter function now. We conduct extensive experiments on diverse graph datasets to evaluate the fitting and generalization capability of LON-GNN, where the results imply its superiori
&lt;/p&gt;</description></item><item><title>FixFit&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#30001;&#20110;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#30340;&#22810;&#21442;&#25968;&#38598;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13746</link><description>&lt;p&gt;
FixFit&#65306;&#20351;&#29992;&#21442;&#25968;&#21387;&#32553;&#35299;&#20915;&#36229;&#23450;&#27169;&#22411;&#20013;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FixFit: using parameter-compression to solve the inverse problem in overdetermined models. (arXiv:2303.13746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13746
&lt;/p&gt;
&lt;p&gt;
FixFit&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#30001;&#20110;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#30340;&#22810;&#21442;&#25968;&#38598;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#31185;&#23398;&#39046;&#22495;&#37117;&#20381;&#36182;&#20110;&#25968;&#23398;&#27169;&#22411;&#12290;&#20351;&#29992;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#21442;&#25968;&#20272;&#35745;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#22810;&#20010;&#21442;&#25968;&#38598;&#21516;&#26679;&#36866;&#21512;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;FixFit&#65292;&#23558;&#32473;&#23450;&#25968;&#23398;&#27169;&#22411;&#30340;&#21442;&#25968;&#21387;&#32553;&#20026;&#29420;&#29305;&#20110;&#27169;&#22411;&#36755;&#20986;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#25968;&#25454;&#23545;&#19978;&#35757;&#32451;&#24102;&#26377;&#29942;&#39048;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#33719;&#24471;&#27492;&#34920;&#31034;&#12290;&#29942;&#39048;&#23618;&#33410;&#28857;&#23545;&#24212;&#20110;&#21807;&#19968;&#30340;&#28508;&#22312;&#21442;&#25968;&#65292;&#20854;&#32500;&#24230;&#34920;&#31034;&#27169;&#22411;&#30340;&#20449;&#24687;&#20869;&#23481;&#12290;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#29942;&#39048;&#23618;&#20998;&#35010;&#25104;&#32534;&#30721;&#22120;&#26469;&#25551;&#36848;&#20887;&#20313;&#20449;&#24687;&#21644;&#35299;&#30721;&#22120;&#26469;&#20174;&#27979;&#37327;&#20013;&#21807;&#19968;&#25512;&#26029;&#28508;&#22312;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#32463;&#20856;&#29289;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#20004;&#20010;&#24212;&#29992;&#26696;&#20363;&#20013;&#23637;&#31034;&#20102;FixFit&#12290;
&lt;/p&gt;
&lt;p&gt;
All fields of science depend on mathematical models. One of the fundamental problems with using complex nonlinear models is that data-driven parameter estimation often fails because interactions between model parameters lead to multiple parameter sets fitting the data equally well. Here, we develop a new method to address this problem, FixFit, which compresses a given mathematical model's parameters into a latent representation unique to model outputs. We acquire this representation by training a neural network with a bottleneck layer on data pairs of model parameters and model outputs. The bottleneck layer nodes correspond to the unique latent parameters, and their dimensionality indicates the information content of the model. The trained neural network can be split at the bottleneck layer into an encoder to characterize the redundancies and a decoder to uniquely infer latent parameters from measurements. We demonstrate FixFit in two use cases drawn from classical physics and neurosci
&lt;/p&gt;</description></item><item><title>EdgeTran&#26694;&#26550;&#26088;&#22312;&#35774;&#35745;&#36866;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35774;&#22791;&#19988;&#22312;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#12289;&#33021;&#32791;&#21644;&#27874;&#23792;&#21151;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#30340;Transformer&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22359;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;GPTran&#20197;&#24110;&#21161;&#20248;&#21270;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.13745</link><description>&lt;p&gt;
EdgeTran&#65306;&#22312;&#31227;&#21160;&#36793;&#32536;&#24179;&#21488;&#19978;&#20849;&#21516;&#35774;&#35745;Transformer&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms. (arXiv:2303.13745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13745
&lt;/p&gt;
&lt;p&gt;
EdgeTran&#26694;&#26550;&#26088;&#22312;&#35774;&#35745;&#36866;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35774;&#22791;&#19988;&#22312;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#12289;&#33021;&#32791;&#21644;&#27874;&#23792;&#21151;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#30340;Transformer&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22359;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;GPTran&#20197;&#24110;&#21161;&#20248;&#21270;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#35774;&#35745;&#39640;&#25928;Transformer&#27169;&#22411;&#36817;&#26469;&#24191;&#21463;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#20851;&#27880;&#12290;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#21482;&#20851;&#27880;&#26576;&#20123;&#24230;&#37327;&#26631;&#20934;&#65292;&#32780;&#25628;&#32034;&#26368;&#20339;&#24615;&#33021;&#30340;Transformer&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#35745;&#31639;&#36793;&#32536;&#24179;&#21488;&#19978;&#36816;&#34892;&#20256;&#32479;&#30340;&#22797;&#26434;&#22823;&#22411;Transformer&#27169;&#22411;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProTran&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#19981;&#21516;&#31181;&#31867;&#30340;&#36793;&#32536;&#35774;&#22791;&#36827;&#34892;&#30828;&#20214;&#24615;&#33021;&#24230;&#37327;&#26469;&#21066;&#20943;&#24310;&#36831;&#12289;&#33021;&#32791;&#21644;&#27874;&#23792;&#21151;&#32791;&#65292;&#20174;&#32780;&#33719;&#24471;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#20849;&#21516;&#20248;&#21270;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#24615;&#33021;&#24230;&#37327;&#30340;&#26694;&#26550;&#20026;EdgeTran&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTran&#65292;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#22359;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#24110;&#21161;&#20248;&#21270;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated design of efficient transformer models has recently attracted significant attention from industry and academia. However, most works only focus on certain metrics while searching for the best-performing transformer architecture. Furthermore, running traditional, complex, and large transformer models on low-compute edge platforms is a challenging problem. In this work, we propose a framework, called ProTran, to profile the hardware performance measures for a design space of transformer architectures and a diverse set of edge devices. We use this profiler in conjunction with the proposed co-design technique to obtain the best-performing models that have high accuracy on the given task and minimize latency, energy consumption, and peak power draw to enable edge deployment. We refer to our framework for co-optimizing accuracy and hardware performance measures as EdgeTran. It searches for the best transformer model and edge device pair. Finally, we propose GPTran, a multi-stage blo
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#35768;&#21487;&#35777;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#20844;&#24320;&#25968;&#25454;&#38598;&#36136;&#37327;&#19981;&#20339;&#65292;&#32570;&#20047;&#21830;&#19994;&#21487;&#29992;&#24615;&#12290;&#24403;&#21069;&#25968;&#25454;&#38598;&#32570;&#23569;&#35768;&#21487;&#35777;&#65292;&#38656;&#35201;&#26356;&#31185;&#23398;&#21644;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35843;&#26597;&#12290;</title><link>http://arxiv.org/abs/2303.13735</link><description>&lt;p&gt;
&#22522;&#20110;GQM&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#35768;&#21487;&#35777;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An investigation of licensing of datasets for machine learning based on the GQM model. (arXiv:2303.13735v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13735
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#35768;&#21487;&#35777;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#20844;&#24320;&#25968;&#25454;&#38598;&#36136;&#37327;&#19981;&#20339;&#65292;&#32570;&#20047;&#21830;&#19994;&#21487;&#29992;&#24615;&#12290;&#24403;&#21069;&#25968;&#25454;&#38598;&#32570;&#23569;&#35768;&#21487;&#35777;&#65292;&#38656;&#35201;&#26356;&#31185;&#23398;&#21644;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#35768;&#21487;&#35777;&#30446;&#21069;&#22312;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#20013;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#20013;&#65292;&#26368;&#24120;&#29992;&#30340;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#20027;&#35201;&#26159;&#20174;&#20114;&#32852;&#32593;&#19978;&#33719;&#21462;&#30340;&#65292;&#22240;&#27492;&#19968;&#20123;&#22270;&#20687;&#24182;&#19981;&#26159;&#21830;&#19994;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#25968;&#25454;&#38598;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#24320;&#21457;&#20154;&#21592;&#36890;&#24120;&#19981;&#20851;&#24515;&#25968;&#25454;&#38598;&#30340;&#35768;&#21487;&#35777;&#12290;&#24635;&#32467;&#32780;&#35328;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#35768;&#21487;&#35777;&#22312;&#25152;&#26377;&#26041;&#38754;&#37117;&#22788;&#20110;&#19981;&#23436;&#25972;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20004;&#20010;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#25968;&#25454;&#38598;&#32570;&#20047;&#35768;&#21487;&#35777;&#65292;&#32570;&#20047;&#35768;&#21487;&#35777;&#20351;&#24471;&#26080;&#27861;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#21830;&#19994;&#21487;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20915;&#23450;&#37319;&#21462;&#26356;&#31185;&#23398;&#21644;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35843;&#26597;&#25968;&#25454;&#38598;&#30340;&#35768;&#21487;&#35777;&#21644;&#20351;&#29992;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35768;&#21487;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset licensing is currently an issue in the development of machine learning systems. And in the development of machine learning systems, the most widely used are publicly available datasets. However, since the images in the publicly available dataset are mainly obtained from the Internet, some images are not commercially available. Furthermore, developers of machine learning systems do not often care about the license of the dataset when training machine learning models with it. In summary, the licensing of datasets for machine learning systems is in a state of incompleteness in all aspects at this stage.  Our investigation of two collection datasets revealed that most of the current datasets lacked licenses, and the lack of licenses made it impossible to determine the commercial availability of the datasets. Therefore, we decided to take a more scientific and systematic approach to investigate the licensing of datasets and the licensing of machine learning systems that use the data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#35270;&#35273;&#20998;&#26512;&#26041;&#27861;&#22238;&#31572;&#20102;ViT&#20013;&#22836;&#30340;&#37325;&#35201;&#24615;&#12289;&#19981;&#21516;&#22836;&#23545;&#31354;&#38388;&#37051;&#23621;&#30340;&#20851;&#27880;&#24378;&#24230;&#12289;&#20197;&#21450;&#27599;&#20010;&#22836;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13731</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#22312;&#35270;&#35273;Transformer&#20013;&#26159;&#22914;&#20309;&#24037;&#20316;&#30340;&#65311;&#19968;&#27425;&#35270;&#35273;&#20998;&#26512;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
How Does Attention Work in Vision Transformers? A Visual Analytics Attempt. (arXiv:2303.13731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#35270;&#35273;&#20998;&#26512;&#26041;&#27861;&#22238;&#31572;&#20102;ViT&#20013;&#22836;&#30340;&#37325;&#35201;&#24615;&#12289;&#19981;&#21516;&#22836;&#23545;&#31354;&#38388;&#37051;&#23621;&#30340;&#20851;&#27880;&#24378;&#24230;&#12289;&#20197;&#21450;&#27599;&#20010;&#22836;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViT&#65289;&#25193;&#23637;&#20102;&#23558;Transformer&#27169;&#22411;&#20174;&#24207;&#21015;&#25968;&#25454;&#24212;&#29992;&#21040;&#22270;&#20687;&#30340;&#25104;&#21151;&#12290;&#35813;&#27169;&#22411;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#35768;&#22810;&#36739;&#23567;&#30340;patch&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#25490;&#21015;&#25104;&#19968;&#20010;&#24207;&#21015;&#12290;&#25509;&#19979;&#26469;&#23545;&#24207;&#21015;&#24212;&#29992;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#23398;&#20064;patch&#20043;&#38388;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#25104;&#21151;&#35299;&#37322;&#24207;&#21015;&#25968;&#25454;&#19978;Transformer&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;ViT&#30340;&#35299;&#37322;&#21364;&#40092;&#26377;&#30740;&#31350;&#65292;&#35768;&#22810;&#38382;&#39064;&#20381;&#28982;&#27809;&#26377;&#24471;&#21040;&#26126;&#30830;&#30340;&#22238;&#31572;&#12290;&#20363;&#22914;&#65292;&#22312;&#20247;&#22810;&#30340;&#27880;&#24847;&#21147;&#22836;&#20013;&#65292;&#21738;&#20010;&#26356;&#37325;&#35201;&#65311;&#19981;&#21516;&#30340;&#22836;&#23545;&#20854;&#31354;&#38388;&#37051;&#23621;&#30340;&#29305;&#23450;patch&#36827;&#34892;&#30340;&#20851;&#27880;&#26377;&#22810;&#24378;&#65311;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#23398;&#20064;&#20102;&#21738;&#20123;&#20851;&#27880;&#27169;&#24335;&#65311;&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#35270;&#35273;&#20998;&#26512;&#26041;&#27861;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#22522;&#20110;&#21098;&#26525;&#30340;&#24230;&#37327;&#26469;&#30830;&#23450;ViT&#20013;&#21738;&#20123;&#22836;&#26356;&#37325;&#35201;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#20869;&#37096;&#30340;patch&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#24378;&#24230;&#36827;&#34892;&#31354;&#38388;&#20998;&#24067;&#20998;&#26512;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#27599;&#20010;&#22836;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformer (ViT) expands the success of transformer models from sequential data to images. The model decomposes an image into many smaller patches and arranges them into a sequence. Multi-head self-attentions are then applied to the sequence to learn the attention between patches. Despite many successful interpretations of transformers on sequential data, little effort has been devoted to the interpretation of ViTs, and many questions remain unanswered. For example, among the numerous attention heads, which one is more important? How strong are individual patches attending to their spatial neighbors in different heads? What attention patterns have individual heads learned? In this work, we answer these questions through a visual analytics approach. Specifically, we first identify what heads are more important in ViTs by introducing multiple pruning-based metrics. Then, we profile the spatial distribution of attention strengths between patches inside individual heads, as well as
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#19982;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#32508;&#36848;&#19982;&#24212;&#29992;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#38450;&#27490;&#38544;&#31169;&#19982;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#20943;&#23569;&#30001;&#36164;&#28304;&#19981;&#36275;&#24341;&#36215;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13727</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#19982;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#32508;&#36848;&#65306;&#22312;&#36164;&#28304;&#21463;&#38480;&#35745;&#31639;&#20013;&#30340;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey on Secure and Private Federated Learning Using Blockchain: Theory and Application in Resource-constrained Computing. (arXiv:2303.13727v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13727
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#19982;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#32508;&#36848;&#19982;&#24212;&#29992;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#38450;&#27490;&#38544;&#31169;&#19982;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#20943;&#23569;&#30001;&#36164;&#28304;&#19981;&#36275;&#24341;&#36215;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#38543;&#30528;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#26032;&#20852;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#30340;&#20986;&#29616;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#24191;&#21463;&#27426;&#36814;&#12290; FL&#20351;&#36793;&#32536;&#35774;&#22791;&#30340;&#26412;&#22320;&#25968;&#25454;&#23384;&#20648;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#23558;&#25935;&#24863;&#25968;&#25454;&#36879;&#38706;&#32473;&#20219;&#20309;&#23454;&#20307;&#12290;&#23613;&#31649;&#36825;&#31181;&#33539;&#20363;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#20102;&#29992;&#25143;&#25935;&#24863;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#32593;&#32476;&#23041;&#32961;&#21644;&#38544;&#31169;&#20405;&#29359;&#25216;&#26415;&#19979;&#65292;FL&#36807;&#31243;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#23041;&#32961;&#24182;&#36798;&#21040;&#29942;&#39048;&#12290;&#20026;&#20102;&#20419;&#36827;FL&#36807;&#31243;&#30340;&#26222;&#21450;&#65292;&#21306;&#22359;&#38142;&#19982;FL&#29615;&#22659;&#30340;&#38598;&#25104;&#24050;&#32463;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#21306;&#22359;&#38142;&#20855;&#26377;&#20998;&#25955;&#21270;&#12289;&#19981;&#21487;&#21464;&#24615;&#12289;&#20849;&#35782;&#21644;&#36879;&#26126;&#24230;&#31561;&#29305;&#24615;&#65292;&#26377;&#28508;&#21147;&#38450;&#27490;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#21306;&#22359;&#38142;&#26426;&#21046;&#38656;&#35201;&#26114;&#36149;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#37027;&#20040;&#36164;&#28304;&#21463;&#38480;&#30340;FL&#21487;&#33021;&#20250;&#25104;&#20026;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has gained widespread popularity in recent years due to the fast booming of advanced machine learning and artificial intelligence along with emerging security and privacy threats. FL enables efficient model generation from local data storage of the edge devices without revealing the sensitive data to any entities. While this paradigm partly mitigates the privacy issues of users' sensitive data, the performance of the FL process can be threatened and reached a bottleneck due to the growing cyber threats and privacy violation techniques. To expedite the proliferation of FL process, the integration of blockchain for FL environments has drawn prolific attention from the people of academia and industry. Blockchain has the potential to prevent security and privacy threats with its decentralization, immutability, consensus, and transparency characteristic. However, if the blockchain mechanism requires costly computational resources, then the resource-constrained FL cli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;VAE&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#20445;&#30495;&#22320;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#22312;&#36991;&#20813;&#23545;&#22823;&#37096;&#20998;&#32454;&#33410;&#36827;&#34892;&#24314;&#27169;&#30340;&#22522;&#30784;&#19978;&#65292;&#37325;&#28857;&#23398;&#20064;&#22270;&#20687;&#30340;&#32467;&#26500;&#32452;&#25104;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.13714</link><description>&lt;p&gt;
&#28145;&#24230;VAE&#22312;&#28508;&#31354;&#38388;&#20013;&#39640;&#20445;&#30495;&#22270;&#20687;&#21512;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
High Fidelity Image Synthesis With Deep VAEs In Latent Space. (arXiv:2303.13714v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13714
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;VAE&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#20445;&#30495;&#22320;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#22312;&#36991;&#20813;&#23545;&#22823;&#37096;&#20998;&#32454;&#33410;&#36827;&#34892;&#24314;&#27169;&#30340;&#22522;&#30784;&#19978;&#65292;&#37325;&#28857;&#23398;&#20064;&#22270;&#20687;&#30340;&#32467;&#26500;&#32452;&#25104;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#30830;&#23450;&#24615;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#24555;&#36895;&#12289;&#36924;&#30495;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#20004;&#38454;&#27573;&#30340;&#35774;&#32622;&#20013;&#65292;&#33258;&#32534;&#30721;&#22120;&#23558;&#22270;&#20687;&#21387;&#32553;&#20026;&#20854;&#35821;&#20041;&#29305;&#24449;&#65292;&#28982;&#21518;&#29992;&#28145;&#24230;VAE&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;VAE&#36991;&#20813;&#20102;&#23545;&#26500;&#25104;&#22270;&#20687;&#22823;&#37096;&#20998;&#20195;&#30721;&#38271;&#24230;&#30340;&#32454;&#33410;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#23398;&#20064;&#20854;&#32467;&#26500;&#32452;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;ImageNet-256&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;9.34&#30340;FID&#65292;&#36825;&#19982;BigGAN&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#20195;&#30721;&#24050;&#32463;&#22312;&#32447;&#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present fast, realistic image generation on high-resolution, multimodal datasets using hierarchical variational autoencoders (VAEs) trained on a deterministic autoencoder's latent space. In this two-stage setup, the autoencoder compresses the image into its semantic features, which are then modeled with a deep VAE. With this method, the VAE avoids modeling the fine-grained details that constitute the majority of the image's code length, allowing it to focus on learning its structural components. We demonstrate the effectiveness of our two-stage approach, achieving a FID of 9.34 on the ImageNet-256 dataset which is comparable to BigGAN. We make our implementation available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#26041;&#27861;DOODL&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#24182;&#22312;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.13703</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#25193;&#25955;&#28508;&#22312;&#20248;&#21270;&#25552;&#39640;&#20998;&#31867;&#22120;&#24341;&#23548;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-End Diffusion Latent Optimization Improves Classifier Guidance. (arXiv:2303.13703v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#26041;&#27861;DOODL&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#24182;&#22312;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#25351;&#23548;&#8212;&#8212;&#21033;&#29992;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#8212;&#8212;&#26377;&#28508;&#21147;&#22823;&#24133;&#25193;&#23637;&#23545;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#21019;&#36896;&#24615;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20998;&#31867;&#22120;&#25351;&#23548;&#35201;&#20040;&#38656;&#35201;&#35757;&#32451;&#26032;&#30340;&#22122;&#22768;&#24863;&#30693;&#27169;&#22411;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#26799;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#27493;&#21435;&#22122;&#30340;&#36817;&#20284;&#26368;&#32456;&#29983;&#25104;&#29289;&#65292;&#24182;&#23548;&#33268;&#26799;&#24230;&#19981;&#23545;&#40784;&#21644;&#27425;&#20248;&#25511;&#21046;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#36825;&#31181;&#36817;&#20284;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#23548;&#26041;&#27861;&#65306;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#65288;DOODL&#65289;&#65292;&#23427;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#20351;&#29992;&#21487;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20869;&#23384;&#26377;&#25928;&#30340;&#21453;&#21521;&#20256;&#36882;&#12290;&#23637;&#31034;&#20102;&#26356;&#31934;&#30830;&#25351;&#23548;&#28508;&#21147;&#30340; DOODL &#22312;&#19981;&#21516;&#24418;&#24335;&#30340;&#25351;&#23548;&#30340;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier guidance -- using the gradients of an image classifier to steer the generations of a diffusion model -- has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation's shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidanc
&lt;/p&gt;</description></item><item><title>OFA$^2$&#26159;&#19968;&#20010;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#25628;&#32034;&#38454;&#27573;&#26500;&#24819;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#26435;&#34913;&#30446;&#26631;&#20043;&#38388;&#25214;&#21040;&#39640;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2303.13683</link><description>&lt;p&gt;
OFA$^2$: &#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#30340;Once-for-All&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural Architecture Search. (arXiv:2303.13683v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13683
&lt;/p&gt;
&lt;p&gt;
OFA$^2$&#26159;&#19968;&#20010;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#25628;&#32034;&#38454;&#27573;&#26500;&#24819;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#26435;&#34913;&#30446;&#26631;&#20043;&#38388;&#25214;&#21040;&#39640;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Once-for-All&#65288;OFA&#65289;&#26159;&#19968;&#20010;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#31163;&#35757;&#32451;&#21644;&#25628;&#32034;&#38454;&#27573;&#26469;&#35299;&#20915;&#20026;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35774;&#22791;&#25628;&#32034;&#39640;&#25928;&#26550;&#26500;&#30340;&#38382;&#39064;&#12290; Ofa&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#21482;&#38656;&#35201;&#36827;&#34892;&#19968;&#27425;&#65292;&#28982;&#21518;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#37096;&#32626;&#26041;&#26696;&#20174;&#27492;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#20013;&#25552;&#21462;&#22810;&#20010;&#23376;&#32593;&#32476;&#36827;&#34892;&#22810;&#27425;&#25628;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#25628;&#32034;&#38454;&#27573;&#26126;&#30830;&#26500;&#24819;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23547;&#27714;&#25928;&#29575;&#12290; &#28982;&#21518;&#20351;&#29992;&#20219;&#20309;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;&#20363;&#22914;NSGA-II&#21644;SMS-EMOA&#65289;&#22312;&#25628;&#32034;&#38454;&#27573;&#22635;&#20805;Pareto&#21069;&#27839;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#26435;&#34913;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#31070;&#32463;&#32467;&#26500;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#31070;&#32463;&#32593;&#32476;&#21482;&#38656;&#35757;&#32451;&#19968;&#27425;&#65292;&#28982;&#21518;&#20197;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#24418;&#24335;&#25191;&#34892;&#23376;&#32593;&#25628;&#32034;&#65292;&#24182;&#33719;&#24471;&#19968;&#32452;&#39640;&#25928;&#12289;&#39044;&#35757;&#32451;&#19988;&#22810;&#26679;&#21270;&#30340;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Once-for-All (OFA) is a Neural Architecture Search (NAS) framework designed to address the problem of searching efficient architectures for devices with different resources constraints by decoupling the training and the searching stages. The computationally expensive process of training the OFA neural network is done only once, and then it is possible to perform multiple searches for subnetworks extracted from this trained network according to each deployment scenario. In this work we aim to give one step further in the search for efficiency by explicitly conceiving the search stage as a multi-objective optimization problem. A Pareto frontier is then populated with efficient, and already trained, neural architectures exhibiting distinct trade-offs among the conflicting objectives. This could be achieved by using any multi-objective evolutionary algorithm during the search stage, such as NSGA-II and SMS-EMOA. In other words, the neural network is trained once, the searching for subnetwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#28151;&#21512;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#38477;&#32500;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13665</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#28151;&#21512;&#30340;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Clustering based on Mixtures of Sparse Gaussian Processes. (arXiv:2303.13665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#28151;&#21512;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#38477;&#32500;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#21019;&#24314;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#20302;&#32500;&#34920;&#31034;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22914;&#20309;&#20351;&#29992;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#32858;&#31867;&#25968;&#25454;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#37325;&#28857;&#25918;&#22312;&#25552;&#20986;&#32858;&#31867;&#21644;&#38477;&#32500;&#30340;&#32852;&#21512;&#34920;&#36848;&#19978;&#12290;&#24403;&#38656;&#35201;&#27010;&#29575;&#27169;&#22411;&#26102;&#65292;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#28151;&#21512;&#27169;&#22411;&#65292;&#22312;&#20854;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#32858;&#31867;&#25351;&#31034;&#22120;&#21644;&#20302;&#32500;&#24230;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#28151;&#21512;&#32858;&#31867;&#65288;SGP-MIC&#65289;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#65292;&#27010;&#29575;&#27169;&#22411;&#30340;&#24615;&#36136;&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#30340;&#30830;&#23450;&#24615;&#26041;&#27861;&#26356;&#22810;&#30340;&#20248;&#21183;&#65292;&#26500;&#36896;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#38750;&#24120;&#30452;&#25509;&#65292;&#21516;&#26102;&#24212;&#29992;&#31232;&#30095;&#27169;&#22411;&#21644;&#39640;&#25928;&#21464;&#20998;EM&#36924;&#36817;&#26377;&#21161;&#20110;&#21152;&#36895;&#31639;&#27861;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating low dimensional representations of a high dimensional data set is an important component in many machine learning applications. How to cluster data using their low dimensional embedded space is still a challenging problem in machine learning. In this article, we focus on proposing a joint formulation for both clustering and dimensionality reduction. When a probabilistic model is desired, one possible solution is to use the mixture models in which both cluster indicator and low dimensional space are learned. Our algorithm is based on a mixture of sparse Gaussian processes, which is called Sparse Gaussian Process Mixture Clustering (SGP-MIC). The main advantages to our approach over existing methods are that the probabilistic nature of this model provides more advantages over existing deterministic methods, it is straightforward to construct non-linear generalizations of the model, and applying a sparse model and an efficient variational EM approximation help to speed up the alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;LQR&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#36924;&#36817;&#26080;&#38480;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#26377;&#38480;&#19968;&#32452;&#26102;&#21051;&#65292;&#35299;&#20915;&#20102;DRL&#20013;&#31574;&#30053;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13657</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;LQR&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Policy Evaluation in Distributional LQR. (arXiv:2303.13657v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;LQR&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#36924;&#36817;&#26080;&#38480;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#26377;&#38480;&#19968;&#32452;&#26102;&#21051;&#65292;&#35299;&#20915;&#20102;DRL&#20013;&#31574;&#30053;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#20102;&#23545;&#29615;&#22659;&#20013;&#38543;&#26426;&#24615;&#24433;&#21709;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#35753;&#20195;&#29702;&#23398;&#20064;&#38543;&#26426;&#22238;&#25253;&#30340;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#20687;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#19968;&#26679;&#23398;&#20064;&#26399;&#26395;&#20540;&#12290;&#20294;&#26159;&#65292;&#22312;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31574;&#30053;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#22238;&#25253;&#20998;&#24067;&#30340;&#34920;&#31034;&#65292;&#38656;&#35201;&#20180;&#32454;&#35774;&#35745;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#31867;&#22522;&#20110;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#36827;&#34892;&#25511;&#21046;&#30340;DRL&#38382;&#39064;&#30340;&#36825;&#19968;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#20998;&#24067;&#24335;LQR&#8221;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38543;&#26426;&#22238;&#25253;&#20998;&#24067;&#30340;&#38381;&#21512;&#34920;&#36798;&#24335;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#22806;&#37096;&#25200;&#21160;&#30340;&#21160;&#21147;&#23398;&#12290;&#34429;&#28982;&#25152;&#25552;&#20986;&#30340;&#31934;&#30830;&#22238;&#25253;&#20998;&#24067;&#21253;&#21547;&#26080;&#38480;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#20998;&#24067;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#30340;&#19968;&#32452;&#26102;&#21051;&#26377;&#25928;&#36924;&#36817;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#36924;&#36817;&#26159;&#20934;&#30830;&#19988;&#21487;&#35745;&#31639;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#25511;&#21046;&#20855;&#26377;&#38543;&#26426;&#25200;&#21160;&#30340;&#32447;&#24615;&#23450;&#24120;&#31995;&#32479;&#26102;&#65292;&#20998;&#24067;&#24335;LQR&#30340;&#25928;&#26524;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional reinforcement learning (DRL) enhances the understanding of the effects of the randomness in the environment by letting agents learn the distribution of a random return, rather than its expected value as in standard RL. At the same time, a main challenge in DRL is that policy evaluation in DRL typically relies on the representation of the return distribution, which needs to be carefully designed. In this paper, we address this challenge for a special class of DRL problems that rely on linear quadratic regulator (LQR) for control, advocating for a new distributional approach to LQR, which we call \emph{distributional LQR}. Specifically, we provide a closed-form expression of the distribution of the random return which, remarkably, is applicable to all exogenous disturbances on the dynamics, as long as they are independent and identically distributed (i.i.d.). While the proposed exact return distribution consists of infinitely many random variables, we show that this distri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#26500;&#24314;&#39046;&#22495;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#36335;&#21644;&#21407;&#21017;&#65292;&#36890;&#36807;&#30740;&#31350;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21644;&#31995;&#32479;&#32423;&#20998;&#24067;&#32593;&#32476;&#36890;&#20449;&#12289;&#36882;&#24402;&#21644;&#30701;&#26399;&#25299;&#25169;&#21464;&#21270;&#65292;&#20026;&#24314;&#31435;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#23453;&#36149;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.13651</link><description>&lt;p&gt;
&#24314;&#31435;&#20154;&#24037;&#31070;&#32463;&#30005;&#36335;&#29992;&#20110;&#39046;&#22495;&#36890;&#29992;&#35748;&#30693;&#65306;&#33041;&#21551;&#21457;&#24335;&#31995;&#32479;&#32423;&#26550;&#26500;&#20837;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture. (arXiv:2303.13651v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#26500;&#24314;&#39046;&#22495;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#36335;&#21644;&#21407;&#21017;&#65292;&#36890;&#36807;&#30740;&#31350;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21644;&#31995;&#32479;&#32423;&#20998;&#24067;&#32593;&#32476;&#36890;&#20449;&#12289;&#36882;&#24402;&#21644;&#30701;&#26399;&#25299;&#25169;&#21464;&#21270;&#65292;&#20026;&#24314;&#31435;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#23453;&#36149;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26377;&#19968;&#31995;&#21015;&#30340;&#21162;&#21147;&#33268;&#21147;&#20110;&#24314;&#31435;&#33021;&#22815;&#35299;&#20915;&#24191;&#27867;&#35748;&#30693;&#20219;&#21153;&#19988;&#26080;&#38656;&#22312;&#21508;&#20010;&#38382;&#39064;&#31354;&#38388;&#21644;&#39046;&#22495;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20197;&#26500;&#24314;&#39046;&#22495;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#27169;&#22411;&#38656;&#35201;&#36866;&#24403;&#30340;&#20808;&#39564;&#21450;&#24402;&#32435;&#20559;&#35265;&#65292;&#35757;&#32451;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#23547;&#25214;&#26032;&#38382;&#39064;&#31354;&#38388;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#36171;&#20104;&#20854;&#26580;&#24615;&#35748;&#30693;&#21151;&#33021;&#30340;&#26631;&#24535;&#24615;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#29305;&#23450;&#30340;&#31995;&#32479;&#23618;&#20998;&#24067;&#32593;&#32476;&#36890;&#20449;&#21450;&#36882;&#24402;&#30340;&#20316;&#29992;&#65292;&#27492;&#22806;&#36824;&#35752;&#35770;&#20102;&#30701;&#26399;&#25299;&#25169;&#21464;&#21270;&#22312;&#39640;&#25928;&#23616;&#37096;&#35745;&#31639;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#26102;&#20505;&#65292;&#36825;&#20123;&#21407;&#21017;&#21487;&#33021;&#20250;&#23545;&#36825;&#20010;&#22797;&#26434;&#19988;&#21160;&#24577;&#30340;&#39046;&#22495;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a concerted effort to build domain-general artificial intelligence in the form of universal neural network models with sufficient computational flexibility to solve a wide variety of cognitive tasks but without requiring fine-tuning on individual problem spaces and domains. To do this, models need appropriate priors and inductive biases, such that trained models can generalise to out-of-distribution examples and new problem sets. Here we provide an overview of the hallmarks endowing biological neural networks with the functionality needed for flexible cognition, in order to establish which features might also be important to achieve similar functionality in artificial systems. We specifically discuss the role of system-level distribution of network communication and recurrence, in addition to the role of short-term topological changes for efficient local computation. As machine learning models become more complex, these principles may provide valuable directions in an otherwis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#39550;&#39542;&#21592;&#30130;&#21171;&#26816;&#27979;&#65292;&#26368;&#21487;&#38752;&#30340;&#27169;&#22411;&#26159;XGB&#65292;&#26368;&#20339;&#31383;&#21475;&#26102;&#38388;&#20026;120&#33267;150&#31186;&#20043;&#38388;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;SHAP&#26041;&#27861;&#36873;&#25321;&#20102;18&#20010;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#65292;&#24182;&#35757;&#32451;&#20986;&#26032;&#30340;&#36739;&#23567;&#27169;&#22411;&#20197;&#33719;&#24471;&#19982;&#26368;&#21021;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#25152;&#26377;&#27169;&#22411;&#37117;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20294;&#23545;&#25239;&#24615;&#35757;&#32451;&#20351;&#23427;&#20204;&#33021;&#22815;&#20445;&#25345;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13649</link><description>&lt;p&gt;
&#39550;&#39542;&#21592;&#30130;&#21171;&#26816;&#27979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#29305;&#24449;&#24433;&#21709;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness and Feature Impact Analysis for Driver Drowsiness Detection. (arXiv:2303.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#39550;&#39542;&#21592;&#30130;&#21171;&#26816;&#27979;&#65292;&#26368;&#21487;&#38752;&#30340;&#27169;&#22411;&#26159;XGB&#65292;&#26368;&#20339;&#31383;&#21475;&#26102;&#38388;&#20026;120&#33267;150&#31186;&#20043;&#38388;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;SHAP&#26041;&#27861;&#36873;&#25321;&#20102;18&#20010;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#65292;&#24182;&#35757;&#32451;&#20986;&#26032;&#30340;&#36739;&#23567;&#27169;&#22411;&#20197;&#33719;&#24471;&#19982;&#26368;&#21021;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#25152;&#26377;&#27169;&#22411;&#37117;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20294;&#23545;&#25239;&#24615;&#35757;&#32451;&#20351;&#23427;&#20204;&#33021;&#22815;&#20445;&#25345;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30130;&#21171;&#39550;&#39542;&#26159;&#36947;&#36335;&#20107;&#25925;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#20294;&#21496;&#26426;&#20204;&#24448;&#24448;&#20302;&#20272;&#20102;&#30130;&#21171;&#23545;&#20182;&#20204;&#21453;&#24212;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#22312;&#21457;&#29983;&#20219;&#20309;&#38556;&#30861;&#20043;&#21069;&#26816;&#27979;&#21040;&#30130;&#21171;&#65292;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#26469;&#30417;&#27979;&#24515;&#29575;&#21464;&#24322;(Heart Rate Variability, HRV)&#20449;&#21495;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;HRV&#26102;&#38388;&#31383;&#21475;&#21644;ML&#27169;&#22411;&#65292;&#20351;&#29992;Shapley Additive Explanations (SHAP)&#36827;&#34892;&#29305;&#24449;&#24433;&#21709;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#22788;&#29702;&#38169;&#35823;&#36755;&#20837;&#25968;&#25454;&#21644;&#21463;&#24178;&#25200;&#30340;HRV&#20449;&#21495;&#26102;&#30340;&#21487;&#38752;&#24615;&#12290;&#26368;&#21487;&#38752;&#30340;&#27169;&#22411;&#26159;Extreme Gradient Boosting (XGB)&#65292;&#26368;&#20339;&#26102;&#38388;&#31383;&#21475;&#20026;120&#33267;150&#31186;&#20043;&#38388;&#12290;&#27492;&#22806;&#65292;SHAP&#20801;&#35768;&#36873;&#25321;18&#20010;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#65292;&#24182;&#35757;&#32451;&#26032;&#30340;&#36739;&#23567;&#27169;&#22411;&#65292;&#20854;&#34920;&#29616;&#19982;&#26368;&#21021;&#30340;&#27169;&#22411;&#19968;&#26679;&#22909;&#12290;&#23613;&#31649;&#25152;&#26377;&#27169;&#22411;&#37117;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20294;&#23545;&#25239;&#24615;&#35757;&#32451;&#20351;&#23427;&#20204;&#33021;&#22815;&#20445;&#25345;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drowsy driving is a major cause of road accidents, but drivers are dismissive of the impact that fatigue can have on their reaction times. To detect drowsiness before any impairment occurs, a promising strategy is using Machine Learning (ML) to monitor Heart Rate Variability (HRV) signals. This work presents multiple experiments with different HRV time windows and ML models, a feature impact analysis using Shapley Additive Explanations (SHAP), and an adversarial robustness analysis to assess their reliability when processing faulty input data and perturbed HRV signals. The most reliable model was Extreme Gradient Boosting (XGB) and the optimal time window had between 120 and 150 seconds. Furthermore, SHAP enabled the selection of the 18 most impactful features and the training of new smaller models that achieved a performance as good as the initial ones. Despite the susceptibility of all models to adversarial attacks, adversarial training enabled them to preserve significantly higher r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#25512;&#26029;&#20986;&#24515;&#29575;&#21464;&#24322;&#24615;&#12290;&#22312;&#22823;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#21333;&#29420;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25110;&#26426;&#22120;&#23398;&#20064;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13637</link><description>&lt;p&gt;
&#21033;&#29992;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#39640;&#25928;&#30452;&#25509;&#25512;&#26029;&#24515;&#29575;&#21464;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficient and Direct Inference of Heart Rate Variability using Both Signal Processing and Machine Learning. (arXiv:2303.13637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#25512;&#26029;&#20986;&#24515;&#29575;&#21464;&#24322;&#24615;&#12290;&#22312;&#22823;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#21333;&#29420;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25110;&#26426;&#22120;&#23398;&#20064;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29575;&#21464;&#24322;&#24615;(HRV)&#27979;&#37327;&#36830;&#32493;&#24515;&#36339;&#26102;&#38388;&#30340;&#21464;&#21270;&#65292;&#26159;&#36523;&#24515;&#20581;&#24247;&#30340;&#20027;&#35201;&#25351;&#26631;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20351;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20202;(PPG)&#20256;&#24863;&#22120;&#25512;&#26029;HRV&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#23384;&#22312;&#36739;&#39640;&#30340;&#35823;&#24046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25110;&#26426;&#22120;&#23398;&#20064;(ML)&#65292;&#25110;&#32773;&#22240;&#20026;&#23427;&#20204;&#38388;&#25509;&#25512;&#26029;HRV&#65292;&#25110;&#32773;&#22240;&#20026;&#32570;&#23569;&#22823;&#25968;&#25454;&#38598;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20063;&#21487;&#33021;&#38656;&#35201;&#22823;&#30340;ML&#27169;&#22411;&#12290;&#20302;&#30340;&#20934;&#30830;&#29575;&#21644;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23567;&#22411;&#23884;&#20837;&#24335;&#35774;&#22791;&#21644;&#26410;&#26469;&#21487;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#30340;PPG&#20449;&#21495;&#21644;HRV&#30340;&#22522;&#26412;&#20107;&#23454;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;ML&#30340;HRV&#27169;&#22411;&#65292;&#30452;&#25509;&#25512;&#26029;HRV&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35823;&#24046;&#22312;3.5%&#21040;25.7%&#20043;&#38388;&#65292;&#24182;&#19988;&#20248;&#20110;&#21333;&#29420;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25110;ML&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart Rate Variability (HRV) measures the variation of the time between consecutive heartbeats and is a major indicator of physical and mental health. Recent research has demonstrated that photoplethysmography (PPG) sensors can be used to infer HRV. However, many prior studies had high errors because they only employed signal processing or machine learning (ML), or because they indirectly inferred HRV, or because there lacks large training datasets. Many prior studies may also require large ML models. The low accuracy and large model sizes limit their applications to small embedded devices and potential future use in healthcare. To address the above issues, we first collected a large dataset of PPG signals and HRV ground truth. With this dataset, we developed HRV models that combine signal processing and ML to directly infer HRV. Evaluation results show that our method had errors between 3.5% to 25.7% and outperformed signal-processing-only and ML-only methods. We also explored differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;PPG&#24515;&#29575;&#20272;&#35745;&#25216;&#26415;&#24212;&#29992;&#20110;&#20302;&#21151;&#32791;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#65292;&#22312;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#25104;&#21151;&#23558;PPG&#37319;&#26679;&#39057;&#29575;&#38477;&#33267;&#20165;25Hz&#65292;&#24182;&#25552;&#39640;&#20102;&#24515;&#29575;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20063;&#20943;&#23567;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#22823;&#23567;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.13636</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#25928;&#20256;&#24863;&#22120;&#37319;&#26679;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;PPG&#24515;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PPG-based Heart Rate Estimation with Efficient Sensor Sampling and Learning Models. (arXiv:2303.13636v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;PPG&#24515;&#29575;&#20272;&#35745;&#25216;&#26415;&#24212;&#29992;&#20110;&#20302;&#21151;&#32791;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#65292;&#22312;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#25104;&#21151;&#23558;PPG&#37319;&#26679;&#39057;&#29575;&#38477;&#33267;&#20165;25Hz&#65292;&#24182;&#25552;&#39640;&#20102;&#24515;&#29575;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20063;&#20943;&#23567;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#22823;&#23567;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#20013;&#23884;&#20837;&#30340;PPG&#20256;&#24863;&#22120;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#20272;&#35745;&#24515;&#29575;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#23558;&#22522;&#20110;PPG&#20256;&#24863;&#22120;&#30340;&#24515;&#29575;&#20272;&#35745;&#24212;&#29992;&#20110;&#23884;&#20837;&#24335;&#35774;&#22791;&#20173;&#38754;&#20020;&#30528;&#39640;&#33021;&#32791;&#30340;&#39640;&#39057;PPG&#37319;&#26679;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#26356;&#36866;&#21512;&#20302;&#21151;&#32791;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#24515;&#29575;&#20272;&#35745;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#20986;&#21487;&#20197;&#25552;&#20379;&#39640;&#31934;&#24230;&#24515;&#29575;&#20272;&#35745;&#30340;&#25216;&#26415;&#65292;&#20854;&#37319;&#26679;&#39057;&#29575;&#20302;&#65292;&#27169;&#22411;&#23610;&#23544;&#23567;&#19988;&#25512;&#26029;&#26102;&#38388;&#24555;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;PPG&#37319;&#26679;&#39057;&#29575;&#20174;125 Hz&#38477;&#33267;&#20165;25 Hz&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#39640;&#30340;&#24515;&#29575;&#20272;&#35745;&#31934;&#24230;&#12290;&#36825;&#31181;&#32452;&#21512;&#36824;&#26377;&#21161;&#20110;&#20943;&#23567;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#22823;&#23567;&#65292;&#23548;&#33268;&#27169;&#22411;&#26356;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21576;&#29616;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;
&lt;/p&gt;
&lt;p&gt;
Recent studies showed that Photoplethysmography (PPG) sensors embedded in wearable devices can estimate heart rate (HR) with high accuracy. However, despite of prior research efforts, applying PPG sensor based HR estimation to embedded devices still faces challenges due to the energy-intensive high-frequency PPG sampling and the resource-intensive machine-learning models. In this work, we aim to explore HR estimation techniques that are more suitable for lower-power and resource-constrained embedded devices. More specifically, we seek to design techniques that could provide high-accuracy HR estimation with low-frequency PPG sampling, small model size, and fast inference time. First, we show that by combining signal processing and ML, it is possible to reduce the PPG sampling frequency from 125 Hz to only 25 Hz while providing higher HR estimation accuracy. This combination also helps to reduce the ML model feature size, leading to smaller models. Additionally, we present a comprehensiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#20302;&#31209;&#20248;&#21270;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#36890;&#36807;&#20302;&#31209;&#36924;&#36817;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#26082;&#20943;&#23569;&#23384;&#20648;&#35201;&#27714;&#21448;&#21487;&#23454;&#29616;&#39640;&#25928;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#32508;&#21512;&#25216;&#26415;&#65292;&#21487;&#23558;&#20197;&#19978;&#26041;&#27861;&#38598;&#25104;&#21040;LRI-Net&#26694;&#26550;&#20013;&#65292;&#20197;&#20849;&#21516;&#38477;&#20302;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13635</link><description>&lt;p&gt;
&#25552;&#39640;&#25928;&#29575;&#30340;&#20302;&#31209;&#20248;&#21270;&#65306;&#20351;&#32039;&#20945;&#32467;&#26500;&#21644;&#24555;&#36895;&#35757;&#32451;&#36798;&#21040;&#24179;&#34913;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Low Rank Optimization for Efficient Deep Learning: Making A Balance between Compact Architecture and Fast Training. (arXiv:2303.13635v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#20302;&#31209;&#20248;&#21270;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#36890;&#36807;&#20302;&#31209;&#36924;&#36817;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#26082;&#20943;&#23569;&#23384;&#20648;&#35201;&#27714;&#21448;&#21487;&#23454;&#29616;&#39640;&#25928;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#32508;&#21512;&#25216;&#26415;&#65292;&#21487;&#23558;&#20197;&#19978;&#26041;&#27861;&#38598;&#25104;&#21040;LRI-Net&#26694;&#26550;&#20013;&#65292;&#20197;&#20849;&#21516;&#38477;&#20302;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#25968;&#25454;&#22788;&#29702;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#23384;&#20648;&#25104;&#26412;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#38590;&#20197;&#24212;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#65292;&#20063;&#19981;&#29615;&#20445;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#33021;&#32791;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#25552;&#39640;&#25928;&#29575;&#30340;&#20302;&#31209;&#20248;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#32593;&#32476;&#21442;&#25968;&#30340;&#20302;&#31209;&#36924;&#36817;&#26469;&#21387;&#32553;&#31354;&#38388;&#22495;&#65292;&#20943;&#23569;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#23384;&#20648;&#35201;&#27714;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#22312;&#26102;&#38388;&#22495;&#20869;&#36890;&#36807;&#22312;&#23569;&#25968;&#23376;&#31354;&#38388;&#20013;&#35757;&#32451;&#32593;&#32476;&#21442;&#25968;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#24555;&#36895;&#25910;&#25947;&#35757;&#32451;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#31354;&#38388;&#22495;&#20013;&#30340;&#19977;&#31181;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65306;&#39044;&#35757;&#32451;&#12289;&#39044;&#35774;&#32622;&#21644;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#21487;&#25972;&#21512;&#25216;&#26415;&#36827;&#34892;&#32508;&#21512;&#35752;&#35770;&#65292;&#20363;&#22914;&#31232;&#30095;&#21098;&#26525;&#12289;&#37327;&#21270;&#21644;&#29109;&#32534;&#30721;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#19968;&#20010;&#21517;&#20026;&#20302;&#31209;&#38598;&#25104;&#32593;&#32476;&#65288;LRI-Net&#65289;&#30340;&#38598;&#25104;&#26694;&#26550;&#20013;&#65292;&#20197;&#20849;&#21516;&#38477;&#20302;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LRI-Net&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#37327;&#65292;&#20351;&#24471;&#20302;&#21151;&#32791;&#35774;&#22791;&#21644;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved great success in many data processing applications. However, the high computational complexity and storage cost makes deep learning hard to be used on resource-constrained devices, and it is not environmental-friendly with much power cost. In this paper, we focus on low-rank optimization for efficient deep learning techniques. In the space domain, deep neural networks are compressed by low rank approximation of the network parameters, which directly reduces the storage requirement with a smaller number of network parameters. In the time domain, the network parameters can be trained in a few subspaces, which enables efficient training for fast convergence. The model compression in the spatial domain is summarized into three categories as pre-train, pre-set, and compression-aware methods, respectively. With a series of integrable techniques discussed, such as sparse pruning, quantization, and entropy coding, we can ensemble them in an integration framew
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PIPN&#65292;&#23427;&#33021;&#21033;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#21516;&#26102;&#39044;&#27979;&#25152;&#38656;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#20960;&#20309;&#20307;&#19978;&#30340;&#35299;&#65292;&#26377;&#26395;&#22312;&#24037;&#19994;&#30028;&#36827;&#34892;&#24555;&#36895;&#30340;&#20960;&#20309;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.13634</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;PointNet&#65306;&#23427;&#33021;&#21516;&#26102;&#35299;&#20915;&#22810;&#23569;&#19981;&#35268;&#21017;&#20960;&#20309;&#20307;&#30340;&#21453;&#38382;&#39064;&#65311;&#20197;&#32447;&#24377;&#24615;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity. (arXiv:2303.13634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PIPN&#65292;&#23427;&#33021;&#21033;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#21516;&#26102;&#39044;&#27979;&#25152;&#38656;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#20960;&#20309;&#20307;&#19978;&#30340;&#35299;&#65292;&#26377;&#26395;&#22312;&#24037;&#19994;&#30028;&#36827;&#34892;&#24555;&#36895;&#30340;&#20960;&#20309;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35268;&#30340;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21033;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#39044;&#27979;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#20294;&#21482;&#38480;&#20110;&#21333;&#19968;&#30340;&#22495;&#12290;&#30456;&#21453;&#65292;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#39318;&#20808;&#22312;&#24050;&#30693;&#35299;&#65288;&#21363;&#26631;&#35760;&#25968;&#25454;&#65289;&#30340;&#20960;&#21315;&#20010;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#39044;&#27979;&#22312;&#19968;&#20123;&#26410;&#30693;&#22495;&#19978;&#30340;&#35299;&#12290;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;PointNet&#65288;PIPN&#65289;&#20027;&#35201;&#26088;&#22312;&#22635;&#34917;PINN&#65288;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65289;&#21644;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PIPN&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#25152;&#38656;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#25968;&#30334;&#20010;&#22495;&#19978;&#30340;&#35299;&#65292;&#32780;&#21482;&#20351;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20010;&#26694;&#26550;&#26377;&#21161;&#20110;&#22312;&#24037;&#19994;&#30028;&#36827;&#34892;&#24555;&#36895;&#30340;&#20960;&#20309;&#35774;&#35745;&#65292;&#23588;&#20854;&#24403;&#21482;&#26377;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PIPN&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#24179;&#38754;&#24212;&#21147;&#38382;&#39064;&#22312;500&#22810;&#20010;&#19981;&#21516;&#20960;&#20309;&#20307;&#19978;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular physics-informed neural networks (PINNs) predict the solution of partial differential equations using sparse labeled data but only over a single domain. On the other hand, fully supervised learning models are first trained usually over a few thousand domains with known solutions (i.e., labeled data) and then predict the solution over a few hundred unseen domains. Physics-informed PointNet (PIPN) is primarily designed to fill this gap between PINNs (as weakly supervised learning models) and fully supervised learning models. In this article, we demonstrate that PIPN predicts the solution of desired partial differential equations over a few hundred domains simultaneously, while it only uses sparse labeled data. This framework benefits fast geometric designs in the industry when only sparse labeled data are available. Particularly, we show that PIPN predicts the solution of a plane stress problem over more than 500 domains with different geometries, simultaneously. Moreover, we pio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Associated Random Neural Networks&#30340;Botnet&#25915;&#20987;&#38598;&#25104;&#20998;&#31867;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#34987;&#25915;&#38519;&#30340;&#33410;&#28857;&#24182;&#22312;&#20010;&#21035;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#19978;&#36827;&#34892;&#26356;&#22810;&#26377;&#25928;&#30340;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2303.13627</link><description>&lt;p&gt;
&#20851;&#20110;Botnet&#25915;&#20987;&#20013;&#33410;&#28857;&#30340;&#38598;&#25104;&#20998;&#31867;&#30340;Associated Random Neural Networks&#65288;&#35770;&#25991;&#32763;&#35793;&#65289;
&lt;/p&gt;
&lt;p&gt;
Associated Random Neural Networks for Collective Classification of Nodes in Botnet Attacks. (arXiv:2303.13627v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Associated Random Neural Networks&#30340;Botnet&#25915;&#20987;&#38598;&#25104;&#20998;&#31867;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#34987;&#25915;&#38519;&#30340;&#33410;&#28857;&#24182;&#22312;&#20010;&#21035;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#19978;&#36827;&#34892;&#26356;&#22810;&#26377;&#25928;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Botnet&#25915;&#20987;&#23545;&#32593;&#32476;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23558;&#23427;&#20204;&#25915;&#38519;&#30340;&#32593;&#32476;&#33410;&#28857;&#21464;&#25104;&#39069;&#22806;&#30340;&#25915;&#20987;&#32773;&#65292;&#23548;&#33268;&#39640;&#23481;&#37327;&#25915;&#20987;&#22312;&#38271;&#26102;&#38388;&#20869;&#34067;&#24310;&#12290;&#26816;&#27979;&#36825;&#31181;Botnet&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#22312;&#20110;&#65292;&#22810;&#20010;&#32593;&#32476;IP&#22320;&#22336;&#23558;&#21516;&#26102;&#34987;&#25915;&#38519;&#12290;&#20026;&#27492;&#65292;&#38500;&#20102;&#24050;&#26377;&#30340;&#38024;&#23545;&#20010;&#21035;&#33410;&#28857;&#30340;&#20256;&#32479;&#26041;&#27861;&#22806;&#65292;&#23545;&#20110; compromised nodes &#30340;&#38598;&#20307;&#20998;&#31867;&#20063;&#24456;&#26377;&#29992;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#20307;Botnet&#25915;&#20987;&#20998;&#31867;&#25216;&#26415;&#65292;&#23427;&#23545;&#19968;&#20010;n&#33410;&#28857;IP&#32593;&#32476;&#30340;&#27969;&#37327;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;Associated Random Neural Network&#65288;ARNN&#65289;&#26469;&#35782;&#21035;&#34987;&#25915;&#38519;&#30340;&#33410;&#28857;&#12290;ARNN&#26159;&#19968;&#31181;&#24490;&#29615;&#20307;&#26550;&#26500;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#12289;&#20114;&#30456;&#36830;&#25509;&#24182;&#19988;&#32467;&#26500;&#30456;&#21516;&#30340;n&#31070;&#32463;&#20803;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20204;&#21516;&#26102;&#20316;&#20026;&#20114;&#30456;&#25209;&#21028;&#30340;&#35780;&#21028;&#32773;&#26469;&#30830;&#23450;&#21738;&#20123;n&#20010;&#33410;&#28857;&#34987;&#25915;&#20987;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Botnet attacks are a major threat to networked systems because of their ability to turn the network nodes that they compromise into additional attackers, leading to the spread of high volume attacks over long periods. The detection of such Botnets is complicated by the fact that multiple network IP addresses will be simultaneously compromised, so that Collective Classification of compromised nodes, in addition to the already available traditional methods that focus on individual nodes, can be useful. Thus this work introduces a collective Botnet attack classification technique that operates on traffic from an n-node IP network with a novel Associated Random Neural Network (ARNN) that identifies the nodes which are compromised. The ARNN is a recurrent architecture that incorporates two mutually associated, interconnected and architecturally identical n-neuron random neural networks, that act simultneously as mutual critics to reach the decision regarding which of n nodes have been compr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20809;&#23398;&#25104;&#20687;&#25216;&#26415;&#65292;&#21487;&#20197;&#24555;&#36895;&#12289;&#26080;&#26631;&#35760;&#30340;&#23545;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#36827;&#34892;&#20998;&#23376;&#35786;&#26029;&#65292;&#20026;&#20854;&#27835;&#30103;&#25552;&#20379;&#26356;&#21152;&#20934;&#30830;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.13610</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#12289;&#26080;&#26631;&#35760;&#20809;&#23398;&#25104;&#20687;&#20998;&#23376;&#20998;&#31867;&#35786;&#26029;&#30340;&#24212;&#29992;&#20110;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging. (arXiv:2303.13610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20809;&#23398;&#25104;&#20687;&#25216;&#26415;&#65292;&#21487;&#20197;&#24555;&#36895;&#12289;&#26080;&#26631;&#35760;&#30340;&#23545;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#36827;&#34892;&#20998;&#23376;&#35786;&#26029;&#65292;&#20026;&#20854;&#27835;&#30103;&#25552;&#20379;&#26356;&#21152;&#20934;&#30830;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20998;&#31867;&#30340;&#24212;&#29992;&#20351;&#24471;&#33041;&#32959;&#30244;&#30340;&#27835;&#30103;&#24471;&#21040;&#36716;&#21464;&#65292;&#20351;&#35786;&#26029;&#26356;&#21152;&#20934;&#30830;&#65292;&#27835;&#30103;&#26356;&#21152;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24739;&#26377;&#33041;&#32959;&#30244;&#30340;&#30149;&#20154;&#65292;&#21450;&#26102;&#36827;&#34892;&#20998;&#23376;&#35786;&#26029;&#27979;&#35797;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#20351;&#24471;&#25163;&#26415;&#21644;&#21103;&#36741;&#21161;&#27835;&#30103;&#26356;&#20026;&#22797;&#26434;&#65292;&#38459;&#25747;&#20102;&#20020;&#24202;&#35797;&#39564;&#30340;&#25253;&#21517;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;DeepGlioma&#35786;&#26029;&#31579;&#36873;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#65288;&lt;90&#31186;&#65289;&#12289;&#26080;&#26631;&#35760;&#30340;&#20809;&#23398;&#25104;&#20687;&#35786;&#26029;&#25216;&#26415;&#12290;DeepGlioma&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#21050;&#28608;&#25289;&#26364;&#32452;&#32455;&#23398;&#65288;SRH&#65289;&#21644;&#22823;&#22411;&#12289;&#20844;&#20849;&#22522;&#22240;&#32452;&#25968;&#25454;&#12290;&#22312;153&#20363;&#36827;&#34892;SRH&#25104;&#20687;&#30340;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#24739;&#32773;&#32452;&#25104;&#30340;&#21069;&#30651;&#24615;&#12289;&#22810;&#20013;&#24515;&#12289;&#22269;&#38469;&#33539;&#22260;&#30340;&#27979;&#35797;&#38431;&#21015;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DeepGlioma&#21487;&#20197;&#39044;&#27979;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#29992;&#20110;&#23450;&#20041;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#20998;&#31867;&#30340;&#20998;&#23376;&#25913;&#21464;&#65288;IDH mut&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular classification has transformed the management of brain tumors by enabling more accurate prognostication and personalized treatment. However, timely molecular diagnostic testing for patients with brain tumors is limited, complicating surgical and adjuvant treatment and obstructing clinical trial enrollment. In this study, we developed DeepGlioma, a rapid ($&lt; 90$ seconds), artificial-intelligence-based diagnostic screening system to streamline the molecular diagnosis of diffuse gliomas. DeepGlioma is trained using a multimodal dataset that includes stimulated Raman histology (SRH); a rapid, label-free, non-consumptive, optical imaging method; and large-scale, public genomic data. In a prospective, multicenter, international testing cohort of patients with diffuse glioma ($n=153$) who underwent real-time SRH imaging, we demonstrate that DeepGlioma can predict the molecular alterations used by the World Health Organization to define the adult-type diffuse glioma taxonomy (IDH mut
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#24182;&#23548;&#20986;&#20102;&#21518;&#24724;&#19978;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#32771;&#34385;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#26102;&#32988;&#36807;&#20854;&#20182;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13604</link><description>&lt;p&gt;
&#24102;&#26377;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#36172;&#24466;&#21453;&#39304;&#30340;&#38543;&#26426;&#27425;&#27169;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback. (arXiv:2303.13604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#24182;&#23548;&#20986;&#20102;&#21518;&#24724;&#19978;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#32771;&#34385;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#26102;&#32988;&#36807;&#20854;&#20182;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26399;&#26395;&#19979;&#30340;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#65292;&#24310;&#36831;&#21453;&#39304;&#34987;&#20551;&#23450;&#20026;&#32452;&#21512;&#21644;&#21311;&#21517;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24310;&#36831;&#21453;&#39304;&#26159;&#30001;&#36807;&#21435;&#34892;&#21160;&#30340;&#22870;&#21169;&#32452;&#25104;&#30340;&#65292;&#36825;&#20123;&#22870;&#21169;&#30001;&#23376;&#32452;&#20214;&#26500;&#25104;&#65292;&#20854;&#26410;&#30693;&#30340;&#20998;&#37197;&#26041;&#24335;&#12290;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#65306;&#26377;&#30028;&#23545;&#25239;&#27169;&#22411;&#12289;&#38543;&#26426;&#29420;&#31435;&#27169;&#22411;&#21644;&#38543;&#26426;&#26465;&#20214;&#29420;&#31435;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#27599;&#31181;&#24310;&#36831;&#27169;&#22411;&#23548;&#20986;&#20102;&#21518;&#24724;&#30028;&#12290;&#24573;&#30053;&#38382;&#39064;&#30456;&#20851;&#21442;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#24310;&#36831;&#27169;&#22411;&#30340;&#21518;&#24724;&#30028;&#20026; $\tilde{O}(T^{2/3} + T^{1/3} \nu)$&#65292;&#20854;&#20013; $T$ &#26159;&#26102;&#38388;&#33539;&#22260;&#65292;$\nu$ &#26159;&#19977;&#31181;&#24773;&#20917;&#19979;&#19981;&#21516;&#23450;&#20041;&#30340;&#24310;&#36831;&#21442;&#25968;&#65292;&#22240;&#27492;&#23637;&#31034;&#20102;&#24102;&#26377;&#24310;&#36831;&#30340;&#34917;&#20607;&#39033;&#12290;&#25152;&#32771;&#34385;&#30340;&#31639;&#27861;&#34987;&#35777;&#26126;&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;&#32771;&#34385;&#20102;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#30340;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the sub-components. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models. The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedbac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#23384;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#22312;&#32570;&#22833;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23545;&#20110;&#25152;&#26377;&#24773;&#20917;&#27809;&#26377;&#21333;&#19968;&#30340;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#27604;&#36739;&#20854;&#20182;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#21644;(&#25110;)&#29983;&#23384;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13590</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#30340;&#29983;&#23384;&#23398;&#20064;&#31639;&#27861;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Une comparaison des algorithmes d'apprentissage pour la survie avec donn\'ees manquantes. (arXiv:2303.13590v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#23384;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#22312;&#32570;&#22833;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23545;&#20110;&#25152;&#26377;&#24773;&#20917;&#27809;&#26377;&#21333;&#19968;&#30340;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#27604;&#36739;&#20854;&#20182;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#21644;(&#25110;)&#29983;&#23384;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#30740;&#31350;&#20581;&#24247;&#25968;&#25454;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#36825;&#31181;&#25968;&#25454;&#30340;&#22266;&#26377;&#32452;&#25104;&#37096;&#20998;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#29983;&#23384;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#23545;&#37319;&#29992;&#19981;&#21516;&#32570;&#22833;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#30340;&#27492;&#31867;&#31639;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#21453;&#26144;&#20102;&#29616;&#23454;&#24773;&#20917;&#65292;&#21363;&#20010;&#20307;&#23646;&#20110;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#32676;&#20307;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#33021;&#22815;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#32780;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#29305;&#24449;&#24037;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#27604;&#36739;&#20854;&#20182;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#21644;(&#25110;)&#29983;&#23384;&#27169;&#22411;&#12290;Python&#20195;&#30721;&#36890;&#36807;survivalsim&#21253;&#21487;&#20197;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is an essential tool for the study of health data. An inherent component of such data is the presence of missing values. In recent years, researchers proposed new learning algorithms for survival tasks based on neural networks. Here, we studied the predictive performance of such algorithms coupled with different methods for handling missing values on simulated data that reflect a realistic situation, i.e., when individuals belong to unobserved clusters. We investigated different patterns of missing data. The results show that, without further feature engineering, no single imputation method is better than the others in all cases. The proposed methodology can be used to compare other missing data patterns and/or survival models. The Python code is accessible via the package survivalsim.  - L'analyse de survie est un outil essentiel pour l'\'etude des donn\'ees de sant\'e. Une composante inh\'erente \`a ces donn\'ees est la pr\'esence de valeurs manquantes. Ces derni\
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#35823;&#24046;&#39044;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;&#32622;&#20449;&#24230;&#12289;&#23616;&#37096;&#27969;&#24418;&#24179;&#28369;&#24230;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20998;&#20989;&#25968;&#30340;&#20248;&#32570;&#28857;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#26426;&#21046;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#26080;&#27861;&#22312;&#20998;&#24067;&#36716;&#31227;&#21644;&#25439;&#22351;&#19979;&#36229;&#36234;&#31616;&#21333;&#30340;&#27169;&#22411;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#22312;&#21463;&#25439;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#19968;&#33268;&#24615;&#25171;&#20998;&#20173;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#38598;&#25104;&#22810;&#26679;&#24615;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13589</link><description>&lt;p&gt;
Scoring Functions &#21644; Generalization Prediction &#30340;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Scoring Functions and Generalization Prediction. (arXiv:2303.13589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#35823;&#24046;&#39044;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;&#32622;&#20449;&#24230;&#12289;&#23616;&#37096;&#27969;&#24418;&#24179;&#28369;&#24230;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20998;&#20989;&#25968;&#30340;&#20248;&#32570;&#28857;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#26426;&#21046;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#26080;&#27861;&#22312;&#20998;&#24067;&#36716;&#31227;&#21644;&#25439;&#22351;&#19979;&#36229;&#36234;&#31616;&#21333;&#30340;&#27169;&#22411;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#22312;&#21463;&#25439;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#19968;&#33268;&#24615;&#25171;&#20998;&#20173;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#38598;&#25104;&#22810;&#26679;&#24615;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#35823;&#24046;&#39044;&#27979;&#22120;&#65288;GEPs&#65289;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123; GEPs &#26088;&#22312;&#36890;&#36807;&#20174;&#26679;&#26412;&#32423;&#20998;&#25968;&#20013;&#25512;&#23548;&#20986;&#25968;&#25454;&#38598;&#32423;&#35823;&#24046;&#20272;&#35745;&#20540;&#65292;&#20174;&#32780;&#39044;&#27979;&#27169;&#22411;&#22312;&#26410;&#35265;&#20998;&#24067;&#19978;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;GEPs &#24120;&#24120;&#21033;&#29992;&#19981;&#21516;&#30340;&#26426;&#21046;&#65288;&#20363;&#22914;&#65292;&#22238;&#24402;&#22120;&#12289;&#38408;&#20540;&#20989;&#25968;&#12289;&#26657;&#20934;&#25968;&#25454;&#38598;&#31561;&#65289;&#65292;&#26469;&#25512;&#23548;&#36825;&#31181;&#35823;&#24046;&#20272;&#35745;&#20540;&#65292;&#36825;&#20250;&#28151;&#28102;&#29305;&#23450;&#35780;&#20998;&#20989;&#25968;&#30340;&#20248;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22312;&#26426;&#21046;&#36873;&#25321;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#35780;&#20998;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#65288;&#32622;&#20449;&#24230;&#12289;&#23616;&#37096;&#27969;&#24418;&#24179;&#28369;&#24230;&#12289;&#27169;&#22411;&#19968;&#33268;&#24615;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22797;&#26434;&#26426;&#21046;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#20272;&#35745;&#20998;&#24067;&#36716;&#31227;&#21644;&#25439;&#22351;&#19979;&#30340;&#35823;&#24046;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;&#32622;&#20449;&#24230;&#21644;&#24179;&#28369;&#24230;&#22522;&#30784;&#35780;&#20998;&#26080;&#27861;&#36229;&#36234;&#31616;&#21333;&#30340;&#27169;&#22411;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#21463;&#21040;&#25439;&#23475;&#26102;&#65288;&#20363;&#22914;&#26631;&#31614;&#22122;&#22768;&#12289;&#27979;&#37327;&#22122;&#22768;&#12289;&#27424;&#37319;&#26679;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#19968;&#33268;&#24615;&#25171;&#20998;&#20173;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#38598;&#25104;&#22810;&#26679;&#24615;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization error predictors (GEPs) aim to predict model performance on unseen distributions by deriving dataset-level error estimates from sample-level scores. However, GEPs often utilize disparate mechanisms (e.g., regressors, thresholding functions, calibration datasets, etc), to derive such error estimates, which can obfuscate the benefits of a particular scoring function. Therefore, in this work, we rigorously study the effectiveness of popular scoring functions (confidence, local manifold smoothness, model agreement), independent of mechanism choice. We find, absent complex mechanisms, that state-of-the-art confidence- and smoothness- based scores fail to outperform simple model-agreement scores when estimating error under distribution shifts and corruptions. Furthermore, on realistic settings where the training data has been compromised (e.g., label noise, measurement noise, undersampling), we find that model-agreement scores continue to perform well and that ensemble diversi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31526;&#21495;&#25512;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#39564;&#35777;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#32534;&#30721;&#35768;&#22810;&#39564;&#35777;&#38382;&#39064;&#20026;&#20108;&#27425;&#31243;&#24207;&#65292;&#24182;&#23558;&#20854;&#26494;&#24347;&#20026;&#21322;&#23450;&#31243;&#24207;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#39564;&#35777;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#65292;&#24182;&#20026;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.13588</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#39640;&#25928;&#31526;&#21495;&#25512;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Efficient Symbolic Reasoning for Neural-Network Verification. (arXiv:2303.13588v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31526;&#21495;&#25512;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#39564;&#35777;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#32534;&#30721;&#35768;&#22810;&#39564;&#35777;&#38382;&#39064;&#20026;&#20108;&#27425;&#31243;&#24207;&#65292;&#24182;&#23558;&#20854;&#26494;&#24347;&#20026;&#21322;&#23450;&#31243;&#24207;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#39564;&#35777;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#65292;&#24182;&#20026;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31243;&#24207;&#25512;&#29702;&#26694;&#26550;&#65292;&#31216;&#20026;&#31526;&#21495;&#25512;&#29702;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#31526;&#21495;&#22495;&#21644;&#20108;&#27425;&#20851;&#31995;&#30340;&#20351;&#29992;&#12290;&#31526;&#21495;&#22495;&#20855;&#26377;&#38750;&#24120;&#28789;&#27963;&#30340;&#35821;&#20041;&#65292;&#32780;&#20108;&#27425;&#20851;&#31995;&#21017;&#38750;&#24120;&#34920;&#36798;&#33021;&#21147;&#24378;&#12290;&#23427;&#20204;&#20801;&#35768;&#25105;&#20204;&#23558;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#38382;&#39064;&#32534;&#30721;&#20026;&#20108;&#27425;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23558;&#20108;&#27425;&#31243;&#24207;&#26494;&#24347;&#20026;&#21322;&#23450;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#12290;&#36825;&#20010;&#26694;&#26550;&#35753;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#39564;&#35777;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23545;&#38750;&#31526;&#21495;&#22495;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34920;&#31034;&#21644;&#39564;&#35777;&#20219;&#21153;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24102;&#26469;&#37325;&#35201;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#35299;&#20915;&#23427;&#20204;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural network has become an integral part of modern software systems. However, they still suffer from various problems, in particular, vulnerability to adversarial attacks. In this work, we present a novel program reasoning framework for neural-network verification, which we refer to as symbolic reasoning. The key components of our framework are the use of the symbolic domain and the quadratic relation. The symbolic domain has very flexible semantics, and the quadratic relation is quite expressive. They allow us to encode many verification problems for neural networks as quadratic programs. Our scheme then relaxes the quadratic programs to semidefinite programs, which can be efficiently solved. This framework allows us to verify various neural-network properties under different scenarios, especially those that appear challenging for non-symbolic domains. Moreover, it introduces new representations and perspectives for the verification tasks. We believe that our framework can bring
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21487;&#36870;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#19982;&#20854;&#20182;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#36755;&#20986;&#23618;&#37325;&#24314;&#36755;&#20837;&#24207;&#21015;&#30340;&#21333;&#35789;&#21521;&#37327;&#65292;&#20854;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#24555;&#36895;&#35757;&#32451;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#21512;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#38656;&#35201;&#39640;&#36136;&#37327;&#21477;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#20351;&#29992;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13570</link><description>&lt;p&gt;
RNN &#30340;&#22238;&#24402;&#65306;&#29992;&#21487;&#36870;&#21477;&#23884;&#20837;&#30340;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21487;&#36870;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#19982;&#20854;&#20182;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#36755;&#20986;&#23618;&#37325;&#24314;&#36755;&#20837;&#24207;&#21015;&#30340;&#21333;&#35789;&#21521;&#37327;&#65292;&#20854;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#24555;&#36895;&#35757;&#32451;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#21512;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#38656;&#35201;&#39640;&#36136;&#37327;&#21477;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#20351;&#29992;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#20351;&#29992;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#30417;&#30563;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#21487;&#36870;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#30456;&#27604;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#24120;&#35265;&#30340;&#27010;&#29575;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#36755;&#20986;&#23618;&#26469;&#37325;&#24314;&#36755;&#20837;&#24207;&#21015;&#30340;&#21333;&#35789;&#21521;&#37327;&#12290;&#35813;&#27169;&#22411;&#22312;&#20351;&#29992; ADAM &#20248;&#21270;&#22120;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27531;&#24046;&#36830;&#25509;&#21644;&#8220;match drop&#8221;&#25216;&#26415;&#65292;&#21363;&#21482;&#35745;&#31639;&#38169;&#35823;&#21333;&#35789;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#36136;&#37327;&#21477;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel model for invertible sentence embeddings using a residual recurrent network trained on an unsupervised encoding task. Rather than the probabilistic outputs common to neural machine translation models, our approach employs a regression-based output layer to reconstruct the input sequence's word vectors. The model achieves high accuracy and fast training with the ADAM optimizer, a significant finding given that RNNs typically require memory units, such as LSTMs, or second-order optimization methods. We incorporate residual connections and introduce a "match drop" technique, where gradients are calculated only for incorrect words. Our approach demonstrates potential for various natural language processing applications, particularly in neural network-based systems that require high-quality sentence embeddings.
&lt;/p&gt;</description></item><item><title>TinyML&#26159;&#19968;&#31181;&#23884;&#20837;&#24335;ML&#25216;&#26415;&#65292;&#21487;&#22312;&#22810;&#31181;&#24265;&#20215;&#65292;&#36164;&#28304;&#26377;&#38480;&#21644;&#21151;&#29575;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;ML&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#22312;&#20854;&#23454;&#29616;&#36807;&#31243;&#20013;&#65292;&#38656;&#35201;&#21450;&#26102;&#35299;&#20915;&#22810;&#31181;&#25361;&#25112;&#65292;&#20363;&#22914;&#22788;&#29702;&#33021;&#21147;&#20248;&#21270;&#65292;&#25552;&#39640;&#21487;&#38752;&#24615;&#20197;&#21450;&#32500;&#25252;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13569</link><description>&lt;p&gt;
TinyML&#65306;&#24037;&#20855;&#65292;&#24212;&#29992;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
TinyML: Tools, Applications, Challenges, and Future Research Directions. (arXiv:2303.13569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13569
&lt;/p&gt;
&lt;p&gt;
TinyML&#26159;&#19968;&#31181;&#23884;&#20837;&#24335;ML&#25216;&#26415;&#65292;&#21487;&#22312;&#22810;&#31181;&#24265;&#20215;&#65292;&#36164;&#28304;&#26377;&#38480;&#21644;&#21151;&#29575;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;ML&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#22312;&#20854;&#23454;&#29616;&#36807;&#31243;&#20013;&#65292;&#38656;&#35201;&#21450;&#26102;&#35299;&#20915;&#22810;&#31181;&#25361;&#25112;&#65292;&#20363;&#22914;&#22788;&#29702;&#33021;&#21147;&#20248;&#21270;&#65292;&#25552;&#39640;&#21487;&#38752;&#24615;&#20197;&#21450;&#32500;&#25252;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#37117;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ML&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#28385;&#36275;&#25152;&#38656;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#20027;&#35201;&#29992;&#20110;&#39640;&#24615;&#33021;&#35774;&#22791;&#65288;&#22914;&#32593;&#32476;&#33410;&#28857;&#65289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29289;&#32852;&#32593;&#21644;&#36793;&#32536;&#35745;&#31639;&#31561;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23558;ML&#25216;&#26415;&#32435;&#20837;&#21040;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#20197;&#23454;&#29616;&#20998;&#24067;&#24335;&#21644;&#26222;&#36866;&#24615;&#26234;&#33021;&#20063;&#21464;&#24471;&#21313;&#20998;&#26377;&#24517;&#35201;&#12290;&#36825;&#20419;&#20351;&#20986;&#29616;&#20102;TinyML&#33539; paradigm&#65292;&#23427;&#26159;&#19968;&#31181;&#23884;&#20837;&#24335;ML&#25216;&#26415;&#65292;&#21487;&#22312;&#22810;&#31181;&#24265;&#20215;&#65292;&#36164;&#28304;&#26377;&#38480;&#21644;&#21151;&#29575;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;ML&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#21521;TinyML&#25216;&#26415;&#36827;&#34892;&#36866;&#24403;&#23454;&#29616;&#30340;&#36807;&#31243;&#20013;&#65292;&#38656;&#35201;&#21450;&#26102;&#35299;&#20915;&#22810;&#20010;&#25361;&#25112;&#65292;&#20363;&#22914;&#22788;&#29702;&#33021;&#21147;&#20248;&#21270;&#65292;&#25552;&#39640;&#21487;&#38752;&#24615;&#20197;&#21450;&#32500;&#25252;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Artificial Intelligence (AI) and Machine learning (ML) have gained significant interest from both, industry and academia. Notably, conventional ML techniques require enormous amounts of power to meet the desired accuracy, which has limited their use mainly to high-capability devices such as network nodes. However, with many advancements in technologies such as the Internet of Things (IoT) and edge computing, it is desirable to incorporate ML techniques into resource-constrained embedded devices for distributed and ubiquitous intelligence. This has motivated the emergence of the TinyML paradigm which is an embedded ML technique that enables ML applications on multiple cheap, resource- and power-constrained devices. However, during this transition towards appropriate implementation of the TinyML technology, multiple challenges such as processing capacity optimization, improved reliability, and maintenance of learning models' accuracy require timely solutions. In this art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25913;&#36827;&#21518;&#30340;&#26041;&#27861;&#20174;&#22823;&#37327;&#24179;&#38754;&#22270;&#20687;&#20013;&#25552;&#21462;&#35775;&#38382;&#22270;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8220;&#24179;&#38754;&#22270;&#20215;&#20540;&#8221;&#20272;&#35745;&#35775;&#38382;&#22270;&#30340;&#25151;&#22320;&#20135;&#20215;&#20540;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#31199;&#37329;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;&#35813;&#27169;&#22411;&#20026;&#20840;&#38754;&#20272;&#35745;&#24179;&#38754;&#22270;&#30340;&#20215;&#20540;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.13568</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#31199;&#36161;&#20844;&#23507;&#24179;&#38754;&#22270;&#30340;&#25151;&#22320;&#20135;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Extracting real estate values of rental apartment floor plans using graph convolutional networks. (arXiv:2303.13568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25913;&#36827;&#21518;&#30340;&#26041;&#27861;&#20174;&#22823;&#37327;&#24179;&#38754;&#22270;&#20687;&#20013;&#25552;&#21462;&#35775;&#38382;&#22270;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8220;&#24179;&#38754;&#22270;&#20215;&#20540;&#8221;&#20272;&#35745;&#35775;&#38382;&#22270;&#30340;&#25151;&#22320;&#20135;&#20215;&#20540;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#31199;&#37329;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;&#35813;&#27169;&#22411;&#20026;&#20840;&#38754;&#20272;&#35745;&#24179;&#38754;&#22270;&#30340;&#20215;&#20540;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22823;&#37327;&#20301;&#20110;&#26085;&#26412;&#22823;&#38442;&#30340;&#23478;&#24237;&#24335;&#31199;&#36161;&#20844;&#23507;&#30340;&#24179;&#38754;&#22270;&#20687;&#20013;&#25552;&#21462;&#21453;&#26144;&#25151;&#38388;&#27969;&#32447;&#37051;&#25509;&#20851;&#31995;&#30340;&#20960;&#20309;&#22270;&#24418;&#65292;&#24182;&#20351;&#29992;&#25913;&#36827;&#21518;&#30340;&#35775;&#38382;&#22270;&#25552;&#21462;&#26041;&#27861;&#23450;&#20041;&#21644;&#23454;&#29616;&#35775;&#38382;&#22270;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#35775;&#38382;&#22270;&#30340;&#25151;&#22320;&#20135;&#20215;&#20540;&#30340;&#26041;&#27861;&#65292;&#21363;&#8220;&#24179;&#38754;&#22270;&#20215;&#20540;&#8221;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24179;&#38754;&#22270;&#20215;&#20540;&#12289;&#20351;&#29992;&#20854;&#20182;&#19968;&#33324;&#24615;&#35299;&#37322;&#21464;&#37327;&#30340;&#20139;&#21463;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#20272;&#35745;&#31199;&#37329;&#65292;&#27604;&#36739;&#20272;&#35745;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#32593;&#32476;&#65292;&#20998;&#26512;&#20102;&#35299;&#37322;&#31199;&#37329;&#30340;&#24179;&#38754;&#22270;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#19968;&#31181;&#20840;&#38754;&#20272;&#35745;&#25151;&#22320;&#20135;&#24179;&#38754;&#22270;&#20215;&#20540;&#30340;&#26032;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31199;&#37329;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Access graphs that indicate adjacency relationships from the perspective of flow lines of rooms are extracted automatically from a large number of floor plan images of a family-oriented rental apartment complex in Osaka Prefecture, Japan, based on a recently proposed access graph extraction method with slight modifications. We define and implement a graph convolutional network (GCN) for access graphs and propose a model to estimate the real estate value of access graphs as the floor plan value. The model, which includes the floor plan value and hedonic method using other general explanatory variables, is used to estimate rents and their estimation accuracies are compared. In addition, the features of the floor plan that explain the rent are analyzed from the learned convolution network. Therefore, a new model for comprehensively estimating the value of real estate floor plans is proposed and validated. The results show that the proposed method significantly improves the accuracy of ren
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#22312;&#20840;&#29699;21&#23478;&#21307;&#38498;&#30340;10,000&#22810;&#20301;COVID&#24739;&#32773;&#30340;&#33016;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#21512;&#25104;&#29983;&#25104;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#20026;&#21307;&#23398;AI&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.13567</link><description>&lt;p&gt;
&#20351;&#29992;&#33016;&#37096;CT&#36827;&#34892;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Heterogenous Data using Chest CT. (arXiv:2303.13567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#22312;&#20840;&#29699;21&#23478;&#21307;&#38498;&#30340;10,000&#22810;&#20301;COVID&#24739;&#32773;&#30340;&#33016;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#21512;&#25104;&#29983;&#25104;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#20026;&#21307;&#23398;AI&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#65292;&#26469;&#33258;&#36951;&#20256;&#12289;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#39278;&#39135;&#21644;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#30340;&#20154;&#32676;&#24046;&#24322;&#23545;&#30142;&#30149;&#36129;&#29486;&#26174;&#33879;&#65292;&#20294;&#26159;&#65292;&#22312;&#21307;&#23398;&#20013;&#36827;&#34892;&#30340;AI&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20855;&#26377;&#36739;&#23569;&#25968;&#25454;&#26469;&#28304;&#21644;&#36739;&#23569;&#26679;&#26412;&#24046;&#24322;&#30340;&#21306;&#22495;&#24739;&#32773;&#38431;&#21015;&#20013;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110;&#22312;&#21307;&#23398;&#20013;&#23454;&#29616;&#22823;&#35268;&#27169;&#25968;&#25454;&#20849;&#20139;&#30340;&#38556;&#30861;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#36947;&#24503;&#25285;&#24551;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;AI&#21457;&#23637;&#36884;&#24452;&#65292;&#21487;&#20197;&#22312;&#21307;&#38498;&#20043;&#38388;&#36827;&#34892;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19977;&#31181;&#25216;&#26415;&#65288;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#12289;&#22686;&#37327;&#21046;&#24230;&#23398;&#20064;&#65288;IIL&#65289;&#21644;&#24490;&#29615;&#22686;&#37327;&#21046;&#24230;&#23398;&#20064;&#65288;CIIL&#65289;&#65289;&#22312;&#28085;&#30422;5&#20010;&#22823;&#38470;&#30340;21&#23478;&#21442;&#19982;&#21307;&#38498;&#20013;&#36827;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#22312;COVID-19&#33016;&#37096;CT&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#26368;&#22823;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#65292;&#21253;&#25324;&gt;10,000&#20301;&#24739;&#32773;&#21644;&gt;1,000,000&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#29983;&#25104;&#25968;&#25454;&#21644;FL&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large data have accelerated advances in AI. While it is well known that population differences from genetics, sex, race, diet, and various environmental factors contribute significantly to disease, AI studies in medicine have largely focused on locoregional patient cohorts with less diverse data sources. Such limitation stems from barriers to large-scale data share in medicine and ethical concerns over data privacy. Federated learning (FL) is one potential pathway for AI development that enables learning across hospitals without data share. In this study, we show the results of various FL strategies on one of the largest and most diverse COVID-19 chest CT datasets: 21 participating hospitals across five continents that comprise &gt;10,000 patients with &gt;1 million images. We present three techniques: Fed Averaging (FedAvg), Incremental Institutional Learning (IIL), and Cyclical Incremental Institutional Learning (CIIL). We also propose an FL strategy that leverages synthetically generated 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#24212;&#29992;&#20110;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;PharmKG&#25968;&#25454;&#38598;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#23454;&#39564;&#29615;&#22659;&#20013;&#20851;&#31995;&#20107;&#23454;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13566</link><description>&lt;p&gt;
&#20351;&#29992;&#36923;&#36753;&#30693;&#35782;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Embedding Representations of Biomedical Data using Logic Knowledge. (arXiv:2303.13566v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#24212;&#29992;&#20110;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;PharmKG&#25968;&#25454;&#38598;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#23454;&#39564;&#29615;&#22659;&#20013;&#20851;&#31995;&#20107;&#23454;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#26412;&#20307;&#35770;&#21644;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38544;&#24335;&#22320;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#32534;&#30721;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290; KGE&#25216;&#26415;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#23545;&#35937;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#22788;&#29702;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#38750;&#24120;&#24120;&#35265;&#12290;&#26368;&#36817;&#65292;PharmKG&#25968;&#25454;&#38598;&#34987;&#25552;&#20986;&#20316;&#20026;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#30693;&#35782;&#22270;&#35889;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#28041;&#21450;&#22522;&#22240;&#65292;&#30142;&#30149;&#21644;&#21270;&#23398;&#29289;&#36136;&#20043;&#38388;&#25968;&#21313;&#19975;&#20010;&#20851;&#31995;&#20107;&#23454;&#12290;&#23613;&#31649; KGE &#21487;&#20197;&#22788;&#29702;&#38750;&#24120;&#22823;&#30340;&#20851;&#31995;&#22495;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#34920;&#31034;&#20851;&#31995;&#20107;&#23454;&#20043;&#38388;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#36923;&#36753;&#35268;&#21017;&#65292;&#22312;&#22797;&#26434;&#30340;&#23454;&#39564;&#29615;&#22659;&#20013;&#21487;&#33021;&#26159;&#22522;&#26412;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#22686;&#24378;KGE&#22312;PharmKG&#25968;&#25454;&#38598;&#19978;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embeddings (KGE) have become a quite popular class of models specifically devised to deal with ontologies and graph structure data, as they can implicitly encode statistical dependencies between entities and relations in a latent space. KGE techniques are particularly effective for the biomedical domain, where it is quite common to deal with large knowledge graphs underlying complex interactions between biological and chemical objects. Recently in the literature, the PharmKG dataset has been proposed as one of the most challenging knowledge graph biomedical benchmark, with hundreds of thousands of relational facts between genes, diseases and chemicals. Despite KGEs can scale to very large relational domains, they generally fail at representing more complex relational dependencies between facts, like logic rules, which may be fundamental in complex experimental settings. In this paper, we exploit logic rules to enhance the embedding representations of KGEs on the PharmKG
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#25968;&#23398;&#24050;&#32463;&#34987;&#24191;&#27867;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65292;&#20294;&#22312;&#24418;&#24335;&#21270;&#35774;&#35745;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#36824;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22270;&#24352;&#37327;&#32593;&#32476;&#65288;GTN&#65289;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#32780;&#20005;&#35880;&#30340;&#22270;&#24418;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#39046;&#22495;&#19978;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#23398;&#20064;&#31995;&#32479;&#19978;&#31995;&#32479;&#22320;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#19988;&#36890;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#24456;&#24378;&#12290;</title><link>http://arxiv.org/abs/2303.13565</link><description>&lt;p&gt;
&#22270;&#24352;&#37327;&#32593;&#32476;&#65306;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#35774;&#35745;&#22823;&#35268;&#27169;&#31070;&#32463;&#23398;&#20064;&#31995;&#32479;&#30340;&#30452;&#35266;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Graph Tensor Networks: An Intuitive Framework for Designing Large-Scale Neural Learning Systems on Multiple Domains. (arXiv:2303.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13565
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#25968;&#23398;&#24050;&#32463;&#34987;&#24191;&#27867;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65292;&#20294;&#22312;&#24418;&#24335;&#21270;&#35774;&#35745;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#36824;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22270;&#24352;&#37327;&#32593;&#32476;&#65288;GTN&#65289;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#32780;&#20005;&#35880;&#30340;&#22270;&#24418;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#39046;&#22495;&#19978;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#23398;&#20064;&#31995;&#32479;&#19978;&#31995;&#32479;&#22320;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#19988;&#36890;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#24456;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24352;&#37327;&#21450;&#20854;&#36816;&#31639;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20869;&#65292;&#20351;&#29992;&#24352;&#37327;&#25968;&#23398;&#26469;&#24418;&#24335;&#21270;&#35774;&#35745;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22270;&#24352;&#37327;&#32593;&#32476;&#65288;GTN&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#30452;&#35266;&#32780;&#20005;&#35880;&#30340;&#22270;&#24418;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#35774;&#35745;&#21644;&#23454;&#29616;&#22312;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#39046;&#22495;&#19978;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#23398;&#20064;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#34987;&#35777;&#26126;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#20197;&#21253;&#25324;&#35768;&#22810;&#27969;&#34892;&#30340;&#20307;&#31995;&#32467;&#26500;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#19988;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#22788;&#29702;&#20219;&#20309;&#21644;&#35768;&#22810;&#25968;&#25454;&#22495;&#19978;&#30340;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#24471;&#21040;&#35777;&#26126;&#65292;&#36890;&#36807;&#24352;&#37327;&#20195;&#25968;&#30340;&#20248;&#28857;&#65292;&#32467;&#26524;&#22312;&#26497;&#20302;&#30340;&#22797;&#26434;&#24230;&#25104;&#26412;&#19979;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the omnipresence of tensors and tensor operations in modern deep learning, the use of tensor mathematics to formally design and describe neural networks is still under-explored within the deep learning community. To this end, we introduce the Graph Tensor Network (GTN) framework, an intuitive yet rigorous graphical framework for systematically designing and implementing large-scale neural learning systems on both regular and irregular domains. The proposed framework is shown to be general enough to include many popular architectures as special cases, and flexible enough to handle data on any and many data domains. The power and flexibility of the proposed framework is demonstrated through real-data experiments, resulting in improved performance at a drastically lower complexity costs, by virtue of tensor algebra.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36339;&#36291;&#36830;&#25509;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#20248;&#21270;&#36339;&#36291;&#36830;&#25509;&#30340;&#20301;&#32622;&#12289;&#31867;&#22411;&#21644;&#25968;&#37327;&#31561;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;SNN&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20351;&#20854;&#25910;&#25947;&#26356;&#24555;&#12289;&#20449;&#24687;&#27969;&#26356;&#30021;&#12290;</title><link>http://arxiv.org/abs/2303.13563</link><description>&lt;p&gt;
&#36339;&#36291;&#36830;&#25509;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65306;&#23545;&#32593;&#32476;&#35757;&#32451;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skip Connections in Spiking Neural Networks: An Analysis of Their Effect on Network Training. (arXiv:2303.13563v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36339;&#36291;&#36830;&#25509;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#20248;&#21270;&#36339;&#36291;&#36830;&#25509;&#30340;&#20301;&#32622;&#12289;&#31867;&#22411;&#21644;&#25968;&#37327;&#31561;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;SNN&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20351;&#20854;&#25910;&#25947;&#26356;&#24555;&#12289;&#20449;&#24687;&#27969;&#26356;&#30021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20855;&#26377;&#28508;&#22312;&#30340;&#33021;&#37327;&#25928;&#29575;&#21644;&#27169;&#25311;&#29983;&#29289;&#31995;&#32479;&#23574;&#23792;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#24050;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#25104;&#20026;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#26377;&#21147;&#26367;&#20195;&#12290;&#28982;&#32780;&#65292;SNN&#30340;&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#26032;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36339;&#36291;&#36830;&#25509;&#23545;SNN&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#20248;&#21270;&#25216;&#26415;&#65292;&#23558;ANN&#27169;&#22411;&#35843;&#25972;&#20026;SNN&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20248;&#21270;&#36339;&#36291;&#36830;&#25509;&#30340;&#20301;&#32622;&#12289;&#31867;&#22411;&#21644;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;SNN&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#21644;&#22686;&#21152;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#27969;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;CIFAR-10-DVS&#21644;DVS128&#25163;&#21183;&#25968;&#25454;&#38598;&#19978;&#65292;&#22810;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;8%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have gained attention as a promising alternative to traditional artificial neural networks (ANNs) due to their potential for energy efficiency and their ability to model spiking behavior in biological systems. However, the training of SNNs is still a challenging problem, and new techniques are needed to improve their performance. In this paper, we study the impact of skip connections on SNNs and propose a hyperparameter optimization technique that adapts models from ANN to SNN. We demonstrate that optimizing the position, type, and number of skip connections can significantly improve the accuracy and efficiency of SNNs by enabling faster convergence and increasing information flow through the network. Our results show an average +8% accuracy increase on CIFAR-10-DVS and DVS128 Gesture datasets adaptation of multiple state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#23398;&#20064;&#21333;&#21521;&#32806;&#21512;&#30340;&#33021;&#21147;&#12290;&#20165;&#20351;&#29992;&#31995;&#32479;&#30340;&#20960;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21518;&#65292;ESN&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#39537;&#21160;&#20449;&#21495;&#19979;&#21709;&#24212;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#19968;&#20123;$A-B$&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#35753;ESN&#23398;&#20064;&#36825;&#31181;&#32806;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.13562</link><description>&lt;p&gt;
&#21033;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23398;&#20064;&#21333;&#21521;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Learning unidirectional coupling using echo-state network. (arXiv:2303.13562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#23398;&#20064;&#21333;&#21521;&#32806;&#21512;&#30340;&#33021;&#21147;&#12290;&#20165;&#20351;&#29992;&#31995;&#32479;&#30340;&#20960;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21518;&#65292;ESN&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#39537;&#21160;&#20449;&#21495;&#19979;&#21709;&#24212;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#19968;&#20123;$A-B$&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#35753;ESN&#23398;&#20064;&#36825;&#31181;&#32806;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27785;&#31215;&#35745;&#31639;&#24050;&#32463;&#22312;&#22797;&#26434;&#21160;&#21147;&#23398;&#39046;&#22495;&#25214;&#21040;&#20102;&#35768;&#22810;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#27169;&#22411;&#30340;&#24322;&#24120;&#33021;&#21147;&#65292;&#20165;&#20351;&#29992;&#31995;&#32479;&#30340;&#20960;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23601;&#20351;&#20854;&#23398;&#20064;&#21333;&#21521;&#32806;&#21512;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#19968;&#26086;&#35813;&#27169;&#22411;&#36890;&#36807;&#19968;&#20123;&#39537;&#21160;&#21709;&#24212;&#31995;&#32479;&#30340;&#21160;&#24577;&#35757;&#32451;&#20043;&#21518;&#65292;&#21363;&#20351;&#25105;&#20204;&#23558;&#39537;&#21160;&#31995;&#32479;$A$&#26367;&#25442;&#20026;&#20854;&#20182;&#31995;&#32479;$C$&#65292;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#20063;&#33021;&#20165;&#20165;&#29992;&#26032;&#30340;&#39537;&#21160;&#31995;&#32479;$C$&#30340;&#21160;&#24577;&#39044;&#27979;&#21709;&#24212;&#31995;&#32479;$B$&#30340;&#21160;&#24577;&#65292;&#25105;&#20204;&#20165;&#20165;&#38656;&#35201;&#19968;&#20123;$A-B$&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#35753;ESN&#23398;&#20064;&#36825;&#31181;&#32806;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing has found many potential applications in the field of complex dynamics. In this article, we exploit the exceptional capability of the echo-state network (ESN) model to make it learn a unidirectional coupling scheme from only a few time series data of the system. We show that, once trained with a few example dynamics of a drive-response system, the machine is able to predict the response system's dynamics for any driver signal with the same coupling. Only a few time series data of an $A-B$ type drive-response system in training is sufficient for the ESN to learn the coupling scheme. After training even if we replace drive system $A$ with a different system $C$, the ESN can reproduce the dynamics of response system $B$ using the dynamics of new drive system $C$ only.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;GAN&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.13559</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;GAN&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing Unsupervised Speech Recognition with Diffusion GANs. (arXiv:2303.13559v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;GAN&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25193;&#25955;GAN&#22686;&#24378;&#20102;&#29992;&#20110;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#30340;&#26222;&#36890;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;(1)&#27880;&#20837;&#20102;&#24378;&#24230;&#19981;&#21516;&#30340;&#23454;&#20363;&#22122;&#22768;&#21040;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#21644;&#26410;&#26631;&#35760;&#30340;&#21442;&#32771;&#25991;&#26412;&#65292;&#36825;&#20123;&#25991;&#26412;&#26159;&#20174;&#24102;&#26377;&#38271;&#24230;&#32422;&#26463;&#30340;&#39044;&#35757;&#32451;&#38899;&#32032;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#65292;(2)&#35831;&#27714;&#25193;&#25955;&#26102;&#38388;&#27493;&#39588;&#30456;&#20851;&#30340;&#21028;&#21035;&#22120;&#23558;&#23427;&#20204;&#20998;&#24320;&#65292;(3)&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#20197;&#26356;&#26032;&#29983;&#25104;&#22120;&#12290;&#22312;Librispeech(&#27979;&#35797;&#24178;&#20928;&#21644;&#27979;&#35797;&#20854;&#20182;&#30340;&#35823;&#24046;&#29575;&#20998;&#21035;&#20026;3.1%&#21644;5.6%)&#12289;TIMIT&#21644;MLS&#25968;&#25454;&#38598;&#19979;&#65292;&#19982;wav2vec-U&#36827;&#34892;&#21333;&#35789;/&#38899;&#32032;&#38169;&#35823;&#29575;&#27604;&#36739;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22686;&#24378;&#31574;&#30053;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We enhance the vanilla adversarial training method for unsupervised Automatic Speech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance noises of various intensities to the generator's output and unlabeled reference text which are sampled from pretrained phoneme language models with a length constraint, (2) asks diffusion timestep-dependent discriminators to separate them, and (3) back-propagates the gradients to update the generator. Word/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for test-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our enhancement strategies work effectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#27169;&#22411;&#21644;&#21457;&#29616;&#21560;&#38468;&#25668;&#21462;&#27169;&#22411;&#30340;&#21560;&#38468;&#21160;&#21147;&#23398;&#23450;&#24459;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.13555</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#24314;&#27169;&#21644;&#21560;&#38468;&#27169;&#22411;&#25506;&#32034;&#65306;&#31995;&#32479;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach. (arXiv:2303.13555v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#27169;&#22411;&#21644;&#21457;&#29616;&#21560;&#38468;&#25668;&#21462;&#27169;&#22411;&#30340;&#21560;&#38468;&#21160;&#21147;&#23398;&#23450;&#24459;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#27169;&#22411;&#21644;&#21457;&#29616;&#21560;&#38468;&#25668;&#21462;&#27169;&#22411;&#12290;&#23427;&#28436;&#31034;&#20102;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#12289;&#20276;&#38543;&#28789;&#25935;&#24230;&#20998;&#26512;&#21644;JIT&#32534;&#35793;&#30340;&#21521;&#37327;&#38597;&#21508;&#24067;&#20056;&#31215;&#65292;&#32467;&#21512;&#31354;&#38388;&#31163;&#25955;&#21644;&#33258;&#36866;&#24212;&#31215;&#20998;&#22120;&#26469;&#35757;&#32451;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#31232;&#30095;&#21644;&#31526;&#21495;&#22238;&#24402;&#34987;&#29992;&#26469;&#35782;&#21035;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#32570;&#22833;&#30340;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#22312;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22266;&#23450;&#24202;&#21560;&#38468;&#30340;&#22122;&#22768;&#31361;&#30772;&#26354;&#32447;&#35266;&#27979;&#32467;&#26524;&#65292;&#24471;&#20986;&#20102;&#25311;&#21512;&#33391;&#22909;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#25104;&#21151;&#22320;&#21033;&#29992;&#31232;&#30095;&#21644;&#31526;&#21495;&#22238;&#24402;&#37325;&#24314;&#20102;&#21560;&#38468;&#25668;&#21462;&#21160;&#21147;&#23398;&#65292;&#24182;&#21033;&#29992;&#30830;&#23450;&#30340;&#22810;&#39033;&#24335;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#31361;&#30772;&#26354;&#32447;&#65292;&#31361;&#26174;&#20102;&#35813;&#26694;&#26550;&#21457;&#29616;&#21560;&#38468;&#21160;&#21147;&#23398;&#23450;&#24459;&#32467;&#26500;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a systematic machine learning approach for creating efficient hybrid models and discovering sorption uptake models in non-linear advection-diffusion-sorption systems. It demonstrates an effective method to train these complex systems using gradientbased optimizers, adjoint sensitivity analysis, and JIT-compiled vector Jacobian products, combined with spatial discretization and adaptive integrators. Sparse and symbolic regression were employed to identify missing functions in the artificial neural network. The robustness of the proposed method was tested on an in-silico data set of noisy breakthrough curve observations of fixed-bed adsorption, resulting in a well-fitted hybrid model. The study successfully reconstructed sorption uptake kinetics using sparse and symbolic regression, and accurately predicted breakthrough curves using identified polynomials, highlighting the potential of the proposed framework for discovering sorption kinetic law structures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#22359;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#30340;&#22312;&#32447;&#22260;&#26827;&#28216;&#25103;&#31995;&#32479; CH-Go&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.13553</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#22359;&#25968;&#25454;&#23384;&#20648;&#30340;&#22312;&#32447;&#22260;&#26827;&#31995;&#32479;CH-Go
&lt;/p&gt;
&lt;p&gt;
CH-Go: Online Go System Based on Chunk Data Storage. (arXiv:2303.13553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#22359;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#30340;&#22312;&#32447;&#22260;&#26827;&#28216;&#25103;&#31995;&#32479; CH-Go&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#26469;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#65292;&#22914;&#21021;&#22987;&#22260;&#26827;&#28216;&#25103;&#35760;&#24405;&#65292;&#34920;&#31034;&#23398;&#20064;&#33719;&#24471;&#30340;&#29305;&#24449;&#25968;&#25454;&#38598;&#65292;&#33258;&#25105;&#23545;&#24328;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#65292;&#38543;&#26426;&#37319;&#26679;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#31561;&#65292;&#26159;&#23454;&#29616;&#22312;&#32447;&#22260;&#26827;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36816;&#34892;&#25152;&#24517;&#38656;&#30340;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24456;&#23569;&#25552;&#21040;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#20915;&#23450;&#20102;&#22260;&#26827;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#22359;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#30340;&#22312;&#32447;&#22260;&#26827;&#28216;&#25103;&#31995;&#32479; (CH-Go)&#65292;&#23427;&#22788;&#29702;&#20102;Kiseido Go Server (KGS)&#21457;&#24067;&#30340;160k&#26684;&#24335;&#30340;&#22260;&#26827;&#28216;&#25103;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;11&#20010;&#24179;&#38754;&#30340;&#22260;&#26827;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#24182;&#34892;&#22788;&#29702;&#22120;&#21644;&#19968;&#20010;&#29983;&#25104;&#22120;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20869;&#23384;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25226;&#25968;&#25454;&#23384;&#20648;&#22312;&#22359;&#20013;&#65292;&#20197;1024&#20026;&#25209;&#22788;&#29702;&#22823;&#23567;&#65292;&#24182;&#23558;&#27599;&#20010;&#22359;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#20445;&#23384;&#20026;&#20108;&#36827;&#21046;&#25991;&#20214;&#12290;&#28982;&#21518;&#27599;&#27425;&#38543;&#26426;&#25277;&#21462;&#19968;&#23567;&#32452;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training and running of an online Go system require the support of effective data management systems to deal with vast data, such as the initial Go game records, the feature data set obtained by representation learning, the experience data set of self-play, the randomly sampled Monte Carlo tree, and so on. Previous work has rarely mentioned this problem, but the ability and efficiency of data management systems determine the accuracy and speed of the Go system. To tackle this issue, we propose an online Go game system based on the chunk data storage method (CH-Go), which processes the format of 160k Go game data released by Kiseido Go Server (KGS) and designs a Go encoder with 11 planes, a parallel processor and generator for better memory performance. Specifically, we store the data in chunks, take the chunk size of 1024 as a batch, and save the features and labels of each chunk as binary files. Then a small set of data is randomly sampled each time for the neural network training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DaToBS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#29031;&#29255;&#20013;&#26816;&#27979;&#21644;&#36716;&#24405;Tifinagh&#23383;&#31526;&#65292;&#20197;&#25552;&#39640;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#38463;&#39532;&#40784;&#35821;&#22312;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#32593;&#32476;&#24212;&#29992;&#31561;&#26041;&#38754;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.13549</link><description>&lt;p&gt;
&#20174;&#20302;&#36164;&#28304;&#35821;&#35328;&#38463;&#39532;&#40784;&#35821;&#30340;&#22270;&#20687;&#20013;&#36827;&#34892;Tifinagh&#23383;&#31526;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#21644;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh. (arXiv:2303.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DaToBS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#29031;&#29255;&#20013;&#26816;&#27979;&#21644;&#36716;&#24405;Tifinagh&#23383;&#31526;&#65292;&#20197;&#25552;&#39640;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#38463;&#39532;&#40784;&#35821;&#22312;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#32593;&#32476;&#24212;&#29992;&#31561;&#26041;&#38754;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26575;&#26575;&#23572;&#35821;&#26159;&#19968;&#31181;&#20302;&#36164;&#28304;&#30340;&#21271;&#38750;&#22303;&#35821;&#65292;&#22312;&#25705;&#27931;&#21733;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#31561;&#22320;&#30340;&#26575;&#26575;&#23572;&#31038;&#21306;&#20013;&#20351;&#29992;&#33258;&#24049;&#29420;&#29305;&#30340;&#23383;&#27597;&#34920;Tifinagh&#12290;&#36825;&#31181;&#38750;&#27954;&#20122;&#32454;&#20122;&#35821;&#35328;&#26159;&#30001;1400&#19975;&#20154;&#20351;&#29992;&#30340;&#65292;&#20294;&#32570;&#20047;&#36275;&#22815;&#30340;&#25945;&#32946;&#12289;&#30740;&#31350;&#12289;&#32593;&#32476;&#24212;&#29992;&#31561;&#26041;&#38754;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;DaToBS&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#36716;&#24405;Berber&#23383;&#31526;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#33258;&#28982;&#29615;&#22659;&#30340;&#29031;&#29255;&#20013;&#33258;&#21160;&#35782;&#21035;&#21644;&#36716;&#24405;Tifinagh&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Berber, or Amazigh language family is a low-resource North African vernacular language spoken by the indigenous Berber ethnic group. It has its own unique alphabet called Tifinagh used across Berber communities in Morocco, Algeria, and others. The Afroasiatic language Berber is spoken by 14 million people, yet lacks adequate representation in education, research, web applications etc. For instance, there is no option of translation to or from Amazigh / Berber on Google Translate, which hosts over 100 languages today. Consequently, we do not find specialized educational apps, L2 (2nd language learner) acquisition, automated language translation, and remote-access facilities enabled in Berber. Motivated by this background, we propose a supervised approach called DaToBS for Detection and Transcription of Berber Signs. The DaToBS approach entails the automatic recognition and transcription of Tifinagh characters from signs in photographs of natural environments. This is achieved by sel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28436;&#31034;&#20102;&#26234;&#33021;&#20010;&#20154;&#21161;&#25163;Dona&#65292;&#29992;&#20110;&#23398;&#29983;&#36873;&#35838;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#37319;&#29992;&#35821;&#38899;&#36755;&#20837;&#12289;&#20219;&#21153;&#35268;&#21010;&#20248;&#21270;&#21644;&#35821;&#35328;&#32763;&#35793;&#31561;&#25216;&#26415;&#65292;&#20351;&#23398;&#29983;&#19981;&#38656;&#35201;&#33258;&#24049;&#23436;&#25104;&#22797;&#26434;&#30340;&#36873;&#35838;&#34920;&#26684;&#12290;</title><link>http://arxiv.org/abs/2303.13548</link><description>&lt;p&gt;
&#22079;&#65292;Dona&#65281;&#20320;&#33021;&#24110;&#25105;&#22788;&#29702;&#23398;&#29983;&#36873;&#35838;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Hey Dona! Can you help me with student course registration?. (arXiv:2303.13548v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28436;&#31034;&#20102;&#26234;&#33021;&#20010;&#20154;&#21161;&#25163;Dona&#65292;&#29992;&#20110;&#23398;&#29983;&#36873;&#35838;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#37319;&#29992;&#35821;&#38899;&#36755;&#20837;&#12289;&#20219;&#21153;&#35268;&#21010;&#20248;&#21270;&#21644;&#35821;&#35328;&#32763;&#35793;&#31561;&#25216;&#26415;&#65292;&#20351;&#23398;&#29983;&#19981;&#38656;&#35201;&#33258;&#24049;&#23436;&#25104;&#22797;&#26434;&#30340;&#36873;&#35838;&#34920;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28436;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;Hey Dona&#65288;&#25110;&#20165;&#31216;&#20026;Dona&#65289;&#30340;&#26234;&#33021;&#20010;&#20154;&#21161;&#25163;&#65292;&#23427;&#20855;&#26377;&#34394;&#25311;&#35821;&#38899;&#21161;&#25163;&#65292;&#29992;&#20110;&#23398;&#29983;&#36873;&#35838;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#24212;&#29992;&#20110;&#25945;&#32946;&#39046;&#22495;&#30340;&#39033;&#30446;&#12290;Hey Dona &#36866;&#29992;&#20110;&#21508;&#31181;&#21475;&#38899;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#20248;&#21270;&#21644;&#35821;&#35328;&#32763;&#35793;&#65292;&#25509;&#21463;&#21487;&#20197;&#36890;&#36807;&#40614;&#20811;&#39118;&#65288;&#34013;&#29273;&#12289;&#26377;&#32447;&#40614;&#20811;&#39118;&#65289;&#30340;&#35821;&#38899;&#36755;&#20837;&#65292;&#24182;&#25353;&#29031;&#29992;&#25143;&#21629;&#20196;&#25191;&#34892;&#26597;&#35810;&#22788;&#29702;&#65292;&#36830;&#25509;&#32593;&#32476;&#25628;&#32034;&#22238;&#31572;&#65292;&#24314;&#31435;&#20219;&#21153;&#20381;&#36182;&#20851;&#31995;&#65292;&#30830;&#20445;&#25104;&#21151;&#27880;&#20876;&#12290;Dona &#36991;&#20813;&#20102;&#23398;&#29983;&#33258;&#24049;&#36755;&#20837;&#12289;&#28857;&#20987;&#21644;&#27983;&#35272;&#22797;&#26434;&#30340;&#36873;&#35838;&#34920;&#26684;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a demo of an intelligent personal agent called Hey Dona (or just Dona) with virtual voice assistance in student course registration. It is a deployed project in the theme of AI for education. In this digital age with a myriad of smart devices, users often delegate tasks to agents. While pointing and clicking supersedes the erstwhile command-typing, modern devices allow users to speak commands for agents to execute tasks, enhancing speed and convenience. In line with this progress, Dona is an intelligent agent catering to student needs by automated, voice-operated course registration, spanning a multitude of accents, entailing task planning optimization, with some language translation as needed. Dona accepts voice input by microphone (Bluetooth, wired microphone), converts human voice to computer understandable language, performs query processing as per user commands, connects with the Web to search for answers, models task dependencies, imbibes quality control
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#23376;&#22270;&#29109;&#22270;&#26680;&#65292;&#23427;&#20855;&#26377;&#36739;&#22909;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#35780;&#20272;&#24615;&#33021;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#35268;&#21010;&#23376;&#22270;&#26522;&#20030;&#31639;&#27861;&#26377;&#25928;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26631;&#35760;&#23376;&#22270;&#26469;&#20016;&#23500;&#23376;&#32467;&#26500;&#25299;&#25169;&#65292;&#29992;&#20840;&#23616;&#22270;&#29109;&#26469;&#25551;&#36848;&#32593;&#32476;&#65292;&#24050;&#24212;&#29992;&#20110;&#22810;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#24182;&#33021;&#23545;&#19981;&#21516;&#20219;&#21153;&#20135;&#29983;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.13543</link><description>&lt;p&gt;
&#26631;&#35760;&#23376;&#22270;&#29109;&#26680;
&lt;/p&gt;
&lt;p&gt;
Labeled Subgraph Entropy Kernel. (arXiv:2303.13543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#23376;&#22270;&#29109;&#22270;&#26680;&#65292;&#23427;&#20855;&#26377;&#36739;&#22909;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#35780;&#20272;&#24615;&#33021;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#35268;&#21010;&#23376;&#22270;&#26522;&#20030;&#31639;&#27861;&#26377;&#25928;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26631;&#35760;&#23376;&#22270;&#26469;&#20016;&#23500;&#23376;&#32467;&#26500;&#25299;&#25169;&#65292;&#29992;&#20840;&#23616;&#22270;&#29109;&#26469;&#25551;&#36848;&#32593;&#32476;&#65292;&#24050;&#24212;&#29992;&#20110;&#22810;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#24182;&#33021;&#23545;&#19981;&#21516;&#20219;&#21153;&#20135;&#29983;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26680;&#26041;&#27861;&#22312;&#30456;&#20284;&#24615;&#27979;&#37327;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22270;&#26680;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#21270;&#23398;&#21644;&#37329;&#34701;&#25968;&#25454;&#20998;&#26512;&#31561;&#39046;&#22495;&#37117;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#29109;&#30340;&#22270;&#26680;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#22823;&#21644;&#24573;&#30053;&#33410;&#28857;&#32423;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#23376;&#22270;&#29109;&#22270;&#26680;&#65292;&#22312;&#32467;&#26500;&#30456;&#20284;&#24615;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#23376;&#22270;&#26522;&#20030;&#31639;&#27861;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26631;&#35760;&#23376;&#22270;&#65292;&#23427;&#29992;&#35821;&#20041;&#20449;&#24687;&#20016;&#23500;&#20102;&#23376;&#32467;&#26500;&#25299;&#25169;&#12290;&#31867;&#27604;&#20110;&#32479;&#35745;&#21147;&#23398;&#20013;&#27668;&#22242;&#31751;&#25193;&#23637;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#37325;&#26032;&#25512;&#23548;&#20102;&#37197;&#20998;&#20989;&#25968;&#65292;&#24182;&#35745;&#31639;&#20840;&#23616;&#22270;&#29109;&#26469;&#25551;&#36848;&#32593;&#32476;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, kernel methods are widespread in tasks of similarity measuring. Specifically, graph kernels are widely used in fields of bioinformatics, chemistry and financial data analysis. However, existing methods, especially entropy based graph kernels are subject to large computational complexity and the negligence of node-level information. In this paper, we propose a novel labeled subgraph entropy graph kernel, which performs well in structural similarity assessment. We design a dynamic programming subgraph enumeration algorithm, which effectively reduces the time complexity. Specially, we propose labeled subgraph, which enriches substructure topology with semantic information. Analogizing the cluster expansion process of gas cluster in statistical mechanics, we re-derive the partition function and calculate the global graph entropy to characterize the network. In order to test our method, we apply several real-world datasets and assess the effects in different tasks. To captu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26816;&#27979;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#24182;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.13540</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;: &#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20419;&#36827;&#21487;&#25345;&#32493;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision. (arXiv:2303.13540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26816;&#27979;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#24182;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#23454;&#29616;&#28165;&#27905;&#29983;&#20135;&#21644;&#21487;&#25345;&#32493;&#24615;&#30446;&#30340;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#23588;&#20854;&#26159;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26469;&#35782;&#21035;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#65292;&#24182;&#23558;&#36825;&#20123;&#32467;&#26524;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25104;&#26524;&#39044;&#35745;&#23558;&#20419;&#36827;&#20135;&#21697;&#20351;&#29992;&#30340;&#25913;&#36827;&#21644;&#30740;&#21457;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#20135;&#21697;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;:&#21152;&#24037;&#24037;&#20855;&#21644;&#26059;&#36716;X&#23556;&#32447;&#38451;&#26497;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usage and impact of deep learning for cleaner production and sustainability purposes remain little explored. This work shows how deep learning can be harnessed to increase sustainability in production and product usage. Specifically, we utilize deep learning-based computer vision to determine the wear states of products. The resulting insights serve as a basis for novel product-service systems with improved integration and result orientation. Moreover, these insights are expected to facilitate product usage improvements and R&amp;D innovations. We demonstrate our approach on two products: machining tools and rotating X-ray anodes. From a technical standpoint, we show that it is possible to recognize the wear state of these products using deep-learning-based computer vision. In particular, we detect wear through microscopic images of the two products. We utilize a U-Net for semantic segmentation to detect wear based on pixel granularity. The resulting mean dice coefficients of 0.631 and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#36830;&#32493;&#31354;&#38388;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#31574;&#30053;&#26356;&#26032;&#25509;&#36817;&#26368;&#20248;&#65292;&#21516;&#26102;&#25512;&#23548;&#20102;&#19968;&#33324;&#31867;&#31639;&#27861;&#30340;&#20840;&#23616;&#31574;&#30053;&#26356;&#26032;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.13539</link><description>&lt;p&gt;
&#36830;&#32493;&#31354;&#38388;&#38543;&#26426;&#21338;&#24328;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Agent Reinforcement Learning for Continuous-Space Stochastic Games. (arXiv:2303.13539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#36830;&#32493;&#31354;&#38388;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#31574;&#30053;&#26356;&#26032;&#25509;&#36817;&#26368;&#20248;&#65292;&#21516;&#26102;&#25512;&#23548;&#20102;&#19968;&#33324;&#31867;&#31639;&#27861;&#30340;&#20840;&#23616;&#31574;&#30053;&#26356;&#26032;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21338;&#24328;&#26159;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#38480;&#29366;&#24577;&#30340;&#28216;&#25103;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#24102;&#26377;&#36890;&#29992;&#29366;&#24577;&#31354;&#38388;&#21644;&#20449;&#24687;&#32467;&#26500;&#30340;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31574;&#30053;&#26356;&#26032;&#30340;&#36817;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#22522;&#20110;&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#33324;&#31867;&#31639;&#27861;&#30340;&#20840;&#23616;&#31574;&#30053;&#26356;&#26032;&#21160;&#24577;&#65292;&#24182;&#25512;&#23548;&#20102;&#32852;&#21512;&#31574;&#30053;&#31354;&#38388;&#20869;&#25910;&#25947;&#27010;&#29575;&#30340;&#38381;&#24335;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic games are a popular framework for studying multi-agent reinforcement learning (MARL). Recent advances in MARL have focused primarily on games with finitely many states. In this work, we study multi-agent learning in stochastic games with general state spaces and an information structure in which agents do not observe each other's actions. In this context, we propose a decentralized MARL algorithm and we prove the near-optimality of its policy updates. Furthermore, we study the global policy-updating dynamics for a general class of best-reply based algorithms and derive a closed-form characterization of convergence probabilities over the joint policy space.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25429;&#33719;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#21830;&#29992;&#35774;&#22791;&#30495;&#23454;&#23556;&#39057;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#35782;&#21035;&#38750;&#27861;&#25110;&#26410;&#25480;&#26435;&#21457;&#23556;&#22120;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.13538</link><description>&lt;p&gt;
&#29992;&#20110;&#21830;&#29992;&#35774;&#22791;&#30495;&#23454;&#23556;&#39057;&#25351;&#32441;&#30340;&#34013;&#29273;&#21644;WiFi&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices. (arXiv:2303.13538v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25429;&#33719;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#21830;&#29992;&#35774;&#22791;&#30495;&#23454;&#23556;&#39057;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#35782;&#21035;&#38750;&#27861;&#25110;&#26410;&#25480;&#26435;&#21457;&#23556;&#22120;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#25351;&#32441;&#20316;&#20026;&#19968;&#31181;&#29289;&#29702;&#23618;&#23433;&#20840;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#22312;&#20849;&#20139;RF&#39057;&#35889;&#30340;&#38750;&#27861;&#25110;&#26410;&#25480;&#26435;&#21457;&#23556;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#30005;&#65288;SDR&#65289;&#29983;&#25104;&#21512;&#25104;&#27874;&#24418;&#65292;&#36825;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#20165;&#20851;&#27880;&#29983;&#25104;&#19968;&#31181;&#27874;&#24418;&#30340;&#33455;&#29255;&#32452;&#12290;&#21830;&#29992;&#29616;&#25104;&#65288;COTS&#65289;&#32452;&#21512;&#33455;&#29255;&#32452;&#25903;&#25345;&#20351;&#29992;&#20849;&#20139;&#21452;&#39057;&#27573;&#22825;&#32447;&#30340;&#20004;&#31181;&#26080;&#32447;&#26631;&#20934;&#65288;&#20363;&#22914;WiFi&#21644;&#34013;&#29273;&#65289;&#65292;&#22914;&#22312;&#31508;&#35760;&#26412;&#30005;&#33041;&#12289;&#36866;&#37197;&#22120;&#12289;&#26080;&#32447;&#20805;&#30005;&#22120;&#12289;&#26641;&#33683;&#27966;&#31561;IoT&#35774;&#22791;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#36319;&#19978;&#29616;&#20195;IoT&#29615;&#22659;&#30340;&#27493;&#20240;&#65292;&#36843;&#20999;&#38656;&#35201;&#25429;&#33719;&#36825;&#20123;&#32452;&#21512;&#33455;&#29255;&#32452;&#21457;&#23556;&#24322;&#26500;&#36890;&#20449;&#21327;&#35758;&#30340;&#30495;&#23454;&#19990;&#30028;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25429;&#33719;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#27492;&#31867;&#20844;&#24320;&#21487;&#35775;&#38382;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#21830;&#19994;&#35774;&#22791;&#30340;COTS Wi-Fi&#21644;&#34013;&#29273;&#32452;&#21512;&#33455;&#29255;&#32452;&#30340;RF&#21457;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
RF fingerprinting is emerging as a physical layer security scheme to identify illegitimate and/or unauthorized emitters sharing the RF spectrum. However, due to the lack of publicly accessible real-world datasets, most research focuses on generating synthetic waveforms with software-defined radios (SDRs) which are not suited for practical deployment settings. On other hand, the limited datasets that are available focus only on chipsets that generate only one kind of waveform. Commercial off-the-shelf (COTS) combo chipsets that support two wireless standards (for example WiFi and Bluetooth) over a shared dual-band antenna such as those found in laptops, adapters, wireless chargers, Raspberry Pis, among others are becoming ubiquitous in the IoT realm. Hence, to keep up with the modern IoT environment, there is a pressing need for real-world open datasets capturing emissions from these combo chipsets transmitting heterogeneous communication protocols. To this end, we capture the first kno
&lt;/p&gt;</description></item><item><title>PBSHM&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#32467;&#26500;&#32676;&#20307;&#36827;&#34892;&#30417;&#27979;&#65292;&#23558;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#22312;&#32467;&#26500;&#23454;&#20363;&#20043;&#38388;&#20256;&#36882;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.13533</link><description>&lt;p&gt;
&#22522;&#20110;&#39118;&#38505;&#30340;&#32676;&#20307;&#32467;&#26500;&#20581;&#24247;&#30417;&#35270;&#29702;&#35770;&#65306;&#20197;&#23618;&#27425;&#31995;&#32479;&#20013;&#30340;&#20154;&#21475;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Towards risk-informed PBSHM: Populations as hierarchical systems. (arXiv:2303.13533v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13533
&lt;/p&gt;
&lt;p&gt;
PBSHM&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#32467;&#26500;&#32676;&#20307;&#36827;&#34892;&#30417;&#27979;&#65292;&#23558;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#22312;&#32467;&#26500;&#23454;&#20363;&#20043;&#38388;&#20256;&#36882;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#20013;&#65292;&#26377;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#39118;&#38505;&#30340;&#20915;&#31574;&#26694;&#26550;&#24050;&#32463;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23398;&#20064;&#20915;&#31574;&#25152;&#38656;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#38656;&#35201;&#20174;&#24863;&#20852;&#36259;&#30340;&#32467;&#26500;&#33719;&#24471;&#27979;&#37327;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23569;&#22312;&#24517;&#35201;&#30340;&#29615;&#22659;&#21644;&#25805;&#20316;&#26465;&#20214;&#19979;&#28085;&#30422;&#36275;&#22815;&#30340;&#33539;&#22260;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#30340;&#33391;&#22909;&#27867;&#21270;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21487;&#20197;&#23558;SHM&#25193;&#23637;&#21040;&#32467;&#26500;&#32676;&#20307;&#30340;&#25216;&#26415;&#65292;&#20197;&#20415;&#22312;&#36275;&#22815;&#30456;&#20284;&#30340;&#32467;&#26500;&#23454;&#20363;&#20043;&#38388;&#20256;&#36882;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#26032;&#26041;&#27861;&#34987;&#31216;&#20026;&#22522;&#20110;&#32676;&#20307;&#30340;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(PBSHM)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20154;&#32676;&#32467;&#26500;&#30340;&#27491;&#24335;&#34920;&#36798;&#65292;&#20197;&#36827;&#34892;&#22522;&#20110;&#39118;&#38505;&#30340;&#20915;&#31574;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prospect of informed and optimal decision-making regarding the operation and maintenance (O&amp;M) of structures provides impetus to the development of structural health monitoring (SHM) systems. A probabilistic risk-based framework for decision-making has already been proposed. However, in order to learn the statistical models necessary for decision-making, measured data from the structure of interest are required. Unfortunately, these data are seldom available across the range of environmental and operational conditions necessary to ensure good generalisation of the model.  Recently, technologies have been developed that overcome this challenge, by extending SHM to populations of structures, such that valuable knowledge may be transferred between instances of structures that are sufficiently similar. This new approach is termed population-based structural heath monitoring (PBSHM).  The current paper presents a formal representation of populations of structures, such that risk-based d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#35299;&#20026;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;&#22122;&#22768;&#19977;&#20010;&#37096;&#20998;&#65292;&#24182;&#20998;&#21035;&#39044;&#27979;&#23792;&#20540;&#27969;&#37327;&#65292;&#25552;&#39640;&#20102;&#23792;&#20540;&#32593;&#32476;&#27969;&#37327;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.13529</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25552;&#39640;&#23792;&#20540;&#32593;&#32476;&#27969;&#37327;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Peak Network Traffic Prediction via Time-Series Decomposition. (arXiv:2303.13529v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#35299;&#20026;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;&#22122;&#22768;&#19977;&#20010;&#37096;&#20998;&#65292;&#24182;&#20998;&#21035;&#39044;&#27979;&#23792;&#20540;&#27969;&#37327;&#65292;&#25552;&#39640;&#20102;&#23792;&#20540;&#32593;&#32476;&#27969;&#37327;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32593;&#32476;&#31649;&#29702;&#21644;&#32500;&#25252;&#65292;&#39044;&#27979;&#32593;&#32476;&#23558;&#25509;&#25910;&#21040;&#30340;&#39640;&#23792;&#27969;&#37327;&#30340;&#26102;&#38388;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#20415;&#21487;&#20197;&#20026;&#26381;&#21153;&#22120;&#26381;&#21153;&#35831;&#27714;&#20998;&#37197;&#36275;&#22815;&#30340;&#36164;&#28304;&#12290;&#22914;&#26524;&#27809;&#26377;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#36275;&#22815;&#30340;&#36164;&#28304;&#65292;&#21017;&#23427;&#20204;&#21487;&#33021;&#20250;&#23481;&#26131;&#21457;&#29983;&#25925;&#38556;&#21644;&#23433;&#20840;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36890;&#24120;&#21253;&#21547;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;&#22122;&#22768;&#31561;&#19981;&#21516;&#29305;&#24449;&#65292;&#22240;&#27492;&#24120;&#35265;&#30340;&#39044;&#27979;&#27169;&#22411;&#22914;&#33258;&#22238;&#24402;&#31227;&#21160;&#24179;&#22343;&#27169;&#22411;(ARIMA)&#21482;&#33021;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19968;&#33324;&#24773;&#20917;&#65292;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23792;&#20540;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#35299;&#20026;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;&#22122;&#22768;&#19977;&#20010;&#37096;&#20998;&#65292;&#24182;&#20998;&#21035;&#39044;&#27979;&#23792;&#20540;&#27969;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#27969;&#34892;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#23792;&#20540;&#27969;&#37327;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
For network administration and maintenance, it is critical to anticipate when networks will receive peak volumes of traffic so that adequate resources can be allocated to service requests made to servers. In the event that sufficient resources are not allocated to servers, they can become prone to failure and security breaches. On the contrary, we would waste a lot of resources if we always allocate the maximum amount of resources. Therefore, anticipating peak volumes in network traffic becomes an important problem. However, popular forecasting models such as Autoregressive Integrated Moving Average (ARIMA) forecast time-series data generally, thus lack in predicting peak volumes in these time-series. More than often, a time-series is a combination of different features, which may include but are not limited to 1) Trend, the general movement of the traffic volume, 2) Seasonality, the patterns repeated over some time periods (e.g. daily and monthly), and 3) Noise, the random changes in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25506;&#35752;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#24335;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.13525</link><description>&lt;p&gt;
&#20113;&#35745;&#31639;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Workload Prediction in Cloud Computing. (arXiv:2303.13525v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25506;&#35752;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#24335;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#23545;&#20110;&#31649;&#29702;&#20113;&#25968;&#25454;&#20013;&#24515;&#24182;&#20445;&#35777;&#23458;&#25143;&#26368;&#20302;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#24314;&#27169;&#26410;&#26469;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#36136;&#37327;&#24182;&#20943;&#23569;&#30001;&#20110;&#36164;&#28304;&#36807;&#24230;&#20998;&#37197;&#32780;&#24102;&#26469;&#30340;&#28010;&#36153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#36164;&#28304;&#38656;&#27714;&#30340;&#20998;&#24067;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#24773;&#26223;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#36807;&#31243;&#26159;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#37197;&#32622;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27493;&#39588;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#25105;&#20204;&#36824;&#23558;&#21452;&#21464;&#37327;&#27169;&#22411;&#19982;&#20854;&#21333;&#21464;&#37327;&#23545;&#24212;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#29992;&#21333;&#20010;&#25110;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#24182;&#24433;&#21709;QoS&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting future resource demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing customers a minimum Quality of Service (QoS) level. Modelling the uncertainty of future demand improves the quality of the prediction and reduces the waste due to overallocation. In this paper, we propose univariate and bivariate Bayesian deep learning models to predict the distribution of future resource demand and its uncertainty. We design different training scenarios to train these models, where each procedure is a different combination of pretraining and fine-tuning steps on multiple datasets configurations. We also compare the bivariate model to its univariate counterpart training with one or more datasets to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate whether our models have transfer learning capabilities. Extensive experiments show that pretraining with multiple datasets boosts performances 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#21644;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13511</link><description>&lt;p&gt;
&#31070;&#32463;&#39044;&#35774;&#65306;&#29992;&#20110;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#30340;&#26032;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Neural Preset for Color Style Transfer. (arXiv:2303.13511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#21644;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#20266;&#24433;&#12289;&#22823;&#37327;&#20869;&#23384;&#38656;&#27714;&#21644;&#32531;&#24930;&#30340;&#39118;&#26684;&#20999;&#25442;&#36895;&#24230;&#12290;&#26412;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#26680;&#24515;&#35774;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#39068;&#33394;&#26144;&#23556;&#30697;&#38453;&#22312;&#27599;&#20010;&#20687;&#32032;&#19978;&#36827;&#34892;&#19968;&#33268;&#30340;&#25805;&#20316;&#65292;&#36991;&#20813;&#20102;&#20266;&#24433;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#20026;&#39068;&#33394;&#24402;&#19968;&#21270;&#21644;&#39118;&#26684;&#21270;&#20004;&#20010;&#38454;&#27573;&#26469;&#24320;&#21457;&#19968;&#20010;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#39068;&#33394;&#39118;&#26684;&#20316;&#20026;&#39044;&#35774;&#25552;&#21462;&#65292;&#24182;&#22312;&#24402;&#19968;&#21270;&#30340;&#36755;&#20837;&#22270;&#20687;&#19978;&#37325;&#22797;&#20351;&#29992;&#23427;&#20204;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#39118;&#26684;&#20999;&#25442;&#12290;&#30001;&#20110;&#23384;&#22312;&#25104;&#23545;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#36890;&#36807;&#33258;&#30417;&#30563;&#31574;&#30053;&#35757;&#32451;&#31070;&#32463;&#39044;&#35774;&#27169;&#22411;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#31070;&#32463;&#39044;&#35774;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21508;&#31181;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#28982;&#22320;&#25903;&#25345;&#22810;&#20010;&#39118;&#26684;&#65292;&#24182;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed. Our method is based on two core designs. First, we propose Deterministic Neural Color Mapping (DNCM) to consistently operate on each pixel via an image-adaptive color mapping matrix, avoiding artifacts and supporting high-resolution inputs with a small memory footprint. Second, we develop a two-stage pipeline by dividing the task into color normalization and stylization, which allows efficient style switching by extracting color styles as presets and reusing them on normalized input images. Due to the unavailability of pairwise datasets, we describe how to train Neural Preset via a self-supervised strategy. Various advantages of Neural Preset over existing methods are demonstrated through comprehensive evaluations. Besides, we show that our trained model can naturally support multipl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21516;&#26102;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.13113</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Regularization for Class-Incremental Learning. (arXiv:2303.13113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21516;&#26102;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#25351;&#22312;&#32500;&#25345;&#20808;&#21069;&#23398;&#20064;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#65292;&#26356;&#26032;&#20855;&#26377;&#26032;&#31867;&#21035;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#12290;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#26469;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#26159;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#25972;&#20010;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#24658;&#23450;&#30340;&#24378;&#24230;&#65292;&#21487;&#33021;&#26080;&#27861;&#21453;&#26144;&#25152;&#36935;&#21040;&#30340;&#20219;&#21153;&#38590;&#24230;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#25163;&#22836;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#30830;&#23450;&#27599;&#20010;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#20339;&#27491;&#21017;&#21270;&#24378;&#24230;&#12290;&#36890;&#36807;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#23545;&#20110;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning updates a deep classifier with new categories while maintaining the previously observed class accuracy. Regularizing the neural network weights is a common method to prevent forgetting previously learned classes while learning novel ones. However, existing regularizers use a constant magnitude throughout the learning sessions, which may not reflect the varying levels of difficulty of the tasks encountered during incremental learning. This study investigates the necessity of adaptive regularization in Class-Incremental Learning, which dynamically adjusts the regularization strength according to the complexity of the task at hand. We propose a Bayesian Optimization-based approach to automatically determine the optimal regularization magnitude for each learning task. Our experiments on two datasets via two regularizers demonstrate the importance of adaptive regularization for achieving accurate and less forgetful visual incremental learning.
&lt;/p&gt;</description></item><item><title>&#26032;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#35745;&#31639;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30028;&#65292;&#20801;&#35768;&#20351;&#29992;&#20960;&#20046;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#65292;&#33021;&#25552;&#20379;&#19982;&#20272;&#35745;&#22120;&#26080;&#20851;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;&#24050;&#24212;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21442;&#25968;&#38382;&#39064;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2303.12497</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#24230;&#37327;&#30340;&#19979;&#30028;&#36125;&#21494;&#26031;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Lower Bound on the Bayesian Risk via Information Measure. (arXiv:2303.12497v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12497
&lt;/p&gt;
&lt;p&gt;
&#26032;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#35745;&#31639;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30028;&#65292;&#20801;&#35768;&#20351;&#29992;&#20960;&#20046;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#65292;&#33021;&#25552;&#20379;&#19982;&#20272;&#35745;&#22120;&#26080;&#20851;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;&#24050;&#24212;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21442;&#25968;&#38382;&#39064;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21442;&#25968;&#20272;&#35745;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30028;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#20960;&#20046;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#65292;&#21253;&#25324;R&#233;nyi&#30340;&#945;&#65292;&#966;-&#20998;&#27495;&#21644;Sibson&#30340;&#945;-&#20114;&#20449;&#24687;&#12290;&#35813; &#26041;&#27861;&#23558;&#20998;&#27495;&#35270;&#20026;&#24230;&#37327;&#30340;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#24230;&#37327;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#19981;&#31561;&#24335;&#23545;&#20854;&#23545;&#20598;&#36827;&#34892;&#19978;&#30028;&#38480;&#21046;&#65292;&#23601;&#21487;&#20197;&#29992;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#35745;&#31639;&#39118;&#38505;&#30340;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#20998;&#27495;&#28385;&#36275;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#19982;&#20272;&#35745;&#22120;&#26080;&#20851;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#28041;&#21450;&#31163;&#25955;&#21644;&#36830;&#32493;&#21442;&#25968;&#30340;&#26377;&#36259;&#38382;&#39064;&#65292;&#21253;&#25324;&#8220;&#25417;&#36855;&#34255;&#8221;&#38382;&#39064;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#37325;&#35201;&#30340;&#35266;&#23519;&#26159;&#19979;&#30028;&#22312;&#26679;&#26412;&#25968;&#19978;&#30340;&#34892;&#20026;&#21463;&#21040;t&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on parameter estimation and introduces a new method for lower bounding the Bayesian risk. The method allows for the use of virtually \emph{any} information measure, including R\'enyi's $\alpha$, $\varphi$-Divergences, and Sibson's $\alpha$-Mutual Information. The approach considers divergences as functionals of measures and exploits the duality between spaces of measures and spaces of functions. In particular, we show that one can lower bound the risk with any information measure by upper bounding its dual via Markov's inequality. We are thus able to provide estimator-independent impossibility results thanks to the Data-Processing Inequalities that divergences satisfy. The results are then applied to settings of interest involving both discrete and continuous parameters, including the ``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An important observation is that the behaviour of the lower bound in the number of samples is influenced by t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20174;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#31354;&#38388;&#32676;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#21462;&#24471;&#20102;&#27604;&#30452;&#25509;&#22312;ICSD&#26230;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11699</link><description>&lt;p&gt;
&#22312;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms. (arXiv:2303.11699v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20174;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#31354;&#38388;&#32676;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#21462;&#24471;&#20102;&#27604;&#30452;&#25509;&#22312;ICSD&#26230;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#26230;&#20307;&#31354;&#38388;&#32676;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#22312;ICSD&#31561;&#25968;&#25454;&#24211;&#30340;&#27169;&#25311;&#34893;&#23556;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#20854;&#35268;&#27169;&#26377;&#38480;&#12289;&#31867;&#21035;&#19981;&#22343;&#21248;&#24182;&#19988;&#20559;&#21521;&#26576;&#20123;&#32467;&#26500;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#27599;&#20010;&#31354;&#38388;&#32676;&#30340;&#23545;&#31216;&#25805;&#20316;&#65292;&#22312;&#38543;&#26426;&#22352;&#26631;&#19979;&#29983;&#25104;&#21512;&#25104;&#26230;&#20307;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;ResNet&#31867;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#65292;&#27599;&#23567;&#26102;&#26368;&#22810;&#21487;&#29983;&#25104;&#23569;&#37327;&#30334;&#19975;&#20010;&#21807;&#19968;&#30340;&#21512;&#25104;&#34893;&#23556;&#22270;&#12290;&#38024;&#23545;&#25105;&#20204;&#36873;&#25321;&#30340;&#31354;&#38388;&#32676;&#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#22823;&#22810;&#25968;&#31354;&#38388;&#32676;&#30340;&#26410;&#35265;ICSD&#32467;&#26500;&#31867;&#22411;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;79.9%&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#36825;&#36229;&#36807;&#20102;&#30452;&#25509;&#22312;ICSD&#26230;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;56.1%&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques have successfully been used to extract structural information such as the crystal space group from powder X-ray diffractograms. However, training directly on simulated diffractograms from databases such as the ICSD is challenging due to its limited size, class-inhomogeneity, and bias toward certain structure types. We propose an alternative approach of generating synthetic crystals with random coordinates by using the symmetry operations of each space group. Based on this approach, we demonstrate online training of deep ResNet-like models on up to a few million unique on-the-fly generated synthetic diffractograms per hour. For our chosen task of space group classification, we achieved a test accuracy of 79.9% on unseen ICSD structure types from most space groups. This surpasses the 56.1% accuracy of the current state-of-the-art approach of training on ICSD crystals directly. Our results demonstrate that synthetically generated crystals can be used to extract
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#29699;&#20307;&#19978;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20351;&#29992;SGD&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11602</link><description>&lt;p&gt;
&#24102;&#24212;&#29992;&#20110;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#30340;&#21442;&#25968;&#21270;&#29699;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation. (arXiv:2303.11602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#29699;&#20307;&#19978;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20351;&#29992;SGD&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#30001;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20026;&#24120;&#25968;&#20493;&#30340;&#39640;&#32500;&#29699;&#19978;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31867;&#22411;&#31639;&#27861;&#12290;&#25105;&#20204;&#20026;&#26377;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#39318;&#27425;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#35774;&#32622;&#30340;&#25910;&#25947;&#35777;&#26126;&#65292;&#35813;&#35774;&#32622;&#23545;&#24212;&#20110;&#37327;&#23376;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#65288;VMC&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze stochastic gradient descent (SGD) type algorithms on a high-dimensional sphere which is parameterized by a neural network up to a normalization constant. We provide a new algorithm for the setting of supervised learning and show its convergence both theoretically and numerically. We also provide the first proof of convergence for the unsupervised setting, which corresponds to the widely used variational Monte Carlo (VMC) method in quantum physics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#37319;&#29992;&#22686;&#24378;&#25311;&#21512;&#33021;&#21147;&#65292;&#36880;&#28176;&#22686;&#21152;&#40065;&#26834;&#24615;&#30340;&#26435;&#37325;&#26469;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#12290;&#22312;&#21518;&#26399;&#38454;&#27573;&#65292;&#24341;&#20837;&#33258;&#20030;&#39033;&#65292;&#35753;DNN&#26356;&#21152;&#37325;&#35270;&#23481;&#26131;&#30340;&#26679;&#20363;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11562</link><description>&lt;p&gt;
&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic-Aware Loss for Learning with Label Noise. (arXiv:2303.11562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#37319;&#29992;&#22686;&#24378;&#25311;&#21512;&#33021;&#21147;&#65292;&#36880;&#28176;&#22686;&#21152;&#40065;&#26834;&#24615;&#30340;&#26435;&#37325;&#26469;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#12290;&#22312;&#21518;&#26399;&#38454;&#27573;&#65292;&#24341;&#20837;&#33258;&#20030;&#39033;&#65292;&#35753;DNN&#26356;&#21152;&#37325;&#35270;&#23481;&#26131;&#30340;&#26679;&#20363;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#20351;&#29992;&#26082;&#33021;&#25311;&#21512;&#21448;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24378;&#20581;&#25439;&#22833;&#20989;&#25968;&#26159;&#22788;&#29702;&#27492;&#38382;&#39064;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#22240;&#32032;&#20043;&#38388;&#30340;&#38745;&#24577;&#26435;&#34913;&#19982;DNN&#23398;&#20064;&#26631;&#31614;&#22122;&#22768;&#30340;&#21160;&#24577;&#24615;&#30456;&#30683;&#30462;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;(DAL)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;DNN&#20542;&#21521;&#20110;&#20808;&#23398;&#20064;&#19968;&#33324;&#21270;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#36880;&#28176;&#36807;&#25311;&#21512;&#26631;&#31614;&#22122;&#22768;&#65292;DAL&#26368;&#21021;&#22686;&#24378;&#20102;&#25311;&#21512;&#33021;&#21147;&#65292;&#28982;&#21518;&#36880;&#28176;&#22686;&#21152;&#20102;&#40065;&#26834;&#24615;&#30340;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#22312;&#21518;&#26399;&#38454;&#27573;&#65292;&#25105;&#20204;&#35753;DNN&#26356;&#21152;&#37325;&#35270;&#23481;&#26131;&#30340;&#26679;&#20363;&#65292;&#36825;&#20123;&#26679;&#20363;&#26356;&#23481;&#26131;&#26631;&#35760;&#20026;&#27491;&#30830;&#30340;&#26631;&#31614;&#65292;&#24182;&#24341;&#20837;&#33258;&#20030;&#39033;&#26469;&#36827;&#19968;&#27493;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss function which reconciles fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamic nature of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn generalized patterns, then gradually overfit label noise, DAL strengthens the fitting ability initially, then gradually increases the weight of robustness. Moreover, at the later stage, we let DNNs put more emphasis on easy examples which are more likely to be correctly labeled than hard ones and introduce a bootstrapping term to further reduce the negative impact of label noise. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#33021;&#28304;&#39640;&#25928;&#24615;&#21644;&#23454;&#26102;&#24615;&#30340;&#31038;&#20132;&#27675;&#22260;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;ERSAM&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#36866;&#21512;SAM&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#28385;&#36275;&#33021;&#28304;&#25928;&#29575;&#12289;&#23454;&#26102;&#22788;&#29702;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#35201;&#27714;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.10727</link><description>&lt;p&gt;
ERSAM: &#38754;&#21521;&#33021;&#28304;&#39640;&#25928;&#21644;&#23454;&#26102;&#31038;&#20132;&#27675;&#22260;&#27979;&#37327;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time Social Ambiance Measurement. (arXiv:2303.10727v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#33021;&#28304;&#39640;&#25928;&#24615;&#21644;&#23454;&#26102;&#24615;&#30340;&#31038;&#20132;&#27675;&#22260;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;ERSAM&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#36866;&#21512;SAM&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#28385;&#36275;&#33021;&#28304;&#25928;&#29575;&#12289;&#23454;&#26102;&#22788;&#29702;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#35201;&#27714;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#27675;&#22260;&#25551;&#36848;&#20102;&#31038;&#20132;&#20114;&#21160;&#21457;&#29983;&#30340;&#32972;&#26223;&#65292;&#21487;&#20197;&#20351;&#29992;&#35821;&#38899;&#38899;&#39057;&#36890;&#36807;&#35745;&#31639;&#21516;&#26102;&#21457;&#35328;&#32773;&#30340;&#25968;&#37327;&#26469;&#27979;&#37327;&#12290;&#36825;&#31181;&#27979;&#37327;&#24050;&#32463;&#23454;&#29616;&#20102;&#21508;&#31181;&#24515;&#29702;&#20581;&#24247;&#36319;&#36394;&#21644;&#38754;&#21521;&#20154;&#31867;&#30340;&#29289;&#32852;&#32593;&#24212;&#29992;&#12290;&#34429;&#28982;&#35774;&#22791;&#19978;&#30340;&#31038;&#20132;&#27675;&#22260;&#27979;&#37327; (SAM) &#38750;&#24120;&#29702;&#24819;&#65292;&#20197;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#24182;&#20419;&#36827;&#19978;&#36848;&#24212;&#29992;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#39537;&#21160;&#30340;SAM&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#24120;&#35265;&#36164;&#28304;&#30456;&#30683;&#30462;&#12290;&#27492;&#22806;&#65292;&#22312;&#20020;&#24202;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#21508;&#31181;&#38544;&#31169;&#38480;&#21046;&#21644;&#25152;&#38656;&#30340;&#20154;&#21147;&#21171;&#21160;&#65292;&#21482;&#26377;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#25110;&#23454;&#38469;&#21487;&#34892;&#65292;&#36825;&#36827;&#19968;&#27493;&#25361;&#25112;&#20102;&#35774;&#22791;&#19978;SAM&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#23454;&#29616;&#20934;&#30830;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#38754;&#21521;&#33021;&#28304;&#39640;&#25928;&#21644;&#23454;&#26102;SAM&#30340;ERSAM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;ERSAM&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#36866;&#21512;SAM&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#28385;&#36275;&#33021;&#28304;&#25928;&#29575;&#12289;&#23454;&#26102;&#22788;&#29702;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;ERSAM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;SAM&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#39640;&#20102;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social ambiance describes the context in which social interactions happen, and can be measured using speech audio by counting the number of concurrent speakers. This measurement has enabled various mental health tracking and human-centric IoT applications. While on-device Socal Ambiance Measure (SAM) is highly desirable to ensure user privacy and thus facilitate wide adoption of the aforementioned applications, the required computational complexity of state-of-the-art deep neural networks (DNNs) powered SAM solutions stands at odds with the often constrained resources on mobile devices. Furthermore, only limited labeled data is available or practical when it comes to SAM under clinical settings due to various privacy constraints and the required human effort, further challenging the achievable accuracy of on-device SAM solutions. To this end, we propose a dedicated neural architecture search framework for Energy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM framework can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MSC-DBSCAN&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19977;&#20803;&#32858;&#31867;&#20013;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#19981;&#21516;&#23376;&#31354;&#38388;&#30340;&#19981;&#21516;&#20999;&#29255;&#32858;&#31867;&#65292;&#24182;&#21487;&#20197;&#33719;&#24471;&#19982; MSC &#31639;&#27861;&#22312;&#22788;&#29702;&#31209;&#19968;&#24352;&#37327;&#25968;&#25454;&#26102;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.07768</link><description>&lt;p&gt;
&#22810;&#32500;&#25968;&#32452;&#30340;&#22810;&#20999;&#29255;&#32858;&#31867;&#20013;&#30340;DBSCAN&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DBSCAN of Multi-Slice Clustering for three-order Tensor. (arXiv:2303.07768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MSC-DBSCAN&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19977;&#20803;&#32858;&#31867;&#20013;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#19981;&#21516;&#23376;&#31354;&#38388;&#30340;&#19981;&#21516;&#20999;&#29255;&#32858;&#31867;&#65292;&#24182;&#21487;&#20197;&#33719;&#24471;&#19982; MSC &#31639;&#27861;&#22312;&#22788;&#29702;&#31209;&#19968;&#24352;&#37327;&#25968;&#25454;&#26102;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19977;&#32500;&#25968;&#25454;&#30340;&#19977;&#20803;&#32858;&#31867;&#65292;&#29616;&#26377;&#30340;&#20960;&#31181;&#26041;&#27861;&#38656;&#35201;&#25351;&#23450;&#27599;&#20010;&#32500;&#24230;&#30340;&#32858;&#31867;&#22823;&#23567;&#25110;&#32858;&#31867;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19977;&#20803;&#32858;&#31867;(MSC)&#31639;&#27861;&#21487;&#20197;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#25214;&#21040;&#20445;&#30041;&#20449;&#21495;&#30340;&#20999;&#29255;&#20197;&#20415;&#22522;&#20110;&#30456;&#20284;&#24230;&#38408;&#20540;&#25214;&#21040;&#32858;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; MSC-DBSCAN&#25193;&#23637;&#31639;&#27861;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20301;&#20110;&#19981;&#21516;&#23376;&#31354;&#38388;&#30340;&#19981;&#21516;&#20999;&#29255;&#32858;&#31867;(&#22914;&#26524;&#25968;&#25454;&#38598;&#26159;r&#20010;&#31209;&#19968;&#24352;&#37327;(r&gt;1)&#30340;&#24635;&#21644;)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#21644; MSC &#31639;&#27861;&#30456;&#21516;&#30340;&#36755;&#20837;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#31209;&#19968;&#24352;&#37327;&#25968;&#25454;&#26102;&#19982; MSC &#31639;&#27861;&#33719;&#24471;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several methods for triclustering three-dimensional data require the cluster size or the number of clusters in each dimension to be specified. To address this issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal slices that lie in a low dimensional subspace for a rank-one tensor dataset in order to find a cluster based on the threshold similarity. We propose an extension algorithm called MSC-DBSCAN to extract the different clusters of slices that lie in the different subspaces from the data if the dataset is a sum of r rank-one tensor (r &gt; 1). Our algorithm uses the same input as the MSC algorithm and can find the same solution for rank-one tensor data as MSC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#21333;&#27493;&#23545;&#25239;&#31034;&#20363;&#20998;&#35299;&#20026;&#25968;&#25454;&#20449;&#24687;&#21644;&#33258;&#20449;&#24687;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#8220;&#33258;&#25105;&#25311;&#21512;&#8221;&#65292;&#21363;&#32593;&#32476;&#23398;&#20064;&#21333;&#27493;&#25200;&#21160;&#20013;&#23884;&#20837;&#30340;&#33258;&#20449;&#24687;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.11963</link><description>&lt;p&gt;
&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#30340;&#33258;&#25105;&#25311;&#21512;&#35270;&#35282;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective. (arXiv:2302.11963v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#21333;&#27493;&#23545;&#25239;&#31034;&#20363;&#20998;&#35299;&#20026;&#25968;&#25454;&#20449;&#24687;&#21644;&#33258;&#20449;&#24687;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#8220;&#33258;&#25105;&#25311;&#21512;&#8221;&#65292;&#21363;&#32593;&#32476;&#23398;&#20064;&#21333;&#27493;&#25200;&#21160;&#20013;&#23884;&#20837;&#30340;&#33258;&#20449;&#24687;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#26500;&#24314;&#24378;&#20581;&#32593;&#32476;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#38754;&#20020;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#65292;&#21363;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#65288;CO&#65289;&#65292;&#20854;&#20013;&#22810;&#27493;&#24378;&#20581;&#20934;&#30830;&#29575;&#31361;&#28982;&#38477;&#33267;&#38646;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#21333;&#27493;&#23545;&#25239;&#31034;&#20363;&#20998;&#35299;&#20026;&#25968;&#25454;&#20449;&#24687;&#21644;&#33258;&#20449;&#24687;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#8220;&#33258;&#25105;&#25311;&#21512;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although fast adversarial training provides an efficient approach for building robust networks, it may suffer from a serious problem known as catastrophic overfitting (CO), where multi-step robust accuracy suddenly collapses to zero. In this paper, we for the first time decouple single-step adversarial examples into data-information and self-information, which reveals an interesting phenomenon called "self-fitting". Self-fitting, i.e., the network learns the self-information embedded in single-step perturbations, naturally leads to the occurrence of CO. When self-fitting occurs, the network experiences an obvious "channel differentiation" phenomenon that some convolution channels accounting for recognizing self-information become dominant, while others for data-information are suppressed. In this way, the network can only recognize images with sufficient self-information and loses generalization ability to other types of data. Based on self-fitting, we provide new insights into the exi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34913;&#39044;&#27979;&#24615;&#33021;&#12289;&#36328;&#32452;&#24046;&#24322;&#12289;&#20445;&#25252;&#25935;&#24863;&#32676;&#32452;&#23646;&#24615;&#21644;&#24037;&#31243;&#25104;&#26412;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#24178;&#39044;&#30340;&#8220;&#20309;&#26102;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#65292;&#21516;&#26102;&#36890;&#36807;&#39044;&#27979;&#24179;&#31561;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.01574</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#24178;&#39044;&#30340;&#25805;&#20316;&#35270;&#35282;&#65306;&#20309;&#26102;&#21644;&#22914;&#20309;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
An Operational Perspective to Fairness Interventions: Where and How to Intervene. (arXiv:2302.01574v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01574
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34913;&#39044;&#27979;&#24615;&#33021;&#12289;&#36328;&#32452;&#24046;&#24322;&#12289;&#20445;&#25252;&#25935;&#24863;&#32676;&#32452;&#23646;&#24615;&#21644;&#24037;&#31243;&#25104;&#26412;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#24178;&#39044;&#30340;&#8220;&#20309;&#26102;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#65292;&#21516;&#26102;&#36890;&#36807;&#39044;&#27979;&#24179;&#31561;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#31995;&#32479;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#36816;&#33829;&#38656;&#35201;&#22312;&#39044;&#27979;&#24615;&#33021;&#12289;&#36328;&#32452;&#24046;&#24322;&#12289;&#20445;&#25252;&#25935;&#24863;&#32676;&#32452;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#65289;&#21644;&#24037;&#31243;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#21644;&#23558;&#20844;&#24179;&#24615;&#24178;&#39044;&#19982;&#19978;&#36848;&#26399;&#26395;&#30456;&#32467;&#21512;&#12290;&#23454;&#36341;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#20004;&#20010;&#20851;&#38190;&#28857;&#26159;&#24178;&#39044;&#30340;&#8220;&#20309;&#26102;&#8221;&#65288;&#39044;&#22788;&#29702;&#12289;&#22788;&#29702;&#36807;&#31243;&#20013;&#12289;&#21518;&#22788;&#29702;&#65289;&#21644;&#8220;&#22914;&#20309;&#8221;&#65288;&#25935;&#24863;&#32676;&#32452;&#25968;&#25454;&#20351;&#29992;&#30340;&#26041;&#27861;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#39044;&#27979;&#24179;&#31561;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#22312;&#20854;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#23454;&#29616;&#39044;&#27979;&#24179;&#31561;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#32676;&#32452;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#36817;400&#31181;&#19981;&#21516;&#21464;&#20307;&#30340;&#20004;&#20010;&#20027;&#35201;&#27169;&#22411;&#31867;&#22411;&#65288;XGBoost vs. Neural Net&#65289;&#12289;&#21313;&#20010;&#25968;&#25454;&#38598;&#21644;&#20108;&#21313;&#22810;&#31181;&#29420;&#29305;&#26041;&#27861;&#30340;&#22522;&#20934;&#30740;&#31350;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI-based decision systems proliferate, their successful operationalization requires balancing multiple desiderata: predictive performance, disparity across groups, safeguarding sensitive group attributes (e.g., race), and engineering cost. We present a holistic framework for evaluating and contextualizing fairness interventions with respect to the above desiderata. The two key points of practical consideration are \emph{where} (pre-, in-, post-processing) and \emph{how} (in what way the sensitive group data is used) the intervention is introduced. We demonstrate our framework with a case study on predictive parity. In it, we first propose a novel method for achieving predictive parity fairness without using group data at inference time via distibutionally robust optimization. Then, we showcase the effectiveness of these methods in a benchmarking study of close to 400 variations across two major model types (XGBoost vs. Neural Net), ten datasets, and over twenty unique methodologies.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01047</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#36827;&#34892;&#23454;&#26102;&#35780;&#20272;&#65306;&#19968;&#20010;&#26032;&#24076;&#26395;
&lt;/p&gt;
&lt;p&gt;
Real-Time Evaluation in Online Continual Learning: A New Hope. (arXiv:2302.01047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#20272;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#26041;&#38754;&#27809;&#26377;&#38480;&#21046;&#12290;&#36825;&#23545;&#20110;&#20219;&#20309;&#23454;&#38469;&#19990;&#30028;&#30340;&#29615;&#22659;&#37117;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#27969;&#19981;&#31561;&#24453;&#27169;&#22411;&#23436;&#25104;&#35757;&#32451;&#21363;&#25581;&#31034;&#19979;&#19968;&#20010;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#35745;&#31639;&#25104;&#26412;&#30340;&#35282;&#24230;&#35780;&#20272;&#24403;&#21069;&#30340;CL&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#35780;&#20272;&#19979;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#36825;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#21508;&#31181;CL&#32452;&#20214;&#65292;&#21253;&#25324;&#35760;&#24518;&#37319;&#26679;&#31574;&#30053;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#26377;&#32771;&#34385;&#30340;&#26041;&#27861;&#37117;&#26080;&#27861;&#19982;&#25105;&#20204;&#30340;&#31616;&#21333;&#22522;&#32447;&#27169;&#22411;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current evaluations of Continual Learning (CL) methods typically assume that there is no constraint on training time and computation. This is an unrealistic assumption for any real-world setting, which motivates us to propose: a practical real-time evaluation of continual learning, in which the stream does not wait for the model to complete training before revealing the next data for predictions. To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experiments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of existing methods in realistic settings. In addition, we explore various CL components commonly used in the literature, including memory sampling strategies and regularization approaches. We find that all considered methods fail to be competitive against ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMQ&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;Huber&#33021;&#37327;&#26680;&#30340;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#26465;&#20214;&#27979;&#24230;&#37327;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#20363;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.06907</link><description>&lt;p&gt;
&#28145;&#24230;&#26465;&#20214;&#27979;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Conditional Measure Quantization. (arXiv:2301.06907v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06907
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMQ&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;Huber&#33021;&#37327;&#26680;&#30340;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#26465;&#20214;&#27979;&#24230;&#37327;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#20363;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#27979;&#24230;&#30340;&#37327;&#21270;&#24847;&#21619;&#30528;&#20351;&#29992;&#19968;&#32452;&#26377;&#38480;&#30340;&#29380;&#25289;&#20811;&#20998;&#24067;&#26469;&#36817;&#20284;&#34920;&#31034;&#36755;&#20837;&#20998;&#24067;&#65288;&#22312;&#19968;&#20123;&#27010;&#29575;&#27979;&#24230;&#24230;&#37327;&#31354;&#38388;&#20013;&#65289;&#12290;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20294;&#21487;&#33021;&#20250;&#23384;&#22312;&#26465;&#20214;&#27861;&#30340;&#37327;&#21270;&#38656;&#35201;&#25506;&#32034;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DCMQ&#30340;&#26041;&#27861;&#65292;&#23427;&#28041;&#21450;&#21040;&#22522;&#20110;Huber&#33021;&#37327;&#26680;&#30340;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#32806;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#33719;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization of a probability measure means representing it with a finite set of Dirac masses that approximates the input distribution well enough (in some metric space of probability measures). Various methods exists to do so, but the situation of quantizing a conditional law has been less explored. We propose a method, called DCMQ, involving a Huber-energy kernel-based approach coupled with a deep neural network architecture. The method is tested on several examples and obtains promising results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#26679;&#26412;&#20998;&#24067;&#24773;&#20917;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#65292;&#36890;&#36807;&#20108;&#36827;&#21046;&#34920;&#31034;&#25552;&#21462;&#21367;&#31215;&#23618;&#30340;&#28608;&#27963;&#27169;&#24335;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.14268</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#36827;&#21046;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#26816;&#27979;&#26679;&#26412;&#30340;&#20998;&#24067;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Detection of out-of-distribution samples using binary neuron activation patterns. (arXiv:2212.14268v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#26679;&#26412;&#20998;&#24067;&#24773;&#20917;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#65292;&#36890;&#36807;&#20108;&#36827;&#21046;&#34920;&#31034;&#25552;&#21462;&#21367;&#31215;&#23618;&#30340;&#28608;&#27963;&#27169;&#24335;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#20316;&#20986;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#65292;&#20294;&#26159;&#26679;&#26412;&#30340;&#20998;&#24067;&#24773;&#20917;&#20173;&#28982;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#37325;&#35201;&#23616;&#38480;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12289;&#26080;&#20154;&#26426;&#21644;&#26426;&#22120;&#20154;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#23558;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#35782;&#21035;&#20026;&#26032;&#39062;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#40657;&#30418;&#65292;&#24182;&#35780;&#20272;&#20854;&#36755;&#20986;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27809;&#26377;&#34987;&#35757;&#32451;&#26469;&#20943;&#23569;OOD&#36755;&#20837;&#30340;&#32622;&#20449;&#24230;&#65292;&#22240;&#27492;&#35813;&#26041;&#27861;&#32463;&#24120;&#22833;&#36133;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;ReLU-based&#26550;&#26500;&#20013;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#65288;NAP&#65289;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#12290;&#30001;&#20110;&#20174;&#21367;&#31215;&#23618;&#25552;&#21462;&#30340;&#28608;&#27963;&#27169;&#24335;&#30340;&#20108;&#36827;&#21046;&#34920;&#31034;&#65292;&#25152;&#20197;&#35813;&#26041;&#27861;&#27809;&#26377;&#24341;&#20837;&#39640;&#35745;&#31639;&#36127;&#25285;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have outstanding performance in various applications. Despite numerous efforts of the research community, out-of-distribution (OOD) samples remain a significant limitation of DNN classifiers. The ability to identify previously unseen inputs as novel is crucial in safety-critical applications such as self-driving cars, unmanned aerial vehicles, and robots. Existing approaches to detect OOD samples treat a DNN as a black box and evaluate the confidence score of the output predictions. Unfortunately, this method frequently fails, because DNNs are not trained to reduce their confidence for OOD inputs. In this work, we introduce a novel method for OOD detection. Our method is motivated by theoretical analysis of neuron activation patterns (NAP) in ReLU-based architectures. The proposed method does not introduce a high computational overhead due to the binary representation of the activation patterns extracted from convolutional layers. The extensive empirical eval
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.12380</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#25193;&#23637;&#29289;&#29702;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#25968;&#25454;&#39537;&#21160;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (arXiv:2212.12380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#34987;&#25910;&#38598;&#65292;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#22312;&#29289;&#29702;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#20294;&#36890;&#24120;&#24456;&#38590;&#35782;&#21035;&#21644;&#25193;&#23637;&#65292;&#24182;&#19988;&#21463;&#20854;&#26377;&#38480;&#30340;&#34920;&#29616;&#21147;&#24433;&#21709;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24120;&#24120;&#20381;&#36182;&#31070;&#32463;&#32593;&#32476; (NNs) &#30340;&#32463;&#20856;&#40657;&#30418;&#26041;&#27861;&#36890;&#24120;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#32479;&#35745;&#27169;&#24335;&#65292;&#21363;&#20351;&#22312;&#25193;&#23637;&#26041;&#38754;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#28508;&#22312;&#30340;&#29289;&#29702;&#23450;&#24459;&#23436;&#20840;&#26080;&#35270;&#65292;&#22914;&#26524;&#22522;&#20110;&#23427;&#20204;&#20570;&#20915;&#31574;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#26368;&#36817;&#24320;&#21457;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476; (PCNNs) &#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992; NNs &#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558; PCNN &#25193;&#23637;&#21040;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#19982;&#32463;&#20856;&#28784;&#30418;&#21644;&#40657;&#30418;&#26041;&#27861;&#30340;&#24443;&#24213;&#27604;&#36739;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#22810;&#21306;&#22495;&#24314;&#31569;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#21306;&#22495;&#30340;&#28909;&#34892;&#20026;&#30001;&#33021;&#37327;&#24179;&#34913;&#26041;&#31243;&#24335;&#32479;&#27835;&#65292;&#20854;&#21442;&#25968;&#24517;&#39035;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#35782;&#21035;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#28041;&#21450;&#35768;&#22810;&#30456;&#20114;&#20316;&#29992;&#30340;&#32452;&#20214;&#26500;&#25104;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#31995;&#32479;&#65292;PCNNs &#20063;&#21487;&#20197;&#22312;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126; PCNN &#22312;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy.  In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-b
&lt;/p&gt;</description></item><item><title>POTATO&#26159;&#19968;&#20010;&#20813;&#36153;&#12289;&#24320;&#28304;&#30340;&#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26631;&#27880;&#65292;&#25552;&#20379;&#26131;&#20110;&#37197;&#32622;&#30340;&#21151;&#33021;&#20197;&#26368;&#22823;&#21270;&#29983;&#20135;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#25991;&#26723;&#21644;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.08620</link><description>&lt;p&gt;
POTATO: &#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
POTATO: The Portable Text Annotation Tool. (arXiv:2212.08620v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08620
&lt;/p&gt;
&lt;p&gt;
POTATO&#26159;&#19968;&#20010;&#20813;&#36153;&#12289;&#24320;&#28304;&#30340;&#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26631;&#27880;&#65292;&#25552;&#20379;&#26131;&#20110;&#37197;&#32622;&#30340;&#21151;&#33021;&#20197;&#26368;&#22823;&#21270;&#29983;&#20135;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#25991;&#26723;&#21644;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;POTATO&#65292;&#21363;&#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#20813;&#36153;&#12289;&#24320;&#28304;&#30340;&#27880;&#37322;&#31995;&#32479;&#65292;&#25903;&#25345;&#26631;&#27880;&#22810;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#25552;&#20379;&#26131;&#20110;&#37197;&#32622;&#30340;&#21151;&#33021;&#20197;&#26368;&#22823;&#21270;&#37096;&#32626;&#21644;&#27880;&#37322;&#32773;&#30340;&#29983;&#20135;&#21147;&#65288;&#20415;&#25463;&#30340;&#26426;&#22120;&#23398;&#20064;/&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#27169;&#26495;&#12289;&#20027;&#21160;&#23398;&#20064;&#12289;&#25353;&#38190;&#32553;&#20889;&#38190;&#12289;&#20851;&#38190;&#23383;&#39640;&#20142;&#12289;&#25552;&#31034;&#24037;&#20855;&#65289;&#65292;&#24182;&#25903;&#25345;&#39640;&#24230;&#23450;&#21046;&#21270;&#65288;&#21487;&#32534;&#36753;&#30340;UI&#65292;&#25554;&#20837;&#39044;&#31579;&#36873;&#38382;&#39064;&#65292;&#27880;&#24847;&#21147;&#21644;&#36164;&#26684;&#27979;&#35797;&#65289;&#12290;&#20004;&#39033;&#27880;&#37322;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;POTATO&#36890;&#36807;&#20854;&#29305;&#21035;&#35774;&#35745;&#30340;&#29983;&#20135;&#21147;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#38271;&#25991;&#26723;&#21644;&#22797;&#26434;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#36895;&#24230;&#12290;POTATO&#21487;&#22312; https://github.com/davidjurgens/potato &#19978;&#33719;&#21462;&#65292;&#24182;&#23558;&#32487;&#32493;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present POTATO, the Portable text annotation tool, a free, fully open-sourced annotation system that 1) supports labeling many types of text and multimodal data; 2) offers easy-to-configure features to maximize the productivity of both deployers and annotators (convenient templates for common ML/NLP tasks, active learning, keypress shortcuts, keyword highlights, tooltips); and 3) supports a high degree of customization (editable UI, inserting pre-screening questions, attention and qualification tests). Experiments over two annotation tasks suggest that POTATO improves labeling speed through its specially-designed productivity features, especially for long documents and complex tasks. POTATO is available at https://github.com/davidjurgens/potato and will continue to be updated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FlexiViT&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#26435;&#37325;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#34917;&#19969;&#22823;&#23567;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#23481;&#26131;&#26681;&#25454;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#26469;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.08013</link><description>&lt;p&gt;
FlexiViT&#65306;&#36866;&#29992;&#20110;&#25152;&#26377;&#34917;&#19969;&#22823;&#23567;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FlexiViT: One Model for All Patch Sizes. (arXiv:2212.08013v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FlexiViT&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#26435;&#37325;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#34917;&#19969;&#22823;&#23567;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#23481;&#26131;&#26681;&#25454;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#26469;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#23558;&#22270;&#20687;&#20999;&#25104;&#34917;&#19969;&#20197;&#23558;&#20854;&#36716;&#25442;&#20026;&#24207;&#21015;&#12290;&#36825;&#20123;&#34917;&#19969;&#30340;&#22823;&#23567;&#25511;&#21046;&#36895;&#24230;/&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#34917;&#19969;&#36234;&#23567;&#65292;&#31934;&#24230;&#36234;&#39640;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20063;&#36234;&#39640;&#65292;&#20294;&#26356;&#25913;&#34917;&#19969;&#22823;&#23567;&#36890;&#24120;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#31616;&#21333;&#38543;&#26426;&#21270;&#35757;&#32451;&#26102;&#30340;&#34917;&#19969;&#22823;&#23567;&#20250;&#23548;&#33268;&#19968;&#32452;&#26435;&#37325;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#22312;&#21508;&#31181;&#34917;&#19969;&#22823;&#23567;&#33539;&#22260;&#20869;&#20351;&#29992;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#23481;&#26131;&#26681;&#25454;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#26469;&#23450;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#23545;&#29983;&#25104;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#65292;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#65292;&#24320;&#25918;&#24335;&#26816;&#27979;&#65292;&#20840;&#26223;&#20998;&#21106;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#24471;&#20986;&#32467;&#35770;&#65306;&#23427;&#36890;&#24120;&#21487;&#20197;&#21305;&#37197;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20248;&#20110;&#20197;&#21333;&#20010;&#34917;&#19969;&#22823;&#23567;&#35757;&#32451;&#30340;&#26631;&#20934;ViT&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22312;ViT&#20013;&#20351;&#29992;FlexiViT&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38169;&#35823;&#35268;&#33539;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22411;&#19982;&#28436;&#31034;&#31574;&#30053;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#20197;&#21450;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#27169;&#22411;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2212.03201</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38169;&#35823;&#35268;&#33539;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Misspecification in Inverse Reinforcement Learning. (arXiv:2212.03201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38169;&#35823;&#35268;&#33539;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22411;&#19982;&#28436;&#31034;&#31574;&#30053;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#20197;&#21450;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#27169;&#22411;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#20174;&#31574;&#30053;&#960;&#20013;&#25512;&#26029;&#20986;&#22870;&#21169;&#20989;&#25968;R&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#34920;&#26126;&#960;&#19982;R&#20851;&#31995;&#30340;&#27169;&#22411;&#12290;&#30446;&#21069;&#25991;&#29486;&#20013;&#26368;&#24120;&#35265;&#30340;&#27169;&#22411;&#26159;&#26368;&#20248;&#24615;&#12289;Boltzmann&#21512;&#29702;&#24615;&#21644;&#22240;&#26524;&#29109;&#26368;&#22823;&#21270;&#12290;IRL&#30340;&#19968;&#20010;&#20027;&#35201;&#21160;&#26426;&#26159;&#20174;&#20154;&#31867;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20559;&#22909;&#19982;&#20154;&#31867;&#34892;&#20026;&#20043;&#38388;&#30340;&#30495;&#23454;&#20851;&#31995;&#27604;IRL&#30446;&#21069;&#20351;&#29992;&#30340;&#20219;&#20309;&#27169;&#22411;&#37117;&#26356;&#21152;&#22797;&#26434;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#34987;&#35268;&#33539;&#38169;&#35823;&#65292;&#36825;&#24341;&#21457;&#20102;&#36825;&#26679;&#30340;&#25285;&#24515;&#65306;&#22914;&#26524;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function $R$ from a policy $\pi$. To do this, we need a model of how $\pi$ relates to $R$. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function $R$. We also introduce a framework for reasoning about misspecificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#21367;&#31215;&#12289;&#32858;&#21512;&#21644;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20026;&#23398;&#20064;&#22266;&#20307;&#21464;&#24418;&#30340;&#39640;&#24230;&#38750;&#32447;&#24615;&#21464;&#21270;&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01386</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#12289;&#32858;&#21512;&#21644;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21152;&#36895;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Convolution, aggregation and attention based deep neural networks for accelerating simulations in mechanics. (arXiv:2212.01386v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#21367;&#31215;&#12289;&#32858;&#21512;&#21644;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20026;&#23398;&#20064;&#22266;&#20307;&#21464;&#24418;&#30340;&#39640;&#24230;&#38750;&#32447;&#24615;&#21464;&#21270;&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#20316;&#20026;&#26114;&#36149;&#20256;&#32479;&#25968;&#20540;&#25216;&#26415;&#30340;&#26367;&#20195;&#21697;&#65292;&#22312;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#26041;&#38754;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22797;&#26434;&#30495;&#23454;&#19990;&#30028;&#30340;&#26679;&#20363;&#26102;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#23398;&#20064;&#22266;&#20307;&#21464;&#24418;&#30340;&#39640;&#24230;&#38750;&#32447;&#24615;&#21464;&#21270;&#12290;&#21069;&#20004;&#31181;&#26550;&#26500;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;CNN U-NET&#21644;MAgNET&#65288;&#22270;&#24418;U-NET&#65289;&#26694;&#26550;&#65292;&#24050;&#34920;&#29616;&#20986;&#22312;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#31532;&#19977;&#31181;&#26550;&#26500;&#26159;Perceiver IO&#65292;&#23427;&#26159;&#26368;&#36817;&#30340;&#19968;&#31181;&#26550;&#26500;&#65292;&#23646;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#65292;&#36825;&#20010;&#31867;&#24050;&#32463;&#22312;&#21508;&#31181;&#24037;&#31243;&#39046;&#22495;&#20013;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#22312;&#35745;&#31639;&#21147;&#23398;&#39046;&#22495;&#20173;&#28982;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#36825;&#19977;&#20010;&#32593;&#32476;&#22312;&#20004;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#20934;&#30830;&#39044;&#27979;&#38750;&#32447;&#24615;&#21147;&#23398;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning surrogate models are being increasingly used in accelerating scientific simulations as a replacement for costly conventional numerical techniques. However, their use remains a significant challenge when dealing with real-world complex examples. In this work, we demonstrate three types of neural network architectures for efficient learning of highly non-linear deformations of solid bodies. The first two architectures are based on the recently proposed CNN U-NET and MAgNET (graph U-NET) frameworks which have shown promising performance for learning on mesh-based data. The third architecture is Perceiver IO, a very recent architecture that belongs to the family of attention-based neural networks--a class that has revolutionised diverse engineering fields and is still unexplored in computational mechanics. We study and compare the performance of all three networks on two benchmark examples, and show their capabilities to accurately predict the non-linear mechanical responses 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHCCL&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#26469;&#28388;&#38500;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#34917;&#20805;&#27491;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.01141</link><description>&lt;p&gt;
MHCCL&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#23618;&#27425;&#25513;&#34109;&#32858;&#31867;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series. (arXiv:2212.01141v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHCCL&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#26469;&#28388;&#38500;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#34917;&#20805;&#27491;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#26080;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#23545;&#20110;&#20998;&#31867;&#21644;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#27604;&#23398;&#20064;&#26368;&#36817;&#23637;&#31034;&#20102;&#22312;&#32570;&#20047;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#29420;&#31435;&#22788;&#29702;&#27599;&#20010;&#23454;&#20363;&#65292;&#23548;&#33268;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#30340;&#20551;&#36127;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MHCCL&#65292;&#19968;&#31181;&#23618;&#27425;&#25513;&#34109;&#32858;&#31867;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30001;&#22810;&#20010;&#28508;&#22312;&#20998;&#21306;&#32452;&#25104;&#30340;&#23618;&#27425;&#32467;&#26500;&#33719;&#24471;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#21463;&#21040;&#32454;&#31890;&#24230;&#32858;&#31867;&#20445;&#30041;&#26356;&#39640;&#32431;&#24230;&#65292;&#32780;&#31895;&#31890;&#24230;&#32858;&#31867;&#21453;&#26144;&#26356;&#39640;&#32423;&#21035;&#35821;&#20041;&#30340;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21521;&#19979;&#25513;&#34109;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#32858;&#31867;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#65292;&#36807;&#28388;&#25481;&#34394;&#20551;&#36127;&#38754;&#23454;&#20363;&#24182;&#34917;&#20805;&#27491;&#38754;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning semantic-rich representations from raw unlabeled time series data is critical for downstream tasks such as classification and forecasting. Contrastive learning has recently shown its promising representation learning capability in the absence of expert annotations. However, existing contrastive approaches generally treat each instance independently, which leads to false negative pairs that share the same semantics. To tackle this problem, we propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model, which exploits semantic information obtained from the hierarchical structure consisting of multiple latent partitions for multivariate time series. Motivated by the observation that fine-grained clustering preserves higher purity while coarse-grained one reflects higher-level semantics, we propose a novel downward masking strategy to filter out fake negatives and supplement positives by incorporating the multi-granularity information from the clustering hierarchy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#26377;&#20107;&#20214;&#30340;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#22522;&#20110;&#25910;&#38598;&#21040;&#30340;&#26102;&#38388;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#21472;&#21152;&#25968;&#25454;&#38598;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#22312;&#21464;&#37327;&#31532;&#19968;&#27425;&#32463;&#21382;&#20302;&#27010;&#29575;&#23454;&#29616;&#26102;&#25165;&#20250;&#26174;&#29616;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.16596</link><description>&lt;p&gt;
&#38754;&#21521;&#31232;&#26377;&#20107;&#20214;&#30340;&#21160;&#24577;&#22240;&#26524;&#21457;&#29616;&#65306;&#19968;&#31181;&#38750;&#21442;&#25968;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16596
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#26377;&#20107;&#20214;&#30340;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#22522;&#20110;&#25910;&#38598;&#21040;&#30340;&#26102;&#38388;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#21472;&#21152;&#25968;&#25454;&#38598;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#22312;&#21464;&#37327;&#31532;&#19968;&#27425;&#32463;&#21382;&#20302;&#27010;&#29575;&#23454;&#29616;&#26102;&#25165;&#20250;&#26174;&#29616;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#31232;&#26377;&#20107;&#20214;&#30456;&#20851;&#32852;&#30340;&#22240;&#26524;&#29616;&#35937;&#22312;&#35768;&#22810;&#24037;&#31243;&#38382;&#39064;&#20013;&#37117;&#23384;&#22312;&#65292;&#20363;&#22914;&#38024;&#23545;&#39118;&#38505;&#30340;&#23433;&#20840;&#20998;&#26512;&#12289;&#20107;&#25925;&#20998;&#26512;&#21644;&#39044;&#38450;&#20197;&#21450;&#26497;&#20540;&#29702;&#35770;&#31561;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#21457;&#29616;&#22312;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#21407;&#22240;&#32852;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#21464;&#21160;&#29615;&#22659;&#19979;&#65292;&#20165;&#22312;&#21464;&#37327;&#31532;&#19968;&#27425;&#32463;&#21382;&#20302;&#27010;&#29575;&#23454;&#29616;&#26102;&#25165;&#20250;&#26174;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21457;&#29983;&#31232;&#26377;&#20294;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#26102;&#38388;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#24213;&#23618;&#25968;&#25454;&#30340;&#26102;&#38388;&#19981;&#21464;&#24615;&#26469;&#26500;&#24314;&#19968;&#20010;&#21472;&#21152;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#20043;&#21069;&#31232;&#26377;&#20107;&#20214;&#21457;&#29983;&#21069;&#31995;&#32479;&#29366;&#24577;&#30340;&#25968;&#25454;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#37325;&#26032;&#32452;&#32455;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#19968;&#33268;&#24615;&#30340;&#38750;&#28176;&#36817;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal phenomena associated with rare events occur across a wide range of engineering problems, such as risk-sensitive safety analysis, accident analysis and prevention, and extreme value theory. However, current methods for causal discovery are often unable to uncover causal links, between random variables in a dynamic setting, that manifest only when the variables first experience low-probability realizations. To address this issue, we introduce a novel statistical independence test on data collected from time-invariant dynamical systems in which rare but consequential events occur. In particular, we exploit the time-invariance of the underlying data to construct a superimposed dataset of the system state before rare events happen at different timesteps. We then design a conditional independence test on the reorganized data. We provide non-asymptotic sample complexity bounds for the consistency of our method, and validate its performance across various simulated and real-world datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25670;&#23039;&#21183;&#30340;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#27861;RUST&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23039;&#21183;&#32534;&#30721;&#22120;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#28508;&#22312;&#23039;&#24577;&#23884;&#20837;&#65292;&#21487;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#22810;&#20010;&#22330;&#26223;&#21644;&#25552;&#39640;&#35270;&#35273;&#36136;&#37327;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.14306</link><description>&lt;p&gt;
RUST&#65306;&#20174;&#26410;&#25670;&#23039;&#21183;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#28508;&#22312;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
RUST: Latent Neural Scene Representations from Unposed Imagery. (arXiv:2211.14306v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25670;&#23039;&#21183;&#30340;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#27861;RUST&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23039;&#21183;&#32534;&#30721;&#22120;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#28508;&#22312;&#23039;&#24577;&#23884;&#20837;&#65292;&#21487;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#22810;&#20010;&#22330;&#26223;&#21644;&#25552;&#39640;&#35270;&#35273;&#36136;&#37327;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#26029;&#19977;&#32500;&#22330;&#26223;&#30340;&#32467;&#26500;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#22522;&#20110;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#30340;&#26041;&#27861;&#36817;&#24180;&#26469;&#24191;&#21463;&#20851;&#27880;&#65292;&#24182;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#20013;&#12290;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#28508;&#22312;&#34920;&#31034;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#22810;&#20010;&#22330;&#26223;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;&#65288;SRT&#65289;&#22312;&#36825;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#22823;&#25968;&#37327;&#30340;&#22810;&#26679;&#21270;&#22330;&#26223;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#38656;&#35201;&#20934;&#30830;&#30340;&#25670;&#23039;&#25968;&#25454;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RUST&#65288;&#30495;&#27491;&#30340;&#26410;&#25670;&#23039;&#21183;&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;RGB&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#30340;&#26080;&#23039;&#21183;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35265;&#35299;&#26159;&#65292;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#23039;&#21183;&#32534;&#30721;&#22120;&#65292;&#23427;&#21487;&#20197;&#26597;&#30475;&#30446;&#26631;&#22270;&#20687;&#24182;&#23398;&#20064;&#19968;&#20010;&#28508;&#22312;&#30340;&#23039;&#21183;&#23884;&#20837;&#65292;&#35813;&#23039;&#21183;&#34987;&#35299;&#30721;&#22120;&#29992;&#20110;&#35270;&#35282;&#21512;&#25104;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;RUST&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#27867;&#21270;&#21040;&#26032;&#22330;&#26223;&#26041;&#38754;&#22343;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring the structure of 3D scenes from 2D observations is a fundamental challenge in computer vision. Recently popularized approaches based on neural scene representations have achieved tremendous impact and have been applied across a variety of applications. One of the major remaining challenges in this space is training a single model which can provide latent representations which effectively generalize beyond a single scene. Scene Representation Transformer (SRT) has shown promise in this direction, but scaling it to a larger set of diverse scenes is challenging and necessitates accurately posed ground truth data. To address this problem, we propose RUST (Really Unposed Scene representation Transformer), a pose-free approach to novel view synthesis trained on RGB images alone. Our main insight is that one can train a Pose Encoder that peeks at the target image and learns a latent pose embedding which is used by the decoder for view synthesis. We perform an empirical investigation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#35753;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#23481;&#26131;&#30340;&#21306;&#22495;&#65292;&#20943;&#23569;&#23545;&#38590;&#30340;&#21306;&#22495;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#22855;&#24322;&#25668;&#21160;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#38382;&#39064;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.12685</link><description>&lt;p&gt;
&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#22855;&#24322;&#25668;&#21160;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#38382;&#39064;&#20013;&#30340;PINN&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Less Emphasis on Difficult Layer Regions: Curriculum Learning for Singularly Perturbed Convection-Diffusion-Reaction Problems. (arXiv:2210.12685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12685
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#35753;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#23481;&#26131;&#30340;&#21306;&#22495;&#65292;&#20943;&#23569;&#23545;&#38590;&#30340;&#21306;&#22495;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#22855;&#24322;&#25668;&#21160;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#38382;&#39064;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20294;&#22312;&#31245;&#20855;&#25361;&#25112;&#30340;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#38382;&#39064;&#20013;&#65292;&#24448;&#24448;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#22522;&#26412;&#35299;&#12290;&#26412;&#25991;&#20174;&#21306;&#22495;&#20998;&#24067;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#31181;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#21516;&#26102;&#23398;&#20064;&#22810;&#23610;&#24230;&#22330;&#20351;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#23481;&#26131;&#38519;&#20837;&#36739;&#24046;&#30340;&#23616;&#22495;&#26497;&#23567;&#20540;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#39640;&#25439;&#22833;&#23618;&#21306;&#22495;&#37319;&#26679;&#26356;&#22810;&#30896;&#25758;&#28857;&#30340;&#24191;&#27867;&#32463;&#39564;&#20960;&#20046;&#19981;&#33021;&#24110;&#21161;&#20248;&#21270;&#65292;&#24182;&#19988;&#29978;&#33267;&#20250;&#20351;&#32467;&#26524;&#21464;&#24046;&#12290;&#36825;&#20123;&#21457;&#29616;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#40723;&#21169;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#26356;&#23481;&#26131;&#30340;&#38750;&#23618;&#21306;&#22495;&#65292;&#38477;&#20302;&#23545;&#38590;&#24230;&#26356;&#22823;&#30340;&#23618;&#21306;&#22495;&#30340;&#23398;&#20064;&#37325;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;PINNs&#33258;&#21160;&#35843;&#25972;&#23398;&#20064;&#37325;&#28857;&#65292;&#20174;&#32780;&#20419;&#36827;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Physics-Informed Neural Networks (PINNs) have been successfully applied in a wide variety of science and engineering fields, they can fail to accurately predict the underlying solution in slightly challenging convection-diffusion-reaction problems. In this paper, we investigate the reason of this failure from a domain distribution perspective, and identify that learning multi-scale fields simultaneously makes the network unable to advance its training and easily get stuck in poor local minima. We show that the widespread experience of sampling more collocation points in high-loss layer regions hardly help optimize and may even worsen the results. These findings motivate the development of a novel curriculum learning method that encourages neural networks to prioritize learning on easier non-layer regions while downplaying learning on harder layer regions. The proposed method helps PINNs automatically adjust the learning emphasis and thereby facilitate the optimization procedur
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#32452;&#21512;&#20449;&#24687;&#26041;&#38754;&#30340;&#34920;&#29616;&#23384;&#22312;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Attribution&#12289;Relation&#21644;Order&#65288;ARO&#65289;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;VLMs&#36827;&#34892;Attention&#26426;&#21046;&#21644;&#23545;&#25239;&#35757;&#32451;&#31561;&#20462;&#25913;&#20197;&#25552;&#39640;&#20854;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.01936</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#34892;&#20026;&#20687;&#35789;&#34955;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#65311;
&lt;/p&gt;
&lt;p&gt;
When and why vision-language models behave like bags-of-words, and what to do about it?. (arXiv:2210.01936v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01936
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#32452;&#21512;&#20449;&#24687;&#26041;&#38754;&#30340;&#34920;&#29616;&#23384;&#22312;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Attribution&#12289;Relation&#21644;Order&#65288;ARO&#65289;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;VLMs&#36827;&#34892;Attention&#26426;&#21046;&#21644;&#23545;&#25239;&#35757;&#32451;&#31561;&#20462;&#25913;&#20197;&#25552;&#39640;&#20854;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32534;&#30721;&#32452;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Attribution&#12289;Relation&#21644;Order&#65288;ARO&#65289;&#22522;&#20934;&#26469;&#31995;&#32479;&#35780;&#20272;VLM&#29702;&#35299;&#19981;&#21516;&#31867;&#22411;&#20851;&#31995;&#12289;&#23646;&#24615;&#21644;&#39034;&#24207;&#30340;&#33021;&#21147;&#12290;ARO&#30001;Visual Genome Attribution&#27979;&#35797;&#23545;&#35937;&#23646;&#24615;&#30340;&#29702;&#35299;&#33021;&#21147;&#65307;Visual Genome Relation&#27979;&#35797;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#65307;&#20197;&#21450;COCO&#65286;Flickr30k-Order&#27979;&#35797;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;ARO&#27604;&#20197;&#21069;&#30340;&#32452;&#21512;&#24615;&#22522;&#20934;&#22823;&#22810;&#20010;&#25968;&#37327;&#32423;&#65292;&#21253;&#25324;50,000&#22810;&#20010;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;VLM&#22312;&#21738;&#20123;&#26041;&#38754;&#23384;&#22312;&#20851;&#31995;&#29702;&#35299;&#38382;&#39064;&#65292;&#24403;&#38142;&#25509;&#23545;&#35937;&#21644;&#23646;&#24615;&#26102;&#23481;&#26131;&#20986;&#38169;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26126;&#26174;&#30340;&#32570;&#20047;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;VLM&#20027;&#35201;&#22312;&#20855;&#26377;&#20016;&#23500;&#32452;&#21512;&#32467;&#26500;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24182;&#19981;&#33021;&#20445;&#35777;&#22312;&#38656;&#35201;&#32452;&#21512;&#29702;&#35299;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#20462;&#25913;VLMs&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24378;&#35843;&#32452;&#25104;&#20851;&#31995;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#25913;&#21892;&#23646;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20462;&#25913;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;VLM&#22312;ARO&#22522;&#20934;&#19978;&#30340;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO &amp; Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#25104;&#26412;&#21644;&#28145;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20854;&#20250;&#25910;&#25947;&#21040;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#31209;&#30340;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#22312;&#19968;&#23450;&#30340;&#28145;&#24230;&#33539;&#22260;&#20869;&#65292;&#20840;&#23616;&#26368;&#23567;&#20540;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;&#25968;&#25454;&#31209;&#65292;&#24182;&#25506;&#35752;&#20102;&#20998;&#31867;&#22120;&#31209;&#23545;&#31867;&#36793;&#30028;&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2209.15055</link><description>&lt;p&gt;
&#22823;&#28145;&#24230;&#32593;&#32476;&#30340;&#38544;&#24335;&#20559;&#35265;&#65306;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#31209;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#25104;&#26412;&#21644;&#28145;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20854;&#20250;&#25910;&#25947;&#21040;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#31209;&#30340;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#22312;&#19968;&#23450;&#30340;&#28145;&#24230;&#33539;&#22260;&#20869;&#65292;&#20840;&#23616;&#26368;&#23567;&#20540;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;&#25968;&#25454;&#31209;&#65292;&#24182;&#25506;&#35752;&#20102;&#20998;&#31867;&#22120;&#31209;&#23545;&#31867;&#36793;&#30028;&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#40784;&#27425;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#25104;&#26412;&#8212;&#8212;&#25551;&#36848;&#20102;&#20855;&#26377;$L_2$&#27491;&#21017;&#21270;&#25110;&#20132;&#21449;&#29109;&#31561;&#25439;&#22833;&#32593;&#32476;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38544;&#24335;&#20559;&#35265;&#8212;&#8212;&#38543;&#30528;&#32593;&#32476;&#28145;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#20250;&#25910;&#25947;&#21040;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#31209;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#21487;&#20197;&#24674;&#22797;&#8220;&#30495;&#23454;&#8221;&#25968;&#25454;&#31209;&#65306;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#23545;&#20110;&#22826;&#22823;&#30340;&#28145;&#24230;&#65292;&#20840;&#23616;&#26368;&#23567;&#20540;&#20250;&#36817;&#20284;&#20026;&#31209;1&#65288;&#20302;&#20272;&#31209;&#65289;&#65307;&#25105;&#20204;&#38543;&#21518;&#35770;&#35777;&#20102;&#26377;&#19968;&#31995;&#21015;&#28145;&#24230;&#65292;&#38543;&#30528;&#25968;&#25454;&#28857;&#25968;&#37327;&#22686;&#21152;&#65292;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#31209;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20998;&#31867;&#22120;&#31209;&#23545;&#32467;&#26524;&#31867;&#36793;&#30028;&#30340;&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#20855;&#26377;&#26368;&#20339;&#38750;&#32447;&#24615;&#31209;&#30340;&#33258;&#32534;&#30721;&#22120;&#20855;&#26377;&#33258;&#28982;&#21435;&#22122;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true' rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36830;&#32493;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#21644;&#22522;&#20110;&#36830;&#32493;&#28508;&#31354;&#38388;&#30340;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2209.10584</link><description>&lt;p&gt;
&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#30340;&#36830;&#32493;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Continuous Mixtures of Tractable Probabilistic Models. (arXiv:2209.10584v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36830;&#32493;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#21644;&#22522;&#20110;&#36830;&#32493;&#28508;&#31354;&#38388;&#30340;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36830;&#32493;&#28508;&#31354;&#38388;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#22312;&#28508;&#21464;&#37327;&#19978;&#36830;&#32493;&#20381;&#36182;&#30340;&#19981;&#21487;&#25968;&#28151;&#21512;&#27169;&#22411;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#26159;&#34920;&#36798;&#29983;&#25104;&#21644;&#27010;&#29575;&#24314;&#27169;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#20294;&#19982;&#33021;&#22815;&#35745;&#31639;&#25152;&#34920;&#31034;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#36739;&#20026;&#31616;&#21333;&#30340;&#21487;&#35745;&#31639;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#23384;&#22312;&#30683;&#30462;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#65292;&#22914;&#27010;&#29575;&#30005;&#36335;(PCs)&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#23618;&#27425;&#31163;&#25955;&#28151;&#21512;&#27169;&#22411;&#65292;&#22240;&#27492;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#20934;&#30830;&#25512;&#26029;&#65292;&#20294;&#22312;&#19982;&#36830;&#32493;&#28508;&#31354;&#38388;&#27169;&#22411;&#30456;&#27604;&#26102;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#23567;&#28508;&#21464;&#37327;&#32500;&#24230;&#30340;&#21487;&#35745;&#31639;&#27169;&#22411;&#30340;&#36830;&#32493;&#28151;&#21512;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#26512;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#20294;&#23427;&#20204;&#36866;&#21512;&#20110;&#22522;&#20110;&#26377;&#38480;&#31215;&#20998;&#28857;&#38598;&#30340;&#25968;&#20540;&#31215;&#20998;&#26041;&#26696;&#12290;&#36890;&#36807;&#36275;&#22815;&#22823;&#30340;&#31215;&#20998;&#28857;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#32039;&#23494;&#22320;&#36817;&#20284;&#27010;&#29575;&#27169;&#22411;&#24182;&#36827;&#34892;&#39640;&#25928;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic models based on continuous latent spaces, such as variational autoencoders, can be understood as uncountable mixture models where components depend continuously on the latent code. They have proven to be expressive tools for generative and probabilistic modelling, but are at odds with tractable probabilistic inference, that is, computing marginals and conditionals of the represented probability distribution. Meanwhile, tractable probabilistic models such as probabilistic circuits (PCs) can be understood as hierarchical discrete mixture models, and thus are capable of performing exact inference efficiently but often show subpar performance in comparison to continuous latent-space models. In this paper, we investigate a hybrid approach, namely continuous mixtures of tractable models with a small latent dimension. While these models are analytically intractable, they are well amenable to numerical integration schemes based on a finite set of integration points. With a large 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#30340;&#38544;&#31192;&#23454;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;TrojViT&#65292;&#36890;&#36807;&#34917;&#19969;&#32423;&#35302;&#21457;&#22120;&#23558;&#19968;&#20123;&#26131;&#21463;&#25915;&#20987;&#20301;&#32452;&#25104;&#30340;&#21442;&#25968;&#24314;&#31435;&#20026;&#29305;&#27931;&#20234;&#26408;&#39532;&#12290;</title><link>http://arxiv.org/abs/2208.13049</link><description>&lt;p&gt;
TrojViT: &#35270;&#35273;Transformer&#20013;&#30340;&#21518;&#38376;&#25554;&#20837;
&lt;/p&gt;
&lt;p&gt;
TrojViT: Trojan Insertion in Vision Transformers. (arXiv:2208.13049v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#30340;&#38544;&#31192;&#23454;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;TrojViT&#65292;&#36890;&#36807;&#34917;&#19969;&#32423;&#35302;&#21457;&#22120;&#23558;&#19968;&#20123;&#26131;&#21463;&#25915;&#20987;&#20301;&#32452;&#25104;&#30340;&#21442;&#25968;&#24314;&#31435;&#20026;&#29305;&#27931;&#20234;&#26408;&#39532;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViT&#65289;&#22312;&#21508;&#31181;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;ViTs&#30340;&#25104;&#21151;&#28608;&#21169;&#20102;&#25915;&#20987;&#32773;&#23545;&#20854;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#20294;ViTs&#30340;&#21518;&#38376;&#25915;&#20987;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#19982;&#36890;&#36807;&#21367;&#31215;&#25429;&#25417;&#20687;&#32032;&#32423;&#23616;&#37096;&#29305;&#24449;&#30340;CNNs&#30456;&#27604;&#65292;ViTs&#36890;&#36807;&#34917;&#19969;&#21644;&#20851;&#27880;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#23558;CNN&#29305;&#23450;&#30340;&#21518;&#38376;&#25915;&#20987;&#22825;&#30495;&#22320;&#31227;&#26893;&#21040;ViTs&#21482;&#33021;&#20135;&#29983;&#20302;&#24178;&#20928;&#25968;&#25454;&#20934;&#30830;&#24615;&#21644;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#31192;&#32780;&#23454;&#29992;&#30340;ViT&#29305;&#23450;&#21518;&#38376;&#25915;&#20987;TrojViT&#12290;TrojViT&#29983;&#25104;&#19968;&#20010;&#34917;&#19969;&#32423;&#30340;&#35302;&#21457;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#34917;&#19969;&#26174;&#33879;&#24615;&#25490;&#24207;&#21644;&#20851;&#27880;&#30446;&#26631;&#25439;&#22833;&#65292;&#22312;DRAM&#23384;&#20648;&#22120;&#20013;&#30340;&#19968;&#20123;&#26131;&#21463;&#25915;&#20987;&#20301;&#32452;&#25104;&#30340;ViT&#30340;&#21442;&#25968;&#19978;&#24314;&#31435;&#29305;&#27931;&#20234;&#26408;&#39532;&#12290; TrojViT&#36824;&#20351;&#29992;&#26368;&#23567;&#21270;&#35843;&#25972;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have demonstrated the state-of-the-art performance in various vision-related tasks. The success of ViTs motivates adversaries to perform backdoor attacks on ViTs. Although the vulnerability of traditional CNNs to backdoor attacks is well-known, backdoor attacks on ViTs are seldom-studied. Compared to CNNs capturing pixel-wise local features by convolutions, ViTs extract global context information through patches and attentions. Na\"ively transplanting CNN-specific backdoor attacks to ViTs yields only a low clean data accuracy and a low attack success rate. In this paper, we propose a stealth and practical ViT-specific backdoor attack $TrojViT$. Rather than an area-wise trigger used by CNN-specific backdoor attacks, TrojViT generates a patch-wise trigger designed to build a Trojan composed of some vulnerable bits on the parameters of a ViT stored in DRAM memory through patch salience ranking and attention-target loss. TrojViT further uses minimum-tuned paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#35774;&#35745;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#31232;&#30095;&#37319;&#26679;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.11356</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#26377;&#25928;&#21033;&#29992;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors. (arXiv:2208.11356v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#35774;&#35745;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#31232;&#30095;&#37319;&#26679;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#29305;&#24449;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#24040;&#22823;&#29978;&#33267;&#26159;&#31105;&#27490;&#24615;&#30340;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26368;&#36817;&#30340;&#22522;&#20110;Transformer&#30340;&#26816;&#27979;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;&#65288;IMFA&#65289;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#39640;&#25928;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#26469;&#33258;&#20165;&#26377;&#20960;&#20010;&#20851;&#38190;&#20301;&#32622;&#30340;&#31232;&#30095;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#35774;&#35745;&#23454;&#29616;&#12290;&#39318;&#20808;&#65292;IMFA&#37325;&#26032;&#25490;&#21015;Transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31649;&#36947;&#65292;&#20197;&#20415;&#26681;&#25454;&#26816;&#27979;&#39044;&#27979;&#36845;&#20195;&#26356;&#26032;&#32534;&#30721;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;IMFA&#22312;&#20197;&#20808;&#21069;&#30340;&#26816;&#27979;&#39044;&#27979;&#20316;&#20026;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#20165;&#26377;&#20960;&#20010;&#20851;&#38190;&#28857;&#37319;&#29992;&#31232;&#30095;&#30340;&#23610;&#24230;&#33258;&#36866;&#24212;&#29305;&#24449;&#29992;&#20110;&#31934;&#32454;&#26816;&#27979;&#12290;&#22240;&#27492;&#65292;&#37319;&#26679;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#26159;&#31232;&#30095;&#30340;&#65292;&#20294;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#20173;&#28982;&#38750;&#24120;&#26377;&#30410;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;IMFA&#25216;&#26415;&#21487;&#20197;&#22312;&#20445;&#35777;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-scale features have been proven highly effective for object detection but often come with huge and even prohibitive extra computation costs, especially for the recent Transformer-based detectors. In this paper, we propose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm that enables efficient use of multi-scale features in Transformer-based object detectors. The core idea is to exploit sparse multi-scale features from just a few crucial locations, and it is achieved with two novel designs. First, IMFA rearranges the Transformer encoder-decoder pipeline so that the encoded features can be iteratively updated based on the detection predictions. Second, IMFA sparsely samples scale-adaptive features for refined detection from just a few keypoint locations under the guidance of prior detection predictions. As a result, the sampled multi-scale features are sparse yet still highly beneficial for object detection. Extensive experiments show that the proposed IMFA bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#12290;&#20854;&#20013;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26159;&#20027;&#35201;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.10210</link><description>&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#30340;&#34701;&#21512;&#65306;&#31995;&#32479;&#24615;&#26144;&#23556;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Integration of Machine Learning into Automated Test Generation: A Systematic Mapping Study. (arXiv:2206.10210v4 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#12290;&#20854;&#20013;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26159;&#20027;&#35201;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21487;&#33021;&#23454;&#29616;&#26377;&#25928;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#30740;&#31350;&#27979;&#35797;&#23454;&#36341;&#12289;&#30740;&#31350;&#32773;&#30446;&#26631;&#12289;&#24212;&#29992;&#30340;ML&#25216;&#26415;&#12289;&#35780;&#20272;&#21644;&#25361;&#25112;&#31561;&#26041;&#38754;&#30340;&#20852;&#36215;&#30740;&#31350;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#22312;102&#31687;&#35770;&#25991;&#20013;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#26144;&#23556;&#12290;&#32467;&#26524;&#65306;ML&#29983;&#25104;&#31995;&#32479;&#12289;GUI&#12289;&#21333;&#20803;&#12289;&#24615;&#33021;&#21644;&#32452;&#21512;&#27979;&#35797;&#30340;&#36755;&#20837;&#25110;&#25913;&#36827;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#12290;ML&#36824;&#29992;&#20110;&#29983;&#25104;&#27979;&#35797;&#35009;&#20915;&#12289;&#22522;&#20110;&#23646;&#24615;&#30340;&#21644;&#26399;&#26395;&#36755;&#20986;oracle&#12290;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#24120;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#24120;&#22522;&#20110;Q-learning&#65292;&#26159;&#24120;&#35265;&#30340;&#65292;&#26377;&#20123;&#20986;&#29256;&#29289;&#36824;&#37319;&#29992;&#20102;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;(&#21322;/&#26080;)&#30417;&#30563;&#26041;&#27861;&#20351;&#29992;&#20256;&#32479;&#27979;&#35797;&#25351;&#26631;&#21644;ML&#30456;&#20851;&#25351;&#26631;&#65288;&#20363;&#22914;&#20934;&#30830;&#24615;&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#19982;&#22870;&#21169;&#20989;&#25968;&#30456;&#20851;&#30340;&#27979;&#35797;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#35770;&#65306;&#36804;&#20170;&#20026;&#27490;&#30340;&#24037;&#20316;&#26174;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#36824;&#25581;&#31034;&#20102;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Machine learning (ML) may enable effective automated test generation.  Objective: We characterize emerging research, examining testing practices, researcher goals, ML techniques applied, evaluation, and challenges.  Methods: We perform a systematic mapping on a sample of 102 publications.  Results: ML generates input for system, GUI, unit, performance, and combinatorial testing or improves the performance of existing generation methods. ML is also used to generate test verdicts, property-based, and expected output oracles. Supervised learning - often based on neural networks and reinforcement learning - often based on Q-learning - are common, and some publications also employ unsupervised or semi-supervised learning. (Semi-/Un-)Supervised approaches are evaluated using both traditional testing metrics and ML-related metrics (e.g., accuracy), while reinforcement learning is often evaluated using testing metrics tied to the reward function.  Conclusion: Work-to-date shows grea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#38750;&#23545;&#31216;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#38468;&#21152;&#30340;&#40065;&#26834;&#24615;&#36136;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#35813;&#38382;&#39064;&#30340;&#38598;&#20013;&#24335;&#24773;&#20917;&#19979;&#30340;&#39318;&#20010;&#32463;&#36807;&#35777;&#26126;&#30340;&#26377;&#25928;&#30340;UCB&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.03324</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#30340;&#39640;&#25928;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient decentralized multi-agent learning in asymmetric bipartite queueing systems. (arXiv:2206.03324v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03324
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#38750;&#23545;&#31216;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#38468;&#21152;&#30340;&#40065;&#26834;&#24615;&#36136;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#35813;&#38382;&#39064;&#30340;&#38598;&#20013;&#24335;&#24773;&#20917;&#19979;&#30340;&#39318;&#20010;&#32463;&#36807;&#35777;&#26126;&#30340;&#26377;&#25928;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#20010;&#26381;&#21153;&#31995;&#32479;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;N&#20010;&#20195;&#29702;&#20197;&#23436;&#20840;&#20998;&#25955;&#30340;&#26041;&#24335;&#20174;K&#20010;&#26381;&#21153;&#22120;&#35831;&#27714;&#26381;&#21153;&#65292;&#21363;&#19981;&#36827;&#34892;&#36890;&#20449;&#22320;&#36816;&#34892;&#30456;&#21516;&#30340;&#31639;&#27861;&#12290;&#20808;&#21069;&#30340;&#20998;&#25955;&#24335;&#31639;&#27861;&#20165;&#23616;&#38480;&#20110;&#23545;&#31216;&#31995;&#32479;&#65292;&#20854;&#24615;&#33021;&#38543;&#30528;&#26381;&#21153;&#22120;&#25968;&#37327;&#25351;&#25968;&#32423;&#38477;&#20302;&#65292;&#38656;&#35201;&#36890;&#36807;&#20849;&#20139;&#38543;&#26426;&#24615;&#21644;&#21807;&#19968;&#30340;&#20195;&#29702;&#36523;&#20221;&#36827;&#34892;&#36890;&#20449;&#65292;&#19988;&#35745;&#31639;&#37327;&#24040;&#22823;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24403;&#27599;&#20010;&#20195;&#29702;&#20998;&#25955;&#22320;&#36816;&#34892;&#26102;&#65292;&#22312;&#26222;&#36890;&#30340;&#38750;&#23545;&#31216;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#20855;&#22791;&#38468;&#21152;&#30340;&#40065;&#26834;&#24615;&#36136;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20026;&#35813;&#38382;&#39064;&#30340;&#38598;&#20013;&#24335;&#24773;&#20917;&#25552;&#20379;&#20102;&#39318;&#20010;&#32463;&#36807;&#35777;&#26126;&#30340;&#26377;&#25928;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study decentralized multi-agent learning in bipartite queueing systems, a standard model for service systems. In particular, N agents request service from K servers in a fully decentralized way, i.e, by running the same algorithm without communication. Previous decentralized algorithms are restricted to symmetric systems, have performance that is degrading exponentially in the number of servers, require communication through shared randomness and unique agent identities, and are computationally demanding. In contrast, we provide a simple learning algorithm that, when run decentrally by each agent, leads the queueing system to have efficient performance in general asymmetric bipartite queueing systems while also having additional robustness properties. Along the way, we provide the first provably efficient UCB-based algorithm for the centralized case of the problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#65292;&#21457;&#29616;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20256;&#25773;&#21040;&#25968;&#25454;&#30340;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#26550;&#26500;&#24433;&#21709;&#23398;&#20064;&#21644;&#39044;&#27979;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.14258</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Symmetries of Deep Learning Models and their Internal Representations. (arXiv:2205.14258v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#65292;&#21457;&#29616;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20256;&#25773;&#21040;&#25968;&#25454;&#30340;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#26550;&#26500;&#24433;&#21709;&#23398;&#20064;&#21644;&#39044;&#27979;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#26159;&#30740;&#31350;&#22797;&#26434;&#31995;&#32479;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#31216;&#24615;&#24050;&#32463;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#24471;&#21040;&#20102;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#30340;&#19968;&#32452;&#22522;&#26412;&#23545;&#31216;&#32676;&#65292;&#21363;&#27169;&#22411;&#30340;intertwiner groups&#65292;&#23558;&#27169;&#22411;&#26550;&#26500;&#30340;&#23545;&#31216;&#24615;&#19982;&#35813;&#27169;&#22411;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#30456;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#23558;intertwiner groups&#36830;&#25509;&#21040;&#20855;&#26377;&#30456;&#21516;&#26550;&#26500;&#30340;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20256;&#25773;&#21040;&#35813;&#32593;&#32476;&#23545;&#25968;&#25454;&#30340;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#26550;&#26500;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#21644;&#39044;&#27979;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25512;&#27979;&#65292;&#23545;&#20110;ReLU&#32593;&#32476;&#65292;intertwiner groups&#21487;&#33021;&#20026;&#24120;&#35265;&#30340;&#22534;&#21472;&#32593;&#32476;&#32467;&#26500;&#25552;&#20379;&#20102;&#27491;&#24403;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry is a fundamental tool in the exploration of a broad range of complex systems. In machine learning symmetry has been explored in both models and data. In this paper we seek to connect the symmetries arising from the architecture of a family of models with the symmetries of that family's internal representation of data. We do this by calculating a set of fundamental symmetry groups, which we call the intertwiner groups of the model. We connect intertwiner groups to a model's internal representations of data through a range of experiments that probe similarities between hidden states across models with the same architecture. Our work suggests that the symmetries of a network are propagated into the symmetries in that network's representation of data, providing us with a better understanding of how architecture affects the learning and prediction process. Finally, we speculate that for ReLU networks, the intertwiner groups may provide a justification for the common practice of con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#31354;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CoST-GCN&#65289;&#26469;&#35299;&#20915;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#22312;&#32447;&#25512;&#29702;&#35774;&#32622;&#20013;&#24212;&#29992;&#26102;&#37325;&#22797;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26368;&#22810;&#21487;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20943;&#23569;109&#20493;&#12290;</title><link>http://arxiv.org/abs/2203.11009</link><description>&lt;p&gt;
&#25345;&#32493;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Spatio-Temporal Graph Convolutional Networks. (arXiv:2203.11009v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#31354;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CoST-GCN&#65289;&#26469;&#35299;&#20915;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#22312;&#32447;&#25512;&#29702;&#35774;&#32622;&#20013;&#24212;&#29992;&#26102;&#37325;&#22797;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26368;&#22810;&#21487;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20943;&#23569;109&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#25512;&#29702;&#22312;&#39592;&#39612;&#25968;&#25454;&#30340;&#24212;&#29992;&#24050;&#25104;&#20026;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20197;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#30340;&#20808;&#21069;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#22312;&#32447;&#25512;&#29702;&#35774;&#32622;&#20013;&#24212;&#29992;&#20250;&#23548;&#33268;&#37325;&#22797;&#30340;&#35745;&#31639;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#26102;&#31354;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#26500;&#24314;&#20026;&#36830;&#32493;&#25512;&#29702;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20010;&#32593;&#32476;&#21487;&#20197;&#22312;&#26102;&#38388;&#19978;&#36880;&#27493;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#37325;&#22797;&#24103;&#22788;&#29702;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ST-GCN&#32593;&#32476;&#30340;&#36830;&#32493;&#29256;&#26412;CoST-GCN&#65292;&#20197;&#21450;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#34893;&#29983;&#26041;&#27861;&#8212;&#8212;CoAGCN&#21644;CoS-TR&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26435;&#37325;&#20256;&#36882;&#31574;&#30053;&#21644;&#26550;&#26500;&#20462;&#25913;&#20197;&#21152;&#36895;&#25512;&#29702;&#65292;&#24182;&#22312;NTU RGB+D 60&#12289;NTU RGB+D 120&#21644;Kinetics Skeleton 400&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;&#20445;&#25345;&#31867;&#20284;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26102;&#38388;&#22797;&#26434;&#24230;&#26368;&#22810;&#20943;&#23569;&#20102;109&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based reasoning over skeleton data has emerged as a promising approach for human action recognition. However, the application of prior graph-based methods, which predominantly employ whole temporal sequences as their input, to the setting of online inference entails considerable computational redundancy. In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph Convolutional Neural Network as a Continual Inference Network, which can perform step-by-step predictions in time without repeat frame processing. To evaluate our method, we create a continual version of ST-GCN, CoST-GCN, alongside two derived methods with different self-attention mechanisms, CoAGCN and CoS-TR. We investigate weight transfer strategies and architectural modifications for inference acceleration, and perform experiments on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar predictive accuracy, we observe up to 109x reduction in time complexity, on-hard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27700;&#24211;&#35745;&#31639;&#27169;&#22411;EuSN&#65292;&#20854;&#21033;&#29992;&#21069;&#21521;&#27431;&#25289;&#31163;&#25955;&#21270;&#21644;&#21453;&#23545;&#31216;&#24490;&#29615;&#30697;&#38453;&#26469;&#35774;&#35745;&#27700;&#24211;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25509;&#36817;&#31283;&#23450;&#36793;&#32536;&#30340;&#39281;&#21644;&#26377;&#25928;&#35889;&#21322;&#24452;&#21644;&#38646;&#23616;&#37096;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#12290;&#22312;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.09382</link><description>&lt;p&gt;
Euler State Networks: &#38750;&#32791;&#25955;&#24615;&#27700;&#24211;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Euler State Networks: Non-dissipative Reservoir Computing. (arXiv:2203.09382v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27700;&#24211;&#35745;&#31639;&#27169;&#22411;EuSN&#65292;&#20854;&#21033;&#29992;&#21069;&#21521;&#27431;&#25289;&#31163;&#25955;&#21270;&#21644;&#21453;&#23545;&#31216;&#24490;&#29615;&#30697;&#38453;&#26469;&#35774;&#35745;&#27700;&#24211;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25509;&#36817;&#31283;&#23450;&#36793;&#32536;&#30340;&#39281;&#21644;&#26377;&#25928;&#35889;&#21322;&#24452;&#21644;&#38646;&#23616;&#37096;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#12290;&#22312;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#21040;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#35299;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27700;&#24211;&#35745;&#31639;(EuSN)&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21521;&#27431;&#25289;&#31163;&#25955;&#21270;&#21644;&#21453;&#23545;&#31216;&#24490;&#29615;&#30697;&#38453;&#26469;&#35774;&#35745;&#27700;&#24211;&#21160;&#21147;&#23398;&#12290;&#35813;&#27169;&#22411;&#30340;&#25968;&#23398;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#20855;&#26377;&#25509;&#36817;&#31283;&#23450;&#36793;&#32536;&#30340;&#39281;&#21644;&#26377;&#25928;&#35889;&#21322;&#24452;&#21644;&#38646;&#23616;&#37096;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#12290;&#23545;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#27700;&#24211;&#35745;&#31639;&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38656;&#35201;&#22810;&#20010;&#26102;&#27493;&#26377;&#25928;&#20256;&#25773;&#36755;&#20837;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;EuSN&#33021;&#22815;&#21305;&#37197;&#29978;&#33267;&#36229;&#36807;&#21487;&#35757;&#32451;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27700;&#24211;&#35745;&#31639;&#23478;&#26063;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the numerical solution of ordinary differential equations, in this paper we propose a novel Reservoir Computing (RC) model, called the Euler State Network (EuSN). The presented approach makes use of forward Euler discretization and antisymmetric recurrent matrices to design reservoir dynamics that are both stable and non-dissipative by construction.  Our mathematical analysis shows that the resulting model is biased towards a unitary effective spectral radius and zero local Lyapunov exponents, intrinsically operating near to the edge of stability. Experiments on long-term memory tasks show the clear superiority of the proposed approach over standard RC models in problems requiring effective propagation of input information over multiple time-steps. Furthermore, results on time-series classification benchmarks indicate that EuSN is able to match (or even exceed) the accuracy of trainable Recurrent Neural Networks, while retaining the training efficiency of the RC family, res
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20915;&#31574;&#30340;&#31232;&#30095;&#25915;&#20987;&#26041;&#27861;SparseEvo&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25200;&#21160;&#30340;&#20687;&#32032;&#25968;&#37327;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27450;&#39575;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#23427;&#30340;&#20869;&#37096;&#26500;&#36896;&#12290;</title><link>http://arxiv.org/abs/2202.00091</link><description>&lt;p&gt;
&#22522;&#20110;&#20915;&#31574;&#30340;&#31232;&#30095;&#25915;&#20987;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models. (arXiv:2202.00091v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00091
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20915;&#31574;&#30340;&#31232;&#30095;&#25915;&#20987;&#26041;&#27861;SparseEvo&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25200;&#21160;&#30340;&#20687;&#32032;&#25968;&#37327;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27450;&#39575;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#23427;&#30340;&#20869;&#37096;&#26500;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#20570;&#20102;&#26368;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24494;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#21046;&#20316;&#40657;&#30418;&#27169;&#22411;&#30340;&#23545;&#25239;&#25200;&#21160;&#26159;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25110;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#20844;&#24320;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290; &#31232;&#30095;&#25915;&#20987;&#23588;&#20854;&#21463;&#21040;&#20851;&#27880;&#12290; &#31232;&#30095;&#25915;&#20987;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#23454;&#29616;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27604;&#25105;&#20204;&#24819;&#35937;&#30340;&#26356;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#30340;&#31232;&#30095;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25200;&#21160;&#30340;&#20687;&#32032;&#25968;&#37327;&#65292;&#26377;&#25928;&#22320;&#27450;&#39575;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#23427;&#30340;&#20869;&#37096;&#26500;&#36896;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#30340;SparseEvo&#65292;&#29992;&#20110;&#35299;&#20915;NP&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a machine learning model to craft adversarial perturbations to black-box models is a practical threat against real-world systems, such as autonomous cars or machine learning models exposed as a service (MLaaS). Of particular interest are sparse attacks. The realization of sparse attacks in black-box models demonstrates that machine learning models are more vulnerable than we believe. Because these attacks aim to minimize the number of perturbed pixels measured by l_0 norm-required to mislead a model by solely observing the decision (the predicted label) returned to a model query; the so-called decision-based attack setting. But, such an attack leads to an NP-hard optimization problem. We develop an evolution-based algorithm-SparseEvo-for the problem and evaluate against both convolutional deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32467;&#26500;&#21270;&#32422;&#26463;&#30340;&#20998;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#36880;&#22352;&#26631;&#38745;&#24577;&#28857;&#12290;</title><link>http://arxiv.org/abs/2201.12691</link><description>&lt;p&gt;
&#20998;&#25968;&#35268;&#21010;&#30340;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Coordinate Descent Methods for Fractional Minimization. (arXiv:2201.12691v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32467;&#26500;&#21270;&#32422;&#26463;&#30340;&#20998;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#36880;&#22352;&#26631;&#38745;&#24577;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31867;&#32467;&#26500;&#21270;&#30340;&#20998;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#30340;&#20998;&#23376;&#37096;&#20998;&#26159;&#19968;&#20010;&#21487;&#24494;&#30340;&#20984;&#20989;&#25968;&#21644;&#19968;&#20010;&#20984;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#21644;&#65292;&#32780;&#20998;&#27597;&#37096;&#20998;&#26159;&#19968;&#20010;&#20984;&#25110;&#20985;&#20989;&#25968;&#12290;&#30001;&#20110;&#35813;&#38382;&#39064;&#26159;&#38750;&#20984;&#30340;&#65292;&#25152;&#20197;&#24456;&#38590;&#35299;&#20915;&#12290;&#36890;&#36807;&#21033;&#29992;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36845;&#20195;&#22320;&#35299;&#20915;&#19968;&#20010;&#19968;&#32500;&#23376;&#38382;&#39064;&#65292;&#32780;&#19988;&#20445;&#35777;&#25910;&#25947;&#21040;&#36880;&#22352;&#26631;&#38745;&#27490;&#28857;&#12290;&#22312;&#20998;&#27597;&#20026;&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24369;&#23616;&#37096;&#26377;&#30028;&#38750;&#20984;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36880;&#22352;&#26631;&#38745;&#24577;&#28857;&#30340;&#26368;&#20248;&#24615;&#27604;&#26631;&#20934;&#20020;&#30028;&#28857;&#21644;&#26041;&#21521;&#24615;&#28857;&#30340;&#26368;&#20248;&#24615;&#26356;&#24378;&#12290;&#22312;&#28385;&#36275;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#20197;Q-&#32447;&#24615;&#26041;&#24335;&#25910;&#25947;&#21040;&#36880;&#22352;&#26631;&#38745;&#24577;&#28857;&#12290;&#22312;&#20998;&#27597;&#20026;&#20985;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#20020;&#30028;&#28857;&#37117;&#26159;&#36880;&#22352;&#26631;&#26368;&#20248;&#28857;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a class of structured fractional minimization problems, in which the numerator part of the objective is the sum of a differentiable convex function and a convex non-smooth function, while the denominator part is a convex or concave function. This problem is difficult to solve since it is non-convex. By exploiting the structure of the problem, we propose two Coordinate Descent (CD) methods for solving this problem. The proposed methods iteratively solve a one-dimensional subproblem \textit{globally}, and they are guaranteed to converge to coordinate-wise stationary points. In the case of a convex denominator, under a weak \textit{locally bounded non-convexity condition}, we prove that the optimality of coordinate-wise stationary point is stronger than that of the standard critical point and directional point. Under additional suitable conditions, CD methods converge Q-linearly to coordinate-wise stationary points. In the case of a concave denominator, we show that any critic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24335;RBMLE-UCB&#31639;&#27861;&#65292;&#23558;RBMLE&#26041;&#27861;&#30340;&#24809;&#32602;&#19982;UCB&#26041;&#27861;&#30340;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#33258;&#36866;&#24212;LQ&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#22122;&#22768;&#36739;&#39640;&#25110;&#26679;&#26412;&#25968;&#36739;&#23569;&#26102;&#65292;&#35813;&#31639;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2201.10542</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#24378;&#24335;RBMLE-UCB&#26041;&#27861;&#30340;&#32447;&#24615;&#20108;&#27425;&#31995;&#32479;&#33258;&#36866;&#24212;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Augmented RBMLE-UCB Approach for Adaptive Control of Linear Quadratic Systems. (arXiv:2201.10542v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24335;RBMLE-UCB&#31639;&#27861;&#65292;&#23558;RBMLE&#26041;&#27861;&#30340;&#24809;&#32602;&#19982;UCB&#26041;&#27861;&#30340;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#33258;&#36866;&#24212;LQ&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#22122;&#22768;&#36739;&#39640;&#25110;&#26679;&#26412;&#25968;&#36739;&#23569;&#26102;&#65292;&#35813;&#31639;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#25511;&#21046;&#19968;&#20010;&#26410;&#30693;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#20108;&#27425;&#20195;&#20215;&#65292;&#21363;&#33258;&#36866;&#24212;LQ&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#22870;&#21169;&#20559;&#21521;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#8221;(RBMLE) &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;40&#22810;&#24180;&#21069;&#65292;&#22312;&#8220;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#27861;&#8221;(UCB)&#26041;&#27861;&#21644;&#38024;&#23545;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#8220;&#36951;&#25022;&#8221;&#30340;&#23450;&#20041;&#20043;&#21069;&#12290;&#23427;&#31616;&#21333;&#22320;&#20026;&#21442;&#25968;&#20272;&#35745;&#30340;&#26631;&#20934;&#28155;&#21152;&#20102;&#19968;&#20010;&#20542;&#21521;&#20110;&#20855;&#26377;&#26356;&#22823;&#22870;&#21169;&#30340;&#21442;&#25968;&#30340;&#39033;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21327;&#35843;RBMLE&#21644;UCB&#26041;&#27861;&#65292;&#24182;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;RBMLE-UCB&#31639;&#27861;&#65292;&#23427;&#23558;RBMLE&#26041;&#27861;&#30340;&#24809;&#32602;&#19982;UCB&#26041;&#27861;&#30340;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#23558;&#20004;&#31181;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#36215;&#26469;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20445;&#25345;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;$\Tilde{\mathcal{O}}(\sqrt{T})$&#36951;&#25022;&#20540;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#22686;&#24378;&#24335;RBMLE-UCB&#21644;&#26631;&#20934;RBMLE(&#19981;&#24102;&#22686;&#24378;UCB&#26041;&#27861;)&#22312;&#27169;&#25311;&#31995;&#32479;&#19978;&#30340;&#23454;&#35777;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#22686;&#24378;&#31639;&#27861;&#22312;&#22122;&#22768;&#36739;&#39640;&#25110;&#26679;&#26412;&#25968;&#36739;&#23569;&#26102;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of controlling an unknown stochastic linear system with quadratic costs - called the adaptive LQ control problem. We re-examine an approach called ''Reward Biased Maximum Likelihood Estimate'' (RBMLE) that was proposed more than forty years ago, and which predates the ''Upper Confidence Bound'' (UCB) method as well as the definition of ''regret'' for bandit problems. It simply added a term favoring parameters with larger rewards to the criterion for parameter estimation. We show how the RBMLE and UCB methods can be reconciled, and thereby propose an Augmented RBMLE-UCB algorithm that combines the penalty of the RBMLE method with the constraints of the UCB method, uniting the two approaches to optimism in the face of uncertainty. We establish that theoretically, this method retains $\Tilde{\mathcal{O}}(\sqrt{T})$ regret, the best-known so far. We further compare the empirical performance of the proposed Augmented RBMLE-UCB and the standard RBMLE (without the augm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#22270;&#21644;&#24352;&#37327;&#31215;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#20986;&#19968;&#31181;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21040;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26032;&#22411;&#26144;&#23556;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2201.05158</link><description>&lt;p&gt;
&#20998;&#35299;&#24335;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Decompositional Quantum Graph Neural Network. (arXiv:2201.05158v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#22270;&#21644;&#24352;&#37327;&#31215;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#20986;&#19968;&#31181;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21040;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26032;&#22411;&#26144;&#23556;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#31639;&#27861;&#21644;&#37327;&#23376;&#35745;&#31639;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#30001;&#20110;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#32570;&#20047;&#21644;&#32570;&#20047;&#26377;&#25928;&#30340;&#23558;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20174;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#26144;&#23556;&#21040;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#37327;&#23376;&#31867;&#27604;&#25110;&#36807;&#31243;&#27169;&#25311;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#37327;&#23376;&#27604;&#29305;&#30340;&#20855;&#20307;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22522;&#20110;&#33258;&#25105;&#22270;&#30340;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;egoQGNN&#65289;&#12290;egoQGNN&#20351;&#29992;&#24352;&#37327;&#31215;&#21644;&#21333;&#20301;&#30697;&#38453;&#34920;&#31034;&#23454;&#29616;&#20102;GNN&#29702;&#35770;&#26694;&#26550;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12290;&#24403;&#34987;&#32463;&#20856;&#35745;&#31639;&#26426;&#25511;&#21046;&#26102;&#65292;egoQGNN&#21487;&#20197;&#36890;&#36807;&#20174;&#36755;&#20837;&#22270;&#30340;&#33258;&#25105;&#22270;&#20013;&#22788;&#29702;&#26469;&#23481;&#32435;&#20219;&#24847;&#22823;&#23567;&#30340;&#22270;&#65292;&#20351;&#29992;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#30340;&#37327;&#23376;&#35774;&#22791;&#12290;&#35813;&#20307;&#31995;&#32467;&#26500;&#22522;&#20110;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21040;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26032;&#22411;&#26144;&#23556;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is a fast-emerging field that aims to tackle machine learning using quantum algorithms and quantum computing. Due to the lack of physical qubits and an effective means to map real-world data from Euclidean space to Hilbert space, most of these methods focus on quantum analogies or process simulations rather than devising concrete architectures based on qubits. In this paper, we propose a novel hybrid quantum-classical algorithm for graph-structured data, which we refer to as the Ego-graph based Quantum Graph Neural Network (egoQGNN). egoQGNN implements the GNN theoretical framework using the tensor product and unity matrix representation, which greatly reduces the number of model parameters required. When controlled by a classical computer, egoQGNN can accommodate arbitrarily sized graphs by processing ego-graphs from the input graph using a modestly-sized quantum device. The architecture is based on a novel mapping from real-world data to Hilbert space. This m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#20351;&#29992;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#21462;&#20195;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;DeBERTaV3&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.09543</link><description>&lt;p&gt;
DeBERTaV3&#65306;&#20351;&#29992;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#30340;ELECTRA&#39118;&#26684;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;DeBERTa&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. (arXiv:2111.09543v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#20351;&#29992;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#21462;&#20195;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;DeBERTaV3&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#23427;&#36890;&#36807;&#23558;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#26367;&#25442;&#20026;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#26469;&#25913;&#36827;&#21407;&#22987;&#30340;DeBERTa&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ELECTRA&#20013;&#30340;&#39321;&#33609;&#23884;&#20837;&#20849;&#20139;&#20250;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25439;&#22833;&#23558;&#20196;&#29260;&#23884;&#20837;&#25289;&#21521;&#19981;&#21516;&#30340;&#26041;&#21521;&#65292;&#20250;&#36896;&#25104;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;DeBERTa&#30456;&#21516;&#30340;&#35774;&#32622;&#39044;&#35757;&#32451;&#20102;DeBERTaV3&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;&#20197;&#20843;&#39033;&#20219;&#21153;&#20026;&#20363;&#30340;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;DeBERTaV3 Large&#27169;&#22411;&#24179;&#22343;&#24471;&#20998;&#20026;91.37&#65285;&#65292;&#27604;D&#39640;1.37&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#31895;&#31961;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21482;&#38656;&#36275;&#22815;&#20449;&#24687;&#30340;&#31895;&#26631;&#31614;&#21363;&#21487;&#22312;&#20174;&#32454;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#20219;&#20309;&#38382;&#39064;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2108.09805</link><description>&lt;p&gt;
&#23398;&#20064;&#31895;&#26631;&#31614;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for Learning from Coarse Labels. (arXiv:2108.09805v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.09805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#31895;&#31961;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21482;&#38656;&#36275;&#22815;&#20449;&#24687;&#30340;&#31895;&#26631;&#31614;&#21363;&#21487;&#22312;&#20174;&#32454;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#20219;&#20309;&#38382;&#39064;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#32454;&#31890;&#24230;&#26631;&#31614;&#20449;&#24687;&#65307;&#20363;&#22914;&#65292;&#26681;&#25454;&#27880;&#37322;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#19968;&#24133;&#22270;&#20687;&#21487;&#20197;&#34987;&#26631;&#35760;&#20026;&#21704;&#22763;&#22855;&#12289;&#29399;&#29978;&#33267;&#21160;&#29289;&#12290;&#26412;&#25991;&#23545;&#36825;&#20123;&#24773;&#20917;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#24182;&#30740;&#31350;&#20102;&#20174;&#27492;&#31867;&#31895;&#25968;&#25454;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#19981;&#26159;&#35266;&#23519;&#26469;&#33258;&#38598;&#21512; $\mathcal{Z}$ &#30340;&#23454;&#38469;&#26631;&#31614;&#65292;&#32780;&#26159;&#35266;&#23519;&#23545;&#24212;&#20110; $\mathcal{Z}$ &#30340;&#21010;&#20998;&#65288;&#25110;&#22810;&#31181;&#21010;&#20998;&#30340;&#28151;&#21512;&#65289;&#30340;&#31895;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#31639;&#27861;&#32467;&#26524;&#26159;&#65306;&#20960;&#20046;&#20219;&#20309;&#21487;&#20174;&#32454;&#31890;&#24230;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24403;&#31895;&#25968;&#25454;&#36275;&#22815;&#20449;&#24687;&#26102;&#20063;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#27867;&#21270;&#20943;&#23569;&#22238;&#31572;&#32479;&#35745;&#26597;&#35810;&#65288;SQ) &#30340;&#26041;&#24335;&#26469;&#33719;&#24471;&#32467;&#26524;&#65292;&#21482;&#32473;&#20986;&#31895;&#26631;&#31614;&#32780;&#19981;&#26159;&#32454;&#26631;&#31614;&#12290;&#25152;&#38656;&#31895;&#26631;&#31614;&#25968;&#37327;&#19982;&#31895;&#31961;&#21270;&#20449;&#24687;&#22833;&#30495;&#21644;&#32454;&#26631;&#31614;&#25968;&#37327; $|\mathcal{Z}|$ &#25104;&#22810;&#39033;&#24335;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#65288;&#26080;&#38480;&#22810;&#20010;&#65289;&#23454;&#20540;&#26631;&#31614;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many learning problems one may not have access to fine grained label information; e.g., an image can be labeled as husky, dog, or even animal depending on the expertise of the annotator. In this work, we formalize these settings and study the problem of learning from such coarse data. Instead of observing the actual labels from a set $\mathcal{Z}$, we observe coarse labels corresponding to a partition of $\mathcal{Z}$ (or a mixture of partitions).  Our main algorithmic result is that essentially any problem learnable from fine grained labels can also be learned efficiently when the coarse data are sufficiently informative. We obtain our result through a generic reduction for answering Statistical Queries (SQ) over fine grained labels given only coarse labels. The number of coarse labels required depends polynomially on the information distortion due to coarsening and the number of fine labels $|\mathcal{Z}|$.  We also investigate the case of (infinitely many) real valued labels foc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#30693;&#36172;&#21338;&#26426;&#21442;&#25968;&#30340;&#26412;&#22320;&#32858;&#31867;&#31574;&#30053;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#29992;&#25143;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#20869;&#23481;&#25512;&#33616;&#21644;&#23450;&#21521;&#24191;&#21578;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2103.00063</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#26412;&#22320;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Local Clustering in Contextual Multi-Armed Bandits. (arXiv:2103.00063v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#30693;&#36172;&#21338;&#26426;&#21442;&#25968;&#30340;&#26412;&#22320;&#32858;&#31867;&#31574;&#30053;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#29992;&#25143;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#20869;&#23481;&#25512;&#33616;&#21644;&#23450;&#21521;&#24191;&#21578;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#20013;&#35782;&#21035;&#29992;&#25143;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;&#19978;&#19979;&#25991;MAB&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#20363;&#22914;&#20869;&#23481;&#25512;&#33616;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#29992;&#25143;&#20381;&#36182;&#20851;&#31995;&#22312;&#29992;&#25143;&#30340;&#34892;&#20026;&#21644;&#22870;&#21169;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23545;&#31867;&#20284;&#29992;&#25143;&#36827;&#34892;&#32858;&#31867;&#21487;&#20197;&#25552;&#39640;&#22870;&#21169;&#20272;&#35745;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#20869;&#23481;&#25512;&#33616;&#21644;&#23450;&#21521;&#24191;&#21578;&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#32858;&#31867;&#35774;&#32622;&#65292;&#25105;&#20204;&#22522;&#20110;&#26410;&#30693;&#30340;&#36172;&#21338;&#26426;&#21442;&#25968;&#23545;&#29992;&#25143;&#36827;&#34892;&#32858;&#31867;&#65292;&#36825;&#20123;&#21442;&#25968;&#23558;&#34987;&#36880;&#27493;&#20272;&#35745;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19978;&#19979;&#25991;MAB&#20013;&#30340;&#32858;&#31867;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#26412;&#22320;&#32858;&#31867;&#36807;&#31243;&#30340;&#36172;&#21338;&#31639;&#27861;LOCB&#12290;&#25105;&#20204;&#23545;LOCB&#30340;&#32858;&#31867;&#27491;&#30830;&#24615;&#21644;&#25928;&#29575;&#20197;&#21450;&#20854;&#36951;&#25022;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20174;&#21508;&#20010;&#26041;&#38754;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study identifying user clusters in contextual multi-armed bandits (MAB). Contextual MAB is an effective tool for many real applications, such as content recommendation and online advertisement. In practice, user dependency plays an essential role in the user's actions, and thus the rewards. Clustering similar users can improve the quality of reward estimation, which in turn leads to more effective content recommendation and targeted advertising. Different from traditional clustering settings, we cluster users based on the unknown bandit parameters, which will be estimated incrementally. In particular, we define the problem of cluster detection in contextual MAB, and propose a bandit algorithm, LOCB, embedded with local clustering procedure. And, we provide theoretical analysis about LOCB in terms of the correctness and efficiency of clustering and its regret bound. Finally, we evaluate the proposed algorithm from various aspects, which outperforms state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;UCB&#26368;&#20248;&#25289;&#33218;&#31574;&#30053;&#65292;&#25104;&#26412;&#20026;$\sqrt{\log T}$&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#27492;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2008.09312</link><description>&lt;p&gt;
UCB Bandits&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#25915;&#20987;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.09312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;UCB&#26368;&#20248;&#25289;&#33218;&#31574;&#30053;&#65292;&#25104;&#26412;&#20026;$\sqrt{\log T}$&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#27492;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#22351;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#36890;&#36807;&#25805;&#20316;UCB&#21407;&#21017;&#26469;&#25289;&#21160;&#19968;&#20123;&#38750;&#26368;&#20248;&#30446;&#26631;&#33218;$T-o(T)$&#27425;&#65292;&#32047;&#31215;&#25104;&#26412;&#30340;&#26631;&#24230;&#20026;$\sqrt{\log T}$&#65292;&#20854;&#20013;$T$&#20026;&#22238;&#21512;&#25968;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#32047;&#31215;&#25915;&#20987;&#25104;&#26412;&#30340;&#31532;&#19968;&#20010;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#19982;&#25105;&#20204;&#30340;&#19978;&#30028;&#21305;&#37197;&#65292;&#38500;&#20102;$\log\log T$&#22240;&#23376;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a stochastic multi-arm bandit problem where rewards are subject to adversarial corruption. We propose a novel attack strategy that manipulates a UCB principle into pulling some non-optimal target arm $T - o(T)$ times with a cumulative cost that scales as $\sqrt{\log T}$, where $T$ is the number of rounds. We also prove the first lower bound on the cumulative attack cost. Our lower bound matches our upper bound up to $\log \log T$ factors, showing our attack to be near optimal.
&lt;/p&gt;</description></item></channel></rss>