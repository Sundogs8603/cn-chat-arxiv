<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;RNN&#35757;&#32451;&#20013;&#30340;&#20998;&#27495;&#29616;&#35937;&#21644;&#25439;&#22833;&#36339;&#36291;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;RNN&#20013;&#23384;&#22312;&#30528;&#26576;&#20123;&#20998;&#27495;&#12290;</title><link>http://arxiv.org/abs/2310.17561</link><description>&lt;p&gt;
RNN&#35757;&#32451;&#20013;&#30340;&#20998;&#27495;&#21644;&#25439;&#22833;&#36339;&#36291;
&lt;/p&gt;
&lt;p&gt;
Bifurcations and loss jumps in RNN training. (arXiv:2310.17561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17561
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;RNN&#35757;&#32451;&#20013;&#30340;&#20998;&#27495;&#29616;&#35937;&#21644;&#25439;&#22833;&#36339;&#36291;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;RNN&#20013;&#23384;&#22312;&#30528;&#26576;&#20123;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26159;&#29992;&#20110;&#24314;&#27169;&#21644;&#39044;&#27979;&#24207;&#21015;&#25968;&#25454;&#20197;&#21450;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#25512;&#26029;&#21160;&#21147;&#31995;&#32479;&#65288;DS&#65289;&#30340;&#24120;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#12290;DS&#29702;&#35770;&#30340;&#27010;&#24565;&#24050;&#34987;&#29992;&#20110;&#36827;&#19968;&#27493;&#29702;&#35299;&#32463;&#36807;&#35757;&#32451;&#30340;RNN&#22914;&#20309;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#20197;&#21450;&#35757;&#32451;&#36807;&#31243;&#26412;&#36523;&#12290;&#20998;&#27495;&#26159;DS&#20013;&#29305;&#21035;&#37325;&#35201;&#30340;&#29616;&#35937;&#65292;&#21253;&#25324;RNN&#65292;&#22312;&#31995;&#32479;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#21442;&#25968;&#21464;&#21270;&#26102;&#65292;&#25351;&#31995;&#32479;&#30340;&#21160;&#21147;&#34892;&#20026;&#30340;&#25299;&#25169;&#65288;&#23450;&#24615;&#65289;&#21464;&#21270;&#12290;&#20102;&#35299;RNN&#30340;&#20998;&#27495;&#32467;&#26500;&#23558;&#26377;&#21161;&#20110;&#25512;&#26029;&#20854;&#35768;&#22810;&#35745;&#31639;&#21644;&#21160;&#21147;&#23646;&#24615;&#65292;&#20363;&#22914;&#23545;&#21442;&#25968;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#25110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#26159;&#65292;&#20998;&#27495;&#21487;&#33021;&#35299;&#37322;RNN&#35757;&#32451;&#20013;&#35266;&#23519;&#21040;&#30340;&#31361;&#28982;&#25439;&#22833;&#36339;&#36291;&#65292;&#36825;&#21487;&#33021;&#20005;&#37325;&#38459;&#30861;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#20808;&#25968;&#23398;&#22320;&#35777;&#26126;&#20102;&#38024;&#23545;&#19968;&#31867;&#22522;&#20110;ReLU&#30340;RNN&#65292;&#30830;&#23454;&#23384;&#22312;&#19968;&#20123;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#25163;&#26426;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#21305;&#37197;&#32858;&#31867;&#20013;&#24515;&#19982;&#25163;&#26426;&#23884;&#20837;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#21644;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#65292;&#23454;&#39564;&#35777;&#26126;&#21305;&#37197;&#32467;&#26524;&#25429;&#25417;&#21040;&#20102;&#25163;&#26426;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#25163;&#26426;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17558</link><description>&lt;p&gt;
&#23454;&#29616;&#25163;&#26426;&#19982;&#35821;&#38899;&#34920;&#31034;&#30340;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Towards Matching Phones and Speech Representations. (arXiv:2310.17558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#25163;&#26426;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#21305;&#37197;&#32858;&#31867;&#20013;&#24515;&#19982;&#25163;&#26426;&#23884;&#20837;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#21644;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#65292;&#23454;&#39564;&#35777;&#26126;&#21305;&#37197;&#32467;&#26524;&#25429;&#25417;&#21040;&#20102;&#25163;&#26426;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#25163;&#26426;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25163;&#26426;&#31867;&#22411;&#20174;&#25163;&#26426;&#23454;&#20363;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#20294;&#20173;&#28982;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#25552;&#20986;&#20026;&#23558;&#32858;&#31867;&#20013;&#24515;&#19982;&#25163;&#26426;&#23884;&#20837;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#23427;&#20204;&#20351;&#21305;&#37197;&#25104;&#20026;&#21487;&#33021;&#65292;&#21363;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#32858;&#31867;&#20013;&#24515;&#26159;&#21542;&#20943;&#23569;&#20102;&#25163;&#26426;&#23454;&#20363;&#30340;&#21464;&#21270;&#24615;&#24182;&#19988;&#26159;&#21542;&#23562;&#37325;&#25163;&#26426;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21305;&#37197;&#32467;&#26524;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#33258;&#30417;&#30563;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21305;&#37197;&#32467;&#26524;&#25429;&#25417;&#21040;&#20102;&#25163;&#26426;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23558;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#19982;APC&#21644;CPC&#31561;&#24120;&#35268;&#30340;&#33258;&#30417;&#30563;&#25439;&#22833;&#19968;&#36215;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#25163;&#26426;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#38459;&#23612;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#30340;&#39640;&#25928;&#25968;&#20540;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Cholesky&#20998;&#35299;&#65292;&#35299;&#20915;&#20102;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#26679;&#26412;&#25968;&#37327;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2310.17556</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#38459;&#23612;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#30340;&#39640;&#25928;&#25968;&#20540;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Numerical Algorithm for Large-Scale Damped Natural Gradient Descent. (arXiv:2310.17556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17556
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#38459;&#23612;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#30340;&#39640;&#25928;&#25968;&#20540;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Cholesky&#20998;&#35299;&#65292;&#35299;&#20915;&#20102;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#26679;&#26412;&#25968;&#37327;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36229;&#36807;&#21487;&#29992;&#26679;&#26412;&#25968;&#37327;&#30340;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#39640;&#25928;&#27714;&#35299;&#38459;&#23612;Fisher&#30697;&#38453;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#37325;&#26500;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;Cholesky&#20998;&#35299;&#65292;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#22522;&#20934;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#26126;&#26174;&#24555;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new algorithm for efficiently solving the damped Fisher matrix in large-scale scenarios where the number of parameters significantly exceeds the number of available samples. This problem is fundamental for natural gradient descent and stochastic reconfiguration. Our algorithm is based on Cholesky decomposition and is generally applicable. Benchmark results show that the algorithm is significantly faster than existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;OLAF&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#26085;&#24120;&#29992;&#25143;&#36890;&#36807;&#21475;&#22836;&#32416;&#27491;&#25945;&#25480;&#26426;&#22120;&#20154;&#65292;&#24182;&#33021;&#26681;&#25454;&#21475;&#22836;&#21453;&#39304;&#26356;&#26032;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#36816;&#21160;&#31070;&#32463;&#31574;&#30053;&#65292;&#20174;&#32780;&#36991;&#20813;&#37325;&#22797;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#29289;&#29702;&#30828;&#20214;&#19978;&#65292;&#35813;&#31995;&#32479;&#22312;&#38271;&#26102;&#38388;&#32447;&#30340;&#25805;&#32437;&#20219;&#21153;&#20013;&#24179;&#22343;&#25913;&#21892;&#20102;20.0%&#30340;&#31574;&#30053;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.17555</link><description>&lt;p&gt;
&#20174;&#21475;&#22836;&#32416;&#27491;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interactive Robot Learning from Verbal Correction. (arXiv:2310.17555v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;OLAF&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#26085;&#24120;&#29992;&#25143;&#36890;&#36807;&#21475;&#22836;&#32416;&#27491;&#25945;&#25480;&#26426;&#22120;&#20154;&#65292;&#24182;&#33021;&#26681;&#25454;&#21475;&#22836;&#21453;&#39304;&#26356;&#26032;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#36816;&#21160;&#31070;&#32463;&#31574;&#30053;&#65292;&#20174;&#32780;&#36991;&#20813;&#37325;&#22797;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#29289;&#29702;&#30828;&#20214;&#19978;&#65292;&#35813;&#31995;&#32479;&#22312;&#38271;&#26102;&#38388;&#32447;&#30340;&#25805;&#32437;&#20219;&#21153;&#20013;&#24179;&#22343;&#25913;&#21892;&#20102;20.0%&#30340;&#31574;&#30053;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25105;&#20204;&#23558;&#26426;&#22120;&#20154;&#35774;&#35745;&#25104;&#33021;&#22312;&#23478;&#24237;&#31561;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#23398;&#20064;&#21644;&#25913;&#36827;&#34892;&#20026;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;OLAF&#30340;&#26032;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#26085;&#24120;&#29992;&#25143;&#36890;&#36807;&#21475;&#22836;&#32416;&#27491;&#25945;&#25480;&#26426;&#22120;&#20154;&#65292;&#24403;&#26426;&#22120;&#20154;&#29359;&#38169;&#35823;&#26102;&#65292;&#20363;&#22914;&#35828;&#8220;&#20572;&#19979;&#20320;&#27491;&#22312;&#20570;&#30340;&#20107;&#24773;&#12290;&#20320;&#24212;&#35813;&#38752;&#36817;&#26479;&#23376;&#12290;&#8221; OLAF&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#23427;&#33021;&#22815;&#26681;&#25454;&#21475;&#22836;&#21453;&#39304;&#26356;&#26032;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#36816;&#21160;&#31070;&#32463;&#31574;&#30053;&#65292;&#20197;&#36991;&#20813;&#23558;&#26469;&#37325;&#22797;&#38169;&#35823;&#12290;&#36825;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#20165;&#20165;&#36981;&#24490;&#21475;&#22836;&#21629;&#20196;&#25110;&#32416;&#27491;&#65292;&#32780;&#19981;&#20250;&#20174;&#20013;&#23398;&#20064;&#12290;&#22312;&#27169;&#25311;&#21644;&#29289;&#29702;&#30828;&#20214;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#29992;&#25143;&#25945;&#26426;&#22120;&#20154;&#25191;&#34892;&#38271;&#26102;&#38388;&#32447;&#30340;&#25805;&#32437;&#20219;&#21153;&#65292;&#22312;&#31574;&#30053;&#25104;&#21151;&#29575;&#26041;&#38754;&#24179;&#22343;&#25913;&#21892;&#20102;20.0%&#12290;&#35270;&#39057;&#21644;&#26356;&#22810;&#32467;&#26524;&#21487;&#22312;ht&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn and refine behavior after deployment has become ever more important for robots as we design them to operate in unstructured environments like households. In this work, we design a new learning system based on large language model (LLM), OLAF, that allows everyday users to teach a robot using verbal corrections when the robot makes mistakes, e.g., by saying "Stop what you're doing. You should move closer to the cup." A key feature of OLAF is its ability to update the robot's visuomotor neural policy based on the verbal feedback to avoid repeating mistakes in the future. This is in contrast to existing LLM-based robotic systems, which only follow verbal commands or corrections but not learn from them. We demonstrate the efficacy of our design in experiments where a user teaches a robot to perform long-horizon manipulation tasks both in simulation and on physical hardware, achieving on average 20.0% improvement in policy success rate. Videos and more results are at ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#30417;&#25511;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37096;&#32626;&#25968;&#25454;&#26469;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#24182;&#39044;&#27979;&#25925;&#38556;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17552</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#30417;&#25511;&#19982;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Based Runtime Monitoring with Interactive Imitation Learning. (arXiv:2310.17552v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#30417;&#25511;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37096;&#32626;&#25968;&#25454;&#26469;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#24182;&#39044;&#27979;&#25925;&#38556;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#26041;&#27861;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#30340;&#25361;&#25112;&#20173;&#28982;&#21046;&#32422;&#30528;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26080;&#27861;&#26816;&#27979;&#21644;&#35299;&#20915;&#28508;&#22312;&#25925;&#38556;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#31995;&#32479;&#26080;&#27861;&#22312;&#39640;&#39118;&#38505;&#20219;&#21153;&#20013;&#25237;&#20837;&#20351;&#29992;&#12290;&#26368;&#36817;&#22312;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23433;&#20840;&#25805;&#20316;&#24182;&#22312;&#38271;&#26399;&#37096;&#32626;&#20013;&#19981;&#26029;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25345;&#32493;&#30340;&#20154;&#24037;&#30417;&#30563;&#21644;&#39044;&#20808;&#30340;&#21453;&#39304;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36171;&#20104;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#30417;&#25511;&#21644;&#26816;&#27979;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#30417;&#25511;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37096;&#32626;&#25968;&#25454;&#26469;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#24182;&#39044;&#27979;&#25925;&#38556;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#39044;&#35265;&#26410;&#26469;&#30340;&#25925;&#38556;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#25925;&#38556;&#32463;&#39564;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning methods have recently made great strides, but generalization and robustness challenges still hinder their widespread deployment. Failing to detect and address potential failures renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains. This work aims to endow a robot with the ability to monitor and detect errors during task execution. We introduce a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, our method l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#20339;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#20063;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17550</link><description>&lt;p&gt;
&#20154;&#31867;&#24341;&#23548;&#30340;&#22797;&#26434;&#24230;&#25511;&#21046;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#20339;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#20063;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#36825;&#20123;&#34920;&#31034;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#29615;&#22659;&#25110;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#21508;&#31181;&#25277;&#35937;&#32423;&#21035;&#65288;&#20363;&#22914;&#65292;&#8220;&#40479;&#8221;&#19982;&#8220;&#40635;&#38592;&#8221;&#65289;&#19978;&#23398;&#20064;&#31163;&#25955;&#34920;&#31034;&#65288;&#21363;&#27010;&#24565;&#25110;&#21333;&#35789;&#65289;&#65292;&#24182;&#26681;&#25454;&#20219;&#21153;&#20351;&#29992;&#36866;&#24403;&#30340;&#25277;&#35937;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#20998;&#24067;&#30340;&#29109;&#26469;&#25511;&#21046;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#65288;&#22823;&#33268;&#19978;&#26159;&#20026;&#32534;&#30721;&#36755;&#20837;&#20998;&#37197;&#20102;&#22810;&#23569;&#20301;&#65289;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#31034;&#20363;&#29992;&#20110;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65288;1&#65289;&#35843;&#25972;&#34920;&#31034;&#20197;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#39640;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#19968;&#20010;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#65292;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#31163;&#25955;&#34920;&#31034;&#30340;&#21487;&#35270;&#21270;&#26469;&#30830;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24403;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#38598;&#25104;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#38750;&#24179;&#31283;&#21644;&#29305;&#24449;&#25968;&#30446;&#24222;&#22823;&#19988;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17544</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#38598;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29305;&#24449;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting. (arXiv:2310.17544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17544
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#38598;&#25104;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#38750;&#24179;&#31283;&#21644;&#29305;&#24449;&#25968;&#30446;&#24222;&#22823;&#19988;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#21644;&#26679;&#26412;&#26377;&#38480;&#30340;&#22823;&#37327;&#29305;&#24449;&#24773;&#20917;&#19979;&#30340;&#29305;&#24449;&#36873;&#25321;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#26469;&#21033;&#29992;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#29305;&#24449;&#23376;&#38598;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#21478;&#19968;&#31181;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#25439;&#22833;&#12290;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#20801;&#35768;&#28789;&#27963;&#30340;&#28145;&#24230;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#23618;&#27425;&#22320;&#21033;&#29992;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#24615;&#33021;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a novel ensemble approach for feature selection based on hierarchical stacking in cases of non-stationarity and limited number of samples with large number of features. Our approach exploits the co-dependency between features using a hierarchical structure. Initially, a machine learning model is trained using a subset of features, and then the model's output is updated using another algorithm with the remaining features to minimize the target loss. This hierarchical structure allows for flexible depth and feature selection. By exploiting feature co-dependency hierarchically, our proposed approach overcomes the limitations of traditional feature selection methods and feature importance scores. The effectiveness of the approach is demonstrated on synthetic and real-life datasets, indicating improved performance with scalability and stability compared to the traditional methods and state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;EqDrive&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;EqMotion&#31561;&#21464;&#31890;&#23376;&#21644;&#20154;&#31867;&#39044;&#27979;&#27169;&#22411;&#20197;&#21450;&#22810;&#27169;&#24335;&#39044;&#27979;&#26426;&#21046;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#27169;&#22411;&#23481;&#37327;&#36739;&#20302;&#12289;&#21442;&#25968;&#26356;&#23569;&#12289;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17540</link><description>&lt;p&gt;
EqDrive: &#33258;&#21160;&#39550;&#39542;&#30340;&#39640;&#25928;&#31561;&#21464;&#36816;&#21160;&#39044;&#27979;&#19982;&#22810;&#27169;&#24335;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
EqDrive: Efficient Equivariant Motion Forecasting with Multi-Modality for Autonomous Driving. (arXiv:2310.17540v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;EqDrive&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;EqMotion&#31561;&#21464;&#31890;&#23376;&#21644;&#20154;&#31867;&#39044;&#27979;&#27169;&#22411;&#20197;&#21450;&#22810;&#27169;&#24335;&#39044;&#27979;&#26426;&#21046;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#27169;&#22411;&#23481;&#37327;&#36739;&#20302;&#12289;&#21442;&#25968;&#26356;&#23569;&#12289;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#39044;&#27979;&#36710;&#36742;&#36816;&#21160;&#38656;&#35201;&#23545;&#36710;&#36742;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#24182;&#20445;&#25345;&#22312;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#21464;&#25442;&#19979;&#30340;&#36816;&#21160;&#31561;&#21464;&#24615;&#12290;&#20256;&#32479;&#27169;&#22411;&#24448;&#24448;&#32570;&#20047;&#22788;&#29702;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#22330;&#26223;&#20013;&#21508;&#20027;&#20307;&#20043;&#38388;&#20132;&#20114;&#20851;&#31995;&#25152;&#38656;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#23481;&#37327;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#36739;&#20302;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;EqMotion&#65292;&#19968;&#20010;&#39046;&#20808;&#30340;&#31561;&#21464;&#31890;&#23376;&#21644;&#20154;&#31867;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36824;&#32771;&#34385;&#21040;&#19981;&#21464;&#30340;&#20027;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#29992;&#20110;&#22810;&#20195;&#29702;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#27169;&#24335;&#39044;&#27979;&#26426;&#21046;&#20197;&#27010;&#29575;&#21270;&#26041;&#24335;&#32771;&#34385;&#22810;&#20010;&#21487;&#33021;&#30340;&#26410;&#26469;&#36335;&#24452;&#12290;&#36890;&#36807;&#21033;&#29992;EqMotion&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#26356;&#23569;&#65288;120&#19975;&#65289;&#21644;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#65288;&#23569;&#20110;..&#65289;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting vehicular motions in autonomous driving requires a deep understanding of agent interactions and the preservation of motion equivariance under Euclidean geometric transformations. Traditional models often lack the sophistication needed to handle the intricate dynamics inherent to autonomous vehicles and the interaction relationships among agents in the scene. As a result, these models have a lower model capacity, which then leads to higher prediction errors and lower training efficiency. In our research, we employ EqMotion, a leading equivariant particle, and human prediction model that also accounts for invariant agent interactions, for the task of multi-agent vehicle motion forecasting. In addition, we use a multi-modal prediction mechanism to account for multiple possible future paths in a probabilistic manner. By leveraging EqMotion, our model achieves state-of-the-art (SOTA) performance with fewer parameters (1.2 million) and a significantly reduced training time (less 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;UCB$^\tau$&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#22256;&#38590;&#24230;&#30340;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#33021;&#21644;&#36739;&#20302;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.17538</link><description>&lt;p&gt;
&#21482;&#38656;&#31245;&#24494;&#25506;&#32034;&#65306;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#20048;&#35266;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
Little Exploration is All You Need. (arXiv:2310.17538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;UCB$^\tau$&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#22256;&#38590;&#24230;&#30340;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#33021;&#21644;&#36739;&#20302;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#20048;&#35266;&#20027;&#20041;&#8221;&#26159;&#19968;&#31181;&#20027;&#24352;&#23558;&#25506;&#32034;&#22870;&#21169;&#22240;&#23376;&#32435;&#20837;&#32771;&#34385;&#30340;&#21407;&#21017;&#65292;&#36890;&#24120;&#34987;&#20551;&#23450;&#25104;&#19982;&#29366;&#24577;-&#21160;&#20316;&#23545;&#35775;&#38382;&#27425;&#25968;&#30340;&#20498;&#25968;&#24179;&#26041;&#26681;&#25104;&#27604;&#20363;($1/\sqrt{n}$)&#65292;&#20854;&#20013;$n$&#26159;&#35775;&#38382;&#26576;&#20010;&#29305;&#23450;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#27425;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19987;&#27880;&#20110;"&#19981;&#30830;&#23450;&#24615;"&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#36873;&#39033;&#30340;&#22266;&#26377;"&#22256;&#38590;&#24615;"&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#24341;&#20837;&#20102;&#26631;&#20934;UCB&#31639;&#27861;&#30340;&#19968;&#31181;&#26032;&#39062;&#20462;&#25913;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;&#20219;&#21153;&#30340;&#38590;&#24230;&#36827;&#34892;&#35843;&#25972;&#30340;&#22870;&#21169;&#39033;$1/n^\tau$&#65292;&#20854;&#20013;$\tau &gt; 1/2$&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#31639;&#27861;&#34987;&#26631;&#35760;&#20026;UCB$^\tau$&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#36951;&#25022;&#21644;&#39118;&#38505;&#20998;&#26512;&#26469;&#35777;&#26126;&#20854;&#29702;&#35770;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#19982;&#26631;&#20934;UCB&#31639;&#27861;&#21644;Thompson&#37319;&#26679;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#35780;&#20272;&#34920;&#26126;&#65292;UCB$^\tau$&#19981;&#20165;&#22312;&#25928;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36824;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#26174;&#31034;&#20986;&#36739;&#20302;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevailing principle of "Optimism in the Face of Uncertainty" advocates for the incorporation of an exploration bonus, generally assumed to be proportional to the inverse square root of the visit count ($1/\sqrt{n}$), where $n$ is the number of visits to a particular state-action pair. This approach, however, exclusively focuses on "uncertainty," neglecting the inherent "difficulty" of different options. To address this gap, we introduce a novel modification of standard UCB algorithm in the multi-armed bandit problem, proposing an adjusted bonus term of $1/n^\tau$, where $\tau &gt; 1/2$, that accounts for task difficulty. Our proposed algorithm, denoted as UCB$^\tau$, is substantiated through comprehensive regret and risk analyses, confirming its theoretical robustness. Comparative evaluations with standard UCB and Thompson Sampling algorithms on synthetic datasets demonstrate that UCB$^\tau$ not only outperforms in efficacy but also exhibits lower risk across various environmental co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#27573;&#21644;&#22238;&#28335;&#26469;&#20811;&#26381;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#28608;&#21169;&#20195;&#29702;&#25506;&#32034;&#26032;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.17537</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#27573;&#21644;&#22238;&#28335;&#27861;&#20811;&#26381;&#22909;&#22855;&#24515;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity. (arXiv:2310.17537v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#27573;&#21644;&#22238;&#28335;&#26469;&#20811;&#26381;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#28608;&#21169;&#20195;&#29702;&#25506;&#32034;&#26032;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#31232;&#30095;&#22870;&#21169;&#30340;&#22256;&#38590;&#25506;&#32034;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#36890;&#36807;&#21069;&#21521;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#29983;&#25104;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#36825;&#20123;&#35823;&#24046;&#38543;&#30528;&#29615;&#22659;&#30340;&#29087;&#24713;&#31243;&#24230;&#32780;&#20943;&#23569;&#65292;&#24182;&#28608;&#21169;&#20195;&#29702;&#25506;&#32034;&#26032;&#30340;&#29366;&#24577;&#12290;&#34429;&#28982;&#22522;&#20110;&#39044;&#27979;&#30340;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#20195;&#29702;&#35299;&#20915;&#22256;&#38590;&#30340;&#25506;&#32034;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19988;&#23454;&#38469;&#19978;&#20250;&#22312;&#35775;&#38382;&#30340;&#29366;&#24577;&#19978;&#22686;&#21152;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#26684;&#29366;&#19990;&#30028;&#29615;&#22659;&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26465;&#20214;&#21644;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#21644;&#21160;&#29289;&#23398;&#20064;&#21551;&#21457;&#30340;&#26032;&#26041;&#27861;FARCuriosity&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20998;&#27573;&#21644;&#22238;&#28335;&#65306;&#20195;&#29702;&#26681;&#25454;&#24778;&#35766;&#24615;&#23545;&#29615;&#22659;&#36827;&#34892;&#20998;&#27573;&#65292;&#24182;&#20026;&#27599;&#20010;&#20998;&#27573;&#20351;&#29992;&#19981;&#21516;&#30340;&#26412;&#22320;&#22909;&#22855;&#27169;&#22359;&#65288;&#22522;&#20110;&#39044;&#27979;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65289;&#65292;&#20197;&#20351;&#27169;&#22359;&#19981;&#26159;&#22312;&#25972;&#20010;&#29615;&#22659;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#27599;&#20010;&#20998;&#27573;&#20013;&#65292;&#20195;&#29702;&#21487;&#20197;&#21516;&#26102;&#23384;&#20648;&#24182;&#22238;&#28335;&#20808;&#21069;&#30340;&#22870;&#21169;&#20540;&#20197;&#24212;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning methods exhibit impressive performance on a range of tasks but still struggle on hard exploration tasks in large environments with sparse rewards. To address this, intrinsic rewards can be generated using forward model prediction errors that decrease as the environment becomes known, and incentivize an agent to explore novel states. While prediction-based intrinsic rewards can help agents solve hard exploration tasks, they can suffer from catastrophic forgetting and actually increase at visited states. We first examine the conditions and causes of catastrophic forgetting in grid world environments. We then propose a new method FARCuriosity, inspired by how humans and animals learn. The method depends on fragmentation and recall: an agent fragments an environment based on surprisal, and uses different local curiosity modules (prediction-based intrinsic reward functions) for each fragment so that modules are not trained on the entire environment. At each fragm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#30340;&#20998;&#31867;&#27861;&#65292;&#25581;&#31034;&#20102;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#35774;&#32622;&#19978;&#24050;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17534</link><description>&lt;p&gt;
SoK&#65306;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#20013;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#30340;&#20998;&#31867;&#27861;&#65292;&#25581;&#31034;&#20102;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#35774;&#32622;&#19978;&#24050;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#28041;&#21450;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#23545;&#23545;&#25163;&#30340;&#30693;&#35782;&#20551;&#35774;&#19981;&#21516;&#65292;&#30446;&#21069;&#30340;&#25991;&#29486;&#32570;&#20047;&#22260;&#32469;&#23041;&#32961;&#27169;&#22411;&#36827;&#34892;&#32452;&#32455;&#30340;&#36830;&#36143;&#24615;&#12290;&#20026;&#20102;&#31995;&#32479;&#21270;&#35813;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#23041;&#32961;&#31354;&#38388;&#30340;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;&#21453;&#39304;&#31890;&#24230;&#12289;&#20132;&#20114;&#24335;&#26597;&#35810;&#30340;&#35775;&#38382;&#21644;&#25915;&#20987;&#32773;&#21487;&#29992;&#30340;&#36741;&#21161;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#19977;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26032;&#20998;&#31867;&#27861;&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#38190;&#35265;&#35299;&#12290;1) &#23613;&#31649;&#26377;&#24191;&#27867;&#25991;&#29486;&#65292;&#20173;&#23384;&#22312;&#35768;&#22810;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#26080;&#27861;&#36890;&#36807;&#20174;&#24050;&#30693;&#39046;&#22495;&#30340;&#25216;&#26415;&#31616;&#21333;&#22320;&#25913;&#36827;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22522;&#20110;&#24050;&#30693;&#39046;&#22495;&#20174;&#23436;&#25972;&#32622;&#20449;&#21521;&#37327;&#35775;&#38382;&#30340;&#25216;&#26415;&#36866;&#24212;&#21040;&#35775;&#38382;&#21069;k&#20010;&#32622;&#20449;&#24471;&#20998;&#30340;&#36739;&#23569;&#30740;&#31350;&#30340;&#35774;&#32622;&#20013;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#20294;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#23427;&#22312;&#20165;&#33719;&#24471;&#39044;&#27979;&#26631;&#31614;&#30340;&#26356;&#20005;&#26684;&#35774;&#32622;&#20013;&#20173;&#28982;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous works study black-box attacks on image classifiers. However, these works make different assumptions on the adversary's knowledge and current literature lacks a cohesive organization centered around the threat model. To systematize knowledge in this area, we propose a taxonomy over the threat space spanning the axes of feedback granularity, the access of interactive queries, and the quality and quantity of the auxiliary data available to the attacker. Our new taxonomy provides three key insights. 1) Despite extensive literature, numerous under-explored threat spaces exist, which cannot be trivially solved by adapting techniques from well-explored settings. We demonstrate this by establishing a new state-of-the-art in the less-studied setting of access to top-k confidence scores by adapting techniques from well-explored settings of accessing the complete confidence vector, but show how it still falls short of the more restrictive setting that only obtains the prediction label, h
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35774;&#35745;&#20102;&#29992;&#20110;&#23398;&#20064;&#22270;&#26680;&#22343;&#20540;&#22330;&#21338;&#24328;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;GMFG-PPO&#20197;&#21450;&#26680;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#26410;&#30693;&#22270;&#26680;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#25913;&#36827;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.17531</link><description>&lt;p&gt;
&#23398;&#20064;&#24102;&#26410;&#30693;&#22270;&#26680;&#30340;&#35268;&#33539;&#21270;&#22270;&#22343;&#22330;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Learning Regularized Graphon Mean-Field Games with Unknown Graphons. (arXiv:2310.17531v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17531
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#29992;&#20110;&#23398;&#20064;&#22270;&#26680;&#22343;&#20540;&#22330;&#21338;&#24328;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;GMFG-PPO&#20197;&#21450;&#26680;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#26410;&#30693;&#22270;&#26680;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#25913;&#36827;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;&#22270;&#22343;&#22330;&#21338;&#24328;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#31934;&#30830;&#22270;&#26680;&#20540;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#26088;&#22312;&#23398;&#20064;&#24403;&#22270;&#26680;&#26410;&#30693;&#26102;&#30340;&#35268;&#33539;&#21270;&#22270;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22270;&#22343;&#22330;&#21338;&#24328;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65288;GMFG-PPO&#65289;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#20272;&#35745;&#39044;&#35328;&#26426;&#35777;&#26126;&#20102;&#20854;&#22312;T&#27425;&#36845;&#20195;&#21518;&#20197;$O(T^{-1/3})$&#30340;&#36895;&#29575;&#25910;&#25947;&#65292;&#25913;&#36827;&#20102;Xie&#31561;&#20154;&#65288;ICML&#65292;2021&#65289;&#30340;&#21069;&#26399;&#24037;&#20316;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#20998;&#24067;&#30340;&#26680;&#23884;&#20837;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#20272;&#35745;&#37319;&#26679;&#20195;&#29702;&#30340;&#36716;&#31227;&#26680;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#22270;&#26680;&#12290;&#24403;&#20195;&#29702;&#30340;&#20301;&#32622;&#24050;&#30693;&#25110;&#26410;&#30693;&#26102;&#65292;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#21518;&#25552;&#20379;&#20102;&#36816;&#29992;GMFG-PPO&#20248;&#21270;&#31639;&#27861;&#21644;&#20272;&#35745;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#31639;&#27861;&#26159;&#39318;&#27425;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23398;&#20064;&#22270;&#26680;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design and analyze reinforcement learning algorithms for Graphon Mean-Field Games (GMFGs). In contrast to previous works that require the precise values of the graphons, we aim to learn the Nash Equilibrium (NE) of the regularized GMFGs when the graphons are unknown. Our contributions are threefold. First, we propose the Proximal Policy Optimization for GMFG (GMFG-PPO) algorithm and show that it converges at a rate of $O(T^{-1/3})$ after $T$ iterations with an estimation oracle, improving on a previous work by Xie et al. (ICML, 2021). Second, using kernel embedding of distributions, we design efficient algorithms to estimate the transition kernels, reward functions, and graphons from sampled agents. Convergence rates are then derived when the positions of the agents are either known or unknown. Results for the combination of the optimization algorithm GMFG-PPO and the estimation algorithm are then provided. These algorithms are the first specifically designed for learning graphons f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24615;&#21035;&#20013;&#31435;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21518;&#30340;&#20559;&#35265;&#25918;&#22823;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#27492;&#22806;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#24615;&#21035;&#20013;&#31435;&#25968;&#25454;&#26377;&#21033;&#65292;&#21487;&#20197;&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#20419;&#36827;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.17530</link><description>&lt;p&gt;
&#35780;&#20272;&#24615;&#21035;&#20013;&#31435;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24615;&#21035;&#20013;&#31435;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21518;&#30340;&#20559;&#35265;&#25918;&#22823;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#27492;&#22806;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#24615;&#21035;&#20013;&#31435;&#25968;&#25454;&#26377;&#21033;&#65292;&#21487;&#20197;&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#20419;&#36827;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20250;&#20445;&#25345;&#29978;&#33267;&#25918;&#22823;&#25968;&#25454;&#20013;&#29616;&#26377;&#30340;&#20559;&#35265;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#26368;&#32456;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#20250;&#23545;&#29305;&#23450;&#32676;&#20307;&#25110;&#20154;&#21475;&#20135;&#29983;&#27495;&#35270;&#24615;&#34892;&#20026;&#65292;&#29702;&#35299;&#36825;&#20123;&#20559;&#35265;&#20559;&#21521;&#30340;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#24615;&#21035;&#20559;&#35265;&#20316;&#20026;&#26696;&#20363;&#36827;&#34892;&#23450;&#20041;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#19977;&#20010;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26063;&#32676;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21518;&#30340;&#20559;&#35265;&#25918;&#22823;&#65292;&#24182;&#35843;&#26597;&#20102;&#36825;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35780;&#20272;&#20102;&#20559;&#35265;&#25918;&#22823;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21518;&#30340;&#20559;&#35265;&#25918;&#22823;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24615;&#21035;&#20013;&#31435;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#36825;&#21487;&#20197;&#20943;&#23569;&#32676;&#20307;&#24046;&#36317;&#65292;&#21363;&#22312;VQAv2&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#20419;&#36827;&#20844;&#24179;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#25439;&#23475;&#20219;&#21153;&#25191;&#34892;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17526</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#20154;&#31867;&#22312;&#31995;&#32479;&#35780;&#20215;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#65311;&#35780;&#20272;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#35780;&#20215;&#23545;&#20110;&#25351;&#23548;&#23454;&#36341;&#12289;&#30740;&#31350;&#21644;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24120;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#20154;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#33021;&#22815;&#21152;&#24555;&#21644;&#33258;&#21160;&#21270;&#31995;&#32479;&#35780;&#20215;&#30340;&#36807;&#31243;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#32780;&#19988;&#36824;&#27809;&#26377;&#30740;&#31350;&#27979;&#35797;&#36807;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;LLM&#8212;&#8212;GPT-4&#12290;&#26412;&#39044;&#27880;&#20876;&#30740;&#31350;&#37319;&#29992;&#8220;&#26080;&#20154;&#21442;&#19982;&#8221;&#30340;&#26041;&#27861;&#35780;&#20272;&#20102;GPT-4&#22312;&#26631;&#39064;/&#25688;&#35201;&#31579;&#36873;&#12289;&#20840;&#25991;&#23457;&#26597;&#21644;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#22312;&#19981;&#21516;&#25991;&#29486;&#31867;&#22411;&#21644;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#32467;&#26524;&#21463;&#21040;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#35843;&#25972;&#20102;&#36825;&#20123;&#22240;&#32032;&#21518;&#65292;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;&#20351;&#29992;&#39640;&#21487;&#38752;&#24615;&#25552;&#31034;&#36827;&#34892;&#31579;&#36873;&#30340;&#30740;&#31350;&#20013;&#65292;&#31579;&#36873;&#20840;&#25991;&#25991;&#29486;&#30340;&#34920;&#29616;&#27700;&#24179;&#22312;&#19981;&#21516;&#38454;&#27573;&#21644;&#35821;&#35328;&#19978;&#22343;&#20026;&#26080;&#21040;&#20013;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.17513</link><description>&lt;p&gt;
&#12298;&#20302;&#31209;&#36866;&#24212;&#30340;&#34920;&#36798;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#30697;&#38453;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;LoRA&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#29702;&#35770;&#35282;&#24230;&#20998;&#26512;LoRA&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#39318;&#27425;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#65292;&#21017;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#24403;LoRA-rank&#20302;&#20110;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#19981;&#33021;&#19982;&#30495;&#23454;&#20154;&#31867;&#30456;&#20851;&#32852;&#30340;&#20154;&#24037;&#35828;&#35805;&#32773;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#22768;&#38899;&#21644;&#35828;&#35805;&#39118;&#26684;&#30340;&#30452;&#35266;&#21644;&#31934;&#32454;&#25511;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#31614;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#23558;&#30495;&#23454;&#20154;&#31867;&#30340;&#23884;&#20837;&#19982;&#20154;&#24037;&#21487;&#25511;&#23884;&#20837;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30830;&#20445;&#20102;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.17502</link><description>&lt;p&gt;
&#36890;&#36807;&#21457;&#29616;&#20027;&#35201;&#26041;&#21521;&#23454;&#29616;&#20154;&#24037;&#35828;&#35805;&#32773;&#23884;&#20837;&#30340;&#21487;&#25511;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions. (arXiv:2310.17502v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#19981;&#33021;&#19982;&#30495;&#23454;&#20154;&#31867;&#30456;&#20851;&#32852;&#30340;&#20154;&#24037;&#35828;&#35805;&#32773;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#22768;&#38899;&#21644;&#35828;&#35805;&#39118;&#26684;&#30340;&#30452;&#35266;&#21644;&#31934;&#32454;&#25511;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#31614;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#23558;&#30495;&#23454;&#20154;&#31867;&#30340;&#23884;&#20837;&#19982;&#20154;&#24037;&#21487;&#25511;&#23884;&#20837;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30830;&#20445;&#20102;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#20855;&#26377;&#30452;&#35266;&#21644;&#31934;&#32454;&#25511;&#21046;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#20013;&#23450;&#21046;&#22768;&#38899;&#21644;&#35828;&#35805;&#39118;&#26684;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#24456;&#23569;&#26377;&#24102;&#26377;&#21512;&#36866;&#26631;&#31614;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#32534;&#36753;&#29616;&#26377;&#20154;&#30340;&#22768;&#38899;&#20063;&#28041;&#21450;&#36947;&#24503;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#19981;&#33021;&#19982;&#30495;&#23454;&#20154;&#31867;&#30456;&#20851;&#32852;&#30340;&#20154;&#24037;&#35828;&#35805;&#32773;&#23884;&#20837;&#65292;&#32780;&#19988;&#33021;&#22815;&#22312;&#23884;&#20837;&#30340;&#22768;&#38899;&#21644;&#35828;&#35805;&#39118;&#26684;&#19978;&#25552;&#20379;&#30452;&#35266;&#21644;&#31934;&#32454;&#30340;&#25511;&#21046;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#35828;&#35805;&#32773;&#25110;&#39118;&#26684;&#30340;&#26631;&#31614;&#12290;&#36825;&#20123;&#20154;&#24037;&#21487;&#25511;&#30340;&#23884;&#20837;&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#19982;&#30495;&#23454;&#20154;&#31867;&#30340;&#23884;&#20837;&#26465;&#20214;&#19979;&#36755;&#20837;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#20013;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#29306;&#29298;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customizing voice and speaking style in a speech synthesis system with intuitive and fine-grained controls is challenging, given that little data with appropriate labels is available. Furthermore, editing an existing human's voice also comes with ethical concerns. In this paper, we propose a method to generate artificial speaker embeddings that cannot be linked to a real human while offering intuitive and fine-grained control over the voice and speaking style of the embeddings, without requiring any labels for speaker or style. The artificial and controllable embeddings can be fed to a speech synthesis system, conditioned on embeddings of real humans during training, without sacrificing privacy during inference.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25913;&#36827;&#20102;&#25105;&#20204;&#22312;Blizzard Challenge 2021&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#21040;&#38899;&#32032;&#22788;&#29702;&#31995;&#32479;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#25968;&#25454;&#22788;&#29702;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#26469;&#22788;&#29702;Blizzard Challenge 2023&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26631;&#35782;&#31526;&#26159;G&#12290;</title><link>http://arxiv.org/abs/2310.17499</link><description>&lt;p&gt;
IMS Toucan&#31995;&#32479;&#29992;&#20110;Blizzard Challenge 2023
&lt;/p&gt;
&lt;p&gt;
The IMS Toucan System for the Blizzard Challenge 2023. (arXiv:2310.17499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25913;&#36827;&#20102;&#25105;&#20204;&#22312;Blizzard Challenge 2021&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#21040;&#38899;&#32032;&#22788;&#29702;&#31995;&#32479;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#25968;&#25454;&#22788;&#29702;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#26469;&#22788;&#29702;Blizzard Challenge 2023&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26631;&#35782;&#31526;&#26159;G&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21442;&#21152;Blizzard Challenge 2023&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#22312;Blizzard Challenge 2021&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#21040;&#38899;&#32032;&#22788;&#29702;&#31995;&#32479;&#65292;&#21253;&#25324;&#23545;&#27861;&#35821;&#20013;&#30340;&#21516;&#38899;&#24322;&#24418;&#35789;&#36827;&#34892;&#22522;&#20110;&#35268;&#21017;&#30340;&#28040;&#27495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Conformer&#21644;Glow&#30340;&#24555;&#36895;&#39640;&#25928;&#30340;&#38750;&#33258;&#22238;&#24402;&#21512;&#25104;&#26550;&#26500;&#23558;&#38899;&#32032;&#36716;&#25442;&#20026;&#20013;&#38388;&#34920;&#31034; - &#39057;&#35889;&#22270;&#12290;&#19968;&#20010;&#22522;&#20110;GAN&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#20808;&#36827;&#26041;&#27861;&#65292;&#23558;&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#26368;&#32456;&#30340;&#27874;&#24418;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#29992;&#20110;&#25361;&#25112;&#25968;&#25454;&#30340;&#25968;&#25454;&#22788;&#29702;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26631;&#35782;&#31526;&#26159;G&#12290;&#25552;&#20379;&#24320;&#28304;&#20195;&#30721;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
For our contribution to the Blizzard Challenge 2023, we improved on the system we submitted to the Blizzard Challenge 2021. Our approach entails a rule-based text-to-phoneme processing system that includes rule-based disambiguation of homographs in the French language. It then transforms the phonemes to spectrograms as intermediate representations using a fast and efficient non-autoregressive synthesis architecture based on Conformer and Glow. A GAN based neural vocoder that combines recent state-of-the-art approaches converts the spectrogram to the final wave. We carefully designed the data processing, training, and inference procedures for the challenge data. Our system identifier is G. Open source code and demo are available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20449;&#21518;&#38376;&#26816;&#27979;&#22120;&#65288;CBD&#65289;&#65292;&#23427;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35843;&#33410;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#26696;&#65292;&#21363;&#23616;&#37096;&#20027;&#23548;&#27010;&#29575;&#32479;&#35745;&#12290;CBD&#33021;&#22815;&#25552;&#20379;&#23545;&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#25512;&#26029;&#32467;&#26524;&#65292;&#24182;&#32473;&#20986;&#25915;&#20987;&#20445;&#35777;&#21487;&#26816;&#27979;&#30340;&#26465;&#20214;&#21644;&#20551;&#38451;&#24615;&#29575;&#30340;&#27010;&#29575;&#19978;&#30028;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20855;&#26377;&#26356;&#39640;&#40065;&#26834;&#24615;&#30340;&#35302;&#21457;&#22120;&#21644;&#26356;&#23567;&#25200;&#21160;&#24133;&#24230;&#30340;&#25915;&#20987;&#26356;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2310.17498</link><description>&lt;p&gt;
CBD: &#22522;&#20110;&#23616;&#37096;&#20027;&#23548;&#27010;&#29575;&#30340;&#21487;&#20449;&#21518;&#38376;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
CBD: A Certified Backdoor Detector Based on Local Dominant Probability. (arXiv:2310.17498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20449;&#21518;&#38376;&#26816;&#27979;&#22120;&#65288;CBD&#65289;&#65292;&#23427;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35843;&#33410;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#26696;&#65292;&#21363;&#23616;&#37096;&#20027;&#23548;&#27010;&#29575;&#32479;&#35745;&#12290;CBD&#33021;&#22815;&#25552;&#20379;&#23545;&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#25512;&#26029;&#32467;&#26524;&#65292;&#24182;&#32473;&#20986;&#25915;&#20987;&#20445;&#35777;&#21487;&#26816;&#27979;&#30340;&#26465;&#20214;&#21644;&#20551;&#38451;&#24615;&#29575;&#30340;&#27010;&#29575;&#19978;&#30028;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20855;&#26377;&#26356;&#39640;&#40065;&#26834;&#24615;&#30340;&#35302;&#21457;&#22120;&#21644;&#26356;&#23567;&#25200;&#21160;&#24133;&#24230;&#30340;&#25915;&#20987;&#26356;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#24120;&#35265;&#23041;&#32961;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#23884;&#20837;&#20102;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#26679;&#26412;&#23558;&#34987;&#21518;&#38376;&#27169;&#22411;&#35823;&#20998;&#31867;&#20026;&#23545;&#25239;&#30446;&#26631;&#65292;&#32780;&#27809;&#26377;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#26679;&#26412;&#23558;&#34987;&#27491;&#30830;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20449;&#21518;&#38376;&#26816;&#27979;&#22120;&#65288;CBD&#65289;&#65292;&#23427;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#23616;&#37096;&#20027;&#23548;&#27010;&#29575;&#32479;&#35745;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35843;&#33410;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#26696;&#12290;&#23545;&#20110;&#21463;&#26816;&#27979;&#30340;&#20219;&#20309;&#20998;&#31867;&#22120;&#65292;CBD&#25552;&#20379;&#20102;1&#65289;&#26816;&#27979;&#25512;&#26029;&#32467;&#26524;&#65292;2&#65289;&#25915;&#20987;&#22312;&#21516;&#19968;&#20998;&#31867;&#22495;&#19979;&#20445;&#35777;&#21487;&#26816;&#27979;&#30340;&#26465;&#20214;&#65292;&#24182;&#19988;3&#65289;&#20551;&#38451;&#24615;&#29575;&#30340;&#27010;&#29575;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#39640;&#40065;&#26834;&#24615;&#30340;&#35302;&#21457;&#22120;&#12289;&#26356;&#23567;&#25200;&#21160;&#24133;&#24230;&#30340;&#25915;&#20987;&#26356;&#26377;&#21487;&#33021;&#34987;&#26377;&#20445;&#35777;&#22320;&#26816;&#27979;&#20986;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#32771;&#34385;&#21508;&#31181;&#21518;&#38376;&#31867;&#22411;&#30340;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attack is a common threat to deep neural networks. During testing, samples embedded with a backdoor trigger will be misclassified as an adversarial target by a backdoored model, while samples without the backdoor trigger will be correctly classified. In this paper, we present the first certified backdoor detector (CBD), which is based on a novel, adjustable conformal prediction scheme based on our proposed statistic local dominant probability. For any classifier under inspection, CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate. Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees. Moreover, we conduct extensive experiments on four benchmark datasets considering various backdoor types, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;A/B&#27979;&#35797;&#20013;&#30001;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#23454;&#39564;&#32452;&#25110;&#25511;&#21046;&#32452;&#20013;&#20986;&#29616;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#21152;&#26435;&#25439;&#22833;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#23567;&#26041;&#24046;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#24341;&#36215;&#35757;&#32451;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.17496</link><description>&lt;p&gt;
&#35299;&#20915;A/B&#27979;&#35797;&#20013;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#65306;&#19968;&#31181;&#21152;&#26435;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach. (arXiv:2310.17496v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;A/B&#27979;&#35797;&#20013;&#30001;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#23454;&#39564;&#32452;&#25110;&#25511;&#21046;&#32452;&#20013;&#20986;&#29616;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#21152;&#26435;&#25439;&#22833;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#23567;&#26041;&#24046;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#24341;&#36215;&#35757;&#32451;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26631;&#20934;&#27969;&#31243;&#28041;&#21450;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#29992;&#25143;&#34892;&#20026;&#24182;&#25345;&#32493;&#25913;&#36827;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#24490;&#29615;&#21487;&#33021;&#22312;A/B&#27979;&#35797;&#20013;&#24341;&#20837;&#24178;&#25200;&#65292;&#20854;&#20013;&#25511;&#21046;&#32452;&#21644;&#23454;&#39564;&#32452;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21152;&#26435;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#27599;&#20010;&#25968;&#25454;&#28857;&#20986;&#29616;&#22312;&#23454;&#39564;&#32452;&#25110;&#25511;&#21046;&#32452;&#25968;&#25454;&#20013;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#21152;&#26435;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#20272;&#35745;&#37327;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#26041;&#24046;&#65292;&#19988;&#19981;&#20250;&#23548;&#33268;&#35757;&#32451;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#35777;&#26126;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. However, these data training loops can introduce interference in A/B tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. To address these challenges, we introduce a novel approach called weighted training. This approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. We demonstrate that this approach achieves the least variance among all estimators without causing shifts in the training distributions. Through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#22120;&#36741;&#21161;&#30340;&#31227;&#21160;&#36793;&#32536;&#35843;&#20248;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#32534;&#25490;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#26032;&#30340;&#27169;&#25311;&#22120;&#36866;&#37197;&#22120;&#26550;&#26500;&#21644;&#28151;&#21512;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#37096;&#32626;&#21644;&#31934;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26412;&#22320;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17492</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#22120;&#36741;&#21161;&#30340;&#31227;&#21160;&#36793;&#32536;&#35843;&#20248;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#32534;&#25490;&#65306;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach. (arXiv:2310.17492v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#22120;&#36741;&#21161;&#30340;&#31227;&#21160;&#36793;&#32536;&#35843;&#20248;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#32534;&#25490;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#26032;&#30340;&#27169;&#25311;&#22120;&#36866;&#37197;&#22120;&#26550;&#26500;&#21644;&#28151;&#21512;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#37096;&#32626;&#21644;&#31934;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26412;&#22320;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#39640;&#25928;&#37096;&#32626;&#21644;&#31934;&#35843;&#22522;&#30784;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#33539;&#24335;&#65292;&#23558;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#19982;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#65292;&#26088;&#22312;&#22686;&#24378;&#29992;&#25143;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#21019;&#26032;&#30340;&#27169;&#25311;&#22120;&#36866;&#37197;&#22120;&#26550;&#26500;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#20998;&#21106;&#20026;&#20004;&#20010;&#21327;&#21516;&#27169;&#22359;&#12290;&#36825;&#31181;&#35774;&#35745;&#19981;&#20165;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#65292;&#36824;&#30830;&#20445;&#20102;&#36866;&#24212;&#24615;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#31934;&#35843;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36164;&#28304;&#20998;&#37197;&#26426;&#21046;&#65292;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#25311;&#22120;&#36866;&#37197;&#22120;&#32467;&#26500;&#30340;&#38656;&#27714;&#36827;&#34892;&#31934;&#35843;&#12290;&#20026;&#24212;&#23545;&#35813;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31574;&#30053;&#65292;&#25797;&#38271;&#22788;&#29702;&#28151;&#21512;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#65292;&#30830;&#20445;&#21160;&#24577;&#21644;&#26368;&#20248;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficient deployment and fine-tuning of foundation models are pivotal in contemporary artificial intelligence. In this study, we present a groundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundation models, specifically designed to enhance local task performance on user equipment (UE). Central to our approach is the innovative Emulator-Adapter architecture, segmenting the foundation model into two cohesive modules. This design not only conserves computational resources but also ensures adaptability and fine-tuning efficiency for downstream tasks. Additionally, we introduce an advanced resource allocation mechanism that is fine-tuned to the needs of the Emulator-Adapter structure in decentralized settings. To address the challenges presented by this system, we employ a hybrid multi-agent Deep Reinforcement Learning (DRL) strategy, adept at handling mixed discrete-continuous action spaces, ensuring dynamic and optimal resource allocations. Our comprehensive simula
&lt;/p&gt;</description></item><item><title>FedPEAT&#26159;&#23558;&#36741;&#21161;&#35843;&#20248;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#27169;&#22411;&#38544;&#31169;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.17491</link><description>&lt;p&gt;
FedPEAT: &#32852;&#37030;&#23398;&#20064;&#12289;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#19982;&#36741;&#21161;&#35843;&#20248;&#22312;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing. (arXiv:2310.17491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17491
&lt;/p&gt;
&lt;p&gt;
FedPEAT&#26159;&#23558;&#36741;&#21161;&#35843;&#20248;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#27169;&#22411;&#38544;&#31169;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#65292;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#39046;&#22495;&#65292;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#33021;&#21147;&#12290;&#37096;&#32626;&#21644;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#65292;&#22914;GPT-3&#21644;BERT&#65292;&#22312;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36741;&#21161;&#35843;&#20248;&#65288;EAT&#65289;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#21442;&#25968;&#39640;&#25928;&#36741;&#21161;&#35843;&#20248;&#65288;PEAT&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#32852;&#37030;PEAT&#65288;FedPEAT&#65289;&#12290;FedPEAT&#20351;&#29992;&#36866;&#37197;&#22120;&#12289;&#20223;&#30495;&#22120;&#21644;PEFT&#36827;&#34892;&#32852;&#37030;&#27169;&#22411;&#35843;&#20248;&#65292;&#25552;&#39640;&#27169;&#22411;&#38544;&#31169;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;&#36866;&#37197;&#22120;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#20223;&#30495;&#22120;&#32473;&#20986;&#21407;&#22987;&#27169;&#22411;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#21516;&#26102;&#35299;&#20915;&#38544;&#31169;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29420;&#29305;&#30340;&#22330;&#26223;&#20013;&#20351;&#29992;FedPEAT&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#21442;&#19982;&#21327;&#20316;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of foundation models, including language and vision models, has reshaped AI's landscape, offering capabilities across various applications. Deploying and fine-tuning these large models, like GPT-3 and BERT, presents challenges, especially in the current foundation model era. We introduce Emulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning (PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we expand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses adapters, emulators, and PEFT for federated model tuning, enhancing model privacy and memory efficiency. Adapters adjust pre-trained models, while emulators give a compact representation of original models, addressing both privacy and efficiency. Adaptable to various neural networks, our approach also uses deep reinforcement learning for hyper-parameter optimization. We tested FedPEAT in a unique scenario with a server participating in collaborative federate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#20998;&#26512;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#30495;&#23454;&#25928;&#29992;&#20998;&#24067;&#36716;&#21270;&#20026;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#26469;&#32771;&#34385;&#20559;&#35265;&#65292;&#24182;&#23545;&#21442;&#25968;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#25506;&#31350;&#20854;&#23545;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#21644;&#25968;&#25454;&#25311;&#21512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17489</link><description>&lt;p&gt;
&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#65306;&#22522;&#20110;&#20248;&#21270;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bias in Evaluation Processes: An Optimization-Based Model. (arXiv:2310.17489v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#20998;&#26512;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#30495;&#23454;&#25928;&#29992;&#20998;&#24067;&#36716;&#21270;&#20026;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#26469;&#32771;&#34385;&#20559;&#35265;&#65292;&#24182;&#23545;&#21442;&#25968;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#25506;&#31350;&#20854;&#23545;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#21644;&#25968;&#25454;&#25311;&#21512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#25307;&#29983;&#21644;&#25307;&#32856;&#31561;&#35774;&#32622;&#20013;&#65292;&#20851;&#20110;&#20010;&#20154;&#31038;&#20250;&#26174;&#33879;&#23646;&#24615;&#30340;&#20559;&#35265;&#24050;&#34987;&#24191;&#27867;&#35760;&#24405;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#35780;&#20272;&#36807;&#31243;&#35270;&#20026;&#23558;&#20010;&#20154;&#23545;&#20219;&#21153;&#30340;&#30495;&#23454;&#25928;&#29992;&#20998;&#24067;&#36716;&#21270;&#20026;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#24314;&#27169;&#20026;&#22312;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#25439;&#22833;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#20004;&#20010;&#21442;&#25968;&#65292;&#34987;&#35270;&#20026;&#23548;&#33268;&#20559;&#35265;&#30340;&#22240;&#32032;&#65306;&#20449;&#24687;&#32422;&#26463;&#20013;&#30340;&#36164;&#28304;&#20449;&#24687;&#20132;&#25442;&#21442;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#39118;&#38505;&#21388;&#24694;&#21442;&#25968;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#30001;&#25105;&#20204;&#30340;&#27169;&#22411;&#20135;&#29983;&#30340;&#20998;&#24067;&#65292;&#24182;&#30740;&#31350;&#20102;&#21442;&#25968;&#23545;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#36755;&#20986;&#20016;&#23500;&#20102;&#21487;&#29992;&#20110;&#25429;&#25417;&#35266;&#23519;&#35780;&#20272;&#20013;&#32676;&#32452;&#38388;&#21464;&#21270;&#30340;&#20998;&#24067;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#25311;&#21512;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#23454;&#35777;&#39564;&#35777;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#30740;&#31350;&#20171;&#20837;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biases with respect to socially-salient attributes of individuals have been well documented in evaluation processes used in settings such as admissions and hiring. We view such an evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution and model it as a solution to a loss minimization problem subject to an information constraint. Our model has two parameters that have been identified as factors leading to biases: the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function. We characterize the distributions that arise from our model and study the effect of the parameters on the observed distribution. The outputs of our model enrich the class of distributions that can be used to capture variation across groups in the observed evaluations. We empirically validate our model by fitting real-world datasets and use it to study the effect of interve
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20844;&#24179;&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#20316;&#35848;&#21028;&#21338;&#24328;&#27169;&#22411;&#65292;&#24182;&#22312;&#29983;&#20135;&#20013;&#26174;&#24335;&#25512;&#29702;&#32780;&#19981;&#26159;&#35775;&#38382;&#29305;&#24449;&#20989;&#25968;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17485</link><description>&lt;p&gt;
&#20844;&#24179;&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#65306;&#19968;&#31181;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach. (arXiv:2310.17485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17485
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20844;&#24179;&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#20316;&#35848;&#21028;&#21338;&#24328;&#27169;&#22411;&#65292;&#24182;&#22312;&#29983;&#20135;&#20013;&#26174;&#24335;&#25512;&#29702;&#32780;&#19981;&#26159;&#35775;&#38382;&#29305;&#24449;&#20989;&#25968;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#26159;&#25351;&#25215;&#36816;&#21830;&#36890;&#36807;&#20849;&#20139;&#36816;&#36755;&#35831;&#27714;&#24182;&#20195;&#34920;&#23545;&#26041;&#36827;&#34892;&#36816;&#36755;&#35831;&#27714;&#65292;&#20197;&#23454;&#29616;&#35268;&#27169;&#32463;&#27982;&#65292;&#20174;&#32780;&#38477;&#20302;&#25104;&#26412;&#12289;&#20943;&#23569;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#21644;&#36947;&#36335;&#25317;&#22581;&#12290;&#28982;&#32780;&#65292;&#24212;&#35813;&#30001;&#21738;&#20010;&#25215;&#36816;&#21830;&#19982;&#35841;&#21512;&#20316;&#65292;&#27599;&#20010;&#25215;&#36816;&#21830;&#24212;&#35813;&#24471;&#21040;&#22810;&#23569;&#34917;&#20607;&#21602;&#65311;&#20256;&#32479;&#30340;&#21338;&#24328;&#35770;&#35299;&#26041;&#26696;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#29305;&#24449;&#20989;&#25968;&#38543;&#30528;&#20195;&#29702;&#20154;&#25968;&#37327;&#21576;&#25351;&#25968;&#20493;&#22686;&#12290;&#36825;&#23601;&#38656;&#35201;&#25351;&#25968;&#27425;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;NP-hard&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#35813;&#38382;&#39064;&#24314;&#27169;&#20026;&#20351;&#29992;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#30340;&#21512;&#20316;&#35848;&#21028;&#21338;&#24328;&#65292;&#20851;&#38190;&#26159;&#20195;&#29702;&#20154;&#26080;&#27861;&#35775;&#38382;&#29305;&#24449;&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#38544;&#21547;&#22320;&#25512;&#29702;&#29305;&#24449;&#20989;&#25968;&#65307;&#22240;&#27492;&#65292;&#22312;&#29983;&#20135;&#20013;&#37096;&#32626;&#26102;&#65292;&#20165;&#38656;&#35780;&#20272;&#26114;&#36149;&#30340;&#21512;&#20316;&#21518;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative vehicle routing occurs when carriers collaborate through sharing their transportation requests and performing transportation requests on behalf of each other. This achieves economies of scale, thus reducing cost, greenhouse gas emissions and road congestion. But which carrier should partner with whom, and how much should each carrier be compensated? Traditional game theoretic solution concepts are expensive to calculate as the characteristic function scales exponentially with the number of agents. This would require solving the vehicle routing problem (NP-hard) an exponential number of times. We therefore propose to model this problem as a coalitional bargaining game solved using deep multi-agent reinforcement learning, where - crucially agents are not given access to the characteristic function. Instead, we implicitly reason about the characteristic function; thus, when deployed in production, we only need to evaluate the expensive post-collaboration vehicle routing pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#30340;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38544;&#31169;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17477</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#30005;&#32593;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Secure short-term load forecasting for smart grids with transformer-based federated learning. (arXiv:2310.17477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#30340;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38544;&#31169;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#26159;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#24179;&#34913;&#20379;&#38656;&#12290;&#23613;&#31649;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#30701;&#26399;&#36127;&#33655;&#65292;&#20294;&#32454;&#31890;&#24230;&#30340;&#36127;&#33655;&#26354;&#32447;&#21487;&#33021;&#26292;&#38706;&#29992;&#25143;&#30340;&#29992;&#30005;&#34892;&#20026;&#65292;&#24341;&#21457;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#19968;&#31181;&#25913;&#21892;&#25968;&#25454;&#38544;&#31169;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#32852;&#37030;&#23398;&#20064;&#65292;&#21363;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21442;&#25968;&#21512;&#24182;&#21644;&#26356;&#26032;&#21040;&#20840;&#23616;&#26381;&#21153;&#22120;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#30701;&#26399;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#19982;&#20013;&#24515;&#21270;&#21644;&#26412;&#22320;&#23398;&#20064;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#22522;&#20110;&#24503;&#22269;&#19968;&#25152;&#22823;&#23398;&#26657;&#22253;&#30340;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;Transformer-based federated learning&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electricity load forecasting is an essential task within smart grids to assist demand and supply balance. While advanced deep learning models require large amounts of high-resolution data for accurate short-term load predictions, fine-grained load profiles can expose users' electricity consumption behaviors, which raises privacy and security concerns. One solution to improve data privacy is federated learning, where models are trained locally on private data, and only the trained model parameters are merged and updated on a global server. Therefore, this paper presents a novel transformer-based deep learning approach with federated learning for short-term electricity load prediction. To evaluate our results, we benchmark our federated learning architecture against central and local learning and compare the performance of our model to long short-term memory models and convolutional neural networks. Our simulations are based on a dataset from a German university campus and show that tran
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20174;&#25968;&#25454;&#12289;&#26234;&#33021;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#23454;&#29616;6G&#21407;&#29983;AI&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;6G&#21407;&#29983;AI&#26694;&#26550;&#65292;&#21253;&#25324;&#33258;&#23450;&#20041;&#26041;&#27861;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26032;&#30340;&#20113;&#36793;&#32536;&#21327;&#21516;&#21512;&#20316;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.17471</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;6G&#21407;&#29983;AI&#26694;&#26550;&#19982;&#20113;&#36793;&#32536;&#21327;&#21516;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Foundation Model Based Native AI Framework in 6G with Cloud-Edge-End Collaboration. (arXiv:2310.17471v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20174;&#25968;&#25454;&#12289;&#26234;&#33021;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#23454;&#29616;6G&#21407;&#29983;AI&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;6G&#21407;&#29983;AI&#26694;&#26550;&#65292;&#21253;&#25324;&#33258;&#23450;&#20041;&#26041;&#27861;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26032;&#30340;&#20113;&#36793;&#32536;&#21327;&#21516;&#21512;&#20316;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#26377;&#26395;&#36229;&#36234;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#12289;&#20197;&#35774;&#22791;&#20026;&#23548;&#21521;&#30340;&#36830;&#25509;&#26041;&#24335;&#65292;&#25552;&#20379;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#36830;&#25509;&#30340;&#26234;&#33021;&#27785;&#28024;&#24335;&#20307;&#39564;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65288;PFM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;6G&#21407;&#29983;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#21457;&#23637;&#24895;&#26223;&#19979;&#12290;&#22240;&#27492;&#65292;&#22312;6G&#20013;&#65292;&#37325;&#26032;&#23450;&#20041;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#27169;&#24335;&#65292;&#26500;&#24314;&#21407;&#29983;&#26234;&#33021;&#24211;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#12289;&#26234;&#33021;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#23454;&#29616;6G&#21407;&#29983;AI&#30340;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;6G&#21407;&#29983;AI&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24847;&#22270;&#24863;&#30693;PFM&#30340;&#23450;&#21046;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#38754;&#21521;&#20219;&#21153;&#30340;AI&#24037;&#20855;&#21253;&#30340;&#26500;&#24314;&#65292;&#24182;&#27010;&#36848;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#20113;&#36793;&#32536;&#21327;&#21516;&#21512;&#20316;&#33539;&#24335;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#32534;&#25490;&#65292;&#23454;&#29616;&#20102;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#26368;&#22823;&#36895;&#29575;&#20043;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future wireless communication networks are in a position to move beyond data-centric, device-oriented connectivity and offer intelligent, immersive experiences based on task-oriented connections, especially in the context of the thriving development of pre-trained foundation models (PFM) and the evolving vision of 6G native artificial intelligence (AI). Therefore, redefining modes of collaboration between devices and servers and constructing native intelligence libraries become critically important in 6G. In this paper, we analyze the challenges of achieving 6G native AI from the perspectives of data, intelligence, and networks. Then, we propose a 6G native AI framework based on foundation models, provide a customization approach for intent-aware PFM, present a construction of a task-oriented AI toolkit, and outline a novel cloud-edge-end collaboration paradigm. As a practical use case, we apply this framework for orchestration, achieving the maximum sum rate within a wireless communic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#20027;&#21160;&#20114;&#34917;&#23398;&#20064;&#26694;&#26550;&#65288;CRCL&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#20027;&#21160;&#20114;&#34917;&#25439;&#22833;&#65288;ACL&#65289;&#21644;&#39640;&#25928;&#30340;&#33258;&#25105;&#23436;&#21892;&#23545;&#24212;&#20851;&#31995;&#20462;&#27491;&#65288;SCC&#65289;&#65292;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17468</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#20027;&#21160;&#20114;&#34917;&#23398;&#20064;&#19982;&#33258;&#25105;&#23436;&#21892;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Cross-modal Active Complementary Learning with Self-refining Correspondence. (arXiv:2310.17468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#20027;&#21160;&#20114;&#34917;&#23398;&#20064;&#26694;&#26550;&#65288;CRCL&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#20027;&#21160;&#20114;&#34917;&#25439;&#22833;&#65288;ACL&#65289;&#21644;&#39640;&#25928;&#30340;&#33258;&#25105;&#23436;&#21892;&#23545;&#24212;&#20851;&#31995;&#20462;&#27491;&#65288;SCC&#65289;&#65292;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21305;&#37197;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#29702;&#35299;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#28508;&#22312;&#23545;&#24212;&#20851;&#31995;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38544;&#24335;&#20551;&#35774;&#35757;&#32451;&#23545;&#26159;&#23545;&#40784;&#33391;&#22909;&#30340;&#65292;&#32780;&#24573;&#30053;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#27880;&#37322;&#22122;&#38899;&#65292;&#21363;&#22122;&#22768;&#23545;&#24212;&#65288;NC&#65289;&#65292;&#20174;&#32780;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#35299;&#20915;&#36825;&#31181;&#22122;&#22768;&#65292;&#20294;&#20173;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#65306;&#36807;&#24230;&#35760;&#24518;/&#36807;&#25311;&#21512;&#21644;&#23545;&#20110;NC&#30340;&#19981;&#21487;&#38752;&#20462;&#27491;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#22122;&#22768;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36328;&#27169;&#24577;&#40065;&#26834;&#20114;&#34917;&#23398;&#20064;&#26694;&#26550;&#65288;CRCL&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#20114;&#34917;&#25439;&#22833;&#65288;ACL&#65289;&#21644;&#39640;&#25928;&#30340;&#33258;&#25105;&#23436;&#21892;&#23545;&#24212;&#20851;&#31995;&#20462;&#27491;&#65288;SCC&#65289;&#26469;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ACL&#21033;&#29992;&#20027;&#21160;&#21644;&#20114;&#34917;&#30340;&#23398;&#20064;&#25439;&#22833;&#26469;&#20943;&#23569;&#25552;&#20379;&#38169;&#35823;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, image-text matching has attracted more and more attention from academia and industry, which is fundamental to understanding the latent correspondence across visual and textual modalities. However, most existing methods implicitly assume the training pairs are well-aligned while ignoring the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby inevitably leading to a performance drop. Although some methods attempt to address such noise, they still face two challenging problems: excessive memorizing/overfitting and unreliable correction for NC, especially under high noise. To address the two problems, we propose a generalized Cross-modal Robust Complementary Learning framework (CRCL), which benefits from a novel Active Complementary Loss (ACL) and an efficient Self-refining Correspondence Correction (SCC) to improve the robustness of existing methods. Specifically, ACL exploits active and complementary learning losses to reduce the risk of providing erroneous s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#24212;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20108;&#38454;&#30456;&#21464;&#29616;&#35937;&#65292;&#24182;&#19988;&#35748;&#20026;&#36825;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2310.17467</link><description>&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#35745;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
The statistical thermodynamics of generative diffusion models. (arXiv:2310.17467v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#24212;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20108;&#38454;&#30456;&#21464;&#29616;&#35937;&#65292;&#24182;&#19988;&#35748;&#20026;&#36825;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#24605;&#24819;&#26469;&#33258;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#65292;&#20294;&#26412;&#25991;&#20013;&#25105;&#20204;&#34920;&#26126;&#65292;&#21487;&#20197;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#26469;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#31181;&#37325;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#32463;&#21382;&#20102;&#19982;&#23545;&#31216;&#24615;&#30772;&#32570;&#29616;&#35937;&#30456;&#23545;&#24212;&#30340;&#20108;&#38454;&#30456;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#65292;&#23427;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#26680;&#24515;&#65292;&#24182;&#21487;&#20197;&#29992;&#19968;&#32452;&#24179;&#22343;&#22330;&#20020;&#30028;&#25351;&#25968;&#26469;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#28909;&#21147;&#23398;&#30340;&#20844;&#24335;&#20998;&#26512;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20851;&#32852;&#35760;&#24518;&#32593;&#32476;&#36830;&#25509;&#30340;&#26368;&#36817;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We argue that this lead to a form of instability that lies at the heart of their generative capabilities and that can be described by a set of mean field critical exponents. We conclude by analyzing recent work connecting diffusion models and associative memory networks in view of the thermodynamic formulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#23545;&#28508;&#22312;&#32467;&#26524;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.17463</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation. (arXiv:2310.17463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#23545;&#28508;&#22312;&#32467;&#26524;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#21307;&#23398;&#20013;&#65292;&#36830;&#32493;&#26102;&#38388;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#32473;&#20986;&#28508;&#22312;&#32467;&#26524;&#30340;&#28857;&#20272;&#35745;&#65292;&#24573;&#30053;&#20102;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#21487;&#38752;&#30340;&#20915;&#31574;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;(BNCDE)&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#22312;&#25105;&#20204;&#30340;BNCDE&#20013;&#65292;&#26102;&#38388;&#32500;&#24230;&#36890;&#36807;&#19968;&#32452;&#32806;&#21512;&#30340;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#21644;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#65292;&#20854;&#20013;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20801;&#35768;&#21487;&#34892;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#27835;&#30103;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;BNCDE&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#32467;&#26524;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#25552;&#20379;&#27835;&#30103;&#25928;&#26524;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31070;&#32463;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation in continuous time is crucial for personalized medicine. However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored. Needless to say, uncertainty quantification is crucial for reliable decision-making in medical applications. To fill this gap, we propose a novel Bayesian neural controlled differential equation (BNCDE) for treatment effect estimation in continuous time. In our BNCDE, the time dimension is modeled through a coupled system of neural controlled differential equations and neural stochastic differential equations, where the neural stochastic differential equations allow for tractable variational Bayesian inference. Thereby, for an assigned sequence of treatments, our BNCDE provides meaningful posterior predictive distributions of the potential outcomes. To the best of our knowledge, ours is the first tailored neural method to provide uncertainty estimates of tre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;3D&#29289;&#20307;&#23450;&#20301;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17462</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#36816;&#21160;&#23450;&#24459;&#20174;2D&#26631;&#27880;&#20013;&#23398;&#20064;&#21333;&#30446;3D&#29289;&#20307;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;3D&#29289;&#20307;&#23450;&#20301;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#30340;&#36816;&#21160;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#20010;&#26657;&#20934;&#30456;&#26426;&#30340;&#21333;&#20010;&#22270;&#20687;&#20013;&#31934;&#30830;&#23450;&#20301;3D&#29289;&#20307;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#38544;&#21547;&#30340;&#31532;&#19977;&#20010;&#32500;&#24230;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#27492;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20316;&#20026;&#23398;&#20064;3D&#29289;&#20307;&#23450;&#20301;&#20272;&#35745;&#30340;&#19968;&#27493;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for precise 3D object localization in single images from a single calibrated camera using only 2D labels. No expensive 3D labels are needed. Thus, instead of using 3D labels, our model is trained with easy-to-annotate 2D labels along with the physical knowledge of the object's motion. Given this information, the model can infer the latent third dimension, even though it has never seen this information during training. Our method is evaluated on both synthetic and real-world datasets, and we are able to achieve a mean distance error of just 6 cm in our experiments on real data. The results indicate the method's potential as a step towards learning 3D object location estimation, where collecting 3D data for training is not feasible.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#20013;&#21512;&#20316;&#20249;&#20276;&#36873;&#25321;&#21644;&#34917;&#20607;&#20998;&#37197;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#32852;&#21512;&#35752;&#20215;&#36824;&#20215;&#21338;&#24328;&#65292;&#36890;&#36807;&#38544;&#24335;&#25512;&#29702;&#29305;&#24449;&#20989;&#25968;&#26469;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17458</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#21512;&#35752;&#20215;&#36824;&#20215;&#65306;&#24212;&#29992;&#20110;&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Coalitional Bargaining via Reinforcement Learning: An Application to Collaborative Vehicle Routing. (arXiv:2310.17458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#20013;&#21512;&#20316;&#20249;&#20276;&#36873;&#25321;&#21644;&#34917;&#20607;&#20998;&#37197;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#32852;&#21512;&#35752;&#20215;&#36824;&#20215;&#21338;&#24328;&#65292;&#36890;&#36807;&#38544;&#24335;&#25512;&#29702;&#29305;&#24449;&#20989;&#25968;&#26469;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#26159;&#25351;&#22810;&#23478;&#36816;&#36755;&#20844;&#21496;&#36890;&#36807;&#20849;&#20139;&#36816;&#36755;&#20449;&#24687;&#21644;&#20195;&#34920;&#23545;&#26041;&#25191;&#34892;&#36816;&#36755;&#35831;&#27714;&#32780;&#36827;&#34892;&#21512;&#20316;&#12290;&#36825;&#26679;&#21487;&#20197;&#23454;&#29616;&#35268;&#27169;&#32463;&#27982;&#65292;&#20174;&#32780;&#38477;&#20302;&#25104;&#26412;&#12289;&#20943;&#23569;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#21644;&#36947;&#36335;&#25317;&#25380;&#12290;&#28982;&#32780;&#65292;&#21738;&#23478;&#20844;&#21496;&#24212;&#35813;&#19982;&#21738;&#23478;&#20844;&#21496;&#21512;&#20316;&#65292;&#27599;&#23478;&#20844;&#21496;&#24212;&#35813;&#24471;&#21040;&#22810;&#23569;&#34917;&#20607;&#65292;&#20256;&#32479;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#27010;&#24565;&#65292;&#22914;Shapley&#20540;&#25110;&#26680;&#20540;&#65292;&#30001;&#20110;&#29305;&#24449;&#20989;&#25968;&#38543;&#21442;&#19982;&#32773;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#38590;&#20197;&#35745;&#31639;&#29992;&#20110;&#23454;&#38469;&#30340;&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36825;&#23558;&#38656;&#35201;&#25351;&#25968;&#27425;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#35813;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#31181;&#32852;&#21512;&#35752;&#20215;&#36824;&#20215;&#21338;&#24328;&#65292;&#37325;&#35201;&#30340;&#26159;&#65292;&#20195;&#29702;&#21830;&#19981;&#34987;&#36171;&#20104;&#29305;&#24449;&#20989;&#25968;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#38544;&#24335;&#25512;&#29702;&#29305;&#24449;&#20989;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#35780;&#20272;VRP&#30340;&#38656;&#35201;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Vehicle Routing is where delivery companies cooperate by sharing their delivery information and performing delivery requests on behalf of each other. This achieves economies of scale and thus reduces cost, greenhouse gas emissions, and road congestion. But which company should partner with whom, and how much should each company be compensated? Traditional game theoretic solution concepts, such as the Shapley value or nucleolus, are difficult to calculate for the real-world problem of Collaborative Vehicle Routing due to the characteristic function scaling exponentially with the number of agents. This would require solving the Vehicle Routing Problem (an NP-Hard problem) an exponential number of times. We therefore propose to model this problem as a coalitional bargaining game where - crucially - agents are not given access to the characteristic function. Instead, we implicitly reason about the characteristic function, and thus eliminate the need to evaluate the VRP an exp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#25163;&#35821;&#35782;&#21035;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21463;&#24103;&#24207;&#21015;&#32422;&#26463;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#21033;&#29992;&#20301;&#32622;&#12289;&#36816;&#21160;&#21644;&#25163;&#24418;&#31561;&#29305;&#24449;&#36827;&#34892;&#25163;&#21183;&#20998;&#31867;&#65292;&#36890;&#36807;&#35789;&#34955;&#27169;&#22411;&#26041;&#27861;&#25506;&#32034;&#25163;&#21183;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.17437</link><description>&lt;p&gt;
&#19981;&#21463;&#24103;&#24207;&#21015;&#32422;&#26463;&#30340;&#25163;&#35821;&#35782;&#21035;&#65306;&#38024;&#23545;&#38463;&#26681;&#24311;&#25163;&#35821;&#30340;&#27010;&#24565;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sign Languague Recognition without frame-sequencing constraints: A proof of concept on the Argentinian Sign Language. (arXiv:2310.17437v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17437
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#25163;&#35821;&#35782;&#21035;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21463;&#24103;&#24207;&#21015;&#32422;&#26463;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#21033;&#29992;&#20301;&#32622;&#12289;&#36816;&#21160;&#21644;&#25163;&#24418;&#31561;&#29305;&#24449;&#36827;&#34892;&#25163;&#21183;&#20998;&#31867;&#65292;&#36890;&#36807;&#35789;&#34955;&#27169;&#22411;&#26041;&#27861;&#25506;&#32034;&#25163;&#21183;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25163;&#35821;&#35782;&#21035;&#65288;SLR&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#23427;&#20855;&#26377;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#28041;&#21450;&#21508;&#31181;&#30693;&#35782;&#39046;&#22495;&#65292;&#22914;&#35270;&#39057;&#22788;&#29702;&#12289;&#22270;&#20687;&#22788;&#29702;&#12289;&#26234;&#33021;&#31995;&#32479;&#21644;&#35821;&#35328;&#23398;&#12290;&#25163;&#35821;&#30340;&#31283;&#20581;&#35782;&#21035;&#21487;&#20197;&#24110;&#21161;&#32763;&#35793;&#36807;&#31243;&#12289;&#21548;&#21147;&#38556;&#30861;&#32773;&#30340;&#34701;&#20837;&#65292;&#20197;&#21450;&#23545;&#21548;&#35273;&#20154;&#32676;&#36827;&#34892;&#25163;&#35821;&#25945;&#23398;&#12290;SLR&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#12289;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#25110;&#31867;&#20284;&#27169;&#22411;&#26469;&#35782;&#21035;&#25163;&#21183;&#12290;&#36825;&#20123;&#25216;&#26415;&#21033;&#29992;&#24103;&#30340;&#39034;&#24207;&#26469;&#20943;&#23569;&#20551;&#35774;&#30340;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25163;&#21183;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#19981;&#21516;&#31867;&#22411;&#29305;&#24449;&#65288;&#22914;&#20301;&#32622;&#12289;&#36816;&#21160;&#21644;&#25163;&#24418;&#65289;&#30340;&#23376;&#20998;&#31867;&#22120;&#12290;&#27169;&#22411;&#22312;&#25152;&#26377;&#20998;&#31867;&#27493;&#39588;&#20013;&#37319;&#29992;&#35789;&#34955;&#27169;&#22411;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#25163;&#21183;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic sign language recognition (SLR) is an important topic within the areas of human-computer interaction and machine learning. On the one hand, it poses a complex challenge that requires the intervention of various knowledge areas, such as video processing, image processing, intelligent systems and linguistics. On the other hand, robust recognition of sign language could assist in the translation process and the integration of hearing-impaired people, as well as the teaching of sign language for the hearing population.  SLR systems usually employ Hidden Markov Models, Dynamic Time Warping or similar models to recognize signs. Such techniques exploit the sequential ordering of frames to reduce the number of hypothesis. This paper presents a general probabilistic model for sign classification that combines sub-classifiers based on different types of features such as position, movement and handshape. The model employs a bag-of-words approach in all classification steps, to explore t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21463;&#36755;&#20837;&#26679;&#26412;&#22797;&#26434;&#24615;&#24433;&#21709;&#30340;&#35266;&#23519;&#65292;&#26500;&#24314;&#20102;&#22797;&#26434;&#24230;&#26657;&#27491;&#20284;&#28982;&#27604;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#31163;&#32676;&#26816;&#27979;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.17432</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#31163;&#32676;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Out-of-Distribution Detection with Denoising Diffusion Probabilistic Models. (arXiv:2310.17432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21463;&#36755;&#20837;&#26679;&#26412;&#22797;&#26434;&#24615;&#24433;&#21709;&#30340;&#35266;&#23519;&#65292;&#26500;&#24314;&#20102;&#22797;&#26434;&#24230;&#26657;&#27491;&#20284;&#28982;&#27604;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#31163;&#32676;&#26816;&#27979;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#31163;&#32676;&#26816;&#27979;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#20284;&#28982;&#30340;&#31163;&#32676;&#26816;&#27979;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21463;&#36755;&#20837;&#26679;&#26412;&#22797;&#26434;&#24615;&#24433;&#21709;&#30340;&#35266;&#23519;&#26469;&#25193;&#23637;&#21040;&#25193;&#25955;&#27169;&#22411;&#12290;&#30446;&#21069;&#65292;&#25152;&#26377;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#37325;&#26500;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32676;&#26816;&#27979;&#20284;&#28982;&#27604;&#65292;&#31216;&#20026;&#22797;&#26434;&#24230;&#26657;&#27491;&#20284;&#28982;&#27604;&#12290;&#25105;&#20204;&#30340;&#20284;&#28982;&#27604;&#26159;&#36890;&#36807;&#22312;&#21508;&#31181;&#22122;&#22768;&#27700;&#24179;&#19979;&#20351;&#29992;&#20010;&#20307;&#27169;&#22411;&#30340;&#35777;&#25454;&#19979;&#30028;&#35780;&#20272;&#26500;&#36896;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#30340;&#32467;&#26524;&#19982;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution detection between dataset pairs has been extensively explored with generative models. We show that likelihood-based Out-of-Distribution detection can be extended to diffusion models by leveraging the fact that they, like other likelihood-based generative models, are dramatically affected by the input sample complexity. Currently, all Out-of-Distribution detection methods with Diffusion Models are reconstruction-based. We propose a new likelihood ratio for Out-of-Distribution detection with Deep Denoising Diffusion Models, which we call the Complexity Corrected Likelihood Ratio. Our likelihood ratio is constructed using Evidence Lower-Bound evaluations from an individual model at various noising levels. We present results that are comparable to state-of-the-art Out-of-Distribution detection methods with generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#39318;&#27425;&#21019;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#26681;&#24311;&#25163;&#35821;&#30340;&#25163;&#21183;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ProbSom&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#21644;&#25163;&#21183;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;&#30446;&#21069;&#30340;&#30740;&#31350;&#20013;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2310.17427</link><description>&lt;p&gt;
&#20351;&#29992;ProbSom&#36827;&#34892;&#38463;&#26681;&#24311;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Handshape recognition for Argentinian Sign Language using ProbSom. (arXiv:2310.17427v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#39318;&#27425;&#21019;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#26681;&#24311;&#25163;&#35821;&#30340;&#25163;&#21183;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ProbSom&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#21644;&#25163;&#21183;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;&#30446;&#21069;&#30340;&#30740;&#31350;&#20013;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25163;&#35821;&#35782;&#21035;&#26159;&#20154;&#26426;&#20132;&#20114;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#36825;&#26082;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#28041;&#21450;&#35270;&#39057;&#22788;&#29702;&#12289;&#22270;&#20687;&#22788;&#29702;&#12289;&#26234;&#33021;&#31995;&#32479;&#21644;&#35821;&#35328;&#23398;&#31561;&#22810;&#20010;&#30693;&#35782;&#39046;&#22495;&#30340;&#20171;&#20837;&#65292;&#21448;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25163;&#35821;&#35782;&#21035;&#65292;&#21487;&#20197;&#36741;&#21161;&#32763;&#35793;&#36807;&#31243;&#21644;&#34701;&#20837;&#21548;&#38556;&#20154;&#22763;&#30340;&#25972;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#26681;&#24311;&#25163;&#35821;&#65288;LSA&#65289;&#30340;&#25163;&#21183;&#25968;&#25454;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#36804;&#20170;&#20026;&#27490;&#20960;&#20046;&#27809;&#26377;&#35752;&#35770;&#36807;&#30340;&#35805;&#39064;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#19968;&#31181;&#21517;&#20026;ProbSom&#30340;&#33258;&#32452;&#32455;&#26144;&#23556;&#30340;&#30417;&#30563;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#21518;&#32493;&#25163;&#21183;&#20998;&#31867;&#12290;&#35813;&#25216;&#26415;&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic sign language recognition is an important topic within the areas of human-computer interaction and machine learning. On the one hand, it poses a complex challenge that requires the intervention of various knowledge areas, such as video processing, image processing, intelligent systems and linguistics. On the other hand, robust recognition of sign language could assist in the translation process and the integration of hearing-impaired people.  This paper offers two main contributions: first, the creation of a database of handshapes for the Argentinian Sign Language (LSA), which is a topic that has barely been discussed so far. Secondly, a technique for image processing, descriptor extraction and subsequent handshape classification using a supervised adaptation of self-organizing maps that is called ProbSom. This technique is compared to others in the state of the art, such as Support Vector Machines (SVM), Random Forests, and Neural Networks.  The database that was built conta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#31995;&#32479;&#34892;&#20026;&#65292;&#19981;&#38656;&#35201;&#22240;&#26524;&#22270;&#30340;&#24418;&#24335;&#21270;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#24178;&#39044;&#30340;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.17405</link><description>&lt;p&gt;
&#24102;&#26377;&#24179;&#31283;&#25193;&#25955;&#30340;&#22240;&#26524;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Causal Modeling with Stationary Diffusions. (arXiv:2310.17405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#31995;&#32479;&#34892;&#20026;&#65292;&#19981;&#38656;&#35201;&#22240;&#26524;&#22270;&#30340;&#24418;&#24335;&#21270;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#24178;&#39044;&#30340;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;&#19982;&#20351;&#29992;&#22240;&#26524;&#22270;&#30340;&#32467;&#26500;&#26041;&#31243;&#19981;&#21516;&#65292;&#25105;&#20204;&#23398;&#20064;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#65292;&#20854;&#24179;&#31283;&#23494;&#24230;&#21487;&#20197;&#27169;&#25311;&#31995;&#32479;&#22312;&#24178;&#39044;&#19979;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#24179;&#31283;&#25193;&#25955;&#27169;&#22411;&#19981;&#38656;&#35201;&#22240;&#26524;&#22270;&#30340;&#24418;&#24335;&#21270;&#65292;&#26356;&#19981;&#38656;&#35201;&#24120;&#35265;&#30340;&#26080;&#29615;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#21464;&#37327;&#19978;&#30340;&#26410;&#35265;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#25512;&#26029;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#34920;&#36798;&#20102;&#25193;&#25955;&#30340;&#29983;&#25104;&#22120;&#30340;&#31283;&#23450;&#26465;&#20214;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26680;&#20174;&#31283;&#24577;&#20559;&#31163;(KDS)&#26159;&#19968;&#20010;&#20540;&#24471;&#29420;&#31435;&#20851;&#27880;&#30340;&#23458;&#35266;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel approach towards causal inference. Rather than structural equations over a causal graph, we learn stochastic differential equations (SDEs) whose stationary densities model a system's behavior under interventions. These stationary diffusion models do not require the formalism of causal graphs, let alone the common assumption of acyclicity. We show that in several cases, they generalize to unseen interventions on their variables, often better than classical approaches. Our inference method is based on a new theoretical result that expresses a stationarity condition on the diffusion's generator in a reproducing kernel Hilbert space. The resulting kernel deviation from stationarity (KDS) is an objective function of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#19981;&#21464;&#24615;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25935;&#24863;&#19988;&#21487;&#35299;&#37322;&#65292;&#24182;&#33021;&#24212;&#29992;&#20110;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#22312;&#20223;&#23556;&#21464;&#25442;&#39046;&#22495;&#21644;CIFAR10&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#19981;&#21464;&#24615;&#23545;&#20110;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#38750;&#24120;&#31283;&#23450;&#65292;&#20294;&#23545;&#20110;&#25968;&#25454;&#38598;&#25110;&#21464;&#25442;&#30340;&#25913;&#21464;&#19981;&#31283;&#23450;&#12290;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#23558;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17404</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21464;&#24615;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Invariance Measures for Neural Networks. (arXiv:2310.17404v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#19981;&#21464;&#24615;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25935;&#24863;&#19988;&#21487;&#35299;&#37322;&#65292;&#24182;&#33021;&#24212;&#29992;&#20110;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#22312;&#20223;&#23556;&#21464;&#25442;&#39046;&#22495;&#21644;CIFAR10&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#19981;&#21464;&#24615;&#23545;&#20110;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#38750;&#24120;&#31283;&#23450;&#65292;&#20294;&#23545;&#20110;&#25968;&#25454;&#38598;&#25110;&#21464;&#25442;&#30340;&#25913;&#21464;&#19981;&#31283;&#23450;&#12290;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#23558;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#21464;&#24615;&#23545;&#35768;&#22810;&#20219;&#21153;&#37117;&#26159;&#26377;&#29992;&#19988;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#19981;&#21464;&#24615;&#34920;&#31034;&#23578;&#26410;&#34987;&#26126;&#30830;&#34920;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#19981;&#21464;&#24615;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#19982;&#20043;&#21069;&#23450;&#20041;&#30340;&#27979;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#23545;&#19981;&#21464;&#24615;&#26356;&#20026;&#25935;&#24863;&#12290;&#25105;&#20204;&#22312;&#20223;&#23556;&#21464;&#25442;&#39046;&#22495;&#21644;CIFAR10&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#21450;&#20854;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#31283;&#23450;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#21033;&#29992;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;CNN&#27169;&#22411;&#36827;&#34892;&#20102;&#39318;&#27425;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#20869;&#37096;&#19981;&#21464;&#24615;&#23545;&#20110;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#38750;&#24120;&#31283;&#23450;&#65292;&#20294;&#23545;&#20110;&#25968;&#25454;&#38598;&#25110;&#21464;&#25442;&#30340;&#25913;&#21464;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#23558;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariances in neural networks are useful and necessary for many tasks. However, the representation of the invariance of most neural network models has not been characterized. We propose measures to quantify the invariance of neural networks in terms of their internal representation. The measures are efficient and interpretable, and can be applied to any neural network model. They are also more sensitive to invariance than previously defined measures. We validate the measures and their properties in the domain of affine transformations and the CIFAR10 and MNIST datasets, including their stability and interpretability. Using the measures, we perform a first analysis of CNN models and show that their internal invariance is remarkably stable to random weight initializations, but not to changes in dataset or transformation. We believe the measures will enable new avenues of research in invariance representation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20809;&#27969;&#20013;&#30340;&#23545;&#25239;&#24615;&#36148;&#29255;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#26816;&#27979;&#21644;&#21435;&#38500;&#38450;&#24481;&#31574;&#30053;&#19981;&#20165;&#38477;&#20302;&#20102;&#20809;&#27969;&#36136;&#37327;&#65292;&#21516;&#26102;&#20063;&#25439;&#23475;&#20102;&#25269;&#24481;&#36148;&#29255;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17403</link><description>&lt;p&gt;
&#26816;&#27979;&#38450;&#24481;: &#20809;&#27969;&#20013;&#23545;&#25239;&#24615;&#36148;&#29255;&#25915;&#20987;&#30340;&#31354;&#27934;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow. (arXiv:2310.17403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20809;&#27969;&#20013;&#30340;&#23545;&#25239;&#24615;&#36148;&#29255;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#26816;&#27979;&#21644;&#21435;&#38500;&#38450;&#24481;&#31574;&#30053;&#19981;&#20165;&#38477;&#20302;&#20102;&#20809;&#27969;&#36136;&#37327;&#65292;&#21516;&#26102;&#20063;&#25439;&#23475;&#20102;&#25269;&#24481;&#36148;&#29255;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25918;&#32622;&#22312;&#20219;&#24847;&#22330;&#26223;&#20301;&#32622;&#26102;&#65292;&#23545;&#25239;&#24615;&#36148;&#29255;&#30772;&#22351;&#20102;&#20809;&#27969;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36816;&#21160;&#26816;&#27979;&#21450;&#20854;&#19979;&#28216;&#24212;&#29992;&#26500;&#25104;&#20102;&#30495;&#23454;&#23041;&#32961;&#12290;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#26816;&#27979;&#21644;&#21435;&#38500;&#23545;&#25239;&#24615;&#36148;&#29255;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#20294;&#20854;&#23545;&#24213;&#23618;&#36816;&#21160;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#21487;&#29992;&#30340;&#26816;&#27979;&#21644;&#21435;&#38500;&#38450;&#24481;&#31574;&#30053;ILP&#21644;LGS&#23545;&#19968;&#31995;&#21015;&#20808;&#36827;&#20809;&#27969;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#23545;&#26368;&#32456;&#20809;&#27969;&#39044;&#27979;&#30340;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340;&#21103;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#38450;&#24481;&#24863;&#30693;&#25915;&#20987;&#65292;&#20197;&#35843;&#26597;&#24403;&#21069;&#30340;&#38450;&#24481;&#26159;&#21542;&#33021;&#22815;&#25269;&#24481;&#32771;&#34385;&#38450;&#24481;&#26426;&#21046;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#24471;&#20986;&#20102;&#20004;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65306;&#26816;&#27979;&#21644;&#21435;&#38500;&#38450;&#24481;&#31574;&#30053;&#19981;&#20165;&#38477;&#20302;&#20102;&#33391;&#22909;&#22330;&#26223;&#19979;&#30340;&#20809;&#27969;&#36136;&#37327;&#65292;&#21516;&#26102;&#20063;&#25439;&#23475;&#20102;&#38024;&#23545;&#36148;&#29255;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial patches undermine the reliability of optical flow predictions when placed in arbitrary scene locations. Therefore, they pose a realistic threat to real-world motion detection and its downstream applications. Potential remedies are defense strategies that detect and remove adversarial patches, but their influence on the underlying motion prediction has not been investigated. In this paper, we thoroughly examine the currently available detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art optical flow methods, and illuminate their side effects on the quality and robustness of the final flow predictions. In particular, we implement defense-aware attacks to investigate whether current defenses are able to withstand attacks that take the defense mechanism into account. Our experiments yield two surprising results: Detect-and-remove defenses do not only lower the optical flow quality on benign scenes, in doing so, they also harm the robustness under patc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25552;&#31034;&#26041;&#27861;&#65288;SAP&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#35843;&#25972;&#38454;&#27573;&#37117;&#19968;&#33268;&#22320;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17394</link><description>&lt;p&gt;
&#20197;&#32467;&#26500;&#20026;&#22522;&#30784;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Enhancing Graph Neural Networks with Structure-Based Prompt. (arXiv:2310.17394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17394
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25552;&#31034;&#26041;&#27861;&#65288;SAP&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#35843;&#25972;&#38454;&#27573;&#37117;&#19968;&#33268;&#22320;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#35821;&#20041;&#26041;&#38754;&#20855;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8220;pre-train, prompt&#8221;&#22312;&#20351;&#29992;&#36739;&#23569;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;GNNs&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33539;&#24335;&#30340;&#25104;&#21151;&#21487;&#20197;&#24402;&#22240;&#20110;&#39044;&#35757;&#32451;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#30340;&#26356;&#19968;&#33268;&#30340;&#30446;&#26631;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#26159;&#65292;&#22312;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#36890;&#24120;&#21033;&#29992;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#22312;&#25552;&#31034;&#35843;&#25972;&#38454;&#27573;&#24573;&#35270;&#20102;&#32467;&#26500;&#20449;&#24687;&#30340;&#21033;&#29992;&#20197;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20197;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;GNNs&#25552;&#31034;&#26041;&#27861;&#65292;&#21363;SAP&#65292;&#23427;&#22312;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#35843;&#25972;&#38454;&#27573;&#37117;&#19968;&#33268;&#22320;&#21033;&#29992;&#20102;&#32467;&#26500;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAP 1&#65289;&#37319;&#29992;&#20102;&#21452;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23545;&#40784;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#30340;&#28508;&#22312;&#35821;&#20041;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are powerful in learning semantics of graph data. Recently, a new paradigm "pre-train, prompt" has shown promising results in adapting GNNs to various tasks with less supervised data. The success of such paradigm can be attributed to the more consistent objectives of pre-training and task-oriented prompt tuning, where the pre-trained knowledge can be effectively transferred to downstream tasks. However, an overlooked issue of existing studies is that the structure information of graph is usually exploited during pre-training for learning node representations, while neglected in the prompt tuning stage for learning task-specific parameters. To bridge this gap, we propose a novel structure-based prompting method for GNNs, namely SAP, which consistently exploits structure information in both pre-training and prompt tuning stages. In particular, SAP 1) employs a dual-view contrastive learning to align the latent semantic spaces of node attributes and graph stru
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20195;&#29702;&#21482;&#33021;&#36890;&#36807;&#37051;&#23621;&#20132;&#25442;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#19978;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#36951;&#25022;&#20540;&#26174;&#33879;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#25439;&#22833;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#21487;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.17385</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#65306;&#20542;&#21548;&#31038;&#21306;&#30340;&#21927;&#22179;
&lt;/p&gt;
&lt;p&gt;
Multitask Online Learning: Listen to the Neighborhood Buzz. (arXiv:2310.17385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17385
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20195;&#29702;&#21482;&#33021;&#36890;&#36807;&#37051;&#23621;&#20132;&#25442;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#19978;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#36951;&#25022;&#20540;&#26174;&#33879;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#25439;&#22833;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#21487;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#20195;&#29702;&#21482;&#33021;&#22312;&#20219;&#24847;&#36890;&#20449;&#32593;&#32476;&#19978;&#19982;&#20854;&#37051;&#23621;&#20132;&#25442;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#31639;&#27861;$\texttt{MT-CO}_2\texttt{OL}$&#65292;&#20854;&#36951;&#25022;&#20540;&#21462;&#20915;&#20110;&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#32593;&#32476;&#32467;&#26500;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;$\texttt{MT-CO}_2\texttt{OL}$&#30340;&#36951;&#25022;&#20540;&#65288;&#24120;&#25968;&#38500;&#22806;&#65289;&#27704;&#36828;&#19981;&#20250;&#27604;&#20195;&#29702;&#19981;&#20849;&#20139;&#20449;&#24687;&#26102;&#33719;&#24471;&#30340;&#19978;&#30028;&#24046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;&#30456;&#37051;&#20195;&#29702;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#19978;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26174;&#33879;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25439;&#22833;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#38544;&#31169;&#20445;&#25252;&#24615;&#19978;&#20570;&#21040;&#24494;&#19981;&#36275;&#36947;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multitask online learning in a setting where agents can only exchange information with their neighbors on an arbitrary communication network. We introduce $\texttt{MT-CO}_2\texttt{OL}$, a decentralized algorithm for this setting whose regret depends on the interplay between the task similarities and the network structure. Our analysis shows that the regret of $\texttt{MT-CO}_2\texttt{OL}$ is never worse (up to constants) than the bound obtained when agents do not share information. On the other hand, our bounds significantly improve when neighboring agents operate on similar tasks. In addition, we prove that our algorithm can be made differentially private with a negligible impact on the regret when the losses are linear. Finally, we provide experimental support for our theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#21644;&#30524;&#21160;&#36861;&#36394;&#30340;&#28216;&#25103;&#31867;&#22411;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#20998;&#31867;&#22120;&#22312;&#19981;&#21516;&#28216;&#25103;&#21644;&#28216;&#25103;&#38388;&#26242;&#20572;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#26234;&#33021;&#30417;&#25511;&#21644;&#37327;&#21270;&#33258;&#36523;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.17383</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#21644;&#30524;&#21160;&#36861;&#36394;&#30340;&#28216;&#25103;&#31867;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
On the recognition of the game type based on physiological signals and eye tracking. (arXiv:2310.17383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#21644;&#30524;&#21160;&#36861;&#36394;&#30340;&#28216;&#25103;&#31867;&#22411;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#20998;&#31867;&#22120;&#22312;&#19981;&#21516;&#28216;&#25103;&#21644;&#28216;&#25103;&#38388;&#26242;&#20572;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#26234;&#33021;&#30417;&#25511;&#21644;&#37327;&#21270;&#33258;&#36523;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#35299;&#35835;&#20449;&#21495;&#65292;&#22312;&#24773;&#24863;&#35745;&#31639;&#21644;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#39046;&#22495;&#24471;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29305;&#23450;&#20449;&#21495;&#38598;&#21512;&#30340;&#22522;&#30784;&#19978;&#26159;&#21542;&#21487;&#20197;&#35782;&#21035;&#35748;&#30693;&#27963;&#21160;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20197;&#21442;&#19982;&#32773;&#25152;&#29609;&#28216;&#25103;&#30340;&#35782;&#21035;&#20316;&#20026;&#38382;&#39064;&#25506;&#32034;&#30340;&#23454;&#39564;&#22330;&#25152;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#31181;&#19981;&#21516;&#28216;&#25103;&#65288;&#22826;&#31354;&#20405;&#30053;&#32773;&#12289;&#20420;&#32599;&#26031;&#26041;&#22359;&#12289;&#22612;&#38450;&#65289;&#21644;&#28216;&#25103;&#38388;&#26242;&#20572;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#29420;&#31435;&#20110;&#29609;&#23478;&#21644;&#20381;&#36182;&#20110;&#29609;&#23478;&#20004;&#31181;&#24773;&#20917;&#19979;&#39564;&#35777;&#20102;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#20381;&#36182;&#20110;&#29609;&#23478;&#24773;&#26223;&#20013;&#30340;&#25913;&#36827;&#65292;&#20197;&#21450;&#22312;&#29983;&#29289;&#29305;&#24449;&#20154;&#21592;&#35782;&#21035;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;&#26681;&#25454;&#28216;&#25103;&#20998;&#31867;&#32467;&#26524;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#26234;&#33021;&#30417;&#25511;&#21644;&#37327;&#21270;&#33258;&#36523;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated interpretation of signals yields many impressive applications from the area of affective computing and human activity recognition (HAR). In this paper we ask the question about possibility of cognitive activity recognition on the base of particular set of signals. We use recognition of the game played by the participant as a playground for exploration of the problem. We build classifier of three different games (Space Invaders, Tetris, Tower Defence) and inter-game pause. We validate classifier in the player-independent and player-dependent scenario. We discuss the improvement in the player-dependent scenario in the context of biometric person recognition. On the base of the results obtained in game classification, we consider potential applications in smart surveillance and quantified self.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#24471;&#21040;&#30340;&#21021;&#22987;&#21442;&#25968;&#21521;&#37327;&#20013;&#21487;&#29992;&#30340;&#32593;&#32476;&#38598;&#21512;&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21069;&#21521;ReLU&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;PAC&#31867;&#22411;&#30028;&#38480;&#65292;&#36890;&#36807;&#38480;&#21046;&#32593;&#32476;&#26799;&#24230;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#22312;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#30340;&#28789;&#25935;&#24230;&#65292;&#19981;&#26174;&#24335;&#22320;&#20381;&#36182;&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17378</link><description>&lt;p&gt;
&#22522;&#20110;&#20999;&#21521;&#31354;&#38388;&#20013;&#30340;&#28789;&#25935;&#24230;&#30340;ReLU&#32593;&#32476;&#30340;&#20248;&#21270;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#24471;&#21040;&#30340;&#21021;&#22987;&#21442;&#25968;&#21521;&#37327;&#20013;&#21487;&#29992;&#30340;&#32593;&#32476;&#38598;&#21512;&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21069;&#21521;ReLU&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;PAC&#31867;&#22411;&#30028;&#38480;&#65292;&#36890;&#36807;&#38480;&#21046;&#32593;&#32476;&#26799;&#24230;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#22312;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#30340;&#28789;&#25935;&#24230;&#65292;&#19981;&#26174;&#24335;&#22320;&#20381;&#36182;&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#19968;&#20123;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#28982;&#32780;&#25991;&#29486;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#35299;&#37322;&#20026;&#20160;&#20040;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#65292;&#21516;&#26102;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#24471;&#21040;&#30340;&#21021;&#22987;&#21442;&#25968;&#21521;&#37327;&#20013;&#21487;&#29992;&#30340;&#32593;&#32476;&#38598;&#21512;&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21069;&#21521;ReLU&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;PAC&#31867;&#22411;&#30028;&#38480;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#32593;&#32476;&#26799;&#24230;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#22312;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#30340;&#28789;&#25935;&#24230;&#38480;&#21046;&#22312;&#19968;&#20010;&#30028;&#38480;&#20869;&#12290;&#25152;&#24471;&#21040;&#30340;&#30028;&#38480;&#19981;&#26174;&#24335;&#22320;&#20381;&#36182;&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have given us some very promising results on the generalization ability of deep neural networks, however literature still lacks a comprehensive theory explaining why heavily over-parametrized models are able to generalize well while fitting the training data. In this paper we propose a PAC type bound on the generalization error of feedforward ReLU networks via estimating the Rademacher complexity of the set of networks available from an initial parameter vector via gradient descent. The key idea is to bound the sensitivity of the network's gradient to perturbation of the input data along the optimization trajectory. The obtained bound does not explicitly depend on the depth of the network. Our results are experimentally verified on the MNIST and CIFAR-10 datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#26102;&#31354;&#25193;&#25955;&#27169;&#22411;&#65288;USTD&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#29289;&#32852;&#32593;&#26102;&#20195;&#30340;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#12290;USTD&#32508;&#21512;&#32771;&#34385;&#20102;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#30340;&#26102;&#31354;&#27169;&#24335;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17360</link><description>&lt;p&gt;
&#12298;&#38754;&#21521;&#27010;&#29575;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;&#32479;&#19968;&#21270;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal Graph Learning. (arXiv:2310.17360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#26102;&#31354;&#25193;&#25955;&#27169;&#22411;&#65288;USTD&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#29289;&#32852;&#32593;&#26102;&#20195;&#30340;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#12290;USTD&#32508;&#21512;&#32771;&#34385;&#20102;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#30340;&#26102;&#31354;&#27169;&#24335;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#26102;&#31354;&#22270;&#23398;&#20064;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26234;&#33021;&#22478;&#24066;&#12289;&#20154;&#31867;&#31227;&#21160;&#24615;&#21644;&#27668;&#20505;&#20998;&#26512;&#31561;&#20247;&#22810;&#32593;&#32476;&#24212;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#35299;&#20915;&#19981;&#21516;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#26681;&#25454;&#20219;&#21153;&#29305;&#28857;&#35843;&#25972;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#27169;&#25311;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#19987;&#38376;&#35774;&#35745;&#30340;&#27169;&#22411;&#38480;&#21046;&#20102;&#20854;&#20316;&#20026;&#26222;&#36866;&#26102;&#31354;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#32479;&#19968;&#35270;&#35282;&#24314;&#27169;&#23398;&#20064;&#20219;&#21153;&#65292;&#23558;&#20854;&#35270;&#20026;&#22522;&#20110;&#20849;&#20139;&#26102;&#31354;&#27169;&#24335;&#30340;&#26465;&#20214;&#20449;&#24687;&#39044;&#27979;&#12290;&#22522;&#20110;&#36825;&#19968;&#24314;&#35758;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#26102;&#31354;&#25193;&#25955;&#27169;&#22411;&#65288;USTD&#65289;&#65292;&#22312;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#25193;&#25955;&#26694;&#26550;&#19979;&#32479;&#19968;&#22788;&#29702;&#20219;&#21153;&#12290;USTD&#30340;&#35774;&#35745;&#26159;&#20840;&#38754;&#30340;&#65292;&#21253;&#25324;&#19968;&#20010;&#20849;&#20139;&#26102;&#31354;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21435;&#22122;&#32593;&#32476;&#65292;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal graph learning is a fundamental problem in the Web of Things era, which enables a plethora of Web applications such as smart cities, human mobility and climate analysis. Existing approaches tackle different learning tasks independently, tailoring their models to unique task characteristics. These methods, however, fall short of modeling intrinsic uncertainties in the spatio-temporal data. Meanwhile, their specialized designs limit their universality as general spatio-temporal learning solutions. In this paper, we propose to model the learning tasks in a unified perspective, viewing them as predictions based on conditional information with shared spatio-temporal patterns. Based on this proposal, we introduce Unified Spatio-Temporal Diffusion Models (USTD) to address the tasks uniformly within the uncertainty-aware diffusion framework. USTD is holistically designed, comprising a shared spatio-temporal encoder and attention-based denoising networks that are task-specific. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22825;&#31354;&#22270;&#20687;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22826;&#38451;&#36752;&#29031;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22825;&#31354;&#22270;&#20687;&#29305;&#24449;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20272;&#35745;&#22826;&#38451;&#36752;&#29031;&#24230;&#12290;&#32463;&#36807;&#19982;&#29616;&#26377;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;(&#26426;&#22120;&#32763;&#35793;)</title><link>http://arxiv.org/abs/2310.17356</link><description>&lt;p&gt;
&#22522;&#20110;&#22825;&#31354;&#22270;&#20687;&#30340;&#26426;&#22120;&#23398;&#20064;&#22826;&#38451;&#36752;&#29031;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sky Imager-Based Forecast of Solar Irradiance Using Machine Learning. (arXiv:2310.17356v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22825;&#31354;&#22270;&#20687;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22826;&#38451;&#36752;&#29031;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22825;&#31354;&#22270;&#20687;&#29305;&#24449;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20272;&#35745;&#22826;&#38451;&#36752;&#29031;&#24230;&#12290;&#32463;&#36807;&#19982;&#29616;&#26377;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;(&#26426;&#22120;&#32763;&#35793;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21069;&#39044;&#27979;&#21457;&#30005;&#21378;&#30340;&#36755;&#20986;&#21151;&#29575;&#23545;&#20110;&#30005;&#32593;&#30340;&#31283;&#23450;&#21644;&#30830;&#20445;&#19981;&#38388;&#26029;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#33021;&#28304;&#30340;&#28151;&#27788;&#34892;&#20026;&#65292;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38590;&#24230;&#36739;&#22823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22825;&#31354;&#22270;&#20687;&#20272;&#35745;&#30701;&#26399;&#22826;&#38451;&#36752;&#29031;&#24230;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20174;&#22825;&#31354;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#25216;&#26415;&#26469;&#20272;&#35745;&#22826;&#38451;&#36752;&#29031;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22825;&#31354;&#22270;&#20687;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;16&#24180;&#30340;&#36229;&#36807;350,000&#20010;&#22270;&#20687;&#65292;&#20174;2004&#24180;&#21040;2020&#24180;&#65292;&#27599;&#20010;&#22270;&#20687;&#30340;&#23545;&#24212;&#20840;&#29699;&#27700;&#24179;&#36752;&#29031;&#24230;(GHI)&#20316;&#20026;&#22522;&#20934;&#12290;&#19982;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#26102;&#39044;&#27979;&#21644;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ahead-of-time forecasting of the output power of power plants is essential for the stability of the electricity grid and ensuring uninterrupted service. However, forecasting renewable energy sources is difficult due to the chaotic behavior of natural energy sources. This paper presents a new approach to estimate short-term solar irradiance from sky images. The~proposed algorithm extracts features from sky images and use learning-based techniques to estimate the solar irradiance. The~performance of proposed machine learning (ML) algorithm is evaluated using two publicly available datasets of sky images. The~datasets contain over 350,000 images for an interval of 16 years, from 2004 to 2020, with the corresponding global horizontal irradiance (GHI) of each image as the ground truth. Compared to the state-of-the-art computationally heavy algorithms proposed in the literature, our approach achieves competitive results with much less computational complexity for both nowcasting and forecast
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#31216;&#20026;&#35268;&#21017;&#21069;&#32512;&#26641;&#65292;&#29992;&#20110;&#23384;&#20648;&#30001;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#29983;&#25104;&#30340;&#35268;&#21017;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#32467;&#26500;&#33021;&#22815;&#39640;&#25928;&#22320;&#34920;&#31034;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#25366;&#25496;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.17355</link><description>&lt;p&gt;
&#25506;&#32034;&#35268;&#21017;&#21069;&#32512;&#26641;&#65306;&#19968;&#31181;&#29992;&#20110;&#20851;&#32852;&#35268;&#21017;&#34920;&#31034;&#30340;&#39640;&#25928;&#25968;&#25454;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploring the Trie of Rules: a fast data structure for the representation of association rules. (arXiv:2310.17355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#31216;&#20026;&#35268;&#21017;&#21069;&#32512;&#26641;&#65292;&#29992;&#20110;&#23384;&#20648;&#30001;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#29983;&#25104;&#30340;&#35268;&#21017;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#32467;&#26500;&#33021;&#22815;&#39640;&#25928;&#22320;&#34920;&#31034;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#25366;&#25496;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#25216;&#26415;&#22312;&#20107;&#21153;&#24615;&#25968;&#25454;&#24211;&#19978;&#30340;&#23454;&#29616;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30340;&#39034;&#24207;&#25968;&#25454;&#12290;&#20174;&#22823;&#37327;&#20851;&#32852;&#35268;&#21017;&#20013;&#25552;&#21462;&#27934;&#23519;&#21147;&#34987;&#21457;&#29616;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#12290;&#24403;&#26816;&#26597;&#19968;&#32452;&#35268;&#21017;&#26102;&#65292;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#39640;&#25928;&#22320;&#27719;&#24635;&#21644;&#34920;&#31034;&#26377;&#24847;&#20041;&#30340;&#25366;&#25496;&#30693;&#35782;&#12290;&#35768;&#22810;&#31639;&#27861;&#21644;&#31574;&#30053;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#26469;&#35299;&#20915;&#30693;&#35782;&#25552;&#21462;&#30340;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#25968;&#25454;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;&#26356;&#22909;&#30340;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#26377;&#25928;&#22320;&#24433;&#21709;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#30340;&#36895;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#31216;&#20026;&#35268;&#21017;&#21069;&#32512;&#26641;&#65292;&#29992;&#20110;&#23384;&#20648;&#30001;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#29983;&#25104;&#30340;&#35268;&#21017;&#38598;&#12290;&#32467;&#26524;&#25968;&#25454;&#32467;&#26500;&#26159;&#19968;&#20010;&#30001;&#39044;&#20808;&#25366;&#25496;&#30340;&#35268;&#21017;&#32452;&#25104;&#30340;&#21069;&#32512;&#26641;&#22270;&#32467;&#26500;&#12290;&#36825;&#20010;&#22270;&#23558;&#35268;&#21017;&#20197;&#36335;&#24452;&#30340;&#26041;&#24335;&#23384;&#20648;&#22312;&#21069;&#32512;&#26641;&#20013;&#65292;&#31867;&#20284;&#30340;&#35268;&#21017;&#20250;&#20114;&#30456;&#35206;&#30422;&#12290;&#26641;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#19968;&#26465;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Association rule mining techniques can generate a large volume of sequential data when implemented on transactional databases. Extracting insights from a large set of association rules has been found to be a challenging process. When examining a ruleset, the fundamental question is how to summarise and represent meaningful mined knowledge efficiently. Many algorithms and strategies have been developed to address issue of knowledge extraction; however, the effectiveness of this process can be limited by the data structures. A better data structure can sufficiently affect the speed of the knowledge extraction process. This paper proposes a novel data structure, called the Trie of rules, for storing a ruleset that is generated by association rule mining. The resulting data structure is a prefix-tree graph structure made of pre-mined rules. This graph stores the rules as paths within the prefix-tree in a way that similar rules overlay each other. Each node in the tree represents a rule whe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#20020;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#65292;&#20351;&#29992;&#26032;&#30340;&#21453;&#24212;Smiles-like&#34920;&#31034;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#21270;&#23398;&#21453;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21457;&#29616;&#24494;&#35843;&#21327;&#35758;&#23545;&#27169;&#22411;&#29983;&#25104;&#33539;&#22260;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.17341</link><description>&lt;p&gt;
&#36890;&#36807;&#26242;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20840;&#26032;&#21270;&#23398;&#21453;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks. (arXiv:2310.17341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#20020;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#65292;&#20351;&#29992;&#26032;&#30340;&#21453;&#24212;Smiles-like&#34920;&#31034;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#21270;&#23398;&#21453;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21457;&#29616;&#24494;&#35843;&#21327;&#35758;&#23545;&#27169;&#22411;&#29983;&#25104;&#33539;&#22260;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20351;&#29992;&#26032;&#39062;&#30340;&#21453;&#24212;Smiles-like&#34920;&#31034;&#65288;CGRSmiles&#65289;&#26102;&#65292;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#20020;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#30340;&#21453;&#24212;&#29983;&#25104;&#65292;&#24182;&#30452;&#25509;&#34701;&#21512;&#20102;&#21407;&#23376;&#26144;&#23556;&#12290;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#33258;&#22238;&#24402;&#29305;&#24615;&#32780;&#38395;&#21517;&#65292;&#24182;&#32463;&#24120;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#20351;&#29992;&#65292;&#30452;&#25509;&#24212;&#29992;&#20110;SMILES&#29983;&#25104;&#12290;&#30456;&#23545;&#36739;&#26032;&#30340;TCN&#20855;&#26377;&#31867;&#20284;&#30340;&#24615;&#36136;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24863;&#21463;&#37326;&#65292;&#24182;&#36981;&#23432;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25152;&#38656;&#30340;&#22240;&#26524;&#24615;&#12290;&#36890;&#36807;TCN&#21644;RNN&#34920;&#36798;&#30340;&#20004;&#31181;&#28508;&#22312;&#34920;&#31034;&#30340;&#32452;&#21512;&#30456;&#27604;&#20165;&#20351;&#29992;RNN&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23558;&#19981;&#21516;&#30340;&#24494;&#35843;&#21327;&#35758;&#24212;&#29992;&#20110;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#23545;&#27169;&#22411;&#30340;&#29983;&#25104;&#33539;&#22260;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present here a combination of two networks, Recurrent Neural Networks (RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction generation using the novel Reaction Smiles-like representation of reactions (CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks are known for their autoregressive properties and are frequently used in language modelling with direct application to SMILES generation. The relatively novel TCNs possess similar properties with wide receptive field while obeying the causality required for natural language processing (NLP). The combination of both latent representations expressed through TCN and RNN results in an overall better performance compared to RNN alone. Additionally, it is shown that different fine-tuning protocols have a profound impact on generative scope of the model when applied on a dataset of interest via transfer learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#31283;&#23450;&#24615;&#30340;&#20004;&#31181;&#31867;&#22411;&#65306;&#22402;&#30452;&#31283;&#23450;&#24615;&#21644;&#27700;&#24179;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#30340;&#31616;&#21333;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#32780;&#31283;&#23450;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.17332</link><description>&lt;p&gt;
&#20851;&#20110;&#39044;&#27979;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Forecast Stability. (arXiv:2310.17332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#31283;&#23450;&#24615;&#30340;&#20004;&#31181;&#31867;&#22411;&#65306;&#22402;&#30452;&#31283;&#23450;&#24615;&#21644;&#27700;&#24179;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#30340;&#31616;&#21333;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#32780;&#31283;&#23450;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36890;&#24120;&#19981;&#26159;&#22312;&#30495;&#31354;&#20013;&#20135;&#29983;&#30340;&#65292;&#32780;&#26159;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#65292;&#39044;&#27979;&#26159;&#23450;&#26399;&#29983;&#25104;&#30340;&#65292;&#24182;&#19988;&#24444;&#27492;&#20043;&#38388;&#20114;&#30456;&#24433;&#21709;&#12290;&#23545;&#20110;&#20915;&#31574;&#26469;&#35828;&#65292;&#39044;&#27979;&#19981;&#20250;&#20219;&#24847;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#31283;&#23450;&#30340;&#21487;&#33021;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#25991;&#29486;&#20013;&#65292;&#36825;&#20010;&#39046;&#22495;&#21482;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#22402;&#30452;&#31283;&#23450;&#24615;&#21644;&#27700;&#24179;&#31283;&#23450;&#24615;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#31867;&#22411;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#24037;&#20316;&#21482;&#36866;&#29992;&#20110;&#26576;&#20123;&#22522;&#30784;&#27169;&#22411;&#65292;&#23558;&#36825;&#20123;&#26694;&#26550;&#25193;&#23637;&#25104;&#19982;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#20860;&#23481;&#24182;&#19981;&#23481;&#26131;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26694;&#26550;&#21482;&#33021;&#20351;&#39044;&#27979;&#22402;&#30452;&#31283;&#23450;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22402;&#30452;&#21644;&#27700;&#24179;&#31283;&#23450;&#21270;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#32780;&#31283;&#23450;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasts are typically not produced in a vacuum but in a business context, where forecasts are generated on a regular basis and interact with each other. For decisions, it may be important that forecasts do not change arbitrarily, and are stable in some sense. However, this area has received only limited attention in the forecasting literature. In this paper, we explore two types of forecast stability that we call vertical stability and horizontal stability. The existing works in the literature are only applicable to certain base models and extending these frameworks to be compatible with any base model is not straightforward. Furthermore, these frameworks can only stabilise the forecasts vertically. To fill this gap, we propose a simple linear-interpolation-based approach that is applicable to stabilise the forecasts provided by any base model vertically and horizontally. The approach can produce both accurate and stable forecasts. Using N-BEATS, Pooled Regression and LightGBM as the
&lt;/p&gt;</description></item><item><title>CQM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#23450;&#20041;&#35821;&#20041;&#30446;&#26631;&#31354;&#38388;&#21644;&#25552;&#20986;&#35838;&#31243;&#30446;&#26631;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VQ-VAE)&#23558;&#36830;&#32493;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#36890;&#36807;&#22270;&#24418;&#24674;&#22797;&#31163;&#25955;&#35266;&#23519;&#32467;&#26524;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#26102;&#38388;&#36317;&#31163;&#24863;&#30693;&#30340;&#35838;&#31243;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.17330</link><description>&lt;p&gt;
CQM&#65306;&#20855;&#26377;&#37327;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#35838;&#31243;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CQM: Curriculum Reinforcement Learning with a Quantized World Model. (arXiv:2310.17330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17330
&lt;/p&gt;
&lt;p&gt;
CQM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#23450;&#20041;&#35821;&#20041;&#30446;&#26631;&#31354;&#38388;&#21644;&#25552;&#20986;&#35838;&#31243;&#30446;&#26631;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VQ-VAE)&#23558;&#36830;&#32493;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#36890;&#36807;&#22270;&#24418;&#24674;&#22797;&#31163;&#25955;&#35266;&#23519;&#32467;&#26524;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#26102;&#38388;&#36317;&#31163;&#24863;&#30693;&#30340;&#35838;&#31243;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35838;&#31243;&#22686;&#24378;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#20195;&#29702;&#20219;&#21153;&#30340;&#39034;&#24207;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#35838;&#31243;&#30446;&#26631;&#26102;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#25351;&#23450;&#30340;&#30446;&#26631;&#31354;&#38388;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38480;&#21046;&#24182;&#25552;&#39640;&#35838;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#26041;&#27861;&#65292;&#23427;&#33258;&#21160;&#23450;&#20041;&#21253;&#21547;&#35838;&#31243;&#36807;&#31243;&#30340;&#20851;&#38190;&#20449;&#24687;&#30340;&#35821;&#20041;&#30446;&#26631;&#31354;&#38388;&#65292;&#24182;&#22312;&#20854;&#19978;&#25552;&#20986;&#35838;&#31243;&#30446;&#26631;&#12290;&#20026;&#20102;&#23450;&#20041;&#35821;&#20041;&#30446;&#26631;&#31354;&#38388;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VQ-VAE)&#23558;&#36830;&#32493;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#36890;&#36807;&#22270;&#24418;&#24674;&#22797;&#31163;&#25955;&#35266;&#23519;&#32467;&#26524;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#26102;&#38388;&#36317;&#31163;&#24863;&#30693;&#30340;&#35838;&#31243;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#22312;&#33258;&#21160;&#32452;&#21512;&#30340;&#30446;&#26631;&#31354;&#38388;&#20013;&#25910;&#25947;&#21040;&#26368;&#32456;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recent curriculum Reinforcement Learning (RL) has shown notable progress in solving complex tasks by proposing sequences of surrogate tasks. However, the previous approaches often face challenges when they generate curriculum goals in a high-dimensional space. Thus, they usually rely on manually specified goal spaces. To alleviate this limitation and improve the scalability of the curriculum, we propose a novel curriculum method that automatically defines the semantic goal space which contains vital information for the curriculum process, and suggests curriculum goals over it. To define the semantic goal space, our method discretizes continuous observations via vector quantized-variational autoencoders (VQ-VAE) and restores the temporal relations between the discretized observations by a graph. Concurrently, ours suggests uncertainty and temporal distance-aware curriculum goals that converges to the final goals over the automatically composed goal space. We demonstrate that the propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;C-Disentanglement&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#19988;&#21463;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#30340;&#29983;&#25104;&#22240;&#23376;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17325</link><description>&lt;p&gt;
C-Disentanglement: &#24102;&#26377;&#28151;&#28102;&#22240;&#23376;&#24402;&#32435;&#20559;&#24046;&#30340;&#22240;&#26524;&#29420;&#31435;&#29983;&#25104;&#22240;&#23376;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder. (arXiv:2310.17325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;C-Disentanglement&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#19988;&#21463;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#30340;&#29983;&#25104;&#22240;&#23376;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#20551;&#35774;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#30001;&#20960;&#20010;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#29983;&#25104;&#22240;&#23376;&#65288;&#21363;&#21464;&#24322;&#28304;&#65289;&#20135;&#29983;&#30340;&#65292;&#24182;&#26088;&#22312;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#21457;&#29616;&#23427;&#20204;&#12290;&#36825;&#20123;&#22240;&#23376;&#34987;&#26399;&#26395;&#26159;&#22240;&#26524;&#19978;&#35299;&#32544;&#30340;&#65292;&#24847;&#21619;&#30528;&#19981;&#21516;&#30340;&#22240;&#23376;&#34987;&#32534;&#30721;&#20026;&#21333;&#29420;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#24182;&#19988;&#19968;&#20010;&#22240;&#23376;&#30340;&#21464;&#21270;&#19981;&#20250;&#24433;&#21709;&#20854;&#20182;&#22240;&#23376;&#30340;&#20540;&#12290;&#19982;&#32479;&#35745;&#29420;&#31435;&#24615;&#30456;&#27604;&#65292;&#22240;&#26524;&#35299;&#32544;&#20801;&#35768;&#26356;&#21487;&#25511;&#30340;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#21457;&#29616;&#36807;&#31243;&#20013;&#20551;&#35774;&#27809;&#26377;&#28151;&#28102;&#22240;&#32032;&#65292;&#21363;&#29983;&#25104;&#22240;&#23376;&#27809;&#26377;&#20849;&#21516;&#30340;&#21407;&#22240;&#65292;&#22240;&#27492;&#21482;&#33719;&#24471;&#32479;&#35745;&#29420;&#31435;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#22312;&#21457;&#29616;&#22240;&#26524;&#29983;&#25104;&#22240;&#23376;&#20013;&#24314;&#31435;&#28151;&#28102;&#22240;&#32032;&#30340;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36825;&#20123;&#22240;&#32032;&#26159;&#26080;&#27861;&#35782;&#21035;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#28151;&#28102;-&#35299;&#32544;&#65288;C-Di&#65289;&#30340;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning assumes that real-world data is generated by a few semantically meaningful generative factors (i.e., sources of variation) and aims to discover them in the latent space. These factors are expected to be causally disentangled, meaning that distinct factors are encoded into separate latent variables, and changes in one factor will not affect the values of the others. Compared to statistical independence, causal disentanglement allows more controllable data generation, improved robustness, and better generalization. However, most existing work assumes unconfoundedness in the discovery process, that there are no common causes to the generative factors and thus obtain only statistical independence. In this paper, we recognize the importance of modeling confounders in discovering causal generative factors. Unfortunately, such factors are not identifiable without proper inductive bias. We fill the gap by introducing a framework entitled Confounded-Disentanglement (C-Di
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28436;&#31034;-&#27491;&#21017;&#21270;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35813;&#22797;&#26434;&#24230;&#19982;&#19987;&#23478;&#28436;&#31034;&#25968;&#37327;&#25104;&#21453;&#27604;&#12290;</title><link>http://arxiv.org/abs/2310.17303</link><description>&lt;p&gt;
&#36890;&#36807;&#28436;&#31034;-&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#37319;&#26679;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Demonstration-Regularized RL. (arXiv:2310.17303v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17303
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28436;&#31034;-&#27491;&#21017;&#21270;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35813;&#22797;&#26434;&#24230;&#19982;&#19987;&#23478;&#28436;&#31034;&#25968;&#37327;&#25104;&#21453;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#19987;&#23478;&#28436;&#31034;&#32435;&#20837;&#20854;&#20013;&#65292;&#21487;&#20197;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;(SRL)&#30340;&#37319;&#26679;&#25928;&#29575;&#26041;&#38754;&#20135;&#29983;&#32463;&#39564;&#25928;&#26524;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#37327;&#21270;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#38477;&#20302;&#20102;SRL&#30340;&#37319;&#26679;&#22797;&#26434;&#24615;&#30340;&#31243;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;KL&#27491;&#21017;&#21270;&#21033;&#29992;&#19987;&#23478;&#28436;&#31034;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#28436;&#31034;-&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#29366;&#24577;&#19979;&#65292;&#22312;$\widetilde{\mathcal{O}}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#65292;&#20351;&#29992;$N^{\mathrm{E}}$&#20010;&#19987;&#23478;&#28436;&#31034;&#33021;&#22815;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#22312;&#32447;&#24615;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#22312;$\widetilde{\mathcal{O}}(\mathrm{Poly}(d,H)/(\varepsilon^2 N^{\mathrm{E}}))$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;$\varepsilon$&#26159;&#30446;&#26631;&#31934;&#24230;&#65292;$H$&#26159;&#35268;&#23450;&#65292;$A$&#26159;&#21160;&#20316;&#30340;&#25968;&#37327;&#65292;$S$&#26159;&#26377;&#38480;&#29366;&#24577;&#30340;&#25968;&#37327;&#65292;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;$d$&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning that leverages the expert demonstrations by KL-regularization for a policy learned by behavior cloning. Our findings reveal that using $N^{\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\widetilde{\mathcal{O}}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in finite and $\widetilde{\mathcal{O}}(\mathrm{Poly}(d,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in linear Markov decision processes, where $\varepsilon$ is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight con
&lt;/p&gt;</description></item><item><title>BEVContrast&#26159;&#19968;&#31181;&#22312;&#27773;&#36710;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#20013;&#20351;&#29992;BEV&#31354;&#38388;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.17281</link><description>&lt;p&gt;
BEVContrast: &#27773;&#36710;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;BEV&#31354;&#38388;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds. (arXiv:2310.17281v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17281
&lt;/p&gt;
&lt;p&gt;
BEVContrast&#26159;&#19968;&#31181;&#22312;&#27773;&#36710;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#20013;&#20351;&#29992;BEV&#31354;&#38388;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#27773;&#36710;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;3D&#20027;&#24178;&#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#30456;&#21516;&#22330;&#26223;&#20013;&#25429;&#33719;&#30340;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#29305;&#24449;&#36827;&#34892;&#23545;&#27604;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#23545;&#27604;&#26041;&#27861;&#20174;&#28857;&#32423;&#21035;&#65288;PointContrast&#65289;&#21040;&#20998;&#27573;&#32423;&#21035;&#65288;TARL&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;Bird's Eye View&#24179;&#38754;&#30340;2D&#21333;&#20803;&#32423;&#21035;&#23450;&#20041;&#20102;&#25105;&#20204;&#30340;&#23545;&#27604;&#12290;&#25152;&#24471;&#21040;&#30340;&#21333;&#20803;&#32423;&#34920;&#31034;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#26082;&#20445;&#25345;&#20102;PointContrast&#30340;&#31616;&#21333;&#24615;&#65288;&#21333;&#20803;&#34920;&#31034;&#35745;&#31639;&#25104;&#26412;&#20302;&#65289;&#65292;&#21516;&#26102;&#20063;&#36229;&#36234;&#20102;TARL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a surprisingly simple and efficient method for self-supervision of 3D backbone on automotive Lidar point clouds. We design a contrastive loss between features of Lidar scans captured in the same scene. Several such approaches have been proposed in the literature from PointConstrast, which uses a contrast at the level of points, to the state-of-the-art TARL, which uses a contrast at the level of segments, roughly corresponding to objects. While the former enjoys a great simplicity of implementation, it is surpassed by the latter, which however requires a costly pre-processing. In BEVContrast, we define our contrast at the level of 2D cells in the Bird's Eye View plane. Resulting cell-level representations offer a good trade-off between the point-level representations exploited in PointContrast and segment-level representations exploited in TARL: we retain the simplicity of PointContrast (cell representations are cheap to compute) while surpassing the performance of TARL in do
&lt;/p&gt;</description></item><item><title>&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17273</link><description>&lt;p&gt;
&#23558;&#24490;&#29615;&#24341;&#20837;&#20154;&#31867;&#65306;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17273
&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#35768;&#22810;&#20248;&#21270;&#22120;&#19968;&#26679;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#33719;&#24471;&#29992;&#25143;&#20449;&#20219;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#20854;&#19981;&#36879;&#26126;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23581;&#35797;&#24320;&#21457;&#38754;&#21521;&#20154;&#31867;&#30340;&#20248;&#21270;&#22120;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#29992;&#25143;&#30693;&#35782;&#26159;&#26126;&#30830;&#19988;&#26080;&#35823;&#30340;&#65292;&#24182;&#20027;&#35201;&#23558;&#29992;&#25143;&#20316;&#20026;&#20248;&#21270;&#36807;&#31243;&#30340;&#30417;&#30563;&#32773;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#34913;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65292;&#21363;&#25105;&#20204;&#30340;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CoExBO&#65289;&#26694;&#26550;&#12290;CoExBO&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#26080;&#32541;&#22320;&#23558;&#20154;&#31867;&#35265;&#35299;&#25972;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19982;&#29992;&#25143;&#20351;&#29992;&#20559;&#22909;&#19968;&#33268;&#30340;&#31639;&#27861;&#24314;&#35758;&#12290;CoExBO&#35299;&#37322;&#20854;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20197;&#22521;&#20859;&#20449;&#20219;&#65292;&#20351;&#29992;&#25143;&#26356;&#28165;&#26970;&#22320;&#25484;&#25569;&#20248;&#21270;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;CoExBO&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#65292;&#20801;&#35768;&#29992;&#25143;&#29359;&#38169;&#35823;&#65307;&#21363;&#20351;&#22312;&#26497;&#31471;&#23545;&#25239;&#24615;&#24178;&#25200;&#19979;&#65292;&#31639;&#27861;&#20063;&#20250;&#28176;&#36827;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#26041;&#24046;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#26041;&#24046;&#23545;&#20110;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.17264</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#30340;&#26041;&#24046;&#65306;&#25105;&#20204;&#30495;&#30340;&#22312;&#25913;&#36827;&#32570;&#38519;&#39044;&#27979;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Variance of ML-based software fault predictors: are we really improving fault prediction?. (arXiv:2310.17264v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#26041;&#24046;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#26041;&#24046;&#23545;&#20110;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36719;&#20214;&#31995;&#32479;&#36234;&#26469;&#36234;&#22797;&#26434;&#24182;&#25345;&#32493;&#22686;&#38271;&#65292;&#36719;&#20214;&#36136;&#37327;&#20445;&#35777;&#27963;&#21160;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#23545;&#22823;&#35268;&#27169;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#30340;&#25104;&#26412;&#20063;&#26356;&#39640;&#12290;&#20026;&#20102;&#26377;&#25928;&#20998;&#37197;&#36136;&#37327;&#20445;&#35777;&#36164;&#28304;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#26377;&#32570;&#38519;&#20195;&#30721;&#21306;&#22495;&#30340;&#32570;&#38519;&#39044;&#27979;&#65288;FP&#65289;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21033;&#29992;&#38543;&#26426;&#20803;&#32032;&#26469;&#22686;&#21152;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#38543;&#26426;&#20803;&#32032;&#65292;&#20063;&#31216;&#20026;&#24341;&#20837;&#38750;&#30830;&#23450;&#24615;&#22240;&#32032;&#65288;NI&#65289;&#30340;&#22240;&#32032;&#65292;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#30340;&#26041;&#24046;&#12290;&#36825;&#31181;&#26041;&#24046;&#23545;&#20110;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#23454;&#39564;&#23460;&#20013;&#65292;&#34429;&#28982;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#33391;&#22909;&#65288;&#20363;&#22914;&#65292;&#36890;&#24120;&#28041;&#21450;&#22810;&#27425;&#36816;&#34892;&#21644;&#24179;&#22343;&#32467;&#26524;&#65289;&#65292;&#20294;&#39640;&#26041;&#24046;&#20250;&#38480;&#21046;&#20854;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software quality assurance activities become increasingly difficult as software systems become more and more complex and continuously grow in size. Moreover, testing becomes even more expensive when dealing with large-scale systems. Thus, to effectively allocate quality assurance resources, researchers have proposed fault prediction (FP) which utilizes machine learning (ML) to predict fault-prone code areas. However, ML algorithms typically make use of stochastic elements to increase the prediction models' generalizability and efficiency of the training process. These stochastic elements, also known as nondeterminism-introducing (NI) factors, lead to variance in the training process and as a result, lead to variance in prediction accuracy and training time. This variance poses a challenge for reproducibility in research. More importantly, while fault prediction models may have shown good performance in the lab (e.g., often-times involving multiple runs and averaging outcomes), high var
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;fairret&#30340;&#21487;&#24494;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#30446;&#26631;&#37327;&#21270;&#20559;&#35265;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#33258;&#21160;&#24494;&#20998;&#27969;&#31243;&#20013;&#12290;&#36890;&#36807;&#20174;&#32447;&#24615;&#20998;&#24335;&#32479;&#35745;&#35282;&#24230;&#23450;&#20041;&#20844;&#24179;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#22810;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;fairret&#26694;&#26550;&#19982;&#22522;&#20934;&#30456;&#27604;&#22312;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#26102;&#20960;&#20046;&#19981;&#25439;&#22833;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17256</link><description>&lt;p&gt;
fairret&#65306;&#19968;&#31181;&#21487;&#24494;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
fairret: a Framework for Differentiable Fairness Regularization Terms. (arXiv:2310.17256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;fairret&#30340;&#21487;&#24494;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#30446;&#26631;&#37327;&#21270;&#20559;&#35265;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#33258;&#21160;&#24494;&#20998;&#27969;&#31243;&#20013;&#12290;&#36890;&#36807;&#20174;&#32447;&#24615;&#20998;&#24335;&#32479;&#35745;&#35282;&#24230;&#23450;&#20041;&#20844;&#24179;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#22810;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;fairret&#26694;&#26550;&#19982;&#22522;&#20934;&#30456;&#27604;&#22312;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#26102;&#20960;&#20046;&#19981;&#25439;&#22833;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#24037;&#20855;&#20165;&#25509;&#21463;&#26377;&#38480;&#33539;&#22260;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#19988;&#19982;&#33258;&#21160;&#24494;&#20998;&#24211;&#30340;&#25972;&#21512;&#36739;&#23569;&#65292;&#23613;&#31649;&#36825;&#20123;&#24211;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#65288;fairret&#65289;&#30340;&#26694;&#26550;&#65292;&#20197;&#27169;&#22359;&#21270;&#30446;&#26631;&#30340;&#24418;&#24335;&#37327;&#21270;&#20559;&#35265;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#33258;&#21160;&#24494;&#20998;&#27969;&#31243;&#20013;&#12290;&#36890;&#36807;&#37319;&#29992;&#32447;&#24615;&#20998;&#24335;&#32479;&#35745;&#30340;&#24191;&#20041;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#19968;&#31867;&#24191;&#27867;&#30340;fairret&#12290;&#23454;&#39564;&#26174;&#31034;&#20102;&#23427;&#20204;&#30340;&#26799;&#24230;&#34892;&#20026;&#20197;&#21450;&#19982;&#22522;&#20934;&#30456;&#27604;&#23558;&#20844;&#24179;&#24615;&#24378;&#21046;&#25191;&#34892;&#30340;&#23454;&#29992;&#24615;&#32780;&#26368;&#23567;&#21270;&#39044;&#27979;&#33021;&#21147;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;fairret&#26694;&#26550;&#30340;PyTorch&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current tools for machine learning fairness only admit a limited range of fairness definitions and have seen little integration with automatic differentiation libraries, despite the central role these libraries play in modern machine learning pipelines.  We introduce a framework of fairness regularization terms (fairrets) which quantify bias as modular objectives that are easily integrated in automatic differentiation pipelines. By employing a general definition of fairness in terms of linear-fractional statistics, a wide class of fairrets can be computed efficiently. Experiments show the behavior of their gradients and their utility in enforcing fairness with minimal loss of predictive power compared to baselines. Our contribution includes a PyTorch implementation of the fairret framework.
&lt;/p&gt;</description></item><item><title>IDENAS&#26159;&#19968;&#31181;&#38598;&#25104;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#37096;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17250</link><description>&lt;p&gt;
IDENAS: &#20869;&#37096;&#20381;&#36182;&#24615;&#25506;&#32034;&#29992;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
IDENAS: Internal Dependency Exploration for Neural Architecture Search. (arXiv:2310.17250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17250
&lt;/p&gt;
&lt;p&gt;
IDENAS&#26159;&#19968;&#31181;&#38598;&#25104;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#37096;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#20174;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#21644;&#36827;&#34892;&#21508;&#31181;&#39044;&#27979;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20256;&#32479;&#31639;&#27861;&#20381;&#36182;&#20110;&#26126;&#30830;&#23450;&#20041;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#65292;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#21306;&#21035;&#20197;&#21450;&#27169;&#22411;&#30340;&#24213;&#23618;&#20851;&#32852;&#65288;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#23618;&#26159;&#26410;&#30693;&#30340;&#12290;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21644;&#29305;&#24449;&#36873;&#25321;&#24050;&#25104;&#20026;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;IDENAS&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#20381;&#36182;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#23558;NAS&#19982;&#29305;&#24449;&#36873;&#25321;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#28041;&#21450;1D&#20256;&#24863;&#22120;&#21644;2D&#22270;&#20687;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#25506;&#32034;&#20102;&#23436;&#25972;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#20869;&#37096;&#20381;&#36182;&#24615;&#12290;IDENAS&#37319;&#29992;&#20102;&#20462;&#25913;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#39034;&#24207;&#21069;&#21521;&#25628;&#32034;&#65288;SFS&#65289;&#31639;&#27861;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#37197;&#32622;&#25628;&#32034;&#19982;&#23884;&#20837;&#24335;&#29305;&#24449;&#36873;&#25321;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;IDENAS&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is a powerful tool for extracting valuable information and making various predictions from diverse datasets. Traditional algorithms rely on well-defined input and output variables however, there are scenarios where the distinction between the input and output variables and the underlying, associated (input and output) layers of the model, are unknown. Neural Architecture Search (NAS) and Feature Selection have emerged as promising solutions in such scenarios. This research proposes IDENAS, an Internal Dependency-based Exploration for Neural Architecture Search, integrating NAS with feature selection. The methodology explores internal dependencies in the complete parameter space for classification involving 1D sensor and 2D image data as well. IDENAS employs a modified encoder-decoder model and the Sequential Forward Search (SFS) algorithm, combining input-output configuration search with embedded feature selection. Experimental results demonstrate IDENASs superior perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;grokking&#29616;&#35937;&#19981;&#20165;&#23616;&#38480;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#20986;&#29616;&#22312;&#20854;&#20182;&#31639;&#27861;&#21644;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#28155;&#21152;&#34394;&#20551;&#20449;&#24687;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#35825;&#21457;grokking&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;grokking&#29616;&#35937;&#22312;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#21463;&#22797;&#26434;&#24615;&#21644;&#38169;&#35823;&#25351;&#23548;&#30340;&#20219;&#20309;&#24773;&#20917;&#19979;&#21487;&#33021;&#21457;&#29983;&#12290;&#36825;&#23545;&#29702;&#35299;grokking&#29616;&#35937;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.17247</link><description>&lt;p&gt;
&#36229;&#36234;&#31070;&#32463;&#32593;&#32476;&#65306;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#32463;&#39564;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity. (arXiv:2310.17247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;grokking&#29616;&#35937;&#19981;&#20165;&#23616;&#38480;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#20986;&#29616;&#22312;&#20854;&#20182;&#31639;&#27861;&#21644;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#28155;&#21152;&#34394;&#20551;&#20449;&#24687;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#35825;&#21457;grokking&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;grokking&#29616;&#35937;&#22312;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#21463;&#22797;&#26434;&#24615;&#21644;&#38169;&#35823;&#25351;&#23548;&#30340;&#20219;&#20309;&#24773;&#20917;&#19979;&#21487;&#33021;&#21457;&#29983;&#12290;&#36825;&#23545;&#29702;&#35299;grokking&#29616;&#35937;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#23637;&#29616;&#20986;&#19968;&#31181;&#31216;&#20026;&#8220;grokking&#8221;&#30340;&#29616;&#35937;&#65292;&#21363;&#23427;&#20204;&#22312;&#39564;&#35777;&#38598;&#19978;&#23454;&#29616;&#23436;&#32654;&#25110;&#25509;&#36817;&#23436;&#32654;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;&#35757;&#32451;&#38598;&#19978;&#21017;&#26089;&#24050;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;grokking&#19981;&#20165;&#38480;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#20986;&#29616;&#22312;&#20854;&#20182;&#35774;&#32622;&#20013;&#65292;&#20363;&#22914;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20998;&#31867;&#12289;GP&#22238;&#24402;&#21644;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#36890;&#36807;&#28155;&#21152;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#30340;&#32500;&#24230;&#26469;&#35825;&#21457;&#22522;&#20110;&#31639;&#27861;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;grokking&#29616;&#35937;&#30340;&#26426;&#21046;&#12290;&#38750;&#31070;&#32463;&#32467;&#26500;&#20013;&#30340;&#36825;&#31181;&#29616;&#35937;&#30340;&#23384;&#22312;&#35777;&#26126;&#20102;grokking&#19981;&#23616;&#38480;&#20110;SGD&#25110;&#26435;&#37325;&#33539;&#25968;&#27491;&#21017;&#21270;&#12290;&#30456;&#21453;&#65292;grokking&#21487;&#33021;&#21457;&#29983;&#22312;&#20219;&#20309;&#30001;&#22797;&#26434;&#24615;&#21644;&#38169;&#35823;&#25351;&#23548;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#30340;&#24773;&#20917;&#20013;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21644;&#25105;&#20204;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#21644;GP&#22238;&#24402;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#20013;&#35266;&#23519;&#21040;&#30340;&#36827;&#19968;&#27493;&#36235;&#21183;&#65292;&#25105;&#20204;&#22312;grokking&#30340;&#26356;&#19968;&#33324;&#30340;&#29702;&#35770;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some settings neural networks exhibit a phenomenon known as grokking, where they achieve perfect or near-perfect accuracy on the validation set long after the same performance has been achieved on the training set. In this paper, we discover that grokking is not limited to neural networks but occurs in other settings such as Gaussian process (GP) classification, GP regression and linear regression. We also uncover a mechanism by which to induce grokking on algorithmic datasets via the addition of dimensions containing spurious information. The presence of the phenomenon in non-neural architectures provides evidence that grokking is not specific to SGD or weight norm regularisation. Instead, grokking may be possible in any setting where solution search is guided by complexity and error. Based on this insight and further trends we see in the training trajectories of a Bayesian neural network (BNN) and GP regression model, we make progress towards a more general theory of grokking. Spe
&lt;/p&gt;</description></item><item><title>CROP&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#22870;&#21169;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#65292;&#36890;&#36807;&#21516;&#26102;&#26368;&#23567;&#21270;&#20272;&#35745;&#35823;&#24046;&#21644;&#38543;&#26426;&#21160;&#20316;&#22870;&#21169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#22870;&#21169;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.17245</link><description>&lt;p&gt;
CROP: &#20445;&#23432;&#22870;&#21169;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
CROP: Conservative Reward for Model-based Offline Policy Optimization. (arXiv:2310.17245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17245
&lt;/p&gt;
&lt;p&gt;
CROP&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#22870;&#21169;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#65292;&#36890;&#36807;&#21516;&#26102;&#26368;&#23567;&#21270;&#20272;&#35745;&#35823;&#24046;&#21644;&#38543;&#26426;&#21160;&#20316;&#22870;&#21169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#22870;&#21169;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20351;&#29992;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25361;&#25112;&#26041;&#38754;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#26469;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#20013;&#23558;&#20445;&#23432;&#24615;&#24341;&#20837;&#27169;&#22411;&#25110;Q&#20989;&#25968;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22870;&#21169;&#20272;&#35745;&#20013;&#20445;&#23432;&#24615;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;CROP&#65292;&#35813;&#31639;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20445;&#23432;&#22320;&#20272;&#35745;&#22870;&#21169;&#12290;&#20026;&#20102;&#23454;&#29616;&#20445;&#23432;&#30340;&#22870;&#21169;&#20272;&#35745;&#65292;CROP&#21516;&#26102;&#26368;&#23567;&#21270;&#20272;&#35745;&#35823;&#24046;&#21644;&#38543;&#26426;&#21160;&#20316;&#30340;&#22870;&#21169;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#20445;&#23432;&#30340;&#22870;&#21169;&#26426;&#21046;&#23548;&#33268;...&#65288;&#25991;&#31456;&#25688;&#35201;&#26410;&#23436;&#65292;&#19979;&#21516;&#65289;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to optimize policy using collected data without online interactions. Model-based approaches are particularly appealing for addressing offline RL challenges due to their capability to mitigate the limitations of offline data through data generation using models. Prior research has demonstrated that introducing conservatism into the model or Q-function during policy optimization can effectively alleviate the prevalent distribution drift problem in offline RL. However, the investigation into the impacts of conservatism in reward estimation is still lacking. This paper proposes a novel model-based offline RL algorithm, Conservative Reward for model-based Offline Policy optimization (CROP), which conservatively estimates the reward in model training. To achieve a conservative reward estimation, CROP simultaneously minimizes the estimation error and the reward of random actions. Theoretical analysis shows that this conservative reward mechanism leads 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;span&#21098;&#26525;&#26426;&#21046;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#36827;&#34892;&#39640;&#38454;&#24314;&#27169;&#65292;&#23454;&#29616;&#22810;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2310.17238</link><description>&lt;p&gt;
&#36890;&#36807;span&#21098;&#26525;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;span&#21098;&#26525;&#26426;&#21046;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#36827;&#34892;&#39640;&#38454;&#24314;&#27169;&#65292;&#23454;&#29616;&#22810;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#65288;ERE&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#26368;&#36817;&#22522;&#20110;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;ERE&#27169;&#22411;&#22312;&#22810;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#19981;&#32771;&#34385;&#39640;&#38454;&#20132;&#20114;&#65292;&#32780;&#39640;&#38454;&#24314;&#27169;&#21487;&#33021;&#20250;&#26377;&#30410;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#29992;&#20110;ERE&#65292;&#23427;&#26159;&#24314;&#31435;&#22312;PL-marker&#65288;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#27169;&#22411;&#65289;&#20043;&#19978;&#30340;&#12290;&#20026;&#20102;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#21484;&#22238;&#21098;&#26525;&#22120;&#26426;&#21046;&#23558;&#23454;&#20307;&#30340;&#35782;&#21035;&#21644;&#26631;&#27880;&#36127;&#25285;&#20174;NER&#27169;&#22359;&#36716;&#31227;&#21040;&#25105;&#20204;&#27169;&#22411;&#30340;&#32852;&#21512;&#27169;&#22359;&#12290;&#23545;&#20110;&#39640;&#38454;&#24314;&#27169;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#26159;&#23454;&#20307;&#65288;&#30001;span&#21098;&#26525;&#22120;&#25552;&#20379;&#65289;&#65292;&#20197;&#21450;&#20854;&#20851;&#31995;&#65292;&#24182;&#19988;&#36229;&#36793;&#32534;&#30721;&#20102;&#20004;&#20010;&#19981;&#21516;&#20851;&#31995;&#20043;&#38388;&#25110;&#20851;&#31995;&#19982;&#20854;&#30456;&#20851;&#30340;&#20027;&#20307;&#21644;&#23486;&#35821;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25509;&#30528;&#25105;&#20204;&#36816;&#34892;...
&lt;/p&gt;
&lt;p&gt;
Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose HyperGraph neural network for ERE ($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based pipleline model). To alleviate error propagation,we use a high-recall pruner mechanism to transfer the burden of entity identification and labeling from the NER module to the joint module of our model. For higher-order modeling, we build a hypergraph, where nodes are entities (provided by the span pruner) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities. We then run
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;codebook&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#29305;&#24449;&#37327;&#21270;&#20026;&#31163;&#25955;&#21521;&#37327;&#30721;&#30340;&#24635;&#21644;&#26469;&#23454;&#29616;&#31232;&#30095;&#21644;&#31163;&#25955;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#26497;&#31471;&#29942;&#39048;&#26465;&#20214;&#19979;&#36816;&#34892;&#26102;&#24615;&#33021;&#19979;&#38477;&#36866;&#24230;&#65292;&#21516;&#26102;&#36825;&#31181;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#25511;&#21046;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.17230</link><description>&lt;p&gt;
Codebook&#29305;&#24449;&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#21644;&#31163;&#25955;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Codebook Features: Sparse and Discrete Interpretability for Neural Networks. (arXiv:2310.17230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;codebook&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#29305;&#24449;&#37327;&#21270;&#20026;&#31163;&#25955;&#21521;&#37327;&#30721;&#30340;&#24635;&#21644;&#26469;&#23454;&#29616;&#31232;&#30095;&#21644;&#31163;&#25955;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#26497;&#31471;&#29942;&#39048;&#26465;&#20214;&#19979;&#36816;&#34892;&#26102;&#24615;&#33021;&#19979;&#38477;&#36866;&#24230;&#65292;&#21516;&#26102;&#36825;&#31181;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#25511;&#21046;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#38544;&#34255;&#29366;&#24577;&#26159;&#23494;&#38598;&#21644;&#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#23558;&#36830;&#32493;&#29305;&#24449;&#37327;&#21270;&#20026;&#25105;&#20204;&#31216;&#20043;&#20026;codebook&#29305;&#24449;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20855;&#26377;&#31232;&#30095;&#12289;&#31163;&#25955;&#19988;&#26356;&#26131;&#35299;&#37322;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#36890;&#36807;&#22312;&#27599;&#23618;&#24341;&#20837;&#21521;&#37327;&#37327;&#21270;&#29942;&#39048;&#26469;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#20135;&#29983;&#30340;codebook&#29305;&#24449;&#30001;&#20174;&#26356;&#22823;&#30340;codebook&#20013;&#36873;&#25321;&#30340;&#23569;&#37327;&#31163;&#25955;&#21521;&#37327;&#30721;&#30340;&#24635;&#21644;&#32452;&#25104;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#36825;&#31181;&#26497;&#31471;&#29942;&#39048;&#19979;&#36816;&#34892;&#65292;&#24615;&#33021;&#21482;&#26377;&#36866;&#24230;&#30340;&#19979;&#38477;&#12290;&#36825;&#31181;&#31232;&#30095;&#12289;&#31163;&#25955;&#30340;&#29942;&#39048;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#25214;&#21040;&#22312;&#25152;&#38656;&#34892;&#20026;&#20986;&#29616;&#26102;&#28608;&#27963;&#30340;&#30721;&#65292;&#28982;&#21518;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#28608;&#27963;&#30456;&#21516;&#30340;&#30721;&#20197;&#24341;&#21457;&#35813;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;codebook Transformers&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during generation to elicit that behavior. We validate our approach by training codebook Transformers on sever
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20989;&#25968;&#30340;&#35757;&#32451;&#30446;&#26631;&#31867;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38381;&#21512;&#22411;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#20351;&#24471;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.17217</link><description>&lt;p&gt;
&#36229;&#36234;MLE: &#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#20984;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20989;&#25968;&#30340;&#35757;&#32451;&#30446;&#26631;&#31867;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38381;&#21512;&#22411;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#20351;&#24471;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26159;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26368;&#33021;&#35299;&#37322;&#35266;&#27979;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#21442;&#25968;&#12290;&#22312;&#25991;&#26412;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;MLE&#32463;&#24120;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#26032;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;MLE&#24182;&#19981;&#24635;&#26159;&#24517;&#35201;&#19988;&#26368;&#20248;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38381;&#21512;&#22411;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#26426;&#22120;&#32763;&#35793;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#26368;&#21512;&#36866;&#30340;&#21709;&#24212;&#65292;&#36825;&#24182;&#19981;&#19968;&#23450;&#38656;&#35201;&#20351;&#29992;MLE&#26469;&#20272;&#35745;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20989;&#25968;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#31867;&#65292;&#20351;&#24471;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#38598;&#20013;&#20110;&#39640;&#27010;&#29575;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23558;&#20984;&#20989;&#25968;&#24212;&#29992;&#20110;&#25439;&#22833;&#20989;&#25968;&#26102;&#30340;&#26368;&#20248;&#39044;&#27979;&#20998;&#24067;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#20984;&#20989;&#25968;&#21487;&#20197;&#20351;&#24471;&#26368;&#20248;&#39044;&#27979;&#20998;&#24067;&#21464;&#24471;&#26356;&#21152;&#38160;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#22312;&#27599;&#24103;&#30340;&#38454;&#27573;&#39044;&#27979;&#20013;&#32467;&#21512;&#20102;&#22270;&#20998;&#21106;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#31232;&#30095;&#26102;&#38388;&#25139;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#24369;&#30417;&#30563;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17209</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Surgical Phase Recognition. (arXiv:2310.17209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#22312;&#27599;&#24103;&#30340;&#38454;&#27573;&#39044;&#27979;&#20013;&#32467;&#21512;&#20102;&#22270;&#20998;&#21106;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#31232;&#30095;&#26102;&#38388;&#25139;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#24369;&#30417;&#30563;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#36741;&#21161;&#25163;&#26415;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#25163;&#26415;&#35270;&#39057;&#30340;&#38454;&#27573;&#35782;&#21035;&#12290;&#29616;&#26377;&#30340;&#38454;&#27573;&#35782;&#21035;&#31639;&#27861;&#38656;&#35201;&#23545;&#22823;&#37327;&#35270;&#39057;&#36827;&#34892;&#36880;&#24103;&#27880;&#37322;&#65292;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20998;&#21106;&#30340;&#27010;&#24565;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#28216;&#36208;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#27599;&#24103;&#30340;&#38454;&#27573;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24418;&#24335;&#30340;&#24369;&#30417;&#30563;&#65306;&#31232;&#30095;&#26102;&#38388;&#25139;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#36816;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20844;&#24320;&#30340;Cholec80&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#23637;&#29616;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key element of computer-assisted surgery systems is phase recognition of surgical videos. Existing phase recognition algorithms require frame-wise annotation of a large number of videos, which is time and money consuming. In this work we join concepts of graph segmentation with self-supervised learning to derive a random-walk solution for per-frame phase prediction. Furthermore, we utilize within our method two forms of weak supervision: sparse timestamps or few-shot learning. The proposed algorithm enjoys low complexity and can operate in lowdata regimes. We validate our method by running experiments with the public Cholec80 dataset of laparoscopic cholecystectomy videos, demonstrating promising performance in multiple setups.
&lt;/p&gt;</description></item><item><title>MidiTok&#26159;&#19968;&#20010;&#29992;&#20110;&#23558;&#31526;&#21495;&#38899;&#20048;&#36827;&#34892;&#20998;&#35789;&#30340;Python&#21253;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#25193;&#23637;&#24615;&#65292;&#25903;&#25345;&#26368;&#27969;&#34892;&#30340;&#38899;&#20048;&#20998;&#35789;&#26041;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;API&#12290;</title><link>http://arxiv.org/abs/2310.17202</link><description>&lt;p&gt;
miditok&#65306;&#19968;&#20010;&#29992;&#20110;MIDI&#25991;&#20214;&#20998;&#35789;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
miditok: A Python package for MIDI file tokenization. (arXiv:2310.17202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17202
&lt;/p&gt;
&lt;p&gt;
MidiTok&#26159;&#19968;&#20010;&#29992;&#20110;&#23558;&#31526;&#21495;&#38899;&#20048;&#36827;&#34892;&#20998;&#35789;&#30340;Python&#21253;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#25193;&#23637;&#24615;&#65292;&#25903;&#25345;&#26368;&#27969;&#34892;&#30340;&#38899;&#20048;&#20998;&#35789;&#26041;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;API&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#24050;&#32463;&#34987;&#36866;&#24212;&#21040;&#20102;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#12290;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;Transformer&#65292;&#24050;&#32463;&#34987;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#38899;&#20048;&#29983;&#25104;&#12289;&#24314;&#27169;&#25110;&#36716;&#24405;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#24320;&#22987;&#34987;&#24212;&#29992;&#20110;&#23454;&#38469;&#20135;&#21697;&#20013;&#12290;&#20026;&#20102;&#23545;&#20027;&#24178;&#27169;&#22411;&#36827;&#34892;&#38899;&#20048;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#23427;&#20204;&#38656;&#35201;&#20381;&#38752;&#20998;&#35789;&#22120;&#65292;&#20998;&#35789;&#22120;&#30340;&#20316;&#29992;&#26159;&#23558;&#38899;&#20048;&#24207;&#21015;&#21270;&#20026;&#19968;&#31995;&#21015;&#19981;&#21516;&#20803;&#32032;&#65288;&#31216;&#20026;&#26631;&#35760;&#65289;&#30340;&#24207;&#21015;&#12290;MidiTok&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#21487;&#20197;&#20197;&#26497;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#25193;&#23637;&#24615;&#23545;&#31526;&#21495;&#38899;&#20048;&#36827;&#34892;&#20998;&#35789;&#12290;&#23427;&#21253;&#21547;&#20102;&#26368;&#27969;&#34892;&#30340;&#38899;&#20048;&#20998;&#35789;&#26041;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;API&#12290;&#23427;&#26088;&#22312;&#20026;&#27599;&#20010;&#20154;&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#21644;&#25193;&#23637;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in natural language processing has been adapted to the symbolic music modality. Language models, such as Transformers, have been used with symbolic music for a variety of tasks among which music generation, modeling or transcription, with state-of-the-art performances. These models are beginning to be used in production products. To encode and decode music for the backbone model, they need to rely on tokenizers, whose role is to serialize music into sequences of distinct elements called tokens. MidiTok is an open-source library allowing to tokenize symbolic music with great flexibility and extended features. It features the most popular music tokenizations, under a unified API. It is made to be easily used and extensible for everyone.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNCV&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#26799;&#24230;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#32423;&#21035;&#23454;&#29616;REINFORCE Leave-One-Out (RLOO)&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#21333;&#20803;&#65292;&#20248;&#21270;&#20102;&#26412;&#22320;&#26799;&#24230;&#26356;&#26032;&#24182;&#25552;&#20379;&#20102;&#26080;&#20559;&#21644;&#20302;&#26041;&#24046;&#30340;&#32858;&#21512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#20840;&#23616;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2310.17200</link><description>&lt;p&gt;
&#29992;&#32593;&#32476;&#25511;&#21046;&#21464;&#37327;&#39535;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Taming Gradient Variance in Federated Learning with Networked Control Variates. (arXiv:2310.17200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNCV&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#26799;&#24230;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#32423;&#21035;&#23454;&#29616;REINFORCE Leave-One-Out (RLOO)&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#21333;&#20803;&#65292;&#20248;&#21270;&#20102;&#26412;&#22320;&#26799;&#24230;&#26356;&#26032;&#24182;&#25552;&#20379;&#20102;&#26080;&#20559;&#21644;&#20302;&#26041;&#24046;&#30340;&#32858;&#21512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#20840;&#23616;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38754;&#20020;&#30528;&#35832;&#22914;&#27807;&#36890;&#24320;&#38144;&#22823;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#25913;&#36827;&#19981;&#31283;&#23450;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#28304;&#20110;&#30001;&#24322;&#26500;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#24341;&#36215;&#30340;&#26799;&#24230;&#26041;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25511;&#21046;&#21464;&#37327;&#65288;FedNCV&#65289;&#26694;&#26550;&#26469;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;FedNCV&#26694;&#26550;&#30340;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#32423;&#21035;&#37117;&#37319;&#29992;REINFORCE Leave-One-Out&#65288;RLOO&#65289;&#20316;&#20026;&#22522;&#26412;&#25511;&#21046;&#21464;&#37327;&#21333;&#20803;&#12290;&#22312;&#23458;&#25143;&#31471;&#32423;&#21035;&#65292;RLOO&#25511;&#21046;&#21464;&#37327;&#29992;&#20110;&#20248;&#21270;&#26412;&#22320;&#26799;&#24230;&#26356;&#26032;&#65292;&#20943;&#36731;&#25968;&#25454;&#26679;&#26412;&#24341;&#20837;&#30340;&#26041;&#24046;&#12290;&#19968;&#26086;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#65292;&#22522;&#20110;RLOO&#30340;&#20272;&#35745;&#37327;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#26080;&#20559;&#21644;&#20302;&#26041;&#24046;&#30340;&#32858;&#21512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#20840;&#23616;&#26356;&#26032;&#12290;&#36825;&#31181;&#21452;&#20391;&#24212;&#29992;&#34987;&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#25511;&#21046;&#21464;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#20844;&#24335;&#26469;&#25429;&#25417;&#36825;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning, a decentralized approach to machine learning, faces significant challenges such as extensive communication overheads, slow convergence, and unstable improvements. These challenges primarily stem from the gradient variance due to heterogeneous client data distributions. To address this, we introduce a novel Networked Control Variates (FedNCV) framework for Federated Learning. We adopt the REINFORCE Leave-One-Out (RLOO) as a fundamental control variate unit in the FedNCV framework, implemented at both client and server levels. At the client level, the RLOO control variate is employed to optimize local gradient updates, mitigating the variance introduced by data samples. Once relayed to the server, the RLOO-based estimator further provides an unbiased and low-variance aggregated gradient, leading to robust global updates. This dual-side application is formalized as a linear combination of composite control variates. We provide a mathematical expression capturing this i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.17191</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23558;&#23454;&#20307;&#32465;&#23450;&#21040;&#19978;&#19979;&#25991;&#20013;?
&lt;/p&gt;
&lt;p&gt;
How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27491;&#30830;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24517;&#39035;&#23558;&#23454;&#20307;&#19982;&#20854;&#23646;&#24615;&#36827;&#34892;&#32465;&#23450;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#25551;&#36848;&#8220;&#32511;&#33394;&#26041;&#22359;&#8221;&#21644;&#8220;&#34013;&#33394;&#22278;&#24418;&#8221;&#30340;&#19978;&#19979;&#25991;&#65292;LMs&#24517;&#39035;&#23558;&#24418;&#29366;&#19982;&#23427;&#20204;&#23545;&#24212;&#30340;&#39068;&#33394;&#36827;&#34892;&#32465;&#23450;&#12290;&#25105;&#20204;&#20998;&#26512;LM&#34920;&#31034;&#24182;&#30830;&#23450;&#32465;&#23450;ID&#26426;&#21046;&#65306;&#36825;&#26159;&#19968;&#31181;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#25105;&#20204;&#22312;Pythia&#21644;LLaMA&#23478;&#26063;&#30340;&#27599;&#20010;&#36275;&#22815;&#22823;&#30340;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#12290;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LMs&#20869;&#37096;&#28608;&#27963;&#36890;&#36807;&#23558;&#32465;&#23450;ID&#21521;&#37327;&#38468;&#21152;&#21040;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#23646;&#24615;&#19978;&#26469;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#32465;&#23450;ID&#21521;&#37327;&#24418;&#25104;&#36830;&#32493;&#30340;&#23376;&#31354;&#38388;&#65292;&#22312;&#36825;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#32465;&#23450;ID&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;LMs&#22312;&#19978;&#19979;&#25991;&#20013;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#65292;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;LMs&#20013;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#35201;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;Deep Ritz&#26041;&#27861;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20004;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#19968;&#20010;&#29992;&#20110;&#36924;&#36817;&#35299;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#26032;&#30340;&#25554;&#20540;&#28857;&#20197;&#25913;&#36827;&#35757;&#32451;&#38598;&#12290;&#36825;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#25554;&#20540;&#28857;&#30456;&#20851;&#30340;&#21464;&#20998;&#25439;&#22833;&#26469;&#27714;&#35299;PDE&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#36817;&#20284;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.17185</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#37325;&#35201;&#37319;&#26679;&#26041;&#27861;&#22312;Deep Ritz&#35299;&#27861;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive important sampling for Deep Ritz. (arXiv:2310.17185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#35201;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;Deep Ritz&#26041;&#27861;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20004;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#19968;&#20010;&#29992;&#20110;&#36924;&#36817;&#35299;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#26032;&#30340;&#25554;&#20540;&#28857;&#20197;&#25913;&#36827;&#35757;&#32451;&#38598;&#12290;&#36825;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#25554;&#20540;&#28857;&#30456;&#20851;&#30340;&#21464;&#20998;&#25439;&#22833;&#26469;&#27714;&#35299;PDE&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#36817;&#20284;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;Deep Ritz&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#19968;&#20010;&#32593;&#32476;&#29992;&#20110;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#32780;&#21478;&#19968;&#20010;&#32593;&#32476;&#26159;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#26032;&#30340;&#25554;&#20540;&#28857;&#20197;&#25913;&#36827;&#35757;&#32451;&#38598;&#12290;&#36866;&#24212;&#24615;&#37319;&#26679;&#36807;&#31243;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;Deep Ritz&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#35757;&#32451;&#38598;&#20013;&#30340;&#25554;&#20540;&#28857;&#31163;&#25955;&#21270;&#30456;&#20851;&#30340;&#21464;&#20998;&#25439;&#22833;&#26469;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#31532;&#20108;&#27493;&#28041;&#21450;&#29983;&#25104;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#38598;&#65292;&#28982;&#21518;&#22312;&#21518;&#32493;&#35745;&#31639;&#20013;&#20351;&#29992;&#35813;&#35757;&#32451;&#38598;&#36827;&#19968;&#27493;&#25552;&#39640;&#24403;&#21069;&#36817;&#20284;&#35299;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;&#21464;&#20998;&#25439;&#22833;&#20013;&#30340;&#34987;&#31215;&#20989;&#25968;&#35270;&#20026;&#19968;&#20010;&#38750;&#26631;&#20934;&#21270;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;&#65292;&#24182;&#20351;&#29992;&#31216;&#20026;&#26377;&#30028;KRnet&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36817;&#20284;&#12290;&#26032;&#30340;&#26679;&#26412;&#21450;&#20854;&#30456;&#20851;&#30340;PDF&#20540;&#26159;&#20174;&#26377;&#30028;KRnet&#20013;&#33719;&#21462;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an adaptive sampling method for the Deep Ritz method aimed at solving partial differential equations (PDEs). Two deep neural networks are used. One network is employed to approximate the solution of PDEs, while the other one is a deep generative model used to generate new collocation points to refine the training set. The adaptive sampling procedure consists of two main steps. The first step is solving the PDEs using the Deep Ritz method by minimizing an associated variational loss discretized by the collocation points in the training set. The second step involves generating a new training set, which is then used in subsequent computations to further improve the accuracy of the current approximate solution. We treat the integrand in the variational loss as an unnormalized probability density function (PDF) and approximate it using a deep generative model called bounded KRnet. The new samples and their associated PDF values are obtained from the bounded KRnet. With these ne
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21033;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#26377;&#25928;&#22320;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22635;&#34917;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#36866;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.17178</link><description>&lt;p&gt;
&#22270;&#24418;&#21270;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graphical Object-Centric Actor-Critic. (arXiv:2310.17178v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17178
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21033;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#26377;&#25928;&#22320;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22635;&#34917;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#36866;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#23398;&#20064;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26368;&#26032;&#30340;&#30740;&#31350;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#37319;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#33021;&#22815;&#20419;&#36827;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#23545;&#35937;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22635;&#34917;&#20102;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21487;&#20197;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20010;&#20855;&#26377;&#22797;&#26434;&#35270;&#35273;3D&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#19968;&#20010;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#30340;2D&#29615;&#22659;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have recently been significant advances in the problem of unsupervised object-centric representation learning and its application to downstream tasks. The latest works support the argument that employing disentangled object representations in image-based object-centric reinforcement learning tasks facilitates policy learning. We propose a novel object-centric reinforcement learning algorithm combining actor-critic and model-based approaches to utilize these representations effectively. In our approach, we use a transformer encoder to extract object representations and graph neural networks to approximate the dynamics of an environment. The proposed method fills a research gap in developing efficient object-centric world models for reinforcement learning settings that can be used for environments with discrete or continuous action spaces. Our algorithm performs better in a visually complex 3D robotic environment and a 2D environment with compositional structure than the state-of-t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17176</link><description>&lt;p&gt;
&#20174;&#20840;&#26223;X&#23556;&#32447;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#22312;&#29616;&#20195;&#21475;&#33108;&#20445;&#20581;&#20013;&#26159;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#31934;&#30830;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#29273;&#40831;&#31181;&#26893;&#35774;&#35745;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#25105;&#20204;&#26681;&#25454;FUSegNet&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;&#21019;&#38754;&#20998;&#21106;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#20110;&#32593;&#26684;&#30340;&#27880;&#24847;&#21147;&#38376;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#24341;&#20837;&#23450;&#21521;&#36793;&#30028;&#26694;&#65288;OBB&#65289;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#29273;&#40831;&#23450;&#20301;&#20272;&#35745;&#12290;&#22312;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;DNS&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;543&#20010;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#65292;&#25105;&#20204;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#26368;&#39640;&#30340;&#20132;&#24182;&#27604;&#65288;IoU&#65289;&#24471;&#20998;82.43%&#65292;Dice&#30456;&#20284;&#31995;&#25968;&#65288;DSC&#65289;&#24471;&#20998;90.37%&#65292;&#22312;OBB&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26059;&#36716;&#30340;&#20132;&#24182;&#27604;&#65288;RIoU&#65289;&#24471;&#20998;82.82%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate teeth segmentation and orientation are fundamental in modern oral healthcare, enabling precise diagnosis, treatment planning, and dental implant design. In this study, we present a comprehensive approach to teeth segmentation and orientation from panoramic X-ray images, leveraging deep learning techniques. We build our model based on FUSegNet, a popular model originally developed for wound segmentation, and introduce modifications by incorporating grid-based attention gates into the skip connections. We introduce oriented bounding box (OBB) generation through principal component analysis (PCA) for precise tooth orientation estimation. Evaluating our approach on the publicly available DNS dataset, comprising 543 panoramic X-ray images, we achieve the highest Intersection-over-Union (IoU) score of 82.43% and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU) score of 82.82%. We
&lt;/p&gt;</description></item><item><title>DSAC-C&#26159;&#19968;&#31181;&#32422;&#26463;&#26368;&#22823;&#29109;&#30340;&#40065;&#26834;&#31163;&#25955;&#36719;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#39069;&#22806;&#30340;&#32479;&#35745;&#32422;&#26463;&#25552;&#20379;&#20102;&#23545;&#28508;&#22312;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#21644;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17173</link><description>&lt;p&gt;
DSAC-C: &#32422;&#26463;&#26368;&#22823;&#29109;&#29992;&#20110;&#40065;&#26834;&#24615;&#31163;&#25955;&#21270;&#36719;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic. (arXiv:2310.17173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17173
&lt;/p&gt;
&lt;p&gt;
DSAC-C&#26159;&#19968;&#31181;&#32422;&#26463;&#26368;&#22823;&#29109;&#30340;&#40065;&#26834;&#31163;&#25955;&#36719;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#39069;&#22806;&#30340;&#32479;&#35745;&#32422;&#26463;&#25552;&#20379;&#20102;&#23545;&#28508;&#22312;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#21644;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Soft Actor-Critic (SAC)&#31639;&#27861;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#26367;&#20195;&#35780;&#35770;&#23478;&#31574;&#30053;&#20013;&#24471;&#21040;&#30340;&#39069;&#22806;&#32479;&#35745;&#32422;&#26463;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#31163;&#25955;&#30340;SAC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#32422;&#26463;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;Atari 2600&#28216;&#25103;&#30340;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#21464;&#20307;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension to the family of Soft Actor-Critic (SAC) algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC can be further improved via additional statistical constraints derived from a surrogate critic policy. Furthermore, our findings suggests that these constraints provide an added robustness against potential domain shifts, which are essential for safe deployment of reinforcement learning agents in the real-world. We provide theoretical analysis and show empirical results on low data regimes for both in-distribution and out-of-distribution variants of Atari 2600 games.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#20855;&#26377;&#19968;&#33324;&#24211;&#23384;&#21040;&#36135;&#21160;&#24577;&#19979;&#30340;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20801;&#35768;&#20462;&#25913;&#35746;&#36141;&#25968;&#37327;&#20197;&#28385;&#36275;&#20379;&#24212;&#21830;&#30340;&#38480;&#21046;&#65292;&#24182;&#23558;&#21608;&#26399;&#24615;&#23457;&#26680;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#23450;&#20041;&#20026;&#22806;&#37096;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.17168</link><description>&lt;p&gt;
&#23398;&#20064;&#22788;&#29702;&#20855;&#26377;&#19968;&#33324;&#24211;&#23384;&#21040;&#36135;&#21160;&#24577;&#30340;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning an Inventory Control Policy with General Inventory Arrival Dynamics. (arXiv:2310.17168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#20855;&#26377;&#19968;&#33324;&#24211;&#23384;&#21040;&#36135;&#21160;&#24577;&#19979;&#30340;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20801;&#35768;&#20462;&#25913;&#35746;&#36141;&#25968;&#37327;&#20197;&#28385;&#36275;&#20379;&#24212;&#21830;&#30340;&#38480;&#21046;&#65292;&#24182;&#23558;&#21608;&#26399;&#24615;&#23457;&#26680;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#23450;&#20041;&#20026;&#22806;&#37096;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38754;&#23545;&#19968;&#33324;&#21040;&#36135;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#23398;&#20064;&#21644;&#22238;&#27979;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#25968;&#37327;&#38543;&#26102;&#38388;&#21040;&#36135;&#27169;&#22411;&#65288;QOT&#65289;&#12290;&#22312;&#23454;&#38469;&#20379;&#24212;&#38142;&#20013;&#65292;&#25105;&#20204;&#36824;&#20801;&#35768;&#20462;&#25913;&#35746;&#36141;&#25968;&#37327;&#20197;&#28385;&#36275;&#20379;&#24212;&#21830;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#35746;&#36141;&#26368;&#20302;&#25968;&#37327;&#21644;&#25209;&#27425;&#22823;&#23567;&#32422;&#26463;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22788;&#29702;&#20219;&#24847;&#21040;&#36135;&#21160;&#24577;&#25110;&#20219;&#24847;&#21518;&#32493;&#22788;&#29702;&#30340;&#35746;&#36141;&#25968;&#37327;&#30340;&#30740;&#31350;&#12290;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;Madeka&#31561;&#65292;2022&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21516;&#26679;&#23558;&#21608;&#26399;&#24615;&#23457;&#26680;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#23450;&#20041;&#20026;&#22806;&#37096;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#29366;&#24577;&#19981;&#21463;&#20195;&#29702;&#30340;&#25511;&#21046;&#12290;Madeka&#31561;&#20154;&#65288;2022&#65289;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#22238;&#25918;&#21382;&#21490;&#25968;&#25454;&#20197;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#20363;&#23376;&#20013;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#32435;&#20837;&#21040;&#36135;&#36807;&#31243;&#30340;&#21382;&#21490;&#22238;&#25918;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we address the problem of learning and backtesting inventory control policies in the presence of general arrival dynamics -- which we term as a quantity-over-time arrivals model (QOT). We also allow for order quantities to be modified as a post-processing step to meet vendor constraints such as order minimum and batch size constraints -- a common practice in real supply chains. To the best of our knowledge this is the first work to handle either arbitrary arrival dynamics or an arbitrary downstream post-processing of order quantities. Building upon recent work (Madeka et al., 2022) we similarly formulate the periodic review inventory control problem as an exogenous decision process, where most of the state is outside the control of the agent. Madeka et al. (2022) show how to construct a simulator that replays historic data to solve this class of problem. In our case, we incorporate a deep generative model for the arrivals process as part of the history replay. By formulat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#25193;&#25955;&#36807;&#31243;&#24182;&#30452;&#25509;&#20272;&#35745;&#22270;&#20687;&#21644;&#22122;&#22768;&#65292;&#26412;&#25991;&#25913;&#36827;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#30340;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.17167</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#22270;&#20687;&#21644;&#22122;&#22768;&#25913;&#36827;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise. (arXiv:2310.17167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17167
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#25193;&#25955;&#36807;&#31243;&#24182;&#30452;&#25509;&#20272;&#35745;&#22270;&#20687;&#21644;&#22122;&#22768;&#65292;&#26412;&#25991;&#25913;&#36827;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#30340;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#36129;&#29486;&#65292;&#26088;&#22312;&#36890;&#36807;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#36890;&#36807;&#20197;&#22270;&#20687;&#21644;&#22122;&#22768;&#20043;&#38388;&#30340;&#22235;&#20998;&#20043;&#19968;&#22278;&#24359;&#19978;&#30340;&#35282;&#24230;&#37325;&#26032;&#21442;&#25968;&#21270;&#25193;&#25955;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#35774;&#32622;&#20256;&#32479;&#30340; $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$&#12290;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#28040;&#38500;&#20102;&#20004;&#20010;&#22855;&#24322;&#28857;&#65292;&#24182;&#20801;&#35768;&#23558;&#25193;&#25955;&#28436;&#21270;&#34920;&#36798;&#20026;&#19968;&#20010;&#33391;&#22909;&#34892;&#20026;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#20174;&#32780;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#26356;&#39640;&#38454;&#30340;ODE&#27714;&#35299;&#22120;&#65292;&#22914;Runge-Kutta&#26041;&#27861;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#30452;&#25509;&#20351;&#29992;&#25105;&#20204;&#30340;&#32593;&#32476;&#20272;&#35745;&#22270;&#20687;&#65288;$\mathbf{x}_0$&#65289;&#21644;&#22122;&#22768;&#65288;$\mathbf{\epsilon}$&#65289;&#65292;&#36825;&#20351;&#24471;&#36870;&#21521;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#26356;&#26032;&#27493;&#39588;&#35745;&#31639;&#26356;&#21152;&#31283;&#23450;&#65292;&#22240;&#20026;&#22312;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#21644;&#22122;&#22768;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#20123;&#21464;&#21270;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary differential equation (ODE). In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be used effectively. The second contribution is to directly estimate both the image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which enables more stable calculations of the update step in the inverse diffusion steps, as accurate estimation of both the image and noise are crucial at different stages of the process. Together with these changes, our model achie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#36716;&#25442;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#32479;&#35745;&#32422;&#26463;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#22411;&#26657;&#20934;&#25928;&#26524;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17159</link><description>&lt;p&gt;
&#26368;&#22823;&#29109;&#25439;&#22833;&#65306;&#38024;&#23545;&#36229;&#20986;&#20998;&#24067;&#36716;&#25442;&#30340;&#26657;&#20934;&#30340;&#32422;&#26463;&#26368;&#22823;&#29109;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MaxEnt Loss: Constrained Maximum Entropy for Calibration under Out-of-Distribution Shift. (arXiv:2310.17159v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#36716;&#25442;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#32479;&#35745;&#32422;&#26463;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#22411;&#26657;&#20934;&#25928;&#26524;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#36716;&#25442;&#26657;&#20934;&#38382;&#39064;&#30340;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#30446;&#26631;&#20989;&#25968;&#34987;&#25552;&#20986;&#26469;&#26377;&#25928;&#22320;&#22312;&#20998;&#24067;&#20869;&#26657;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#23427;&#20204;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#24182;&#19981;&#22909;&#12290;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#26377;&#29992;&#30340;&#32479;&#35745;&#32422;&#26463;&#65292;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new loss function that addresses the out-of-distribution (OOD) calibration problem. While many objective functions have been proposed to effectively calibrate models in-distribution, our findings show that they do not always fare well OOD. Based on the Principle of Maximum Entropy, we incorporate helpful statistical constraints observed during training, delivering better model calibration without sacrificing accuracy. We provide theoretical analysis and show empirically that our method works well in practice, achieving state-of-the-art calibration on both synthetic and real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#31232;&#30095;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#31995;&#32479;Deja Vu&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#21442;&#25968;&#38598;&#21512;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#21069;&#25552;&#19979;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.17157</link><description>&lt;p&gt;
Deja Vu: &#19978;&#19979;&#25991;&#31232;&#30095;&#24615;&#22312;LLMs&#25512;&#29702;&#20013;&#30340;&#39640;&#25928;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. (arXiv:2310.17157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#31232;&#30095;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#31995;&#32479;Deja Vu&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#21442;&#25968;&#38598;&#21512;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#21069;&#25552;&#19979;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#25968;&#30334;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24341;&#21457;&#20102;&#19968;&#27874;&#26032;&#30340;&#20196;&#20154;&#20852;&#22859;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#26102;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#31232;&#30095;&#24615;&#26159;&#38477;&#20302;&#36825;&#31181;&#25104;&#26412;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#27861;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#35201;&#20040;&#25918;&#24323;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#35201;&#20040;&#22312;&#29616;&#20195;&#30828;&#20214;&#19978;&#26080;&#27861;&#25552;&#20379;&#22681;&#19978;&#26102;&#38388;&#36895;&#24230;&#25552;&#21319;&#12290;&#25105;&#20204;&#20551;&#35774;&#19978;&#19979;&#25991;&#31232;&#30095;&#24615;&#65292;&#21363;&#23545;&#20110;&#32473;&#23450;&#36755;&#20837;&#32780;&#35328;&#65292;&#20135;&#29983;&#19982;&#31264;&#23494;&#27169;&#22411;&#22823;&#33268;&#30456;&#21516;&#36755;&#20986;&#30340;&#23567;&#30340;&#12289;&#36755;&#20837;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#21442;&#25968;&#38598;&#21512;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#31232;&#30095;&#24615;&#30340;&#23384;&#22312;&#65292;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#34987;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#22312;&#22681;&#19978;&#26102;&#38388;&#19978;&#21152;&#36895;LLM&#30340;&#25512;&#29702;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;LLM&#30340;&#36136;&#37327;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deja Vu&#65292;&#19968;&#20010;&#20351;&#29992;&#20302;&#25104;&#26412;&#31639;&#27861;&#26681;&#25454;&#27599;&#23618;&#30340;&#36755;&#20837;&#23454;&#26102;&#39044;&#27979;&#19978;&#19979;&#25991;&#31232;&#30095;&#24615;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;0.55T&#20302;&#22330;MRI&#20013;&#23545;&#20581;&#24247;&#25511;&#21046;&#32773;&#33181;&#20851;&#33410;&#36827;&#34892;&#26631;&#35760;&#29289;&#23450;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#25216;&#26415;&#22312;&#20998;&#21106;&#36719;&#39592;&#21306;&#22495;&#26041;&#38754;&#19982;3.0T&#20960;&#20046;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.17152</link><description>&lt;p&gt;
&#25216;&#26415;&#27880;&#35299;&#65306;&#23558;&#22312;&#20302;&#22330;MRI 0.55T&#33181;&#20851;&#33410;MRI gesunden &#25511;&#21046;&#32773;&#36827;&#34892;&#35757;&#32451;&#30340;3.0T&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls. (arXiv:2310.17152v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;0.55T&#20302;&#22330;MRI&#20013;&#23545;&#20581;&#24247;&#25511;&#21046;&#32773;&#33181;&#20851;&#33410;&#36827;&#34892;&#26631;&#35760;&#29289;&#23450;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#25216;&#26415;&#22312;&#20998;&#21106;&#36719;&#39592;&#21306;&#22495;&#26041;&#38754;&#19982;3.0T&#20960;&#20046;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#37327;&#21270;&#20581;&#24247;&#25511;&#21046;&#32773;0.55T&#33181;&#20851;&#33410;&#29983;&#29289;&#26631;&#35760;&#29289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19982;3.0T&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#30740;&#31350;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#35780;&#20272;&#20102;0.55T&#19979;&#26631;&#20934;&#30340;&#39592;&#39612;&#21644;&#36719;&#39592;&#20998;&#21106;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#27604;&#36739;&#20102;&#20998;&#21106;&#24615;&#33021;&#30340;&#24046;&#24322;&#12289;&#25913;&#36827;&#31354;&#38388;&#30340;&#21306;&#22495;&#20197;&#21450;0.55T&#21644;3.0T&#20043;&#38388;&#30340;&#36719;&#39592;&#21402;&#24230;&#20540;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#33021;&#22815;&#22312;0.55T&#30340;&#33181;&#20851;&#33410;MRI&#20013;&#23454;&#29616;&#21487;&#34892;&#30340;&#36716;&#35793;&#65292;&#23588;&#20854;&#22312;&#20998;&#21106;&#36719;&#39592;&#21306;&#22495;&#26041;&#38754;&#65292;&#27169;&#22411;&#22312;Likert&#25490;&#21517;&#19978;&#34920;&#29616;&#20960;&#20046;&#31561;&#21516;&#20110;3.0T&#12290;&#22240;&#27492;&#65292;0.55T&#20302;&#22330;&#21487;&#25345;&#32493;&#21644;&#26131;&#20110;&#23433;&#35013;&#30340;MRI&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#20581;&#24247;&#25511;&#21046;&#32773;&#30340;&#33181;&#20851;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current study, our purpose is to evaluate the feasibility of applying deep learning (DL) enabled algorithms to quantify bilateral knee biomarkers in healthy controls scanned at 0.55T and compared with 3.0T. The current study assesses the performance of standard in-practice bone, and cartilage segmentation algorithms at 0.55T, both qualitatively and quantitatively, in terms of comparing segmentation performance, areas of improvement, and compartment-wise cartilage thickness values between 0.55T vs. 3.0T. Initial results demonstrate a usable to good technical feasibility of translating existing quantitative deep-learning-based image segmentation techniques, trained on 3.0T, out of 0.55T for knee MRI, in a multi-vendor acquisition environment. Especially in terms of segmenting cartilage compartments, the models perform almost equivalent to 3.0T in terms of Likert ranking. The 0.55T low-field sustainable and easy-to-install MRI, as demonstrated, thus, can be utilized for evaluating 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STExplainer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#22478;&#24066;&#36164;&#28304;&#20998;&#37197;&#21644;&#25919;&#31574;&#21046;&#23450;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17149</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Explainable Spatio-Temporal Graph Neural Networks. (arXiv:2310.17149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STExplainer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#22478;&#24066;&#36164;&#28304;&#20998;&#37197;&#21644;&#25919;&#31574;&#21046;&#23450;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#22478;&#24066;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#26234;&#33021;&#20132;&#36890;&#21644;&#20844;&#20849;&#23433;&#20840;&#31561;&#65292;&#22240;&#20854;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#22478;&#24066;&#36164;&#28304;&#20998;&#37197;&#21644;&#25919;&#31574;&#21046;&#23450;&#30456;&#20851;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STExplainer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;STGNNs&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#32479;&#19968;&#30340;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19982;&#20301;&#32622;&#20449;&#24687;&#34701;&#21512;&#23618;&#20316;&#20026;STG&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#36827;&#34892;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#21407;&#21017;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#35299;&#37322;&#30446;&#26631;&#30340;&#32467;&#26500;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#20363;&#21270;GIB&#21407;&#21017;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal graph neural networks (STGNNs) have gained popularity as a powerful tool for effectively modeling spatio-temporal dependencies in diverse real-world urban applications, including intelligent transportation and public safety. However, the black-box nature of STGNNs limits their interpretability, hindering their application in scenarios related to urban resource allocation and policy formulation. To bridge this gap, we propose an Explainable Spatio-Temporal Graph Neural Networks (STExplainer) framework that enhances STGNNs with inherent explainability, enabling them to provide accurate predictions and faithful explanations simultaneously. Our framework integrates a unified spatio-temporal graph attention network with a positional information fusion layer as the STG encoder and decoder, respectively. Furthermore, we propose a structure distillation approach based on the Graph Information Bottleneck (GIB) principle with an explainable objective, which is instantiated by the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;&#36890;&#36807;&#20154;&#31867;&#29992;&#25143;&#25552;&#20379;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#30340;&#27880;&#37322;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#21644;&#21152;&#26435;&#30340;&#26032;&#22411;OPE&#20272;&#35745;&#22120;&#31995;&#21015;&#12290;</title><link>http://arxiv.org/abs/2310.17146</link><description>&lt;p&gt;
&#21322;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#37325;&#35201;&#24615;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation. (arXiv:2310.17146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17146
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;&#36890;&#36807;&#20154;&#31867;&#29992;&#25143;&#25552;&#20379;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#30340;&#27880;&#37322;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#21644;&#21152;&#26435;&#30340;&#26032;&#22411;OPE&#20272;&#35745;&#22120;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#39046;&#22495;&#26102;&#65292;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#21487;&#20197;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#20102;&#35299;&#26032;&#31574;&#30053;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31163;&#32593;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#22312;&#26412;&#36136;&#19978;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#31163;&#32447;&#25968;&#25454;&#21487;&#33021;&#19981;&#21453;&#26144;&#30001;&#20110;&#24212;&#29992;&#26032;&#31574;&#30053;&#32780;&#23548;&#33268;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#26681;&#25454;&#26032;&#31574;&#30053;&#25910;&#38598;&#36712;&#36857;&#36827;&#34892;&#22312;&#32447;&#35780;&#20272;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#22312;&#36825;&#20123;&#39046;&#22495;&#37096;&#32626;&#26032;&#31574;&#30053;&#21487;&#33021;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#35780;&#20272;&#26694;&#26550;&#65292;&#20316;&#20026;&#31163;&#32447;&#21644;&#22312;&#32447;&#35780;&#20272;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#20854;&#20013;&#20154;&#31867;&#29992;&#25143;&#25552;&#20379;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#30340;&#27880;&#37322;&#12290;&#34429;&#28982;&#35825;&#20154;&#22320;&#31616;&#21333;&#22320;&#29992;&#36825;&#20123;&#27880;&#37322;&#26469;&#22686;&#21152;&#29616;&#26377;&#25968;&#25454;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#22825;&#30495;&#30340;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26377;&#20559;&#30340;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#21644;&#26032;&#39062;&#21152;&#26435;&#30340;OPE&#20272;&#35745;&#22120;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
In applying reinforcement learning (RL) to high-stakes domains, quantitative and qualitative evaluation using observational data can help practitioners understand the generalization performance of new policies. However, this type of off-policy evaluation (OPE) is inherently limited since offline data may not reflect the distribution shifts resulting from the application of new policies. On the other hand, online evaluation by collecting rollouts according to the new policy is often infeasible, as deploying new policies in these domains can be unsafe. In this work, we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories. While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results. Instead, we design a new family of OPE estimators based on importance sampling (IS) and a novel weighting s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21452;&#27169;&#25311;&#30340;&#34920;&#31034;&#26041;&#27861;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#30340;&#36716;&#25442;&#20197;&#21450;&#22870;&#21169;&#32553;&#25918;&#23545;&#20854;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#24212;&#29992;&#26399;&#26395;&#20540;&#31639;&#23376;&#19982;&#36866;&#24403;&#30340;&#22870;&#21169;&#32553;&#25918;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17139</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#35299;&#20915;&#22522;&#20110;&#21452;&#27169;&#25311;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning. (arXiv:2310.17139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21452;&#27169;&#25311;&#30340;&#34920;&#31034;&#26041;&#27861;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#30340;&#36716;&#25442;&#20197;&#21450;&#22870;&#21169;&#32553;&#25918;&#23545;&#20854;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#24212;&#29992;&#26399;&#26395;&#20540;&#31639;&#23376;&#19982;&#36866;&#24403;&#30340;&#22870;&#21169;&#32553;&#25918;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#21452;&#27169;&#25311;&#30340;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#40065;&#26834;&#30340;&#29366;&#24577;&#34920;&#31034;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20854;&#22312;&#31163;&#32447;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#24182;&#19981;&#29702;&#24819;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20854;&#24615;&#33021;&#29978;&#33267;&#26126;&#26174;&#20302;&#20110;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#29702;&#35299;&#20026;&#20160;&#20040;&#21452;&#27169;&#25311;&#26041;&#27861;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25104;&#21151;&#65292;&#20294;&#22312;&#31163;&#32447;&#20219;&#21153;&#20013;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#30340;&#36716;&#25442;&#23545;&#21452;&#27169;&#25311;&#21407;&#21017;&#30340;&#29305;&#21035;&#26377;&#23475;&#65292;&#23548;&#33268;&#20272;&#35745;&#26080;&#25928;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22870;&#21169;&#32553;&#25918;&#22312;&#38480;&#21046;&#21452;&#27169;&#25311;&#27979;&#37327;&#33539;&#22260;&#21644;&#20854;&#24341;&#36215;&#30340;&#20540;&#35823;&#24046;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#24212;&#29992;&#26399;&#26395;&#20540;&#31639;&#23376;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#26377;&#21161;&#20110;&#38450;&#27490;&#23545;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#36807;&#25311;&#21512;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#24403;&#30340;&#22870;&#21169;&#32553;&#25918;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#23849;&#28291;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
While bisimulation-based approaches hold promise for learning robust state representations for Reinforcement Learning (RL) tasks, their efficacy in offline RL tasks has not been up to par. In some instances, their performance has even significantly underperformed alternative methods. We aim to understand why bisimulation methods succeed in online settings, but falter in offline tasks. Our analysis reveals that missing transitions in the dataset are particularly harmful to the bisimulation principle, leading to ineffective estimation. We also shed light on the critical role of reward scaling in bounding the scale of bisimulation measurements and of the value error they induce. Based on these findings, we propose to apply the expectile operator for representation learning to our offline RL setting, which helps to prevent overfitting to incomplete data. Meanwhile, by introducing an appropriate reward scaling strategy, we avoid the risk of feature collapse in representation space. We imple
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17137</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39640;&#26031;&#36807;&#31243;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Gaussian Processes via Alternating Projection. (arXiv:2310.17137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#36229;&#21442;&#25968;&#20248;&#21270;&#38656;&#35201;&#21453;&#22797;&#27714;&#35299;&#20855;&#26377; nxn &#26680;&#30697;&#38453;&#30340;&#32447;&#24615;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915; O(n^3) &#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#24555;&#36895;&#36845;&#20195;&#25968;&#20540;&#26041;&#27861;&#65292;&#22914;&#20849;&#36717;&#26799;&#24230;&#65288;CG&#65289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#30456;&#24212;&#30340;&#26680;&#30697;&#38453;&#21464;&#24471;&#36234;&#26469;&#36234;&#30149;&#24577;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38656;&#35201; O(n^2) &#30340;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982; CG &#22686;&#21152;&#20102;&#21487;&#35757;&#32451; GP &#22522;&#20110;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#20294;&#29616;&#20195;&#25968;&#25454;&#38598;&#24050;&#32463;&#36798;&#21040;&#36229;&#20986;&#20854;&#36866;&#29992;&#33539;&#22260;&#30340;&#35268;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#35775;&#38382;&#26680;&#30697;&#38453;&#30340;&#23376;&#22359;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#23567;&#25209;&#37327;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#20132;&#26367;&#25237;&#24433;&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20026; O(n)&#65292;&#35299;&#20915;&#20102;&#23558; GP &#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#26102;&#30340;&#35768;&#22810;&#23454;&#38469;&#25361;&#25112;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#20174;&#23454;&#35777;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Gaussian process (GP) hyperparameter optimization requires repeatedly solving linear systems with $n \times n$ kernel matrices. To address the prohibitive $\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative numerical methods, like conjugate gradients (CG). However, as datasets increase in magnitude, the corresponding kernel matrices become increasingly ill-conditioned and still require $\mathcal{O}(n^2)$ space without partitioning. Thus, while CG increases the size of datasets GPs can be trained on, modern datasets reach scales beyond its applicability. In this work, we propose an iterative method which only accesses subblocks of the kernel matrix, effectively enabling \emph{mini-batching}. Our algorithm, based on alternating projection, has $\mathcal{O}(n)$ per-iteration time and space complexity, solving many of the practical challenges of scaling GPs to very large datasets. Theoretically, we prove our method enjoys linear convergence and empirically we demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;GNN&#26410;&#20805;&#20998;&#21033;&#29992;&#20869;&#22312;&#30340;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#30693;&#35782;&#20256;&#36882;&#30340;&#25554;&#25300;&#24335;&#26041;&#27861;&#65292;&#20351;GNN&#33021;&#22815;&#20805;&#20998;&#37322;&#25918;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#20462;&#25913;&#21407;&#22987;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.17132</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#30693;&#35782;&#20256;&#36882;&#37322;&#25918;GNN&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the potential of GNNs via Bi-directional Knowledge Transfer. (arXiv:2310.17132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;GNN&#26410;&#20805;&#20998;&#21033;&#29992;&#20869;&#22312;&#30340;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#30693;&#35782;&#20256;&#36882;&#30340;&#25554;&#25300;&#24335;&#26041;&#27861;&#65292;&#20351;GNN&#33021;&#22815;&#20805;&#20998;&#37322;&#25918;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#20462;&#25913;&#21407;&#22987;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29305;&#24449;&#20256;&#25773;&#26426;&#21046;&#26469;&#25552;&#39640;GNN&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29305;&#24449;&#36716;&#25442;&#65292;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#25805;&#20316;&#65292;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#20960;&#31181;&#20856;&#22411;GNN&#20013;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#24615;&#33021;&#12290;&#24847;&#22806;&#30340;&#26159;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;GNN&#27809;&#26377;&#23436;&#20840;&#37322;&#25918;&#20986;&#20869;&#22312;&#30340;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#21521;&#30693;&#35782;&#20256;&#36882;&#65288;BiKT&#65289;&#65292;&#19968;&#31181;&#25554;&#25300;&#24335;&#26041;&#27861;&#65292;&#26088;&#22312;&#37322;&#25918;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#20462;&#25913;&#21407;&#22987;&#26550;&#26500;&#12290;&#23558;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#35270;&#20026;&#19982;&#21407;&#22987;GNN&#20849;&#20139;&#21442;&#25968;&#30340;&#27966;&#29983;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#35813;&#27169;&#22411;&#30340;&#30452;&#25509;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#19982;&#25299;&#25169;&#26080;&#20851;&#30340;&#30693;&#35782;&#21453;&#39304;&#65292;&#36827;&#19968;&#27493;&#25351;&#23548;&#20102;GNN&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the message-passing paradigm, there has been an amount of research proposing diverse and impressive feature propagation mechanisms to improve the performance of GNNs. However, less focus has been put on feature transformation, another major operation of the message-passing framework. In this paper, we first empirically investigate the performance of the feature transformation operation in several typical GNNs. Unexpectedly, we notice that GNNs do not completely free up the power of the inherent feature transformation operation. By this observation, we propose the Bi-directional Knowledge Transfer (BiKT), a plug-and-play approach to unleash the potential of the feature transformation operations without modifying the original architecture. Taking the feature transformation operation as a derived representation learning model that shares parameters with the original GNN, the direct prediction by this model provides a topological-agnostic knowledge feedback that can further instru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#20027;&#39064;&#20998;&#27573;&#27169;&#22411;&#22312;&#27492;&#31867;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#20998;&#27573;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#20351;&#29992;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#20943;&#36731;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.17120</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23545;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#20998;&#27573;
&lt;/p&gt;
&lt;p&gt;
Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#20027;&#39064;&#20998;&#27573;&#27169;&#22411;&#22312;&#27492;&#31867;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#20998;&#27573;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#20351;&#29992;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#20943;&#36731;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23558;&#25991;&#26723;&#25110;&#23545;&#35805;&#26681;&#25454;&#20854;&#35821;&#20041;&#32467;&#26500;&#20998;&#35299;&#20026;&#22810;&#20010;&#36830;&#32493;&#29255;&#27573;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#24110;&#21161;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20851;&#20110;&#20027;&#39064;&#20998;&#27573;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#32467;&#26500;&#21270;&#25991;&#26412;&#30340;&#20998;&#27573;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20027;&#39064;&#20998;&#27573;&#27169;&#22411;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;a&#65289;&#30446;&#21069;&#22312;&#22823;&#35268;&#27169;&#32467;&#26500;&#21270;&#25991;&#26412;&#35821;&#26009;&#24211;&#65288;&#22914;Wiki-727K&#65289;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#23545;&#20110;&#22312;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#19978;&#30340;&#21487;&#20256;&#36882;&#24615;&#24182;&#19981;&#26377;&#24110;&#21161;&#12290;&#65288;b&#65289;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#30446;&#26631;&#38750;&#32467;&#26500;&#21270;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#20998;&#27573;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23581;&#35797;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#26469;&#36827;&#34892;&#25105;&#20204;&#30340;&#20027;&#39064;&#20998;&#27573;&#26041;&#27861;&#30340;&#24378;&#21270;&#27979;&#35797;&#65292;&#20197;&#20943;&#36731;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;Fo
&lt;/p&gt;
&lt;p&gt;
Breaking down a document or a conversation into multiple contiguous segments based on its semantic structure is an important and challenging problem in NLP, which can assist many downstream tasks. However, current works on topic segmentation often focus on segmentation of structured texts. In this paper, we comprehensively analyze the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts. We find that: (a) Current strategies of pre-training on a large corpus of structured text such as Wiki-727K do not help in transferability to unstructured conversational data. (b) Training from scratch with only a relatively small-sized dataset of the target unstructured domain improves the segmentation results by a significant margin. We stress-test our proposed Topic Segmentation approach by experimenting with multiple loss functions, in order to mitigate effects of imbalance in unstructured conversational datasets. Our empirical evaluation indicates that Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22238;&#24402;&#35774;&#32622;&#19979;CART&#30340;&#25910;&#25947;&#24615;&#65292;&#24314;&#31435;&#20102;&#36275;&#22815;&#19981;&#32431;&#24230;&#20943;&#23569;&#26465;&#20214;&#19979;CART&#39044;&#27979;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#26131;&#20110;&#39564;&#35777;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36825;&#23545;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.17114</link><description>&lt;p&gt;
CART&#22312;&#36275;&#22815;&#19981;&#32431;&#24230;&#20943;&#23569;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of CART under Sufficient Impurity Decrease Condition. (arXiv:2310.17114v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22238;&#24402;&#35774;&#32622;&#19979;CART&#30340;&#25910;&#25947;&#24615;&#65292;&#24314;&#31435;&#20102;&#36275;&#22815;&#19981;&#32431;&#24230;&#20943;&#23569;&#26465;&#20214;&#19979;CART&#39044;&#27979;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#26131;&#20110;&#39564;&#35777;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36825;&#23545;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#36890;&#24120;&#20351;&#29992;CART&#20197;&#36882;&#24402;&#36138;&#23146;&#30340;&#26041;&#24335;&#25311;&#21512;&#20915;&#31574;&#26641;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22238;&#24402;&#35774;&#32622;&#19979;CART&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#36275;&#22815;&#19981;&#32431;&#24230;&#20943;&#23569;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;CART&#30340;&#39044;&#27979;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#35813;&#32467;&#26524;&#25913;&#36827;&#20102;&#20043;&#21069;&#31867;&#20284;&#20551;&#35774;&#19979;&#30340;&#24050;&#30693;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#31034;&#20363;&#35777;&#26126;&#35823;&#24046;&#30028;&#38480;&#26080;&#27861;&#36890;&#36807;&#24120;&#25968;&#25110;&#23545;&#25968;&#22240;&#23376;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#26131;&#20110;&#39564;&#35777;&#30340;&#36275;&#22815;&#26465;&#20214;&#20197;&#28385;&#36275;&#19981;&#32431;&#24230;&#20943;&#23569;&#26465;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21152;&#24615;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#21482;&#35201;&#32452;&#20214;&#20989;&#25968;&#31526;&#21512;&#8220;&#23616;&#37096;&#21453;&#21521;&#27874;&#26494;&#19981;&#31561;&#24335;&#8221;&#65292;&#23601;&#21487;&#20197;&#28385;&#36275;&#19981;&#32431;&#24230;&#20943;&#23569;&#26465;&#20214;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#20010;&#22312;&#38750;&#21442;&#25968;&#35774;&#32622;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#20989;&#25968;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The decision tree is a flexible machine learning model that finds its success in numerous applications. It is usually fitted in a recursively greedy manner using CART. In this paper, we investigate the convergence rate of CART under a regression setting. First, we establish an upper bound on the prediction error of CART under a sufficient impurity decrease (SID) condition \cite{chi2022asymptotic} -- our result improves upon the known result by \cite{chi2022asymptotic} under a similar assumption. Furthermore, we provide examples that demonstrate the error bound cannot be further improved by more than a constant or a logarithmic factor. Second, we introduce a set of easily verifiable sufficient conditions for the SID condition. Specifically, we demonstrate that the SID condition can be satisfied in the case of an additive model, provided that the component functions adhere to a ``locally reverse Poincar{\'e} inequality". We discuss several well-known function classes in non-parametric es
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;LLM4DyG&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.17110</link><description>&lt;p&gt;
LLM4DyG&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#35299;&#20915;&#21160;&#24577;&#22270;&#19978;&#30340;&#38382;&#39064;&#21527;?
&lt;/p&gt;
&lt;p&gt;
LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?. (arXiv:2310.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;LLM4DyG&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#30340;&#26102;&#20195;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#25506;&#32034;LLMs&#22312;&#22788;&#29702;&#32593;&#32476;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#21160;&#24577;&#22270;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#23427;&#20204;&#25429;&#25417;&#20102;&#32593;&#32476;&#28436;&#21270;&#27169;&#24335;&#12290;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;Web&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#36825;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#27425;&#25552;&#20986;&#22312;&#21160;&#24577;&#22270;&#19978;&#35780;&#20272;LLMs&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM4DyG&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#20061;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#20219;&#21153;&#65292;&#32771;&#34385;&#20102;LLMs&#22312;&#26102;&#24577;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#25968;&#25454;&#32479;&#35745;&#12289;&#25552;&#31034;&#25216;&#26415;&#21644;LLMs&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era marked by the increasing adoption of Large Language Models (LLMs) for various tasks, there is a growing focus on exploring LLMs' capabilities in handling web data, particularly graph data. Dynamic graphs, which capture temporal network evolution patterns, are ubiquitous in real-world web data. Evaluating LLMs' competence in understanding spatial-temporal information on dynamic graphs is essential for their adoption in web applications, which remains unexplored in the literature. In this paper, we bridge the gap via proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic graphs, to the best of our knowledge, for the first time. Specifically, we propose the LLM4DyG benchmark, which includes nine specially designed tasks considering the capability evaluation of LLMs from both temporal and spatial dimensions. Then, we conduct extensive experiments to analyze the impacts of different data generators, data statistics, prompting techniques, and LLMs on the mo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20808;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#65292;&#21457;&#29616;&#36793;&#32536;&#30340;&#36873;&#25321;&#21463;&#21040;&#32467;&#26500;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#32534;&#36753;&#26102;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#25913;&#36827;&#32534;&#36753;&#65292;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ORE&#30340;&#36845;&#20195;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#36753;&#26368;&#39640;&#35780;&#20998;&#30340;&#36793;&#32536;&#24182;&#37325;&#26032;&#23884;&#20837;&#22270;&#26469;&#21047;&#26032;&#26799;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#22909;&#24615;&#36739;&#39640;&#30340;&#36793;&#32536;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.17100</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32593;&#32476;&#35774;&#35745;&#65306;&#35782;&#21035;&#25361;&#25112;&#24182;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Network Design through Graph Neural Networks: Identifying Challenges and Improving Performance. (arXiv:2310.17100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17100
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20808;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#65292;&#21457;&#29616;&#36793;&#32536;&#30340;&#36873;&#25321;&#21463;&#21040;&#32467;&#26500;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#32534;&#36753;&#26102;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#25913;&#36827;&#32534;&#36753;&#65292;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ORE&#30340;&#36845;&#20195;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#36753;&#26368;&#39640;&#35780;&#20998;&#30340;&#36793;&#32536;&#24182;&#37325;&#26032;&#23884;&#20837;&#22270;&#26469;&#21047;&#26032;&#26799;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#22909;&#24615;&#36739;&#39640;&#30340;&#36793;&#32536;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30740;&#31350;&#24050;&#32463;&#20135;&#29983;&#20102;&#19968;&#20123;&#31574;&#30053;&#65292;&#20351;&#29992;&#35757;&#32451;&#36807;&#30340;GNN&#30340;&#26799;&#24230;&#26469;&#20462;&#25913;&#22270;&#30340;&#36793;&#65292;&#20197;&#23454;&#29616;&#32593;&#32476;&#35774;&#35745;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#22522;&#20110;&#26799;&#24230;&#30340;&#32534;&#36753;&#30340;&#22240;&#32032;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#36825;&#23548;&#33268;&#36873;&#25321;&#36793;&#32536;&#30340;&#21407;&#22240;&#20197;&#21450;&#32534;&#36753;&#26159;&#21542;&#22522;&#20110;&#36793;&#32536;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#27169;&#31946;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#38416;&#26126;&#20102;&#24433;&#21709;&#32534;&#36753;&#30340;&#22240;&#32032;&#65292;&#24182;&#31361;&#20986;&#20102;&#23545;&#32467;&#26500;&#23646;&#24615;&#36807;&#24230;&#20381;&#36182;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#32536;&#21487;&#20197;&#23454;&#29616;&#39640;&#26799;&#24230;&#65292;&#21407;&#22240;&#26159;&#32467;&#26500;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#37325;&#35201;&#24615;&#65292;&#23548;&#33268;&#24403;&#36825;&#20123;&#22240;&#32032;&#19982;&#35774;&#35745;&#20219;&#21153;&#26080;&#20851;&#26102;&#20986;&#29616;&#38169;&#35823;&#30340;&#32534;&#36753;&#12290;&#20026;&#20102;&#25913;&#36827;&#32534;&#36753;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ORE&#65292;&#19968;&#31181;&#36845;&#20195;&#32534;&#36753;&#26041;&#27861;&#65292;&#65288;a&#65289;&#32534;&#36753;&#26368;&#39640;&#35780;&#20998;&#30340;&#36793;&#32536;&#65292;&#65288;b&#65289;&#37325;&#26032;&#23884;&#20837;&#32534;&#36753;&#21518;&#30340;&#22270;&#20197;&#21047;&#26032;&#26799;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#22909;&#24615;&#36739;&#39640;&#30340;&#36793;&#32536;&#36873;&#25321;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#32452;&#25311;&#35758;&#30340;&#35774;&#35745;&#20219;&#21153;&#23545;ORE&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#19968;&#20010;&#22806;&#37096;&#39564;&#35777;&#26041;&#27861;&#65292;&#35777;&#26126;ORE&#21487;&#20197;&#25913;&#21892;&#32534;&#36753;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) research has produced strategies to modify a graph's edges using gradients from a trained GNN, with the goal of network design. However, the factors which govern gradient-based editing are understudied, obscuring why edges are chosen and if edits are grounded in an edge's importance. Thus, we begin by analyzing the gradient computation in previous works, elucidating the factors that influence edits and highlighting the potential over-reliance on structural properties. Specifically, we find that edges can achieve high gradients due to structural biases, rather than importance, leading to erroneous edits when the factors are unrelated to the design task. To improve editing, we propose ORE, an iterative editing method that (a) edits the highest scoring edges and (b) re-embeds the edited graph to refresh gradients, leading to less biased edge choices. We empirically study ORE through a set of proposed design tasks, each with an external validation method, demonst
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#23398;&#20064;&#29575;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#20135;&#29983;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#21253;&#25324;&#31283;&#23450;&#30340;&#36793;&#30028;&#12289;&#24179;&#34913;&#21644;&#24377;&#23556;&#65292;&#24182;&#36890;&#36807;&#21457;&#23637;&#26032;&#30340;&#20840;&#23616;&#25910;&#25947;&#29702;&#35770;&#21644;&#30740;&#31350;&#33391;&#22909;&#35268;&#21017;&#24615;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#36825;&#20123;&#29616;&#35937;&#24402;&#32435;&#20026;&#21516;&#19968;&#29616;&#35937;&#30340;&#19981;&#21516;&#34920;&#29616;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.17087</link><description>&lt;p&gt;
&#33391;&#22909;&#30340;&#35268;&#21017;&#24615;&#21019;&#36896;&#20102;&#22823;&#23398;&#20064;&#29575;&#30340;&#38544;&#24615;&#20559;&#24046;&#65306;&#31283;&#23450;&#30340;&#36793;&#30028;&#65292;&#24179;&#34913;&#21644;&#24377;&#23556;
&lt;/p&gt;
&lt;p&gt;
Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult. (arXiv:2310.17087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#23398;&#20064;&#29575;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#20135;&#29983;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#21253;&#25324;&#31283;&#23450;&#30340;&#36793;&#30028;&#12289;&#24179;&#34913;&#21644;&#24377;&#23556;&#65292;&#24182;&#36890;&#36807;&#21457;&#23637;&#26032;&#30340;&#20840;&#23616;&#25910;&#25947;&#29702;&#35770;&#21644;&#30740;&#31350;&#33391;&#22909;&#35268;&#21017;&#24615;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#36825;&#20123;&#29616;&#35937;&#24402;&#32435;&#20026;&#21516;&#19968;&#29616;&#35937;&#30340;&#19981;&#21516;&#34920;&#29616;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#26102;&#65292;&#22823;&#23398;&#20064;&#29575;&#20250;&#20135;&#29983;&#21508;&#31181;&#38544;&#24615;&#20559;&#24046;&#65292;&#21253;&#25324;&#31283;&#23450;&#30340;&#36793;&#30028;&#12289;&#24179;&#34913;&#21644;&#24377;&#23556;&#12290;&#36825;&#20123;&#29616;&#35937;&#26080;&#27861;&#29992;&#32463;&#20856;&#30340;&#20248;&#21270;&#29702;&#35770;&#24456;&#22909;&#22320;&#35299;&#37322;&#12290;&#23613;&#31649;&#22312;&#29702;&#35299;&#36825;&#20123;&#38544;&#24615;&#20559;&#24046;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#19981;&#28165;&#26970;&#23427;&#20204;&#22312;&#21738;&#20123;&#30446;&#26631;&#20989;&#25968;&#19978;&#20250;&#21457;&#29983;&#12290;&#26412;&#25991;&#23545;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#30340;&#27493;&#39588;&#65292;&#21363;&#36825;&#20123;&#38544;&#24615;&#20559;&#24046;&#23454;&#38469;&#19978;&#26159;&#21516;&#19968;&#20912;&#23665;&#30340;&#21508;&#31181;&#23574;&#31471;&#12290;&#24403;&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#19968;&#23450;&#30340;&#33391;&#22909;&#35268;&#21017;&#24615;&#65292;&#24182;&#19982;&#22823;&#23398;&#20064;&#29575;&#26799;&#24230;&#19979;&#38477;&#23545;&#21521;&#26356;&#24179;&#22374;&#21306;&#22495;&#31227;&#21160;&#30340;&#21487;&#35777;&#26126;&#20559;&#22909;&#30456;&#32467;&#21512;&#26102;&#65292;&#23601;&#20250;&#20135;&#29983;&#36825;&#20123;&#38750;&#24179;&#20961;&#30340;&#21160;&#21147;&#23398;&#29616;&#35937;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#23398;&#20064;&#29575;&#20840;&#23616;&#25910;&#25947;&#29702;&#35770;&#65292;&#38024;&#23545;&#19968;&#26063;&#38750;&#20984;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large learning rates, when applied to gradient descent for nonconvex optimization, yield various implicit biases including the edge of stability (Cohen et al., 2021), balancing (Wang et al., 2022), and catapult (Lewkowycz et al., 2020). These phenomena cannot be well explained by classical optimization theory. Though significant theoretical progress has been made in understanding these implicit biases, it remains unclear for which objective functions would they occur. This paper provides an initial step in answering this question, namely that these implicit biases are in fact various tips of the same iceberg. They occur when the objective function of optimization has some good regularity, which, in combination with a provable preference of large learning rate gradient descent for moving toward flatter regions, results in these nontrivial dynamical phenomena. To establish this result, we develop a new global convergence theory under large learning rates, for a family of nonconvex functi
&lt;/p&gt;</description></item><item><title>Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.17086</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#19968;&#39033;&#19982;&#32447;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17086
&lt;/p&gt;
&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#33021;&#36890;&#36807;&#20869;&#37096;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21363;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#20197;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20026;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#19968;&#20010;&#38750;&#24120;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;&#20174;&#23454;&#35777;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;Transformer&#23618;&#30340;&#39044;&#27979;&#19982;&#29275;&#39039;&#27861;&#30340;&#19981;&#21516;&#36845;&#20195;&#38750;&#24120;&#25509;&#36817;&#65292;&#27599;&#20010;&#20013;&#38388;&#23618;&#22823;&#33268;&#35745;&#31639;&#20102;3&#27425;&#36845;&#20195;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#25165;&#33021;&#21305;&#37197;&#39069;&#22806;&#30340;Transformer&#23618;&#65307;&#36825;&#34920;&#26126;Transformers&#20855;&#26377;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22320;&#34915;&#30417;&#27979;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#22320;&#34915;&#30340;&#29983;&#29289;&#37327;&#21644;&#29366;&#24577;&#65292;&#20174;&#32780;&#26041;&#20415;&#29983;&#24577;&#23398;&#23478;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.17080</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#22320;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#30340;&#23454;&#20363;&#20998;&#21106;&#26469;&#30417;&#27979;&#29983;&#24577;&#30740;&#31350;&#20013;&#30340;&#22320;&#34915;
&lt;/p&gt;
&lt;p&gt;
Automating lichen monitoring in ecological studies using instance segmentation of time-lapse images. (arXiv:2310.17080v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22320;&#34915;&#30417;&#27979;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#22320;&#34915;&#30340;&#29983;&#29289;&#37327;&#21644;&#29366;&#24577;&#65292;&#20174;&#32780;&#26041;&#20415;&#29983;&#24577;&#23398;&#23478;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#34915;&#26159;&#30001;&#30495;&#33740;&#12289;&#34299;&#31867;&#21644;/&#25110;&#34013;&#32454;&#33740;&#32452;&#25104;&#30340;&#20849;&#29983;&#29983;&#29289;&#65292;&#33021;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#32321;&#33635;&#29983;&#38271;&#12290;&#22320;&#34915;&#22312;&#30899;&#21644;&#27694;&#24490;&#29615;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#30452;&#25509;&#25110;&#38388;&#25509;&#22320;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#20570;&#20986;&#36129;&#29486;&#12290;&#29983;&#24577;&#23398;&#23478;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#22320;&#34915;&#20316;&#20026;&#25351;&#31034;&#21058;&#26469;&#35780;&#20272;&#31354;&#27668;&#36136;&#37327;&#21644;&#26646;&#24687;&#22320;&#24773;&#20917;&#26469;&#30417;&#27979;&#22320;&#34915;&#12290;&#29305;&#21035;&#26159;&#29983;&#38271;&#22312;&#26641;&#19978;&#30340;&#38468;&#29983;&#22320;&#34915;&#65292;&#26159;&#35780;&#20272;&#31354;&#27668;&#36136;&#37327;&#21644;&#29615;&#22659;&#20581;&#24247;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#19968;&#31181;&#26032;&#30340;&#30417;&#27979;&#38468;&#29983;&#22320;&#34915;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#30456;&#26426;&#26469;&#25910;&#38598;&#22320;&#34915;&#31181;&#32676;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#30456;&#26426;&#34987;&#32445;&#33452;&#20848;&#21644;&#25289;&#24067;&#25289;&#22810;&#30340;&#29983;&#24577;&#23398;&#23478;&#29992;&#26469;&#38543;&#21518;&#20998;&#26512;&#21644;&#25163;&#21160;&#20998;&#21106;&#22270;&#20687;&#20197;&#30830;&#23450;&#22320;&#34915;&#30340;&#29366;&#24577;&#21644;&#21464;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#32791;&#26102;&#19988;&#23481;&#26131;&#21463;&#21040;&#35266;&#23519;&#32773;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#33258;&#21160;&#21270;&#30417;&#27979;&#22320;&#34915;&#38271;&#36798;&#19968;&#27573;&#26102;&#38388;&#65292;&#24182;&#20272;&#35745;&#23427;&#20204;&#30340;&#29983;&#29289;&#37327;&#21644;&#29366;&#24577;&#65292;&#20197;&#20415;&#20026;&#29983;&#24577;&#23398;&#23478;&#30340;&#20219;&#21153;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lichens are symbiotic organisms composed of fungi, algae, and/or cyanobacteria that thrive in a variety of environments. They play important roles in carbon and nitrogen cycling, and contribute directly and indirectly to biodiversity. Ecologists typically monitor lichens by using them as indicators to assess air quality and habitat conditions. In particular, epiphytic lichens, which live on trees, are key markers of air quality and environmental health. A new method of monitoring epiphytic lichens involves using time-lapse cameras to gather images of lichen populations. These cameras are used by ecologists in Newfoundland and Labrador to subsequently analyze and manually segment the images to determine lichen thalli condition and change. These methods are time-consuming and susceptible to observer bias. In this work, we aim to automate the monitoring of lichens over extended periods and to estimate their biomass and condition to facilitate the task of ecologists. To accomplish this, ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;ConvNet-Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#32423;&#24085;&#37329;&#26862;&#30149;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;ConvNets&#21644;Transformers&#30340;&#20248;&#21183;&#65292;&#20934;&#30830;&#22320;&#26816;&#27979;PD&#24182;&#30830;&#23450;&#20854;&#20005;&#37325;&#31243;&#24230;&#38454;&#27573;&#65292;&#22312;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;PD&#26816;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;</title><link>http://arxiv.org/abs/2310.17078</link><description>&lt;p&gt;
HCT&#65306;&#22522;&#20110;&#28151;&#21512;ConvNet-Transformer&#30340;&#24085;&#37329;&#26862;&#30149;&#27493;&#24577;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HCT: Hybrid Convnet-Transformer for Parkinson's disease detection and severity prediction from gait. (arXiv:2310.17078v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17078
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;ConvNet-Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#32423;&#24085;&#37329;&#26862;&#30149;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;ConvNets&#21644;Transformers&#30340;&#20248;&#21183;&#65292;&#20934;&#30830;&#22320;&#26816;&#27979;PD&#24182;&#30830;&#23450;&#20854;&#20005;&#37325;&#31243;&#24230;&#38454;&#27573;&#65292;&#22312;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;PD&#26816;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28151;&#21512;ConvNet-Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27493;&#24577;&#25968;&#25454;&#20013;&#26816;&#27979;&#21644;&#20998;&#32423;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#30340;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;ConvNet-Transformer&#27169;&#22411;&#39318;&#20808;&#21306;&#20998;&#20581;&#24247;&#21644;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#12290;&#22914;&#26524;&#24739;&#32773;&#26159;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#65292;&#22810;&#31867;&#21035;&#30340;&#28151;&#21512;ConvNet-Transformer&#27169;&#22411;&#30830;&#23450;Hoehn&#21644;Yahr&#65288;H&amp;Y&#65289;&#20998;&#25968;&#65292;&#20197;&#35780;&#20272;PD&#30340;&#20005;&#37325;&#31243;&#24230;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#26550;&#26500;&#20805;&#20998;&#21033;&#29992;&#20102;Convolutional Neural Networks&#65288;ConvNets&#65289;&#21644;Transformers&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;PD&#24182;&#30830;&#23450;&#20005;&#37325;&#31243;&#24230;&#38454;&#27573;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;ConvNets&#26469;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#65292;&#32780;&#21033;&#29992;Transformers&#26469;&#22788;&#29702;&#36755;&#20837;&#20449;&#21495;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#28151;&#21512;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;PD&#26816;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel deep learning method based on a new Hybrid ConvNet-Transformer architecture to detect and stage Parkinson's disease (PD) from gait data. We adopt a two-step approach by dividing the problem into two sub-problems. Our Hybrid ConvNet-Transformer model first distinguishes healthy versus parkinsonian patients. If the patient is parkinsonian, a multi-class Hybrid ConvNet-Transformer model determines the Hoehn and Yahr (H&amp;Y) score to assess the PD severity stage. Our hybrid architecture exploits the strengths of both Convolutional Neural Networks (ConvNets) and Transformers to accurately detect PD and determine the severity stage. In particular, we take advantage of ConvNets to capture local patterns and correlations in the data, while we exploit Transformers for handling long-term dependencies in the input signal. We show that our hybrid method achieves superior performance when compared to other state-of-the-art methods, with a PD detection accuracy of 97%
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#20854;&#26435;&#37325;&#30340;&#25391;&#33633;&#65292;&#33021;&#22815;&#22312;&#29305;&#24449;&#22122;&#22768;&#25968;&#25454;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17074</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#33391;&#24615;&#25391;&#33633;
&lt;/p&gt;
&lt;p&gt;
Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates. (arXiv:2310.17074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17074
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#20854;&#26435;&#37325;&#30340;&#25391;&#33633;&#65292;&#33021;&#22815;&#22312;&#29305;&#24449;&#22122;&#22768;&#25968;&#25454;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#23398;&#20064;&#29575;&#35757;&#32451;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#22312;&#36825;&#31181;&#35757;&#32451;&#26041;&#24335;&#19979;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#65292;&#30001;&#20110;&#22823;&#23398;&#20064;&#29575;SGD&#35757;&#32451;&#24341;&#36215;&#30340;NN&#26435;&#37325;&#30340;&#25391;&#33633;&#23545;NN&#30340;&#27867;&#21270;&#26377;&#30410;&#65292;&#36825;&#26377;&#21487;&#33021;&#20248;&#20110;&#36890;&#36807;&#25910;&#25947;&#36739;&#24179;&#28369;&#30340;&#23567;&#23398;&#20064;&#29575;SGD&#35757;&#32451;&#30340;&#30456;&#21516;NN&#12290;&#22522;&#20110;&#36825;&#20010;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#8220;&#33391;&#24615;&#25391;&#33633;&#8221;&#12290;&#25105;&#20204;&#35299;&#23494;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#24314;&#31435;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#23398;&#20064;&#35282;&#24230;&#19978;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#29305;&#24449;&#22122;&#22768;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21253;&#25324;&#65288;i&#65289;&#20855;&#26377;&#23567;&#30340;$\ell_2$-&#33539;&#25968;&#24182;&#20986;&#29616;&#22312;&#27599;&#20010;&#25968;&#25454;&#28857;&#20013;&#30340;&#24369;&#29305;&#24449;&#65307;&#65288;ii&#65289;&#20855;&#26377;&#36739;&#22823;&#30340;$\ell_2$-&#33539;&#25968;&#20294;&#21482;&#20986;&#29616;&#22312;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#19968;&#37096;&#20998;&#20013;&#30340;&#24378;&#29305;&#24449;&#65307;&#21644;&#65288;iii&#65289;&#22122;&#22768;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#25391;&#33633;&#35757;&#32451;&#30340;NN&#33021;&#22815;&#22312;&#36825;&#20010;&#29305;&#24449;&#22122;&#22768;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#19978;&#23454;&#29616;&#36739;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we theoretically investigate the generalization properties of neural networks (NN) trained by stochastic gradient descent (SGD) algorithm with large learning rates. Under such a training regime, our finding is that, the oscillation of the NN weights caused by the large learning rate SGD training turns out to be beneficial to the generalization of the NN, which potentially improves over the same NN trained by SGD with small learning rates that converges more smoothly. In view of this finding, we call such a phenomenon "benign oscillation". Our theory towards demystifying such a phenomenon builds upon the feature learning perspective of deep learning. Specifically, we consider a feature-noise data generation model that consists of (i) weak features which have a small $\ell_2$-norm and appear in each data point; (ii) strong features which have a larger $\ell_2$-norm but only appear in a certain fraction of all data points; and (iii) noise. We prove that NNs trained by oscill
&lt;/p&gt;</description></item><item><title>Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.</title><link>http://arxiv.org/abs/2310.17072</link><description>&lt;p&gt;
&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17072
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;MMP&#65289;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#19968;&#31995;&#21015;&#36830;&#32493;&#36712;&#36857;&#27969;&#24418;&#65292;&#27599;&#19968;&#20010;&#36712;&#36857;&#27969;&#24418;&#37117;&#33021;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#30001;&#23545;&#27969;&#24418;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#35299;&#30721;&#20989;&#25968;&#20197;&#21450;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#23494;&#24230;&#32452;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#25197;&#26354;&#65292;MMP&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;--&#36890;&#36807;&#21464;&#24418;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#36816;&#21160;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#27861;&#30456;&#37051;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;IMMP&#65289;&#65292;&#20854;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20445;&#25345;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#21644;&#20351;&#29992;&#20102;&#19968;&#20010;Riemannian&#24230;&#37327;&#65292;&#29992;&#20110;&#36816;&#21160;&#31354;&#38388;&#65288;&#21363;&#65292;&#21442;&#25968;&#21270;&#26354;&#32447;&#31354;&#38388;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CurveGeom Riemannian&#24230;&#37327;&#12290;&#23545;&#20110;&#24179;&#38754;&#38556;&#30861;&#36991;&#35753;&#36816;&#21160;&#21644;&#25512;&#21160;&#25805;&#32437;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMMP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MMP&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Gabe-YHLee/IMMP&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#24418;&#24335;&#21270;&#39640;&#32423;&#25968;&#23398;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#21644;&#26816;&#26597;&#30740;&#31350;&#35770;&#25991;&#20013;&#25968;&#23398;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.17064</link><description>&lt;p&gt;
math-PVS:&#19968;&#20010;&#23558;&#31185;&#23398;&#20986;&#29256;&#29289;&#26144;&#23556;&#21040;PVS&#29702;&#35770;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories. (arXiv:2310.17064v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#24418;&#24335;&#21270;&#39640;&#32423;&#25968;&#23398;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#21644;&#26816;&#26597;&#30740;&#31350;&#35770;&#25991;&#20013;&#25968;&#23398;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#23427;&#22312;&#25968;&#23398;&#21457;&#29616;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#24341;&#23548;&#29468;&#24819;&#29983;&#25104;&#65292;&#26500;&#36896;&#21453;&#20363;&#65292;&#21327;&#21161;&#24418;&#24335;&#21270;&#25968;&#23398;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#25968;&#23398;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#31561;&#31561;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;&#35745;&#31639;&#26426;&#36827;&#34892;&#35814;&#23613;&#30340;&#25968;&#23398;&#35777;&#26126;&#25628;&#32034;&#65292;&#20294;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#21162;&#21147;&#33268;&#21147;&#20110;&#23558;&#35745;&#31639;&#24179;&#21488;&#23450;&#20301;&#20026;&#25968;&#23398;&#30740;&#31350;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#36129;&#29486;&#32773;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#21644;&#25968;&#23398;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#23558;&#23450;&#29702;&#35777;&#26126;&#31995;&#32479;&#19982;&#22522;&#30784;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLMs&#22312;&#24418;&#24335;&#21270;&#39640;&#32423;&#25968;&#23398;&#27010;&#24565;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#21644;&#26816;&#26597;&#30740;&#31350;&#35770;&#25991;&#20013;&#25968;&#23398;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.  While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#65288;DTW&#65289;&#32858;&#31867;&#21644;k&#22343;&#20540;&#32858;&#31867;&#26041;&#27861;&#65292;&#38024;&#23545;&#24503;&#20811;&#33832;&#26031;&#24030;&#30340;&#30005;&#21160;&#36710;&#20805;&#30005;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#25972;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#25112;&#30053;&#20915;&#31574;&#30340;&#22797;&#26434;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#21487;&#25345;&#32493;&#21644;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#30340;&#33021;&#28304;&#26410;&#26469;&#30340;&#25972;&#21512;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.17056</link><description>&lt;p&gt;
&#24503;&#20811;&#33832;&#26031;&#24030;&#30340;&#30005;&#21160;&#36710;&#20805;&#30005;&#31574;&#30053;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Strategizing EV Charging and Renewable Integration in Texas. (arXiv:2310.17056v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#65288;DTW&#65289;&#32858;&#31867;&#21644;k&#22343;&#20540;&#32858;&#31867;&#26041;&#27861;&#65292;&#38024;&#23545;&#24503;&#20811;&#33832;&#26031;&#24030;&#30340;&#30005;&#21160;&#36710;&#20805;&#30005;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#25972;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#25112;&#30053;&#20915;&#31574;&#30340;&#22797;&#26434;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#21487;&#25345;&#32493;&#21644;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#30340;&#33021;&#28304;&#26410;&#26469;&#30340;&#25972;&#21512;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30005;&#21160;&#36710;&#65288;EV&#65289;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#26234;&#33021;&#30005;&#32593;&#25216;&#26415;&#22312;&#24503;&#20811;&#33832;&#26031;&#24030;&#30340;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;EV&#24191;&#27867;&#37319;&#29992;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#20851;&#27880;&#30005;&#32593;&#31283;&#23450;&#24615;&#38382;&#39064;&#12289;&#19981;&#21327;&#35843;&#30340;&#20805;&#30005;&#27169;&#24335;&#20197;&#21450;EV&#19982;&#21487;&#20877;&#29983;&#33021;&#28304;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#65288;DTW&#65289;&#32858;&#31867;&#21644;k&#22343;&#20540;&#32858;&#31867;&#26041;&#27861;&#65292;&#26681;&#25454;&#24635;&#36127;&#33655;&#21644;&#20928;&#36127;&#33655;&#23558;&#27599;&#22825;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#26085;&#24120;&#30005;&#21147;&#28040;&#32791;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#29983;&#25104;&#27169;&#24335;&#30340;&#32454;&#33268;&#27934;&#23519;&#12290;&#36890;&#36807;&#24314;&#31435;&#38024;&#23545;&#29305;&#23450;&#36127;&#33655;&#29305;&#24449;&#30340;&#26368;&#20339;&#20805;&#30005;&#21644;&#36710;&#36742;&#23545;&#30005;&#32593;&#65288;V2G&#65289;&#26102;&#38388;&#31383;&#21475;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#33021;&#28304;&#28040;&#32791;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#25972;&#21512;&#26041;&#38754;&#36827;&#34892;&#25112;&#30053;&#20915;&#31574;&#30340;&#22797;&#26434;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#23454;&#29616;&#21487;&#25345;&#32493;&#21644;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#30340;&#33021;&#28304;&#26410;&#26469;&#30340;&#39034;&#21033;&#25972;&#21512;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the convergence of electric vehicles (EVs), renewable energy, and smart grid technologies in the context of Texas, this study addresses challenges hindering the widespread adoption of EVs. Acknowledging their environmental benefits, the research focuses on grid stability concerns, uncoordinated charging patterns, and the complicated relationship between EVs and renewable energy sources. Dynamic time warping (DTW) clustering and k-means clustering methodologies categorize days based on total load and net load, offering nuanced insights into daily electricity consumption and renewable energy generation patterns. By establishing optimal charging and vehicle-to-grid (V2G) windows tailored to specific load characteristics, the study provides a sophisticated methodology for strategic decision-making in energy consumption and renewable integration. The findings contribute to the ongoing discourse on achieving a sustainable and resilient energy future through the seamless integration
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#40657;&#30418;&#25511;&#21046;&#26469;&#24341;&#23548;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#29983;&#25104;&#26356;&#21152;&#24120;&#35782;&#24615;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2310.17054</link><description>&lt;p&gt;
&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#24120;&#35782;&#33021;&#21147;&#65306;&#21033;&#29992;&#40657;&#30418;&#25511;&#21046;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation. (arXiv:2310.17054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#40657;&#30418;&#25511;&#21046;&#26469;&#24341;&#23548;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#29983;&#25104;&#26356;&#21152;&#24120;&#35782;&#24615;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-3&#24050;&#32463;&#23637;&#31034;&#20102;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23427;&#20204;&#30340;&#25104;&#21151;&#20043;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#23427;&#20204;&#29983;&#25104;&#30340;&#36755;&#20986;&#26377;&#26102;&#20173;&#28982;&#32570;&#20047;&#24120;&#35782;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#19981;&#21487;&#34892;&#30340;&#35805;&#65292;&#23558;&#25972;&#20010;LLM&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#26356;&#21152;&#24120;&#35782;&#24615;&#30340;&#36755;&#20986;&#26159;&#35745;&#31639;&#19978;&#20195;&#20215;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#24341;&#23548;&#21521;&#26356;&#21152;&#24120;&#35782;&#24615;&#30340;&#29983;&#25104;&#65288;&#21363;&#20197;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#20135;&#29983;&#21253;&#21547;&#19968;&#31995;&#21015;&#27010;&#24565;&#30340;&#21512;&#29702;&#36755;&#20986;&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#30340;&#35780;&#20272;&#22120;&#65292;&#36890;&#36807;&#23558;&#21477;&#23376;&#19982;&#19968;&#20010;&#21160;&#24577;&#24120;&#35782;&#30693;&#35782;&#24211;&#22312;&#22235;&#20010;&#19981;&#21516;&#20851;&#31995;&#26041;&#38754;&#30456;&#36830;&#26469;&#20026;&#21477;&#23376;&#20998;&#37197;&#19968;&#20010;&#24120;&#35782;&#24471;&#20998;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35780;&#20998;&#22120;&#20316;&#20026;&#24120;&#35782;&#30693;&#35782;&#30340;&#21442;&#32771;&#65292;&#25193;&#23637;&#20102;&#21517;&#20026;NADO&#30340;&#21487;&#25511;&#29983;&#25104;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#36741;&#21161;&#22836;&#37096;&#26469;&#24341;&#23548;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#36873;&#25321;&#26410;&#26631;&#35760;&#23454;&#20363;&#36827;&#34892;&#25968;&#25454;&#33719;&#21462;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#22810;&#20219;&#21153;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#39044;&#27979;&#19981;&#21516;&#35757;&#32451;&#38598;&#30340;&#30456;&#23545;&#25928;&#29992;&#24182;&#30830;&#20445;&#27867;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17044</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#21452;&#23618;&#20248;&#21270;&#23398;&#20064;&#20027;&#21160;&#23398;&#20064;&#30340;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank for Active Learning via Multi-Task Bilevel Optimization. (arXiv:2310.17044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#36873;&#25321;&#26410;&#26631;&#35760;&#23454;&#20363;&#36827;&#34892;&#25968;&#25454;&#33719;&#21462;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#22810;&#20219;&#21153;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#39044;&#27979;&#19981;&#21516;&#35757;&#32451;&#38598;&#30340;&#30456;&#23545;&#25928;&#29992;&#24182;&#30830;&#20445;&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#35831;&#27714;&#26631;&#31614;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20197;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#25910;&#36141;&#20989;&#25968;&#35745;&#31639;&#12289;&#22823;&#37327;&#30340;&#24314;&#27169;&#37325;&#35757;&#32451;&#20197;&#21450;&#19982;&#26631;&#35760;&#32773;&#30340;&#22810;&#36718;&#20114;&#21160;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#36873;&#25321;&#25209;&#27425;&#26410;&#26631;&#35760;&#23454;&#20363;&#36827;&#34892;&#25968;&#25454;&#33719;&#21462;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#20010;&#33391;&#22909;&#27867;&#21270;&#30340;&#25910;&#36141;&#20989;&#25968;&#65292;&#22240;&#20026;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;&#20316;&#20026;&#25928;&#29992;&#20989;&#25968;&#36755;&#20837;&#30340;&#21382;&#21490;&#25968;&#25454;&#19981;&#26029;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36129;&#29486;&#26159;&#19968;&#20010;&#21452;&#23618;&#22810;&#20219;&#21153;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#39044;&#27979;&#19981;&#21516;&#35757;&#32451;&#38598;&#30340;&#30456;&#23545;&#25928;&#29992;&#65292;&#20197;&#39564;&#35777;&#20934;&#30830;&#24615;&#20026;&#24230;&#37327;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#30340;&#25910;&#36141;&#20989;&#25968;&#26377;&#25928;&#27867;&#21270;&#12290;&#23545;&#20110;&#39564;&#35777;&#20934;&#30830;&#24615;&#26114;&#36149;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a promising paradigm to reduce the labeling cost by strategically requesting labels to improve model performance. However, existing active learning methods often rely on expensive acquisition function to compute, extensive modeling retraining and multiple rounds of interaction with annotators. To address these limitations, we propose a novel approach for active learning, which aims to select batches of unlabeled instances through a learned surrogate model for data acquisition. A key challenge in this approach is developing an acquisition function that generalizes well, as the history of data, which forms part of the utility function's input, grows over time. Our novel algorithmic contribution is a bilevel multi-task bilevel optimization framework that predicts the relative utility -- measured by the validation accuracy -- of different training sets, and ensures the learned acquisition function generalizes effectively. For cases where validation accuracy is expensive 
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#21644;&#32463;&#20856;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#22312;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;QLSTM&#20855;&#26377;&#21152;&#24555;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#27979;&#35797;&#25439;&#22833;&#30340;&#20248;&#21183;&#65292;&#25317;&#26377;&#21560;&#32435;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.17032</link><description>&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#19982;&#32463;&#20856;LSTM&#30340;&#27604;&#36739;&#30740;&#31350;&#65306;&#20197;&#22826;&#38451;&#33021;&#39044;&#27979;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting. (arXiv:2310.17032v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#21644;&#32463;&#20856;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#22312;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;QLSTM&#20855;&#26377;&#21152;&#24555;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#27979;&#35797;&#25439;&#22833;&#30340;&#20248;&#21183;&#65292;&#25317;&#26377;&#21560;&#32435;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#21521;&#21487;&#25345;&#32493;&#33021;&#28304;&#31995;&#32479;&#21457;&#23637;&#30340;&#36807;&#31243;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#21644;&#32463;&#20856;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#22312;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#20180;&#32454;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;QLSTM&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21253;&#25324;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21152;&#24555;&#21644;&#22312;&#21021;&#22987;&#38454;&#27573;&#26126;&#26174;&#38477;&#20302;&#30340;&#27979;&#35797;&#25439;&#22833;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#32463;&#20856;LSTM&#27169;&#22411;&#12290;&#36825;&#20123;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;QLSTM&#26377;&#28508;&#21147;&#24555;&#36895;&#21560;&#32435;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#20851;&#31995;&#65292;&#36825;&#24471;&#30410;&#20110;&#37327;&#23376;&#29616;&#35937;&#65288;&#22914;&#21472;&#21152;&#65289;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;QLSTM&#30340;&#20840;&#37096;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#27169;&#22411;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#39564;&#35777;&#12289;&#31995;&#32479;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#30828;&#20214;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#30456;&#20851;&#21487;&#20877;&#29983;&#33021;&#28304;&#39044;&#27979;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#19981;&#26029;&#30340;&#36827;&#23637;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22312;&#39044;&#27979;&#21644;&#20248;&#21270;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#24102;&#26469;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately forecasting solar power generation is crucial in the global progression towards sustainable energy systems. In this study, we conduct a meticulous comparison between Quantum Long Short-Term Memory (QLSTM) and classical Long Short-Term Memory (LSTM) models for solar power production forecasting. Our controlled experiments reveal promising advantages of QLSTMs, including accelerated training convergence and substantially reduced test loss within the initial epoch compared to classical LSTMs. These empirical findings demonstrate QLSTM's potential to swiftly assimilate complex time series relationships, enabled by quantum phenomena like superposition. However, realizing QLSTM's full capabilities necessitates further research into model validation across diverse conditions, systematic hyperparameter optimization, hardware noise resilience, and applications to correlated renewable forecasting problems. With continued progress, quantum machine learning can offer a paradigm shift in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#23545;&#20110;&#21333;&#36755;&#20986;&#24773;&#20917;&#65292;&#25105;&#20204;&#21457;&#29616;Matern&#26680;&#28151;&#21512;&#30340;&#24179;&#28369;&#24615;&#30001;&#26368;&#19981;&#24179;&#28369;&#30340;&#32452;&#20214;&#20915;&#23450;&#65292;&#24182;&#19988;&#28151;&#21512;&#26680;&#31561;&#20215;&#20110;&#26368;&#19981;&#24179;&#28369;&#30340;&#26680;&#32452;&#20214;&#12290;&#22312;&#22810;&#36755;&#20986;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#34920;&#26126;&#20056;&#27861;&#28151;&#21512;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.17023</link><description>&lt;p&gt;
&#35770;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability and Interpretability of Gaussian Process Models. (arXiv:2310.17023v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#23545;&#20110;&#21333;&#36755;&#20986;&#24773;&#20917;&#65292;&#25105;&#20204;&#21457;&#29616;Matern&#26680;&#28151;&#21512;&#30340;&#24179;&#28369;&#24615;&#30001;&#26368;&#19981;&#24179;&#28369;&#30340;&#32452;&#20214;&#20915;&#23450;&#65292;&#24182;&#19988;&#28151;&#21512;&#26680;&#31561;&#20215;&#20110;&#26368;&#19981;&#24179;&#28369;&#30340;&#26680;&#32452;&#20214;&#12290;&#22312;&#22810;&#36755;&#20986;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#34920;&#26126;&#20056;&#27861;&#28151;&#21512;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#21152;&#24615;Matern&#26680;&#22312;&#21333;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#20570;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#24182;&#25506;&#35752;&#20102;&#29992;&#20110;&#22810;&#36755;&#20986;GP&#27169;&#22411;&#30340;&#20056;&#27861;Matern&#26680;&#30340;&#24615;&#36136;&#12290;&#23545;&#20110;&#21333;&#36755;&#20986;&#24773;&#20917;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#29702;&#35770;&#32467;&#26524;&#65292;&#34920;&#26126;Matern&#26680;&#28151;&#21512;&#30340;&#24179;&#28369;&#24615;&#30001;&#26368;&#19981;&#24179;&#28369;&#30340;&#32452;&#20214;&#20915;&#23450;&#65292;&#24182;&#19988;&#20855;&#26377;&#36825;&#26679;&#26680;&#30340;GP&#23454;&#38469;&#19978;&#31561;&#20215;&#20110;&#26368;&#19981;&#24179;&#28369;&#30340;&#26680;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21508;&#20010;&#26680;&#32452;&#20214;&#20013;&#30340;&#28151;&#21512;&#26435;&#37325;&#25110;&#21442;&#25968;&#22343;&#26080;&#27861;&#35782;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#22810;&#36755;&#20986;GP&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#20102;&#20056;&#27861;&#26680;$K(x,y) = AK_0(x,y)$&#20013;&#21327;&#26041;&#24046;&#30697;&#38453;$A$&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#20854;&#20013;$K_0$&#26159;&#26631;&#20934;&#30340;&#21333;&#36755;&#20986;&#26680;&#65292;&#22914;Matern&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;$A$&#22312;&#20056;&#27861;&#24120;&#25968;&#19978;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#36825;&#34920;&#26126;&#20056;&#27861;&#28151;&#21512;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we critically examine the prevalent practice of using additive mixtures of Mat\'ern kernels in single-output Gaussian process (GP) models and explore the properties of multiplicative mixtures of Mat\'ern kernels for multi-output GP models. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Mat\'ern kernels is determined by the least smooth component and that a GP with such a kernel is effectively equivalent to the least smooth kernel component. Furthermore, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable. We then turn our attention to multi-output GP models and analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Mat\'ern. We show that $A$ is identifiable up to a multiplicative constant, suggesting that multiplicative mixtures are well 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17022</link><description>&lt;p&gt;
&#21463;&#25511;&#35299;&#30721;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#29992;&#20110;&#25511;&#21046;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#20540;&#20989;&#25968;&#34987;&#31216;&#20026;&#21069;&#32512;&#35780;&#20998;&#22120;&#12290;&#21069;&#32512;&#35780;&#20998;&#22120;&#22312;&#25512;&#29702;&#26102;&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#21521;&#26356;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32512;&#35780;&#20998;&#22120;&#21487;&#20197;&#20174;&#65288;&#21487;&#33021;&#26159;&#65289;&#31163;&#31574;&#30053;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#20174;&#37096;&#20998;&#35299;&#30721;&#30340;&#21709;&#24212;&#32487;&#32493;&#35299;&#30721;&#26102;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;Reddit&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#32463;&#39564;&#35777;&#26126;&#65292;CD&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CD&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CD&#21487;&#20197;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#22359;&#26041;&#24335;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#21516;&#26679;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#22240;&#23376;&#36712;&#36857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24352;&#37327;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#28789;&#27963;&#20272;&#35745;&#22240;&#23376;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#28388;&#27874;&#31639;&#27861;&#26469;&#20272;&#35745;&#22240;&#23376;&#29366;&#24577;&#30340;&#28436;&#21464;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2310.17021</link><description>&lt;p&gt;
&#27969;&#24335;&#22240;&#23376;&#36712;&#36857;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24352;&#37327;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Streaming Factor Trajectory Learning for Temporal Tensor Decomposition. (arXiv:2310.17021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17021
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#22240;&#23376;&#36712;&#36857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24352;&#37327;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#28789;&#27963;&#20272;&#35745;&#22240;&#23376;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#28388;&#27874;&#31639;&#27861;&#26469;&#20272;&#35745;&#22240;&#23376;&#29366;&#24577;&#30340;&#28436;&#21464;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#24352;&#37327;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26102;&#38388;&#20449;&#24687;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26102;&#38388;&#20998;&#35299;&#26041;&#27861;&#20272;&#35745;&#27599;&#20010;&#24352;&#37327;&#27169;&#24335;&#20013;&#23545;&#35937;&#30340;&#19968;&#32452;&#22266;&#23450;&#22240;&#23376;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#23545;&#35937;&#34920;&#31034;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#32570;&#20047;&#20174;&#27969;&#25968;&#25454;&#20013;&#25429;&#25417;&#36825;&#31181;&#28436;&#21464;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26102;&#38388;&#24352;&#37327;&#20998;&#35299;&#30340;&#27969;&#24335;&#22240;&#23376;&#36712;&#36857;&#23398;&#20064;&#65288;SFTL&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#24314;&#27169;&#22240;&#23376;&#30340;&#36712;&#36857;&#65292;&#20174;&#32780;&#28789;&#27963;&#22320;&#20272;&#35745;&#23427;&#20204;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#22788;&#29702;&#27969;&#25968;&#25454;&#26102;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#31561;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#23558;GPs&#36716;&#25442;&#20026;&#29366;&#24577;&#31354;&#38388;&#20808;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#28388;&#27874;&#31639;&#27861;&#65292;&#22312;&#25509;&#25910;&#21040;&#26032;&#25968;&#25454;&#26102;&#20272;&#35745;&#28041;&#21450;&#30340;&#22240;&#23376;&#29366;&#24577;&#30340;&#20998;&#35299;&#24335;&#36816;&#34892;&#21518;&#39564;&#12290;&#20998;&#35299;&#20272;&#35745;&#20351;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#22240;&#23376;&#30340;&#28436;&#21464;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical tensor data is often along with time information. Most existing temporal decomposition approaches estimate a set of fixed factors for the objects in each tensor mode, and hence cannot capture the temporal evolution of the objects' representation. More important, we lack an effective approach to capture such evolution from streaming data, which is common in real-world applications. To address these issues, we propose Streaming Factor Trajectory Learning (SFTL) for temporal tensor decomposition. We use Gaussian processes (GPs) to model the trajectory of factors so as to flexibly estimate their temporal evolution. To address the computational challenges in handling streaming data, we convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE). We develop an efficient online filtering algorithm to estimate a decoupled running posterior of the involved factor states upon receiving new data. The decoupled estimation enables us to co
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;Meta-World&#22522;&#20934;&#65292;&#31216;&#20026;&#8220;&#35821;&#35328;&#19990;&#30028;&#8221;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21644;&#33050;&#26412;&#25216;&#33021;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#35745;&#21010;&#26465;&#20214;&#34892;&#20026;&#20811;&#38534;&#65288;PCBC&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#28436;&#31034;&#23545;&#39640;&#32423;&#35745;&#21010;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;PCBC&#22312;&#35821;&#35328;&#19990;&#30028;&#20013;&#33021;&#22815;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17019</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#26465;&#20214;&#22320;&#32452;&#21512;&#26426;&#22120;&#20154;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Conditionally Combining Robot Skills using Large Language Models. (arXiv:2310.17019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;Meta-World&#22522;&#20934;&#65292;&#31216;&#20026;&#8220;&#35821;&#35328;&#19990;&#30028;&#8221;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21644;&#33050;&#26412;&#25216;&#33021;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#35745;&#21010;&#26465;&#20214;&#34892;&#20026;&#20811;&#38534;&#65288;PCBC&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#28436;&#31034;&#23545;&#39640;&#32423;&#35745;&#21010;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;PCBC&#22312;&#35821;&#35328;&#19990;&#30028;&#20013;&#33021;&#22815;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32467;&#21512;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Meta-World&#22522;&#20934;&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#31216;&#20026;&#8220;&#35821;&#35328;&#19990;&#30028;&#8221;&#65292;&#23427;&#20801;&#35768;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20010;&#27169;&#25311;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#20351;&#29992;&#21322;&#32467;&#26500;&#21270;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21644;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33050;&#26412;&#25216;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#19982;Meta-World&#30456;&#21516;&#30340;&#20219;&#21153;&#38598;&#65292;&#21487;&#20197;&#36731;&#26494;&#27604;&#36739;&#35821;&#35328;&#19990;&#30028;&#21644;Meta-World&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23545;&#26368;&#36817;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#35745;&#21010;&#26465;&#20214;&#34892;&#20026;&#20811;&#38534;&#65288;PCBC&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#39640;&#32423;&#35745;&#21010;&#30340;&#34892;&#20026;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#28436;&#31034;&#12290;&#20351;&#29992;&#35821;&#35328;&#19990;&#30028;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PCBC&#33021;&#22815;&#22312;&#21508;&#31181;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#21462;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#21482;&#38656;&#19968;&#20010;&#28436;&#31034;&#21363;&#21487;&#23454;&#29616;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#19990;&#30028;&#20316;&#20026;&#24320;&#28304;&#36719;&#20214;&#25552;&#20379;&#65292;&#32593;&#22336;&#26159;https://...
&lt;/p&gt;
&lt;p&gt;
This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call "Language-World," which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#24067;&#33713;&#20811;&#38886;&#23572;&#21487;&#25509;&#36817;&#24615;&#23450;&#29702;&#23558;&#22312;&#32447;&#39044;&#27979;&#27169;&#22411;&#36716;&#21270;&#20026;&#26657;&#20934;&#39044;&#27979;&#30340;&#25216;&#26415;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#24050;&#26377;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#36895;&#24230;&#12289;&#28789;&#27963;&#30340;&#26657;&#20934;&#35823;&#24046;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.17002</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25509;&#36817;&#24615;&#26356;&#24555;&#22320;&#37325;&#26032;&#26657;&#20934;&#22312;&#32447;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Faster Recalibration of an Online Predictor via Approachability. (arXiv:2310.17002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#24067;&#33713;&#20811;&#38886;&#23572;&#21487;&#25509;&#36817;&#24615;&#23450;&#29702;&#23558;&#22312;&#32447;&#39044;&#27979;&#27169;&#22411;&#36716;&#21270;&#20026;&#26657;&#20934;&#39044;&#27979;&#30340;&#25216;&#26415;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#24050;&#26377;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#36895;&#24230;&#12289;&#28789;&#27963;&#30340;&#26657;&#20934;&#35823;&#24046;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#21487;&#20449;&#21644;&#21487;&#38752;&#30340;&#29305;&#24615;&#65292;&#36825;&#36890;&#24120;&#33267;&#23569;&#24847;&#21619;&#30528;&#36755;&#20986;&#26657;&#20934;&#30340;&#27010;&#29575;&#12290;&#22312;&#32447;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#24403;&#32467;&#26524;&#24207;&#21015;&#21487;&#20197;&#30001;&#23545;&#25163;&#23545;&#25239;&#22320;&#29983;&#25104;&#26102;&#65292;&#20445;&#35777;&#26657;&#20934;&#24615;&#21487;&#33021;&#29305;&#21035;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24067;&#33713;&#20811;&#38886;&#23572;&#21487;&#25509;&#36817;&#24615;&#23450;&#29702;&#30340;&#25216;&#26415;&#65292;&#23558;&#21487;&#33021;&#19981;&#26657;&#20934;&#30340;&#22312;&#32447;&#39044;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#36716;&#21270;&#20026;&#26657;&#20934;&#30340;&#39044;&#27979;&#65292;&#32780;&#21407;&#27169;&#22411;&#30340;&#25439;&#22833;&#19981;&#20250;&#22823;&#24133;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22312;&#32447;&#35774;&#32622;&#20013;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#23454;&#29616;&#20102;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#30456;&#27604;&#24050;&#26377;&#30340;&#25216;&#26415;(arXiv:1607.03594)&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#22312;&#32447;&#35774;&#32622;&#20013;&#25552;&#20379;&#26657;&#20934;&#35823;&#24046;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#28789;&#27963;&#26435;&#34913;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#25551;&#36848;&#20986;&#21487;&#20849;&#21516;&#23454;&#29616;&#30340;&#26657;&#20934;&#21644;&#36951;&#25022;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive models in ML need to be trustworthy and reliable, which often at the very least means outputting calibrated probabilities. This can be particularly difficult to guarantee in the online prediction setting when the outcome sequence can be generated adversarially. In this paper we introduce a technique using Blackwell's approachability theorem for taking an online predictive model which might not be calibrated and transforming its predictions to calibrated predictions without much increase to the loss of the original model. Our proposed algorithm achieves calibration and accuracy at a faster rate than existing techniques arXiv:1607.03594 and is the first algorithm to offer a flexible tradeoff between calibration error and accuracy in the online setting. We demonstrate this by characterizing the space of jointly achievable calibration and regret using our technique.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#20449;&#20219;&#65292;&#20294;&#35201;&#39564;&#35777;&#8221;&#26041;&#27861;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#39564;&#35777;&#32593;&#32476;&#23545;&#20998;&#21106;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#27492;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#22810;&#31181;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#24212;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#26469;&#26816;&#27979;&#38169;&#35823;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2310.16999</link><description>&lt;p&gt;
&#20449;&#20219;&#65292;&#20294;&#35201;&#39564;&#35777;&#65306;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#40065;&#26834;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Trust, but Verify: Robust Image Segmentation using Deep Learning. (arXiv:2310.16999v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#20449;&#20219;&#65292;&#20294;&#35201;&#39564;&#35777;&#8221;&#26041;&#27861;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#39564;&#35777;&#32593;&#32476;&#23545;&#20998;&#21106;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#27492;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#22810;&#31181;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#24212;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#26469;&#26816;&#27979;&#38169;&#35823;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#36755;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#22810;&#31181;&#31867;&#22411;&#30340;&#38543;&#26426;&#21644;&#26368;&#22351;&#24773;&#20917;&#30340;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20316;&#32773;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#31216;&#20026;&#8220;&#20449;&#20219;&#65292;&#20294;&#35201;&#39564;&#35777;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20854;&#20013;&#36741;&#21161;&#39564;&#35777;&#32593;&#32476;&#20351;&#29992;&#20998;&#21106;&#20316;&#20026;&#36755;&#20837;&#26469;&#23545;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#26576;&#20123;&#34987;&#36974;&#34109;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#35774;&#35745;&#33391;&#22909;&#30340;&#36741;&#21161;&#32593;&#32476;&#23558;&#22312;&#36755;&#20837;&#20998;&#21106;&#20934;&#30830;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#39044;&#27979;&#65292;&#20294;&#22312;&#20998;&#21106;&#19981;&#27491;&#30830;&#26102;&#29983;&#25104;&#20302;&#36136;&#37327;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;&#36825;&#20010;&#32593;&#32476;&#30340;&#39044;&#27979;&#19982;&#21407;&#22987;&#22270;&#20687;&#36827;&#34892;&#26816;&#26597;&#65292;&#25105;&#20204;&#21487;&#20197;&#26816;&#27979;&#20986;&#38169;&#35823;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#39564;&#35777;&#26041;&#27861;&#30495;&#27491;&#40065;&#26834;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#26816;&#26597;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#29992;&#20110;&#20998;&#21106;&#35780;&#20272;&#30340;&#26041;&#27861;&#26080;&#27861;&#24212;&#23545;&#40065;&#26834;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a method for verifying the output of a deep neural network for medical image segmentation that is robust to several classes of random as well as worst-case perturbations i.e. adversarial attacks. This method is based on a general approach recently developed by the authors called ``Trust, but Verify" wherein an auxiliary verification network produces predictions about certain masked features in the input image using the segmentation as an input. A well-designed auxiliary network will produce high-quality predictions when the input segmentations are accurate, but will produce low-quality predictions when the segmentations are incorrect. Checking the predictions of such a network with the original image allows us to detect bad segmentations. However, to ensure the verification method is truly robust, we need a method for checking the quality of the predictions that does not itself rely on a black-box neural network. Indeed, we show that previous methods for segmentation evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#20998;&#24067;&#30340;&#28418;&#31227;&#65292;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#20339;&#27169;&#22411;&#22312;&#23398;&#20064;&#31995;&#32479;&#21464;&#21270;&#24341;&#36215;&#30340;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#30456;&#23545;&#20110;&#31616;&#21333;&#26041;&#27861;&#65292;&#20840;&#38754;&#25968;&#25454;&#24207;&#21015;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.16996</link><description>&lt;p&gt;
&#36808;&#21521;&#25345;&#32493;&#23398;&#20064;&#24212;&#29992;&#24615;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Continually Learning Application Performance Models. (arXiv:2310.16996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#20998;&#24067;&#30340;&#28418;&#31227;&#65292;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#20339;&#27169;&#22411;&#22312;&#23398;&#20064;&#31995;&#32479;&#21464;&#21270;&#24341;&#36215;&#30340;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#30456;&#23545;&#20110;&#31616;&#21333;&#26041;&#27861;&#65292;&#20840;&#38754;&#25968;&#25454;&#24207;&#21015;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#26500;&#24314;&#20851;&#38190;&#30340;&#20316;&#19994;&#35843;&#24230;&#21644;&#24212;&#29992;&#31243;&#24207;&#20248;&#21270;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#25968;&#25454;&#20998;&#24067;&#19981;&#20250;&#21457;&#29983;&#25913;&#21464;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29983;&#20135;HPC&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#24322;&#26500;&#24615;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#30828;&#20214;&#36864;&#21270;&#12289;&#26356;&#25442;&#21644;/&#25110;&#36719;&#20214;&#34917;&#19969;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#65292;&#20174;&#32780;&#23545;&#24615;&#33021;&#27169;&#22411;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20998;&#24067;&#28418;&#31227;&#65292;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#33021;&#22815;&#22312;&#23398;&#20064;&#31995;&#32479;&#21464;&#21270;&#24341;&#36215;&#30340;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#30456;&#23545;&#20110;&#31616;&#21333;&#26041;&#27861;&#65292;&#25972;&#20010;&#25968;&#25454;&#24207;&#21015;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based performance models are increasingly being used to build critical job scheduling and application optimization decisions. Traditionally, these models assume that data distribution does not change as more samples are collected over time. However, owing to the complexity and heterogeneity of production HPC systems, they are susceptible to hardware degradation, replacement, and/or software patches, which can lead to drift in the data distribution that can adversely affect the performance models. To this end, we develop continually learning performance models that account for the distribution drift, alleviate catastrophic forgetting, and improve generalizability. Our best model was able to retain accuracy, regardless of having to learn the new distribution of data inflicted by system changes, while demonstrating a 2x improvement in the prediction accuracy of the whole data sequence in comparison to the naive approach.
&lt;/p&gt;</description></item><item><title>STEER&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;&#30340;&#35821;&#20041;&#36716;&#21521;&#25193;&#23637;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#36827;&#34892;&#36716;&#21521;&#24847;&#22270;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16990</link><description>&lt;p&gt;
STEER: &#35821;&#20041;&#36716;&#21521;&#25193;&#23637;&#35782;&#21035;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants. (arXiv:2310.16990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16990
&lt;/p&gt;
&lt;p&gt;
STEER&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;&#30340;&#35821;&#20041;&#36716;&#21521;&#25193;&#23637;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#36827;&#34892;&#36716;&#21521;&#24847;&#22270;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#21161;&#25163;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#65292;&#36716;&#21521;&#26159;&#25351;&#29992;&#25143;&#21457;&#20986;&#21518;&#32493;&#21629;&#20196;&#65292;&#35797;&#22270;&#24341;&#23548;&#25110;&#28548;&#28165;&#20043;&#21069;&#30340;&#25351;&#20196;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STEER&#65292;&#19968;&#20010;&#36716;&#21521;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21518;&#32493;&#21629;&#20196;&#26159;&#21542;&#26159;&#29992;&#25143;&#20225;&#22270;&#36716;&#21521;&#20043;&#21069;&#25351;&#20196;&#30340;&#23581;&#35797;&#12290;&#30001;&#20110;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#26500;&#24314;&#29992;&#20110;&#36716;&#21521;&#26696;&#20363;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#37319;&#26679;&#36873;&#25321;&#21152;&#20837;&#20351;&#29992;&#25968;&#25454;&#65292;&#36817;&#20284;&#27491;&#36127;&#26679;&#26412;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#35782;&#21035;&#36716;&#21521;&#24847;&#22270;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#25105;&#20204;&#37319;&#26679;&#30340;&#25968;&#25454;&#19978;&#36229;&#36807;95%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;STEER&#32467;&#21512;&#25105;&#20204;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#22312;&#20154;&#24037;&#35780;&#20272;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#19982;&#30495;&#23454;&#30340;&#36716;&#21521;&#22330;&#26223;&#30456;&#21305;&#37197;&#12290;&#38500;&#20102;&#20165;&#20381;&#36182;&#29992;&#25143;&#30340;&#36716;&#24405;&#20316;&#20026;&#36755;&#20837;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;STEER+&#65292;&#36825;&#26159;&#27169;&#22411;&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user's attempt to steer the previous command. Constructing a training dataset for steering use cases poses challenges due to the cold-start problem. To overcome this, we developed heuristic rules to sample opt-in usage data, approximating positive and negative samples without any annotation. Our experimental results show promising performance in identifying steering intent, with over 95% accuracy on our sampled data. Moreover, STEER, in conjunction with our sampling strategy, aligns effectively with real-world steering scenarios, as evidenced by its strong zero-shot performance on a human-graded evaluation set. In addition to relying solely on user transcripts as input, we introduce STEER+, an enhanced version of the model. STEER+ 
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;&#31215;&#20998;&#30005;&#36335;&#65288;PICs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22270;&#35821;&#35328;&#65292;&#36890;&#36807;&#20351;&#29992;&#31215;&#20998;&#21333;&#20803;&#25193;&#23637;&#20102;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#36830;&#32493;&#28508;&#21464;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#36861;&#36394;&#30340;&#25512;&#29702;&#21644;&#36817;&#20284;&#31934;&#24230;&#21487;&#35843;&#30340;&#23618;&#27425;&#36830;&#32493;&#28151;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2310.16986</link><description>&lt;p&gt;
&#27010;&#29575;&#31215;&#20998;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Integral Circuits. (arXiv:2310.16986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16986
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#31215;&#20998;&#30005;&#36335;&#65288;PICs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22270;&#35821;&#35328;&#65292;&#36890;&#36807;&#20351;&#29992;&#31215;&#20998;&#21333;&#20803;&#25193;&#23637;&#20102;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#36830;&#32493;&#28508;&#21464;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#36861;&#36394;&#30340;&#25512;&#29702;&#21644;&#36817;&#20284;&#31934;&#24230;&#21487;&#35843;&#30340;&#23618;&#27425;&#36830;&#32493;&#28151;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#28508;&#21464;&#37327;&#65288;LVs&#65289;&#26159;&#35768;&#22810;&#29983;&#25104;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#24314;&#27169;&#20855;&#26377;&#19981;&#21487;&#25968;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#34920;&#36798;&#24615;&#28151;&#21512;&#29289;&#12290;&#19982;&#36830;&#32493;&#30340;LV&#27169;&#22411;&#30456;&#27604;&#65292;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#26159;&#30001;&#36755;&#20837;&#12289;&#27714;&#21644;&#21644;&#20056;&#31215;&#21333;&#20803;&#32452;&#25104;&#30340;&#35745;&#31639;&#22270;&#34920;&#31034;&#30340;&#23618;&#27425;&#31163;&#25955;&#28151;&#21512;&#29289;&#12290;&#19982;&#36830;&#32493;&#30340;LV&#27169;&#22411;&#19981;&#21516;&#65292;PCs&#25552;&#20379;&#20102;&#21487;&#22788;&#29702;&#30340;&#25512;&#29702;&#65292;&#20294;&#20165;&#38480;&#20110;&#20855;&#26377;&#20998;&#31867;&#65288;&#21363;&#26080;&#24207;&#65289;&#29366;&#24577;&#30340;&#31163;&#25955;LVs&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#31215;&#20998;&#30005;&#36335;&#65288;PICs&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#27169;&#22411;&#31867;&#65292;PICs&#26159;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22270;&#35821;&#35328;&#65292;&#23427;&#20351;&#29992;&#34920;&#31034;&#36830;&#32493;LV&#30340;&#31215;&#20998;&#21333;&#20803;&#25193;&#23637;&#20102;PCs&#12290;&#39318;&#20808;&#65292;PICs&#26159;&#31526;&#21495;&#35745;&#31639;&#22270;&#65292;&#22312;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23436;&#20840;&#21487;&#36861;&#36394;&#65292;&#20854;&#20013;&#21487;&#20197;&#36827;&#34892;&#35299;&#26512;&#31215;&#20998;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;PICs&#65292;&#25552;&#20379;&#19968;&#20010;&#19981;&#21487;&#35299;&#30340;&#23618;&#27425;&#36830;&#32493;&#28151;&#21512;&#29289;&#65292;&#21487;&#20197;&#20351;&#29992;&#25968;&#20540;&#31215;&#20998;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#22823;&#22411;PCs&#12290;&#22312;....
&lt;/p&gt;
&lt;p&gt;
Continuous latent variables (LVs) are a key ingredient of many generative models, as they allow modelling expressive mixtures with an uncountable number of components. In contrast, probabilistic circuits (PCs) are hierarchical discrete mixtures represented as computational graphs composed of input, sum and product units. Unlike continuous LV models, PCs provide tractable inference but are limited to discrete LVs with categorical (i.e. unordered) states. We bridge these model classes by introducing probabilistic integral circuits (PICs), a new language of computational graphs that extends PCs with integral units representing continuous LVs. In the first place, PICs are symbolic computational graphs and are fully tractable in simple cases where analytical integration is possible. In practice, we parameterise PICs with light-weight neural nets delivering an intractable hierarchical continuous mixture that can be approximated arbitrarily well with large PCs using numerical quadrature. On s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25972;&#21512;&#25968;&#25454;&#20013;&#24515;&#30340;AI&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#22797;&#26434;&#32454;&#24494;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.16981</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#20013;&#24515;&#30340;AI&#37325;&#26032;&#26500;&#24819;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark. (arXiv:2310.16981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25972;&#21512;&#25968;&#25454;&#20013;&#24515;&#30340;AI&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#22797;&#26434;&#32454;&#24494;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#26159;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#26377;&#38480;&#25110;&#26080;&#27861;&#35775;&#38382;&#30340;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#21512;&#25104;&#25968;&#25454;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22797;&#26434;&#32454;&#24494;&#24046;&#24322;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#25972;&#21512;&#25968;&#25454;&#20013;&#24515;&#30340;AI&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#26469;&#25351;&#23548;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#24573;&#35270;&#36825;&#20123;&#25968;&#25454;&#37197;&#32622;&#25991;&#20214;&#30340;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#21518;&#26524;&#65292;&#23613;&#31649;&#30475;&#20284;&#32479;&#35745;&#19978;&#39640;&#24230;&#30495;&#23454;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#38598;&#25104;&#25968;&#25454;&#37197;&#32622;&#25991;&#20214;&#20197;&#25351;&#23548;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#21019;&#24314;&#12290;&#22312;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#22312;&#21313;&#19968;&#20010;&#19981;&#21516;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#24403;&#21069;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data serves as an alternative in training machine learning models, particularly when real-world data is limited or inaccessible. However, ensuring that synthetic data mirrors the complex nuances of real-world data is a challenging task. This paper addresses this issue by exploring the potential of integrating data-centric AI techniques which profile the data to guide the synthetic data generation process. Moreover, we shed light on the often ignored consequences of neglecting these data profiles during synthetic data generation -- despite seemingly high statistical fidelity. Subsequently, we propose a novel framework to evaluate the integration of data profiles to guide the creation of more representative synthetic data. In an empirical study, we evaluate the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets. The findings offer critical insights into the successes and limitations of current synthetic data generation te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#33258;&#25105;&#20462;&#27491;&#21644;&#22122;&#22768;&#20687;&#32032;&#23450;&#20301;&#26469;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16979</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#19982;&#20266;&#26631;&#31614;&#33258;&#25105;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement. (arXiv:2310.16979v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#33258;&#25105;&#20462;&#27491;&#21644;&#22122;&#22768;&#20687;&#32032;&#23450;&#20301;&#26469;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#35757;&#32451;&#26102;&#19981;&#21516;&#29305;&#24449;&#30340;&#25968;&#25454;&#19978;&#27979;&#35797;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20351;&#29992;&#26469;&#33258;&#26032;&#22495;&#30340;&#27880;&#37322;&#25968;&#25454;&#26469;&#36866;&#24212;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#20999;&#23454;&#21487;&#34892;&#30340;&#12290;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#27861;&#22312;&#23454;&#38469;&#25805;&#20316;&#26465;&#20214;&#19979;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;UDA&#26041;&#27861;&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#26032;&#25968;&#25454;&#30340;&#20266;&#26631;&#31614;&#65292;&#36827;&#32780;&#25351;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#65292;&#20294;&#21364;&#23384;&#22312;&#20266;&#26631;&#31614;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20256;&#25773;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36741;&#21161;&#20266;&#26631;&#31614;&#31934;&#28860;&#32593;&#32476;&#65288;PRN&#65289;&#65292;&#29992;&#20110;&#22312;&#32447;&#31934;&#28860;&#20266;&#26631;&#31614;&#65292;&#24182;&#23450;&#20301;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#30340;&#20687;&#32032;&#28857;&#30340;&#39044;&#27979;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based solutions for semantic segmentation suffer from significant performance degradation when tested on data with different characteristics than what was used during the training. Adapting the models using annotated data from the new domain is not always practical. Unsupervised Domain Adaptation (UDA) approaches are crucial in deploying these models in the actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ a teacher-student self-training approach, where a teacher model is used to generate pseudo-labels for the new data which in turn guide the training process of the student model. Though this approach has seen a lot of success, it suffers from the issue of noisy pseudo-labels being propagated in the training process. To address this issue, we propose an auxiliary pseudo-label refinement network (PRN) for online refining of the pseudo labels and also localizing the pixels whose predicted labels are likely to be noisy. Being able to improve the 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20020;&#24202;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;ML&#21644;AI&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#26041;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16978</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20020;&#24202;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#24847;&#20041;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Significance of Machine Learning in Clinical Disease Diagnosis: A Review. (arXiv:2310.16978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20020;&#24202;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;ML&#21644;AI&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#26041;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#21508;&#31181;&#30142;&#30149;&#26426;&#21046;&#30340;&#22797;&#26434;&#24615;&#21644;&#24739;&#32773;&#30151;&#29366;&#30340;&#22810;&#26679;&#24615;&#65292;&#26377;&#25928;&#30340;&#30142;&#30149;&#35786;&#26029;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#12289;&#21307;&#29983;&#21644;&#24739;&#32773;&#27491;&#36716;&#21521;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#19968;&#38376;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23398;&#31185;&#65292;&#26469;&#24320;&#21457;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;ML&#21644;AI&#26041;&#27861;&#65292;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#26041;&#21487;&#20197;&#33719;&#24471;&#22686;&#24378;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#38024;&#23545;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#25351;&#26631;&#20013;&#25913;&#21892;&#24515;&#29575;&#25968;&#25454;&#20256;&#36755;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;ML&#31639;&#27861;&#65292;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;ML&#30340;&#30142;&#30149;&#35786;&#26029;&#65288;MLBDD&#65289;&#30340;&#26368;&#26032;&#36235;&#21183;&#21644;&#26041;&#27861;&#12290;&#32771;&#34385;&#30340;&#22240;&#32032;&#21253;&#25324;&#31639;&#27861;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The global need for effective disease diagnosis remains substantial, given the complexities of various disease mechanisms and diverse patient symptoms. To tackle these challenges, researchers, physicians, and patients are turning to machine learning (ML), an artificial intelligence (AI) discipline, to develop solutions. By leveraging sophisticated ML and AI methods, healthcare stakeholders gain enhanced diagnostic and treatment capabilities. However, there is a scarcity of research focused on ML algorithms for enhancing the accuracy and computational efficiency. This research investigates the capacity of machine learning algorithms to improve the transmission of heart rate data in time series healthcare metrics, concentrating particularly on optimizing accuracy and efficiency. By exploring various ML algorithms used in healthcare applications, the review presents the latest trends and approaches in ML-based disease diagnosis (MLBDD). The factors under consideration include the algorith
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36924;&#36817;&#38745;&#24577;&#21644;&#21160;&#24577;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16975</link><description>&lt;p&gt;
&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21450;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference. (arXiv:2310.16975v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36924;&#36817;&#38745;&#24577;&#21644;&#21160;&#24577;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20998;&#21035;&#36924;&#36817;&#38745;&#24577;&#21644;&#21160;&#24577;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#35299;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#65292;&#36825;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#34920;&#31034;&#20026;&#21487;&#22788;&#29702;&#30340;&#21442;&#32771;&#20998;&#24067;&#30340;&#36716;&#25442;&#65292;&#22240;&#27492;&#23646;&#20110;&#27979;&#24230;&#20256;&#36755;&#30340;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;COT&#26144;&#23556;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#36873;&#25321;&#65292;&#20855;&#26377;&#21807;&#19968;&#24615;&#21644;&#21333;&#35843;&#24615;&#31561;&#21487;&#21462;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#30340;COT&#38382;&#39064;&#22312;&#20013;&#31561;&#32500;&#24230;&#19979;&#35745;&#31639;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#30340;&#25968;&#20540;&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;COT&#26144;&#23556;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;COT&#38382;&#39064;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#34920;&#36798;&#24418;&#24335;&#30340;&#32467;&#26500;&#12290;PCP-Map&#23558;&#26465;&#20214;&#20256;&#36755;&#26144;&#23556;&#24314;&#27169;&#20026;&#37096;&#20998;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;PICNN&#65289;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two neural network approaches that approximate the solutions of static and dynamic conditional optimal transport (COT) problems, respectively. Both approaches enable sampling and density estimation of conditional probability distributions, which are core tasks in Bayesian inference. Our methods represent the target conditional distributions as transformations of a tractable reference distribution and, therefore, fall into the framework of measure transport. COT maps are a canonical choice within this framework, with desirable properties such as uniqueness and monotonicity. However, the associated COT problems are computationally challenging, even in moderate dimensions. To improve the scalability, our numerical algorithms leverage neural networks to parameterize COT maps. Our methods exploit the structure of the static and dynamic formulations of the COT problem. PCP-Map models conditional transport maps as the gradient of a partially input convex neural network (PICNN) and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16960</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Privately Aligning Language Models with Reinforcement Learning. (arXiv:2310.16960v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#21644;&#29992;&#25143;&#37096;&#32626;&#20043;&#38388;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#22521;&#35757;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;(&#22914;ChatGPT)&#30340;&#20027;&#27969;&#31574;&#30053;&#12290;&#26412;&#25991;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;(DP)&#26469;&#30740;&#31350;&#38544;&#31169;&#20445;&#25252;&#30340;LLMs&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#33539;&#24335;&#65306;(i)&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#26041;&#27861;(&#22914;&#31215;&#26497;&#35780;&#20215;&#29983;&#25104;)&#65292;(ii)&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#26041;&#27861;(RLHF)(&#22914;&#20197;&#20154;&#31867;&#39318;&#36873;&#26041;&#24335;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP&#26694;&#26550;&#26469;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#40784;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#22312;&#30830;&#20445;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#26377;&#31454;&#20105;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23433;&#20840;&#20998;&#31867;&#22120;&#20013;&#25913;&#21892;&#20102;&#23569;&#26679;&#26412;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#31038;&#20132;&#21270;&#23398;&#39046;&#22495;&#30340;F1&#24471;&#20998;&#25552;&#39640;&#20102;7-17%&#12290;</title><link>http://arxiv.org/abs/2310.16959</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25913;&#21892;&#23569;&#26679;&#26412;&#36890;&#29992;&#24615;&#23433;&#20840;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning. (arXiv:2310.16959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16959
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23433;&#20840;&#20998;&#31867;&#22120;&#20013;&#25913;&#21892;&#20102;&#23569;&#26679;&#26412;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#31038;&#20132;&#21270;&#23398;&#39046;&#22495;&#30340;F1&#24471;&#20998;&#25552;&#39640;&#20102;7-17%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#20986;&#29616;&#20102;&#26032;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#25919;&#31574;&#65292;&#29616;&#26377;&#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#26080;&#27861;&#24456;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#12290;&#22914;&#26524;&#25105;&#20204;&#21482;&#35266;&#23519;&#21040;&#23569;&#37327;&#36829;&#21453;&#26032;&#23433;&#20840;&#35268;&#21017;&#30340;&#31034;&#20363;&#65292;&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#36829;&#35268;&#34892;&#20026;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23433;&#20840;&#20998;&#31867;&#22120;&#30340;&#39046;&#22495;&#36890;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#35774;&#32622;&#12290;&#19982;&#20043;&#21069;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#20123;&#26032;&#30340;&#23433;&#20840;&#38382;&#39064;&#24456;&#38590;&#21457;&#29616;&#65292;&#32780;&#19988;&#25105;&#20204;&#19981;&#33021;&#36873;&#25321;&#23569;&#37327;&#31034;&#20363;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#20808;&#21069;&#29616;&#26377;&#35268;&#21017;&#20013;&#30340;&#30456;&#20284;&#31034;&#20363;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#30456;&#20284;&#24615;&#25968;&#25454;&#22686;&#24378;+&#25552;&#31034;&#24494;&#35843;&#65288;DAPT&#65289;&#26041;&#27861;&#22312;&#31038;&#20132;&#21270;&#23398;&#39046;&#22495;&#30340;F1&#20998;&#25968;&#19978;&#22987;&#32456;&#27604;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#25110;PEFT&#30340;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;7-17%&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. If we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations? In this paper, we study the novel setting of domain-generalized few-shot learning for LLM-based text safety classifiers. Unlike prior few-shot work, these new safety issues can be hard to uncover and we do not get to choose the few examples. We demonstrate that existing few-shot techniques do not perform well in this setting, and rather we propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting training data based on similar examples in prior existing rules. We empirically show that our approach of similarity-based data-augmentation + prompt-tuning (DAPT) consistently outperforms baselines that either do not rely on data augmentation or on PEFT by 7-17% F1 score in the Social Chemistry mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22312;&#23567;&#20998;&#23376;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#32858;&#21512;&#29289;&#24615;&#36136;&#19978;&#24494;&#35843;&#30340;Transformer&#27169;&#22411;&#65292;&#22312;&#32858;&#21512;&#29289;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#20013;&#36798;&#21040;&#19982;&#22312;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16958</link><description>&lt;p&gt;
&#36716;&#31227;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#32858;&#21512;&#29289;&#24615;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transferring a molecular foundation model for polymer property predictions. (arXiv:2310.16958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22312;&#23567;&#20998;&#23376;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#32858;&#21512;&#29289;&#24615;&#36136;&#19978;&#24494;&#35843;&#30340;Transformer&#27169;&#22411;&#65292;&#22312;&#32858;&#21512;&#29289;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#20013;&#36798;&#21040;&#19982;&#22312;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#26448;&#26009;&#21457;&#29616;&#31561;&#39046;&#22495;&#30340;&#35774;&#35745;&#20248;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#33258;&#25105;&#30417;&#30563;&#30340;Transformer&#27169;&#22411;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#22312;&#32858;&#21512;&#29289;&#31185;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#24448;&#24448;&#38750;&#24120;&#31232;&#30095;&#12290;&#30446;&#21069;&#32858;&#21512;&#29289;&#30340;&#26368;&#26032;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#39069;&#22806;&#30340;&#26679;&#26412;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23567;&#20998;&#23376;&#30340;&#22823;&#35268;&#27169;&#24320;&#28304;&#25968;&#25454;&#38598;&#21487;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22312;&#23567;&#20998;&#23376;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#32858;&#21512;&#29289;&#24615;&#36136;&#19978;&#24494;&#35843;&#30340;Transformer&#27169;&#22411;&#65292;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#39044;&#27979;&#20219;&#21153;&#19978;&#36798;&#21040;&#19982;&#22312;&#22686;&#24378;&#32858;&#21512;&#29289;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models have remarkable potential to accelerate design optimization for applications such as drug development and materials discovery. Self-supervised pretraining of transformer models requires large-scale datasets, which are often sparsely populated in topical areas such as polymer science. State-of-the-art approaches for polymers conduct data augmentation to generate additional samples but unavoidably incurs extra computational costs. In contrast, large-scale open-source datasets are available for small molecules and provide a potential solution to data scarcity through transfer learning. In this work, we show that using transformers pretrained on small molecules and fine-tuned on polymer properties achieve comparable accuracy to those trained on augmented polymer datasets for a series of benchmark prediction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#31867;&#23545;&#25163;&#31034;&#20363;&#29983;&#25104;&#26356;&#26377;&#29992;&#30340;&#22823;&#35268;&#27169;&#23545;&#25239;&#31034;&#20363;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#23545;&#20110;&#20154;&#31867;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16955</link><description>&lt;p&gt;
&#25171;&#30772;&#12289;&#27169;&#20223;&#12289;&#20462;&#22797;&#65306;&#36890;&#36807;&#29983;&#25104;&#20154;&#31867;&#25915;&#20987;&#25552;&#39640;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks. (arXiv:2310.16955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#31867;&#23545;&#25163;&#31034;&#20363;&#29983;&#25104;&#26356;&#26377;&#29992;&#30340;&#22823;&#35268;&#27169;&#23545;&#25239;&#31034;&#20363;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#23545;&#20110;&#20154;&#31867;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#38656;&#35201;&#23545;&#25239;&#20154;&#31867;&#23545;&#25163;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25910;&#38598;&#20154;&#31867;&#23545;&#25163;&#30340;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#31181;&#26377;&#25928;&#20294;&#26114;&#36149;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35757;&#32451;&#38024;&#23545;&#23567;&#25200;&#21160;&#65288;&#22914;&#35789;&#26367;&#25442;&#65289;&#30340;&#21512;&#25104;&#25915;&#20987;&#23454;&#38469;&#19978;&#24182;&#19981;&#33021;&#25552;&#39640;&#23545;&#25239;&#20154;&#31867;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#31867;&#23545;&#25163;&#31034;&#20363;&#26469;&#29983;&#25104;&#26356;&#26377;&#29992;&#30340;&#22823;&#35268;&#27169;&#23545;&#25239;&#31034;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;ANLI&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36845;&#20195;&#30340;&#23545;&#25239;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#36807;&#31243;&#25910;&#38598;&#24471;&#21040;&#30340;&#12290;&#19982;&#20165;&#22312;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;&#25915;&#20987;&#19978;&#36827;&#34892;&#35757;&#32451;&#30456;&#27604;&#65292;&#20063;&#22312;&#25105;&#20204;&#30340;&#21512;&#25104;&#23545;&#25239;&#31034;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#26410;&#26469;&#22238;&#21512;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;ANLI&#19978;&#65292;&#25105;&#20204;&#30475;&#21040;&#20102;&#23545;&#24403;&#21069;&#25915;&#20987;&#38598;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65288;44.1% -&gt; 50.1%&#65289;&#65292;&#20197;&#21450;&#23545;&#20004;&#20010;&#26410;&#35265;&#36807;&#30340;&#20154;&#31867;&#29983;&#25104;&#25915;&#20987;&#22238;&#21512;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65288;32.5% -&gt; 43%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations - such as word-substitution - does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets - both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of human generated attacks (32.5%$\,\to\,$43
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;</title><link>http://arxiv.org/abs/2310.16945</link><description>&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection&#65288;CATE&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#22240;&#26524;Q&#38598;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#26159;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#29992;&#20110;CATE&#20272;&#35745;&#30340;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#27169;&#22411;&#36873;&#25321;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#23454;&#35777;&#24037;&#20316;&#25552;&#20379;&#20102;&#26377;&#21033;&#20110;&#20855;&#26377;&#21452;&#37325;&#40065;&#26834;&#24615;&#36136;&#30340;&#20195;&#29702;&#25439;&#22833;&#24230;&#37327;&#21644;&#27169;&#22411;&#38598;&#25104;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#29702;&#35299;&#36824;&#19981;&#22815;&#12290;&#30452;&#25509;&#24212;&#29992;&#20808;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#20250;&#30001;&#20110;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#26377;&#20027;&#35201;CATE&#38598;&#25104;&#26041;&#27861;&#30340;&#36951;&#25022;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#30340;Q&#38598;&#25104;&#30340;&#26032;&#30340;CATE&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#22240;&#26524;Q&#38598;&#25104;&#22312;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#30340;&#36951;&#25022;&#29575;&#19978;&#36798;&#21040;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20248;&#20540;&#20026;$\frac{\log(M)}{n}$&#65288;&#20854;&#20013;$M$&#20026;&#27169;&#22411;&#25968;&#65292;$n$&#20026;&#26679;&#26412;&#25968;&#65289;&#65292;&#21152;&#19978;&#39640;&#38454;&#20272;&#35745;&#35823;&#24046;&#39033;
&lt;/p&gt;
&lt;p&gt;
Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error term
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#33976;&#39311;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#20351;&#29992;AI&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#32842;&#22825;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24847;&#22270;&#23545;&#40784;&#30340;&#25928;&#26524;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#19988;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;7B&#21442;&#25968;&#27169;&#22411;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#22909;&#30340;&#24320;&#25918;&#35775;&#38382;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16944</link><description>&lt;p&gt;
Zephyr: &#30452;&#25509;&#33976;&#39311;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Zephyr: Direct Distillation of LM Alignment. (arXiv:2310.16944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#33976;&#39311;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#20351;&#29992;AI&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#32842;&#22825;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24847;&#22270;&#23545;&#40784;&#30340;&#25928;&#26524;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#19988;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;7B&#21442;&#25968;&#27169;&#22411;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#22909;&#30340;&#24320;&#25918;&#35775;&#38382;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#36739;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#36739;&#22823;&#30340;&#27169;&#22411;&#24212;&#29992;&#33976;&#39311;&#30340;&#30417;&#30563;&#24494;&#35843;&#33021;&#26174;&#33879;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#27809;&#26377;&#23545;&#40784;&#65292;&#21363;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#21709;&#24212;&#33258;&#28982;&#25552;&#31034;&#12290;&#20026;&#20102;&#33976;&#39311;&#36825;&#20010;&#23646;&#24615;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#26469;&#33258;AI&#21453;&#39304;&#65288;AIF&#65289;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;&#20174;&#19968;&#20010;&#30001;&#25945;&#24072;&#27169;&#22411;&#25490;&#21517;&#30340;&#36755;&#20986;&#25968;&#25454;&#38598;&#24320;&#22987;&#65292;&#25105;&#20204;&#24212;&#29992;&#33976;&#39311;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;dDPO&#65289;&#26469;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#24847;&#22270;&#23545;&#40784;&#30340;&#32842;&#22825;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#37319;&#26679;&#12290;&#26368;&#32456;&#32467;&#26524;Zephyr-7B&#22312;7B&#21442;&#25968;&#27169;&#22411;&#30340;&#32842;&#22825;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;MT-Bench&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;Zephyr-7B&#36229;&#36234;&#20102;&#26368;&#22909;&#30340;&#24320;&#25918;&#35775;&#38382;RLHF&#27169;&#22411;Llama2-Chat-70B&#12290;&#31995;&#32479;&#30340;&#20195;&#30721;&#12289;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#25945;&#31243;&#37117;&#21487;&#20197;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are availabl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#33021;&#21147;&#30340;&#21151;&#33021;&#24322;&#36136;&#32676;&#20307;&#26426;&#22120;&#20154;&#20013;&#30830;&#23450;&#32039;&#24613;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#33021;&#21457;&#29616;&#35768;&#22810;&#26377;&#36259;&#30340;&#34892;&#20026;&#65292;&#32780;&#36845;&#20195;&#30340;&#20154;&#26426;&#21327;&#20316;&#21457;&#29616;&#36807;&#31243;&#27604;&#38543;&#26426;&#25628;&#32034;&#12289;&#32676;&#20307;&#21270;&#23398;&#21644;&#33258;&#21160;&#21270;&#34892;&#20026;&#21457;&#29616;&#21457;&#29616;&#20102;&#26356;&#22810;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.16941</link><description>&lt;p&gt;
&#25506;&#32034;&#26377;&#38480;&#33021;&#21147;&#24322;&#36136;&#32676;&#20307;&#26426;&#22120;&#20154;&#34892;&#20026;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Behavior Discovery Methods for Heterogeneous Swarms of Limited-Capability Robots. (arXiv:2310.16941v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16941
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#33021;&#21147;&#30340;&#21151;&#33021;&#24322;&#36136;&#32676;&#20307;&#26426;&#22120;&#20154;&#20013;&#30830;&#23450;&#32039;&#24613;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#33021;&#21457;&#29616;&#35768;&#22810;&#26377;&#36259;&#30340;&#34892;&#20026;&#65292;&#32780;&#36845;&#20195;&#30340;&#20154;&#26426;&#21327;&#20316;&#21457;&#29616;&#36807;&#31243;&#27604;&#38543;&#26426;&#25628;&#32034;&#12289;&#32676;&#20307;&#21270;&#23398;&#21644;&#33258;&#21160;&#21270;&#34892;&#20026;&#21457;&#29616;&#21457;&#29616;&#20102;&#26356;&#22810;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#33021;&#21147;&#30340;&#21151;&#33021;&#24322;&#36136;&#32676;&#20307;&#26426;&#22120;&#20154;&#20013;&#30830;&#23450;&#21487;&#33021;&#20986;&#29616;&#30340;&#32039;&#24613;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#23545;&#20110;&#21516;&#36136;&#32676;&#20307;&#30340;&#34892;&#20026;&#25628;&#32034;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26032;&#22855;&#25628;&#32034;&#20197;&#21450;&#25163;&#21160;&#25351;&#23450;&#25110;&#23398;&#20064;&#30340;&#34892;&#20026;&#31354;&#38388;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#36820;&#22238;&#32473;&#29992;&#25143;&#19968;&#20010;&#32039;&#24613;&#34892;&#20026;&#30340;&#20998;&#31867;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#26356;&#22909;&#22320;&#29702;&#35299;&#26032;&#22855;&#25628;&#32034;&#30340;&#20316;&#29992;&#20197;&#21450;&#20351;&#29992;&#32858;&#31867;&#26469;&#21457;&#29616;&#26032;&#22855;&#32039;&#24613;&#34892;&#20026;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#21066;&#20943;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#24322;&#36136;&#32676;&#20307;&#20013;&#30740;&#31350;&#20102;&#34920;&#31034;&#12289;&#36827;&#21270;&#25628;&#32034;&#21644;&#21508;&#31181;&#32858;&#31867;&#26041;&#27861;&#22312;&#25628;&#32034;&#26032;&#22855;&#34892;&#20026;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#33021;&#21457;&#29616;&#35768;&#22810;&#26377;&#36259;&#30340;&#34892;&#20026;&#65292;&#32780;&#36845;&#20195;&#30340;&#20154;&#26426;&#21327;&#20316;&#21457;&#29616;&#36807;&#31243;&#27604;&#38543;&#26426;&#25628;&#32034;&#12289;&#32676;&#20307;&#21270;&#23398;&#21644;&#33258;&#21160;&#21270;&#34892;&#20026;&#21457;&#29616;&#21457;&#29616;&#20102;&#26356;&#22810;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of determining the emergent behaviors that are possible given a functionally heterogeneous swarm of robots with limited capabilities. Prior work has considered behavior search for homogeneous swarms and proposed the use of novelty search over either a hand-specified or learned behavior space followed by clustering to return a taxonomy of emergent behaviors to the user. In this paper, we seek to better understand the role of novelty search and the efficacy of using clustering to discover novel emergent behaviors. Through a large set of experiments and ablations, we analyze the effect of representations, evolutionary search, and various clustering methods in the search for novel behaviors in a heterogeneous swarm. Our results indicate that prior methods fail to discover many interesting behaviors and that an iterative human-in-the-loop discovery process discovers more behaviors than random search, swarm chemistry, and automated behavior discovery. The combined discov
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#26399;-&#26202;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#24182;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#20855;&#22791;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#36866;&#24212;&#29420;&#29305;&#29305;&#24449;&#30340;&#40065;&#26834;&#39044;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.16936</link><description>&lt;p&gt;
&#20351;&#29992;&#38597;&#21487;&#27604;&#22270;&#35889;&#30340;&#26089;&#26399;-&#26202;&#26399;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Diagnosing Alzheimer's Disease using Early-Late Multimodal Data Fusion with Jacobian Maps. (arXiv:2310.16936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16936
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#26399;-&#26202;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#24182;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#20855;&#22791;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#36866;&#24212;&#29420;&#29305;&#29305;&#24449;&#30340;&#40065;&#26834;&#39044;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#19988;&#20855;&#26377;&#30772;&#22351;&#24615;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#24222;&#22823;&#30340;&#32769;&#24180;&#20154;&#32676;&#20307;&#12290;&#22312;&#25152;&#26377;&#30340;&#26080;&#30151;&#29366;&#21644;&#30151;&#29366;&#20986;&#29616;&#38454;&#27573;&#26816;&#27979;AD&#23545;&#20110;&#26089;&#26399;&#24178;&#39044;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#39033;&#27963;&#36291;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#32988;&#36807;&#21307;&#23398;&#25195;&#25551;&#30340;&#20154;&#24037;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#20887;&#20313;&#35745;&#31639;&#12289;&#22797;&#26434;&#30340;&#26550;&#26500;&#21644;&#23545;&#32570;&#22833;&#25968;&#25454;&#31616;&#21333;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#21307;&#23398;&#25195;&#25551;&#30340;&#39044;&#22788;&#29702;&#27969;&#31243;&#20173;&#28982;&#19981;&#22815;&#35814;&#32454;&#65292;&#24182;&#19988;&#24456;&#23569;&#20026;&#20010;&#20307;&#20027;&#39064;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26089;&#26399;-&#26202;&#26399;&#34701;&#21512;&#65288;ELF&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#20855;&#22791;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36866;&#24212;&#29420;&#29305;&#29305;&#24449;&#30340;&#40065;&#26834;&#39044;&#22788;&#29702;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is a prevalent and debilitating neurodegenerative disorder impacting a large aging population. Detecting AD in all its presymptomatic and symptomatic stages is crucial for early intervention and treatment. An active research direction is to explore machine learning methods that harness multimodal data fusion to outperform human inspection of medical scans. However, existing multimodal fusion models have limitations, including redundant computation, complex architecture, and simplistic handling of missing data. Moreover, the preprocessing pipelines of medical scans remain inadequately detailed and are seldom optimized for individual subjects. In this paper, we propose an efficient early-late fusion (ELF) approach, which leverages a convolutional neural network for automated feature extraction and random forests for their competitive performance on small datasets. Additionally, we introduce a robust preprocessing pipeline that adapts to the unique characteristics
&lt;/p&gt;</description></item><item><title>MimicTouch&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#26469;&#23398;&#20064;&#24182;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.16917</link><description>&lt;p&gt;
MimicTouch: &#20351;&#29992;&#22810;&#27169;&#24577;&#35302;&#35273;&#21453;&#39304;&#23398;&#20064;&#20154;&#31867;&#30340;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile Feedback. (arXiv:2310.16917v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16917
&lt;/p&gt;
&lt;p&gt;
MimicTouch&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#26469;&#23398;&#20064;&#24182;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35302;&#35273;&#22788;&#29702;&#30340;&#25972;&#21512;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#25191;&#34892;&#20687;&#23545;&#20934;&#21644;&#25554;&#20837;&#36825;&#26679;&#22797;&#26434;&#20219;&#21153;&#26102;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#26426;&#22120;&#20154;&#36965;&#25805;&#20316;&#25968;&#25454;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#21463;&#35302;&#35273;&#21453;&#39304;&#24341;&#23548;&#19979;&#30340;&#25511;&#21046;&#31574;&#30053;&#25152;&#25552;&#20379;&#30340;&#20016;&#23500;&#35265;&#35299;&#12290;&#20026;&#20102;&#21033;&#29992;&#20154;&#31867;&#24863;&#35273;&#65292;&#29616;&#26377;&#30340;&#20174;&#20154;&#31867;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#35270;&#35273;&#21453;&#39304;&#65292;&#24120;&#24120;&#24573;&#35270;&#20102;&#20154;&#31867;&#26412;&#33021;&#22320;&#21033;&#29992;&#35302;&#35273;&#21453;&#39304;&#23436;&#25104;&#22797;&#26434;&#25805;&#20316;&#30340;&#23453;&#36149;&#32463;&#39564;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;"MimicTouch"&#65292;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#20154;&#31867;&#31034;&#33539;&#32773;&#37027;&#37324;&#25910;&#38598;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20154;&#31867;&#35302;&#35273;&#24341;&#23548;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#30340;&#27493;&#39588;&#28041;&#21450;&#25351;&#20196;&#30340;&#20256;&#36882;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#31574;&#30053;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In robotics and artificial intelligence, the integration of tactile processing is becoming increasingly pivotal, especially in learning to execute intricate tasks like alignment and insertion. However, existing works focusing on tactile methods for insertion tasks predominantly rely on robot teleoperation data and reinforcement learning, which do not utilize the rich insights provided by human's control strategy guided by tactile feedback. For utilizing human sensations, methodologies related to learning from humans predominantly leverage visual feedback, often overlooking the invaluable tactile feedback that humans inherently employ to finish complex manipulations. Addressing this gap, we introduce "MimicTouch", a novel framework that mimics human's tactile-guided control strategy. In this framework, we initially collect multi-modal tactile datasets from human demonstrators, incorporating human tactile-guided control strategies for task completion. The subsequent step involves instruc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#38750;&#32447;&#24615;&#26550;&#26500;&#65292;&#29992;&#20110;&#22823;&#27668;&#23494;&#24230;&#39044;&#27979;&#65292;&#20197;&#25913;&#36827;&#20043;&#21069;&#30340;&#32447;&#24615;&#20256;&#25773;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#31354;&#38388;&#24577;&#21183;&#24863;&#30693;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.16912</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#27668;&#23494;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Atmospheric Density Forecasting. (arXiv:2310.16912v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#38750;&#32447;&#24615;&#26550;&#26500;&#65292;&#29992;&#20110;&#22823;&#27668;&#23494;&#24230;&#39044;&#27979;&#65292;&#20197;&#25913;&#36827;&#20043;&#21069;&#30340;&#32447;&#24615;&#20256;&#25773;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#31354;&#38388;&#24577;&#21183;&#24863;&#30693;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;2025&#24180;&#22826;&#38451;&#21608;&#26399;&#30340;&#39640;&#23792;&#26399;&#20020;&#36817;&#21644;&#21333;&#20010;&#22320;&#30913;&#26292;&#23545;&#23621;&#20303;&#31354;&#38388;&#29289;&#20307;&#65288;RSOs&#65289;&#36712;&#36947;&#30340;&#26174;&#33879;&#25913;&#21464;&#33021;&#21147;&#65292;&#23545;&#22823;&#27668;&#23494;&#24230;&#39044;&#27979;&#25216;&#26415;&#23545;&#20110;&#31354;&#38388;&#24577;&#21183;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20043;&#21069;&#24050;&#32463;&#20351;&#29992;&#32447;&#24615;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65288;&#20363;&#22914;&#20855;&#26377;&#25511;&#21046;&#30340;&#21160;&#21147;&#27169;&#24577;&#20998;&#35299;&#65288;DMDc&#65289;&#65289;&#26469;&#39044;&#27979;&#22823;&#27668;&#23494;&#24230;&#65292;&#20294;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#20855;&#26377;&#25429;&#25417;&#25968;&#25454;&#20013;&#38750;&#32447;&#24615;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#21382;&#21490;&#22823;&#27668;&#23494;&#24230;&#25968;&#25454;&#20013;&#23398;&#20064;&#22810;&#23618;&#26435;&#37325;&#65292;&#23558;&#24403;&#21069;&#22823;&#27668;&#23494;&#24230;&#29366;&#24577;&#19982;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#22823;&#27668;&#23494;&#24230;&#29366;&#24577;&#30340;&#25511;&#21046;&#36755;&#20837;&#20043;&#38388;&#30340;&#26144;&#23556;&#20013;&#25429;&#33719;&#25968;&#25454;&#38598;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#22823;&#27668;&#23494;&#24230;&#39044;&#27979;&#24320;&#21457;&#38750;&#32447;&#24615;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#32447;&#24615;&#20256;&#25773;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the peak of the solar cycle approaches in 2025 and the ability of a single geomagnetic storm to significantly alter the orbit of Resident Space Objects (RSOs), techniques for atmospheric density forecasting are vital for space situational awareness. While linear data-driven methods, such as dynamic mode decomposition with control (DMDc), have been used previously for forecasting atmospheric density, deep learning-based forecasting has the ability to capture nonlinearities in data. By learning multiple layer weights from historical atmospheric density data, long-term dependencies in the dataset are captured in the mapping between the current atmospheric density state and control input to the atmospheric density state at the next timestep. This work improves upon previous linear propagation methods for atmospheric density forecasting, by developing a nonlinear transformer-based architecture for atmospheric density forecasting. Empirical NRLMSISE-00 and JB2008, as well as physics-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MACP&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#37197;&#22791;&#21512;&#20316;&#33021;&#21147;&#26469;&#25552;&#39640;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#28155;&#21152;&#20960;&#20010;&#36731;&#37327;&#32423;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#21512;&#20316;&#35266;&#27979;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16870</link><description>&lt;p&gt;
MACP&#65306;&#39640;&#25928;&#30340;&#21512;&#20316;&#24863;&#30693;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
MACP: Efficient Model Adaptation for Cooperative Perception. (arXiv:2310.16870v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MACP&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#37197;&#22791;&#21512;&#20316;&#33021;&#21147;&#26469;&#25552;&#39640;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#28155;&#21152;&#20960;&#20010;&#36731;&#37327;&#32423;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#21512;&#20316;&#35266;&#27979;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#38388;&#30340;&#36890;&#20449;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#29616;&#20449;&#24687;&#20849;&#20139;&#20197;"&#31359;&#36879;"&#36974;&#25377;&#29289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#24320;&#21457;&#21644;&#35757;&#32451;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#27169;&#22411;&#21487;&#33021;&#26114;&#36149;&#19988;&#19981;&#24517;&#35201;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#21333;&#26234;&#33021;&#20307;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MACP&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#37197;&#22791;&#20102;&#21512;&#20316;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#23450;&#20174;&#21333;&#26234;&#33021;&#20307;&#21040;&#21512;&#20316;&#29615;&#22659;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#28155;&#21152;&#20960;&#20010;&#36731;&#37327;&#32423;&#27169;&#22359;&#26469;&#20351;&#27169;&#22411;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#21512;&#20316;&#35266;&#27979;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to "see through the occlusions", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#21033;&#29992;WGAN-GP&#21644;VAE&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16867</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation. (arXiv:2310.16867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#21033;&#29992;WGAN-GP&#21644;VAE&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#33041;&#37096;&#35760;&#24405;&#65292;&#36827;&#34892;&#31934;&#31070;&#20998;&#35010;&#30151;&#30340;&#33258;&#21160;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#33021;&#22815;&#21033;&#29992;&#26102;&#39057;&#29305;&#24449;&#65292;&#20174;&#21407;&#22987;&#20449;&#21495;&#20013;&#25552;&#21462;&#20102;&#39057;&#35889;&#22270;&#12290;&#22312;&#25506;&#32034;&#20102;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#35774;&#32622;&#21518;&#65292;&#36873;&#25321;&#20102;&#36866;&#21512;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;Wasserstein GAN with Gradient Penalty(WGAN-GP)&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#29983;&#25104;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#22686;&#24378;&#21021;&#22987;&#25968;&#25454;&#38598;&#24182;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20351;&#29992;VAE&#29983;&#25104;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#22312;&#20934;&#30830;&#24230;&#19978;&#25552;&#39640;&#20102;3.0&#65285;&#65292;&#36798;&#21040;&#20102;99.0&#65285;&#65292;&#21516;&#26102;&#24471;&#21040;&#20102;&#26356;&#20302;&#30340;&#25439;&#22833;&#20540;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;(LIME)&#26041;&#27861;&#35299;&#20915;&#20102;&#23545;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#20449;&#20219;&#32570;&#20047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we leverage a deep learning-based method for the automatic diagnosis of schizophrenia using EEG brain recordings. This approach utilizes generative data augmentation, a powerful technique that enhances the accuracy of the diagnosis. To enable the utilization of time-frequency features, spectrograms were extracted from the raw signals. After exploring several neural network architectural setups, a proper convolutional neural network (CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two different synthetic datasets were generated in order to augment the initial dataset and address the over-fitting issue. The augmented dataset using VAE achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a lower loss value as well as a faster convergence. Finally, we addressed the lack of trust in black-box models using the Local Interpretable Model-agnostic Explanations (LI
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#28857;&#27169;&#22411;&#65288;GPM&#65289;&#65292;&#23427;&#22312;&#28857;&#20113;&#21464;&#25442;&#22120;&#20013;&#26080;&#32541;&#25972;&#21512;&#20102;&#33258;&#32534;&#30721;&#21644;&#33258;&#22238;&#24402;&#20219;&#21153;&#12290;GPM&#36890;&#36807;&#25513;&#30721;&#22635;&#20805;&#20219;&#21153;&#22686;&#24378;&#20102;&#33258;&#32534;&#30721;&#20013;&#30340;&#25513;&#30721;&#39044;&#27979;&#65292;&#24182;&#22312;&#28857;&#20113;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16861</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#32534;&#30721;&#21644;&#33258;&#22238;&#24402;&#30340;&#36890;&#29992;&#28857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General Point Model with Autoencoding and Autoregressive. (arXiv:2310.16861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#28857;&#27169;&#22411;&#65288;GPM&#65289;&#65292;&#23427;&#22312;&#28857;&#20113;&#21464;&#25442;&#22120;&#20013;&#26080;&#32541;&#25972;&#21512;&#20102;&#33258;&#32534;&#30721;&#21644;&#33258;&#22238;&#24402;&#20219;&#21153;&#12290;GPM&#36890;&#36807;&#25513;&#30721;&#22635;&#20805;&#20219;&#21153;&#22686;&#24378;&#20102;&#33258;&#32534;&#30721;&#20013;&#30340;&#25513;&#30721;&#39044;&#27979;&#65292;&#24182;&#22312;&#28857;&#20113;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26550;&#26500;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#65292;&#21253;&#25324;&#33258;&#32534;&#30721;&#27169;&#22411;&#12289;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#35201;&#23558;&#20219;&#20309;&#24418;&#24335;&#30340;&#27169;&#24577;&#37327;&#21270;&#20026;&#31163;&#25955;&#30340;&#31526;&#21495;&#65292;&#23427;&#37117;&#26377;&#21487;&#33021;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#30410;&#12290;&#21463;&#21040;GLM&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#29992;&#28857;&#27169;&#22411;&#65288;GPM&#65289;&#30340;&#27169;&#22411;&#65292;&#23427;&#26080;&#32541;&#22320;&#23558;&#33258;&#32534;&#30721;&#21644;&#33258;&#22238;&#24402;&#20219;&#21153;&#25972;&#21512;&#21040;&#28857;&#20113;&#21464;&#25442;&#22120;&#20013;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#22810;&#21151;&#33021;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#19979;&#28216;&#28857;&#20113;&#34920;&#31034;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#20197;&#21450;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;GPM&#36890;&#36807;&#21508;&#31181;&#24418;&#24335;&#30340;&#25513;&#30721;&#22635;&#20805;&#20219;&#21153;&#22686;&#24378;&#20102;&#33258;&#32534;&#30721;&#20013;&#30340;&#25513;&#30721;&#39044;&#27979;&#65292;&#22312;&#28857;&#20113;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;GPM&#22312;&#26080;&#26465;&#20214;&#28857;&#20113;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#29978;&#33267;&#36890;&#36807;&#20462;&#25913;&#36755;&#20837;&#30340;&#26465;&#20214;&#20449;&#24687;&#23637;&#31034;&#20102;&#26377;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training architectures of large language models encompass various types, including autoencoding models, autoregressive models, and encoder-decoder models. We posit that any modality can potentially benefit from a large language model, as long as it undergoes vector quantization to become discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which seamlessly integrates autoencoding and autoregressive tasks in point cloud transformer. This model is versatile, allowing fine-tuning for downstream point cloud representation tasks, as well as unconditional and conditional generation tasks. GPM enhances masked prediction in autoencoding through various forms of mask padding tasks, leading to improved performance in point cloud understanding. Additionally, GPM demonstrates highly competitive results in unconditional point cloud generation tasks, even exhibiting the potential for conditional generation tasks by modifying the input's conditional information. Compared t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25299;&#25169;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#25913;&#21892;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;MRI&#22270;&#20687;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#31934;&#24230;&#65292;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#35786;&#26029;&#20855;&#26377;&#28508;&#22312;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.16857</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;MRI&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Improvement in Alzheimer's Disease MRI Images Analysis by Convolutional Neural Networks Via Topological Optimization. (arXiv:2310.16857v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16857
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#25913;&#21892;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;MRI&#22270;&#20687;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#31934;&#24230;&#65292;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#35786;&#26029;&#20855;&#26377;&#28508;&#22312;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20613;&#37324;&#21494;&#25299;&#25169;&#20248;&#21270;&#25216;&#26415;&#65292;&#25913;&#21892;MRI&#22270;&#20687;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#36890;&#36807;&#20248;&#21270;&#36793;&#30028;&#22686;&#24378;&#12289;&#23545;&#27604;&#24230;&#35843;&#25972;&#21644;&#20142;&#24230;&#35843;&#25972;&#31561;&#25216;&#26415;&#65292;&#25913;&#21892;&#20102;&#33041;&#37096;&#32467;&#26500;&#30340;&#28165;&#26224;&#24230;&#65292;&#20943;&#23569;&#20102;&#22122;&#22768;&#24182;&#25552;&#39640;&#20102;&#23545;&#27604;&#24230;&#12290;&#20351;&#29992;VGG16&#12289;ResNet50&#12289;InceptionV3&#21644;Xception&#31561;CNN&#26550;&#26500;&#36827;&#34892;&#21518;&#20248;&#21270;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#23558;&#20613;&#37324;&#21494;&#25299;&#25169;&#20248;&#21270;&#19982;CNN&#30456;&#32467;&#21512;&#65292;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#31934;&#32454;&#20998;&#31867;&#20855;&#26377;&#28508;&#22312;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#26377;&#21161;&#20110;&#20854;&#35786;&#26029;&#21442;&#25968;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research underscores the efficacy of Fourier topological optimization in refining MRI imagery, thereby bolstering the classification precision of Alzheimer's Disease through convolutional neural networks. Recognizing that MRI scans are indispensable for neurological assessments, but frequently grapple with issues like blurriness and contrast irregularities, the deployment of Fourier topological optimization offered enhanced delineation of brain structures, ameliorated noise, and superior contrast. The applied techniques prioritized boundary enhancement, contrast and brightness adjustments, and overall image lucidity. Employing CNN architectures VGG16, ResNet50, InceptionV3, and Xception, the post-optimization analysis revealed a marked elevation in performance. Conclusively, the amalgamation of Fourier topological optimization with CNNs delineates a promising trajectory for the nuanced classification of Alzheimer's Disease, portending a transformative impact on its diagnostic para
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;LSTM&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#22312;FPGA&#19978;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#36739;&#20302;&#30340;&#33021;&#32791;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#21644;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.16842</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#20915;&#23884;&#20837;&#24335;FPGA&#20013;LSTM&#21333;&#20803;&#30340;&#21534;&#21520;&#37327;&#29942;&#39048;&#65292;&#22686;&#24378;&#33021;&#25928;
&lt;/p&gt;
&lt;p&gt;
Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;LSTM&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#22312;FPGA&#19978;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#36739;&#20302;&#30340;&#33021;&#32791;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#21644;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20013;&#22788;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#23884;&#20837;&#24335;&#28145;&#24230;&#23398;&#20064;&#23545;&#20110;&#19968;&#32500;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#12290;&#36807;&#21435;&#65292;&#32463;&#24120;&#20351;&#29992;CNN&#22240;&#20026;&#23427;&#20204;&#23545;&#20110;&#29305;&#27530;&#30340;&#23884;&#20837;&#24335;&#30828;&#20214;&#27604;&#22914;FPGA&#26469;&#35828;&#24456;&#23481;&#26131;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#22411;LSTM&#21333;&#20803;&#20248;&#21270;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#30340;&#31616;&#21333;LSTM&#27169;&#22411;&#22312;FPGA XC7S15&#65288;&#26469;&#33258;Spartan-7&#31995;&#21015;&#65289;&#19978;&#27599;&#31186;&#21487;&#23454;&#29616;17534&#20010;&#25512;&#26029;&#65292;&#20165;&#28040;&#32791;&#27599;&#20010;&#25512;&#26029;3.8&#24494;&#28966;&#32819;&#30340;&#33021;&#37327;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#30340;&#21534;&#21520;&#37327;&#33267;&#23569;&#25552;&#39640;&#20102;5.4&#20493;&#65292;&#33021;&#25928;&#25552;&#39640;&#20102;1.37&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To process sensor data in the Internet of Things(IoTs), embedded deep learning for 1-dimensional data is an important technique. In the past, CNNs were frequently used because they are simple to optimise for special embedded hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed at energy-efficient inference on end devices. Using the traffic speed prediction as a case study, a vanilla LSTM model with the optimised LSTM cell achieves 17534 inferences per second while consuming only 3.8 $\mu$J per inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy efficient than existing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#27969;&#26143;&#26816;&#27979;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#21253;&#21547;&#38745;&#24577;&#20803;&#32032;&#30340;&#22270;&#20687;&#20013;&#26816;&#27979;&#27969;&#26143;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#26041;&#27861;&#20934;&#30830;&#22320;&#23450;&#20301;&#27969;&#26143;&#30340;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.16826</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#26143;&#30417;&#27979;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#65306;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#21644;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Deep machine learning for meteor monitoring: advances with transfer learning and gradient-weighted class activation mapping. (arXiv:2310.16826v2 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#27969;&#26143;&#26816;&#27979;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#21253;&#21547;&#38745;&#24577;&#20803;&#32032;&#30340;&#22270;&#20687;&#20013;&#26816;&#27979;&#27969;&#26143;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#26041;&#27861;&#20934;&#30830;&#22320;&#23450;&#20301;&#27969;&#26143;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20809;&#23398;&#25506;&#27979;&#31995;&#32479;&#22312;&#27969;&#26143;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#22823;&#24133;&#22686;&#21152;&#65292;&#23548;&#33268;&#25968;&#25454;&#37327;&#22823;&#24133;&#22686;&#21152;&#12290;&#33258;&#21160;&#21270;&#30340;&#27969;&#26143;&#26816;&#27979;&#24037;&#20855;&#23545;&#20110;&#30740;&#31350;&#36830;&#32493;&#30340;&#27969;&#26143;&#20837;&#23556;&#36890;&#37327;&#12289;&#23547;&#25214;&#26032;&#40092;&#38504;&#30707;&#21644;&#26356;&#22909;&#22320;&#20102;&#35299;&#25105;&#20204;&#30340;&#22826;&#38451;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#20851;&#20110;&#27969;&#26143;&#26816;&#27979;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#25163;&#24037;&#36827;&#34892;&#27969;&#26143;&#21644;&#38750;&#27969;&#26143;&#22270;&#20687;&#30340;&#20551;&#38451;&#24615;&#21306;&#20998;&#65292;&#36825;&#22312;&#26102;&#38388;&#19978;&#21313;&#20998;&#32791;&#36153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#20998;&#31867;&#20505;&#36873;&#27969;&#26143;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#33021;&#22815;&#21363;&#20351;&#22312;&#21253;&#21547;&#20113;&#12289;&#26376;&#20142;&#21644;&#24314;&#31569;&#31561;&#38745;&#24577;&#20803;&#32032;&#30340;&#22270;&#20687;&#20013;&#65292;&#20063;&#33021;&#22815;&#26816;&#27979;&#21040;&#27969;&#26143;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#22312;&#27599;&#20010;&#24103;&#20013;&#23450;&#20301;&#27969;&#26143;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;Grad-CAM&#65289;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22810;&#36890;&#36947;&#26799;&#24230;&#20449;&#21495;&#26469;&#36741;&#21161;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent decades, the use of optical detection systems for meteor studies has increased dramatically, resulting in huge amounts of data being analyzed. Automated meteor detection tools are essential for studying the continuous meteoroid incoming flux, recovering fresh meteorites, and achieving a better understanding of our Solar System. Concerning meteor detection, distinguishing false positives between meteor and non-meteor images has traditionally been performed by hand, which is significantly time-consuming. To address this issue, we developed a fully automated pipeline that uses Convolutional Neural Networks (CNNs) to classify candidate meteor detections. Our new method is able to detect meteors even in images that contain static elements such as clouds, the Moon, and buildings. To accurately locate the meteor within each frame, we employ the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. This method facilitates the identification of the region of interest by mul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#35843;&#25972;&#20197;&#23454;&#29616;&#24179;&#28369;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16779</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#35843;&#25972;&#20197;&#23454;&#29616;&#24179;&#28369;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#38543;&#26426;&#24179;&#28369;&#24050;&#25104;&#20026;&#23569;&#25968;&#20960;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#8220;&#21435;&#22122;&#21644;&#20998;&#31867;&#8221;&#27969;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#21435;&#22122;&#24179;&#28369;&#65292;&#22312;&#20219;&#20309;&#20998;&#31867;&#22120;&#19978;&#25191;&#34892;&#38543;&#26426;&#24179;&#28369;&#65292;&#21069;&#25552;&#26159;&#26377;&#19968;&#20010;&#20934;&#30830;&#30340;&#21435;&#22122;&#22120;&#21487;&#29992;&#65292;&#27604;&#22914;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#36136;&#30097;&#21738;&#31181;&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#31034;&#24418;&#24335;&#33021;&#22815;&#26368;&#22823;&#21270;&#21435;&#22122;&#24179;&#28369;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#65292;&#26088;&#22312;&#23454;&#29616;&#20849;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#24179;&#28369;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#21516;&#26102;&#20063;&#20026;&#20854;&#35748;&#35777;&#40065;&#26834;&#24615;&#34917;&#20607;&#20934;&#30830;&#24230;&#30340;&#25104;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, e.g., those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called denoised smoothing, given that an accurate denoiser is available - such as diffusion model. In this paper, we investigate the trade-off between accuracy and certified robustness of denoised smoothing: for example, we question on which representation of diffusion model would maximize the certified robustness of denoised smoothing. We consider a new objective that aims collective robustness of smoothed classifiers across multiple noise levels at a shared diffusion model, which also suggests a new way to compensate the cost of accuracy in randomized smoothing for its certified robustness. This objective motivates us to fine-tune diffusion model (a) to perform consistent de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#38459;&#22622;&#20316;&#20026;&#25511;&#21046;&#21629;&#20196;&#39044;&#27979;&#21644;&#29992;&#25143;&#36710;&#36742;&#34892;&#20026;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#23618;&#35299;&#37322;&#39034;&#24207;&#39550;&#39542;&#22330;&#26223;&#65292;&#21516;&#26102;&#33719;&#24471;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16639</link><description>&lt;p&gt;
&#39550;&#39542;&#36890;&#36807;&#27010;&#24565;&#38459;&#22622;&#65306;&#35299;&#24320;&#21487;&#35299;&#37322;&#24615;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks. (arXiv:2310.16639v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#38459;&#22622;&#20316;&#20026;&#25511;&#21046;&#21629;&#20196;&#39044;&#27979;&#21644;&#29992;&#25143;&#36710;&#36742;&#34892;&#20026;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#23618;&#35299;&#37322;&#39034;&#24207;&#39550;&#39542;&#22330;&#26223;&#65292;&#21516;&#26102;&#33719;&#24471;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27010;&#24565;&#38459;&#22622;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#19968;&#32452;&#20154;&#20026;&#23450;&#20041;&#30340;&#27010;&#24565;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#20154;&#31867;&#36741;&#21161;&#25110;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#25509;&#21463;&#21644;&#29702;&#35299;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25152;&#20570;&#30340;&#20915;&#31574;&#65292;&#24182;&#29992;&#20110;&#21512;&#29702;&#21270;&#21644;&#35299;&#37322;&#39550;&#39542;&#21592;&#25110;&#36710;&#36742;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27010;&#24565;&#38459;&#22622;&#20316;&#20026;&#25511;&#21046;&#21629;&#20196;&#39044;&#27979;&#21644;&#29992;&#25143;&#36710;&#36742;&#34892;&#20026;&#35299;&#37322;&#30340;&#21487;&#35270;&#29305;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#23618;&#65292;&#29992;&#20110;&#35299;&#37322;&#39034;&#24207;&#39550;&#39542;&#22330;&#26223;&#21516;&#26102;&#23398;&#20064;&#36710;&#36742;&#30340;&#25511;&#21046;&#21629;&#20196;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#30830;&#23450;&#20154;&#31867;&#65288;&#25110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65289;&#23545;&#39318;&#36873;&#32541;&#38553;&#25110;&#36716;&#21521;&#21629;&#20196;&#30340;&#25913;&#21464;&#26159;&#21542;&#30001;&#22806;&#37096;&#21050;&#28608;&#25110;&#20559;&#22909;&#30340;&#25913;&#21464;&#25152;&#24341;&#23548;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human-understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#30340;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#32597;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#20559;&#24046;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.16638</link><description>&lt;p&gt;
&#36866;&#24212;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Covariate Shift Adaptation Robust to Density-Ratio Estimation. (arXiv:2310.16638v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#30340;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#32597;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#20559;&#24046;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#20855;&#26377;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#27979;&#35797;&#25968;&#25454;&#21482;&#21253;&#21547;&#21327;&#21464;&#37327;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#35757;&#32451;&#21442;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#20998;&#24067;&#19981;&#21516;&#12290;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#23494;&#24230;&#27604;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#26469;&#36827;&#34892;&#21327;&#21464;&#37327;&#20559;&#31227;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#25439;&#22833;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#27599;&#20010;&#26435;&#37325;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#65292;&#20197;&#36817;&#20284;&#27979;&#35797;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#23613;&#31649;&#23427;&#20801;&#35768;&#25105;&#20204;&#33719;&#24471;&#19968;&#20010;&#26368;&#23567;&#21270;&#27979;&#35797;&#25968;&#25454;&#39118;&#38505;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23494;&#24230;&#27604;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#35823;&#24046;&#20063;&#20250;&#23548;&#33268;&#22238;&#24402;&#27169;&#22411;&#30340;&#20272;&#35745;&#22120;&#20135;&#29983;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a scenario where we have access to train data with both covariates and outcomes while test data only contains covariates. In this scenario, our primary aim is to predict the missing outcomes of the test data. With this objective in mind, we train parametric regression models under a covariate shift, where covariate distributions are different between the train and test data. For this problem, existing studies have proposed covariate shift adaptation via importance weighting using the density ratio. This approach averages the train data losses, each weighted by an estimated ratio of the covariate densities between the train and test data, to approximate the test-data risk. Although it allows us to obtain a test-data risk minimizer, its performance heavily relies on the accuracy of the density ratio estimation. Moreover, even if the density ratio can be consistently estimated, the estimation errors of the density ratio also yield bias in the estimators of the regression model's 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#29305;&#24449;&#23545;&#27604;&#25439;&#22833;&#23454;&#29616;&#25968;&#25454;&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15890</link><description>&lt;p&gt;
&#36328;&#29305;&#24449;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2310.15890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#29305;&#24449;&#23545;&#27604;&#25439;&#22833;&#23454;&#29616;&#25968;&#25454;&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#22823;&#22810;&#25968;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20998;&#25955;&#24335;&#25968;&#25454;&#38598;&#22312;&#20195;&#29702;&#20043;&#38388;&#21487;&#20197;&#20855;&#26377;&#26174;&#33879;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#29305;&#24449;&#19978;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#25439;&#22833;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#23545;&#20110;&#19968;&#23545;&#30456;&#37051;&#20195;&#29702;&#65292;&#36328;&#29305;&#24449;&#26159;&#20174;&#19968;&#20010;&#20195;&#29702;&#30340;&#25968;&#25454;&#33719;&#21462;&#30340;&#29305;&#24449;&#65288;&#21363;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;&#28608;&#27963;&#65289;&#20851;&#20110;&#21478;&#19968;&#20010;&#20195;&#29702;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#35814;&#23613;&#30340;&#23454;&#39564;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#12289;CIFAR-100&#12289;Fashion MNIST &#21644; ImageNet&#65289;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#32593;&#32476;&#25299;&#25169;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.15694</link><description>&lt;p&gt;
COPF: &#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#25216;&#26415;&#26159;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;RLHF&#30340;LM&#22312;&#24341;&#20837;&#26032;&#30340;&#26597;&#35810;&#25110;&#21453;&#39304;&#26102;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20154;&#31867;&#20559;&#22909;&#22312;&#19981;&#21516;&#39046;&#22495;&#25110;&#20219;&#21153;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#30001;&#20110;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#37325;&#26032;&#35757;&#32451;LM&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#23454;&#38469;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#65288;COPF&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#27861;&#20272;&#35745;&#19968;&#31995;&#21015;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#20989;&#25968;&#27491;&#21017;&#21270;&#19981;&#26029;&#25311;&#21512;&#31574;&#30053;&#24207;&#21015;&#12290;COPF&#21253;&#21547;&#19968;&#20010;&#21333;&#19968;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14814</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#23545;&#27169;&#22411;&#33258;&#20449;&#24230;&#39640;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#20351;&#29992;softmax&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#33258;&#20449;&#24230;&#24230;&#37327;&#65292;&#23613;&#31649;&#24050;&#30693;&#23427;&#20204;&#23545;&#38169;&#35823;&#39044;&#27979;&#20063;&#36807;&#20110;&#33258;&#20449;&#12290;&#24403;&#25968;&#25454;&#26631;&#27880;&#21463;&#21040;&#26576;&#31181;&#32422;&#26463;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20026;&#26126;&#26174;&#65292;&#21363;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;$\mathcal{T}$-&#30456;&#20284;&#24230;&#65292;&#23427;&#22522;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#31283;&#23450;&#28857;&#24182;&#25551;&#36848;&#21333;&#20010;&#25104;&#21592;&#30340;&#22810;&#26679;&#24615;&#19982;&#20854;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25552;&#20379;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#33258;&#20449;&#24230;&#24230;&#37327;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#26679;&#21270;&#30340;&#24322;&#24120;&#20540;&#26333;&#20809;&#65288;DivOE&#65289;&#8221;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#12289;&#20449;&#24687;&#21270;&#30340;&#36741;&#21161;&#24322;&#24120;&#20540;&#26469;&#36827;&#34892;&#26377;&#25928;&#30340;OOD&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20449;&#24687;&#22806;&#25512;&#30340;&#26041;&#24335;&#21512;&#25104;&#20102;&#26356;&#22810;&#30340;&#24322;&#24120;&#20540;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13923</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#22806;&#25512;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#24322;&#24120;&#20540;&#26333;&#20809;&#36827;&#34892;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation. (arXiv:2310.13923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#26679;&#21270;&#30340;&#24322;&#24120;&#20540;&#26333;&#20809;&#65288;DivOE&#65289;&#8221;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#12289;&#20449;&#24687;&#21270;&#30340;&#36741;&#21161;&#24322;&#24120;&#20540;&#26469;&#36827;&#34892;&#26377;&#25928;&#30340;OOD&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20449;&#24687;&#22806;&#25512;&#30340;&#26041;&#24335;&#21512;&#25104;&#20102;&#26356;&#22810;&#30340;&#24322;&#24120;&#20540;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#37096;&#32626;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24322;&#24120;&#20540;&#26333;&#20809;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#36890;&#36807;&#20197;&#20449;&#24687;&#21270;&#37319;&#26679;&#30340;&#36741;&#21161;&#24322;&#24120;&#20540;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#22312;OOD&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20551;&#35774;&#25910;&#38598;&#21040;&#30340;&#24322;&#24120;&#20540;&#21487;&#20197;&#36275;&#22815;&#22823;&#19988;&#20195;&#34920;&#24615;&#22320;&#35206;&#30422;ID&#21644;OOD&#25968;&#25454;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#8220;&#22810;&#26679;&#21270;&#30340;&#24322;&#24120;&#20540;&#26333;&#20809;&#8221;&#65288;DivOE&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#32473;&#23450;&#30340;&#36741;&#21161;&#24322;&#24120;&#20540;&#36827;&#34892;&#22522;&#20110;&#20449;&#24687;&#22806;&#25512;&#30340;&#26377;&#25928;OOD&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DivOE&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26174;&#24335;&#22320;&#21512;&#25104;&#26356;&#22810;&#20449;&#24687;&#21270;&#30340;&#24322;&#24120;&#20540;&#26469;&#20351;&#36741;&#21161;&#20998;&#24067;&#22810;&#26679;&#21270;&#12290;&#23427;&#21033;&#29992;&#20102;&#22810;&#27493;&#20248;&#21270;&#26041;&#27861;&#26469;&#29983;&#25104;&#36229;&#20986;&#21407;&#22987;&#24322;&#24120;&#20540;&#30340;&#26032;&#24322;&#24120;&#20540;&#65292;&#36825;&#19982;&#35768;&#22810;&#24322;&#24120;&#20540;&#26333;&#20809;&#30340;&#21464;&#20307;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier ex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPM-Solver-v3&#65292;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#27169;&#22411;&#32479;&#35745;&#30340;&#26032;&#22411;&#24555;&#36895;ODE&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20248;&#21270;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20197;&#20943;&#23567;ODE&#35299;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#22810;&#27493;&#26041;&#27861;&#21644;&#39044;&#27979;-&#26657;&#27491;&#26694;&#26550;&#26469;&#36827;&#19968;&#27493;&#25913;&#21892;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.13268</link><description>&lt;p&gt;
DPM-Solver-v3: &#20351;&#29992;&#32463;&#39564;&#27169;&#22411;&#32479;&#35745;&#25913;&#36827;&#30340;&#25193;&#25955;ODE&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics. (arXiv:2310.13268v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPM-Solver-v3&#65292;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#27169;&#22411;&#32479;&#35745;&#30340;&#26032;&#22411;&#24555;&#36895;ODE&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20248;&#21270;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20197;&#20943;&#23567;ODE&#35299;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#22810;&#27493;&#26041;&#27861;&#21644;&#39044;&#27979;-&#26657;&#27491;&#26694;&#26550;&#26469;&#36827;&#19968;&#27493;&#25913;&#21892;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21033;&#29992;DPMs&#30340;&#29305;&#23450;ODE&#24418;&#24335;&#30340;&#24555;&#36895;ODE&#27714;&#35299;&#22120;&#26469;&#21152;&#36895;&#37319;&#26679;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#39640;&#24230;&#20381;&#36182;&#29305;&#23450;&#21442;&#25968;&#21270;&#65288;&#22914;&#22122;&#22768;/&#25968;&#25454;&#39044;&#27979;&#65289;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#20197;&#33719;&#24471;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#26368;&#20339;&#21442;&#25968;&#21270;&#65292;&#20197;&#26368;&#23567;&#21270;ODE&#35299;&#30340;&#19968;&#38454;&#31163;&#25955;&#21270;&#38169;&#35823;&#12290;&#22522;&#20110;&#36825;&#31181;&#20844;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#39640;&#25928;&#35745;&#31639;&#30340;&#31995;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DPM-Solver-v3&#30340;&#26032;&#24555;&#36895;ODE&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#32463;&#39564;&#27169;&#22411;&#32479;&#35745;&#8221;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32467;&#21512;&#20102;&#22810;&#27493;&#26041;&#27861;&#21644;&#39044;&#27979;-&#26657;&#27491;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25913;&#21892;&#22312;&#23569;&#37327;&#20989;&#25968;&#35780;&#20272;&#65288;NFE&#65289;&#25110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) have exhibited excellent performance for high-fidelity image generation while suffering from inefficient sampling. Recent works accelerate the sampling procedure by proposing fast ODE solvers that leverage the specific ODE form of DPMs. However, they highly rely on specific parameterization during inference (such as noise/data prediction), which might not be the optimal choice. In this work, we propose a novel formulation towards the optimal parameterization during sampling that minimizes the first-order discretization error of the ODE solution. Based on such formulation, we propose \textit{DPM-Solver-v3}, a new fast ODE solver for DPMs by introducing several coefficients efficiently computed on the pretrained model, which we call \textit{empirical model statistics}. We further incorporate multistep methods and a predictor-corrector framework, and propose some techniques for improving sample quality at small numbers of function evaluations (NFE) or
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F2GNN&#30340;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#22686;&#24378;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.12350</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#32467;&#26500;&#24863;&#30693;&#32676;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equipping Federated Graph Neural Networks with Structure-aware Group Fairness. (arXiv:2310.12350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F2GNN&#30340;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#22686;&#24378;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#22270;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#20219;&#21153;&#12290;&#30001;&#20110;&#38544;&#31169;&#21644;&#30417;&#31649;&#38480;&#21046;&#65292;&#23545;&#38598;&#20013;&#24335;&#22270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#36235;&#21183;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GNN&#21487;&#33021;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#21382;&#21490;&#20559;&#35265;&#24182;&#23548;&#33268;&#27495;&#35270;&#24615;&#39044;&#27979;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#23616;&#37096;&#27169;&#22411;&#30340;&#20559;&#35265;&#24456;&#23481;&#26131;&#20256;&#25773;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#32473;&#22312;&#32852;&#37030;GNN&#20013;&#20943;&#36731;&#20559;&#35265;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;F2GNN&#65292;&#19968;&#31181;&#22686;&#24378;&#32852;&#37030;GNN&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#20559;&#35265;&#21487;&#33021;&#26469;&#33258;&#25968;&#25454;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;F2GNN&#26088;&#22312;&#22312;&#32852;&#37030;&#29615;&#22659;&#19979;&#20943;&#23569;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05858</link><description>&lt;p&gt;
DSAC-T: &#24102;&#26377;&#19977;&#20010;&#25913;&#36827;&#30340;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#8212;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#36807;&#20272;&#35745;&#38382;&#39064;&#25152;&#24341;&#36215;&#30340;&#12290;&#20316;&#20026;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#35780;&#35770;&#32773;&#65288;DSAC&#25110;DSAC-v1&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#30340;&#39640;&#26031;&#20540;&#20998;&#24067;&#26469;&#26377;&#25928;&#25552;&#39640;&#20540;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;DSAC&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#26102;&#32780;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#32553;&#25918;&#30340;&#38656;&#27714;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#22312;&#19968;&#20123;&#29305;&#27530;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19977;&#20010;&#23545;&#26631;&#20934;DSAC&#30340;&#37325;&#35201;&#25913;&#36827;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#25913;&#36827;&#21253;&#25324;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#12290;&#20462;&#25913;&#21518;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31216;&#20026;DSAC-T&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#26041;&#24046;&#20943;&#23569;&#30340; Halpern &#36845;&#20195;&#26469;&#20248;&#21270;&#26377;&#38480;&#21644;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#27714;&#35299;&#36807;&#31243;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.02987</link><description>&lt;p&gt;
&#26041;&#24046;&#20943;&#23569;&#30340; Halpern &#36845;&#20195;&#22312;&#26377;&#38480;&#21644;&#21333;&#35843;&#21253;&#21547;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions. (arXiv:2310.02987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02987
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#26041;&#24046;&#20943;&#23569;&#30340; Halpern &#36845;&#20195;&#26469;&#20248;&#21270;&#26377;&#38480;&#21644;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#27714;&#35299;&#36807;&#31243;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#23545;&#25239;&#31283;&#20581;&#24615;&#25110;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24341;&#21457;&#20102;&#35299;&#20915;&#21338;&#24328;&#22343;&#34913;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#20855;&#26377;&#21487;&#35745;&#31639;&#36924;&#36817;&#35823;&#24046;&#30340;&#26041;&#27861;&#38750;&#24120;&#29702;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#21487;&#39564;&#35777;&#30340;&#32456;&#27490;&#20934;&#21017;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#25311;&#24191;&#27867;&#31867;&#21035;&#22343;&#34913;&#38382;&#39064;&#30340;&#26377;&#38480;&#21644;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340; Halpern &#36845;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#20943;&#23569;&#33719;&#24471;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#20445;&#35777;&#65292;&#22312;&#26377;&#38480;&#21644;&#30340; $n$ &#20010;&#32452;&#25104;&#25805;&#20316;&#31526;&#20013;&#65292;&#8220;&#24179;&#22343;&#8221;&#22320;&#26159;&#20114;&#34917;&#21327;&#21516;&#25110;Lipschitz&#36830;&#32493;&#21644;&#21333;&#35843;&#65292;&#21442;&#25968;&#20026; $L$&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#39044;&#27979;&#20102;&#26368;&#21518;&#30340;&#36845;&#20195;&#21644;&#19968;&#20010;&#65288;compu&#65289;
&lt;/p&gt;
&lt;p&gt;
Machine learning approaches relying on such criteria as adversarial robustness or multi-agent settings have raised the need for solving game-theoretic equilibrium problems. Of particular relevance to these applications are methods targeting finite-sum structure, which generically arises in empirical variants of learning problems in these contexts. Further, methods with computable approximation errors are highly desirable, as they provide verifiable exit criteria. Motivated by these applications, we study finite-sum monotone inclusion problems, which model broad classes of equilibrium problems. Our main contributions are variants of the classical Halpern iteration that employ variance reduction to obtain improved complexity guarantees in which $n$ component operators in the finite sum are ``on average'' either cocoercive or Lipschitz continuous and monotone, with parameter $L$. The resulting oracle complexity of our methods, which provide guarantees for the last iterate and for a (compu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;</title><link>http://arxiv.org/abs/2310.02255</link><description>&lt;p&gt;
MathVista: &#29992;GPT-4V&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20102;&#19981;&#21516;&#25968;&#23398;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#25361;&#25112;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;6141&#20010;&#20363;&#23376;&#65292;&#20854;&#20013;&#26377;28&#20010;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;3&#20010;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65288;&#21363;IQTest&#12289;FunctionQA&#21644;PaperQA&#65289;&#12290;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#31934;&#32454;&#30340;&#12289;&#28145;&#20837;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#32452;&#21512;&#25512;&#29702;&#65292;&#36825;&#20123;&#37117;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;MathVista&#65292;&#25105;&#20204;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#20026;49.9%&#65292;&#26126;&#26174;&#20248;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#65292;&#30456;&#24046;15.1%&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#25321;&#33021;&#21147;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;Top-$k$&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36138;&#23146;&#31639;&#27861;&#21644;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01551</link><description>&lt;p&gt;
&#21033;&#29992;&#36873;&#25321;&#30340;&#33021;&#21147;&#20248;&#21270;&#20915;&#31574;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#25321;&#33021;&#21147;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;Top-$k$&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36138;&#23146;&#31639;&#27861;&#21644;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23545;&#26631;&#20934;&#21644;&#32463;&#39564;&#25104;&#21151;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;ID3&#12289;C4.5&#21644;CART&#65289;&#36827;&#34892;&#25512;&#24191;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20973;&#20511;&#36138;&#23146;&#30340;&#29305;&#24615;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65306;&#23427;&#20204;&#36890;&#36807;&#36845;&#20195;&#22320;&#22522;&#20110;&#26368;&#20339;&#23646;&#24615;&#36827;&#34892;&#21010;&#20998;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;Top-$k$&#21017;&#32771;&#34385;$k$&#20010;&#26368;&#20339;&#23646;&#24615;&#20316;&#20026;&#21487;&#33021;&#30340;&#21010;&#20998;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#26368;&#20339;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#25512;&#24191;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#8220;&#36138;&#23146;&#23618;&#27425;&#23450;&#29702;&#8221;&#65292;&#23545;&#20110;&#27599;&#20010;$k \in \mathbb{N}$&#65292;Top-$(k+1)$&#27604;Top-$k$&#26356;&#21152;&#24378;&#22823;&#65306;&#22312;&#26576;&#20123;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21069;&#32773;&#21487;&#20197;&#36798;&#21040;$1-\varepsilon$&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#21518;&#32773;&#21482;&#33021;&#36798;&#21040;$\frac1{2}+\varepsilon$&#30340;&#20934;&#30830;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;Top-$k$&#31639;&#27861;&#22312;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#20248;&#20110;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#32463;&#20856;&#36138;&#23146;&#31639;&#27861;&#21644;&#36739;&#26032;&#30340;&#8220;&#26368;&#20248;&#20915;&#31574;&#26641;&#8221;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a {\sl greediness hierarchy theorem} showing that for every $k \in \mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\varepsilon$, whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent "optimal decis
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;NDCG&#20248;&#21270;&#26469;&#35299;&#20915;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00270</link><description>&lt;p&gt;
SpatialRank: &#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#19982;NDCG&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data. (arXiv:2310.00270v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;NDCG&#20248;&#21270;&#26469;&#35299;&#20915;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#20132;&#36890;&#20107;&#25925;&#21644;&#29359;&#32618;&#20107;&#20214;&#65289;&#30340;&#39118;&#38505;&#26368;&#39640;&#30340;&#21069;k&#20010;&#22320;&#28857;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20844;&#20849;&#23433;&#20840;&#21644;&#22478;&#24066;&#31649;&#29702;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#28857;&#20043;&#38388;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#31354;&#38388;&#20013;&#22478;&#24066;&#20107;&#20214;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#65292;&#20197;&#21450;&#27491;&#30830;&#23545;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#38468;&#36817;&#22320;&#28857;&#36827;&#34892;&#25490;&#21517;&#30340;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#20027;&#35201;&#26088;&#22312;&#20934;&#30830;&#39044;&#27979;&#25152;&#26377;&#22320;&#28857;&#30340;&#23454;&#38469;&#39118;&#38505;&#24471;&#20998;&#25110;&#20107;&#20214;&#35745;&#25968;&#12290;&#30001;&#20110;&#39044;&#27979;&#38169;&#35823;&#65292;&#30001;&#27492;&#24471;&#21040;&#30340;&#25490;&#21517;&#36890;&#24120;&#36136;&#37327;&#36739;&#20302;&#12290;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#35832;&#22914;&#26631;&#20934;&#21270;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;NDCG&#65289;&#20043;&#31867;&#30340;&#25351;&#26631;&#65292;&#20294;&#19981;&#33021;&#22788;&#29702;&#22320;&#28857;&#20043;&#38388;&#23384;&#22312;&#30340;&#26102;&#31354;&#33258;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of urban event ranking aims at predicting the top-k most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named Spati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#23569;&#37327;&#25968;&#25454;&#28857;&#34987;&#25490;&#38500;&#21518;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#35760;&#24518;&#21644;&#20449;&#24687;&#27844;&#28431;&#30340;&#26377;&#25928;&#34913;&#37327;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.17310</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#27425;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Leave-one-out Distinguishability in Machine Learning. (arXiv:2309.17310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17310
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#23569;&#37327;&#25968;&#25454;&#28857;&#34987;&#25490;&#38500;&#21518;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#35760;&#24518;&#21644;&#20449;&#24687;&#27844;&#28431;&#30340;&#26377;&#25928;&#34913;&#37327;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#23569;&#37327;&#25968;&#25454;&#28857;&#21518;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#27010;&#24565;&#23450;&#20041;&#20026;&#19968;&#27425;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;(LOOD)&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#21644;&#20449;&#24687;&#27844;&#28431;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#26469;&#24314;&#27169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#38543;&#26426;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20351;&#29992;&#24191;&#27867;&#30340;&#32463;&#39564;&#20998;&#26512;&#39564;&#35777;&#20102;LOOD&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20449;&#24687;&#27844;&#28431;&#30340;&#21407;&#22240;&#20197;&#21450;&#27844;&#28431;&#31243;&#24230;&#39640;&#30340;&#20301;&#32622;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#25968;&#25454;&#35760;&#24518;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#20248;&#21270;...
&lt;/p&gt;
&lt;p&gt;
We introduce a new analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This problem is key to measuring data **memorization** and **information leakage** in machine learning, and the **influence** of training data points on model predictions. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of information leakage using membership inference attacks. Our theoretical framework enables us to investigate the causes of information leakage and where the leakage is high. For example, we analyze the influence of activation functions, on data memorization. Additionally, our method allows us to optim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#12290;&#36890;&#36807;&#23545;&#22238;&#25253;&#26223;&#35266;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#22833;&#36133;&#21306;&#22495;&#21644;&#31574;&#30053;&#21697;&#36136;&#30340;&#38544;&#34255;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14597</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#25511;&#21046;&#20013;&#30340;&#22122;&#22768;&#37051;&#22495;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control. (arXiv:2309.14597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#12290;&#36890;&#36807;&#23545;&#22238;&#25253;&#26223;&#35266;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#22833;&#36133;&#21306;&#22495;&#21644;&#31574;&#30053;&#21697;&#36136;&#30340;&#38544;&#34255;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#24615;&#33021;&#19978;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#20174;&#30740;&#31350;&#31574;&#30053;&#21644;&#22238;&#25253;&#20043;&#38388;&#30340;&#26144;&#23556;&#21363;&#22238;&#25253;&#26223;&#35266;&#30340;&#35282;&#24230;&#20026;&#36825;&#20123;&#34892;&#20026;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27969;&#34892;&#30340;&#31639;&#27861;&#22312;&#36825;&#20010;&#26223;&#35266;&#30340;&#22122;&#22768;&#37051;&#22495;&#20013;&#31359;&#34892;&#65292;&#19968;&#20010;&#31574;&#30053;&#21442;&#25968;&#30340;&#21333;&#27425;&#26356;&#26032;&#20250;&#23548;&#33268;&#22238;&#25253;&#22312;&#24456;&#22823;&#33539;&#22260;&#20869;&#21464;&#21270;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#22238;&#25253;&#36827;&#34892;&#20998;&#24067;&#22788;&#29702;&#65292;&#25105;&#20204;&#23545;&#26223;&#35266;&#36827;&#34892;&#20102;&#26144;&#23556;&#65292;&#25551;&#36848;&#20102;&#31574;&#30053;&#31354;&#38388;&#20013;&#23481;&#26131;&#20135;&#29983;&#22833;&#36133;&#30340;&#21306;&#22495;&#65292;&#24182;&#25581;&#31034;&#20102;&#31574;&#30053;&#21697;&#36136;&#30340;&#38544;&#34255;&#32500;&#24230;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26223;&#35266;&#30340;&#24778;&#20154;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#25214;&#21040;&#31616;&#21333;&#30340;&#36335;&#24452;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#24320;&#22122;&#22768;&#37051;&#22495;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20248;&#21270;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time. In this work, we provide a fresh perspective on these behaviors by studying the return landscape: the mapping between a policy and a return. We find that popular algorithms traverse noisy neighborhoods of this landscape, in which a single update to the policy parameters leads to a wide range of returns. By taking a distributional view of these returns, we map the landscape, characterizing failure-prone regions of policy space and revealing a hidden dimension of policy quality. We show that the landscape exhibits surprising structure by finding simple paths in parameter space which improve the stability of a policy. To conclude, we develop a distribution-aware procedure which finds such paths, navigating away from noisy neighborhoods in order to improve the robustness of a policy. Taken together, our results provide new insight into the optimization, eva
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07593</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22797;&#26434;&#23398;&#20064;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#26102;&#65292;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;&#31227;&#38500;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;&#26159;&#21442;&#32771;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#32479;&#35745;&#20445;&#35777;&#26469;&#39564;&#35777;&#21464;&#37327;&#21253;&#21547;&#24615;&#26102;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#20351;&#29992;&#21464;&#37327;&#32622;&#25442;&#26041;&#26696;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23384;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26102;&#23481;&#26131;&#23558;&#19981;&#37325;&#35201;&#30340;&#21464;&#37327;&#35823;&#35782;&#21035;&#20026;&#37325;&#35201;&#21464;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#30740;&#31350;&#26465;&#20214;&#32622;&#25442;&#37325;&#35201;&#24615;&#65288;Conditional Permutation Importance&#65292;CPI&#65289;&#65292;&#23427;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;CPI&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;I&#22411;&#38169;&#35823;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#24403;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;CPI&#22987;&#32456;&#26174;&#31034;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variable importance assessment has become a crucial step in machine-learning applications when using complex learners, such as deep neural networks, on large-scale data. Removal-based importance assessment is currently the reference approach, particularly when statistical guarantees are sought to justify variable inclusion. It is often implemented with variable permutation schemes. On the flip side, these approaches risk misidentifying unimportant variables as important in the presence of correlations among covariates. Here we develop a systematic approach for studying Conditional Permutation Importance (CPI) that is model agnostic and computationally lean, as well as reusable benchmarks of state-of-the-art variable importance estimators. We show theoretically and empirically that $\textit{CPI}$ overcomes the limitations of standard permutation importance by providing accurate type-I error control. When used with a deep neural network, $\textit{CPI}$ consistently showed top accuracy ac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03004</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#22374;&#26497;&#23567;&#20540;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#35299;&#37322;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03004
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;MLP&#23618;&#20013;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#23454;&#35777;&#35266;&#23519;&#20026;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#23558;&#20854;&#24402;&#22240;&#20110;&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#20294;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#20165;&#38480;&#20110;&#27973;&#23618;&#32593;&#32476;&#12289;&#23567;&#35757;&#32451;&#27493;&#38271;&#20197;&#21450;&#20462;&#25913;&#30340;&#35757;&#32451;&#65292;&#23613;&#31649;&#36825;&#31181;&#31232;&#30095;&#24615;&#24050;&#22312;&#36890;&#36807;vanilla&#21327;&#35758;&#36827;&#34892;&#22823;&#27493;&#39588;&#35757;&#32451;&#30340;&#28145;&#23618;&#27169;&#22411;&#20013;&#34987;&#21457;&#29616;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19977;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#31232;&#30095;&#24615;&#30340;&#27010;&#24565;&#20316;&#20026;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28304;&#22836;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#35813;&#35299;&#37322;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;&#36825;&#20010;&#29702;&#35770;&#36866;&#29992;&#20110;&#32463;&#36807;LayerNorm&#26631;&#20934;&#35757;&#32451;&#30340;&#32431;MLP&#65292;&#24182;&#19988;&#22914;&#26524;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#26435;&#37325;&#28155;&#21152;&#22122;&#22768;&#65292;&#36824;&#36866;&#29992;&#20110;Transformers&#25110;&#20854;&#20182;&#26550;&#26500;&#12290;&#20026;&#20102;&#28040;&#38500;&#20854;&#20182;&#26469;&#28304;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources o
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;COMEDIAN&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26102;&#31354;Transformer&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#12290;&#22312;SoccerNet-v2&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.01270</link><description>&lt;p&gt;
COMEDIAN: &#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;Transformer&#36827;&#34892;&#21160;&#20316;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers. (arXiv:2309.01270v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01270
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;COMEDIAN&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26102;&#31354;Transformer&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#12290;&#22312;SoccerNet-v2&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;COMEDIAN&#65292;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#21021;&#22987;&#21270;&#26102;&#31354;Transformer&#36827;&#34892;&#21160;&#20316;&#23450;&#20301;&#30340;&#26032;&#22411;&#27969;&#31243;&#12290;&#21160;&#20316;&#23450;&#20301;&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#25139;&#32423;&#21035;&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65292;&#20854;&#20013;&#26377;&#20004;&#20010;&#21021;&#22987;&#21270;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#30701;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#23545;&#31354;&#38388;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#21021;&#22987;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21021;&#22987;&#21270;&#19968;&#20010;&#26102;&#24207;Transformer&#65292;&#36890;&#36807;&#19982;&#27599;&#20010;&#30701;&#35270;&#39057;&#29255;&#27573;&#23545;&#40784;&#30340;&#39044;&#35745;&#31639;&#29305;&#24449;&#24211;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#22686;&#24378;&#31354;&#38388;Transformer&#30340;&#36755;&#20986;&#19982;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#26368;&#21518;&#19968;&#27493;&#65292;&#25105;&#20204;&#23545;Transformer&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#12290;&#22312;SoccerNet-v2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;COMEDIAN&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#39044;&#35757;&#32451;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#25105;&#20204;&#39044;&#35757;&#32451;&#27969;&#31243;&#30340;&#20960;&#20010;&#20248;&#28857;&#65292;&#21253;&#25324;&#25913;&#36827;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present COMEDIAN, a novel pipeline to initialize spatiotemporal transformers for action spotting, which involves self-supervised learning and knowledge distillation. Action spotting is a timestamp-level temporal action detection task. Our pipeline consists of three steps, with two initialization stages. First, we perform self-supervised initialization of a spatial transformer using short videos as input. Additionally, we initialize a temporal transformer that enhances the spatial transformer's outputs with global context through knowledge distillation from a pre-computed feature bank aligned with each short video segment. In the final step, we fine-tune the transformers to the action spotting task. The experiments, conducted on the SoccerNet-v2 dataset, demonstrate state-of-the-art performance and validate the effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several advantages of our pretraining pipeline, including improved performance and faster convergence c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#20351;&#29992;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.13633</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30333;&#21270;&#65306;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adaptive whitening with fast gain modulation and slow synaptic plasticity. (arXiv:2308.13633v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#20351;&#29992;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24863;&#35273;&#21306;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#20854;&#20010;&#20307;&#21709;&#24212;&#30340;&#26041;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#20197;&#21450;&#20943;&#23569;&#21709;&#24212;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#36716;&#25442;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#30333;&#21270;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#30340;&#26426;&#21046;&#27169;&#22411;&#21482;&#20351;&#29992;&#31361;&#35302;&#21487;&#22609;&#24615;&#25110;&#22686;&#30410;&#35843;&#21046;&#20316;&#20026;&#36866;&#24212;&#30340;&#29983;&#29289;&#22522;&#36136;&#65292;&#28982;&#32780;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#26377;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#33539;&#24615;&#30340;&#22810;&#26102;&#38388;&#23610;&#24230;&#26426;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;&#31361;&#35302;&#21487;&#22609;&#24615;&#21644;&#22686;&#30410;&#35843;&#21046;&#30340;&#35745;&#31639;&#35282;&#33394;&#26469;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#30333;&#21270;&#12290;&#22686;&#30410;&#22312;&#24555;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#26681;&#25454;&#24403;&#21069;&#30340;&#32479;&#35745;&#24773;&#20917;&#36827;&#34892;&#35843;&#25972;&#65292;&#32780;&#31361;&#35302;&#22312;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#35843;&#25972;&#65292;&#23398;&#20064;&#36755;&#20837;&#32479;&#35745;&#20013;&#19982;&#24773;&#22659;&#26080;&#20851;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#33258;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26102;&#38388;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to learn structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#35299;&#20915;&#38750;&#23436;&#22791;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#39564;&#32467;&#26500;&#21644;Feynman-Kac&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07983</link><description>&lt;p&gt;
&#33945;&#29305;&#21345;&#27931;&#24341;&#23548;&#25193;&#25955;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#36870;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo guided Diffusion for Bayesian linear inverse problems. (arXiv:2308.07983v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#35299;&#20915;&#38750;&#23436;&#22791;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#39564;&#32467;&#26500;&#21644;Feynman-Kac&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#23436;&#22791;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#32463;&#24120;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#20174;&#35745;&#31639;&#25668;&#24433;&#21040;&#21307;&#23398;&#25104;&#20687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20351;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#65292;&#22312;&#22635;&#34917;&#38382;&#39064;&#20013;&#20135;&#29983;&#20855;&#26377;&#24863;&#30693;&#21512;&#29702;&#24615;&#30340;&#22270;&#20687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;SGM&#23450;&#20041;&#30340;&#20808;&#39564;&#32467;&#26500;&#26469;&#21046;&#23450;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#30340;&#24674;&#22797;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;Feynman-Kac&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25913;&#32534;&#33258;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110;&#24471;&#20998;&#25193;&#25955;&#30340;&#21069;&#21521;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;Feynman-Kac&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;MCGdiff&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#26681;&#25454;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#22312;&#22788;&#29702;&#38750;&#23436;&#22791;&#36870;&#38382;&#39064;&#26102;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems.
&lt;/p&gt;</description></item><item><title>&#23545;&#36807;&#21435;&#20845;&#21313;&#24180;&#38388;&#20855;&#26377;&#39640;&#24341;&#29992;&#21644;&#37325;&#35201;&#24433;&#21709;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35770;&#25991;&#12289;&#20316;&#32773;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#24182;&#21457;&#29616;&#20102;&#28909;&#38376;&#30740;&#31350;&#20027;&#39064;&#21644;&#26368;&#26032;&#28044;&#29616;&#30340;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00855</link><description>&lt;p&gt;
&#23545;&#36807;&#21435;&#20845;&#21313;&#24180;&#38388;&#20855;&#26377;&#39640;&#24341;&#29992;&#21644;&#37325;&#35201;&#24433;&#21709;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades. (arXiv:2308.00855v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00855
&lt;/p&gt;
&lt;p&gt;
&#23545;&#36807;&#21435;&#20845;&#21313;&#24180;&#38388;&#20855;&#26377;&#39640;&#24341;&#29992;&#21644;&#37325;&#35201;&#24433;&#21709;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35770;&#25991;&#12289;&#20316;&#32773;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#24182;&#21457;&#29616;&#20102;&#28909;&#38376;&#30740;&#31350;&#20027;&#39064;&#21644;&#26368;&#26032;&#28044;&#29616;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20854;&#20182;&#30456;&#20851;&#39046;&#22495;&#20013;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25512;&#21160;&#20854;&#20182;&#24863;&#20852;&#36259;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#38543;&#30528;&#36825;&#20010;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20102;&#35299;&#39640;&#24341;&#29992;&#35770;&#25991;&#30340;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#23450;&#20851;&#38190;&#36235;&#21183;&#12289;&#26377;&#24433;&#21709;&#21147;&#30340;&#20316;&#32773;&#20197;&#21450;&#36804;&#20170;&#20026;&#27490;&#25152;&#20570;&#20986;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#39640;&#24341;&#29992;&#26426;&#22120;&#23398;&#20064;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20221;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;1959&#24180;&#21040;2022&#24180;&#30340;&#22810;&#24180;&#38388;&#20869;&#65292;&#22791;&#21463;&#25512;&#23815;&#30340;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#39640;&#24341;&#29992;&#35770;&#25991;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#25991;&#29486;&#35745;&#37327;&#25216;&#26415;&#23545;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#24341;&#29992;&#20998;&#26512;&#12289;&#21512;&#33879;&#20998;&#26512;&#12289;&#20851;&#38190;&#35789;&#20998;&#26512;&#21644;&#20986;&#29256;&#36235;&#21183;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35770;&#25991;&#12289;&#39640;&#24341;&#29992;&#30340;&#20316;&#32773;&#20197;&#21450;&#21512;&#20316;&#32593;&#32476;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#28909;&#38376;&#30740;&#31350;&#20027;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#36817;&#23835;&#36215;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has emerged as a prominent field of research in computer science and other related fields, thereby driving advancements in other domains of interest. As the field continues to evolve, it is crucial to understand the landscape of highly cited publications to identify key trends, influential authors, and significant contributions made thus far. In this paper, we present a comprehensive bibliometric analysis of highly cited ML publications. We collected a dataset consisting of the top-cited papers from reputable ML conferences and journals, covering a period of several years from 1959 to 2022. We employed various bibliometric techniques to analyze the data, including citation analysis, co-authorship analysis, keyword analysis, and publication trends. Our findings reveal the most influential papers, highly cited authors, and collaborative networks within the machine learning community. We identify popular research themes and uncover emerging topics that have recently 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26174;&#31034;&#20102;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#26080;&#26816;&#32034;&#22522;&#32447;&#30456;&#27604;&#20960;&#20046;&#27809;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#20294;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35299;&#38145;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14338</link><description>&lt;p&gt;
TabR&#65306;&#35299;&#38145;&#26816;&#32034;&#22686;&#24378;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning. (arXiv:2307.14338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26174;&#31034;&#20102;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#26080;&#26816;&#32034;&#22522;&#32447;&#30456;&#27604;&#20960;&#20046;&#27809;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#20294;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35299;&#38145;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#32780;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#30340;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#20854;&#20182;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#26816;&#32034;&#27169;&#22411;&#20174;&#21487;&#29992;&#30340;&#65288;&#35757;&#32451;&#65289;&#25968;&#25454;&#20013;&#26816;&#32034;&#20854;&#20182;&#30456;&#20851;&#23545;&#35937;&#65292;&#20363;&#22914;&#26368;&#36817;&#37051;&#23621;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#30340;&#29305;&#24449;&#29978;&#33267;&#26631;&#31614;&#26469;&#36827;&#34892;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#36866;&#24403;&#35843;&#25972;&#30340;&#31616;&#21333;&#26080;&#26816;&#32034;&#22522;&#32447;&#30456;&#27604;&#65292;&#20960;&#20046;&#27809;&#26377;&#25110;&#32773;&#21482;&#26377;&#24494;&#23567;&#30340;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#26159;&#21542;&#20540;&#24471;&#22312;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#32487;&#32493;&#25506;&#32034;&#36824;&#19981;&#28165;&#26970;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution. Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed. For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction. However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines. Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL.  In this work, we give a strong positive answer to this question. We start by incrementally augmenting a simple feed-forward architecture wit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#30456;&#20851;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;CLIP&#27169;&#22411;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#22312;&#21160;&#20316;&#27010;&#24565;&#31354;&#38388;&#20013;&#36827;&#34892;&#33258;&#33976;&#39311;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#32447;&#24615;&#25512;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10922</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#30340;&#21160;&#20316;&#27010;&#24565;&#31354;&#38388;&#25913;&#36827;&#35270;&#39057;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language-based Action Concept Spaces Improve Video Self-Supervised Learning. (arXiv:2307.10922v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10922
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#30456;&#20851;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;CLIP&#27169;&#22411;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#22312;&#21160;&#20316;&#27010;&#24565;&#31354;&#38388;&#20013;&#36827;&#34892;&#33258;&#33976;&#39311;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#32447;&#24615;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#24050;&#32463;&#23454;&#29616;&#20102;&#23398;&#20064;&#21487;&#20256;&#36882;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#23569;&#37327;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#31616;&#21333;&#27493;&#39588;&#65292;&#20351;&#29992;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#23558;&#22270;&#20687;CLIP&#27169;&#22411;&#35843;&#25972;&#20026;&#35270;&#39057;&#39046;&#22495;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#36866;&#29992;&#20110;&#26102;&#38388;&#24314;&#27169;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#21160;&#20316;&#27010;&#24565;&#31354;&#38388;&#20013;&#20351;&#29992;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#33258;&#33976;&#39311;&#35757;&#32451;&#12290;&#20351;&#29992;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#20174;&#35821;&#35328;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#21508;&#20010;&#21160;&#20316;&#27010;&#24565;&#30340;&#29305;&#24449;&#21521;&#37327;&#26500;&#25104;&#20102;&#36825;&#20010;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#35757;&#32451;&#30446;&#26631;&#65292;&#27010;&#24565;&#33976;&#39311;&#21644;&#27010;&#24565;&#23545;&#40784;&#65292;&#26082;&#20445;&#30041;&#20102;&#21407;&#22987;&#34920;&#31034;&#30340;&#24191;&#27867;&#24615;&#65292;&#21448;&#24378;&#21270;&#20102;&#21160;&#20316;&#21644;&#20854;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#19977;&#20010;&#21160;&#20316;&#35782;&#21035;&#22522;&#20934;&#19978;&#30340;&#38646;&#26679;&#26412;&#21644;&#32447;&#24615;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#28151;&#21512;&#21407;&#22987;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#30340;&#19981;&#21516;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ImageNet&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20339;&#38477;&#22122;&#26041;&#27861;2%&#65292;&#22312;38&#20010;&#20219;&#21153;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102;4%&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#22312;Flickr&#21644;MS-COCO&#26816;&#32034;&#19978;&#20063;&#25552;&#21319;&#20102;2&#20493;&#12290;</title><link>http://arxiv.org/abs/2307.10350</link><description>&lt;p&gt;
&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#25551;&#36848;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10350
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#28151;&#21512;&#21407;&#22987;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#30340;&#19981;&#21516;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ImageNet&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20339;&#38477;&#22122;&#26041;&#27861;2%&#65292;&#22312;38&#20010;&#20219;&#21153;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102;4%&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#22312;Flickr&#21644;MS-COCO&#26816;&#32034;&#19978;&#20063;&#25552;&#21319;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#22312;CLIP&#21644;Flamingo&#31561;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#32593;&#32476;&#25968;&#25454;&#23384;&#22312;&#22122;&#38899;&#65292;&#29616;&#26377;&#30340;&#38477;&#22122;&#26041;&#27861;&#24448;&#24448;&#20250;&#20197;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#36136;&#37327;&#20316;&#20026;&#22122;&#38899;&#30340;&#19968;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#29983;&#25104;&#30340;&#25551;&#36848;&#22686;&#21152;&#21547;&#26377;&#21547;&#20041;&#19981;&#26126;&#30830;&#25991;&#26412;&#30340;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#28857;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#25506;&#32034;&#21407;&#22987;&#27169;&#24335;&#21644;&#29983;&#25104;&#27169;&#24335;&#20004;&#31181;&#19981;&#21516;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;ImageNet&#19978;&#36229;&#36807;&#20102;DataComp&#22522;&#20934;&#25552;&#20986;&#30340;&#26368;&#20339;&#38477;&#22122;&#26041;&#27861;2%&#65292;&#22312;38&#20010;&#20219;&#21153;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102;4%&#65292;&#32473;&#23450;128M&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20505;&#36873;&#27744;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#26041;&#27861;&#22312;Flickr&#21644;MS-COCO&#26816;&#32034;&#19978;&#20063;&#25552;&#21319;&#20102;2&#20493;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21512;&#25104;&#25551;&#36848;&#20026;&#20160;&#20040;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25991;&#26412;&#30417;&#30563;&#26469;&#28304;&#12290;&#22312;&#23581;&#35797;&#19981;&#21516;&#30340;&#22270;&#20687;&#25551;&#36848;&#27169;&#22411;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27169;&#22411;&#22312;&#26631;&#20934;&#22270;&#20687;&#25551;&#36848;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65288;&#20363;&#22914;&#65292;NoCaps CIDEr&#65289;&#24182;&#19981;&#19968;&#23450;&#26159;
&lt;/p&gt;
&lt;p&gt;
Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#31181;&#34394;&#20551;&#30456;&#20851;&#24615;&#26159;&#30001;&#20110;&#19981;&#21487;&#35266;&#23519;&#30340;&#28151;&#26434;&#22240;&#32032;&#24341;&#36215;&#30340;&#65292;&#24182;&#19988;&#26222;&#36941;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.07907</link><description>&lt;p&gt;
&#35270;&#32780;&#19981;&#35265;&#65306;&#38024;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation. (arXiv:2307.07907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#31181;&#34394;&#20551;&#30456;&#20851;&#24615;&#26159;&#30001;&#20110;&#19981;&#21487;&#35266;&#23519;&#30340;&#28151;&#26434;&#22240;&#32032;&#24341;&#36215;&#30340;&#65292;&#24182;&#19988;&#26222;&#36941;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#24418;&#24335;&#65292;&#22914;&#38543;&#26426;&#25200;&#21160;&#12289;&#32597;&#35265;&#20107;&#20214;&#21644;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#31181;&#20851;&#38190;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#29366;&#24577;&#30340;&#19981;&#21516;&#37096;&#20998;&#27809;&#26377;&#22240;&#26524;&#20851;&#31995;&#65292;&#20294;&#21364;&#23384;&#22312;&#30001;&#19981;&#21487;&#35266;&#23519;&#30340;&#28151;&#26434;&#22240;&#32032;&#24341;&#36215;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#34394;&#20551;&#30456;&#20851;&#24615;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#20013;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#36890;&#24120;&#22312;&#30333;&#22825;&#35266;&#23519;&#21040;&#20132;&#36890;&#25317;&#22581;&#65292;&#22812;&#26202;&#35266;&#23519;&#21040;&#20132;&#36890;&#36731;&#26494;&#65292;&#21407;&#22240;&#26159;&#19981;&#21487;&#35266;&#23519;&#30340;&#20154;&#31867;&#27963;&#21160;&#12290;&#27169;&#22411;&#22312;&#23398;&#20064;&#36825;&#31181;&#26080;&#29992;&#29978;&#33267;&#26377;&#23475;&#30456;&#20851;&#24615;&#26102;&#65292;&#24403;&#27979;&#35797;&#26696;&#20363;&#30340;&#28151;&#26434;&#22240;&#32032;&#20559;&#31163;&#35757;&#32451;&#26102;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#22833;&#36133;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20351;&#27169;&#22411;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#30001;&#19981;&#21487;&#35266;&#23519;&#30340;&#28151;&#26434;&#22240;&#32032;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#39034;&#24207;&#32467;&#26500;&#24418;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#24456;&#38590;&#36827;&#34892;&#34920;&#24449;&#21644;&#35782;&#21035;&#12290;&#29616;&#26377;&#30340;&#40065;&#26834;&#31639;&#27861;&#24448;&#24448;&#26080;&#27861;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have causality but have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and sequential structure of RL, is difficult to characterize and identify. Existing robust alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07063</link><description>&lt;p&gt;
&#20351;&#29992;&#35299;&#32806;&#30340;&#35821;&#35328;&#39044;&#35757;&#32451;&#20026;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#24341;&#20837;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36164;&#28304;&#23494;&#38598;&#22411;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#12290;&#24403;&#21069;&#30340;&#33539;&#24335;&#20351;&#29992;&#35270;&#35273;&#29305;&#24449;&#20316;&#20026;&#25552;&#31034;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#19982;&#30456;&#24212;&#25991;&#26412;&#26368;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#20855;&#20307;&#26159;&#30830;&#23450;&#19982;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt-Transformer&#65288;P-Former&#65289;&#65292;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#36825;&#20123;&#29702;&#24819;&#25552;&#31034;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#22312;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#30340;&#38656;&#35201;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#31471;&#21040;&#31471;&#30340;VL&#35757;&#32451;&#36807;&#31243;&#24039;&#22937;&#22320;&#20998;&#20026;&#20102;&#39069;&#22806;&#30340;&#29420;&#31435;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#31283;&#20581;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#22522;&#32447;&#65288;BLIP-2&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#20351;&#29992;4M&#25110;129M&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21463;&#21040;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65292;&#20197;&#24179;&#34913;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#28789;&#27963;&#24615;&#19982;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05891</link><description>&lt;p&gt;
&#21463;PID&#25511;&#21046;&#22120;&#21551;&#21457;&#30340;&#20559;&#24046;&#24402;&#32435;&#27861;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21463;&#21040;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65292;&#20197;&#24179;&#34913;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#28789;&#27963;&#24615;&#19982;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#36890;&#36807;&#25968;&#25454;&#33258;&#24049;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;RL&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#36890;&#24120;&#19981;&#21487;&#35266;&#27979;&#12290;&#24403;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#26102;&#65292;&#31574;&#30053;&#38656;&#35201;&#21033;&#29992;&#35266;&#23519;&#21382;&#21490;&#26469;&#25512;&#26029;&#24403;&#21069;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#24322;&#20351;&#24471;&#31574;&#30053;&#19981;&#20250;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30340;&#24207;&#21015;&#12290;&#22240;&#27492;&#65292;&#22312;&#21382;&#21490;&#35760;&#24405;&#32534;&#30721;&#22120;&#28789;&#27963;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#35201;&#23545;&#29615;&#22659;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#24179;&#34913;&#65292;&#25105;&#20204;&#23547;&#27714;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26029;&#23450;PID&#25511;&#21046;&#22120;&#30340;&#25104;&#21151;&#34920;&#26126;&#65292;&#35768;&#22810;&#25511;&#21046;&#20219;&#21153;&#21482;&#38656;&#35201;&#27714;&#21644;&#21644;&#27714;&#24046;&#26469;&#32047;&#31215;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#20010;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65306;&#19968;&#31181;&#30452;&#25509;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#19978;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#19988;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#20102;&#36712;&#36857;&#23545;&#40784;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#28176;&#36827;&#23574;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#25991;&#29486;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04204</link><description>&lt;p&gt;
&#36712;&#36857;&#23545;&#40784;&#65306;&#36890;&#36807;&#20998;&#21449;&#29702;&#35770;&#29702;&#35299;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory. (arXiv:2307.04204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#19978;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#19988;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#20102;&#36712;&#36857;&#23545;&#40784;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#28176;&#36827;&#23574;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#25991;&#29486;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cohen&#31561;&#20154;&#65288;2021&#65289;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#36712;&#36857;&#19978;&#25439;&#22833;Hessian&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65288;&#21363;&#38160;&#24230;&#65289;&#65292;&#35266;&#23519;&#21040;&#19968;&#31181;&#31216;&#20026;&#31283;&#23450;&#36793;&#32536;&#65288;EoS&#65289;&#30340;&#29616;&#35937;&#12290;&#38160;&#24230;&#22312;&#22521;&#35757;&#30340;&#26089;&#26399;&#38454;&#27573;&#22686;&#21152;&#65288;&#31216;&#20026;&#28176;&#36827;&#23574;&#38160;&#21270;&#65289;&#65292;&#26368;&#32456;&#25509;&#36817;&#38408;&#20540;$2/\text{(&#27493;&#38271;)}$&#38468;&#36817;&#20572;&#28382;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#39318;&#20808;&#35777;&#26126;&#20102;&#24403;EoS&#29616;&#35937;&#21457;&#29983;&#26102;&#65292;&#19981;&#21516;&#30340;GD&#36712;&#36857;&#65288;&#32463;&#36807;&#36866;&#24403;&#30340;&#21442;&#25968;&#21270;&#65289;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#20998;&#21449;&#22270;&#19978;&#23545;&#40784;&#65292;&#32780;&#19982;&#21021;&#22987;&#21270;&#26080;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#20108;&#23618;&#20840;&#36830;&#25509;&#32447;&#24615;&#32593;&#32476;&#21644;&#19968;&#20010;&#20351;&#29992;&#21333;&#20010;&#25968;&#25454;&#28857;&#35757;&#32451;&#30340;&#21333;&#31070;&#32463;&#20803;&#38750;&#32447;&#24615;&#32593;&#32476;&#20005;&#26684;&#35777;&#26126;&#20102;&#36825;&#31181;&#36712;&#36857;&#23545;&#40784;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#36712;&#36857;&#23545;&#40784;&#20998;&#26512;&#24314;&#31435;&#20102;&#28176;&#36827;&#23574;&#38160;&#21270;&#21644;EoS&#29616;&#35937;&#65292;&#28085;&#30422;&#24182;&#25193;&#23637;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cohen et al. (2021) empirically study the evolution of the largest eigenvalue of the loss Hessian, also known as sharpness, along the gradient descent (GD) trajectory and observe a phenomenon called the Edge of Stability (EoS). The sharpness increases at the early phase of training (referred to as progressive sharpening), and eventually saturates close to the threshold of $2 / \text{(step size)}$. In this paper, we start by demonstrating through empirical studies that when the EoS phenomenon occurs, different GD trajectories (after a proper reparameterization) align on a specific bifurcation diagram independent of initialization. We then rigorously prove this trajectory alignment phenomenon for a two-layer fully-connected linear network and a single-neuron nonlinear network trained with a single data point. Our trajectory alignment analysis establishes both progressive sharpening and EoS phenomena, encompassing and extending recent findings in the literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#23398;&#20064;&#26102;&#31354;&#36830;&#32493;&#31070;&#32463;PDE&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#35774;&#35745;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#30340;&#32593;&#26684;&#29420;&#31435;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2307.04110</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#20013;&#23398;&#20064;&#26102;&#31354;&#36830;&#32493;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Space-Time Continuous Neural PDEs from Partially Observed States. (arXiv:2307.04110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04110
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#23398;&#20064;&#26102;&#31354;&#36830;&#32493;&#31070;&#32463;PDE&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#35774;&#35745;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#30340;&#32593;&#26684;&#29420;&#31435;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29420;&#31435;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20855;&#26377;&#22122;&#22768;&#21644;&#37096;&#20998;&#35266;&#27979;&#30340;&#19981;&#35268;&#21017;&#26102;&#31354;&#32593;&#26684;&#19978;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#27010;&#29575;&#26694;&#26550;&#21644;&#25913;&#36827;&#30340;&#32534;&#30721;&#22120;&#35774;&#35745;&#30340;&#26102;&#31354;&#36830;&#32493;&#28508;&#22312;&#31070;&#32463;PDE&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#32593;&#26684;&#29420;&#31435;&#24615;&#12290;&#28508;&#22312;&#29366;&#24577;&#21160;&#21147;&#23398;&#30001;&#32467;&#21512;&#20102;&#25554;&#20540;&#27861;&#21644;&#32447;&#26041;&#27861;&#30340;PDE&#27169;&#22411;&#25152;&#25511;&#21046;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22343;&#25674;&#21464;&#20998;&#25512;&#29702;&#26469;&#36817;&#20284;&#21518;&#39564;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#22810;&#37325;&#23556;&#20987;&#25216;&#26415;&#26469;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#24182;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20102;&#20854;&#25512;&#36827;&#25968;&#25454;&#39537;&#21160;PDE&#24314;&#27169;&#21644;&#23454;&#29616;&#22797;&#26434;&#37096;&#20998;&#35266;&#27979;&#30340;&#32593;&#26684;&#29420;&#31435;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel grid-independent model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. We propose a space-time continuous latent neural PDE model with an efficient probabilistic framework and a novel encoder design for improved data efficiency and grid independence. The latent state dynamics are governed by a PDE model that combines the collocation method and the method of lines. We employ amortized variational inference for approximate posterior estimation and utilize a multiple shooting technique for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. The proposed model outperforms recent methods, showing its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#33410;&#28857;&#20998;&#31867;&#20026;&#20363;&#65292;&#36890;&#36807;&#8220;&#31070;&#32463;&#22604;&#38519;&#8221;&#29616;&#35937;&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#28436;&#21270;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#22312;&#33410;&#28857;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#29305;&#24449;&#30340;&#31867;&#20869;&#21464;&#24322;&#24615;&#20063;&#20250;&#20943;&#23569;&#65292;&#20294;&#19981;&#21450;&#22522;&#20110;&#23454;&#20363;&#30340;&#24773;&#20917;&#37027;&#20040;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2307.01951</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#28436;&#21270;&#30340;&#31070;&#32463;&#22604;&#38519;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks. (arXiv:2307.01951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#33410;&#28857;&#20998;&#31867;&#20026;&#20363;&#65292;&#36890;&#36807;&#8220;&#31070;&#32463;&#22604;&#38519;&#8221;&#29616;&#35937;&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#28436;&#21270;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#22312;&#33410;&#28857;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#29305;&#24449;&#30340;&#31867;&#20869;&#21464;&#24322;&#24615;&#20063;&#20250;&#20943;&#23569;&#65292;&#20294;&#19981;&#21450;&#22522;&#20110;&#23454;&#20363;&#30340;&#24773;&#20917;&#37027;&#20040;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;GNNs&#20013;&#22270;&#25299;&#25169;&#21644;&#29305;&#24449;&#28436;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20197;&#22522;&#20110;&#33410;&#28857;&#30340;&#20998;&#31867;&#20026;&#20027;&#39064;&#65292;&#20197;&#38543;&#26426;&#22359;&#27169;&#22411;&#22270;&#19978;&#30340;&#31038;&#21306;&#26816;&#27979;&#20026;&#20363;&#65292;&#36890;&#36807;&#8220;&#31070;&#32463;&#22604;&#38519;&#8221;&#29616;&#35937;&#26469;&#25506;&#32034;&#29305;&#24449;&#28436;&#21270;&#12290;&#24403;&#35757;&#32451;&#22522;&#20110;&#23454;&#20363;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#36229;&#36807;&#38646;&#35757;&#32451;&#35823;&#24046;&#28857;&#26102;&#65292;&#31070;&#32463;&#22604;&#38519;&#34920;&#29616;&#20026;&#26368;&#28145;&#23618;&#29305;&#24449;&#30340;&#31867;&#20869;&#21464;&#24322;&#24615;&#20943;&#23569;&#65292;&#24182;&#19988;&#31867;&#22343;&#20540;&#19982;&#29305;&#23450;&#30340;&#23545;&#31216;&#32467;&#26500;&#26356;&#21152;&#23545;&#40784;&#12290;&#25105;&#20204;&#20808;&#20174;&#23454;&#35777;&#30740;&#31350;&#24320;&#22987;&#65292;&#26174;&#31034;&#31867;&#20869;&#21464;&#24322;&#24615;&#30340;&#20943;&#23569;&#22312;&#22522;&#20110;&#33410;&#28857;&#30340;&#20998;&#31867;&#29615;&#22659;&#20013;&#20063;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#19981;&#21450;&#22522;&#20110;&#23454;&#20363;&#30340;&#26696;&#20363;&#37027;&#20040;&#26126;&#26174;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#36825;&#31181;&#21306;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#19981;&#32771;&#34385;&#28608;&#27963;&#65292;&#22270;&#25299;&#25169;&#20449;&#24687;&#20063;&#33021;&#23548;&#33268;&#29305;&#24449;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the "Neural Collapse" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an "optimis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#30340;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#38477;&#22122;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;&#38477;&#22122;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.15012</link><description>&lt;p&gt;
&#29992;&#20110;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#30446;&#26631;&#20449;&#21495;&#24674;&#22797;&#30340;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures. (arXiv:2306.15012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#30340;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#38477;&#22122;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;&#38477;&#22122;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21482;&#23545;&#32473;&#23450;&#20449;&#21495;&#30340;&#29305;&#23450;&#23646;&#24615;&#24863;&#20852;&#36259;&#26102;&#65292;&#20174;&#19968;&#20010;&#21152;&#24615;&#28151;&#21512;&#29289;&#20013;&#20998;&#31163;&#20449;&#21495;&#21487;&#33021;&#26159;&#19968;&#20010;&#19981;&#24517;&#35201;&#22320;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26356;&#31616;&#21333;&#30340;&#8220;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;&#8221;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#19987;&#27880;&#20110;&#20174;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#30340;&#39044;&#23450;&#20041;&#32479;&#35745;&#25551;&#36848;&#37327;&#12290;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#22122;&#22768;&#36807;&#31243;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#20351;&#21463;&#22122;&#22768;&#26679;&#26412;&#27745;&#26579;&#30340;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#30340;&#32479;&#35745;&#29305;&#24615;&#19982;&#35266;&#27979;&#30340;&#28151;&#21512;&#29289;&#30340;&#32479;&#35745;&#29305;&#24615;&#21305;&#37197;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#35299;&#26512;&#21487;&#36861;&#36394;&#35745;&#31639;&#30340;&#31616;&#21333;&#31034;&#20363;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#38477;&#22122;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#20102;1&#65289;&#22522;&#20110;&#23567;&#27874;&#30340;&#25551;&#36848;&#31526;&#65292;2&#65289;&#38024;&#23545;&#22825;&#20307;&#29289;&#29702;&#21644;ImageNet&#25968;&#25454;&#30340;ConvNet-based&#25551;&#36848;&#31526;&#12290;&#22312;&#31532;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#27604;&#26631;&#20934;&#38477;&#22122;&#26041;&#27861;&#26356;&#22909;&#22320;&#24674;&#22797;&#20102;&#30446;&#26631;&#25968;&#25454;&#30340;&#25551;&#36848;&#31526;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#19981;&#26159;&#20026;&#27492;&#30446;&#30340;&#26500;&#24314;&#30340;&#65292;&#23427;&#20063;&#34920;&#29616;&#20986;&#23545;&#30446;&#26631;&#20449;&#21495;&#25551;&#36848;&#31526;&#24674;&#22797;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler "statistical component separation" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it pe
&lt;/p&gt;</description></item><item><title>CEIL&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#24191;&#20041;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#35265;&#23884;&#20837;&#20989;&#25968;&#21644;&#19978;&#19979;&#25991;&#31574;&#30053;&#26469;&#23454;&#29616;&#27169;&#20223;&#23398;&#20064;&#30340;&#19987;&#23478;&#21305;&#37197;&#30446;&#26631;&#12290;&#23427;&#33021;&#22815;&#36866;&#29992;&#20110;&#22810;&#31181;&#23398;&#20064;&#35774;&#32622;&#65292;&#21253;&#25324;&#20174;&#35266;&#27979;&#20013;&#23398;&#20064;&#12289;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#12289;&#36328;&#39046;&#22495;&#27169;&#20223;&#23398;&#20064;&#21644;&#19968;&#27425;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CEIL&#30456;&#27604;&#20043;&#21069;&#30340;&#22522;&#20934;&#31639;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.14534</link><description>&lt;p&gt;
CEIL: &#24191;&#20041;&#19978;&#19979;&#25991;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CEIL: Generalized Contextual Imitation Learning. (arXiv:2306.14534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14534
&lt;/p&gt;
&lt;p&gt;
CEIL&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#24191;&#20041;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#35265;&#23884;&#20837;&#20989;&#25968;&#21644;&#19978;&#19979;&#25991;&#31574;&#30053;&#26469;&#23454;&#29616;&#27169;&#20223;&#23398;&#20064;&#30340;&#19987;&#23478;&#21305;&#37197;&#30446;&#26631;&#12290;&#23427;&#33021;&#22815;&#36866;&#29992;&#20110;&#22810;&#31181;&#23398;&#20064;&#35774;&#32622;&#65292;&#21253;&#25324;&#20174;&#35266;&#27979;&#20013;&#23398;&#20064;&#12289;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#12289;&#36328;&#39046;&#22495;&#27169;&#20223;&#23398;&#20064;&#21644;&#19968;&#27425;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CEIL&#30456;&#27604;&#20043;&#21069;&#30340;&#22522;&#20934;&#31639;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CEIL&#65288;&#24191;&#20041;&#19978;&#19979;&#25991;&#27169;&#20223;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#24191;&#27867;&#36866;&#29992;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#12290;&#21463;&#21040;&#21518;&#35265;&#20449;&#24687;&#21305;&#37197;&#30340;&#24418;&#24335;&#21270;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#26174;&#24335;&#23398;&#20064;&#19968;&#31181;&#21518;&#35265;&#23884;&#20837;&#20989;&#25968;&#20197;&#21450;&#20351;&#29992;&#21518;&#35265;&#23884;&#20837;&#30340;&#19978;&#19979;&#25991;&#31574;&#30053;&#65292;&#24471;&#21040;&#20102;CEIL&#12290;&#20026;&#20102;&#23454;&#29616;&#27169;&#20223;&#23398;&#20064;&#30340;&#19987;&#23478;&#21305;&#37197;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20513;&#20248;&#21270;&#19968;&#31181;&#19978;&#19979;&#25991;&#21464;&#37327;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#31574;&#30053;&#12290;&#38500;&#20102;&#20856;&#22411;&#30340;&#31034;&#33539;&#23398;&#20064;&#65288;LfD&#65289;&#35774;&#32622;&#22806;&#65292;CEIL&#36824;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#22810;&#31181;&#35774;&#32622;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20174;&#35266;&#27979;&#20013;&#23398;&#20064;&#65288;LfO&#65289;&#65292;2&#65289;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65292;3&#65289;&#36328;&#39046;&#22495;&#27169;&#20223;&#23398;&#20064;&#65288;&#19981;&#21305;&#37197;&#30340;&#19987;&#23478;&#65289;&#21644;4&#65289;&#19968;&#27425;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;MuJoCo&#20219;&#21153;&#65288;&#22312;&#32447;&#65289;&#21644;D4RL&#25968;&#25454;&#38598;&#65288;&#31163;&#32447;&#65289;&#19978;&#35780;&#20272;CEIL&#12290;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CEIL&#30340;&#26356;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textbf{C}ont\textbf{E}xtual \textbf{I}mitation \textbf{L}earning~(CEIL), a general and broadly applicable algorithm for imitation learning (IL). Inspired by the formulation of hindsight information matching, we derive CEIL by explicitly learning a hindsight embedding function together with a contextual policy using the hindsight embeddings. To achieve the expert matching objective for IL, we advocate for optimizing a contextual variable such that it biases the contextual policy towards mimicking expert behaviors. Beyond the typical learning from demonstrations (LfD) setting, CEIL is a generalist that can be effectively applied to multiple settings including: 1)~learning from observations (LfO), 2)~offline IL, 3)~cross-domain IL (mismatched experts), and 4) one-shot IL settings. Empirically, we evaluate CEIL on the popular MuJoCo tasks (online) and the D4RL dataset (offline). Compared to prior state-of-the-art baselines, we show that CEIL is more sample-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13004</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;?
&lt;/p&gt;
&lt;p&gt;
Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20154;&#30340;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#26694;&#26550;&#20351;&#29992;&#40657;&#30418;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#29366;&#24577;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;Cartpole&#12289;&#35270;&#35273;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#21644;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DDT&#23398;&#20064;&#21487;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26641;&#24418;&#32467;&#26500;&#26377;&#21161;&#20110;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#31243;&#24230;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;DDT&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26641;&#30340;&#31163;&#25955;&#24615;&#20250;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21464;&#23610;&#23544;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#12290;&#36890;&#36807;&#35266;&#23519;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#23545;&#35937;&#25551;&#32472;&#21644;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#37325;&#22797;&#26080;&#24207;&#21576;&#29616;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#29109;&#19982;&#20196;&#29260;&#25968;&#37327;&#21464;&#21270;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#35270;&#35273;&#20445;&#30495;&#24230;&#30340;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#23610;&#23544;&#21644;&#38271;&#23485;&#27604;&#30340;&#22270;&#20687;&#21512;&#25104;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.08645</link><description>&lt;p&gt;
&#38024;&#23545;&#21464;&#23610;&#23544;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis. (arXiv:2306.08645v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21464;&#23610;&#23544;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#12290;&#36890;&#36807;&#35266;&#23519;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#23545;&#35937;&#25551;&#32472;&#21644;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#37325;&#22797;&#26080;&#24207;&#21576;&#29616;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#29109;&#19982;&#20196;&#29260;&#25968;&#37327;&#21464;&#21270;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#35270;&#35273;&#20445;&#30495;&#24230;&#30340;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#23610;&#23544;&#21644;&#38271;&#23485;&#27604;&#30340;&#22270;&#20687;&#21512;&#25104;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36981;&#24490;&#28145;&#24230;&#23398;&#20064;&#30340;&#20256;&#32479;&#65292;DMs&#22312;&#22266;&#23450;&#23610;&#23544;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#38656;&#35201;&#19981;&#21516;&#23610;&#23544;&#21644;&#19981;&#21516;&#38271;&#23485;&#27604;&#30340;&#21508;&#31181;&#22270;&#20687;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20445;&#25345;&#35270;&#35273;&#20445;&#30495;&#24230;&#30340;&#21516;&#26102;&#65292;&#20351;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#36825;&#31181;&#22810;&#26679;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#21512;&#25104;&#36807;&#31243;&#20013;&#65292;&#20302;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#20250;&#22240;&#20026;&#23545;&#35937;&#25551;&#32472;&#19981;&#23436;&#25972;&#65292;&#32780;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21017;&#20250;&#20986;&#29616;&#37325;&#22797;&#26080;&#24207;&#30340;&#21576;&#29616;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#35745;&#20851;&#31995;&#65292;&#35813;&#20851;&#31995;&#34920;&#26126;&#27880;&#24847;&#21147;&#29109;&#38543;&#20196;&#29260;&#25968;&#37327;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#36825;&#34920;&#26126;&#27169;&#22411;&#20250;&#25353;&#29031;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#27604;&#20363;&#32858;&#21512;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;&#35266;&#23519;&#32467;&#26524;&#30340;&#21518;&#32493;&#35299;&#37322;&#26159;&#65292;&#30001;&#20110;&#20302;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#26377;&#38480;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#23545;&#35937;&#34987;&#19981;&#23436;&#25972;&#22320;&#25551;&#32472;&#65292;&#32780;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21017;&#20250;&#20986;&#29616;&#37325;&#22797;&#26080;&#24207;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) have recently gained attention with state-of-the-art performance in text-to-image synthesis. Abiding by the tradition in deep learning, DMs are trained and evaluated on the images with fixed sizes. However, users are demanding for various images with specific sizes and various aspect ratio. This paper focuses on adapting text-to-image diffusion models to handle such variety while maintaining visual fidelity. First we observe that, during the synthesis, lower resolution images suffer from incomplete object portrayal, while higher resolution images exhibit repetitively disordered presentation. Next, we establish a statistical relationship indicating that attention entropy changes with token quantity, suggesting that models aggregate spatial information in proportion to image resolution. The subsequent interpretation on our observations is that objects are incompletely depicted due to limited spatial information for low resolutions, while repetitively disorganized p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;Oracle&#26377;&#25928;&#30340;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.07923</link><description>&lt;p&gt;
&#38754;&#21521;Oracle&#30340;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#65306;&#31163;&#32447;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits. (arXiv:2306.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;Oracle&#26377;&#25928;&#30340;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#32473;&#23450;&#19968;&#20010;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26085;&#24535;&#20132;&#20114;&#12290;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#24754;&#35266;&#24809;&#32602;&#26469;&#32531;&#35299;&#20998;&#24067;&#20559;&#31227;&#65292;&#20294;&#20808;&#21069;&#30340;&#23454;&#29616;&#24182;&#19981;&#35745;&#31639;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;Oracle&#26377;&#25928;&#30340;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65306;&#23427;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20063;&#24471;&#20986;&#20102;&#31867;&#20284;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#24754;&#35266;&#26041;&#27861;&#30340;&#26368;&#20339;&#32479;&#35745;&#20445;&#35777;&#12290;&#25105;&#20204;&#20026;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#37117;&#23454;&#20363;&#21270;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#22312;&#21508;&#31181;&#37197;&#32622;&#20013;&#37117;&#27604;&#26410;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#20248;&#21270;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider policy optimization in contextual bandits, where one is given a fixed dataset of logged interactions. While pessimistic regularizers are typically used to mitigate distribution shift, prior implementations thereof are not computationally efficient. We present the first oracle-efficient algorithm for pessimistic policy optimization: it reduces to supervised learning, leading to broad applicability. We also obtain best-effort statistical guarantees analogous to those for pessimistic approaches in prior work. We instantiate our approach for both discrete and continuous actions. We perform extensive experiments in both settings, showing advantage over unregularized policy optimization across a wide range of configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$f$-&#25104;&#21592;&#25512;&#26029;&#38544;&#31169;($f$-MIP)&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20284;&#28982;&#27604;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;$\mu$-&#39640;&#26031;&#25104;&#21592;&#25512;&#26029;&#38544;&#31169;($\mu$-GMIP)&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#26512;&#24615;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#35757;&#32451;&#22823;&#37327;&#24433;&#23376;&#27169;&#22411;&#12290;&#24378;&#35843;&#20102;&#26041;&#24046;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07273</link><description>&lt;p&gt;
&#39640;&#26031;&#25104;&#21592;&#25512;&#26029;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Gaussian Membership Inference Privacy. (arXiv:2306.07273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$f$-&#25104;&#21592;&#25512;&#26029;&#38544;&#31169;($f$-MIP)&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20284;&#28982;&#27604;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;$\mu$-&#39640;&#26031;&#25104;&#21592;&#25512;&#26029;&#38544;&#31169;($\mu$-GMIP)&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#26512;&#24615;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#35757;&#32451;&#22823;&#37327;&#24433;&#23376;&#27169;&#22411;&#12290;&#24378;&#35843;&#20102;&#26041;&#24046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38544;&#31169;&#27010;&#24565;&#65292;&#31216;&#20026;$f$-&#25104;&#21592;&#25512;&#26029;&#38544;&#31169;($f$-MIP)&#65292;&#23427;&#26126;&#30830;&#32771;&#34385;&#20102;&#22312;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#23041;&#32961;&#27169;&#22411;&#19979;&#29616;&#23454;&#23545;&#25163;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;$f$-MIP&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#25913;&#36827;&#30340;&#25928;&#29992;(&#20363;&#22914;&#26356;&#22909;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;)&#12290;&#25105;&#20204;&#23545;&#22122;&#22768;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30340;&#20284;&#28982;&#27604;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;$f$-MIP&#20445;&#35777;&#26063;&#65292;&#31216;&#20026;$\mu$-&#39640;&#26031;&#25104;&#21592;&#25512;&#26029;&#38544;&#31169;($\mu$-GMIP)&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#20135;&#29983;&#20102;&#19968;&#31181;&#20998;&#26512;&#24615;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#30334;&#20010;&#24433;&#23376;&#27169;&#22411;&#26469;&#36924;&#36817;&#20284;&#28982;&#27604;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25915;&#20987;&#20351;&#24471;$f$-MIP&#30340;&#31616;&#21333;&#23457;&#35745;&#25104;&#20026;&#21487;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#24378;&#35843;&#20102;&#26041;&#24046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new privacy notion called $f$-Membership Inference Privacy ($f$-MIP), which explicitly considers the capabilities of realistic adversaries under the membership inference attack threat model. By doing so $f$-MIP offers interpretable privacy guarantees and improved utility (e.g., better classification accuracy). Our novel theoretical analysis of likelihood ratio-based membership inference attacks on noisy stochastic gradient descent (SGD) results in a parametric family of $f$-MIP guarantees that we refer to as $\mu$-Gaussian Membership Inference Privacy ($\mu$-GMIP). Our analysis additionally yields an analytical membership inference attack that offers distinct advantages over previous approaches. First, unlike existing methods, our attack does not require training hundreds of shadow models to approximate the likelihood ratio. Second, our analytical attack enables straightforward auditing of our privacy notion $f$-MIP. Finally, our analysis emphasizes the importance of vario
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#36716;&#25442;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31163;&#32447;&#36924;&#36817;&#31639;&#27861;&#36716;&#25442;&#20026;&#20855;&#26377;&#20302;&#949;-&#36817;&#20284;&#36951;&#25022;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#31181;&#38382;&#39064;&#24182;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#36817;&#20284;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07163</link><description>&lt;p&gt;
&#19968;&#33324;&#36716;&#25442;&#26500;&#24314;&#19968;&#33268;&#30340;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
General Transformation for Consistent Online Approximation Algorithms. (arXiv:2306.07163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#36716;&#25442;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31163;&#32447;&#36924;&#36817;&#31639;&#27861;&#36716;&#25442;&#20026;&#20855;&#26377;&#20302;&#949;-&#36817;&#20284;&#36951;&#25022;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#31181;&#38382;&#39064;&#24182;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#36817;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36716;&#25442;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#20174;&#31163;&#32447;&#36924;&#36817;&#31639;&#27861;&#20013;&#24320;&#21457;&#20855;&#26377;&#20302;&#949;-&#36817;&#20284;&#36951;&#25022;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#19968;&#20010;&#23558;&#20855;&#26377;&#20302;&#24179;&#22343;&#25935;&#24863;&#24230;&#30340;&#31163;&#32447;&#36924;&#36817;&#31639;&#27861;&#36716;&#25442;&#20026;&#20855;&#26377;&#20302;&#949;-&#36817;&#20284;&#36951;&#25022;&#30340;&#22312;&#32447;&#31639;&#27861;&#30340;&#36890;&#29992;&#32422;&#31616;&#23450;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;coreset&#26500;&#36896;&#26041;&#27861;&#23558;&#31163;&#32447;&#36924;&#36817;&#31639;&#27861;&#36716;&#25442;&#20026;&#20302;&#25935;&#24863;&#24230;&#29256;&#26412;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#21253;&#25324;&#22312;&#32447;(k&#65292;z)-&#32858;&#31867;&#12289;&#22312;&#32447;&#30697;&#38453;&#36924;&#36817;&#21644;&#22312;&#32447;&#22238;&#24402;&#65292;&#24182;&#25104;&#21151;&#22320;&#20026;&#27599;&#20010;&#38382;&#39064;&#23454;&#29616;&#20102;&#23545;&#25968;&#22810;&#39033;&#24335;&#949;-&#36817;&#20284;&#36951;&#25022;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20063;&#20139;&#26377;&#20302;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#21487;&#33021;&#26159;&#26576;&#20123;&#22312;&#32447;&#24212;&#29992;&#31243;&#24207;&#25152;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a transformation framework that can be utilized to develop online algorithms with low $\epsilon$-approximate regret in the random-order model from offline approximation algorithms. We first give a general reduction theorem that transforms an offline approximation algorithm with low average sensitivity to an online algorithm with low $\epsilon$-approximate regret. We then demonstrate that offline approximation algorithms can be transformed into a low-sensitivity version using a coreset construction method. To showcase the versatility of our approach, we apply it to various problems, including online $(k,z)$-clustering, online matrix approximation, and online regression, and successfully achieve polylogarithmic $\epsilon$-approximate regret for each problem. Moreover, we show that in all three cases, our algorithm also enjoys low inconsistency, which may be desired in some online applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#26032;&#21442;&#25968;&#29992;&#20110;&#25511;&#21046;&#21516;&#36136;&#24615;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05557</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#30340;&#24615;&#33021;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks. (arXiv:2306.05557v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#26032;&#21442;&#25968;&#29992;&#20110;&#25511;&#21046;&#21516;&#36136;&#24615;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNN&#30340;&#30740;&#31350;&#24378;&#35843;&#39640;&#21516;&#36136;&#24615;&#65288;&#21363;&#30456;&#20284;&#31867;&#33410;&#28857;&#30456;&#20114;&#36830;&#25509;&#30340;&#20542;&#21521;&#65289;&#19982;&#33410;&#28857;&#20998;&#31867;&#30340;&#24378;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#20851;&#31995;&#26356;&#21152;&#24494;&#22937;&#65292;&#35777;&#26126;&#21363;&#20351;&#31616;&#21333;&#30340;GNN&#20063;&#21487;&#20197;&#22312;&#26576;&#20123;&#24322;&#36136;&#24615;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#21457;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#20551;&#35774;&#65292;&#24182;&#30830;&#23450;&#25968;&#25454;&#38598;&#32463;&#24120;&#34987;&#35270;&#20026;&#22312;&#33410;&#28857;&#38388;&#20855;&#26377;&#24658;&#23450;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20026;&#20102;&#26356;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24110;&#21161;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#21516;&#36136;&#24615;&#20998;&#26512;&#20013;&#24120;&#29992;&#30340;&#20248;&#20808;&#38468;&#21152;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;&#65292;&#20197;&#25511;&#21046;&#29983;&#25104;&#30340;&#22270;&#20013;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#65292;&#20174;&#32780;&#23454;&#29616;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on GNNs has highlighted a relationship between high homophily (i.e., the tendency for nodes of a similar class to connect) and strong predictive performance in node classification. However, recent research has found the relationship to be more nuanced, demonstrating that even simple GNNs can learn in certain heterophilous settings. To bridge the gap between these findings, we revisit the assumptions made in previous works and identify that datasets are often treated as having a constant homophily level across nodes. To align closer to real-world datasets, we theoretically and empirically study the performance of GNNs when the local homophily level of a node deviates at test-time from the global homophily level of its graph. To aid our theoretical analysis, we introduce a new parameter to the preferential attachment model commonly used in homophily analysis to enable the control of local homophily levels in generated graphs, enabling a systematic empirical study on how local ho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#20316;&#32773;&#20204;&#21457;&#29616;OOD&#19982;ID&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04618</link><description>&lt;p&gt;
&#37325;&#28201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;: &#22522;&#20934;&#65292;&#20998;&#26512;&#21644;LLMs&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#20316;&#32773;&#20204;&#21457;&#29616;OOD&#19982;ID&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20013;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;(OOD)&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#20197;&#24448;&#30740;&#31350;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#35774;&#32622;&#26222;&#36941;&#32570;&#20047;&#36275;&#22815;&#30340;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23545;OOD&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#26500;&#24314;&#26041;&#26696;&#65292;&#30830;&#20445;&#20102;&#26126;&#30830;&#30340;&#21306;&#20998;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BOSS&#65292;&#19968;&#20010;&#28085;&#30422;5&#20010;&#20219;&#21153;&#21644;20&#20010;&#25968;&#25454;&#38598;&#30340;&#29992;&#20110;&#35780;&#20272;OOT&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;&#22522;&#20110;BOSS&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#20998;&#26512;&#21644;&#35780;&#20272;OOD&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39321;&#33609;&#24494;&#35843;&#30340;ID&#21644;OOD&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#20856;&#22411;&#31867;&#22411;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#23398;&#20064;&#26426;&#21046;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#39044;&#27979;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#19982;ID&#25968;&#25454;&#38598;&#19978;&#30340;&#36827;&#23637;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;BOSS&#19978;&#35780;&#20272;&#20102;5&#31181;&#32463;&#20856;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;OOD&#24615;&#33021;&#24182;&#19981;&#24635;&#26159;&#19982;ID&#24615;&#33021;&#19968;&#33268;&#65292;&#36825;&#34920;&#26126;&#20102;&#29305;&#21035;&#35780;&#20272;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#65288;&#28508;&#22312;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SACSoN&#30340;&#33258;&#20027;&#23548;&#33322;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21344;&#29992;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35270;&#35273;&#29702;&#35299;&#21644;&#23398;&#20064;&#65292;&#33258;&#20027;&#25910;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25968;&#25454;&#38598;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.01874</link><description>&lt;p&gt;
SACSoN&#65306;&#38754;&#21521;&#31038;&#20132;&#23548;&#33322;&#30340;&#21487;&#25193;&#23637;&#33258;&#20027;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SACSoN: Scalable Autonomous Data Collection for Social Navigation. (arXiv:2306.01874v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SACSoN&#30340;&#33258;&#20027;&#23548;&#33322;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21344;&#29992;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35270;&#35273;&#29702;&#35299;&#21644;&#23398;&#20064;&#65292;&#33258;&#20027;&#25910;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25968;&#25454;&#38598;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#26500;&#24314;&#31526;&#21512;&#31038;&#20132;&#35268;&#33539;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#36229;&#36234;&#20102;&#23545;&#20154;&#31867;&#34892;&#20026;&#30340;&#31616;&#21333;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#29702;&#35299;&#36807;&#21435;&#32463;&#39564;&#20013;&#30340;&#20154;&#31867;&#20132;&#20114;&#65292;&#23398;&#20064;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#31038;&#20132;&#23548;&#33322;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#20154;&#31867;&#21344;&#29992;&#30340;&#29615;&#22659;&#20013;&#25910;&#38598;&#23548;&#33322;&#25968;&#25454;&#21487;&#33021;&#38656;&#35201;&#36828;&#31243;&#25805;&#20316;&#25110;&#25345;&#32493;&#30417;&#35270;&#65292;&#20351;&#24471;&#36825;&#20010;&#36807;&#31243;&#38590;&#20197;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#35270;&#35273;&#23548;&#33322;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;SACSoN&#65292;&#21487;&#20197;&#33258;&#20027;&#23548;&#33322;&#20110;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#34892;&#20154;&#21608;&#22260;&#65292;&#24182;&#40723;&#21169;&#20016;&#23500;&#30340;&#20132;&#20114;&#12290;SACSoN&#20351;&#29992;&#35270;&#35273;&#35266;&#23519;&#26469;&#35266;&#23519;&#21644;&#22238;&#24212;&#20854;&#38468;&#36817;&#30340;&#20154;&#31867;&#12290;&#23427;&#23558;&#36825;&#31181;&#35270;&#35273;&#29702;&#35299;&#19982;&#25345;&#32493;&#30340;&#23398;&#20064;&#21644;&#33258;&#20027;&#30896;&#25758;&#24674;&#22797;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20154;&#25805;&#20316;&#21592;&#30340;&#21442;&#19982;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25193;&#23637;&#20102;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31995;&#32479;&#26469;&#25910;&#38598;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a powerful tool for building socially compliant robotic systems that go beyond simple predictive models of human behavior. By observing and understanding human interactions from past experiences, learning can enable effective social navigation behaviors directly from data. However, collecting navigation data in human-occupied environments may require teleoperation or continuous monitoring, making the process prohibitively expensive to scale. In this paper, we present a scalable data collection system for vision-based navigation, SACSoN, that can autonomously navigate around pedestrians in challenging real-world environments while encouraging rich interactions. SACSoN uses visual observations to observe and react to humans in its vicinity. It couples this visual understanding with continual learning and an autonomous collision recovery system that limits the involvement of a human operator, allowing for better dataset scaling. We use a this system to collect th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26435;&#37325;&#38598;&#25104;&#12289;&#22810;&#26679;&#24615;&#21152;&#26435;&#21644;&#20808;&#39564;&#26657;&#27491;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22240;&#20998;&#24067;&#20559;&#31227;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#28085;&#30422;&#22914;&#27492;&#24191;&#27867;&#33539;&#22260;&#30340;&#24037;&#20316;&#65292;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00650</link><description>&lt;p&gt;
&#36890;&#36807;&#26435;&#37325;&#38598;&#25104;&#12289;&#22810;&#26679;&#24615;&#21152;&#26435;&#21644;&#20808;&#39564;&#26657;&#27491;&#23454;&#29616;&#36890;&#29992;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Universal Test-time Adaptation through Weight Ensembling, Diversity Weighting, and Prior Correction. (arXiv:2306.00650v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00650
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26435;&#37325;&#38598;&#25104;&#12289;&#22810;&#26679;&#24615;&#21152;&#26435;&#21644;&#20808;&#39564;&#26657;&#27491;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22240;&#20998;&#24067;&#20559;&#31227;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#28085;&#30422;&#22914;&#27492;&#24191;&#27867;&#33539;&#22260;&#30340;&#24037;&#20316;&#65292;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27979;&#35797;&#26102;&#38388;&#21487;&#33021;&#21457;&#29983;&#20998;&#24067;&#20559;&#31227;&#19988;&#21487;&#33021;&#20005;&#37325;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#22312;&#37096;&#32626;&#21518;&#32487;&#32493;&#26356;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#24403;&#21069;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;&#20026;&#20102;&#22788;&#29702;&#36890;&#29992;TTA&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#21464;&#37327;&#22240;&#32032;&#22495;&#38750;&#31283;&#24577;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#23545;&#25152;&#26377;&#23454;&#38469;&#30456;&#20851;&#30340;&#35774;&#32622;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#23558;&#20854;&#23450;&#20041;&#20026;&#36890;&#29992;TTA&#12290;&#25105;&#20204;&#24076;&#26395;&#24378;&#35843;&#30340;&#26159;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#28085;&#30422;&#22914;&#27492;&#24191;&#27867;&#33539;&#22260;&#30340;&#24037;&#20316;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36890;&#29992;TTA&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#24378;&#35843;&#20102;&#19968;&#20123;&#33258;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#24212;&#23545;&#30340;&#25361;&#25112;&#65306;1&#65289;&#27169;&#22411;&#20559;&#24046;&#21644;&#22312;&#19981;&#21516;&#24207;&#21015;&#38271;&#24230;&#19978;&#36827;&#34892;&#29109;&#26368;&#23567;&#21270;&#26102;&#20986;&#29616;&#30340;&#24179;&#20961;&#35299;&#20197;&#21450;&#23384;&#22312;&#22810;&#20010;&#39046;&#22495;&#36716;&#31227;&#65292;2&#65289;&#27867;&#21270;&#33021;&#21147;&#30340;&#20007;&#22833;&#65292;&#21152;&#21095;&#20102;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since distribution shifts are likely to occur during test-time and can drastically decrease the model's performance, online test-time adaptation (TTA) continues to update the model after deployment, leveraging the current test data. Clearly, a method proposed for online TTA has to perform well for all kinds of environmental conditions. By introducing the variable factors domain non-stationarity and temporal correlation, we first unfold all practically relevant settings and define the entity as universal TTA. We want to highlight that this is the first work that covers such a broad spectrum, which is indispensable for the use in practice. To tackle the problem of universal TTA, we identify and highlight several challenges a self-training based method has to deal with: 1) model bias and the occurrence of trivial solutions when performing entropy minimization on varying sequence lengths with and without multiple domain shifts, 2) loss of generalization which exacerbates the adaptation to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#39640;&#25928;&#25193;&#25955;&#31574;&#30053;&#65288;EDP&#65289;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#35745;&#31639;&#25928;&#29575;&#20302;&#21644;&#38590;&#20197;&#19982;&#26368;&#22823;&#20284;&#28982;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20860;&#23481;&#12290;EDP&#36890;&#36807;&#36817;&#20284;&#26500;&#24314;&#21160;&#20316;&#26469;&#36991;&#20813;&#36816;&#34892;&#37319;&#26679;&#38142;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.20081</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Diffusion Policies for Offline Reinforcement Learning. (arXiv:2305.20081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#39640;&#25928;&#25193;&#25955;&#31574;&#30053;&#65288;EDP&#65289;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#35745;&#31639;&#25928;&#29575;&#20302;&#21644;&#38590;&#20197;&#19982;&#26368;&#22823;&#20284;&#28982;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20860;&#23481;&#12290;EDP&#36890;&#36807;&#36817;&#20284;&#26500;&#24314;&#21160;&#20316;&#26469;&#36991;&#20813;&#36816;&#34892;&#37319;&#26679;&#38142;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#21442;&#25968;&#21270;&#26159;&#20851;&#38190;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#65292;Diffusion-QL&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#34920;&#31034;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#35813;&#27169;&#22411;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#21442;&#25968;&#21270;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#36827;&#34892;&#37319;&#26679;&#65292;&#20294;&#26159;Diffusion-QL&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290;1&#65289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#25972;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#36827;&#34892;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;2&#65289;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20284;&#28982;&#20989;&#25968;&#38590;&#20197;&#35745;&#31639;&#65292;&#19982;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65289;&#19981;&#20860;&#23481;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#25193;&#25955;&#31574;&#30053;&#65288;EDP&#65289;&#26469;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;EDP&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#20174;&#25439;&#22351;&#30340;&#21160;&#20316;&#20013;&#36817;&#20284;&#26500;&#24314;&#21160;&#20316;&#65292;&#36991;&#20813;&#20102;&#36816;&#34892;&#37319;&#26679;&#38142;&#12290;&#25105;&#20204;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EDP&#33021;&#22815;&#20943;&#23569;&#25193;&#25955;&#30340;&#28508;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#29616;&#35937;&#65292;&#36825;&#23558;&#20854;&#29983;&#25104;&#21160;&#21147;&#23398;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#8220;&#30456;&#8221;&#65292;&#25552;&#20986;&#20102;&#39640;&#26031;&#21518;&#26399;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19693</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;
&lt;/p&gt;
&lt;p&gt;
Spontaneous symmetry breaking in generative diffusion models. (arXiv:2305.19693v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#29616;&#35937;&#65292;&#36825;&#23558;&#20854;&#29983;&#25104;&#21160;&#21147;&#23398;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#8220;&#30456;&#8221;&#65292;&#25552;&#20986;&#20102;&#39640;&#26031;&#21518;&#26399;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#36817;&#26399;&#25104;&#20026;&#20102;&#29983;&#25104;&#39640;&#32500;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#23384;&#22312;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#65292;&#23558;&#29983;&#25104;&#24335;&#21160;&#21147;&#23398;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#8220;&#30456;&#8221;&#65306;1&#65289;&#20013;&#24515;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#32447;&#24615;&#31283;&#24577;&#21160;&#21147;&#23398;&#65292;2&#65289;&#26397;&#21521;&#25968;&#25454;&#27969;&#24418;&#30340;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#12290;&#36825;&#20004;&#31181;&#8220;&#30456;&#8221;&#30001;&#20013;&#24515;&#22266;&#23450;&#28857;&#31283;&#23450;&#24615;&#21464;&#21270;&#25152;&#20998;&#38548;&#65292;&#32780;&#19981;&#31283;&#23450;&#31383;&#21475;&#36127;&#36131;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20934;&#30830;&#22320;&#27169;&#25311;&#26089;&#26399;&#21160;&#21147;&#23398;&#24182;&#19981;&#20250;&#23545;&#26368;&#32456;&#29983;&#25104;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#26089;&#26399;&#28072;&#33853;&#20250;&#22238;&#21040;&#20013;&#24515;&#22266;&#23450;&#28857;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#26031;&#21518;&#26399;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#24555;&#36895;&#21462;&#26679;&#22120;&#19978;&#23454;&#29616;&#20102;&#38271;&#36798;3&#20493;&#30340;FID&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two "phases" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, whi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19470</link><description>&lt;p&gt;
&#29992;Johnson-Lindenstrauss&#30697;&#38453;&#36827;&#34892;&#26631;&#31614;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Label Embedding by Johnson-Lindenstrauss Matrices. (arXiv:2305.19470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19470
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Johnson-Lindenstrauss&#30697;&#38453;&#65288;JLMs&#65289;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#26497;&#31471;&#22810;&#20803;&#20998;&#31867;&#26694;&#26550;&#12290;&#21033;&#29992;JLM&#30340;&#21015;&#26469;&#23884;&#20837;&#26631;&#31614;&#65292;&#23558;&#19968;&#20010;C&#31867;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#20855;&#26377;$\cO(\log C)$&#36755;&#20986;&#32500;&#24230;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#36229;&#37327;&#39118;&#38505;&#38480;&#21046;&#65292;&#38416;&#26126;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;Massart&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#38477;&#32500;&#30340;&#24809;&#32602;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple and scalable framework for extreme multiclass classification based on Johnson-Lindenstrauss matrices (JLMs). Using the columns of a JLM to embed the labels, a $C$-class classification problem is transformed into a regression problem with $\cO(\log C)$ output dimension. We derive an excess risk bound, revealing a tradeoff between computational efficiency and prediction accuracy, and further show that under the Massart noise condition, the penalty for dimension reduction vanishes. Our approach is easily parallelizable, and experimental results demonstrate its effectiveness and scalability in large-scale applications.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Forward-Forward&#31639;&#27861;&#35757;&#32451;&#30340;&#32593;&#32476;&#20869;&#37096;&#34920;&#24449;&#20855;&#26377;&#39640;&#31232;&#30095;&#24230;&#65292;&#31867;&#21035;&#29305;&#23450;&#30340;&#38598;&#21512;&#65292;&#36825;&#19982;&#29983;&#29289;&#23398;&#35266;&#23519;&#21040;&#30340;&#30382;&#23618;&#34920;&#24449;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.18353</link><description>&lt;p&gt;
Forward-Forward&#31639;&#27861;&#35757;&#32451;&#30340;&#32593;&#32476;&#20013;&#30340;&#31361;&#29616;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Emergent representations in networks trained with the Forward-Forward algorithm. (arXiv:2305.18353v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18353
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Forward-Forward&#31639;&#27861;&#35757;&#32451;&#30340;&#32593;&#32476;&#20869;&#37096;&#34920;&#24449;&#20855;&#26377;&#39640;&#31232;&#30095;&#24230;&#65292;&#31867;&#21035;&#29305;&#23450;&#30340;&#38598;&#21512;&#65292;&#36825;&#19982;&#29983;&#29289;&#23398;&#35266;&#23519;&#21040;&#30340;&#30382;&#23618;&#34920;&#24449;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Backpropagation&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#20854;&#32570;&#20047;&#29983;&#29289;&#23398;&#19978;&#30340;&#29616;&#23454;&#24615;&#12290;&#20026;&#20102;&#23547;&#25214;&#19968;&#31181;&#26356;&#20855;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#36991;&#20813;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#65292;&#32780;&#26159;&#20351;&#29992;&#26412;&#22320;&#23398;&#20064;&#35268;&#21017;&#65292;&#26368;&#36817;&#20171;&#32461;&#30340;Forward-Forward&#31639;&#27861;&#23558;Backpropagation&#30340;&#20256;&#36882;&#26367;&#25442;&#20026;&#20004;&#20010;&#21069;&#21521;&#20256;&#36882;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;Forward-Forward&#31639;&#27861;&#33719;&#24471;&#30340;&#20869;&#37096;&#34920;&#24449;&#32452;&#32455;&#20026;&#31283;&#20581;&#30340;&#65292;&#31867;&#21035;&#29305;&#23450;&#30340;&#38598;&#21512;&#65292;&#30001;&#26497;&#23569;&#37327;&#30340;&#26377;&#25928;&#21333;&#20803;(&#39640;&#31232;&#30095;&#24230;)&#32452;&#25104;&#12290;&#36825;&#19982;&#24863;&#35273;&#22788;&#29702;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#30382;&#23618;&#34920;&#24449;&#38750;&#24120;&#30456;&#20284;&#12290;&#34429;&#28982;&#22312;&#20351;&#29992;&#26631;&#20934;Backpropagation&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#27809;&#26377;&#21457;&#29616;&#65292;&#20294;&#26159;&#22312;&#20351;&#29992;&#19982;Forward-Forward&#30456;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#32593;&#32476;&#20013;&#20063;&#20986;&#29616;&#20102;&#31232;&#30095;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;Forward-Forward&#25552;&#35758;&#30340;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#26356;&#25509;&#36817;&#29983;&#29289;&#23398;&#23398;&#20064;&#30340;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Backpropagation algorithm, widely used to train neural networks, has often been criticised for its lack of biological realism. In an attempt to find a more biologically plausible alternative, and avoid to back-propagate gradients in favour of using local learning rules, the recently introduced Forward-Forward algorithm replaces the traditional forward and backward passes of Backpropagation with two forward passes. In this work, we show that internal representations obtained with the Forward-Forward algorithm organize into robust, category-specific ensembles, composed by an extremely low number of active units (high sparsity). This is remarkably similar to what is observed in cortical representations during sensory processing. While not found in models trained with standard Backpropagation, sparsity emerges also in networks optimized by Backpropagation, on the same training objective of Forward-Forward. These results suggest that the learning procedure proposed by Forward-Forward ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#36716;&#25442;&#65292;&#19988;&#21518;&#24724;&#36880;&#28176;&#22686;&#21152;&#19982;&#23545;&#25163;&#30340;&#24694;&#24847;&#31243;&#24230;&#25104;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.17380</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#36716;&#25442;&#30340;&#26080;&#36951;&#25022;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions. (arXiv:2305.17380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#36716;&#25442;&#65292;&#19988;&#21518;&#24724;&#36880;&#28176;&#22686;&#21152;&#19982;&#23545;&#25163;&#30340;&#24694;&#24847;&#31243;&#24230;&#25104;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22312;&#19982;&#23545;&#25163;&#30340;$ T $&#36718;&#20132;&#20114;&#20043;&#21518;&#23454;&#29616;${ O}(\sqrt{T})$&#30340;&#21518;&#24724;&#65292;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#26159;&#30001;&#23545;&#25163;&#20219;&#24847;&#36873;&#25321;&#30340;&#65292;&#20294;&#21069;&#25552;&#26159;&#36716;&#31227;&#20989;&#25968;&#24517;&#39035;&#22266;&#23450;&#12290;&#36825;&#26159;&#22240;&#20026;&#24050;&#32463;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#36716;&#31227;&#20989;&#25968;&#20351;&#26080;&#24724;&#23398;&#20064;&#21464;&#24471;&#19981;&#21487;&#33021;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#31181;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#36716;&#25442;&#30340;&#31639;&#27861;&#65292;&#21518;&#24724;&#36880;&#28176;&#22686;&#21152;&#19982;&#23545;&#25163;&#30340;&#24694;&#24847;&#31243;&#24230;&#25104;&#27604;&#20363;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23427;&#30340;&#21518;&#24724;&#20026;$\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$&#65292;&#20854;&#20013;$C^{\textsf{P}}$&#34920;&#31034;&#36716;&#25442;&#20989;&#25968;&#30340;&#23545;&#25239;&#24615;&#65292;&#26368;&#22810;&#21487;&#20197;&#20026;${O}(T)$&#12290;&#34429;&#28982;&#27492;&#31639;&#27861;&#26412;&#36523;&#38656;&#35201;$C^{\textsf{P}}$&#30340;&#30693;&#35782;&#65292;&#20294;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#40657;&#30418;&#32553;&#20943;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#36827;&#19968;&#27493;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#38170;&#23450;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing online learning algorithms for adversarial Markov Decision Processes achieve ${O}(\sqrt{T})$ regret after $T$ rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys $\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$ regret where $C^{\textsf{P}}$ measures how adversarial the transition functions are and can be at most ${O}(T)$. While this algorithm itself requires knowledge of $C^{\textsf{P}}$, we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#22312;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#20013;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65292;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#38544;&#24335;&#29305;&#24449;&#36873;&#25321;&#24182;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.16905</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65306;&#36125;&#21494;&#26031;&#25512;&#29702;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference. (arXiv:2305.16905v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#22312;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#20013;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65292;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#38544;&#24335;&#29305;&#24449;&#36873;&#25321;&#24182;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#24615;&#36136;&#38459;&#30861;&#20102;&#35299;&#37322;&#24615;&#12290;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#20998;&#20026;&#21152;&#24615;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#20351;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20132;&#20114;&#21464;&#24471;&#26126;&#26174;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65306;a&#65289;&#23427;&#36890;&#36807;&#20272;&#35745;&#23376;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#19981;&#30830;&#23450;&#24615;&#20026;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65307;b&#65289;&#23427;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#32463;&#39564;&#36125;&#21494;&#26031;&#36807;&#31243;&#25191;&#34892;&#29305;&#24449;&#30340;&#38544;&#24335;&#36873;&#25321;&#65307;c&#65289;&#23427;&#21487;&#29992;&#20110;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#65292;&#20316;&#20026;&#31934;&#32454;&#35843;&#25972;&#30340;&#20132;&#20114;&#27169;&#22411;&#20505;&#36873;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;LA-NAM&#65289;&#25552;&#39640;&#20102;NAM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#23376;&#32593;&#32476;&#30340;&#20132;&#20114;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have found successful applications in many fields, but their black-box nature hinders interpretability. This is addressed by the neural additive model (NAM), in which the network is divided into additive sub-networks, thus making apparent the interaction between input features and predictions. In this paper, we approach the additive structure from a Bayesian perspective and develop a practical Laplace approximation. This enhances interpretability in three primary ways: a) It provides credible intervals for the recovered feature interactions by estimating function-space uncertainty of the sub-networks; b) it yields a tractable estimate of the marginal likelihood, which can be used to perform an implicit selection of features through an empirical Bayes procedure; and c) it can be used to rank feature pairs as candidates for second-order interactions in fine-tuned interaction models. We show empirically that our proposed Laplace-approximated NAM (LA-NAM) improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#25903;&#25345;&#29702;&#35770;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16427</link><description>&lt;p&gt;
&#31070;&#32463;&#65288;&#20999;&#21521;&#26680;&#65289;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Neural (Tangent Kernel) Collapse. (arXiv:2305.16427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#25903;&#25345;&#29702;&#35770;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#27010;&#24565;&#65306;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#65292;&#23427;&#25429;&#25417;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35757;&#32451;&#26399;&#38388;&#30340;&#28436;&#21270;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#29616;&#35937;&#65292;&#23427;&#25351;&#30340;&#26159;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#20998;&#31867;DNN&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20013;&#23545;&#31216;&#24615;&#21644;&#32467;&#26500;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#32463;&#39564;NTK&#19982;&#31867;&#26631;&#31614;&#23545;&#40784;&#24182;&#24418;&#25104;&#22359;&#29366;&#32467;&#26500;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#27604;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#26356;&#24378;&#65292;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#35757;&#32451;&#30340;DNN&#21160;&#24577;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#38454;&#27573;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#19981;&#21464;&#37327;&#65292;&#25429;&#25417;&#20102;&#21160;&#24577;&#30340;&#26412;&#36136;&#65292;&#24182;&#29992;&#23427;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#31181;&#24120;&#35265;DNN&#26550;&#26500;&#21644;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#25968;&#20540;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#22120;&#35757;&#32451;&#20316;&#20026;&#31890;&#23376;&#27169;&#22411;&#30340;&#19968;&#20010;&#25512;&#24191;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#31890;&#23376;&#21644;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23558;&#29983;&#25104;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#22312;&#27809;&#26377;&#29983;&#25104;&#22120;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;GAN&#12290;</title><link>http://arxiv.org/abs/2305.16150</link><description>&lt;p&gt;
&#32479;&#19968;GAN&#21644;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#30340;&#31890;&#23376;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unifying GANs and Score-Based Diffusion as Generative Particle Models. (arXiv:2305.16150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#22120;&#35757;&#32451;&#20316;&#20026;&#31890;&#23376;&#27169;&#22411;&#30340;&#19968;&#20010;&#25512;&#24191;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#31890;&#23376;&#21644;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23558;&#29983;&#25104;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#22312;&#27809;&#26377;&#29983;&#25104;&#22120;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#26799;&#24230;&#27969;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#24778;&#20154;&#30340;&#24615;&#33021;&#32780;&#26368;&#36817;&#21463;&#21040;&#20851;&#27880;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#24494;&#20998;&#26041;&#31243;&#26469;&#31227;&#21160;&#31890;&#23376;&#20998;&#24067;&#30340;&#26041;&#27861;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#19982;&#20197;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30456;&#23545;&#31435;&#30340;&#65292;&#21518;&#32773;&#28041;&#21450;&#21040;&#35757;&#32451;&#19968;&#20010;&#21521;&#21069;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#31181;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#31890;&#23376;&#21644;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#22120;&#35757;&#32451;&#20316;&#20026;&#31890;&#23376;&#27169;&#22411;&#30340;&#25512;&#24191;&#12290;&#36825;&#34920;&#26126;&#65292;&#29983;&#25104;&#22120;&#26159;&#20219;&#20309;&#36825;&#26679;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#36873;&#38468;&#20214;&#12290;&#22240;&#27492;&#65292;&#23558;&#29983;&#25104;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#22312;&#27809;&#26377;&#29983;&#25104;&#22120;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;GAN&#33258;&#28982;&#22320;&#20986;&#29616;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#27979;&#35797;&#36825;&#20123;&#21407;&#22987;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#25105;&#20204;&#26694;&#26550;&#21487;&#33021;&#24212;&#29992;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Particle-based deep generative models, such as gradient flows and score-based diffusion models, have recently gained traction thanks to their striking performance. Their principle of displacing particle distributions by differential equations is conventionally seen as opposed to the previously widespread generative adversarial networks (GANs), which involve training a pushforward generator network. In this paper, we challenge this interpretation and propose a novel framework that unifies particle and adversarial generative models by framing generator training as a generalization of particle models. This suggests that a generator is an optional addition to any such generative model. Consequently, integrating a generator into a score-based diffusion model and training a GAN without a generator naturally emerge from our framework. We empirically test the viability of these original models as proofs of concepts of potential applications of our framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#24635;&#25104;&#26412;&#38480;&#21046;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20915;&#31574;&#38382;&#39064;&#65288;CBwK&#65289;&#65292;&#36890;&#36807;&#23545;&#26415;&#35821;&#36827;&#34892;&#37325;&#26032;&#32452;&#21512;&#65292;&#23545;CBwK&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#25903;&#25345;&#23567;&#20110;$T^{3/4}$&#30340;&#24635;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23545;&#20598;&#31574;&#30053;&#23454;&#29616;&#20102;&#24179;&#31561;&#30340;&#25104;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15807</link><description>&lt;p&gt;
&#24102;&#23567;&#24635;&#25104;&#26412;&#38480;&#21046;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20915;&#31574;&#38382;&#39064;&#19982;&#32972;&#21253;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#65292;&#21450;&#20854;&#23545;&#20844;&#24179;&#24615;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness. (arXiv:2305.15807v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#24635;&#25104;&#26412;&#38480;&#21046;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20915;&#31574;&#38382;&#39064;&#65288;CBwK&#65289;&#65292;&#36890;&#36807;&#23545;&#26415;&#35821;&#36827;&#34892;&#37325;&#26032;&#32452;&#21512;&#65292;&#23545;CBwK&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#25903;&#25345;&#23567;&#20110;$T^{3/4}$&#30340;&#24635;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23545;&#20598;&#31574;&#30053;&#23454;&#29616;&#20102;&#24179;&#31561;&#30340;&#25104;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#24102;&#26377;&#32972;&#21253;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20915;&#31574;&#38382;&#39064;&#65288;CBwK&#65289;&#65292;&#27599;&#19968;&#36718;&#33719;&#24471;&#19968;&#20010;&#26631;&#37327;&#22870;&#21169;&#21644;&#19968;&#20010;&#21521;&#37327;&#20540;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#32047;&#35745;&#30340;&#22870;&#21169;&#65292;&#24182;&#30830;&#20445;&#32047;&#35745;&#25104;&#26412;&#20302;&#20110;&#26576;&#20010;&#39044;&#23450;&#30340;&#25104;&#26412;&#38480;&#21046;&#12290;&#25105;&#20204;&#20551;&#35774;&#29615;&#22659;&#26469;&#33258;&#19968;&#20010;&#36830;&#32493;&#38598;&#21512;&#65292;&#25104;&#26412;&#21487;&#20197;&#24102;&#31526;&#21495;&#65292;&#24182;&#19988;&#26410;&#30693;&#30340;&#26399;&#26395;&#22870;&#21169;&#21644;&#25104;&#26412;&#20989;&#25968;&#21487;&#20197;&#34987;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#20856;&#22411;&#20551;&#35774;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36804;&#20170;&#20026;&#27490;&#24635;&#25104;&#26412;&#32422;&#26463;&#33267;&#23569;&#35201;&#20026;$T^{3/4}$&#65292;&#20854;&#20013;$T$&#26159;&#36718;&#25968;&#65292;&#24182;&#19988;&#29978;&#33267;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#19982;$T$&#32447;&#24615;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21463;&#21040;&#40723;&#33310;&#65292;&#20351;&#29992;CBwK&#26469;&#24378;&#21046;&#23454;&#26045;&#23454;&#29616;&#32452;&#20043;&#38388;&#24179;&#22343;&#25104;&#26412;&#24179;&#31561;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65306;&#19982;&#30456;&#24212;&#25104;&#26412;&#32422;&#26463;&#30456;&#20851;&#30340;&#39044;&#31639;&#24212;&#23613;&#21487;&#33021;&#25509;&#36817;&#20110;&#38454;&#25968;&#20026;$\sqrt{T}$&#32423;&#21035;&#30340;&#33258;&#28982;&#20559;&#24046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider contextual bandit problems with knapsacks [CBwK], a problem where at each round, a scalar reward is obtained and vector-valued costs are suffered. The learner aims to maximize the cumulative rewards while ensuring that the cumulative costs are lower than some predetermined cost constraints. We assume that contexts come from a continuous set, that costs can be signed, and that the expected reward and cost functions, while unknown, may be uniformly estimated -- a typical assumption in the literature. In this setting, total cost constraints had so far to be at least of order $T^{3/4}$, where $T$ is the number of rounds, and were even typically assumed to depend linearly on $T$. We are however motivated to use CBwK to impose a fairness constraint of equalized average costs between groups: the budget associated with the corresponding cost constraints should be as close as possible to the natural deviations, of order $\sqrt{T}$. To that end, we introduce a dual strategy based on 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23398;&#20064;&#29575;&#26080;&#20851;&#30340;&#32422;&#26463;&#22495;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#37319;&#26679;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14943</link><description>&lt;p&gt;
&#23398;&#20064;&#29575;&#26080;&#20851;&#30340;&#32422;&#26463;&#22495;Bayesian&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Learning Rate Free Bayesian Inference in Constrained Domains. (arXiv:2305.14943v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14943
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23398;&#20064;&#29575;&#26080;&#20851;&#30340;&#32422;&#26463;&#22495;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#37319;&#26679;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#22495;&#20869;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#26159;&#23436;&#20840;&#19982;&#23398;&#20064;&#29575;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20984;&#20248;&#21270;&#20013;&#30340;&#30828;&#24065;&#25237;&#27880;&#24605;&#24819;&#65292;&#20197;&#21450;&#32422;&#26463;&#37319;&#26679;&#20316;&#20026;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#38236;&#20687;&#20248;&#21270;&#38382;&#39064;&#30340;&#35266;&#28857;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#20960;&#31181;&#29616;&#26377;&#30340;&#32422;&#26463;&#37319;&#26679;&#31639;&#27861;&#65292;&#21253;&#25324;&#38236;&#20687;Langevin&#21160;&#21147;&#23398;&#21644;&#38236;&#20687;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20174;&#21333;&#32431;&#24418;&#30446;&#26631;&#36827;&#34892;&#37319;&#26679;&#12289;&#24102;&#20844;&#24179;&#24615;&#32422;&#26463;&#36827;&#34892;&#37319;&#26679;&#20197;&#21450;&#21518;&#36873;&#25321;&#25512;&#26029;&#20013;&#30340;&#32422;&#26463;&#37319;&#26679;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19981;&#38656;&#35201;&#35843;&#25972;&#20219;&#20309;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#32422;&#26463;&#37319;&#26679;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a suite of new particle-based algorithms for sampling on constrained domains which are entirely learning rate free. Our approach leverages coin betting ideas from convex optimisation, and the viewpoint of constrained sampling as a mirrored optimisation problem on the space of probability measures. Based on this viewpoint, we also introduce a unifying framework for several existing constrained sampling algorithms, including mirrored Langevin dynamics and mirrored Stein variational gradient descent. We demonstrate the performance of our algorithms on a range of numerical examples, including sampling from targets on the simplex, sampling with fairness constraints, and constrained sampling problems in post-selection inference. Our results indicate that our algorithms achieve competitive performance with existing constrained sampling methods, without the need to tune any hyperparameters.
&lt;/p&gt;</description></item><item><title>Pre-RMSNorm&#21644;Pre-CRMSNorm Transformers&#26159;&#31561;&#25928;&#19988;&#39640;&#25928;&#30340;Pre-LN Transformers&#26550;&#26500;&#65292;&#21487;&#20197;&#32479;&#19968;&#20351;&#29992;&#20004;&#31181;&#20027;&#27969;&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;LayerNorm&#21644;RMSNorm&#65292;&#20174;&#32780;&#21152;&#36895;&#21644;&#31283;&#23450;Transformer&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.14858</link><description>&lt;p&gt;
Pre-RMSNorm&#21644;Pre-CRMSNorm Transformers: &#31561;&#25928;&#21644;&#39640;&#25928;&#30340;Pre-LN Transformers
&lt;/p&gt;
&lt;p&gt;
Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. (arXiv:2305.14858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14858
&lt;/p&gt;
&lt;p&gt;
Pre-RMSNorm&#21644;Pre-CRMSNorm Transformers&#26159;&#31561;&#25928;&#19988;&#39640;&#25928;&#30340;Pre-LN Transformers&#26550;&#26500;&#65292;&#21487;&#20197;&#32479;&#19968;&#20351;&#29992;&#20004;&#31181;&#20027;&#27969;&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;LayerNorm&#21644;RMSNorm&#65292;&#20174;&#32780;&#21152;&#36895;&#21644;&#31283;&#23450;Transformer&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#22914;Layer Normalization&#65288;LayerNorm&#65292;LN&#65289;&#21644;Root Mean Square Normalization&#65288;RMSNorm&#65289;&#65292;&#22312;&#21152;&#36895;&#21644;&#31283;&#23450;Transformer&#30340;&#35757;&#32451;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;LayerNorm&#23545;&#36755;&#20837;&#21521;&#37327;&#36827;&#34892;&#37325;&#26032;&#20013;&#24515;&#21270;&#21644;&#37325;&#26032;&#32553;&#25918;&#65292;&#32780;RMSNorm&#20165;&#25353;&#20854;RMS&#20540;&#37325;&#26032;&#32553;&#25918;&#21521;&#37327;&#12290;&#23613;&#31649;RMSNorm&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#65292;&#20294;&#21487;&#33021;&#20250;&#25439;&#23475;Transformer&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#30446;&#21069;&#20851;&#20110;&#39318;&#36873;&#24402;&#19968;&#21270;&#25216;&#26415;&#23578;&#26080;&#20849;&#35782;&#65292;&#22240;&#20026;&#19968;&#20123;&#27169;&#22411;&#20351;&#29992;LayerNorm&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#20351;&#29992;RMSNorm&#65292;&#23588;&#20854;&#26159;&#26368;&#36817;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23558;&#20855;&#26377;&#19968;&#31181;&#24402;&#19968;&#21270;&#30340;Transformer&#36716;&#25442;&#20026;&#21478;&#19968;&#31181;&#31867;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30446;&#21069;&#20004;&#31181;&#24402;&#19968;&#21270;&#31867;&#22411;&#20043;&#38388;&#23384;&#22312;&#20105;&#35758;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#32479;&#19968;&#20004;&#31181;&#20027;&#27969;Transformer&#26550;&#26500;&#65292;Pre-LN&#21644;Pre-RMSNorm Transformers&#12290;&#36890;&#36807;&#21435;&#38500;&#22266;&#26377;&#30340;&#20887;&#20313;&#22343;&#20540;&#20449;&#24687;&#26469;&#23454;&#29616;&#31561;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean informat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SEEDS&#65292;&#36825;&#26159;&#19968;&#31181;&#25351;&#25968;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#39640;&#36136;&#37327;&#30340;&#25277;&#26679;&#12290;&#19982;&#29616;&#26377;&#30340;&#24930;&#36895;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;SEEDS&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#36136;&#37327;&#36827;&#34892;&#27714;&#35299;&#65292;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#20540;&#20989;&#25968;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.14267</link><description>&lt;p&gt;
SEEDS: &#25351;&#25968;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#24555;&#36895;&#39640;&#36136;&#37327;&#22320;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models. (arXiv:2305.14267v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SEEDS&#65292;&#36825;&#26159;&#19968;&#31181;&#25351;&#25968;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#39640;&#36136;&#37327;&#30340;&#25277;&#26679;&#12290;&#19982;&#29616;&#26377;&#30340;&#24930;&#36895;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;SEEDS&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#36136;&#37327;&#36827;&#34892;&#27714;&#35299;&#65292;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#20540;&#20989;&#25968;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#34987;&#31216;&#20026;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#36880;&#28176;&#21521;&#25968;&#25454;&#28155;&#21152;&#22122;&#22768;&#65292;&#32780;&#27169;&#22411;&#36880;&#28176;&#23398;&#20064;&#21435;&#38500;&#22122;&#22768;&#12290;&#20174;&#39044;&#35757;&#32451;&#30340;DPMs&#20013;&#36827;&#34892;&#25277;&#26679;&#26159;&#36890;&#36807;&#35299;&#20915;&#30001;&#23398;&#20064;&#27169;&#22411;&#23450;&#20041;&#30340;&#24494;&#20998;&#26041;&#31243;(DE)&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#19968;&#36807;&#31243;&#34987;&#35777;&#26126;&#26159;&#36807;&#20110;&#32531;&#24930;&#30340;&#12290;&#35768;&#22810;&#21152;&#36895;&#36825;&#19968;&#36807;&#31243;&#30340;&#21162;&#21147;&#21253;&#25324;&#35774;&#35745;&#24378;&#22823;&#30340;ODE&#27714;&#35299;&#22120;&#12290;&#23613;&#31649;&#36895;&#24230;&#24456;&#24555;&#65292;&#20294;&#36825;&#20123;&#27714;&#35299;&#22120;&#36890;&#24120;&#26080;&#27861;&#36798;&#21040;&#29616;&#26377;&#24930;&#36895;SDE&#27714;&#35299;&#22120;&#25152;&#36798;&#21040;&#30340;&#26368;&#20339;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;SDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20960;&#30334;&#25110;&#19978;&#21315;&#27425;NFE(&#25968;&#20540;&#20989;&#25968;&#35780;&#20272;)&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#26368;&#20339;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#26174;&#24335;&#25351;&#25968;&#26080;&#23548;&#25968;&#27714;&#35299;&#22120;(SEEDS)&#65292;&#22312;&#22810;&#20010;&#26694;&#26550;&#19978;&#25913;&#36827;&#21644;&#25512;&#24191;&#20102;&#25351;&#25968;&#31215;&#20998;&#22120;&#26041;&#27861;&#20197;&#36866;&#24212;&#38543;&#26426;&#24773;&#20917;&#12290;&#22312;&#20180;&#32454;&#20998;&#26512;&#25193;&#25955;SDE&#30830;&#20999;&#35299;&#30340;&#20844;&#24335;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;SEEDS&#12290;
&lt;/p&gt;
&lt;p&gt;
A potent class of generative models known as Diffusion Probabilistic Models (DPMs) has become prominent. A forward diffusion process adds gradually noise to data, while a model learns to gradually denoise. Sampling from pre-trained DPMs is obtained by solving differential equations (DE) defined by the learnt model, a process which has shown to be prohibitively slow. Numerous efforts on speeding-up this process have consisted on crafting powerful ODE solvers. Despite being quick, such solvers do not usually reach the optimal quality achieved by available slow SDE solvers. Our goal is to propose SDE solvers that reach optimal quality without requiring several hundreds or thousands of NFEs to achieve that goal. We propose Stochastic Explicit Exponential Derivative-free Solvers (SEEDS), improving and generalizing Exponential Integrator approaches to the stochastic case on several frameworks. After carefully analyzing the formulation of exact solutions of diffusion SDEs, we craft SEEDS to a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#32500;&#24230;&#19979;&#20869;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24230;&#32780;&#19981;&#26159;&#32500;&#25968;&#65292;&#24182;&#35777;&#26126;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#20013;&#24230;&#23548;&#25968;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#24207;&#21015;&#26680;&#36827;&#34892;&#22238;&#24402;&#26159;&#21487;&#33021;&#20986;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.14077</link><description>&lt;p&gt;
&#35686;&#24789;&#23574;&#23792;&#65306;&#22266;&#23450;&#32500;&#24230;&#19979;&#20869;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. (arXiv:2305.14077v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#32500;&#24230;&#19979;&#20869;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24230;&#32780;&#19981;&#26159;&#32500;&#25968;&#65292;&#24182;&#35777;&#26126;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#20013;&#24230;&#23548;&#25968;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#24207;&#21015;&#26680;&#36827;&#34892;&#22238;&#24402;&#26159;&#21487;&#33021;&#20986;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36798;&#21040;&#25509;&#36817;&#38646;&#30340;&#35757;&#32451;&#35823;&#24046;&#30340;&#25104;&#21151;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#21363;&#20351;&#20272;&#35745;&#22120;&#25554;&#20540;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#36824;&#26159;&#20855;&#26377;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#26576;&#20123;&#23398;&#20064;&#26041;&#27861;&#30340;&#22266;&#23450;&#32500;&#24230;&#19979;&#24050;&#32463;&#30830;&#23450;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#23545;&#20110;&#20856;&#22411;&#20869;&#26680;&#26041;&#27861;&#21644;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#24402;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#38656;&#35201;&#39640;&#32500;&#24230;&#35774;&#32622;&#65292;&#20854;&#20013;&#32500;&#25968;&#38543;&#30528;&#26679;&#26412;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24230;&#26159;&#20851;&#38190;&#65292;&#32780;&#19981;&#26159;&#32500;&#25968;&#65306;&#21482;&#26377;&#24403;&#20272;&#35745;&#22120;&#30340;&#23548;&#25968;&#36275;&#22815;&#22823;&#26102;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#25165;&#21487;&#33021;&#21457;&#29983;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615;&#32467;&#26524;&#25512;&#24191;&#21040;&#38750;&#25554;&#20540;&#27169;&#22411;&#21644;&#26356;&#22810;&#20869;&#26680;&#65292;&#20197;&#34920;&#26126;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#20013;&#24230;&#23548;&#25968;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#24207;&#21015;&#26680;&#36827;&#34892;&#22238;&#24402;&#26159;&#21487;&#33021;&#20986;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of over-parameterized neural networks trained to near-zero training error has caused great interest in the phenomenon of benign overfitting, where estimators are statistically consistent even though they interpolate noisy training data. While benign overfitting in fixed dimension has been established for some learning methods, current literature suggests that for regression with typical kernel methods and wide neural networks, benign overfitting requires a high-dimensional setting where the dimension grows with the sample size. In this paper, we show that the smoothness of the estimators, and not the dimension, is the key: benign overfitting is possible if and only if the estimator's derivatives are large enough. We generalize existing inconsistency results to non-interpolating models and more kernels to show that benign overfitting with moderate derivatives is impossible in fixed dimension. Conversely, we show that benign overfitting is possible for regression with a seque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13632</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#33258;&#21160;&#20135;&#29983;&#30340;&#25688;&#35201;&#21487;&#33021;&#27969;&#30021;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23545;&#21407;&#22987;&#25991;&#26723;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#65292;&#22914;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#33521;&#35821;&#65292;&#22240;&#27492;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#29978;&#33267;&#34913;&#37327;&#36825;&#31181;&#29616;&#35937;&#30340;&#31243;&#24230;&#20063;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#33521;&#35821;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#32467;&#26524;&#20013;&#20511;&#37492;&#32763;&#35793;&#22522;&#30784;&#30693;&#35782;&#20026;&#38750;&#33521;&#35821;&#25688;&#35201;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#28982;&#21518;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#24187;&#35273;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#20056;&#20197;&#20854;&#24544;&#23454;&#24615;&#24471;&#20998;&#12290;&#36890;&#36807;&#22810;&#31181;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;mFACT&#26159;&#26368;&#36866;&#21512;&#26816;&#27979;&#24187;&#35273;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#25552;&#20986;&#30340;&#21152;&#26435;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;&#8212;&#8212;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65292;&#20854;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#21644;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#20855;&#26377;&#38381;&#24615;&#26465;&#20214;&#25512;&#26029;&#21644;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.13552</link><description>&lt;p&gt;
&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65306;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;
&lt;/p&gt;
&lt;p&gt;
Squared Neural Families: A New Class of Tractable Density Models. (arXiv:2305.13552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13552
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;&#8212;&#8212;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65292;&#20854;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#21644;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#20855;&#26377;&#38381;&#24615;&#26465;&#20214;&#25512;&#26029;&#21644;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20998;&#24067;&#30340;&#28789;&#27963;&#27169;&#22411;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#20998;&#24067;&#31867;&#21035;&#65292;&#31216;&#20026;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65288;SNEFY&#65289;&#65292;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#24182;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;&#31867;&#20284;&#20110;&#26080;&#31351;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#24191;&#27867;&#32852;&#31995;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#24773;&#20917;&#19979;&#65292;SNEFY&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#65292;&#22240;&#27492;&#26159;&#28789;&#27963;&#19988;&#23436;&#20840;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#12290;SNEFY&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#30340;&#25351;&#25968;&#26063;&#65292;&#23545;&#20110;&#26465;&#20214;&#25512;&#26029;&#20855;&#26377;&#38381;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23494;&#24230;&#20272;&#35745;&#21644;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#20219;&#21153;&#20013;&#23637;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33609;&#22270;&#21644;&#25237;&#24433;&#30340;Newton&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#33258;&#20849;&#36717;&#20989;&#25968;&#65292;&#20855;&#26377;&#33609;&#22270;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#20840;&#31209;Newton&#31867;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#20840;&#23616;&#25910;&#25947;&#29575;&#20197;&#21450;&#38459;&#23612;Newton&#26041;&#27861;&#30340;&#31639;&#27861;&#31616;&#21333;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13082</link><description>&lt;p&gt;
Sketch-and-Project Meets Newton Method: &#20855;&#26377;&#20302;&#31209;&#26356;&#26032;&#30340;&#20840;&#23616;$\mathcal O(k^{-2})$ &#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sketch-and-Project Meets Newton Method: Global $\mathcal O(k^{-2})$ Convergence with Low-Rank Updates. (arXiv:2305.13082v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33609;&#22270;&#21644;&#25237;&#24433;&#30340;Newton&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#33258;&#20849;&#36717;&#20989;&#25968;&#65292;&#20855;&#26377;&#33609;&#22270;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#20840;&#31209;Newton&#31867;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#20840;&#23616;&#25910;&#25947;&#29575;&#20197;&#21450;&#38459;&#23612;Newton&#26041;&#27861;&#30340;&#31639;&#27861;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33609;&#22270;&#21644;&#25237;&#24433;&#30340;Newton&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;$\mathcal O(k^{-2})$ &#20840;&#23616;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#33258;&#20849;&#36717;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#19977;&#20010;&#26041;&#38754;&#26469;&#30475;&#24453;&#65306;i) &#20316;&#20026;&#19968;&#20010;&#33609;&#22270;&#21644;&#25237;&#24433;&#31639;&#27861;&#65292;&#23545;Newton&#26041;&#27861;&#30340;&#26356;&#26032;&#36827;&#34892;&#25237;&#24433;&#65292;ii) &#20316;&#20026;&#22312;&#34987;&#33609;&#22270;&#21270;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#31435;&#26041;&#27491;&#21017;&#21270;&#30340;Newton&#26041;&#27861;&#65292;&#21644; iii) &#20316;&#20026;&#22312;&#34987;&#33609;&#22270;&#21270;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#38459;&#23612;Newton&#26041;&#27861;&#12290;SGN&#32487;&#25215;&#20102;&#36825;&#19977;&#20010;&#26041;&#38754;&#30340;&#20248;&#28857;&#65306;&#33609;&#22270;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#20840;&#31209;Newton&#31867;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;$\mathcal O(k^{-2})$&#20840;&#23616;&#25910;&#25947;&#29575;&#20197;&#21450;&#38459;&#23612;Newton&#26041;&#27861;&#30340;&#31639;&#27861;&#31616;&#21333;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#19982;&#22522;&#20934;&#31639;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the first sketch-and-project Newton method with fast $\mathcal O(k^{-2})$ global convergence rate for self-concordant functions. Our method, SGN, can be viewed in three ways: i) as a sketch-and-project algorithm projecting updates of Newton method, ii) as a cubically regularized Newton ethod in sketched subspaces, and iii) as a damped Newton method in sketched subspaces. SGN inherits best of all three worlds: cheap iteration costs of sketch-and-project methods, state-of-the-art $\mathcal O(k^{-2})$ global convergence rate of full-rank Newton-like methods and the algorithm simplicity of damped Newton methods. Finally, we demonstrate its comparable empirical performance to baseline algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#26041;&#27861;&#30340;&#26080;&#20998;&#24067;&#27169;&#22411;&#26080;&#20559;&#22238;&#24402;&#26657;&#20934;&#26041;&#27861;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#32479;&#35745;&#19968;&#33268;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#30340;&#32479;&#35745;&#20445;&#35777;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.12283</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#26041;&#27861;&#19979;&#30340;&#26080;&#20998;&#24067;&#27169;&#22411;&#26080;&#20559;&#22238;&#24402;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods. (arXiv:2305.12283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#26041;&#27861;&#30340;&#26080;&#20998;&#24067;&#27169;&#22411;&#26080;&#20559;&#22238;&#24402;&#26657;&#20934;&#26041;&#27861;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#32479;&#35745;&#19968;&#33268;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#30340;&#32479;&#35745;&#20445;&#35777;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22238;&#24402;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#26041;&#27861;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#24213;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#32479;&#35745;&#20445;&#35777;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the uncertainty quantification problem for regression models. Specifically, we consider an individual calibration objective for characterizing the quantiles of the prediction model. While such an objective is well-motivated from downstream tasks such as newsvendor cost, the existing methods have been largely heuristic and lack of statistical guarantee in terms of individual calibration. We show via simple examples that the existing methods focusing on population-level calibration guarantees such as average calibration or sharpness can lead to harmful and unexpected results. We propose simple nonparametric calibration methods that are agnostic of the underlying prediction model and enjoy both computational efficiency and statistical consistency. Our approach enables a better understanding of the possibility of individual calibration, and we establish matching upper and lower bounds for the calibration error of our proposed methods. Technically, our analysis co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PC&#30340;&#26032;&#22411;&#26102;&#24207;&#35760;&#24518;&#27169;&#22411;&#65292;&#31216;&#20026;&#26102;&#38388;&#39044;&#27979;&#32534;&#30721;&#65288;tPC&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#23454;&#29616;&#20934;&#30830;&#22320;&#35760;&#24518;&#21644;&#26816;&#32034;&#36830;&#32493;&#36755;&#20837;&#12290;&#20854;&#20013;tPC&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#32463;&#20856;&#24322;&#21521;&#24615;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#65288;AHN&#65289;&#65292;&#20855;&#26377;&#26356;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#32534;&#30721;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#65292;&#21306;&#20998;&#22312;&#24207;&#21015;&#20013;&#20986;&#29616;&#30340;&#37325;&#22797;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.11982</link><description>&lt;p&gt;
&#24102;&#26377;&#26102;&#38388;&#39044;&#27979;&#32534;&#30721;&#30340;&#26102;&#24207;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Sequential Memory with Temporal Predictive Coding. (arXiv:2305.11982v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PC&#30340;&#26032;&#22411;&#26102;&#24207;&#35760;&#24518;&#27169;&#22411;&#65292;&#31216;&#20026;&#26102;&#38388;&#39044;&#27979;&#32534;&#30721;&#65288;tPC&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#23454;&#29616;&#20934;&#30830;&#22320;&#35760;&#24518;&#21644;&#26816;&#32034;&#36830;&#32493;&#36755;&#20837;&#12290;&#20854;&#20013;tPC&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#32463;&#20856;&#24322;&#21521;&#24615;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#65288;AHN&#65289;&#65292;&#20855;&#26377;&#26356;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#32534;&#30721;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#65292;&#21306;&#20998;&#22312;&#24207;&#21015;&#20013;&#20986;&#29616;&#30340;&#37325;&#22797;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29983;&#29289;&#20307;&#23384;&#20648;&#20107;&#20214;&#24207;&#21015;&#30340;&#26102;&#38388;&#39034;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#33041;&#20013;&#25903;&#37197;&#26102;&#24207;&#35760;&#24518;&#30340;&#35745;&#31639;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#21644;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#22312;&#38745;&#24577;&#23384;&#20648;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PC&#30340;&#26032;&#22411;&#26102;&#24207;&#35760;&#24518;&#27169;&#22411;&#65292;&#31216;&#20026;&#26102;&#38388;&#39044;&#27979;&#32534;&#30721;&#65288;tPC&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;tPC&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#23454;&#29616;&#20934;&#30830;&#22320;&#35760;&#24518;&#21644;&#26816;&#32034;&#36830;&#32493;&#36755;&#20837;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#30740;&#31350;&#34920;&#26126;&#65292;tPC&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#20855;&#26377;&#38544;&#24335;&#32479;&#35745;&#30333;&#21270;&#36807;&#31243;&#30340;&#32463;&#20856;&#24322;&#21521;&#24615;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#65288;AHN&#65289;&#65292;&#36825;&#20250;&#22312;&#32467;&#26500;&#21270;&#36755;&#20837;&#30340;&#26102;&#24207;&#35760;&#24518;&#20219;&#21153;&#20013;&#23548;&#33268;&#26356;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#22810;&#23618;&#32467;&#26500;&#30340;tPC&#21487;&#20197;&#32534;&#30721;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#65292;&#22240;&#27492;&#21487;&#20197;&#21306;&#20998;&#22312;&#24207;&#21015;&#20013;&#20986;&#29616;&#30340;&#37325;&#22797;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorizing the temporal order of event sequences is critical for the survival of biological agents. However, the computational mechanism underlying sequential memory in the brain remains unclear. Inspired by neuroscience theories and recent successes in applying predictive coding (PC) to static memory tasks, in this work we propose a novel PC-based model for sequential memory, called temporal predictive coding (tPC). We show that our tPC models can memorize and retrieve sequential inputs accurately with a biologically plausible neural implementation. Importantly, our analytical study reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN) with an implicit statistical whitening process, which leads to more stable performance in sequential memory tasks of structured inputs. Moreover, we find that tPC with a multi-layer structure can encode context-dependent information, thus distinguishing between repeating elements appearing in a sequence, a computation attribute
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#33976;&#39311;&#23631;&#34109;&#26469;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11685</link><description>&lt;p&gt;
&#22238;&#25910;&#21644;&#31934;&#39311;&#65306;&#24102;&#26377;&#27880;&#24847;&#21147;&#26144;&#23556;&#37325;&#29992;&#21644;&#33976;&#39311;&#23631;&#34109;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#33976;&#39311;&#23631;&#34109;&#26469;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899; SSL &#27169;&#22411;&#20013;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#38656;&#35201;&#21387;&#32553;&#25104;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#23398;&#26415;&#30028;&#25110;&#23567;&#20844;&#21496;&#20013;&#26356;&#24191;&#27867;&#22320;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#24314;&#35758;&#37325;&#29992;Transformer&#23618;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#65292;&#22240;&#27492;&#21487;&#20197;&#21024;&#38500;&#38190;&#21644;&#26597;&#35810;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#23618;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#33976;&#39311;&#25439;&#22833;&#65292;&#21033;&#29992;&#36974;&#32617;&#21644;&#26410;&#36974;&#32617;&#30340;&#35821;&#38899;&#24103;&#65292;&#20805;&#20998;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#20135;&#29983;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;SUPERB&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;7.72%&#30340;&#38899;&#32032;&#35823;&#24046;&#29575;&#65288;PER&#65289;&#21644;9.96%&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.11531</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#26032;&#22411;&#30005;&#30913;&#37327;&#35745;&#20960;&#20309;&#27169;&#25311;&#30340;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11531
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#29983;&#25104;&#23545;&#25758;&#20135;&#29289;&#30340;&#27169;&#25311;&#25506;&#27979;&#22120;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#12290;&#20854;&#20013;&#19968;&#20010;&#23376;&#25506;&#27979;&#22120;&#65292;&#30005;&#30913;&#37327;&#35745;&#30001;&#20110;&#20854;&#21333;&#20803;&#26684;&#30340;&#39640;&#31890;&#24230;&#21644;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#32780;&#21344;&#25454;&#20102;&#35745;&#31639;&#26102;&#38388;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#24555;&#30340;&#26679;&#26412;&#29983;&#25104;&#65292;&#20294;&#30446;&#21069;&#38656;&#35201;&#22823;&#37327;&#21162;&#21147;&#26469;&#20248;&#21270;&#29305;&#23450;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#32593;&#32476;&#26469;&#25551;&#36848;&#19981;&#21516;&#30340;&#21333;&#20803;&#26684;&#22823;&#23567;&#21644;&#25490;&#21015;&#26041;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#33021;&#25512;&#24191;&#21040;&#20854;&#20182;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#8220;&#20960;&#20309;&#24863;&#30693;&#8221;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#29983;&#25104;&#30475;&#19981;&#35265;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#27169;&#25311;&#21709;&#24212;&#32780;&#26080;&#38656;&#20854;&#20182;&#35757;&#32451;&#12290;&#35813;&#20960;&#20309;&#24863;&#30693;&#27169;&#22411;&#22312;&#28041;&#21450;&#20851;&#38190;&#21709;&#24212;&#30340;&#29983;&#25104;&#21644;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#31561;&#25351;&#26631;&#19978;&#27604;&#22522;&#32447;&#27169;&#22411;&#20248;&#36234;50&#65285;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#25193;&#23637;&#21040;&#38750;&#24179;&#38754;&#20960;&#20309;&#24418;&#29366;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of simulated detector response to collision products is crucial to data analysis in particle physics, but computationally very expensive. One subdetector, the calorimeter, dominates the computational time due to the high granularity of its cells and complexity of the interaction. Generative models can provide more rapid sample production, but currently require significant effort to optimize performance for specific detector geometries, often requiring many networks to describe the varying cell sizes and arrangements, which do not generalize to other geometries. We develop a {\it geometry-aware} autoregressive model, which learns how the calorimeter response varies with geometry, and is capable of generating simulated responses to unseen geometries without additional training. The geometry-aware model outperforms a baseline, unaware model by 50\% in metrics such as the Wasserstein distance between generated and true distributions of key quantities which summarize the simulate
&lt;/p&gt;</description></item><item><title>ResidualPlanner&#26159;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36793;&#32536;&#30340;&#30697;&#38453;&#26426;&#21046;&#65292;&#26082;&#20248;&#21270;&#21448;&#21487;&#25193;&#23637;&#65292;&#21487;&#20197;&#20248;&#21270;&#35768;&#22810;&#21487;&#20197;&#20889;&#25104;&#36793;&#38469;&#26041;&#24046;&#30340;&#20984;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.08175</link><description>&lt;p&gt;
&#19968;&#31181;&#20248;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#30697;&#38453;&#26426;&#21046;&#29992;&#20110;&#25200;&#21160;&#36793;&#32536;&#25968;&#25454;&#19979;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions. (arXiv:2305.08175v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08175
&lt;/p&gt;
&lt;p&gt;
ResidualPlanner&#26159;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36793;&#32536;&#30340;&#30697;&#38453;&#26426;&#21046;&#65292;&#26082;&#20248;&#21270;&#21448;&#21487;&#25193;&#23637;&#65292;&#21487;&#20197;&#20248;&#21270;&#35768;&#22810;&#21487;&#20197;&#20889;&#25104;&#36793;&#38469;&#26041;&#24046;&#30340;&#20984;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25200;&#21160;&#30340;&#36793;&#32536;&#25968;&#25454;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24418;&#24335;&#65292;&#21487;&#29992;&#20110;&#35832;&#22914;&#21015;&#32852;&#34920;&#20998;&#26512;&#12289;&#36125;&#21494;&#26031;&#32593;&#32476;&#26500;&#24314;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ResidualPlanner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36793;&#32536;&#30340;&#30697;&#38453;&#26426;&#21046;&#65292;&#26082;&#20248;&#21270;&#21448;&#21487;&#25193;&#23637;&#12290;ResidualPlanner&#21487;&#20197;&#20248;&#21270;&#35768;&#22810;&#21487;&#20197;&#20889;&#25104;&#36793;&#38469;&#26041;&#24046;&#30340;&#20984;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;ResidualPlanner&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20248;&#21270;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#30340;&#36793;&#32536;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;HDMM&#65289;&#20063;&#20250;&#21344;&#29992;&#36807;&#22810;&#30340;&#20869;&#23384;&#12290;&#29978;&#33267;&#22312;&#20855;&#26377;100&#20010;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#20063;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;ResidualPlanner&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#27599;&#20010;&#36793;&#32536;&#30340;&#26041;&#24046;/&#21327;&#26041;&#24046;&#20540;&#65288;&#20043;&#21069;&#30340;&#26041;&#27861;&#20250;&#24456;&#24555;&#22833;&#36133;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noisy marginals are a common form of confidentiality-protecting data release and are useful for many downstream tasks such as contingency table analysis, construction of Bayesian networks, and even synthetic data generation. Privacy mechanisms that provide unbiased noisy answers to linear queries (such as marginals) are known as matrix mechanisms.  We propose ResidualPlanner, a matrix mechanism for marginals with Gaussian noise that is both optimal and scalable. ResidualPlanner can optimize for many loss functions that can be written as a convex function of marginal variances (prior work was restricted to just one predefined objective function). ResidualPlanner can optimize the accuracy of marginals in large scale settings in seconds, even when the previous state of the art (HDMM) runs out of memory. It even runs on datasets with 100 attributes in a couple of minutes. Furthermore ResidualPlanner can efficiently compute variance/covariance values for each marginal (prior methods quickly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03598</link><description>&lt;p&gt;
NLI4CT&#65306;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35299;&#37322;&#21644;&#26816;&#32034;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#21307;&#23398;&#35777;&#25454;&#65311;&#22810;&#24180;&#26469;&#65292;&#31215;&#32047;&#19979;&#26469;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21253;&#21547;&#20102;&#21457;&#23637;&#20010;&#24615;&#21270;&#21307;&#23398;&#25152;&#24517;&#38656;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23454;&#39564;&#27835;&#30103;&#35777;&#25454;&#65292;&#25163;&#21160;&#26816;&#26597;&#36229;&#36807;400,000&#20010;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#26159;&#23454;&#38469;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#21487;&#25193;&#23637;&#35745;&#31639;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLI&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20043;&#21069;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;CTR&#25512;&#29702;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#65292;&#20197;&#25512;&#36827;&#20851;&#20110;CTR&#25512;&#29702;&#30340;NLI&#30740;&#31350;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NLI4CT&#65292;&#19968;&#20010;&#22522;&#20110;CTR&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00633</link><description>&lt;p&gt;
&#20998;&#35299;&#22686;&#24378;&#25512;&#29702;&#30340;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26463;&#25628;&#32034;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#22312;GSM8K&#12289;AQUA&#21644;StrategyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23569;&#37327;&#31034;&#20363;&#20934;&#30830;&#24615;&#20998;&#21035;&#36229;&#36234;&#23545;&#24212;&#30340;Codex-backboned&#22522;&#32447;$6.34\%$&#12289;$9.56\%$&#21644;$5.46\%$&#12290;&#23545;&#25105;&#20204;&#30340;&#20998;&#35299;&#24335;&#25512;&#29702;&#20998;&#26512;&#21457;&#29616;&#65292;&#23427;&#21487;&#20197;&#25351;&#20986;&#36923;&#36753;&#38169;&#35823;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Control3Diff&#30340;&#19977;&#32500;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;3D GANs&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#29992;&#20110;&#21333;&#35270;&#22270;&#25968;&#25454;&#38598;&#30340;&#22810;&#21151;&#33021;&#12289;&#21487;&#25511;&#30340;&#19977;&#32500;&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.06700</link><description>&lt;p&gt;
&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#23398;&#20064;&#21487;&#25511;&#19977;&#32500;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Controllable 3D Diffusion Models from Single-view Images. (arXiv:2304.06700v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Control3Diff&#30340;&#19977;&#32500;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;3D GANs&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#29992;&#20110;&#21333;&#35270;&#22270;&#25968;&#25454;&#38598;&#30340;&#22810;&#21151;&#33021;&#12289;&#21487;&#25511;&#30340;&#19977;&#32500;&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;2D&#39046;&#22495;&#29983;&#25104;&#24314;&#27169;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33719;&#21462;&#19977;&#32500;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#22256;&#38590;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#19977;&#32500;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#38544;&#24335;&#19977;&#32500;&#34920;&#31034;&#38598;&#25104;&#21040;GANs&#20013;&#30340;3D GANs&#22312;&#20165;&#35757;&#32451;&#21333;&#35270;&#22270;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#23637;&#31034;&#20102;&#26174;&#30528;&#30340;3D&#24863;&#30693;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;3D GANs&#27809;&#26377;&#25552;&#20379;&#31934;&#30830;&#25511;&#21046;&#22270;&#20687;&#21512;&#25104;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Control3Diff&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;3D GANs&#20248;&#28857;&#30340;&#19977;&#32500;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;&#25968;&#25454;&#38598;&#30340;&#22810;&#21151;&#33021;&#65292;&#21487;&#25511;&#30340;&#19977;&#32500;&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#12290;Control3Diff&#26126;&#30830;&#22320;&#24314;&#27169;&#20102;&#28508;&#22312;&#30340;&#28508;&#22312;&#20998;&#24067;&#65288;&#21487;&#20197;&#26159;&#22806;&#37096;&#36755;&#20837;&#26465;&#20214;&#19979;&#30340;&#28508;&#22312;&#20998;&#24067;&#65289;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#30452;&#25509;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#29992;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#25511;&#21046;&#36755;&#20837;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#30456;&#21516;&#30340;&#22522;&#30784;&#20307;&#31995;&#32467;&#26500;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any type of controlling input, allowing us to train it with the same di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11249</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65311;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#26469;&#33258;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#38024;&#23545;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#22312;&#26576;&#20123;&#29305;&#24449;&#30340;&#35268;&#33539;&#21010;&#20998;&#19979;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#25509;&#21463;&#20302;&#37327;&#23376;&#32416;&#32544;&#26102;&#65292;&#29305;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25165;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#35813;&#25968;&#25454;&#20998;&#24067;&#12290;&#20316;&#20026;&#26412;&#32467;&#26524;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#24067;&#36866;&#21512;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;&#24191;&#27867;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#32416;&#32544;&#23558;&#40723;&#21169;&#24418;&#24335;&#25512;&#29702;&#30340;&#29289;&#29702;&#24037;&#20855;&#26469;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#25429;&#33719;&#20102;&#22810;&#20307;&#21704;&#23494;&#39039;&#37327;&#20013;&#30340;&#25299;&#25169;&#24207;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26159;&#30740;&#31350;&#29289;&#24577;&#30456;&#30340;&#19968;&#20010;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.11207</link><description>&lt;p&gt;
&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#25299;&#25169;&#24207;
&lt;/p&gt;
&lt;p&gt;
Investigating Topological Order using Recurrent Neural Networks. (arXiv:2303.11207v2 [cond-mat.str-el] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#25429;&#33719;&#20102;&#22810;&#20307;&#21704;&#23494;&#39039;&#37327;&#20013;&#30340;&#25299;&#25169;&#24207;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26159;&#30740;&#31350;&#29289;&#24577;&#30456;&#30340;&#19968;&#20010;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#22312;&#25551;&#36848;&#24378;&#20851;&#32852;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#26041;&#38754;&#20063;&#24456;&#26377;&#21069;&#36884;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20108;&#32500;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#20004;&#20010;&#20856;&#22411;&#23637;&#29616;&#25299;&#25169;&#24207;&#30340;&#22810;&#20307;&#21704;&#23494;&#39039;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20272;&#31639;&#20854;&#25299;&#25169;&#32416;&#32544;&#29109;&#65292;&#26377;&#25928;&#22320;&#25429;&#33719;&#25197;&#26354;&#32534;&#30721;&#21644;&#34562;&#24034;&#26684;&#19978; Bose-Hubbard &#33258;&#26059;&#28082;&#20307;&#30340;&#25299;&#25169;&#24207;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26356;&#20542;&#21521;&#20110;&#21033;&#29992;&#24456;&#23569;&#32416;&#32544;&#29366;&#24577;&#30340;&#30456;&#24178;&#21472;&#21152;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26159;&#30740;&#31350;&#19981;&#23545;&#31216;&#30772;&#32570;&#20043;&#22806;&#29289;&#24577;&#30456;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks (RNNs), originally developed for natural language processing, hold great promise for accurately describing strongly correlated quantum many-body systems. Here, we employ 2D RNNs to investigate two prototypical quantum many-body Hamiltonians exhibiting topological order. Specifically, we demonstrate that RNN wave functions can effectively capture the topological order of the toric code and a Bose-Hubbard spin liquid on the kagome lattice by estimating their topological entanglement entropies. We also find that RNNs favor coherent superpositions of minimally-entangled states over minimally-entangled states themselves. Overall, our findings demonstrate that RNN wave functions constitute a powerful tool to study phases of matter beyond Landau's symmetry-breaking paradigm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;OWA&#30340;&#38142;&#25509;&#65292;Lance-Williams&#20844;&#24335;&#21644;&#26641;&#26525;&#21453;&#36716;&#25216;&#26415;&#65292;&#25512;&#24191;&#20102;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26465;&#20214;&#29992;&#20110;&#20445;&#35777;&#32467;&#26524;&#30340;&#26641;&#26525;&#22270;&#27809;&#26377;&#19981;&#32654;&#35266;&#30340;&#21453;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.05683</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;OWA&#30340;&#38142;&#25509;&#12289;Lance-Williams&#20844;&#24335;&#21644;&#26641;&#26525;&#21453;&#36716;&#30340;&#23618;&#27425;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical clustering with OWA-based linkages, the Lance-Williams formula, and dendrogram inversions. (arXiv:2303.05683v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;OWA&#30340;&#38142;&#25509;&#65292;Lance-Williams&#20844;&#24335;&#21644;&#26641;&#26525;&#21453;&#36716;&#25216;&#26415;&#65292;&#25512;&#24191;&#20102;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26465;&#20214;&#29992;&#20110;&#20445;&#35777;&#32467;&#26524;&#30340;&#26641;&#26525;&#22270;&#27809;&#26377;&#19981;&#32654;&#35266;&#30340;&#21453;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#65288;OWA&#65289;&#31639;&#23376;&#30340;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#19981;&#20165;&#25512;&#24191;&#20102;&#21333;&#20010;&#30340;&#12289;&#23436;&#20840;&#30340;&#21644;&#24179;&#22343;&#30340;&#38142;&#25509;&#65292;&#36824;&#21253;&#25324;&#22522;&#20110;&#19968;&#20123;&#26368;&#36817;&#25110;&#26368;&#36828;&#37051;&#23621;&#12289;&#20462;&#21098;&#21644;&#20462;&#25972;&#30340;&#28857;&#23545;&#30456;&#20284;&#24615;&#30340;&#24179;&#22343;&#20540;&#30340;&#38598;&#32676;&#38388;&#36317;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#33879;&#21517;&#30340;Lance-Williams&#26356;&#26032;&#20844;&#24335;&#19982;&#25193;&#23637;&#30340;&#22522;&#20110;OWA&#30340;&#38142;&#25509;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#26080;&#38480;&#31995;&#25968;&#24207;&#21015;&#29983;&#25104;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26465;&#20214;&#65292;&#20197;&#20445;&#35777;&#26435;&#37325;&#29983;&#25104;&#22120;&#20135;&#29983;&#30340;&#26641;&#26525;&#22270;&#19981;&#20250;&#20986;&#29616;&#19981;&#32654;&#35266;&#30340;&#21453;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agglomerative hierarchical clustering based on Ordered Weighted Averaging (OWA) operators not only generalises the single, complete, and average linkages, but also includes intercluster distances based on a few nearest or farthest neighbours, trimmed and winsorised means of pairwise point similarities, amongst many others. We explore the relationships between the famous Lance-Williams update formula and the extended OWA-based linkages with weights generated via infinite coefficient sequences. Furthermore, we provide some conditions for the weight generators to guarantee the resulting dendrograms to be free from unaesthetic inversions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;Wasserstein Belief Updater&#8221;&#31639;&#27861;&#26469;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#65292;&#23454;&#29616;&#20102;&#23545;&#21382;&#21490;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03284</link><description>&lt;p&gt;
"Wasserstein Believer:&#36890;&#36807;&#21487;&#38752;&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;"
&lt;/p&gt;
&lt;p&gt;
The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;Wasserstein Belief Updater&#8221;&#31639;&#27861;&#26469;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#65292;&#23454;&#29616;&#20102;&#23545;&#21382;&#21490;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#26159;&#24314;&#27169;&#20195;&#29702;&#26080;&#27861;&#24863;&#30693;&#21040;&#23436;&#25972;&#29366;&#24577;&#30340;&#29615;&#22659;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#38656;&#35201;&#32771;&#34385;&#36807;&#21435;&#30340;&#35266;&#23519;&#21644;&#34892;&#21160;&#36827;&#34892;&#25512;&#29702;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#21382;&#21490;&#31354;&#38388;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20165;&#20165;&#35760;&#20303;&#23436;&#25972;&#21382;&#21490;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20445;&#25345;&#27169;&#25311;&#30495;&#23454;&#29366;&#24577;&#30340;&#32622;&#20449;&#27010;&#29575;&#20998;&#24067;&#21487;&#20197;&#20316;&#20026;&#21382;&#21490;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#20294;&#20854;&#35745;&#31639;&#38656;&#35201;&#35775;&#38382;&#29615;&#22659;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#20063;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#21387;&#32553;&#35266;&#23519;-&#34892;&#21160;&#21382;&#21490;&#20197;&#23398;&#20064;&#20805;&#20998;&#30340;&#32479;&#35745;&#37327;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#25104;&#21151;&#30340;&#20445;&#35777;&#24182;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#31574;&#30053;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Wasserstein Belief Updater &#65292;&#36825;&#26159;&#19968;&#31181;RL&#31639;&#27861;&#65292;&#23427;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Markov Decision Processes (POMDPs) are useful tools to model environments where the full state cannot be perceived by an agent. As such the agent needs to reason taking into account the past observations and actions. However, simply remembering the full history is generally intractable due to the exponential growth in the history space. Keeping a probability distribution that models the belief over what the true state is can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is also intractable. State-of-the-art algorithms use Recurrent Neural Networks to compress the observation-action history aiming to learn a sufficient statistic, but they lack guarantees of success and can lead to sub-optimal policies. To overcome this, we propose the Wasserstein Belief Updater, an RL algorithm that learns a latent model of the POMDP and an approximation of the belief update. Our approach comes with theoreti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#31283;&#20581;&#35270;&#35273;&#24863;&#30693;&#30340;&#21367;&#31215;&#35270;&#35273;&#25552;&#31034;&#65288;CVP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#36991;&#20813;&#33258;&#36866;&#24212;&#27169;&#22411;&#22312;&#26080;&#26631;&#31614;&#33258;&#30417;&#30563;&#27979;&#35797;&#26102;&#38388;&#35774;&#32622;&#19979;&#30340;&#36807;&#25311;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#25552;&#39640;&#31283;&#20581;&#24615;&#39640;&#36798;5.87%&#12290;</title><link>http://arxiv.org/abs/2303.00198</link><description>&lt;p&gt;
&#38024;&#23545;&#31283;&#20581;&#35270;&#35273;&#24863;&#30693;&#30340;&#21367;&#31215;&#35270;&#35273;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Convolutional Visual Prompt for Robust Visual Perception. (arXiv:2303.00198v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#31283;&#20581;&#35270;&#35273;&#24863;&#30693;&#30340;&#21367;&#31215;&#35270;&#35273;&#25552;&#31034;&#65288;CVP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#36991;&#20813;&#33258;&#36866;&#24212;&#27169;&#22411;&#22312;&#26080;&#26631;&#31614;&#33258;&#30417;&#30563;&#27979;&#35797;&#26102;&#38388;&#35774;&#32622;&#19979;&#30340;&#36807;&#25311;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#25552;&#39640;&#31283;&#20581;&#24615;&#39640;&#36798;5.87%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#27169;&#22411;&#24120;&#24120;&#23545;&#26410;&#36866;&#24212;&#30340;&#31163;&#22495;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#25915;&#20987;&#12290;&#34429;&#28982;&#35270;&#35273;&#25552;&#31034;&#20026;&#22823;&#35268;&#27169;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36755;&#20837;&#31354;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#39640;&#32500;&#24230;&#30340;&#21152;&#24615;&#21521;&#37327;&#21644;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#23548;&#33268;&#22312;&#26080;&#26631;&#31614;&#30340;&#33258;&#30417;&#30563;&#27979;&#35797;&#26102;&#38388;&#35774;&#32622;&#19979;&#65292;&#35843;&#25972;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21367;&#31215;&#35270;&#35273;&#25552;&#31034;&#65288;CVP&#65289;&#26469;&#23454;&#29616;&#26080;&#26631;&#31614;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#20197;&#25552;&#39640;&#31283;&#20581;&#30340;&#35270;&#35273;&#24863;&#30693;&#12290;CVP&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#35201;&#27714;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20165;&#20026;&#26631;&#20934;&#35270;&#35273;&#25552;&#31034;&#30340;1&#65285;&#20197;&#19979;&#65292;&#20174;&#32780;&#25233;&#21046;&#20102;&#36807;&#25311;&#21512;&#12290;&#22312;&#24191;&#27867;&#30340;&#31163;&#22495;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#65292;&#30456;&#27604;&#20960;&#31181;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#39640;&#36798;5.87%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision models are often vulnerable to out-of-distribution (OOD) samples without adapting. While visual prompts offer a lightweight method of input-space adaptation for large-scale vision models, they rely on a high-dimensional additive vector and labeled data. This leads to overfitting when adapting models in a self-supervised test-time setting without labels. We introduce convolutional visual prompts (CVP) for label-free test-time adaptation for robust visual perception. The structured nature of CVP demands fewer trainable parameters, less than 1\% compared to standard visual prompts, combating overfitting. Extensive experiments and analysis on a wide variety of OOD visual perception tasks show that our approach is effective, improving robustness by up to 5.87% over several large-scale models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.00028</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#36830;&#32493;&#21644;&#31163;&#25955;&#31354;&#38388;&#30340;&#22238;&#24402;&#20256;&#24863;&#22120;&#25918;&#32622;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#26696;&#65292;&#29992;&#20110;&#30417;&#27979;&#28201;&#24230;&#12289;&#38477;&#27700;&#31561;&#31354;&#38388;&#65288;&#25110;&#26102;&#31354;&#65289;&#30456;&#20851;&#29616;&#35937;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#24050;&#30693;&#20869;&#26680;&#20989;&#25968;&#21442;&#25968;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#25311;&#21512;&#21040;&#29615;&#22659;&#20013;&#38543;&#26426;&#37319;&#26679;&#30340;&#26410;&#26631;&#35760;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#30340;&#35825;&#23548;&#28857;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#12290;&#20351;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#32423;&#21035;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#21035;&#12290;&#22312;&#20505;&#36873;&#20256;&#24863;&#22120;&#25918;&#32622;&#28857;&#38598;&#21512;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#36138;&#23146;&#39034;&#24207;&#36873;&#25321;&#31639;&#27861;&#26469;&#25214;&#21040;&#36739;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#25913;&#36827;&#30340;FTRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#22120;&#21644;&#26032;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#65292;&#19981;&#20877;&#38656;&#35201;&#20551;&#35774;&#23384;&#22312;&#21807;&#19968;&#26368;&#20248;&#33218;&#65292;&#24182;&#23545;&#26576;&#20123;&#27491;&#21017;&#21270;&#22120;&#30340;&#36951;&#25022;&#30028;&#38480;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.13534</link><description>&lt;p&gt;
&#23545;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;FTRL&#31639;&#27861;&#19982;&#19968;&#33324;&#27491;&#21017;&#21270;&#21644;&#22810;&#20010;&#26368;&#20248;&#33218;&#30340;&#26368;&#20339;&#20445;&#35777;&#26377;&#25152;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms. (arXiv:2302.13534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#25913;&#36827;&#30340;FTRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#22120;&#21644;&#26032;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#65292;&#19981;&#20877;&#38656;&#35201;&#20551;&#35774;&#23384;&#22312;&#21807;&#19968;&#26368;&#20248;&#33218;&#65292;&#24182;&#23545;&#26576;&#20123;&#27491;&#21017;&#21270;&#22120;&#30340;&#36951;&#25022;&#30028;&#38480;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35774;&#35745;&#33258;&#36866;&#24212;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#35774;&#32622;&#21644;&#23545;&#25239;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20248;&#65288;&#36890;&#24120;&#31216;&#20026;&#26368;&#20339;&#20445;&#35777;&#65289;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#27491;&#30830;&#37197;&#32622;&#21644;&#20998;&#26512;&#26102;&#65292;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#23545;&#25239;&#35774;&#32622;&#30340;Follow-the-Regularized-Leader&#65288;FTRL&#65289;&#31639;&#27861;&#23454;&#38469;&#19978;&#21487;&#20197;&#26368;&#20248;&#22320;&#36866;&#24212;&#38543;&#26426;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#20851;&#38190;&#20381;&#36182;&#20110;&#23384;&#22312;&#21807;&#19968;&#26368;&#20248;&#33218;&#30340;&#20551;&#35774;&#12290;&#26368;&#36817;&#65292;Ito&#65288;2021&#65289;&#39318;&#27425;&#37319;&#21462;&#25514;&#26045;&#21024;&#38500;&#20102;&#19968;&#20010;&#29305;&#23450;FTRL&#31639;&#27861;&#23545;&#20110;$\frac{1}{2}$-Tsallis&#29109;&#27491;&#21017;&#21270;&#30340;&#19981;&#21487;&#21462;&#21807;&#19968;&#24615;&#20551;&#35774;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#32467;&#26524;&#36827;&#34892;&#20102;&#26174;&#33879;&#25913;&#36827;&#21644;&#25512;&#24191;&#65292;&#34920;&#26126;FTRL&#31639;&#27861;&#22312;&#24191;&#27867;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#26032;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#19979;&#19981;&#38656;&#35201;&#21807;&#19968;&#24615;&#12290;&#23545;&#20110;&#26576;&#20123;&#27491;&#21017;&#21270;&#22120;&#65292;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#38480;&#19982;&#21069;&#20154;&#30340;&#32467;&#26524;&#30456;&#27604;&#20063;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of designing adaptive multi-armed bandit algorithms that perform optimally in both the stochastic setting and the adversarial setting simultaneously (often known as a best-of-both-world guarantee). A line of recent works shows that when configured and analyzed properly, the Follow-the-Regularized-Leader (FTRL) algorithm, originally designed for the adversarial setting, can in fact optimally adapt to the stochastic setting as well. Such results, however, critically rely on an assumption that there exists one unique optimal arm. Recently, Ito (2021) took the first step to remove such an undesirable uniqueness assumption for one particular FTRL algorithm with the $\frac{1}{2}$-Tsallis entropy regularizer. In this work, we significantly improve and generalize this result, showing that uniqueness is unnecessary for FTRL with a broad family of regularizers and a new learning rate schedule. For some regularizers, our regret bounds also improve upon prior results even when
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Transformer-CNN&#20998;&#21106;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;TransforCNN&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;XCT&#22270;&#20687;&#20013;&#20998;&#21106;&#20986;&#38146;&#37329;&#23646;&#30005;&#27744;&#20013;&#30340;&#26641;&#26525;&#26230;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;TransforCNN&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.04824</link><description>&lt;p&gt;
&#38146;&#37329;&#23646;&#30005;&#27744;&#36136;&#37327;&#25511;&#21046;&#36890;&#36807;Transformer-CNN&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Lithium Metal Battery Quality Control via Transformer-CNN Segmentation. (arXiv:2302.04824v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Transformer-CNN&#20998;&#21106;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;TransforCNN&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;XCT&#22270;&#20687;&#20013;&#20998;&#21106;&#20986;&#38146;&#37329;&#23646;&#30005;&#27744;&#20013;&#30340;&#26641;&#26525;&#26230;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;TransforCNN&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#37329;&#23646;(Li)&#30005;&#27744;&#22240;&#20854;&#39640;&#29702;&#35770;&#33021;&#37327;&#23494;&#24230;&#32780;&#25104;&#20026;&#19979;&#19968;&#20195;&#30005;&#27744;&#31995;&#32479;&#30340;&#28508;&#22312;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#36136;&#24615;&#38146;(Li)&#27785;&#31215;&#65292;&#20250;&#24418;&#25104;&#31216;&#20026;&#26641;&#26525;&#26230;&#30340;&#32570;&#38519;&#65292;&#36825;&#38459;&#30861;&#20102;&#38146;&#37329;&#23646;&#30005;&#27744;&#30340;&#21457;&#23637;&#21644;&#21033;&#29992;&#12290;&#35266;&#23519;&#26641;&#26525;&#26230;&#24418;&#24577;&#30340;&#38750;&#30772;&#22351;&#24615;&#25216;&#26415;&#36890;&#24120;&#20351;&#29992;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;(XCT)&#25552;&#20379;&#26029;&#38754;&#35270;&#22270;&#12290;&#20026;&#20102;&#33719;&#24471;&#30005;&#27744;&#20869;&#37096;&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#22270;&#20687;&#20998;&#21106;&#25104;&#20026;&#23450;&#37327;&#20998;&#26512;XCT&#22270;&#20687;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;TransforCNN&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;XCT&#25968;&#25454;&#20013;&#20998;&#21106;&#20986;&#26641;&#26525;&#26230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;TransforCNN&#19982;U-Net&#12289;Y-Net&#21644;E-Net&#31561;&#19977;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#36825;&#20123;&#31639;&#27861;&#26159;&#29992;&#20110;XCT&#20998;&#26512;&#30340;&#38598;&#25104;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#22312;&#35780;&#20272;&#36807;&#20998;&#21106;&#26102;&#20351;&#29992;TransforCNN&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lithium metal battery (LMB) has the potential to be the next-generation battery system because of its high theoretical energy density. However, defects known as dendrites are formed by heterogeneous lithium (Li) plating, which hinders the development and utilization of LMBs. Non-destructive techniques to observe the dendrite morphology often use X-ray computed tomography (XCT) to provide cross-sectional views. To retrieve three-dimensional structures inside a battery, image segmentation becomes essential to quantitatively analyze XCT images. This work proposes a new semantic segmentation approach using a transformer-based neural network called TransforCNN that is capable of segmenting out dendrites from XCT data. In addition, we compare the performance of the proposed TransforCNN with three other algorithms, such as U-Net, Y-Net, and E-Net, consisting of an Ensemble Network model for XCT analysis. Our results show the advantages of using TransforCNN when evaluating over-segmentation me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04449</link><description>&lt;p&gt;
&#38405;&#35835;&#24182;&#33719;&#24471;&#22238;&#25253;&#65306;&#22312;&#19982;&#25351;&#23548;&#25163;&#20876;&#30340;&#24110;&#21161;&#19979;&#23398;&#20064;&#29609;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#19981;&#20165;&#20165;&#26159;&#36890;&#36807;&#20132;&#20114;&#25110;&#28436;&#31034;&#65292;&#36824;&#21253;&#25324;&#38405;&#35835;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25991;&#26723;&#65292;&#20363;&#22914;&#25351;&#23548;&#25163;&#20876;&#12290;&#25351;&#23548;&#25163;&#20876;&#21644;&#32500;&#22522;&#39029;&#38754;&#26159;&#26368;&#20016;&#23500;&#30340;&#25968;&#25454;&#20043;&#19968;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#23453;&#36149;&#29305;&#24449;&#12289;&#31574;&#30053;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#25105;&#20204;&#20551;&#35774;&#21033;&#29992;&#20154;&#20889;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#24110;&#21161;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#31574;&#30053;&#23558;&#23548;&#33268;&#26356;&#39640;&#25928;&#21644;&#26356;&#20248;&#31168;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#12290;&#38405;&#35835;&#24182;&#22870;&#21169;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#21152;&#36895;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#24635;&#32467;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#26681;&#25454;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#20449;&#24687;&#35780;&#20272;&#29289;&#20307;-&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;&#19968;&#20010;&#36741;&#21161;&#30340;&#21453;&#39304;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary re
&lt;/p&gt;</description></item><item><title>ZipLM&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#25512;&#29702;&#29615;&#22659;&#20013;&#23454;&#29616;&#19982;&#30446;&#26631;&#36816;&#34892;&#36895;&#24230;&#30456;&#21305;&#37197;&#30340;&#26368;&#20808;&#36827;&#21387;&#32553;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ZipLM&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#26435;&#34913;&#65292;&#24182;&#19988;&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04089</link><description>&lt;p&gt;
ZipLM: &#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24863;&#30693;&#32467;&#26500;&#21270;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
ZipLM: Inference-Aware Structured Pruning of Language Models. (arXiv:2302.04089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04089
&lt;/p&gt;
&lt;p&gt;
ZipLM&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#25512;&#29702;&#29615;&#22659;&#20013;&#23454;&#29616;&#19982;&#30446;&#26631;&#36816;&#34892;&#36895;&#24230;&#30456;&#21305;&#37197;&#30340;&#26368;&#20808;&#36827;&#21387;&#32553;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ZipLM&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#26435;&#34913;&#65292;&#24182;&#19988;&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24615;&#24615;&#33021;&#32473;&#35745;&#31639;&#21644;&#37096;&#32626;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#36127;&#25285;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ZipLM&#30340;&#26032;&#22411;&#32467;&#26500;&#21270;&#21387;&#32553;&#26041;&#27861;&#65292;&#21521;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#36808;&#36827;&#12290;ZipLM&#22312;&#36798;&#21040;&#19968;&#32452;&#30446;&#26631;&#25512;&#29702;&#26102;&#36895;&#24230;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#25512;&#29702;&#29615;&#22659;&#20013;&#21305;&#37197;&#19968;&#32452;&#30446;&#26631;&#36816;&#34892;&#26102;&#21152;&#36895;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#12289;&#25512;&#29702;&#29615;&#22659;&#20197;&#21450;&#19968;&#32452;&#21152;&#36895;&#24230;&#30446;&#26631;&#65292;ZipLM&#36845;&#20195;&#22320;&#35782;&#21035;&#24182;&#21024;&#38500;&#25439;&#22833;&#26102;&#38271;&#26435;&#34913;&#26368;&#24046;&#30340;&#32452;&#20214;&#12290;&#19982;&#20808;&#21069;&#19987;&#38376;&#29992;&#20110;&#21518;&#35757;&#32451;/&#19968;&#27425;&#24615;&#25110;&#36880;&#28176;&#21387;&#32553;&#35774;&#32622;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#27169;&#22411;&#23478;&#26063;&#65288;&#22914;BERT&#65288;&#32534;&#30721;&#22120;&#65289;&#25110;GPT&#65288;&#35299;&#30721;&#22120;&#65289;&#65289;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;ZipLM&#22312;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#20013;&#29983;&#25104;&#20102;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19982;&#20808;&#21069;&#30340;&#33976;&#39311;&#21644;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;ZipLM&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and prunin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#24182;&#32500;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.03857</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65306;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection. (arXiv:2302.03857v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#24182;&#32500;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#20294;&#21487;&#20197;&#36755;&#20986;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#19988;&#36866;&#29992;&#20110;&#24191;&#27867;&#19979;&#28216;&#20219;&#21153;&#30340;&#24378;&#40065;&#26834;&#24615;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;ACL&#38656;&#35201;&#24040;&#22823;&#30340;&#36816;&#34892;&#26102;&#38388;&#25165;&#33021;&#29983;&#25104;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#21464;&#20307;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#21152;&#36895;ACL&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#12290;RCS&#19981;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#65292;&#25628;&#32034;&#26368;&#23567;&#21270;&#34920;&#31034;&#20998;&#27495;&#30340;&#20449;&#24687;&#23376;&#38598;&#65292;&#21363;&#33258;&#28982;&#25968;&#25454;&#21644;&#20854;&#34394;&#25311;&#23545;&#25239;&#21464;&#20307;&#20043;&#38388;&#34920;&#31034;&#30340;&#36317;&#31163;&#12290;RCS&#30340;&#22522;&#26412;&#35299;&#27861;&#26159;&#36941;&#21382;&#25152;&#26377;&#21487;&#33021;&#30340;&#23376;&#38598;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23558;RCS&#36716;&#21270;&#20026;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#26367;&#20195;&#38382;&#39064;&#65292;&#21033;&#29992;&#36138;&#24515;&#25628;&#32034;&#26159;&#21407;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20855;&#26377;&#21407;&#38382;&#39064;&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RCS&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#26377;&#25928;&#22320;&#21152;&#36895;ACL&#65292;&#24182;&#19988;&#20173;&#28982;&#20445;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our compr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.02209</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#30340;&#38142;&#36335;&#39044;&#27979;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#31616;&#21333;&#22270;&#19978;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20294;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#30340;&#29702;&#35299;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#31561;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#12289;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#38145;&#20102;&#19968;&#31995;&#21015;&#20854;&#20182;&#27169;&#22411;&#12290;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#65292;&#34920;&#24449;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#20998;&#26512;&#34987;&#25193;&#23637;&#20197;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#25429;&#25417;&#30340;&#20989;&#25968;&#31867;&#36827;&#34892;&#31934;&#30830;&#36923;&#36753;&#25551;&#36848;&#12290;&#25552;&#20986;&#30340;&#29702;&#35770;&#21457;&#29616;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#65292;&#24182;&#24471;&#21040;&#20102;&#32463;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#20110;&#31163;&#25955;&#24207;&#21015;&#20998;&#31867;&#22120;&#30340;&#38543;&#26426;&#21024;&#38500;&#65288;RS-Del&#65289;&#24179;&#28369;&#26426;&#21046;&#65292;&#25552;&#20379;&#38024;&#23545;&#32534;&#36753;&#36317;&#31163;&#21463;&#38480;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#24615;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2302.01757</link><description>&lt;p&gt;
RS-Del: &#38543;&#26426;&#21024;&#38500;&#23545;&#24207;&#21015;&#20998;&#31867;&#22120;&#30340;&#32534;&#36753;&#36317;&#31163;&#40065;&#26834;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion. (arXiv:2302.01757v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#20110;&#31163;&#25955;&#24207;&#21015;&#20998;&#31867;&#22120;&#30340;&#38543;&#26426;&#21024;&#38500;&#65288;RS-Del&#65289;&#24179;&#28369;&#26426;&#21046;&#65292;&#25552;&#20379;&#38024;&#23545;&#32534;&#36753;&#36317;&#31163;&#21463;&#38480;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#26159;&#26500;&#24314;&#20855;&#26377;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#22120;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#24120;&#24120;&#30740;&#31350;$\ell_p$&#33539;&#25968;&#21463;&#38480;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#31163;&#25955;&#25110;&#21487;&#21464;&#22823;&#23567;&#36755;&#20837;&#65288;&#20363;&#22914;&#28304;&#20195;&#30721;&#65289;&#30340;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#38656;&#35201;&#19981;&#21516;&#30340;&#23041;&#32961;&#27169;&#22411;&#21644;&#24179;&#28369;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#20197;&#36866;&#29992;&#20110;&#31163;&#25955;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20197;&#25552;&#20379;&#38024;&#23545;&#32534;&#36753;&#36317;&#31163;&#21463;&#38480;&#30340;&#23545;&#25239;&#24615;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24179;&#28369;&#26426;&#21046;&#38543;&#26426;&#21024;&#38500;&#65288;RS-Del&#65289;&#24212;&#29992;&#20102;&#38543;&#26426;&#21024;&#38500;&#32534;&#36753;&#65292;&#36825;&#31181;&#26041;&#24335;&#65288;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#22320;&#65289;&#36275;&#20197;&#25552;&#20379;&#38024;&#23545;&#23545;&#25239;&#24615;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#26367;&#25442;&#32534;&#36753;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#35748;&#35777;&#35777;&#26126;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;Neyman-Pearson&#26041;&#27861;&#65292;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#35745;&#31639;&#65292;&#32780;&#26159;&#22260;&#32469;&#30528;&#21478;&#19968;&#31181;&#26041;&#24335;&#36827;&#34892;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing is a leading approach for constructing classifiers that are certifiably robust against adversarial examples. Existing work on randomized smoothing has focused on classifiers with continuous inputs, such as images, where $\ell_p$-norm bounded adversaries are commonly studied. However, there has been limited work for classifiers with discrete or variable-size inputs, such as for source code, which require different threat models and smoothing mechanisms. In this work, we adapt randomized smoothing for discrete sequence classifiers to provide certified robustness against edit distance-bounded adversaries. Our proposed smoothing mechanism randomized deletion (RS-Del) applies random deletion edits, which are (perhaps surprisingly) sufficient to confer robustness against adversarial deletion, insertion and substitution edits. Our proof of certification deviates from the established Neyman-Pearson approach, which is intractable in our setting, and is instead organized aro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#28041;&#21450;&#26080;&#32422;&#26463;&#38382;&#39064;&#21644;&#21160;&#24577;&#36951;&#25022;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#65292;&#22312;&#36866;&#24212;&#24615;&#21644;&#24212;&#29992;&#19978;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2301.13349</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23454;&#29616;&#26080;&#32422;&#26463;&#21160;&#24577;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Unconstrained Dynamic Regret via Sparse Coding. (arXiv:2301.13349v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#28041;&#21450;&#26080;&#32422;&#26463;&#38382;&#39064;&#21644;&#21160;&#24577;&#36951;&#25022;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#65292;&#22312;&#36866;&#24212;&#24615;&#21644;&#24212;&#29992;&#19978;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#22312;&#20004;&#20010;&#38382;&#39064;&#32467;&#26500;&#30340;&#32806;&#21512;&#19979;&#30340;&#24773;&#20917;&#65306;&#22495;&#26080;&#30028;&#65292;&#32780;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#36890;&#36807;&#21160;&#24577;&#36951;&#25022;&#26469;&#34913;&#37327;&#30340;&#12290;&#22788;&#29702;&#20219;&#19968;&#38382;&#39064;&#37117;&#35201;&#27714;&#36951;&#25022;&#30028;&#38480;&#20381;&#36182;&#20110;&#27604;&#36739;&#24207;&#21015;&#30340;&#26576;&#20123;&#22797;&#26434;&#24230;&#37327;&#24230; - &#29305;&#21035;&#26159;&#26080;&#32422;&#26463;OLO&#20013;&#30340;&#27604;&#36739;&#22120;&#33539;&#25968;&#65292;&#20197;&#21450;&#21160;&#24577;&#36951;&#25022;&#20013;&#30340;&#36335;&#24452;&#38271;&#24230;&#12290;&#19982;&#26368;&#36817;&#19968;&#31687;&#25991;&#31456;(Jacobsen&amp; Cutkosky&#65292;2022)&#36866;&#24212;&#36825;&#20004;&#20010;&#22797;&#26434;&#24230;&#37327;&#24230;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#12290;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#23454;&#29616;&#36866;&#24212;&#24615;&#65292;&#36825;&#20010;&#26694;&#26550;&#33258;&#28982;&#22320;&#21033;&#29992;&#20102;&#29615;&#22659;&#26356;&#22797;&#26434;&#30340;&#21069;&#32622;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38745;&#24577;&#26080;&#32422;&#26463;OLO&#26799;&#24230;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#26426;&#21046;&#35774;&#35745;&#12290;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by time series forecasting, we study Online Linear Optimization (OLO) under the coupling of two problem structures: the domain is unbounded, and the performance of an algorithm is measured by its dynamic regret. Handling either of them requires the regret bound to depend on certain complexity measure of the comparator sequence -- specifically, the comparator norm in unconstrained OLO, and the path length in dynamic regret. In contrast to a recent work (Jacobsen &amp; Cutkosky, 2022) that adapts to the combination of these two complexity measures, we propose an alternative complexity measure by recasting the problem into sparse coding. Adaptivity can be achieved by a simple modular framework, which naturally exploits more intricate prior knowledge of the environment. Along the way, we also present a new gradient adaptive algorithm for static unconstrained OLO, designed using novel continuous time machinery. This could be of independent interest.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24322;&#26041;&#24046;&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#20989;&#25968;&#27169;&#22411;&#65292;&#35813;&#35770;&#25991;&#22312;&#27491;&#30830;&#35828;&#26126;&#22122;&#22768;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#22312;&#29992;&#25143;&#38169;&#35823;&#25351;&#23450;&#22122;&#22768;&#20998;&#24067;&#30340;&#24418;&#24335;&#26102;&#65292;&#20998;&#26512;&#34920;&#26126;&#22240;&#26524;&#25512;&#26029;&#30340;&#31934;&#24230;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#36873;&#25321;&#23454;&#29616;&#31283;&#23450;&#32780;&#20934;&#30830;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2301.12930</link><description>&lt;p&gt;
&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#25512;&#26029;&#30340;&#26368;&#22823;&#20284;&#28982;&#19982;&#29420;&#31435;&#24615;&#26816;&#39564;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cause-Effect Inference in Location-Scale Noise Models: Maximum Likelihood vs. Independence Testing. (arXiv:2301.12930v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12930
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24322;&#26041;&#24046;&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#20989;&#25968;&#27169;&#22411;&#65292;&#35813;&#35770;&#25991;&#22312;&#27491;&#30830;&#35828;&#26126;&#22122;&#22768;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#22312;&#29992;&#25143;&#38169;&#35823;&#25351;&#23450;&#22122;&#22768;&#20998;&#24067;&#30340;&#24418;&#24335;&#26102;&#65292;&#20998;&#26512;&#34920;&#26126;&#22240;&#26524;&#25512;&#26029;&#30340;&#31934;&#24230;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#36873;&#25321;&#23454;&#29616;&#31283;&#23450;&#32780;&#20934;&#30830;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#25512;&#26029;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#27491;&#30830;&#22240;&#26524;&#26041;&#21521;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#24322;&#26041;&#24046;&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#20989;&#25968;&#27169;&#22411; (LSNM) &#32467;&#21512;&#20102;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35782;&#21035;&#24615;&#20445;&#35777;&#65292;&#22312;&#27491;&#30830;&#25351;&#23450;&#22122;&#22768;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#24403;&#29992;&#25143;&#38169;&#35823;&#25351;&#23450;&#22122;&#22768;&#20998;&#24067;&#30340;&#24418;&#24335;&#26102;&#65292;&#31934;&#24230;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#22833;&#36133;&#20027;&#35201;&#21457;&#29983;&#22312;&#21453;&#22240;&#26524;&#26041;&#21521;&#30340;&#26465;&#20214;&#26041;&#24046;&#23567;&#20110;&#22240;&#26524;&#26041;&#21521;&#30340;&#26465;&#20214;&#26041;&#24046;&#30340;&#24773;&#20917;&#19979;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#21457;&#29616;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#36873;&#25321;&#21487;&#20197;&#22312;&#32570;&#20047;&#22122;&#22768;&#20998;&#24067;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#31283;&#23450;&#32780;&#20934;&#30830;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental problem of causal discovery is cause-effect inference, learning the correct causal direction between two random variables. Significant progress has been made through modelling the effect as a function of its cause and a noise term, which allows us to leverage assumptions about the generating function class. The recently introduced heteroscedastic location-scale noise functional models (LSNMs) combine expressive power with identifiability guarantees. LSNM model selection based on maximizing likelihood achieves state-of-the-art accuracy, when the noise distributions are correctly specified. However, through an extensive empirical evaluation, we demonstrate that the accuracy deteriorates sharply when the form of the noise distribution is misspecified by the user. Our analysis shows that the failure occurs mainly when the conditional variance in the anti-causal direction is smaller than that in the causal direction. As an alternative, we find that causal model selection throu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#24418;&#26354;&#29575;&#25551;&#36848;&#31526;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#29992;&#20110;&#35780;&#20272;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#30340;&#31283;&#20581;&#12289;&#34920;&#36798;&#24615;&#30340;&#25551;&#36848;&#31526;&#12290;</title><link>http://arxiv.org/abs/2301.12906</link><description>&lt;p&gt;
&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#30340;&#26354;&#29575;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Curvature Filtrations for Graph Generative Model Evaluation. (arXiv:2301.12906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12906
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#24418;&#26354;&#29575;&#25551;&#36848;&#31526;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#29992;&#20110;&#35780;&#20272;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#30340;&#31283;&#20581;&#12289;&#34920;&#36798;&#24615;&#30340;&#25551;&#36848;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#38656;&#35201;&#20102;&#35299;&#20998;&#24067;&#32423;&#21035;&#19978;&#30340;&#22270;&#24418;&#24046;&#24322;&#65292;&#36825;&#38656;&#35201;&#33021;&#22815;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#22270;&#24418;&#30340;&#26174;&#33879;&#23646;&#24615;&#12290;&#26354;&#29575;&#26159;&#22270;&#24418;&#30340;&#19968;&#31181;&#23646;&#24615;&#65292;&#26368;&#36817;&#24320;&#22987;&#35777;&#26126;&#20854;&#22312;&#25551;&#36848;&#22270;&#24418;&#26041;&#38754;&#24456;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#20854;&#34920;&#36798;&#24615;&#36136;&#12289;&#31283;&#23450;&#24615;&#21644;&#22312;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#23454;&#38469;&#25928;&#29992;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#22270;&#24418;&#26354;&#29575;&#25551;&#36848;&#31526;&#19982;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#29992;&#20110;&#35780;&#20272;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#30340;&#31283;&#20581;&#12289;&#34920;&#36798;&#24615;&#30340;&#25551;&#36848;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generative model evaluation necessitates understanding differences between graphs on the distributional level. This entails being able to harness salient attributes of graphs in an efficient manner. Curvature constitutes one such property of graphs, and has recently started to prove useful in characterising graphs. Its expressive properties, stability, and practical utility in model evaluation remain largely unexplored, however. We combine graph curvature descriptors with emerging methods from topological data analysis to obtain robust, expressive descriptors for evaluating graph generative models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22833;&#30495;&#39118;&#38505;&#24230;&#37327;&#26469;&#22788;&#29702;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#20855;&#26377;&#39640;&#25928;&#19988;&#19981;&#20381;&#36182;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12593</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#39118;&#38505;&#21388;&#24694;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning. (arXiv:2301.12593v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22833;&#30495;&#39118;&#38505;&#24230;&#37327;&#26469;&#22788;&#29702;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#20855;&#26377;&#39640;&#25928;&#19988;&#19981;&#20381;&#36182;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#39046;&#22495;&#38656;&#35201;&#22312;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#36807;&#28193;&#27169;&#22411;&#30340;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#30456;&#24178;&#22833;&#30495;&#39118;&#38505;&#24230;&#37327;&#26469;&#23545;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37319;&#21462;&#39118;&#38505;&#21388;&#24694;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#23427;&#31561;&#20215;&#20110;&#19968;&#31867;&#29305;&#23450;&#30340;&#20998;&#24067;&#40065;&#26834;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#36825;&#20010;&#26694;&#26550;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#40065;&#26834;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#34920;&#36798;&#19981;&#28041;&#21450;&#26497;&#23567;&#21270;&#26368;&#22823;&#20248;&#21270;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#12289;&#19981;&#20381;&#36182;&#27169;&#22411;&#22320;&#22312;&#21333;&#20010;&#35757;&#32451;&#29615;&#22659;&#20013;&#20165;&#38656;&#35201;&#26631;&#20934;&#25968;&#25454;&#25910;&#38598;&#26469;&#23454;&#26045;&#12290;&#22312;&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#37096;&#32626;&#26102;&#33021;&#22815;&#20135;&#29983;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#39640;&#25928;&#22320;&#35782;&#21035;&#39118;&#38505;&#34913;&#37327;&#23450;&#20041;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#39640;&#27010;&#29575;&#36793;&#30028;&#26694;&#21644;&#36873;&#25321;&#19979;&#19968;&#20010;&#35780;&#20272;&#28857;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11588</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#39118;&#38505;&#34913;&#37327;
&lt;/p&gt;
&lt;p&gt;
Bounding Box-based Multi-objective Bayesian Optimization of Risk Measures under Input Uncertainty. (arXiv:2301.11588v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#39640;&#25928;&#22320;&#35782;&#21035;&#39118;&#38505;&#34913;&#37327;&#23450;&#20041;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#39640;&#27010;&#29575;&#36793;&#30028;&#26694;&#21644;&#36873;&#25321;&#19979;&#19968;&#20010;&#35780;&#20272;&#28857;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;MOBO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#65288;IU&#65289;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#35782;&#21035;&#30001;&#39118;&#38505;&#34913;&#37327;&#23450;&#20041;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65288;PF&#65289;&#12290;&#29616;&#26377;&#30340;IU&#19979;&#24085;&#32047;&#25176;&#20248;&#21270;&#30340;BO&#26041;&#27861;&#26159;&#29305;&#23450;&#39118;&#38505;&#25110;&#32773;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#33324;&#39118;&#38505;&#34913;&#37327;&#24182;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#25152;&#25552;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20551;&#35774;&#40657;&#31665;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;GP&#27169;&#22411;&#26500;&#24314;&#39118;&#38505;&#34913;&#37327;&#30340;&#39640;&#27010;&#29575;&#36793;&#30028;&#26694;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#38750;&#25903;&#37197;&#36793;&#30028;&#26694;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#25311;&#36317;&#31163;&#30340;&#26368;&#22823;&#20540;&#23450;&#20041;&#30340;&#26368;&#22823;&#26368;&#23567;&#36317;&#31163;&#36873;&#25321;&#19979;&#19968;&#20010;&#35780;&#20272;&#28857;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20013;&#36820;&#22238;&#20219;&#24847;&#31934;&#30830;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel multi-objective Bayesian optimization (MOBO) method to efficiently identify the Pareto front (PF) defined by risk measures for black-box functions under the presence of input uncertainty (IU). Existing BO methods for Pareto optimization in the presence of IU are risk-specific or without theoretical guarantees, whereas our proposed method addresses general risk measures and has theoretical guarantees. The basic idea of the proposed method is to assume a Gaussian process (GP) model for the black-box function and to construct high-probability bounding boxes for the risk measures using the GP model. Furthermore, in order to reduce the uncertainty of non-dominated bounding boxes, we propose a method of selecting the next evaluation point using a maximin distance defined by the maximum value of a quasi distance based on bounding boxes. As theoretical analysis, we prove that the algorithm can return an arbitrary-accurate solution in a finite number of iterati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31283;&#20581;&#20248;&#21270;&#35745;&#31639;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#25514;&#26045;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#32467;&#26524;&#65292;&#27492;&#26041;&#27861;&#22312;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#26368;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.11113</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#20248;&#21270;&#25214;&#20986;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Regions of Counterfactual Explanations via Robust Optimization. (arXiv:2301.11113v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11113
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31283;&#20581;&#20248;&#21270;&#35745;&#31639;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#25514;&#26045;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#32467;&#26524;&#65292;&#27492;&#26041;&#27861;&#22312;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#26368;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#26816;&#27979;&#20559;&#35265;&#21644;&#25552;&#39640;&#25968;&#25454;&#39537;&#21160;&#20998;&#31867;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#19968;&#20010;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#26159;&#19968;&#20010;&#26368;&#23567;&#30340;&#25200;&#21160;&#25968;&#25454;&#28857;&#65292;&#20351;&#24471;&#27169;&#22411;&#30340;&#20915;&#31574;&#21457;&#29983;&#21464;&#21270;&#12290;&#29616;&#26377;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#33021;&#25552;&#20379;&#19968;&#20010;CE&#65292;&#21487;&#33021;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#26469;&#35745;&#31639;&#31283;&#20581;CE&#65292;&#21363;&#22312;&#29305;&#24449;&#36731;&#24494;&#25200;&#21160;&#21518;&#20173;&#28982;&#26377;&#25928;&#30340;CE&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25972;&#20010;CE&#21306;&#22495;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#25514;&#26045;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#31283;&#20581;&#20248;&#21270;&#30340;&#31639;&#27861;&#24605;&#24819;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20026;&#21508;&#31181;&#24120;&#35265;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#27169;&#22411;&#29983;&#25104;&#20840;&#23616;&#26368;&#20339;&#30340;&#31283;&#20581;CE&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations play an important role in detecting bias and improving the explainability of data-driven classification models. A counterfactual explanation (CE) is a minimal perturbed data point for which the decision of the model changes. Most of the existing methods can only provide one CE, which may not be achievable for the user. In this work we derive an iterative method to calculate robust CEs, i.e. CEs that remain valid even after the features are slightly perturbed. To this end, our method provides a whole region of CEs allowing the user to choose a suitable recourse to obtain a desired outcome. We use algorithmic ideas from robust optimization and prove convergence results for the most common machine learning methods including logistic regression, decision trees, random forests, and neural networks. Our experiments show that our method can efficiently generate globally optimal robust CEs for a variety of common data sets and classification models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.10813</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20445;&#35777;&#25552;&#39640;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#20110;&#38544;&#34255;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#27491;&#22312;&#22686;&#21152;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#24120;&#29992;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#24230;&#37327;&#21644;&#20960;&#31181;&#32467;&#21512;&#38598;&#25104;&#23398;&#20064;&#30340;&#20844;&#24179;&#24863;&#30693;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#24230;&#37327;&#21482;&#33021;&#20851;&#27880;&#20854;&#20013;&#20043;&#19968;&#65292;&#21363;&#32676;&#20307;&#20844;&#24179;&#24615;&#25110;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#30828;&#24615;&#20860;&#23481;&#24615;&#26263;&#31034;&#20102;&#21363;&#20351;&#20854;&#20013;&#20043;&#19968;&#24471;&#21040;&#28385;&#36275;&#65292;&#20173;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25552;&#21319;&#20844;&#24179;&#24615;&#30340;&#26426;&#21046;&#36890;&#24120;&#21482;&#25552;&#20379;&#32463;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#35770;&#25991;&#35752;&#35770;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#21028;&#21035;&#39118;&#38505;&#65292;&#20197;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;p...
&lt;/p&gt;
&lt;p&gt;
The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#35270;&#39057;&#20998;&#35299;&#21644;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#25552;&#21319;&#30340;&#23545;&#35937;&#21644;&#35270;&#22270;&#30340;&#28508;&#22312;&#34920;&#24449;&#20043;&#38388;&#30340;&#35299;&#32806;&#25216;&#26415;&#20197;&#21450;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#38544;&#24335;&#35270;&#35282;&#35268;&#21017;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#30340;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#23545;&#35937;&#30340;&#24418;&#29366;&#26080;&#27861;&#34987;&#20934;&#30830;&#37325;&#24314;&#21644;&#26032;&#35270;&#35282;&#39044;&#27979;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#35270;&#35282;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.08951</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#34920;&#24449;&#30340;&#35270;&#39057;&#20998;&#35299;&#21644;&#39044;&#27979;&#30340;&#26102;&#38388;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction. (arXiv:2301.08951v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#35270;&#39057;&#20998;&#35299;&#21644;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#25552;&#21319;&#30340;&#23545;&#35937;&#21644;&#35270;&#22270;&#30340;&#28508;&#22312;&#34920;&#24449;&#20043;&#38388;&#30340;&#35299;&#32806;&#25216;&#26415;&#20197;&#21450;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#38544;&#24335;&#35270;&#35282;&#35268;&#21017;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#30340;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#23545;&#35937;&#30340;&#24418;&#29366;&#26080;&#27861;&#34987;&#20934;&#30830;&#37325;&#24314;&#21644;&#26032;&#35270;&#35282;&#39044;&#27979;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#35270;&#35282;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#20174;&#22810;&#20010;&#35270;&#35282;&#24863;&#30693;&#19990;&#30028;&#26102;&#65292;&#21363;&#20351;&#26576;&#20010;&#23545;&#35937;&#34987;&#23436;&#20840;&#36974;&#25377;&#65292;&#20063;&#33021;&#20197;&#32452;&#21512;&#26041;&#24335;&#29702;&#35299;&#23436;&#25972;&#29289;&#20307;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#31867;&#33021;&#22815;&#22312;&#35266;&#23519;&#22810;&#20010;&#35270;&#35282;&#20043;&#21518;&#24819;&#35937;&#26032;&#35270;&#35282;&#12290;&#26368;&#36817;&#22810;&#35270;&#22270;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26174;&#30528;&#36827;&#27493;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;1&#65289;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#23545;&#35937;&#30340;&#24418;&#29366;&#26080;&#27861;&#34987;&#20934;&#30830;&#37325;&#24314;&#12290;2&#65289;&#26032;&#35270;&#35282;&#39044;&#27979;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#35270;&#35282;&#27880;&#37322;&#65292;&#32780;&#19981;&#26159;&#35270;&#22270;&#34920;&#24449;&#20013;&#30340;&#38544;&#24335;&#35268;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#30340;&#26102;&#38388;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;&#20026;&#20102;&#20934;&#30830;&#37325;&#24314;&#23545;&#35937;&#30340;&#23436;&#25972;&#24418;&#29366;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#23545;&#35937;&#21644;&#35270;&#22270;&#30340;&#28508;&#22312;&#34920;&#24449;&#20043;&#38388;&#30340;&#35299;&#32806;&#65292;&#20854;&#20013;&#26102;&#38388;&#26465;&#20214;&#30340;&#35270;&#22270;&#30340;&#28508;&#22312;&#34920;&#24449;&#19982;Transformer&#19968;&#36215;&#32852;&#21512;&#25512;&#26029;&#65292;&#28982;&#21518;&#36755;&#20837;&#21040;Slot Attention Networks (SANs)&#30340;&#39034;&#24207;&#25193;&#23637;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#38544;&#24335;&#35270;&#35282;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#26174;&#24335;&#35270;&#35282;&#27880;&#37322;&#30340;&#38656;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37096;&#20998;&#21644;&#23436;&#20840;&#36974;&#25377;&#23545;&#35937;&#23436;&#25104;&#21644;&#26032;&#35270;&#35282;&#39044;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When perceiving the world from multiple viewpoints, humans have the ability to reason about the complete objects in a compositional manner even when an object is completely occluded from certain viewpoints. Meanwhile, humans are able to imagine novel views after observing multiple viewpoints. Recent remarkable advances in multi-view object-centric learning still leaves some unresolved problems: 1) The shapes of partially or completely occluded objects can not be well reconstructed. 2) The novel viewpoint prediction depends on expensive viewpoint annotations rather than implicit rules in view representations. In this paper, we introduce a time-conditioned generative model for videos. To reconstruct the complete shape of an object accurately, we enhance the disentanglement between the latent representations of objects and views, where the latent representations of time-conditioned views are jointly inferred with a Transformer and then are input to a sequential extension of Slot Attention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;GNN&#22312;&#22788;&#29702;&#38750;&#21516;&#36136;&#22270;&#25968;&#25454;&#26102;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#28151;&#21512;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#30340;&#27010;&#24565;&#65292;&#24182;&#20174;&#23616;&#37096;&#27169;&#24335;&#30340;&#38543;&#26426;&#24615;&#21644;&#36817;&#37051;&#21487;&#32858;&#21512;&#24615;&#20004;&#20010;&#26041;&#38754;&#28145;&#20837;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#26356;&#36890;&#29992;&#30340;GNN&#12290;</title><link>http://arxiv.org/abs/2212.03654</link><description>&lt;p&gt;
&#28151;&#21512;&#23616;&#37096;&#27169;&#24335;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Technical Report of Mixing Local Patterns. (arXiv:2212.03654v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;GNN&#22312;&#22788;&#29702;&#38750;&#21516;&#36136;&#22270;&#25968;&#25454;&#26102;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#28151;&#21512;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#30340;&#27010;&#24565;&#65292;&#24182;&#20174;&#23616;&#37096;&#27169;&#24335;&#30340;&#38543;&#26426;&#24615;&#21644;&#36817;&#37051;&#21487;&#32858;&#21512;&#24615;&#20004;&#20010;&#26041;&#38754;&#28145;&#20837;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#26356;&#36890;&#29992;&#30340;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21516;&#36136;&#22270;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#22788;&#29702;&#38750;&#21516;&#36136;&#22270;&#25968;&#25454;&#26102;&#21364;&#36828;&#36828;&#19981;&#22914;&#21516;&#36136;&#22270;&#25968;&#25454;&#65292;&#36825;&#26159;&#30001;&#20110;GNN&#30340;&#22266;&#26377;&#20302;&#36890;&#28388;&#27874;&#29305;&#24615;&#25152;&#33268;&#12290;&#22312;&#38754;&#23545;&#20998;&#26512;&#20855;&#26377;&#19981;&#21516;&#21516;&#36136;&#24615;&#23646;&#24615;&#30340;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#22270;&#34920;&#26102;&#65292;&#19981;&#24212;&#24573;&#30053;&#22270;&#20013;&#28508;&#22312;&#30340;&#28151;&#21512;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#20805;&#20998;&#32771;&#34385;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#65288;\textbf{Q1}&#65289;&#21644;&#65288;\textbf{Q2}&#65289;&#65292;&#20197;&#23454;&#29616;&#26356;&#36890;&#29992;&#30340;GNN&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#20004;&#20010;&#26041;&#38754;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#65292;&#20998;&#21035;&#26159;\textbf{&#65288;A1&#65289;&#65306;&#23616;&#37096;&#27169;&#24335;&#30340;&#38543;&#26426;&#24615;}&#21644;\textbf{&#65288;A2&#65289;&#65306;&#36817;&#37051;&#21487;&#32858;&#21512;&#24615;}&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown remarkable performance on homophilic graph data while being far less impressive when handling non-homophilic graph data due to the inherent low-pass filtering property of GNNs. In the face of analyzing complex real-world graphs with different homophily properties, the latent mixed local structural patterns in graphs should not be neglected. Therefore, the two questions, i.e., (\textbf{Q1}) and (\textbf{Q2}) as motioned above, should be well considered on the way to implementing a more generic GNN. For this purpose, we attempt to get deeper insights into them from two points, respectively, \textbf{(A1): Randomness of local patterns}, and \textbf{(A2): Aggregability of near-neighbors}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#25233;&#37057;&#30151;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26377;&#25928;&#30340;&#20559;&#24046;&#30699;&#27491;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#26512;&#20844;&#24179;&#24615;&#20197;&#21450;&#36879;&#26126;&#25253;&#21578;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05321</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25233;&#37057;&#30151;&#39044;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;&#19982;&#20559;&#24046;&#30699;&#27491;&#65306;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30740;&#31350;&#20154;&#32676;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness and bias correction in machine learning for depression prediction: results from four different study populations. (arXiv:2211.05321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#25233;&#37057;&#30151;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26377;&#25928;&#30340;&#20559;&#24046;&#30699;&#27491;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#26512;&#20844;&#24179;&#24615;&#20197;&#21450;&#36879;&#26126;&#25253;&#21578;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24515;&#29702;&#20445;&#20581;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#21463;&#20851;&#27880;&#30340;&#20154;&#32676;&#20013;&#65292;&#23384;&#22312;&#30528;&#30456;&#24403;&#31243;&#24230;&#30340;&#27745;&#21517;&#21270;&#21644;&#19981;&#24179;&#31561;&#65292;&#36825;&#31181;&#19981;&#24179;&#31561;&#20250;&#25193;&#25955;&#21040;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#12290;&#22914;&#26524;&#19981;&#36866;&#24403;&#22320;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#25152;&#23398;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#27169;&#22411;&#23601;&#20250;&#24378;&#21270;&#24050;&#32463;&#23384;&#22312;&#20110;&#31038;&#20250;&#20013;&#30340;&#32467;&#26500;&#24615;&#20559;&#24046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#25233;&#37057;&#30151;&#30340;ML&#27169;&#22411;&#30340;&#20559;&#24046;&#36827;&#34892;&#20102;&#26377;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#22269;&#23478;&#21644;&#20154;&#32676;&#12290;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934;&#30340;ML&#26041;&#27861;&#26174;&#31034;&#20986;&#24120;&#35268;&#20559;&#24046;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#32531;&#35299;&#25216;&#26415;&#20197;&#21450;&#25105;&#20204;&#33258;&#24049;&#30340;&#20107;&#21518;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#19981;&#20844;&#24179;&#20559;&#24046;&#30340;&#32423;&#21035;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#24314;&#35758;&#65292;&#20197;&#24320;&#21457;&#39044;&#27979;&#25233;&#37057;&#30151;&#39118;&#38505;&#30340;ML&#27169;&#22411;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#27809;&#26377;&#21333;&#19968;&#26368;&#22909;&#30340;&#39044;&#27979;&#25233;&#37057;&#30151;&#30340;ML&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#32467;&#26524;&#30340;&#24179;&#31561;&#12290;&#36825;&#24378;&#35843;&#20102;&#22312;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#20013;&#20998;&#26512;&#20844;&#24179;&#24615;&#20197;&#21450;&#36879;&#26126;&#25253;&#21578;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations, which spreads through collected data. When not properly accounted for, machine learning (ML) models learned from data can reinforce the structural biases already present in society. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches show regularly biased behaviors. However, we show that standard mitigation techniques, and our own post-hoc method, can be effective in reducing the level of unfair bias. We provide practical recommendations to develop ML models for depression risk prediction with increased fairness and trust in the real world. No single best ML model for depression prediction provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26041;&#24046;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#23398;&#20064;&#20013;&#21152;&#26435;&#27599;&#20010;&#39057;&#35889;&#22270;&#26102;&#38388;&#24103;&#30340;&#36129;&#29486;&#65292;&#24182;&#20351;&#29992;Gamma&#20808;&#39564;&#20998;&#24067;&#23558;&#22797;&#26434;&#20540;&#39640;&#26031;&#20998;&#24067;&#25913;&#20026;&#23398;&#29983;t&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#21644;&#26356;&#40065;&#26834;&#30340;&#35821;&#38899;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2211.00990</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#26435;&#26041;&#24046;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
A weighted-variance variational autoencoder model for speech enhancement. (arXiv:2211.00990v2 [cs.SD] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00990
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26041;&#24046;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#23398;&#20064;&#20013;&#21152;&#26435;&#27599;&#20010;&#39057;&#35889;&#22270;&#26102;&#38388;&#24103;&#30340;&#36129;&#29486;&#65292;&#24182;&#20351;&#29992;Gamma&#20808;&#39564;&#20998;&#24067;&#23558;&#22797;&#26434;&#20540;&#39640;&#26031;&#20998;&#24067;&#25913;&#20026;&#23398;&#29983;t&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#21644;&#26356;&#40065;&#26834;&#30340;&#35821;&#38899;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#35821;&#38899;&#22686;&#24378;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#28041;&#21450;&#22312;&#26102;&#39057;&#65288;TF&#65289;&#22495;&#20013;&#23398;&#20064;&#35821;&#38899;&#20808;&#39564;&#20998;&#24067;&#12290;&#36890;&#24120;&#20551;&#35774;&#29983;&#25104;&#27169;&#22411;&#20026;&#38646;&#22343;&#20540;&#22797;&#25968;&#39640;&#26031;&#20998;&#24067;&#65292;&#20854;&#20013;&#35821;&#38899;&#20449;&#24687;&#32534;&#30721;&#22312;&#26041;&#24046;&#20013;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#30340;&#20989;&#25968;&#12290;&#19982;&#24120;&#29992;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26041;&#24046;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#39057;&#35889;&#22270;&#26102;&#38388;&#24103;&#22312;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#36129;&#29486;&#21152;&#26435;&#12290;&#25105;&#20204;&#23545;&#26435;&#37325;&#26045;&#21152;Gamma&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#23558;&#26377;&#25928;&#22320;&#23548;&#33268;&#20351;&#29992;&#23398;&#29983;t&#20998;&#24067;&#32780;&#19981;&#26159;&#39640;&#26031;&#20998;&#24067;&#36827;&#34892;&#35821;&#38899;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#35821;&#38899;&#22686;&#24378;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#39057;&#35889;&#22270;&#33258;&#32534;&#30721;&#21644;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;&#26410;&#21152;&#26435;&#26041;&#24046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address speech enhancement based on variational autoencoders, which involves learning a speech prior distribution in the time-frequency (TF) domain. A zero-mean complex-valued Gaussian distribution is usually assumed for the generative model, where the speech information is encoded in the variance as a function of a latent variable. In contrast to this commonly used approach, we propose a weighted variance generative model, where the contribution of each spectrogram time-frame in parameter learning is weighted. We impose a Gamma prior distribution on the weights, which would effectively lead to a Student's t-distribution instead of Gaussian for speech generative modeling. We develop efficient training and speech enhancement algorithms based on the proposed generative model. Our experimental results on spectrogram auto-encoding and speech enhancement demonstrate the effectiveness and robustness of the proposed approach compared to the standard unweighted variance model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#25919;&#24220;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#26631;&#20934;&#21270;&#25805;&#20316;&#31243;&#24207;&#21644;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#26399;&#26395;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22810;&#23398;&#31185;&#30740;&#31350;&#32773;&#22312;&#27010;&#24565;&#19978;&#30340;&#30862;&#29255;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.17218</link><description>&lt;p&gt;
&#25919;&#24220;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#27010;&#24565;&#65292;&#26631;&#20934;&#21644;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence in government: Concepts, standards, and a unified framework. (arXiv:2210.17218v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#25919;&#24220;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#26631;&#20934;&#21270;&#25805;&#20316;&#31243;&#24207;&#21644;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#26399;&#26395;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22810;&#23398;&#31185;&#30740;&#31350;&#32773;&#22312;&#27010;&#24565;&#19978;&#30340;&#30862;&#29255;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#25919;&#24220;&#25913;&#21464;&#30340;&#24076;&#26395;&#21464;&#24471;&#33021;&#22815;&#23454;&#29616;&#12290;&#37492;&#20110;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#20351;&#29992;&#26631;&#20934;&#25805;&#20316;&#31243;&#24207;&#23558;&#20854;&#23884;&#20837;&#65292;&#24182;&#31526;&#21512;&#31038;&#20250;&#30340;&#35268;&#33539;&#26399;&#26395;&#12290;&#22810;&#20010;&#39046;&#22495;&#30340;&#23398;&#32773;&#24320;&#22987;&#27010;&#24565;&#21270;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#19981;&#21516;&#24418;&#24335;&#65292;&#24378;&#35843;&#20854;&#28508;&#22312;&#30340;&#22909;&#22788;&#21644;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20173;&#28982;&#30862;&#29255;&#21270;&#65292;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#22914;&#20844;&#20849;&#31649;&#29702;&#21644;&#25919;&#27835;&#31185;&#23398;&#65292;&#20197;&#21450;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#39046;&#22495;&#37117;&#22312;&#30456;&#23545;&#23396;&#31435;&#30340;&#24773;&#20917;&#19979;&#21457;&#23637;&#27010;&#24565;&#12290;&#34429;&#28982;&#26377;&#21628;&#21505;&#23545;&#25919;&#24220;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#24418;&#24335;&#21270;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#29702;&#35299;&#23558;&#20154;&#24037;&#26234;&#33021;&#23884;&#20837;&#20844;&#20849;&#39046;&#22495;&#30340;&#21518;&#26524;&#25152;&#38656;&#30340;&#29702;&#35770;&#35266;&#28857;&#30340;&#20840;&#38754;&#32508;&#21512;&#25551;&#36848;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in artificial intelligence (AI), especially in generative language modelling, hold the promise of transforming government. Given the advanced capabilities of new AI systems, it is critical that these are embedded using standard operational procedures, clear epistemic criteria, and behave in alignment with the normative expectations of society. Scholars in multiple domains have subsequently begun to conceptualize the different forms that AI applications may take, highlighting both their potential benefits and pitfalls. However, the literature remains fragmented, with researchers in social science disciplines like public administration and political science, and the fast-moving fields of AI, ML, and robotics, all developing concepts in relative isolation. Although there are calls to formalize the emerging study of AI in government, a balanced account that captures the full depth of theoretical perspectives needed to understand the consequences of embedding AI into a publi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23545;&#25239;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24046;&#24322;&#26356;&#26032;&#32534;&#30721;&#22120;&#21442;&#25968;&#24182;&#26045;&#21152;&#23545;&#25239;&#25915;&#20987;&#26469;&#22686;&#24378;&#20803;&#23398;&#20064;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#22312;&#26410;&#35265;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#30340;&#21487;&#36801;&#31227;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2210.10485</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#40065;&#26834;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Transferable Adversarial Robust Representations via Multi-view Consistency. (arXiv:2210.10485v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23545;&#25239;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24046;&#24322;&#26356;&#26032;&#32534;&#30721;&#22120;&#21442;&#25968;&#24182;&#26045;&#21152;&#23545;&#25239;&#25915;&#20987;&#26469;&#22686;&#24378;&#20803;&#23398;&#20064;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#22312;&#26410;&#35265;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#30340;&#21487;&#36801;&#31227;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#20803;&#23398;&#20064;&#27169;&#22411;&#21482;&#20851;&#27880;&#22312;&#24178;&#20928;&#26679;&#26412;&#19978;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;&#38754;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#26679;&#26412;&#26102;&#23481;&#26131;&#23849;&#28291;&#12290;&#36817;&#26399;&#30340;&#19968;&#20123;&#24037;&#20316;&#34920;&#26126;&#65292;&#23545;&#25239;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#32467;&#21512;&#21487;&#20197;&#22686;&#24378;&#20803;&#23398;&#20064;&#22120;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#23454;&#29616;&#22312;&#26410;&#35265;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#30340;&#19968;&#33324;&#21270;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#20803;&#23398;&#20064;&#30340;&#26368;&#32456;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#23545;&#25239;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#21452;&#32534;&#30721;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#22686;&#24378;&#26679;&#26412;&#30340;&#25968;&#25454;&#23454;&#20363;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#26356;&#26032;&#32534;&#30721;&#22120;&#30340;&#21442;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#26045;&#21152;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26631;&#31614;&#23545;&#25239;&#25915;&#20987;&#26469;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#22823;&#21270;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#40065;&#26834;&#34920;&#31034;&#36328;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Despite the success on few-shot learning problems, most meta-learned models only focus on achieving good performance on clean examples and thus easily break down when given adversarially perturbed samples. While some recent works have shown that a combination of adversarial learning and meta-learning could enhance the robustness of a meta-learner against adversarial attacks, they fail to achieve generalizable adversarial robustness to unseen domains and tasks, which is the ultimate goal of meta-learning. To address this challenge, we propose a novel meta-adversarial multi-view representation learning framework with dual encoders. Specifically, we introduce the discrepancy across the two differently augmented samples of the same data instance by first updating the encoder parameters with them and further imposing a novel label-free adversarial attack to maximize their discrepancy. Then, we maximize the consistency across the views to learn transferable robust representations across doma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#30340;&#27491;&#21521;&#25366;&#25496;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20196;&#20154;&#22256;&#24785;&#20294;&#30456;&#20284;&#30340;&#30446;&#26631;&#26679;&#26412;&#26469;&#29983;&#25104;&#26377;&#25928;&#30340;&#23545;&#25163;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10482</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26377;&#25928;&#30446;&#26631;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Effective Targeted Attacks for Adversarial Self-Supervised Learning. (arXiv:2210.10482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#30340;&#27491;&#21521;&#25366;&#25496;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20196;&#20154;&#22256;&#24785;&#20294;&#30456;&#20284;&#30340;&#30446;&#26631;&#26679;&#26412;&#26469;&#29983;&#25104;&#26377;&#25928;&#30340;&#23545;&#25163;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#31614;&#20449;&#24687;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#25163;&#27573;&#12290;&#20197;&#24448;&#30340;&#26080;&#30417;&#30563;AT&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#27599;&#20010;&#23454;&#20363;&#30340;&#20998;&#31867;&#25439;&#22833;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#31616;&#21333;&#22320;&#36890;&#36807;&#26080;&#30446;&#26631;&#23545;&#25239;&#25915;&#20987;&#26469;&#26368;&#22823;&#21270;&#33258;&#30417;&#30563;&#35757;&#32451;&#25439;&#22833;&#24448;&#24448;&#20250;&#29983;&#25104;&#26080;&#25928;&#30340;&#23545;&#25163;&#65292;&#36825;&#20123;&#23545;&#25163;&#21487;&#33021;&#26080;&#27861;&#24110;&#21161;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;&#36127;&#26679;&#26412;&#30340;&#38750;&#23545;&#27604;&#24615;SSL&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;SSL&#26694;&#26550;&#30340;&#26032;&#22411;&#27491;&#21521;&#25366;&#25496;&#26041;&#27861;&#26469;&#29983;&#25104;&#26377;&#25928;&#30340;&#23545;&#25163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#21644;&#30456;&#20284;&#24615;&#30340;&#31639;&#27861;&#65292;&#20026;&#32473;&#23450;&#23454;&#20363;&#36873;&#25321;&#26368;&#20196;&#20154;&#22256;&#24785;&#20294;&#30456;&#20284;&#30340;&#30446;&#26631;&#26679;&#26412;&#65292;&#28982;&#21518;&#25200;&#21160;&#32473;&#23450;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, unsupervised adversarial training (AT) has been highlighted as a means of achieving robustness in models without any label information. Previous studies in unsupervised AT have mostly focused on implementing self-supervised learning (SSL) frameworks, which maximize the instance-wise classification loss to generate adversarial examples. However, we observe that simply maximizing the self-supervised training loss with an untargeted adversarial attack often results in generating ineffective adversaries that may not help improve the robustness of the trained model, especially for non-contrastive SSL frameworks without negative examples. To tackle this problem, we propose a novel positive mining for targeted adversarial attack to generate effective adversaries for adversarial SSL frameworks. Specifically, we introduce an algorithm that selects the most confusing yet similar target example for a given instance based on entropy and similarity, and subsequently perturbs the given ins
&lt;/p&gt;</description></item><item><title>&#22312;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#27809;&#26377;&#20849;&#35782;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861;eXirt&#65292;&#29992;&#20110;&#35299;&#37322;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#25490;&#21517;&#26469;&#35299;&#37322;&#27169;&#22411;&#36755;&#20837;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.09933</link><description>&lt;p&gt;
&#22522;&#20110;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#30340;&#35299;&#37322;&#65288;eXirt&#65289;&#65306;&#19968;&#31181;&#22312;&#20449;&#20219;&#35270;&#35282;&#19979;&#35299;&#37322;&#26641;&#38598;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#29305;&#23450;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explanations Based on Item Response Theory (eXirt): A Model-Specific Method to Explain Tree-Ensemble Model in Trust Perspective. (arXiv:2210.09933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09933
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#27809;&#26377;&#20849;&#35782;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861;eXirt&#65292;&#29992;&#20110;&#35299;&#37322;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#25490;&#21517;&#26469;&#35299;&#37322;&#27169;&#22411;&#36755;&#20837;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;XAI&#30740;&#31350;&#20154;&#21592;&#27491;&#35268;&#33539;&#21270;&#25552;&#26696;&#21644;&#24320;&#21457;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#65292;&#20294;&#22312;&#31038;&#21306;&#20013;&#23545;&#20110;&#20351;&#29992;&#21738;&#31181;&#26041;&#27861;&#26469;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#23578;&#26080;&#19968;&#33268;&#24847;&#35265;&#65292;&#32780;&#36825;&#31181;&#36873;&#25321;&#20960;&#20046;&#30452;&#25509;&#19982;&#29305;&#23450;&#26041;&#27861;&#30340;&#27969;&#34892;&#24230;&#30456;&#20851;&#12290;&#35832;&#22914;Ciu&#12289;Dalex&#12289;Eli5&#12289;Lofo&#12289;Shap&#21644;Skater&#31561;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#20840;&#23616;&#25490;&#21517;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#19981;&#21516;&#30340;&#26041;&#27861;&#23398;&#29983;&#25104;&#20840;&#23616;&#35299;&#37322;&#65292;&#35828;&#26126;&#27169;&#22411;&#36755;&#20837;&#22914;&#20309;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20102;41&#20010;&#25968;&#25454;&#38598;&#12289;4&#31181;&#26641;&#38598;&#25104;&#31639;&#27861;&#65288;Light Gradient Boosting&#12289;CatBoost&#12289;Random Forest&#21644;Gradient Boosting&#65289;&#21644;6&#31181;XAI&#26041;&#27861;&#26469;&#25903;&#25345;&#25512;&#20986;&#19968;&#31181;&#21517;&#20026;eXirt&#30340;&#26032;&#30340;XAI&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#65292;&#26088;&#22312;&#35299;&#37322;&#20351;&#29992;&#20851;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#34920;&#26684;&#25968;&#25454;&#30340;&#26641;&#38598;&#25104;&#40657;&#30418;&#27169;&#22411;&#12290;&#22312;&#31532;&#19968;&#32452;&#20998;&#26512;&#20013;&#65292;164&#20010;&#20840;&#23616;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
In recent years, XAI researchers have been formalizing proposals and developing new methods to explain black box models, with no general consensus in the community on which method to use to explain these models, with this choice being almost directly linked to the popularity of a specific method. Methods such as Ciu, Dalex, Eli5, Lofo, Shap and Skater emerged with the proposal to explain black box models through global rankings of feature relevance, which based on different methodologies, generate global explanations that indicate how the model's inputs explain its predictions. In this context, 41 datasets, 4 tree-ensemble algorithms (Light Gradient Boosting, CatBoost, Random Forest, and Gradient Boosting), and 6 XAI methods were used to support the launch of a new XAI method, called eXirt, based on Item Response Theory IRT and aimed at tree-ensemble black box models that use tabular data referring to binary classification problems. In the first set of analyses, the 164 global featur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#22320;&#28909;&#36164;&#28304;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#21644;&#24050;&#30693;&#29305;&#24449;&#22270;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21487;&#29992;&#20110;&#25214;&#20986;&#28508;&#21147;&#26356;&#39640;&#30340;&#21306;&#22495;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#12290;</title><link>http://arxiv.org/abs/2209.15543</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#22320;&#28909;&#36164;&#28304;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#65306;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks for Geothermal Resource Assessment: Prediction with Uncertainty. (arXiv:2209.15543v2 [physics.geo-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#22320;&#28909;&#36164;&#28304;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#21644;&#24050;&#30693;&#29305;&#24449;&#22270;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21487;&#29992;&#20110;&#25214;&#20986;&#28508;&#21147;&#26356;&#39640;&#30340;&#21306;&#22495;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#22320;&#28909;&#36164;&#28304;&#28508;&#21147;&#35780;&#20272;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#32654;&#22269;&#20869;&#21326;&#36798;&#24030;&#30340;10&#20010;&#22320;&#36136;&#21644;&#22320;&#29699;&#29289;&#29702;&#29305;&#24449;&#22270;&#26469;&#30028;&#23450;&#24191;&#27867;&#21306;&#22495;&#20869;&#30340;&#22320;&#28909;&#28508;&#21147;&#12290;&#25105;&#20204;&#26377;&#19968;&#32452;&#30456;&#23545;&#36739;&#23567;&#30340;&#27491;&#26679;&#26412;&#35757;&#32451;&#28857;&#65288;&#24050;&#30693;&#36164;&#28304;&#25110;&#27963;&#36291;&#21457;&#30005;&#21378;&#65289;&#21644;&#36127;&#26679;&#26412;&#35757;&#32451;&#28857;&#65288;&#24050;&#30693;&#38075;&#25506;&#28857;&#20294;&#23384;&#22312;&#19981;&#36866;&#21512;&#30340;&#22320;&#28909;&#26465;&#20214;&#65289;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#28857;&#26469;&#32422;&#26463;&#21644;&#20248;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#24050;&#30693;&#23450;&#20041;&#29305;&#24449;&#30340;&#22823;&#38754;&#31215;&#22320;&#29702;&#21306;&#22495;&#20869;&#39044;&#27979;&#26410;&#30693;&#28857;&#30340;&#22320;&#28909;&#36164;&#28304;&#28508;&#21147;&#12290;&#36825;&#20123;&#39044;&#27979;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#26377;&#21069;&#26223;&#30340;&#21306;&#22495;&#36827;&#34892;&#35814;&#32454;&#35843;&#26597;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#20174;&#23450;&#20041;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21040;&#35757;&#32451;&#21644;&#20248;&#21270;&#35797;&#39564;&#30340;&#28436;&#21464;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the application of machine learning to the evaluation of geothermal resource potential. A supervised learning problem is defined where maps of 10 geological and geophysical features within the state of Nevada, USA are used to define geothermal potential across a broad region. We have available a relatively small set of positive training sites (known resources or active power plants) and negative training sites (known drill sites with unsuitable geothermal conditions) and use these to constrain and optimize artificial neural networks for this classification task. The main objective is to predict the geothermal resource potential at unknown sites within a large geographic area where the defining features are known. These predictions could be used to target promising areas for further detailed investigations. We describe the evolution of our work from defining a specific neural network architecture to training and optimization trials. Upon analysis we expose the inevitable pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#32858;&#31867;&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24341;&#20837;&#19968;&#31181;&#19968;&#33268;&#30340;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#12290;&#36824;&#27719;&#24635;&#12289;&#25913;&#36827;&#21644;&#26631;&#20934;&#21270;&#20102;&#35768;&#22810;&#32858;&#31867;&#22522;&#20934;&#25968;&#25454;&#38598;&#21512;&#65292;&#24182;&#21253;&#21547;&#20102;&#26032;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#20114;&#21160;&#25968;&#25454;&#38598;&#27983;&#35272;&#22120;&#12289;Python API&#30340;&#25991;&#26723;&#20197;&#21450;&#19982;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#26694;&#26550;&#20132;&#20114;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2209.09493</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#32858;&#31867;&#31639;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for benchmarking clustering algorithms. (arXiv:2209.09493v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#32858;&#31867;&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24341;&#20837;&#19968;&#31181;&#19968;&#33268;&#30340;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#12290;&#36824;&#27719;&#24635;&#12289;&#25913;&#36827;&#21644;&#26631;&#20934;&#21270;&#20102;&#35768;&#22810;&#32858;&#31867;&#22522;&#20934;&#25968;&#25454;&#38598;&#21512;&#65292;&#24182;&#21253;&#21547;&#20102;&#26032;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#20114;&#21160;&#25968;&#25454;&#38598;&#27983;&#35272;&#22120;&#12289;Python API&#30340;&#25991;&#26723;&#20197;&#21450;&#19982;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#26694;&#26550;&#20132;&#20114;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#31639;&#27861;&#30340;&#35780;&#20272;&#21487;&#20197;&#28041;&#21450;&#22312;&#21508;&#31181;&#22522;&#20934;&#38382;&#39064;&#19978;&#36816;&#34892;&#23427;&#20204;&#65292;&#24182;&#23558;&#20854;&#36755;&#20986;&#19982;&#19987;&#23478;&#25552;&#20379;&#30340;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#30740;&#31350;&#35770;&#25991;&#21644;&#30740;&#31350;&#29983;&#35770;&#25991;&#21482;&#32771;&#34385;&#20102;&#23569;&#25968;&#25968;&#25454;&#38598;&#12290;&#32780;&#19988;&#65292;&#24456;&#23569;&#32771;&#34385;&#21040;&#22312;&#32473;&#23450;&#38382;&#39064;&#38598;&#19978;&#21487;&#20197;&#26377;&#35768;&#22810;&#21516;&#26679;&#26377;&#25928;&#30340;&#32858;&#31867;&#26041;&#27861;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#30446;&#30340;&#26159;&#24341;&#20837;&#19968;&#31181;&#19968;&#33268;&#30340;&#26041;&#27861;&#26469;&#27979;&#35797;&#32858;&#31867;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27719;&#24635;&#12289;&#25913;&#36827;&#21644;&#26631;&#20934;&#21270;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#25991;&#29486;&#20013;&#25552;&#21040;&#30340;&#35768;&#22810;&#32858;&#31867;&#22522;&#20934;&#25968;&#25454;&#38598;&#21512;&#65292;&#24182;&#21253;&#21547;&#20102;&#20855;&#26377;&#19981;&#21516;&#32500;&#24230;&#12289;&#22823;&#23567;&#21644;&#32858;&#31867;&#31867;&#22411;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#36824;&#26377;&#19968;&#20010;&#20114;&#21160;&#25968;&#25454;&#38598;&#27983;&#35272;&#22120;&#12289;Python API&#30340;&#25991;&#26723;&#20197;&#21450;&#22914;&#20309;&#19982;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;R&#25110;MATLAB&#65289;&#36827;&#34892;&#26694;&#26550;&#20132;&#20114;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of clustering algorithms can involve running them on a variety of benchmark problems, and comparing their outputs to the reference, ground-truth groupings provided by experts. Unfortunately, many research papers and graduate theses consider only a small number of datasets. Also, the fact that there can be many equally valid ways to cluster a given problem set is rarely taken into account. In order to overcome these limitations, we have developed a framework whose aim is to introduce a consistent methodology for testing clustering algorithms. Furthermore, we have aggregated, polished, and standardised many clustering benchmark dataset collections referred to across the machine learning and data mining literature, and included new datasets of different dimensionalities, sizes, and cluster types. An interactive datasets explorer, the documentation of the Python API, a description of the ways to interact with the framework from other programming languages such as R or MATLAB
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06589</link><description>&lt;p&gt;
&#22810;&#27169;&#22359;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#34920;&#24449;&#20419;&#36827;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#19982;&#25512;&#26029;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#20197;&#21450;&#25512;&#24191;&#21040;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#38480;&#21046;&#30340;&#20102;&#35299;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#20351;&#29992;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23395;&#33410;&#27604;&#20363;&#35780;&#20998;(SRS)&#26469;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#20998;&#35299;&#20026;&#25353;&#31867;&#21035;&#30340;&#35821;&#20041;&#32452;&#20214;&#21644;&#20313;&#39033;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#25353;&#31867;&#21035;&#30340;&#26465;&#20214;&#27010;&#29575;&#65292;&#20174;&#32780;&#35745;&#31639;&#23395;&#33410;&#27604;&#20363;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#38408;&#20540;&#21306;&#38388;&#26469;&#26816;&#27979;&#31163;&#32676;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2207.04306</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#31163;&#32676;&#26816;&#27979;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#23395;&#33410;&#27604;&#20363;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection in Time-Series Domain: A Novel Seasonal Ratio Scoring Approach. (arXiv:2207.04306v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23395;&#33410;&#27604;&#20363;&#35780;&#20998;(SRS)&#26469;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#20998;&#35299;&#20026;&#25353;&#31867;&#21035;&#30340;&#35821;&#20041;&#32452;&#20214;&#21644;&#20313;&#39033;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#25353;&#31867;&#21035;&#30340;&#26465;&#20214;&#27010;&#29575;&#65292;&#20174;&#32780;&#35745;&#31639;&#23395;&#33410;&#27604;&#20363;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#38408;&#20540;&#21306;&#38388;&#26469;&#26816;&#27979;&#31163;&#32676;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23433;&#20840;&#37096;&#32626;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#20381;&#36182;&#20110;&#33021;&#22815;&#26816;&#27979;&#19981;&#26159;&#30001;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#36825;&#20010;&#20219;&#21153;&#34987;&#31216;&#20026;&#31163;&#32676;&#26816;&#27979;(out-of-distribution, OOD)&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;ODO&#26816;&#27979;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26469;&#33258;&#22270;&#20687;&#39046;&#22495;&#30340;&#20808;&#21069;&#26041;&#27861;&#25928;&#26524;&#19981;&#20339;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23395;&#33410;&#27604;&#20363;&#35780;&#20998;(SRS)&#26041;&#27861;&#12290;SRS&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#31639;&#27861;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#23558;&#27599;&#20010;&#36755;&#20837;&#20998;&#35299;&#20026;&#25353;&#31867;&#21035;&#30340;&#35821;&#20041;&#32452;&#20214;&#21644;&#20313;&#39033;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20010;&#20998;&#35299;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#20272;&#35745;&#36755;&#20837;&#21644;&#20313;&#39033;&#30340;&#25353;&#31867;&#21035;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#20174;&#36825;&#20123;&#20272;&#35745;&#20540;&#35745;&#31639;&#23395;&#33410;&#27604;&#20363;&#35780;&#20998;&#12290;&#26368;&#21518;&#65292;&#20174;&#27491;&#24120;&#20998;&#24067;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#38408;&#20540;&#21306;&#38388;&#26469;&#26816;&#27979;&#31163;&#32676;&#26679;&#26412;&#12290;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Safe deployment of time-series classifiers for real-world applications relies on the ability to detect the data which is not generated from the same distribution as training data. This task is referred to as out-of-distribution (OOD) detection. We consider the novel problem of OOD detection for the time-series domain. We discuss the unique challenges posed by time-series data and explain why prior methods from the image domain will perform poorly. Motivated by these challenges, this paper proposes a novel {\em Seasonal Ratio Scoring (SRS)} approach. SRS consists of three key algorithmic steps. First, each input is decomposed into class-wise semantic component and remainder. Second, this decomposition is employed to estimate the class-wise conditional likelihoods of the input and remainder using deep generative models. The seasonal ratio score is computed from these estimates. Third, a threshold interval is identified from the in-distribution data to detect OOD examples. Experiments on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#28151;&#27927;&#22411;&#26799;&#24230;&#31639;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#19979;&#23545;&#19968;&#31867;&#38750;&#20984;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#25910;&#25947;&#21040;&#20840;&#23616;&#35299;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#19982;&#19968;&#33324;&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2206.05869</link><description>&lt;p&gt;
&#20851;&#20110;&#28151;&#27927;&#22411;&#26799;&#24230;&#31639;&#27861;&#25910;&#25947;&#21040;&#20840;&#23616;&#35299;&#30340;&#35770;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Convergence to a Global Solution of Shuffling-Type Gradient Algorithms. (arXiv:2206.05869v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#28151;&#27927;&#22411;&#26799;&#24230;&#31639;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#19979;&#23545;&#19968;&#31867;&#38750;&#20984;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#25910;&#25947;&#21040;&#20840;&#23616;&#35299;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#19982;&#19968;&#33324;&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#22240;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#25928;&#29575;&#32780;&#21463;&#38738;&#30544;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28151;&#27927;&#29256;&#26412;&#30340;SGD&#65292;&#35813;&#29256;&#26412;&#19982;&#20027;&#27969;&#30340;&#23454;&#38469;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#19979;&#65292;&#23545;&#20110;&#19968;&#31867;&#38750;&#20984;&#20989;&#25968;&#65292;&#28151;&#27927;SGD&#25910;&#25947;&#21040;&#20840;&#23616;&#35299;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20351;&#29992;&#20102;&#27604;&#20808;&#21069;&#25991;&#29486;&#26356;&#23485;&#26494;&#30340;&#38750;&#20984;&#20551;&#35774;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#20173;&#28982;&#20445;&#25345;&#20102;&#28151;&#27927;SGD&#22312;&#19968;&#33324;&#20984;&#35774;&#32622;&#20013;&#25152;&#21462;&#24471;&#30340;&#26399;&#26395;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) algorithm is the method of choice in many machine learning tasks thanks to its scalability and efficiency in dealing with large-scale problems. In this paper, we focus on the shuffling version of SGD which matches the mainstream practical heuristics. We show the convergence to a global solution of shuffling SGD for a class of non-convex functions under over-parameterized settings. Our analysis employs more relaxed non-convex assumptions than previous literature. Nevertheless, we maintain the desired computational complexity as shuffling SGD has achieved in the general convex setting.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#23545;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#31209;&#26368;&#23567;&#21270;&#30340;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#36739;&#23567;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#39640;&#23398;&#20064;&#29575;&#25110;&#22686;&#21152;&#26435;&#37325;&#34928;&#20943;&#26102;&#26356;&#20026;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#22312;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26102;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#29305;&#21035;&#20302;&#31209;&#12290;&#36825;&#31181;&#20559;&#24046;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2206.05794</link><description>&lt;p&gt;
SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#34987;&#35777;&#26126;&#20250;&#24341;&#20837;&#20302;&#31209;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks. (arXiv:2206.05794v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05794
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#23545;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#31209;&#26368;&#23567;&#21270;&#30340;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#36739;&#23567;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#39640;&#23398;&#20064;&#29575;&#25110;&#22686;&#21152;&#26435;&#37325;&#34928;&#20943;&#26102;&#26356;&#20026;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#22312;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26102;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#29305;&#21035;&#20302;&#31209;&#12290;&#36825;&#31181;&#20559;&#24046;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#26102;&#23398;&#20064;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#23545;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#31209;&#26368;&#23567;&#21270;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#39640;&#30340;&#23398;&#20064;&#29575;&#25110;&#22686;&#21152;&#30340;&#26435;&#37325;&#34928;&#20943;&#26102;&#65292;&#36825;&#31181;&#20559;&#24046;&#26356;&#21152;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39044;&#27979;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#26435;&#37325;&#34928;&#20943;&#26159;&#23454;&#29616;&#36825;&#31181;&#20559;&#24046;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#29305;&#21035;&#20302;&#31209;&#12290;&#19982;&#20808;&#21069;&#30340;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#19981;&#20381;&#36182;&#20110;&#20851;&#20110;&#25968;&#25454;&#12289;&#25910;&#25947;&#24615;&#25110;&#26435;&#37325;&#30697;&#38453;&#20248;&#21270;&#30340;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#23427;&#36866;&#29992;&#20110;&#20219;&#24847;&#23485;&#24230;&#25110;&#28145;&#24230;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#20559;&#24046;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the bias of Stochastic Gradient Descent (SGD) to learn low-rank weight matrices when training deep ReLU neural networks. Our results show that training neural networks with mini-batch SGD and weight decay causes a bias towards rank minimization over the weight matrices. Specifically, we show, both theoretically and empirically, that this bias is more pronounced when using smaller batch sizes, higher learning rates, or increased weight decay. Additionally, we predict and observe empirically that weight decay is necessary to achieve this bias. In addition, we show that in the presence of intermediate neural collapse, the learned weights are particularly low-rank. Unlike previous literature, our analysis does not rely on assumptions about the data, convergence, or optimality of the weight matrices. Furthermore, it applies to a wide range of neural network architectures of any width or depth. Finally, we empirically investigate the connection between this bias and generalization, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;&#30456;&#27604;&#20110;&#24120;&#35265;&#30340;&#27431;&#20960;&#37324;&#24471;&#25104;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#28789;&#27963;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#36741;&#21161;&#20449;&#24687;&#26500;&#24314;&#20256;&#36755;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#31354;&#38388;&#19979;&#22788;&#29702;&#26032;&#25968;&#25454;&#28857;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#35823;&#24046;&#20998;&#26512;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#35813;&#35770;&#25991;&#26500;&#36896;&#20102;&#19968;&#20010;&#33021;&#22815;&#22312;&#20445;&#30041;&#31867;&#21035;&#32467;&#26500;&#30340;&#21516;&#26102;&#26144;&#23556;&#25968;&#25454;&#20998;&#24067;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2205.15403</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#30340;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Neural Optimal Transport with General Cost Functionals. (arXiv:2205.15403v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15403
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;&#30456;&#27604;&#20110;&#24120;&#35265;&#30340;&#27431;&#20960;&#37324;&#24471;&#25104;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#28789;&#27963;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#36741;&#21161;&#20449;&#24687;&#26500;&#24314;&#20256;&#36755;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#31354;&#38388;&#19979;&#22788;&#29702;&#26032;&#25968;&#25454;&#28857;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#35823;&#24046;&#20998;&#26512;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#35813;&#35770;&#25991;&#26500;&#36896;&#20102;&#19968;&#20010;&#33021;&#22815;&#22312;&#20445;&#30041;&#31867;&#21035;&#32467;&#26500;&#30340;&#21516;&#26102;&#26144;&#23556;&#25968;&#25454;&#20998;&#24067;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;&#19982;&#24120;&#35265;&#30340;&#27431;&#20960;&#37324;&#24471;&#25104;&#26412;&#65288;&#22914;$\ell^1$&#25110;$\ell^2$&#65289;&#19981;&#21516;&#65292;&#36825;&#31181;&#20989;&#25968;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#36741;&#21161;&#20449;&#24687;&#65288;&#22914;&#31867;&#21035;&#26631;&#31614;&#65289;&#26469;&#26500;&#24314;&#25152;&#38656;&#30340;&#20256;&#36755;&#26144;&#23556;&#12290;&#29616;&#26377;&#30340;&#19968;&#33324;&#25104;&#26412;&#26041;&#27861;&#26159;&#31163;&#25955;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#21363;&#23427;&#20204;&#19981;&#33021;&#25552;&#20379;&#26679;&#26412;&#22806;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#19968;&#33324;&#25104;&#26412;&#35774;&#35745;&#36830;&#32493;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25512;&#24191;&#21040;&#39640;&#32500;&#31354;&#38388;&#65288;&#22914;&#22270;&#20687;&#65289;&#20013;&#30340;&#26032;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#24674;&#22797;&#30340;&#20256;&#36755;&#26041;&#26696;&#36827;&#34892;&#20102;&#29702;&#35770;&#35823;&#24046;&#20998;&#26512;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65292;&#29992;&#20110;&#22312;&#20445;&#30041;&#31867;&#21035;&#32467;&#26500;&#30340;&#21516;&#26102;&#26144;&#23556;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel neural network-based algorithm to compute optimal transport (OT) plans for general cost functionals. In contrast to common Euclidean costs, i.e., $\ell^1$ or $\ell^2$, such functionals provide more flexibility and allow using auxiliary information, such as class labels, to construct the required transport map. Existing methods for general costs are discrete and have limitations in practice, i.e. they do not provide an out-of-sample estimation. We address the challenge of designing a continuous OT approach for general costs that generalizes to new data points in high-dimensional spaces, such as images. Additionally, we provide the theoretical error analysis for our recovered transport plans. As an application, we construct a cost functional to map data distributions while preserving the class-wise structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#20851;&#20110;&#25968;&#20540;&#29305;&#24449;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26500;&#24314;&#23884;&#20837;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#27169;&#22359;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#22312;&#19968;&#20123;&#20256;&#32479;&#19978;&#36866;&#29992;&#20110;GBDT&#30340;&#22522;&#20934;&#19978;&#19982;&#20043;&#31454;&#20105;&#20855;&#26377;&#37325;&#35201;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2203.05556</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#20540;&#29305;&#24449;&#22312;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
On Embeddings for Numerical Features in Tabular Deep Learning. (arXiv:2203.05556v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#20851;&#20110;&#25968;&#20540;&#29305;&#24449;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26500;&#24314;&#23884;&#20837;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#27169;&#22359;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#22312;&#19968;&#20123;&#20256;&#32479;&#19978;&#36866;&#29992;&#20110;GBDT&#30340;&#22522;&#20934;&#19978;&#19982;&#20043;&#31454;&#20105;&#20855;&#26377;&#37325;&#35201;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31867;&#20284;Transformer&#30340;&#28145;&#24230;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38382;&#39064;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#65288;&#22914;MLP&#65289;&#19981;&#21516;&#65292;&#36825;&#20123;&#26550;&#26500;&#23558;&#25968;&#20540;&#29305;&#24449;&#30340;&#26631;&#37327;&#20540;&#26144;&#23556;&#21040;&#39640;&#32500;&#23884;&#20837;&#20013;&#65292;&#28982;&#21518;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#23558;&#23427;&#20204;&#28151;&#21512;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#25968;&#20540;&#29305;&#24449;&#30340;&#23884;&#20837;&#22312;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#33258;&#30001;&#24230;&#65292;&#23427;&#20801;&#35768;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#19978;&#36866;&#29992;&#20110;GBDT&#30340;&#22522;&#20934;&#36827;&#34892;&#31454;&#20105;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#26500;&#24314;&#23884;&#20837;&#27169;&#22359;&#30340;&#20004;&#31181;&#27010;&#24565;&#19978;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#31181;&#22522;&#20110;&#26631;&#37327;&#20540;&#30340;&#20998;&#27573;&#32447;&#24615;&#32534;&#30721;&#65292;&#31532;&#20108;&#31181;&#21033;&#29992;&#21608;&#26399;&#24615;&#28608;&#27963;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#19982;&#22522;&#20110;&#32447;&#24615;&#23618;&#21644;ReLU&#28608;&#27963;&#30340;&#20256;&#32479;&#27169;&#22359;&#30456;&#27604;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23884;&#20837;&#25968;&#20540;&#29305;&#24449;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with GBDT on some traditionally GBDT-friendly benchmarks. We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#20248;&#21183;&#32593;&#32476;&#65288;LAN&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23545;&#20915;&#26550;&#26500;&#21644;&#20013;&#24515;&#21270;&#35780;&#35770;&#23478;&#26469;&#23398;&#20064;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65292;&#24182;&#22312;StarCraft II&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.12458</link><description>&lt;p&gt;
&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#23616;&#37096;&#20248;&#21183;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2112.12458v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12458
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#20248;&#21183;&#32593;&#32476;&#65288;LAN&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23545;&#20915;&#26550;&#26500;&#21644;&#20013;&#24515;&#21270;&#35780;&#35770;&#23478;&#26469;&#23398;&#20064;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65292;&#24182;&#22312;StarCraft II&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25104;&#21151;&#30340;&#31163;&#31574;&#30053;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#20316;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#23547;&#25214;&#20998;&#35299;&#30340;&#20540;&#20989;&#25968;&#65292;&#23548;&#33268;&#22797;&#26434;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;LAN&#31639;&#27861;&#24314;&#31435;&#22312;&#29420;&#31435;Q&#23398;&#20064;&#32773;&#30340;&#32467;&#26500;&#22522;&#30784;&#19978;&#65292;&#37319;&#29992;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#20915;&#26550;&#26500;&#36890;&#36807;&#20010;&#20307;&#20248;&#21183;&#20989;&#25968;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#20998;&#25955;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#12290;&#36890;&#36807;&#19968;&#20010;&#20013;&#24515;&#21270;&#30340;&#35780;&#35770;&#23478;&#31283;&#23450;&#23398;&#20064;&#65292;&#35780;&#35770;&#23478;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20943;&#23569;&#20010;&#20307;&#20248;&#21183;&#30340;&#31227;&#21160;&#30446;&#26631;&#38382;&#39064;&#12290;&#35780;&#35770;&#23478;&#30340;&#32593;&#32476;&#22823;&#23567;&#19982;&#26234;&#33021;&#20307;&#25968;&#37327;&#26080;&#20851;&#65292;&#22312;&#23398;&#20064;&#21518;&#34987;&#20002;&#24323;&#12290;&#22312;StarCraft II&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;LAN&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#20110;&#26234;&#33021;&#20307;&#25968;&#37327;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#65292;&#20026;MARL&#30740;&#31350;&#24320;&#36767;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#20132;&#36890;&#27969;&#37327;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#65292;&#26222;&#36941;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#27969;&#37327;&#30417;&#27979;&#21644;&#35843;&#25511;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2109.14251</link><description>&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#24341;&#23548;&#30340;&#22478;&#24066;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#20132;&#36890;&#27969;&#37327;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#65292;&#26222;&#36941;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#27969;&#37327;&#30417;&#27979;&#21644;&#35843;&#25511;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#25512;&#26029;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#37327;&#26159;&#19968;&#20010;&#26032;&#20852;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#26497;&#22823;&#22320;&#20943;&#23569;&#25152;&#38656;&#20132;&#36890;&#30417;&#27979;&#20256;&#24863;&#22120;&#30340;&#25968;&#37327;&#20197;&#33410;&#30465;&#25104;&#26412;&#12290;&#26412;&#25991;&#21457;&#29616;&#20132;&#36890;&#27969;&#37327;&#19982;&#36947;&#36335;&#32593;&#32476;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#23436;&#20840;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#65292;&#25110;&#32773;&#20165;&#23558;&#20854;&#35270;&#20026;&#22806;&#37096;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#32593;&#24863;&#30693;&#20132;&#36890;&#27969;&#37327;&#25918;&#22823;&#22120;&#65288;RATFM&#65289;&#65292;&#23427;&#26126;&#30830;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26041;&#21521;1D&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#36947;&#36335;&#32593;&#32476;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#21644;&#31895;&#31890;&#24230;&#27969;&#37327;&#29305;&#24449;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#35268;&#33539;&#21270;&#36947;&#36335;&#30456;&#20851;&#20132;&#36890;&#27969;&#30340;&#30701;&#36317;&#31163;&#31354;&#38388;&#20998;&#24067;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#20316;&#20026;&#26597;&#35810;&#26469;&#25429;&#33719;&#38271;&#36317;&#31163;&#36335;&#27573;&#20132;&#36890;&#27969;&#37327;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate inference of fine-grained traffic flow from coarse-grained one is an emerging yet crucial problem, which can help greatly reduce the number of the required traffic monitoring sensors for cost savings. In this work, we notice that traffic flow has a high correlation with road network, which was either completely ignored or simply treated as an external factor in previous works.To facilitate this problem, we propose a novel Road-Aware Traffic Flow Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks to fully learn the road-aware spatial distribution of fine-grained traffic flow. Specifically, a multi-directional 1D convolutional layer is first introduced to extract the semantic feature of the road network. Subsequently, we incorporate the road network feature and coarse-grained flow feature to regularize the short-range spatial distribution modeling of road-relative traffic flow. Furthermore, we take the road network feature as a query to capture the l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#21442;&#25968;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#38750;&#31283;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#37325;&#32622;&#36172;&#21338;&#26426;(ADR-bandit)&#31639;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#35777;&#26126;&#20102;ADR-bandit&#22312;&#20840;&#23616;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#31283;&#23450;&#29615;&#22659;&#21644;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#22343;&#20855;&#26377;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2107.11419</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#38750;&#31283;&#24577;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits. (arXiv:2107.11419v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#21442;&#25968;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#38750;&#31283;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#37325;&#32622;&#36172;&#21338;&#26426;(ADR-bandit)&#31639;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#35777;&#26126;&#20102;ADR-bandit&#22312;&#20840;&#23616;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#31283;&#23450;&#29615;&#22659;&#21644;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#22343;&#20855;&#26377;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#27169;&#22411;&#21442;&#25968;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#38750;&#31283;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#37325;&#32622;&#36172;&#21338;&#26426;(ADR-bandit)&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#25968;&#25454;&#27969;&#25991;&#29486;&#20013;&#30340;&#33258;&#36866;&#24212;&#31383;&#21475;&#25216;&#26415;&#30340;&#36172;&#21338;&#26426;&#31639;&#27861;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#36866;&#24212;&#31383;&#21475;&#25216;&#26415;&#20135;&#29983;&#30340;&#20272;&#35745;&#22120;&#36136;&#37327;&#30340;&#26032;&#20445;&#35777;&#65292;&#36825;&#23545;&#20110;&#29420;&#31435;&#30340;&#30740;&#31350;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20004;&#31181;&#20856;&#22411;&#29615;&#22659;&#19979;&#23545;ADR-bandit&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65306;&#19968;&#31181;&#26159;&#31361;&#21464;&#29615;&#22659;&#65292;&#20854;&#20013;&#21464;&#21270;&#26159;&#30636;&#26102;&#21457;&#29983;&#30340;&#65307;&#21478;&#19968;&#31181;&#26159;&#28176;&#21464;&#29615;&#22659;&#65292;&#20854;&#20013;&#21464;&#21270;&#26159;&#36880;&#28176;&#21457;&#29983;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#31361;&#21464;&#25110;&#28176;&#21464;&#30340;&#21464;&#21270;&#20197;&#25105;&#20204;&#31216;&#20026;&#20840;&#23616;&#21464;&#21270;&#30340;&#21327;&#21516;&#26041;&#24335;&#21457;&#29983;&#26102;&#65292;ADR-bandit&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20551;&#35774;&#36825;&#31181;&#20840;&#23616;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21046;&#25506;&#32034;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#19982;&#29616;&#26377;&#30340;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#31639;&#27861;&#19981;&#21516;&#65292;ADR-bandit&#22312;&#31283;&#23450;&#29615;&#22659;&#21644;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#22343;&#20855;&#26377;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider nonstationary multi-armed bandit problems where the model parameters of the arms change over time. We introduce the adaptive resetting bandit (ADR-bandit), a bandit algorithm class that leverages adaptive windowing techniques from literature on data streams. We first provide new guarantees on the quality of estimators resulting from adaptive windowing techniques, which are of independent interest. Furthermore, we conduct a finite-time analysis of ADR-bandit in two typical environments: an abrupt environment where changes occur instantaneously and a gradual environment where changes occur progressively. We demonstrate that ADR-bandit has nearly optimal performance when abrupt or gradual changes occur in a coordinated manner that we call global changes. We demonstrate that forced exploration is unnecessary when we assume such global changes. Unlike the existing nonstationary bandit algorithms, ADR-bandit has optimal performance in stationary environments as well as nonstation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22996;&#25176;&#20154;&#23545;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#37096;&#20998;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#25171;&#20998;&#35268;&#21017;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#26469;&#26368;&#22823;&#21270;&#22312;&#20195;&#29702;&#20154;&#20449;&#21495;&#20998;&#24067;&#30340;&#38598;&#21512;&#20013;&#26368;&#22351;&#24773;&#20917;&#19979;&#22238;&#25253;&#30340;&#22686;&#21152;&#12290;&#23545;&#20110;&#26377;&#38480;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65307;&#23545;&#20110;&#26080;&#38480;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2107.07420</link><description>&lt;p&gt;
&#37096;&#20998;&#30693;&#35782;&#19979;&#30340;&#26368;&#20248;&#25171;&#20998;&#35268;&#21017;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Scoring Rule Design under Partial Knowledge. (arXiv:2107.07420v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22996;&#25176;&#20154;&#23545;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#37096;&#20998;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#25171;&#20998;&#35268;&#21017;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#26469;&#26368;&#22823;&#21270;&#22312;&#20195;&#29702;&#20154;&#20449;&#21495;&#20998;&#24067;&#30340;&#38598;&#21512;&#20013;&#26368;&#22351;&#24773;&#20917;&#19979;&#22238;&#25253;&#30340;&#22686;&#21152;&#12290;&#23545;&#20110;&#26377;&#38480;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65307;&#23545;&#20110;&#26080;&#38480;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#22996;&#25176;&#20154;&#23545;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#37096;&#20998;&#20102;&#35299;&#26102;&#65292;&#26368;&#20248;&#36866;&#24403;&#25171;&#20998;&#35268;&#21017;&#30340;&#35774;&#35745;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#22996;&#25176;&#20154;&#23436;&#20840;&#20102;&#35299;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#21487;&#20197;&#30830;&#23450;&#22686;&#21152;&#20195;&#29702;&#20154;&#22238;&#25253;&#30340;&#26368;&#22823;&#36866;&#24403;&#25171;&#20998;&#35268;&#21017;&#65292;&#24403;&#20195;&#29702;&#20154;&#36873;&#25321;&#35775;&#38382;&#26114;&#36149;&#20449;&#21495;&#20197;&#23436;&#21892;&#20854;&#20808;&#39564;&#39044;&#27979;&#30340;&#21518;&#39564;&#20449;&#24565;&#26102;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#22996;&#25176;&#20154;&#21482;&#30693;&#36947;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#23646;&#20110;&#19968;&#32452;&#20998;&#24067;&#20013;&#30340;&#26576;&#20010;&#12290;&#25105;&#20204;&#23558;&#25171;&#20998;&#35268;&#21017;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#20998;&#24067;&#38598;&#21512;&#20013;&#26368;&#22351;&#24773;&#20917;&#19979;&#22238;&#25253;&#30340;&#22686;&#21152;&#12290;&#24403;&#20998;&#24067;&#38598;&#21512;&#26377;&#38480;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#26368;&#20248;&#25171;&#20998;&#35268;&#21017;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26080;&#38480;&#38598;&#21512;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#25171;&#20998;&#35268;&#21017;&#65292;&#22914;&#20108;&#27425;&#26041;&#25171;&#20998;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the design of optimal proper scoring rules when the principal has partial knowledge of an agent's signal distribution. Recent work characterizes the proper scoring rules that maximize the increase of an agent's payoff when the agent chooses to access a costly signal to refine a posterior belief from her prior prediction, under the assumption that the agent's signal distribution is fully known to the principal. In our setting, the principal only knows about a set of distributions where the agent's signal distribution belongs. We formulate the scoring rule design problem as a max-min optimization that maximizes the worst-case increase in payoff across the set of distributions.  We propose an efficient algorithm to compute an optimal scoring rule when the set of distributions is finite, and devise a fully polynomial-time approximation scheme that accommodates various infinite sets of distributions. We further remark that widely used scoring rules, such as the quadratic 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20316;&#20026;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#31867;&#20284;&#20110;ResNet&#30340;&#26550;&#26500;&#21644;&#36866;&#24212;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;Transformer&#26550;&#26500;&#12290;&#36825;&#20123;&#22522;&#20934;&#27169;&#22411;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.11959</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20316;&#20026;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#31867;&#20284;&#20110;ResNet&#30340;&#26550;&#26500;&#21644;&#36866;&#24212;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;Transformer&#26550;&#26500;&#12290;&#36825;&#20123;&#22522;&#20934;&#27169;&#22411;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#29486;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#24120;&#27809;&#26377;&#36827;&#34892;&#36866;&#24403;&#30340;&#27604;&#36739;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24120;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#21644;&#23454;&#39564;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#32780;&#35328;&#65292;&#20160;&#20040;&#26679;&#30340;&#27169;&#22411;&#24615;&#33021;&#26368;&#22909;&#26159;&#19981;&#28165;&#26970;&#30340;&#12290;&#21478;&#22806;&#65292;&#35813;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#21363;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#31454;&#20105;&#21147;&#24615;&#33021;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20027;&#35201;DL&#26550;&#26500;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#30830;&#23450;&#20004;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#28145;&#24230;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#34920;&#26684;DL&#30340;&#22522;&#20934;&#12290;&#31532;&#19968;&#31181;&#26159;&#31867;&#20284;&#20110;ResNet&#30340;&#26550;&#26500;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#26159;&#24120;&#35265;&#30340;&#20808;&#21069;&#24037;&#20316;&#20013;&#24120;&#32570;&#22833;&#30340;&#24378;&#22522;&#20934;&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#25105;&#20204;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;Transformer&#26550;&#26500;&#30340;&#31616;&#21333;&#36866;&#24212;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.  In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#36827;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2010.01992</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#25913;&#36827;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Learning through Multi-task Representation Learning Theory. (arXiv:2010.01992v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.01992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#36827;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22810;&#20219;&#21153;&#34920;&#31034;&#65288;MTR&#65289;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#30446;&#26631;&#26159;&#21033;&#29992;&#28304;&#20219;&#21153;&#26469;&#23398;&#20064;&#19968;&#20010;&#34920;&#31034;&#65292;&#20943;&#23569;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;MTR&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35813;&#26694;&#26550;&#20869;&#23545;&#27969;&#34892;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#26799;&#24230;&#20248;&#21270;&#21644;&#24230;&#37327;&#20248;&#21270;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#26681;&#26412;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#26469;&#35299;&#37322;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#24471;&#21040;&#30340;&#35265;&#35299;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35889;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23558;MTR&#29702;&#35770;&#30340;&#26368;&#26032;&#23398;&#20064;&#30028;&#38480;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#39318;&#27425;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the framework of multi-task representation (MTR) learning where the goal is to use source tasks to learn a representation that reduces the sample complexity of solving a target task. We start by reviewing recent advances in MTR theory and show that they can provide novel insights for popular meta-learning algorithms when analyzed within this framework. In particular, we highlight a fundamental difference between gradient-based and metric-based algorithms in practice and put forward a theoretical analysis to explain it. Finally, we use the derived insights to improve the performance of meta-learning methods via a new spectral-based regularization term and confirm its efficiency through experimental studies on few-shot classification benchmarks. To the best of our knowledge, this is the first contribution that puts the most recent learning bounds of MTR theory into practice for the task of few-shot classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32593;&#32476;&#20107;&#20214;&#27969;&#20013;&#20272;&#35745;&#28508;&#22312;&#30340;&#31038;&#21306;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#28857;&#36807;&#31243;&#28508;&#22312;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#21160;&#24577;&#20107;&#20214;&#21040;&#36798;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#24182;&#26356;&#26032;&#31038;&#21306;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2009.01742</link><description>&lt;p&gt;
&#22312;&#32447;&#20272;&#35745;&#21644;&#32593;&#32476;&#20107;&#20214;&#27969;&#30340;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#32593;&#32476;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Online Estimation and Community Detection of Network Point Processes for Event Streams. (arXiv:2009.01742v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.01742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32593;&#32476;&#20107;&#20214;&#27969;&#20013;&#20272;&#35745;&#28508;&#22312;&#30340;&#31038;&#21306;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#28857;&#36807;&#31243;&#28508;&#22312;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#21160;&#24577;&#20107;&#20214;&#21040;&#36798;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#24182;&#26356;&#26032;&#31038;&#21306;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#24314;&#27169;&#30340;&#19968;&#20010;&#20849;&#21516;&#30446;&#26631;&#26159;&#25581;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#28508;&#22312;&#31038;&#21306;&#32467;&#26500;&#12290;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32593;&#32476;&#65292;&#30495;&#23454;&#30340;&#36830;&#25509;&#37117;&#30001;&#20316;&#20026;&#27969;&#21040;&#36798;&#30340;&#20107;&#20214;&#32452;&#25104;&#65292;&#28982;&#21518;&#23558;&#20854;&#32858;&#21512;&#24418;&#25104;&#36793;&#32536;&#65292;&#24573;&#30053;&#20102;&#21160;&#24577;&#30340;&#26102;&#38388;&#32452;&#20214;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#20132;&#20114;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#27861;&#26159;&#20351;&#29992;&#28857;&#36807;&#31243;&#20316;&#20026;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#22522;&#30784;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#12290;&#35745;&#31639;&#22797;&#26434;&#24615;&#24433;&#21709;&#30528;&#36825;&#31181;&#26041;&#27861;&#22312;&#22823;&#22411;&#31232;&#30095;&#32593;&#32476;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#22312;&#32447;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32593;&#32476;&#19978;&#22522;&#20110;&#21160;&#24577;&#20107;&#20214;&#21040;&#36798;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#28857;&#36807;&#31243;&#28508;&#22312;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#36866;&#29992;&#20110;&#25429;&#25417;&#31038;&#21306;&#32467;&#26500;&#30340;&#32593;&#32476;&#27169;&#22411;&#30340;&#27492;&#36807;&#31243;&#12290;&#24403;&#22312;&#32593;&#32476;&#19978;&#35266;&#23519;&#21040;&#26032;&#20107;&#20214;&#26102;&#65292;&#21487;&#20197;&#23398;&#20064;&#35813;&#32467;&#26500;&#65292;&#24182;&#26356;&#26032;&#25512;&#26029;&#30340;&#31038;&#21306;&#20998;&#37197;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common goal in network modeling is to uncover the latent community structure present among nodes. For many real-world networks, the true connections consist of events arriving as streams, which are then aggregated to form edges, ignoring the dynamic temporal component. A natural way to take account of these temporal dynamics of interactions is to use point processes as the foundation of network models for community detection. Computational complexity hampers the scalability of such approaches to large sparse networks. To circumvent this challenge, we propose a fast online variational inference algorithm for estimating the latent structure underlying dynamic event arrivals on a network, using continuous-time point process latent network models. We describe this procedure for networks models capturing community structure. This structure can be learned as new events are observed on the network, updating the inferred community assignments. We investigate the theoretical properties of suc
&lt;/p&gt;</description></item></channel></rss>