<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#28176;&#36827;&#32593;&#26684;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#23398;&#20064;&#29983;&#25104;&#31354;&#38388;&#21644;&#36880;&#28176;&#20256;&#36755;&#39069;&#22806;&#30340;&#27531;&#20313;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#20114;&#32852;&#32593;&#20256;&#36755;&#22823;&#37327;&#30340;3D&#20960;&#20309;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.05741</link><description>&lt;p&gt;
&#31070;&#32463;&#28176;&#36827;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Neural Progressive Meshes. (arXiv:2308.05741v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#28176;&#36827;&#32593;&#26684;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#23398;&#20064;&#29983;&#25104;&#31354;&#38388;&#21644;&#36880;&#28176;&#20256;&#36755;&#39069;&#22806;&#30340;&#27531;&#20313;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#20114;&#32852;&#32593;&#20256;&#36755;&#22823;&#37327;&#30340;3D&#20960;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21487;&#20197;&#22312;&#25163;&#25345;&#35774;&#22791;&#19978;&#28040;&#36153;&#30340;3D&#20869;&#23481;&#30340;&#28608;&#22686;&#38656;&#35201;&#26377;&#25928;&#30340;&#24037;&#20855;&#26469;&#36890;&#36807;&#20114;&#32852;&#32593;&#20256;&#36755;&#22823;&#37327;&#30340;&#20960;&#20309;&#25968;&#25454;&#65292;&#20363;&#22914;3D&#32593;&#26684;&#12290;&#35814;&#32454;&#30340;&#39640;&#20998;&#36776;&#29575;&#36164;&#28304;&#23545;&#20110;&#23384;&#20648;&#21644;&#20256;&#36755;&#24102;&#23485;&#37117;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36890;&#24120;&#20351;&#29992;&#23618;&#27425;&#32454;&#33410;&#25216;&#26415;&#22312;&#36866;&#24403;&#30340;&#24102;&#23485;&#39044;&#31639;&#19979;&#20256;&#36755;&#36164;&#28304;&#12290;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#26469;&#35828;&#65292;&#20197;&#28176;&#36827;&#30340;&#26041;&#24335;&#20256;&#36755;&#25968;&#25454;&#65292;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21152;&#20837;&#65292;&#25913;&#36827;&#20960;&#20309;&#30340;&#36136;&#37327;&#23588;&#20854;&#21487;&#21462;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;3D&#32593;&#26684;&#30340;&#20960;&#20309;&#32454;&#33410;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#24418;&#29366;&#20043;&#38388;&#37117;&#21576;&#29616;&#20986;&#30456;&#20284;&#30340;&#23616;&#37096;&#27169;&#24335;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20849;&#20139;&#30340;&#23398;&#20064;&#29983;&#25104;&#31354;&#38388;&#26469;&#26377;&#25928;&#34920;&#31034;&#36825;&#20123;&#32454;&#33410;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#32454;&#20998;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#25552;&#21069;&#35757;&#32451;&#22823;&#37327;&#34920;&#38754;&#65292;&#22312;&#27492;&#31354;&#38388;&#20013;&#23398;&#20064;&#36825;&#20123;&#32454;&#33410;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#22312;&#20013;&#38388;&#32454;&#20998;&#32423;&#21035;&#20043;&#38388;&#21487;&#20197;&#36880;&#28176;&#20256;&#36755;&#39069;&#22806;&#30340;&#27531;&#20313;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#28176;&#36827;&#24335;&#20256;&#36755;&#21644;&#25913;&#21892;&#20960;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent proliferation of 3D content that can be consumed on hand-held devices necessitates efficient tools for transmitting large geometric data, e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a challenge to storage as well as transmission bandwidth, and level-of-detail techniques are often used to transmit an asset using an appropriate bandwidth budget. It is especially desirable for these methods to transmit data progressively, improving the quality of the geometry with more data. Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns even across different shapes, and thus can be effectively represented with a shared learned generative space. We learn this space using a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces. We further observe that additional residual features can be transmitted progressively between intermediate levels of subdivision that enable the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ZeroGrads&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#21487;&#24494;&#22270;&#24418;&#30340;&#23616;&#37096;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#26080;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20027;&#21160;&#24179;&#28369;&#21644;&#23616;&#37096;&#24615;&#32422;&#26463;&#20248;&#21270;&#26367;&#20195;&#25439;&#22833;&#30340;&#25311;&#21512;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05739</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#32473;&#20219;&#20309;&#38646;&#26799;&#24230;&#65306;&#23398;&#20064;&#38750;&#21487;&#24494;&#22270;&#24418;&#30340;&#23616;&#37096;&#26367;&#20195;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics. (arXiv:2308.05739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ZeroGrads&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#21487;&#24494;&#22270;&#24418;&#30340;&#23616;&#37096;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#26080;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20027;&#21160;&#24179;&#28369;&#21644;&#23616;&#37096;&#24615;&#32422;&#26463;&#20248;&#21270;&#26367;&#20195;&#25439;&#22833;&#30340;&#25311;&#21512;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22312;&#22270;&#24418;&#39046;&#22495;&#21464;&#24471;&#26222;&#36941;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#26080;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26410;&#23450;&#20041;&#25110;&#38646;&#26799;&#24230;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#26367;&#25442;&#25439;&#22833;&#20989;&#25968;&#26469;&#20351;&#29992;&#31867;&#20284;&#26497;&#23567;&#20540;&#20294;&#21487;&#24494;&#30340;&#8220;&#26367;&#20195;&#25439;&#22833;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;ZeroGrads&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#30340;&#31070;&#32463;&#36924;&#36817;&#65292;&#21363;&#26367;&#20195;&#25439;&#22833;&#65292;&#26469;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#20219;&#24847;&#40657;&#30418;&#22270;&#24418;&#27969;&#31243;&#36827;&#34892;&#24494;&#20998;&#12290;&#25105;&#20204;&#35757;&#32451;&#26367;&#20195;&#25439;&#22833;&#22312;&#30446;&#26631;&#20989;&#25968;&#30340;&#20027;&#21160;&#24179;&#28369;&#29256;&#26412;&#19978;&#65292;&#24182;&#40723;&#21169;&#23616;&#37096;&#24615;&#65292;&#20351;&#26367;&#20195;&#25439;&#22833;&#30340;&#23481;&#37327;&#38598;&#20013;&#22312;&#24403;&#21069;&#35757;&#32451;&#38454;&#27573;&#30340;&#20851;&#38190;&#20869;&#23481;&#19978;&#12290;&#25311;&#21512;&#26159;&#22312;&#32447;&#25191;&#34892;&#30340;&#65292;&#19982;&#21442;&#25968;&#20248;&#21270;&#21516;&#26102;&#36827;&#34892;&#65292;&#33258;&#30417;&#30563;&#36827;&#34892;&#65292;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;&#25968;&#25454;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#30001;&#20110;&#30446;&#26631;&#30340;&#37319;&#26679;&#26159;&#26114;&#36149;&#30340;&#65288;&#38656;&#35201;&#23436;&#25972;&#30340;&#28210;&#26579;&#25110;&#27169;&#25311;&#36816;&#34892;&#65289;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#21487;&#34892;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a "surrogate" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, the surrogate, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#12289;&#36861;&#36394;&#21644;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#65292;&#19981;&#21463;&#35757;&#32451;&#26102;&#27010;&#24565;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22810;&#27169;&#24577;&#26597;&#35810;&#36827;&#34892;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#25551;&#36848;&#31526;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#36319;&#36394;&#29289;&#20307;&#65292;&#21516;&#26102;&#32771;&#34385;&#36974;&#25377;&#21644;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.05737</link><description>&lt;p&gt;
Follow Anything: &#23454;&#26102;&#24320;&#25918;&#38598;&#26816;&#27979;&#12289;&#36861;&#36394;&#21644;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Follow Anything: Open-set detection, tracking, and following in real-time. (arXiv:2308.05737v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#12289;&#36861;&#36394;&#21644;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#65292;&#19981;&#21463;&#35757;&#32451;&#26102;&#27010;&#24565;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22810;&#27169;&#24577;&#26597;&#35810;&#36827;&#34892;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#25551;&#36848;&#31526;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#36319;&#36394;&#29289;&#20307;&#65292;&#21516;&#26102;&#32771;&#34385;&#36974;&#25377;&#21644;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#33258;&#21160;&#21270;&#12289;&#29289;&#27969;&#21644;&#20179;&#20648;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#23433;&#20840;&#31561;&#22810;&#31181;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#36861;&#36394;&#21644;&#36319;&#38543;&#24863;&#20852;&#36259;&#30340;&#29289;&#20307;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#12289;&#36861;&#36394;&#21644;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#8220;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#8221;&#65288;FAn&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#24320;&#25918;&#35789;&#27719;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411; - &#19981;&#21463;&#35757;&#32451;&#26102;&#30340;&#27010;&#24565;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#28857;&#20987;&#26597;&#35810;&#26469;&#24212;&#29992;&#20110;&#25512;&#26029;&#26102;&#30340;&#26032;&#31867;&#21035;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22522;&#30784;&#27169;&#22411;&#65289;&#30340;&#20016;&#23500;&#35270;&#35273;&#25551;&#36848;&#31526;&#65292;FAn&#21487;&#20197;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#26597;&#35810;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#28857;&#20987;&#65289;&#19982;&#36755;&#20837;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#21305;&#37197;&#26469;&#26816;&#27979;&#21644;&#20998;&#21106;&#29289;&#20307;&#12290;&#36825;&#20123;&#26816;&#27979;&#21644;&#20998;&#21106;&#30340;&#29289;&#20307;&#22312;&#22270;&#20687;&#24103;&#20043;&#38388;&#36827;&#34892;&#36319;&#36394;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36974;&#25377;&#21644;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#19978;&#65288;&#24494;&#22411;&#39134;&#34892;&#22120;&#65289;&#28436;&#31034;&#20102;FAn&#65292;&#24182;&#25253;&#21578;&#20102;&#20854;&#26080;&#32541;&#36319;&#38543;&#24863;&#20852;&#36259;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed ``follow anything'' (FAn), is an open-vocabulary and multimodal model -- it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of inter
&lt;/p&gt;</description></item><item><title>PDE-Refiner &#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05732</link><description>&lt;p&gt;
PDE-Refiner: &#21033;&#29992;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#23454;&#29616;&#20934;&#30830;&#30340;&#38271;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers. (arXiv:2308.05732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05732
&lt;/p&gt;
&lt;p&gt;
PDE-Refiner &#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20256;&#32479;&#35299;&#27861;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#36825;&#20123;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#23454;&#29992;&#20215;&#20540;&#20381;&#36182;&#20110;&#23427;&#20204;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#24403;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#24120;&#35265;&#30340;&#26102;&#38388;&#23637;&#24320;&#31574;&#30053;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#21457;&#29616;&#24573;&#30053;&#38750;&#20027;&#23548;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#65288;&#36890;&#24120;&#19982;PDE&#35299;&#20013;&#30340;&#39640;&#39057;&#29575;&#30456;&#20851;&#65289;&#26159;&#38480;&#21046;&#31283;&#23450;&#12289;&#20934;&#30830;&#23637;&#24320;&#24615;&#33021;&#30340;&#20027;&#35201;&#38519;&#38449;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#23519;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24341;&#20837;&#20102;PDE-Refiner&#65307;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#36890;&#36807;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#23454;&#29616;&#23545;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#26356;&#20934;&#30830;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;PDE-Refiner&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05731</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05731
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#20010;&#20154;&#12289;&#20844;&#20849;&#21644;&#36135;&#36816;&#20132;&#36890;&#30340;&#26041;&#24335;&#12290;&#38500;&#20102;&#24863;&#30693;&#29615;&#22659;&#30340;&#24040;&#22823;&#25361;&#25112;&#22806;&#65292;&#21363;&#20934;&#30830;&#22320;&#20351;&#29992;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#24863;&#30693;&#29615;&#22659;&#65292;&#33258;&#21160;&#39550;&#39542;&#36824;&#21253;&#25324;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#36712;&#36857;&#12290;&#20026;&#20102;&#20419;&#36827;&#23433;&#20840;&#21644;&#36827;&#27493;&#65292;&#35768;&#22810;&#24037;&#20316;&#20381;&#36182;&#20110;&#27169;&#22359;&#21270;&#30340;&#20132;&#36890;&#26410;&#26469;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#27169;&#22359;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#24120;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#20316;&#20026;&#39034;&#24207;&#30340;&#29420;&#31435;&#20219;&#21153;&#22788;&#29702;&#12290;&#34429;&#28982;&#36825;&#32771;&#34385;&#20102;&#21608;&#22260;&#20132;&#36890;&#23545;&#33258;&#36710;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#26410;&#33021;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#23545;&#33258;&#36710;&#34892;&#20026;&#30340;&#21453;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#25972;&#21512;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26159;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#33298;&#36866;&#39550;&#39542;&#25152;&#24517;&#38656;&#30340;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#20294;&#23545;&#19981;&#21516;&#21407;&#29702;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Expresso&#25968;&#25454;&#38598;&#65292;&#23545;&#31163;&#25955;&#34920;&#36798;&#24615;&#35821;&#38899;&#20877;&#21512;&#25104;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#25991;&#26412;&#30340;&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#38590;&#20197;&#36716;&#24405;&#30340;&#35821;&#38899;&#34920;&#36798;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2308.05725</link><description>&lt;p&gt;
EXPRESSO: &#19968;&#39033;&#23545;&#31163;&#25955;&#34920;&#36798;&#24615;&#35821;&#38899;&#20877;&#21512;&#25104;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis. (arXiv:2308.05725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Expresso&#25968;&#25454;&#38598;&#65292;&#23545;&#31163;&#25955;&#34920;&#36798;&#24615;&#35821;&#38899;&#20877;&#21512;&#25104;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#25991;&#26412;&#30340;&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#38590;&#20197;&#36716;&#24405;&#30340;&#35821;&#38899;&#34920;&#36798;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#65292;&#32780;&#26159;&#22522;&#20110;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#23398;&#20064;&#21040;&#30340;&#20302;&#27604;&#29305;&#29575;&#31163;&#25955;&#21333;&#20803;&#65292;&#21487;&#20197;&#37325;&#26032;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#38590;&#20197;&#36716;&#24405;&#30340;&#35821;&#38899;&#34920;&#36798;&#26041;&#38754;&#65288;&#38901;&#24459;&#12289;&#22768;&#38899;&#39118;&#26684;&#12289;&#38750;&#35821;&#35328;&#35821;&#38899;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#35821;&#38899;&#21512;&#25104;&#25968;&#25454;&#38598;&#37117;&#26159;&#26391;&#35835;&#30340;&#65292;&#22240;&#27492;&#23545; spontaneity &#21644; expressivity &#30340;&#35201;&#27714;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102; Expresso&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#26080;&#25991;&#26412;&#35821;&#38899;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26391;&#35835;&#35821;&#38899;&#21644;26&#31181;&#33258;&#21457;&#34920;&#36798;&#39118;&#26684;&#30340;&#21363;&#20852;&#23545;&#35805;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#34920;&#36798;&#24615;&#37325;&#26032;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#26469;&#23637;&#31034;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#21644;&#28508;&#21147;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#38656;&#35201;&#20197;&#20302;&#27604;&#29305;&#29575;&#21333;&#20803;&#23545;&#36755;&#20837;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#30446;&#26631;&#38899;&#33394;&#20013;&#37325;&#26032;&#21512;&#25104;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23481;&#21644;&#39118;&#26684;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#23545;&#19981;&#21516;&#30340;&#33258;&#30417;&#30563;&#31163;&#25955;&#21512;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that it is possible to resynthesize high-quality speech based, not on text, but on low bitrate discrete units that have been learned in a self-supervised fashion and can therefore capture expressive aspects of speech that are hard to transcribe (prosody, voice styles, non-verbal vocalization). The adoption of these methods is still limited by the fact that most speech synthesis datasets are read, severely limiting spontaneity and expressivity. Here, we introduce Expresso, a high-quality expressive speech dataset for textless speech synthesis that includes both read speech and improvised dialogues rendered in 26 spontaneous expressive styles. We illustrate the challenges and potentials of this dataset with an expressive resynthesis benchmark where the task is to encode the input in low-bitrate units and resynthesize it in a target voice while preserving content and style. We evaluate resynthesis quality with automatic metrics for different self-supervised discrete 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21160;&#24577;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;&#21069;&#39304;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#20013;&#65292;&#22797;&#26434;&#30340;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#27604;ReLU&#28608;&#27963;&#20989;&#25968;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.05724</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;&#21069;&#39304;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions. (arXiv:2308.05724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21160;&#24577;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;&#21069;&#39304;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#20013;&#65292;&#22797;&#26434;&#30340;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#27604;ReLU&#28608;&#27963;&#20989;&#25968;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#22312;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20154;&#20204;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#28145;&#30340;&#32593;&#32476;&#23618;&#27425;&#32467;&#26500;&#65292;&#22914;&#20855;&#26377;&#32422;152&#23618;&#30340;ResNet&#32467;&#26500;&#12290;&#27973;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20013;&#19968;&#20123;&#29616;&#35937;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#37322;&#12290;&#32593;&#32476;&#20013;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#20026;&#32593;&#32476;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#12290;ReLU&#26159;&#26368;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#38544;&#34255;&#23618;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#20998;&#27573;&#32447;&#24615;&#65288;PWL&#65289;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;PWL&#28608;&#27963;&#20989;&#25968;&#22312;&#25105;&#20204;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#20013;&#27604;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22312;PyTorch&#20013;&#27604;&#36739;&#27973;&#23618;&#21644;&#28145;&#24230;CNN&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#19968;&#27493;&#35777;&#23454;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning training training algorithms are a huge success in recent years in many fields including speech, text,image video etc. Deeper and deeper layers are proposed with huge success with resnet structures having around 152 layers. Shallow convolution neural networks(CNN's) are still an active research, where some phenomena are still unexplained. Activation functions used in the network are of utmost importance, as they provide non linearity to the networks. Relu's are the most commonly used activation function.We show a complex piece-wise linear(PWL) activation in the hidden layer. We show that these PWL activations work much better than relu activations in our networks for convolution neural networks and multilayer perceptrons. Result comparison in PyTorch for shallow and deep CNNs are given to further strengthen our case.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#26262;&#36890;&#31354;&#35843;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#22870;&#21169;&#35774;&#32622;&#30340;&#23454;&#38469;&#32771;&#34385;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#33021;&#28304;&#39640;&#25928;&#21644;&#25104;&#26412;&#26377;&#25928;&#30340;&#25805;&#20316;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05711</link><description>&lt;p&gt;
&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#26262;&#36890;&#31354;&#35843;&#25511;&#21046;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control. (arXiv:2308.05711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05711
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#26262;&#36890;&#31354;&#35843;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#22870;&#21169;&#35774;&#32622;&#30340;&#23454;&#38469;&#32771;&#34385;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#33021;&#28304;&#39640;&#25928;&#21644;&#25104;&#26412;&#26377;&#25928;&#30340;&#25805;&#20316;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#20248;&#21270;&#26262;&#36890;&#31354;&#35843;&#25511;&#21046;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;RL&#25552;&#20379;&#20102;&#25913;&#36827;&#31995;&#32479;&#24615;&#33021;&#12289;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#21644;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;Q-Learning&#21644;Deep-Q-Networks&#65289;&#22312;&#22810;&#20010;&#26262;&#36890;&#31354;&#35843;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#22870;&#21169;&#35843;&#33410;&#30340;&#23454;&#38469;&#32771;&#34385;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#37197;&#32622;HVAC&#31995;&#32479;&#20013;&#30340;RL&#20195;&#29702;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#65292;&#20419;&#36827;&#20102;&#33021;&#28304;&#39640;&#25928;&#21644;&#25104;&#26412;&#26377;&#25928;&#30340;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a promising approach for optimizing HVAC control. RL offers a framework for improving system performance, reducing energy consumption, and enhancing cost efficiency. We benchmark two popular classical and deep RL methods (Q-Learning and Deep-Q-Networks) across multiple HVAC environments and explore the practical consideration of model hyper-parameter selection and reward tuning. The findings provide insight for configuring RL agents in HVAC systems, promoting energy-efficient and cost-effective operation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;CRL&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05707</link><description>&lt;p&gt;
&#38544;&#24418;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Shadow Datasets, New challenging datasets for Causal Representation Learning. (arXiv:2308.05707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;CRL&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21457;&#29616;&#35821;&#20041;&#22240;&#32032;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#35805;&#39064;&#12290;&#22823;&#22810;&#25968;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#26041;&#27861;&#37117;&#26159;&#23436;&#20840;&#30417;&#30563;&#30340;&#65292;&#30001;&#20110;&#26631;&#35760;&#25104;&#26412;&#39640;&#26114;&#32780;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#24369;&#30417;&#30563;&#30340;CRL&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;CRL&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;&#22235;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65306;Pendulum&#12289;Flow&#12289;CelebA&#65288;BEARD&#65289;&#21644;CelebA&#65288;SMILE&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CRL&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#20855;&#26377;&#23569;&#37327;&#29983;&#25104;&#22240;&#32032;&#30340;&#31616;&#21333;&#22270;&#24418;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26356;&#22810;&#31181;&#31867;&#30340;&#29983;&#25104;&#22240;&#32032;&#21644;&#26356;&#22797;&#26434;&#30340;&#22240;&#26524;&#22270;&#12290;&#27492;&#22806;&#65292;&#22312;&#24403;&#21069;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;CelebA&#65288;BEARD&#65289;&#21644;CelebA&#65288;SMILE&#65289;&#20013;&#65292;&#26368;&#21021;&#25552;&#20986;&#30340;&#22240;&#26524;&#22270;&#19982;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#23427;&#20204;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27969;&#24418;&#21644;&#23450;&#20041;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#26469;&#25915;&#20987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2308.05681</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#38754;&#20020;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#21644;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (arXiv:2308.05681v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27969;&#24418;&#21644;&#23450;&#20041;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#26469;&#25915;&#20987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#26041;&#27861;&#35201;&#27714;&#35201;&#20040;&#23436;&#20840;&#20102;&#35299;&#21463;&#23475;&#32773;&#65288;&#21363;&#30333;&#30418;&#25915;&#20987;&#65289;&#65292;&#35201;&#20040;&#26377;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65288;&#21363;&#22522;&#20110;&#36716;&#31227;&#30340;&#25915;&#20987;&#65289;&#65292;&#25110;&#32773;&#39057;&#32321;&#26597;&#35810;&#27169;&#22411;&#65288;&#21363;&#40657;&#30418;&#25915;&#20987;&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#37117;&#38750;&#24120;&#38480;&#21046;&#24615;&#65292;&#24341;&#21457;&#20102;&#23545;&#33030;&#24369;&#24615;&#30340;&#36136;&#30097;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33030;&#24369;&#24615;&#30830;&#23454;&#23384;&#22312;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#20219;&#21153;&#65306;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#21463;&#23475;&#32773;&#27169;&#22411;&#25110;&#35757;&#32451;&#25968;&#25454;&#25110;&#26631;&#31614;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#30828;&#24615;&#26080;&#30418;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#36816;&#21160;&#27969;&#24418;&#65292;&#28982;&#21518;&#23450;&#20041;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#25915;&#20987;&#30340;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#65288;SMI&#26799;&#24230;&#65289;&#12290;&#25105;&#20204;&#30340;&#26799;&#24230;&#21253;&#21547;&#36816;&#21160;&#21160;&#21147;&#23398;&#30340;&#20449;&#24687;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#20551;&#35774;&#25439;&#22833;&#26799;&#24230;&#26159;&#36890;&#36807;&#35745;&#31639;&#32780;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20197;&#21450;&#25552;&#20986;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#38454;&#27573;&#26816;&#32034;&#25214;&#21040;&#24050;&#32463;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#65306;&#23454;&#29616;&#36328;&#35821;&#35328;&#12289;&#36328;&#25968;&#25454;&#38598;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning. (arXiv:2308.05680v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20197;&#21450;&#25552;&#20986;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24050;&#32463;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#30340;&#20219;&#21153;&#26088;&#22312;&#26816;&#27979;&#24050;&#32463;&#32463;&#36807;&#20107;&#23454;&#26680;&#26597;&#30340;&#25925;&#20107;&#12290;&#25104;&#21151;&#26816;&#27979;&#21040;&#24050;&#34987;&#28548;&#28165;&#30340;&#22768;&#26126;&#19981;&#20165;&#20943;&#23569;&#20102;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#30340;&#25163;&#21160;&#21162;&#21147;&#65292;&#36824;&#21487;&#20197;&#26377;&#21161;&#20110;&#20943;&#32531;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#20010;&#30740;&#31350;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#65292;&#21363;&#22312;&#26816;&#26597;&#30340;&#22312;&#32447;&#24086;&#23376;&#30340;&#35821;&#35328;&#19982;&#20107;&#23454;&#26680;&#26597;&#25991;&#31456;&#30340;&#35821;&#35328;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26816;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65306;&#65288;i&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20801;&#35768;&#23545;&#24050;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#25512;&#25991;&#20316;&#20026;&#23545;&#20107;&#23454;&#26680;&#26597;&#25991;&#31456;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#65307;&#65288;ii&#65289;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#32463;&#36807;&#24494;&#35843;&#21644;&#29616;&#25104;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65307;&#65288;iii&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#26694;&#26550;&#65292;&#23558;&#36825;&#20010;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of retrieving already debunked narratives aims to detect stories that have already been fact-checked. The successful detection of claims that have already been debunked not only reduces the manual efforts of professional fact-checkers but can also contribute to slowing the spread of misinformation. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual task, i.e. the retrieval of fact-checking articles in a language different from the language of the online post being checked. This paper fills this gap by (i) creating a novel dataset to enable research on cross-lingual retrieval of already debunked narratives, using tweets as queries to a database of fact-checking articles; (ii) presenting an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task; and (iii) proposing a novel multistage framework that divides this cross-lingual debunk ret
&lt;/p&gt;</description></item><item><title>AST-MHSA&#27169;&#22411;&#36890;&#36807;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#20174;AST&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#29983;&#25104;&#28304;&#20195;&#30721;&#30340;&#31616;&#26126;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.05646</link><description>&lt;p&gt;
AST-MHSA: &#21033;&#29992;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#20195;&#30721;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
AST-MHSA : Code Summarization using Multi-Head Self-Attention. (arXiv:2308.05646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05646
&lt;/p&gt;
&lt;p&gt;
AST-MHSA&#27169;&#22411;&#36890;&#36807;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#20174;AST&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#29983;&#25104;&#28304;&#20195;&#30721;&#30340;&#31616;&#26126;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25688;&#35201;&#26088;&#22312;&#20026;&#28304;&#20195;&#30721;&#29983;&#25104;&#31616;&#26126;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20854;&#20013;&#21033;&#29992;&#28304;&#20195;&#30721;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;AST&#27604;&#23545;&#24212;&#30340;&#28304;&#20195;&#30721;&#35201;&#38271;&#24471;&#22810;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#23558;&#25972;&#20010;&#32447;&#24615;&#21270;&#30340;AST&#36755;&#20837;&#21040;&#32534;&#30721;&#22120;&#20013;&#26469;&#24573;&#30053;&#36825;&#20010;&#22823;&#23567;&#32422;&#26463;&#12290;&#36825;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#20351;&#24471;&#20174;&#36807;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#20013;&#25552;&#21462;&#30495;&#27491;&#26377;&#20215;&#20540;&#30340;&#20381;&#36182;&#20851;&#31995;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#30001;&#20110;&#23545;AST&#20013;&#30340;&#25152;&#26377;&#33410;&#28857;&#24212;&#29992;&#33258;&#27880;&#24847;&#21147;&#65292;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;AST-MHSA&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#20174;AST&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#12290;&#32534;&#30721;&#22120;&#20197;&#20195;&#30721;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;
&lt;/p&gt;
&lt;p&gt;
Code summarization aims to generate concise natural language descriptions for source code. The prevailing approaches adopt transformer-based encoder-decoder architectures, where the Abstract Syntax Tree (AST) of the source code is utilized for encoding structural information. However, ASTs are much longer than the corresponding source code, and existing methods ignore this size constraint by directly feeding the entire linearized AST into the encoders. This simplistic approach makes it challenging to extract truly valuable dependency relations from the overlong input sequence and leads to significant computational overhead due to self-attention applied to all nodes in the AST.  To address this issue effectively and efficiently, we present a model, AST-MHSA that uses multi-head attention to extract the important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes as input the abstract syntax tree (AST) of the code and
&lt;/p&gt;</description></item><item><title>IIHT&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22522;&#20110;&#22270;&#20687;&#21040;&#25351;&#31034;&#22120;&#23618;&#27425;Transformer&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#30340;&#29305;&#24449;&#24182;&#29983;&#25104;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#25351;&#31034;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.05633</link><description>&lt;p&gt;
IIHT: &#22522;&#20110;&#22270;&#20687;&#21040;&#25351;&#31034;&#22120;&#23618;&#27425;Transformer&#30340;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer. (arXiv:2308.05633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05633
&lt;/p&gt;
&lt;p&gt;
IIHT&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22522;&#20110;&#22270;&#20687;&#21040;&#25351;&#31034;&#22120;&#23618;&#27425;Transformer&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#30340;&#29305;&#24449;&#24182;&#29983;&#25104;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#25351;&#31034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#22312;&#21307;&#30103;&#20998;&#26512;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23427;&#21487;&#20197;&#20135;&#29983;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#25551;&#36848;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#36731;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#21463;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21644;&#22270;&#20687;&#25551;&#36848;&#30340;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#25253;&#21578;&#24207;&#21015;&#30340;&#38271;&#24230;&#21644;&#30456;&#20851;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#29983;&#25104;&#30340;&#25253;&#21578;&#21487;&#33021;&#22312;&#35821;&#35328;&#27969;&#30021;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#36275;&#22815;&#30340;&#20020;&#24202;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#21040;&#25351;&#31034;&#22120;&#23618;&#27425;Transformer&#30340;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#26694;&#26550;&#65288;IIHT&#65289;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65292;&#21363;&#20998;&#31867;&#22120;&#27169;&#22359;&#12289;&#25351;&#31034;&#22120;&#25193;&#23637;&#27169;&#22359;&#21644;&#29983;&#25104;&#22120;&#27169;&#22359;&#12290;&#20998;&#31867;&#22120;&#27169;&#22359;&#39318;&#20808;&#20174;&#36755;&#20837;&#30340;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#23545;&#24212;&#29366;&#24577;&#30340;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#25351;&#31034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated medical report generation has become increasingly important in medical analysis. It can produce computer-aided diagnosis descriptions and thus significantly alleviate the doctors' work. Inspired by the huge success of neural machine translation and image captioning, various deep learning methods have been proposed for medical report generation. However, due to the inherent properties of medical data, including data imbalance and the length and correlation between report sequences, the generated reports by existing methods may exhibit linguistic fluency but lack adequate clinical accuracy. In this work, we propose an image-to-indicator hierarchical transformer (IIHT) framework for medical report generation. It consists of three modules, i.e., a classifier module, an indicator expansion module and a generator module. The classifier module first extracts image features from the input medical images and produces disease-related indicators with their corresponding states. The dise
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26032;&#22411;&#30340;&#38376;&#25511;&#26426;&#21046;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#20256;&#32479;&#38376;&#30340;&#20056;&#27861;&#21644;Sigmoid&#20989;&#25968;&#26367;&#25442;&#20026;&#21152;&#27861;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#21463;&#38480;&#30828;&#20214;&#19978;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25191;&#34892;&#25110;&#26356;&#22823;&#22411;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#25429;&#25417;&#21040;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.05629</link><description>&lt;p&gt;
ReLU&#21644;&#22522;&#20110;&#21152;&#27861;&#30340;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ReLU and Addition-based Gated RNN. (arXiv:2308.05629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26032;&#22411;&#30340;&#38376;&#25511;&#26426;&#21046;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#20256;&#32479;&#38376;&#30340;&#20056;&#27861;&#21644;Sigmoid&#20989;&#25968;&#26367;&#25442;&#20026;&#21152;&#27861;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#21463;&#38480;&#30828;&#20214;&#19978;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25191;&#34892;&#25110;&#26356;&#22823;&#22411;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#25429;&#25417;&#21040;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20256;&#32479;&#24490;&#29615;&#38376;&#30340;&#20056;&#27861;&#21644;Sigmoid&#20989;&#25968;&#26367;&#25442;&#20026;&#21152;&#27861;&#21644;ReLU&#28608;&#27963;&#12290;&#35813;&#26426;&#21046;&#26088;&#22312;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#32500;&#25252;&#24207;&#21015;&#22788;&#29702;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20174;&#32780;&#22312;&#21463;&#38480;&#30828;&#20214;&#19978;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25191;&#34892;&#25110;&#26356;&#22823;&#22411;&#30340;&#27169;&#22411;&#12290;&#20855;&#26377;LSTM&#21644;GRU&#31561;&#38376;&#25511;&#26426;&#21046;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#20256;&#32479;&#19978;&#65292;&#22522;&#20110;&#24403;&#21069;&#36755;&#20837;&#21644;&#20808;&#21069;&#29366;&#24577;&#21382;&#21490;&#30340;&#26356;&#26032;&#20998;&#21035;&#19982;&#21160;&#24577;&#26435;&#37325;&#30456;&#20056;&#65292;&#24182;&#32452;&#21512;&#35745;&#31639;&#20986;&#19979;&#19968;&#20010;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#20056;&#27861;&#22312;&#26576;&#20123;&#30828;&#20214;&#26550;&#26500;&#25110;&#26367;&#20195;&#31639;&#26415;&#31995;&#32479;&#65288;&#22914;&#21516;&#24577;&#21152;&#23494;&#65289;&#20013;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26032;&#22411;&#38376;&#25511;&#26426;&#21046;&#21487;&#20197;&#23545;&#26631;&#20934;&#30340;&#21512;&#25104;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#25429;&#25417;&#21040;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We replace the multiplication and sigmoid function of the conventional recurrent gate with addition and ReLU activation. This mechanism is designed to maintain long-term memory for sequence processing but at a reduced computational cost, thereby opening up for more efficient execution or larger models on restricted hardware. Recurrent Neural Networks (RNNs) with gating mechanisms such as LSTM and GRU have been widely successful in learning from sequential data due to their ability to capture long-term dependencies. Conventionally, the update based on current inputs and the previous state history is each multiplied with dynamic weights and combined to compute the next state. However, multiplication can be computationally expensive, especially for certain hardware architectures or alternative arithmetic systems such as homomorphic encryption. It is demonstrated that the novel gating mechanism can capture long-term dependencies for a standard synthetic sequence learning task while signifi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#20934;&#21270;&#26799;&#24230;&#36866;&#24212; H\"{o}lder &#20809;&#28369;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#23616;&#37096; H\"{o}lder &#20809;&#28369;&#24615;&#30340;&#26032;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2308.05621</link><description>&lt;p&gt;
&#25152;&#26377;&#24773;&#20917;&#19979;&#30340;&#26631;&#20934;&#21270;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Normalized Gradients for All. (arXiv:2308.05621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05621
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#20934;&#21270;&#26799;&#24230;&#36866;&#24212; H\"{o}lder &#20809;&#28369;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#23616;&#37096; H\"{o}lder &#20809;&#28369;&#24615;&#30340;&#26032;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#40657;&#30418;&#30340;&#26041;&#24335;&#21033;&#29992;&#26631;&#20934;&#21270;&#26799;&#24230;&#26469;&#36866;&#24212; H\"{o}lder &#20809;&#28369;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#30028;&#38480;&#23558;&#20381;&#36182;&#20110;&#23616;&#37096; H\"{o}lder &#20809;&#28369;&#24615;&#30340;&#19968;&#31181;&#26032;&#27010;&#24565;&#12290;&#20027;&#35201;&#24605;&#24819;&#30452;&#25509;&#26469;&#33258;&#20110; Levy [2017]&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short note, I show how to adapt to H\"{o}lder smoothness using normalized gradients in a black-box way. Moreover, the bound will depend on a novel notion of local H\"{o}lder smoothness. The main idea directly comes from Levy [2017].
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#30340;&#20860;&#23481;&#24615;&#24230;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#26032;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#26356;&#26032;&#27169;&#22411;&#24341;&#20837;&#30340;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;MIMIC&#25968;&#25454;&#30340;&#30149;&#27515;&#29575;&#39118;&#38505;&#20998;&#23618;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#33021;&#20135;&#29983;&#26356;&#20860;&#23481;&#30340;&#27169;&#22411;&#24182;&#20445;&#25345;&#21028;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05619</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#20860;&#23481;&#24615;&#26356;&#26032;&#20020;&#24202;&#39118;&#38505;&#20998;&#23618;&#27169;&#22411;&#65306;&#35780;&#20272;&#21644;&#20248;&#21270;&#20020;&#24202;&#21307;&#29983;-&#27169;&#22411;&#22242;&#38431;&#24615;&#33021;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance. (arXiv:2308.05619v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05619
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#30340;&#20860;&#23481;&#24615;&#24230;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#26032;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#26356;&#26032;&#27169;&#22411;&#24341;&#20837;&#30340;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;MIMIC&#25968;&#25454;&#30340;&#30149;&#27515;&#29575;&#39118;&#38505;&#20998;&#23618;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#33021;&#20135;&#29983;&#26356;&#20860;&#23481;&#30340;&#27169;&#22411;&#24182;&#20445;&#25345;&#21028;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#30340;&#21464;&#21270;&#25110;&#26032;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#26356;&#26032;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#20445;&#25345;&#25110;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#27169;&#22411;&#21487;&#33021;&#20250;&#24341;&#20837;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24403;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#30340;&#34892;&#20026;&#19982;&#29992;&#25143;&#30340;&#26399;&#26395;&#19981;&#19968;&#33268;&#26102;&#65292;&#20250;&#23548;&#33268;&#29992;&#25143;-&#27169;&#22411;&#22242;&#38431;&#34920;&#29616;&#19981;&#20339;&#12290;&#29616;&#26377;&#30340;&#20860;&#23481;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#20915;&#31574;&#38408;&#20540;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22522;&#20110;&#20272;&#35745;&#39118;&#38505;&#30340;&#25490;&#21517;&#29983;&#25104;&#27169;&#22411;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#20860;&#23481;&#24615;&#24230;&#37327;&#65292;$C^R$&#65292;&#20197;&#21450;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#21028;&#21035;&#24615;&#33021;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#40723;&#21169;&#33391;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;&#22312;&#21033;&#29992;MIMIC&#25968;&#25454;&#30340;&#30149;&#27515;&#29575;&#39118;&#38505;&#20998;&#23618;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#20135;&#29983;&#20102;&#26356;&#20860;&#23481;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21028;&#21035;&#24615;&#33021;&#65292;$C^R$&#25552;&#39640;&#20102;0.019&#65288;$95\%$&#32622;&#20449;&#21306;&#38388;&#65306;...
&lt;/p&gt;
&lt;p&gt;
As data shift or new data become available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval: 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22270;&#31354;&#26102;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#36895;&#20844;&#36335;&#27599;&#26085;&#20132;&#36890;&#27969;&#37327;&#12290;&#36890;&#36807;&#25968;&#25454;&#24402;&#19968;&#21270;&#31574;&#30053;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#25429;&#25417;&#31354;&#26102;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36824;&#20351;&#29992;&#20102;&#27668;&#35937;&#21644;&#26085;&#21382;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.05601</link><description>&lt;p&gt;
&#22810;&#22270;&#31354;&#26102;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction. (arXiv:2308.05601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22270;&#31354;&#26102;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#36895;&#20844;&#36335;&#27599;&#26085;&#20132;&#36890;&#27969;&#37327;&#12290;&#36890;&#36807;&#25968;&#25454;&#24402;&#19968;&#21270;&#31574;&#30053;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#25429;&#25417;&#31354;&#26102;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36824;&#20351;&#29992;&#20102;&#27668;&#35937;&#21644;&#26085;&#21382;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20043;&#38388;&#30340;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#23545;&#20110;&#22478;&#24066;&#29983;&#27963;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#21151;&#33021;&#20043;&#19968;&#65292;&#20132;&#36890;&#35780;&#20272;&#22312;&#29616;&#20170;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#27599;&#26085;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#22312;&#25972;&#20010;&#32593;&#32476;&#33539;&#22260;&#30340;&#25910;&#36153;&#31449;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#23454;&#38469;&#20013;&#21508;&#20010;&#20301;&#32622;&#20043;&#38388;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#29366;&#20917;&#21152;&#21095;&#20102;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22797;&#26434;&#30340;&#30456;&#20851;&#31354;&#26102;&#22240;&#32032;&#26080;&#27861;&#20840;&#38754;&#22320;&#24212;&#29992;&#20110;&#38271;&#26399;&#25345;&#32493;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#26102;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#36895;&#20844;&#36335;&#27599;&#26085;&#20132;&#36890;&#27969;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#37319;&#29992;&#25968;&#25454;&#24402;&#19968;&#21270;&#31574;&#30053;&#26469;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#30001;&#20110;&#32593;&#32476;&#33539;&#22260;&#30340;&#25910;&#36153;&#31449;&#20132;&#36890;&#27969;&#30340;&#38271;&#23614;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#30340;&#32593;&#32476;&#26469;&#25429;&#25417;&#31354;&#26102;&#29305;&#24449;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#20351;&#29992;&#20102;&#27668;&#35937;&#21644;&#26085;&#21382;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inter-city highway transportation is significant for urban life. As one of the key functions in intelligent transportation system (ITS), traffic evaluation always plays significant role nowadays, and daily traffic flow prediction still faces challenges at network-wide toll stations. On the one hand, the data imbalance in practice among various locations deteriorates the performance of prediction. On the other hand, complex correlative spatio-temporal factors cannot be comprehensively employed in long-term duration. In this paper, a prediction method is proposed for daily traffic flow in highway domain through spatio-temporal deep learning. In our method, data normalization strategy is used to deal with data imbalance, due to long-tail distribution of traffic flow at network-wide toll stations. And then, based on graph convolutional network, we construct networks in distinct semantics to capture spatio-temporal features. Beside that, meteorology and calendar features are used by our mod
&lt;/p&gt;</description></item><item><title>NUPES&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#38750;&#22343;&#21248;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37327;&#21270;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#33258;&#21516;&#26500;&#26469;&#20445;&#25345;&#26631;&#37327;m&#12290;</title><link>http://arxiv.org/abs/2308.05600</link><description>&lt;p&gt;
NUPES&#65306;&#36890;&#36807;&#25351;&#25968;&#25628;&#32034;&#36827;&#34892;&#38750;&#22343;&#21248;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search. (arXiv:2308.05600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05600
&lt;/p&gt;
&lt;p&gt;
NUPES&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#38750;&#22343;&#21248;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37327;&#21270;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#33258;&#21516;&#26500;&#26469;&#20445;&#25345;&#26631;&#37327;m&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37096;&#32626;&#30001;&#20110;&#20854;&#26114;&#36149;&#30340;&#35745;&#31639;&#38656;&#27714;&#32780;&#23616;&#38480;&#20110;&#36739;&#22823;&#30340;&#30828;&#20214;&#35774;&#22791;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#36825;&#19968;&#25361;&#25112;&#26368;&#36817;&#24050;&#32463;&#36798;&#21040;&#20102;&#21478;&#19968;&#20010;&#23618;&#38754;&#12290;&#20026;&#20102;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#24310;&#36831;&#65292;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#26159;&#37327;&#21270;&#12290;&#23427;&#36890;&#36807;&#23558;&#28014;&#28857;&#34920;&#31034;&#36716;&#25442;&#20026;&#20302;&#20301;&#23485;&#23450;&#28857;&#34920;&#31034;&#26469;&#23454;&#29616;&#65292;&#36890;&#24120;&#20551;&#35774;&#22343;&#21248;&#26144;&#23556;&#21040;&#27491;&#21017;&#32593;&#26684;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#65292;&#22312;&#25991;&#29486;&#20013;&#34987;&#31216;&#20026;&#22343;&#21248;&#37327;&#21270;&#65292;&#21487;&#33021;&#19981;&#36866;&#21512;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;DNN&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#36981;&#24490;&#38047;&#24418;&#20998;&#24067;&#12290;&#22312;LLM&#20013;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#20854;&#26435;&#37325;&#20998;&#24067;&#34987;&#35748;&#20026;&#20855;&#26377;&#22823;&#37327;&#12289;&#39640;&#24433;&#21709;&#21147;&#30340;&#24322;&#24120;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37327;&#21270;&#38480;&#21046;&#26368;&#24120;&#29992;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#21363;&#38750;&#22343;&#21248;&#37327;&#21270;&#12290;NUPES&#21033;&#29992;&#33258;&#21516;&#26500;&#20445;&#25345;&#26631;&#37327;m
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) deployment has been confined to larger hardware devices due to their expensive computational requirements. This challenge has recently reached another scale with the emergence of large language models (LLMs). In order to reduce both their memory footprint and latency, a promising technique is quantization. It consists in converting floating point representations to low bit-width fixed point representations, usually by assuming a uniform mapping onto a regular grid. This process, referred to in the literature as uniform quantization, may however be ill-suited as most DNN weights and activations follow a bell-shaped distribution. This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values. In this work, we propose an improvement over the most commonly adopted way to tackle this limitation in deep learning models quantization, namely, non-uniform quantization. NUPES leverages automorphisms to preserve the scalar m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#31216;&#24615;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#38450;&#24481;XGBoost&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#23545;&#31216;&#24615;&#21487;&#20197;&#20351;&#27169;&#22411;&#23545;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#30340;&#20998;&#31867;&#24674;&#22797;&#21040;&#27491;&#30830;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.05575</link><description>&lt;p&gt;
&#23545;&#25239;XGBoost&#25915;&#20987;&#30340;&#23545;&#31216;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Symmetry Defense Against XGBoost Adversarial Perturbation Attacks. (arXiv:2308.05575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#31216;&#24615;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#38450;&#24481;XGBoost&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#23545;&#31216;&#24615;&#21487;&#20197;&#20351;&#27169;&#22411;&#23545;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#30340;&#20998;&#31867;&#24674;&#22797;&#21040;&#27491;&#30830;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#38450;&#24481;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#20363;&#22914;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDT)&#65292;&#25269;&#24481;&#23545;&#25239;&#24615;&#25200;&#21160;&#25915;&#20987;&#12290;&#36825;&#20010;&#24819;&#27861;&#22522;&#20110;&#26368;&#36817;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;(CNNs)&#20351;&#29992;&#23545;&#31216;&#24615;&#32570;&#22833;&#26469;&#36827;&#34892;&#23545;&#31216;&#38450;&#24481;&#30340;&#30740;&#31350;&#12290;CNNs&#20043;&#25152;&#20197;&#32570;&#20047;&#23545;&#31216;&#24615;&#26159;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#23545;&#24453;&#23545;&#31216;&#26679;&#26412;&#65288;&#27604;&#22914;&#27700;&#24179;&#32763;&#36716;&#30340;&#22270;&#20687;&#65289;&#36827;&#34892;&#19981;&#21516;&#20110;&#21407;&#22987;&#26679;&#26412;&#30340;&#20998;&#31867;&#12290;CNNs&#32570;&#20047;&#23545;&#31216;&#24615;&#20063;&#24847;&#21619;&#30528;CNNs&#21487;&#20197;&#23545;&#23545;&#31216;&#30340;&#23545;&#25239;&#26679;&#26412;&#36827;&#34892;&#19982;&#23545;&#25239;&#26679;&#26412;&#19981;&#21516;&#30340;&#20998;&#31867;&#12290;&#21033;&#29992;CNNs&#30340;&#32570;&#20047;&#23545;&#31216;&#24615;&#65292;&#26368;&#36817;&#30340;CNN&#23545;&#31216;&#38450;&#24481;&#24050;&#32463;&#26174;&#31034;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#30340;&#20998;&#31867;&#24674;&#22797;&#21040;&#20102;&#27491;&#30830;&#30340;&#26679;&#26412;&#20998;&#31867;&#12290;&#20026;&#20102;&#23558;&#30456;&#21516;&#30340;&#23545;&#31216;&#38450;&#24481;&#24212;&#29992;&#21040;GBDTs&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;GBDT&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#39318;&#27425;&#35777;&#26126;GBDTs&#22312;&#23545;&#31216;&#24615;&#26041;&#38754;&#20063;&#32570;&#20047;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine whether symmetry can be used to defend tree-based ensemble classifiers such as gradient-boosting decision trees (GBDTs) against adversarial perturbation attacks. The idea is based on a recent symmetry defense for convolutional neural network classifiers (CNNs) that utilizes CNNs' lack of invariance with respect to symmetries. CNNs lack invariance because they can classify a symmetric sample, such as a horizontally flipped image, differently from the original sample. CNNs' lack of invariance also means that CNNs can classify symmetric adversarial samples differently from the incorrect classification of adversarial samples. Using CNNs' lack of invariance, the recent CNN symmetry defense has shown that the classification of symmetric adversarial samples reverts to the correct sample classification. In order to apply the same symmetry defense to GBDTs, we examine GBDT invariance and are the first to show that GBDTs also lack invariance with respect to symmetries. We apply and ev
&lt;/p&gt;</description></item><item><title>AutoGluon-TimeSeries&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;AutoML&#24211;&#65292;&#19987;&#27880;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#25552;&#20379;&#20102;&#31616;&#21333;&#26131;&#29992;&#12289;&#20934;&#30830;&#30340;&#28857;&#39044;&#27979;&#21644;&#20998;&#20301;&#25968;&#39044;&#27979;&#65292;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#30340;&#32479;&#35745;&#27169;&#22411;&#12289;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26041;&#27861;&#21644;&#38598;&#25104;&#25216;&#26415;&#65292;AutoGluon-TimeSeries&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.05566</link><description>&lt;p&gt;
AutoGluon-TimeSeries: &#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24320;&#28304;&#24211;
&lt;/p&gt;
&lt;p&gt;
AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting. (arXiv:2308.05566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05566
&lt;/p&gt;
&lt;p&gt;
AutoGluon-TimeSeries&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;AutoML&#24211;&#65292;&#19987;&#27880;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#25552;&#20379;&#20102;&#31616;&#21333;&#26131;&#29992;&#12289;&#20934;&#30830;&#30340;&#28857;&#39044;&#27979;&#21644;&#20998;&#20301;&#25968;&#39044;&#27979;&#65292;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#30340;&#32479;&#35745;&#27169;&#22411;&#12289;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26041;&#27861;&#21644;&#38598;&#25104;&#25216;&#26415;&#65292;AutoGluon-TimeSeries&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;AutoGluon-TimeSeries&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24320;&#28304;AutoML&#24211;&#12290;AutoGluon-TimeSeries&#19987;&#27880;&#20110;&#26131;&#29992;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#29992;&#25143;&#21482;&#38656;&#20351;&#29992;3&#34892;Python&#20195;&#30721;&#21363;&#21487;&#29983;&#25104;&#20934;&#30830;&#30340;&#28857;&#39044;&#27979;&#21644;&#20998;&#20301;&#25968;&#39044;&#27979;&#12290;&#22522;&#20110;AutoGluon&#30340;&#35774;&#35745;&#21746;&#23398;&#65292;AutoGluon-TimeSeries&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#30701;&#26102;&#38388;&#20869;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;AutoGluon-TimeSeries&#32467;&#21512;&#20102;&#20256;&#32479;&#30340;&#32479;&#35745;&#27169;&#22411;&#12289;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#26041;&#27861;&#21644;&#38598;&#25104;&#25216;&#26415;&#12290;&#22312;&#23545;29&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;AutoGluon-TimeSeries&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#20301;&#25968;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#19968;&#31995;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#24120;&#29978;&#33267;&#36229;&#36234;&#20808;&#21069;&#26041;&#27861;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AutoGluon-TimeSeries - an open-source AutoML library for probabilistic time series forecasting. Focused on ease of use and robustness, AutoGluon-TimeSeries enables users to generate accurate point and quantile forecasts with just 3 lines of Python code. Built on the design philosophy of AutoGluon, AutoGluon-TimeSeries leverages ensembles of diverse forecasting models to deliver high accuracy within a short training time. AutoGluon-TimeSeries combines both conventional statistical models, machine-learning based forecasting approaches, and ensembling techniques. In our evaluation on 29 benchmark datasets, AutoGluon-TimeSeries demonstrates strong empirical performance, outperforming a range of forecasting methods in terms of both point and quantile forecast accuracy, and often even improving upon the best-in-hindsight combination of prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#23614;&#37096;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#38750;&#23545;&#31216;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2308.05564</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#21246;&#32467;&#30340;&#39640;&#25928;&#21464;&#20998;&#25512;&#29702;&#21450;&#20854;&#22312;&#32929;&#31080;&#25910;&#30410;&#29575;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns. (arXiv:2308.05564v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#23614;&#37096;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#38750;&#23545;&#31216;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#23545;&#37329;&#34701;&#25968;&#25454;&#24314;&#27169;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#30340;&#23614;&#37096;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Azzalini&#21644;Capitanio&#65288;2003&#65289;&#25152;&#38544;&#21547;&#30340;&#20044;&#40486;&#21246;&#32467;&#22312;&#25104;&#23545;&#38750;&#23545;&#31216;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#20004;&#31181;&#27969;&#34892;&#30340;&#20044;&#40486;&#21246;&#32467;&#26356;&#39640;&#12290;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#23545;&#35813;&#20044;&#40486;&#21246;&#32467;&#30340;&#20272;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#39640;&#26031;&#29983;&#25104;&#34920;&#31034;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010;&#21487;&#20197;&#20934;&#30830;&#36817;&#20284;&#30340;&#38468;&#21152;&#21518;&#39564;&#12290;&#20351;&#29992;&#24555;&#36895;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#26469;&#35299;&#20915;&#21464;&#20998;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#20272;&#35745;2017&#24180;&#33267;2021&#24180;&#38388;93&#20010;&#32654;&#22269;&#32929;&#31080;&#30340;&#32929;&#31080;&#25910;&#30410;&#29575;&#30340;&#21246;&#32467;&#27169;&#22411;&#12290;&#38500;&#20102;&#25104;&#23545;&#30456;&#20851;&#24615;&#30340;&#21464;&#21270;&#22806;&#65292;&#35813;&#21246;&#32467;&#36824;&#25429;&#25417;&#21040;&#20102;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20381;&#36182;&#30340;&#22823;&#37327;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large skew-t factor copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#20020;&#30028;&#28857;&#19982;&#38750;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#20020;&#30028;&#28857;&#30340;&#27010;&#24565;&#25512;&#24191;&#20026;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#20165;&#22522;&#20110;&#38750;&#37325;&#35201;&#28857;&#36827;&#34892;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#20250;&#26377;&#20123;&#24615;&#33021;&#25439;&#22833;&#12290;&#24314;&#35758;&#20351;&#29992;&#26631;&#20934;&#21270;&#29109;&#36873;&#25321;&#38750;&#20020;&#30028;&#28857;&#38598;&#21512;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#36825;&#31181;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26497;&#24555;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#35299;&#37322;AI&#12289;&#31163;&#32676;&#20540;&#21435;&#38500;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.05525</link><description>&lt;p&gt;
&#20020;&#30028;&#28857;++&#65306;&#19968;&#31181;&#29992;&#20110;&#40065;&#26834;&#20998;&#31867;&#12289;&#23545;&#25239;&#24615;&#38450;&#24481;&#21644;&#21487;&#35299;&#37322;AI&#30340;&#25935;&#25463;&#28857;&#20113;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI. (arXiv:2308.05525v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#20020;&#30028;&#28857;&#19982;&#38750;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#20020;&#30028;&#28857;&#30340;&#27010;&#24565;&#25512;&#24191;&#20026;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#20165;&#22522;&#20110;&#38750;&#37325;&#35201;&#28857;&#36827;&#34892;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#20250;&#26377;&#20123;&#24615;&#33021;&#25439;&#22833;&#12290;&#24314;&#35758;&#20351;&#29992;&#26631;&#20934;&#21270;&#29109;&#36873;&#25321;&#38750;&#20020;&#30028;&#28857;&#38598;&#21512;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#36825;&#31181;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26497;&#24555;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#35299;&#37322;AI&#12289;&#31163;&#32676;&#20540;&#21435;&#38500;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#38656;&#27714;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#19988;&#24555;&#36895;&#22320;&#22788;&#29702;&#38750;&#20998;&#24067;&#26679;&#26412;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#20020;&#30028;&#28857;&#19982;&#38750;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#35265;&#30340;&#25968;&#25454;&#25439;&#22351;&#21644;&#31163;&#32676;&#28857;&#24448;&#24448;&#20250;&#34987;&#35299;&#37322;&#20026;&#20020;&#30028;&#28857;&#12290;&#25105;&#20204;&#23558;&#20020;&#30028;&#28857;&#30340;&#27010;&#24565;&#25512;&#24191;&#20026;&#37325;&#35201;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20165;&#22522;&#20110;&#38750;&#37325;&#35201;&#28857;&#30340;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#20250;&#31245;&#24494;&#25439;&#22833;&#19968;&#20123;&#24615;&#33021;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26631;&#20934;&#21270;&#29109;&#23545;&#20110;&#25968;&#25454;&#25439;&#22351;&#20998;&#26512;&#38750;&#24120;&#26377;&#20449;&#24687;&#37327;&#12290;&#24314;&#35758;&#22522;&#20110;&#26631;&#20934;&#21270;&#29109;&#36873;&#25321;&#38750;&#20020;&#30028;&#28857;&#38598;&#21512;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#37325;&#35201;&#24615;&#24230;&#37327;&#35745;&#31639;&#26497;&#20854;&#24555;&#36895;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;AI(XAI)&#65292;&#31163;&#32676;&#20540;&#21435;&#38500;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#40065;&#26834;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to cope accurately and fast with Out-Of-Distribution (OOD) samples is crucial in real-world safety demanding applications. In this work we first study the interplay between critical points of 3D point clouds and OOD samples. Our findings are that common corruptions and outliers are often interpreted as critical points. We generalize the notion of critical points into importance measures. We show that training a classification network based only on less important points dramatically improves robustness, at a cost of minor performance loss on the clean set. We observe that normalized entropy is highly informative for corruption analysis. An adaptive threshold based on normalized entropy is suggested for selecting the set of uncritical points. Our proposed importance measure is extremely fast to compute. We show it can be used for a variety of applications, such as Explainable AI (XAI), Outlier Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27493;&#21512;&#25104;&#35268;&#21010;&#65292;&#21457;&#29616;&#21333;&#27493;&#24615;&#33021;&#36739;&#22909;&#24182;&#19981;&#19968;&#23450;&#33021;&#25214;&#21040;&#28508;&#22312;&#30340;&#21453;&#24212;&#36335;&#24452;&#65292;&#24378;&#35843;&#20102;&#23558;&#21333;&#27493;&#27169;&#22411;&#19982;&#21512;&#25104;&#35268;&#21010;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05522</link><description>&lt;p&gt;
&#27169;&#22411;&#24456;&#37325;&#35201;&#65306;&#21333;&#27493;&#21453;&#21512;&#25104;&#23545;&#21512;&#25104;&#35268;&#21010;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning. (arXiv:2308.05522v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27493;&#21512;&#25104;&#35268;&#21010;&#65292;&#21457;&#29616;&#21333;&#27493;&#24615;&#33021;&#36739;&#22909;&#24182;&#19981;&#19968;&#23450;&#33021;&#25214;&#21040;&#28508;&#22312;&#30340;&#21453;&#24212;&#36335;&#24452;&#65292;&#24378;&#35843;&#20102;&#23558;&#21333;&#27493;&#27169;&#22411;&#19982;&#21512;&#25104;&#35268;&#21010;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21512;&#25104;&#26159;&#23558;&#21270;&#23398;&#21270;&#21512;&#29289;&#36880;&#27493;&#36882;&#24402;&#22320;&#20998;&#35299;&#20026;&#20998;&#23376;&#21069;&#20307;&#65292;&#30452;&#21040;&#25214;&#21040;&#19968;&#32452;&#21830;&#19994;&#19978;&#21487;&#29992;&#30340;&#20998;&#23376;&#20026;&#27490;&#65292;&#20197;&#25552;&#20379;&#21512;&#25104;&#36335;&#32447;&#12290;&#23427;&#30340;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#21333;&#27493;&#21453;&#21512;&#25104;&#39044;&#27979;&#21644;&#22810;&#27493;&#21512;&#25104;&#35268;&#21010;&#65292;&#23427;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#21270;&#23398;&#21453;&#24212;&#36923;&#36753;&#21644;&#25214;&#21040;&#27491;&#30830;&#30340;&#21453;&#24212;&#39034;&#24207;&#65292;&#20108;&#32773;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32852;&#31995;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#27809;&#26377;&#24471;&#21040;&#20307;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27493;&#21512;&#25104;&#35268;&#21010;&#24182;&#20351;&#29992;&#20844;&#24320;&#21644;&#19987;&#26377;&#30340;&#21453;&#24212;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#23558;&#36825;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#21333;&#27493;&#24615;&#33021;&#36739;&#22909;&#19982;&#25214;&#21040;&#28508;&#22312;&#30340;&#21453;&#24212;&#36335;&#24452;&#25104;&#21151;&#20043;&#38388;&#23384;&#22312;&#26029;&#35010;&#65292;&#36825;&#34920;&#26126;&#23558;&#26469;&#24517;&#39035;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#35780;&#20272;&#21333;&#27493;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis consists of breaking down a chemical compound recursively step-by-step into molecular precursors until a set of commercially available molecules is found with the goal to provide a synthesis route. Its two primary research directions, single-step retrosynthesis prediction, which models the chemical reaction logic, and multi-step synthesis planning, which tries to find the correct sequence of reactions, are inherently intertwined. Still, this connection is not reflected in contemporary research. In this work, we combine these two major research directions by applying multiple single-step retrosynthesis models within multi-step synthesis planning and analyzing their impact using public and proprietary reaction data. We find a disconnection between high single-step performance and potential route-finding success, suggesting that single-step models must be evaluated within synthesis planning in the future. Furthermore, we show that the commonly used single-step retrosynthesi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ReLU DNNs&#30340;&#26368;&#20339;&#34920;&#36798;&#33021;&#21147;&#21450;&#20854;&#22312;&#36924;&#36817;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ReLU DNNs&#21487;&#20197;&#34920;&#31034;&#20219;&#20309;&#22312;$[0,1]$&#19978;&#30001;$O(N^2L)$&#20010;&#32447;&#24615;&#20998;&#27573;&#26500;&#25104;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#19988;&#36825;&#31181;&#34920;&#31034;&#26041;&#24335;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#21472;&#21152;&#23450;&#29702;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#22788;&#29702;&#36830;&#32493;&#20989;&#25968;&#26102;ReLU DNNs&#30340;&#25552;&#21319;&#36924;&#36817;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.05509</link><description>&lt;p&gt;
&#23545;ReLU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20339;&#34920;&#36798;&#33021;&#21147;&#21450;&#20854;&#22312;&#29992;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#21472;&#21152;&#23450;&#29702;&#36827;&#34892;&#36924;&#36817;&#20013;&#30340;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem. (arXiv:2308.05509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ReLU DNNs&#30340;&#26368;&#20339;&#34920;&#36798;&#33021;&#21147;&#21450;&#20854;&#22312;&#36924;&#36817;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ReLU DNNs&#21487;&#20197;&#34920;&#31034;&#20219;&#20309;&#22312;$[0,1]$&#19978;&#30001;$O(N^2L)$&#20010;&#32447;&#24615;&#20998;&#27573;&#26500;&#25104;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#19988;&#36825;&#31181;&#34920;&#31034;&#26041;&#24335;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#21472;&#21152;&#23450;&#29702;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#22788;&#29702;&#36830;&#32493;&#20989;&#25968;&#26102;ReLU DNNs&#30340;&#25552;&#21319;&#36924;&#36817;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;ReLU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#26368;&#20339;&#34920;&#36798;&#33021;&#21147;&#21450;&#20854;&#22312;&#36890;&#36807;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#21472;&#21152;&#23450;&#29702;&#36827;&#34892;&#36924;&#36817;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#36896;&#24615;&#22320;&#35777;&#26126;&#20102;&#20219;&#20309;&#22312;$[0,1]$&#19978;&#30001;$O(N^2L)$&#20010;&#32447;&#24615;&#20998;&#27573;&#26500;&#25104;&#30340;&#36830;&#32493;&#20989;&#25968;&#37117;&#21487;&#20197;&#30001;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#21644;&#27599;&#23618;$N$&#20010;&#31070;&#32463;&#20803;&#30340;ReLU DNNs&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;ReLU DNNs&#30340;&#30772;&#30862;&#23481;&#37327;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26500;&#36896;&#22312;DNNs&#30340;&#21442;&#25968;&#35745;&#25968;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24341;&#29992;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#21472;&#21152;&#23450;&#29702;&#65292;&#25105;&#20204;&#22312;&#22788;&#29702;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20989;&#25968;&#26102;&#65292;&#23454;&#29616;&#20102;ReLU DNNs&#20219;&#24847;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#25552;&#21319;&#36924;&#36817;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is devoted to studying the optimal expressive power of ReLU deep neural networks (DNNs) and its application in approximation via the Kolmogorov Superposition Theorem. We first constructively prove that any continuous piecewise linear functions on $[0,1]$, comprising $O(N^2L)$ segments, can be represented by ReLU DNNs with $L$ hidden layers and $N$ neurons per layer. Subsequently, we demonstrate that this construction is optimal regarding the parameter count of the DNNs, achieved through investigating the shattering capacity of ReLU DNNs. Moreover, by invoking the Kolmogorov Superposition Theorem, we achieve an enhanced approximation rate for ReLU DNNs of arbitrary width and depth when dealing with continuous functions in high-dimensional spaces.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#24615;&#12289;&#34892;&#20026;&#31232;&#30095;&#24615;&#21644;&#34892;&#20026;&#31354;&#38388;&#38169;&#20301;&#31561;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#20248;&#36136;&#22810;&#26679;&#24615;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#23454;&#39564;&#39046;&#22495;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;...</title><link>http://arxiv.org/abs/2308.05483</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25235;&#21462;&#20013;&#30340;&#31232;&#30095;&#22870;&#21169;&#21644;&#31232;&#30095;&#20132;&#20114;&#19979;&#30340;&#20248;&#36136;&#22810;&#26679;&#24615;&#65306;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#25235;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics. (arXiv:2308.05483v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05483
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#24615;&#12289;&#34892;&#20026;&#31232;&#30095;&#24615;&#21644;&#34892;&#20026;&#31354;&#38388;&#38169;&#20301;&#31561;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#20248;&#36136;&#22810;&#26679;&#24615;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#23454;&#39564;&#39046;&#22495;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#36136;&#22810;&#26679;&#24615;&#65288;Quality-Diversity&#65292;QD&#65289;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32473;&#23450;&#38382;&#39064;&#12290;&#22312;&#36827;&#21270;&#26426;&#22120;&#20154;&#23398;&#20013;&#26368;&#21021;&#24320;&#21457;&#65292;&#22823;&#22810;&#25968;QD&#30740;&#31350;&#37117;&#26159;&#22312;&#26377;&#38480;&#30340;&#19968;&#32452;&#39046;&#22495;&#20013;&#36827;&#34892;&#30340;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#36816;&#21160;&#65292;&#20854;&#20013;&#36866;&#24212;&#24230;&#21644;&#34892;&#20026;&#20449;&#21495;&#26159;&#23494;&#38598;&#30340;&#12290;&#25235;&#21462;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#31038;&#21306;&#30340;&#21162;&#21147;&#65292;&#20294;&#35813;&#20219;&#21153;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#25235;&#21462;&#22312;QD&#25991;&#29486;&#20013;&#38754;&#20020;&#30528;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#65306;&#22870;&#21169;&#31232;&#30095;&#24615;&#65292;&#34892;&#20026;&#31232;&#30095;&#24615;&#21644;&#34892;&#20026;&#31354;&#38388;&#38169;&#20301;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;QD&#22914;&#20309;&#35299;&#20915;&#25235;&#21462;&#38382;&#39064;&#12290;&#22312;10&#20010;&#25235;&#21462;&#39046;&#22495;&#19978;&#36827;&#34892;&#20102;15&#31181;&#19981;&#21516;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#23545;&#24212;&#20110;2&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#22841;&#25345;&#22120;&#35774;&#32622;&#21644;5&#31181;&#26631;&#20934;&#29289;&#20307;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#21306;&#20998;&#31639;&#27861;&#30340;&#35780;&#20272;&#19982;&#20854;&#20869;&#37096;&#32452;&#20214;&#30340;&#35780;&#20272;&#65292;&#20197;&#20415;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#24471;&#21040;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity (QD) methods are algorithms that aim to generate a set of diverse and high-performing solutions to a given problem. Originally developed for evolutionary robotics, most QD studies are conducted on a limited set of domains - mainly applied to locomotion, where the fitness and the behavior signal are dense. Grasping is a crucial task for manipulation in robotics. Despite the efforts of many research communities, this task is yet to be solved. Grasping cumulates unprecedented challenges in QD literature: it suffers from reward sparsity, behavioral sparsity, and behavior space misalignment. The present work studies how QD can address grasping. Experiments have been conducted on 15 different methods on 10 grasping domains, corresponding to 2 different robot-gripper setups and 5 standard objects. An evaluation framework that distinguishes the evaluation of an algorithm from its internal components has also been proposed for a fair comparison. The obtained results show that 
&lt;/p&gt;</description></item><item><title>LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2308.05481</link><description>&lt;p&gt;
LLM&#21464;&#25104;DBA
&lt;/p&gt;
&lt;p&gt;
LLM As DBA. (arXiv:2308.05481v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05481
&lt;/p&gt;
&lt;p&gt;
LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65288;DBA&#65289;&#22312;&#31649;&#29702;&#12289;&#32500;&#25252;&#21644;&#20248;&#21270;&#25968;&#25454;&#24211;&#31995;&#32479;&#20197;&#30830;&#20445;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;DBA&#26469;&#35828;&#65292;&#31649;&#29702;&#22823;&#37327;&#25968;&#25454;&#24211;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#20113;&#25968;&#25454;&#24211;&#19978;&#30340;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#65289;&#26159;&#22256;&#38590;&#21644;&#32321;&#29712;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#29702;&#35299;&#26377;&#20215;&#20540;&#25991;&#20214;&#24182;&#29983;&#25104;&#21512;&#29702;&#31572;&#26696;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D-Bot&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65292;&#23427;&#21487;&#20197;&#25345;&#32493;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#25968;&#25454;&#24211;&#32500;&#25252;&#32463;&#39564;&#65292;&#24182;&#20026;&#30446;&#26631;&#25968;&#25454;&#24211;&#25552;&#20379;&#21512;&#29702;&#12289;&#26377;&#29702;&#12289;&#21450;&#26102;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#20197;LLM&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#24211;&#32500;&#25252;&#26694;&#26550;&#65292;&#21253;&#25324;&#65288;i&#65289;&#20174;&#25991;&#26723;&#21644;&#24037;&#20855;&#20013;&#26816;&#27979;&#25968;&#25454;&#24211;&#32500;&#25252;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#24605;&#32500;&#26641;&#65292;&#21644;&#65288;iii&#65289;&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05476</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#29992;&#20110;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#65306;&#19968;&#39033;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis. (arXiv:2308.05476v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#27450;&#35784;&#25110;&#27450;&#39575;&#24615;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#29992;&#20110;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#65288;&#22914;BERT&#65292;XLNET&#65292;DistilBERT&#21644;RoBERTa&#65289;&#22312;&#26816;&#27979;&#27450;&#35784;&#24615;&#25991;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27450;&#35784;&#24615;&#21644;&#38750;&#27450;&#35784;&#24615;&#25991;&#26412;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30446;&#30340;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#21253;&#25324;&#20934;&#30830;&#29575;&#65292;&#31934;&#30830;&#29575;&#65292;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#33021;&#22815;&#22312;&#22788;&#29702;&#27450;&#35784;&#20869;&#23481;&#26102;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deceptive text classification is a critical task in natural language processing that aims to identify deceptive or fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#20302;&#31209;MDP&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#25429;&#25417;&#20102;&#28145;&#24230;RL&#20013;&#26410;&#30693;&#34920;&#31034;&#30340;&#29305;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#24179;&#22343;&#21160;&#24577;&#27425;&#26368;&#20248;&#38388;&#38553;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.05471</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#24179;&#31283;&#20302;&#31209;MDP&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Algorithm for Nonstationary Low-Rank MDPs. (arXiv:2308.05471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#20302;&#31209;MDP&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#25429;&#25417;&#20102;&#28145;&#24230;RL&#20013;&#26410;&#30693;&#34920;&#31034;&#30340;&#29305;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#24179;&#22343;&#21160;&#24577;&#27425;&#26368;&#20248;&#38388;&#38553;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#25913;&#21464;&#29615;&#22659;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#38750;&#24179;&#31283;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#27169;&#25311;&#20102;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#24182;&#22240;&#27492;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#20851;&#20110;&#38750;&#24179;&#31283;MDPs&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#34920;&#26684;&#21644;&#32447;&#24615;&#65288;&#28151;&#21512;&#65289;MDPs&#19978;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#28145;&#24230;RL&#20013;&#26410;&#30693;&#34920;&#31034;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21162;&#21147;&#30740;&#31350;&#20102;&#22312;&#26131;&#32791;&#22411;&#20302;&#31209;MDPs&#19979;&#30340;&#38750;&#24179;&#31283;RL&#65292;&#20854;&#20013;&#36716;&#31227;&#26680;&#21644;&#22870;&#21169;&#37117;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#20302;&#31209;&#27169;&#22411;&#38500;&#20102;&#32447;&#24615;&#29366;&#24577;&#23884;&#20837;&#20989;&#25968;&#22806;&#36824;&#21253;&#21547;&#26410;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#30456;&#20851;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;PORTAL&#65292;&#28982;&#21518;&#23558;PORTAL&#25913;&#36827;&#20026;&#33258;&#36866;&#24212;&#21442;&#25968;&#26080;&#20851;&#29256;&#26412;&#30340;Ada-PORTAL&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#36229;&#21442;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#23545;&#38750;&#24179;&#31283;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#23545;&#20110;&#36825;&#20004;&#31181;&#31639;&#27861;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#24179;&#22343;&#21160;&#24577;&#27425;&#26368;&#20248;&#38388;&#38553;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) under changing environment models many real-world applications via nonstationary Markov Decision Processes (MDPs), and hence gains considerable interest. However, theoretical studies on nonstationary MDPs in the literature have mainly focused on tabular and linear (mixture) MDPs, which do not capture the nature of unknown representation in deep RL. In this paper, we make the first effort to investigate nonstationary RL under episodic low-rank MDPs, where both transition kernels and rewards may vary over time, and the low-rank model contains unknown representation in addition to the linear state embedding function. We first propose a parameter-dependent policy optimization algorithm called PORTAL, and further improve PORTAL to its parameter-free version of Ada-PORTAL, which is able to tune its hyper-parameters adaptively without any prior knowledge of nonstationarity. For both algorithms, we provide upper bounds on the average dynamic suboptimality gap, which
&lt;/p&gt;</description></item><item><title>$\mathcal{G}^2Pxy$&#26159;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#26041;&#27861;&#30340;&#22270;&#33410;&#28857;&#24320;&#25918;&#38598;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#39564;&#35777;&#36807;&#31243;&#20013;&#27809;&#26377;&#26410;&#30693;&#31867;&#21035;&#30340;&#20449;&#24687;&#65292;&#21033;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#20195;&#29702;&#26410;&#30693;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.05463</link><description>&lt;p&gt;
$\mathcal{G}^2Pxy$: &#24102;&#26377;&#26410;&#30693;&#20195;&#29702;&#30340;&#22270;&#33410;&#28857;&#29983;&#25104;&#24335;&#24320;&#25918;&#38598;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
$\mathcal{G}^2Pxy$: Generative Open-Set Node Classification on Graphs with Proxy Unknowns. (arXiv:2308.05463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05463
&lt;/p&gt;
&lt;p&gt;
$\mathcal{G}^2Pxy$&#26159;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#26041;&#27861;&#30340;&#22270;&#33410;&#28857;&#24320;&#25918;&#38598;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#39564;&#35777;&#36807;&#31243;&#20013;&#27809;&#26377;&#26410;&#30693;&#31867;&#21035;&#30340;&#20449;&#24687;&#65292;&#21033;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#20195;&#29702;&#26410;&#30693;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#20998;&#31867;&#26159;&#22312;&#22270;&#20013;&#39044;&#27979;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#26631;&#31614;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#25152;&#26377;&#26631;&#31614;&#37117;&#21487;&#29992;&#26102;&#21487;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20294;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#27169;&#22411;&#36890;&#24120;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#31867;&#21035;&#30340;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#35268;&#27169;&#30340;&#35823;&#20998;&#31867;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#24320;&#25918;&#38598;&#20998;&#31867;&#26041;&#27861;&#30340;&#24320;&#21457;&#23545;&#20110;&#30830;&#23450;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#24050;&#30693;&#31867;&#21035;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24320;&#25918;&#38598;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#20256;&#23548;&#23398;&#20064;&#65292;&#21033;&#29992;&#30495;&#23454;&#26410;&#35265;&#31867;&#21035;&#33410;&#28857;&#30340;&#37096;&#20998;&#25110;&#20840;&#37096;&#29305;&#24449;&#26469;&#24110;&#21161;&#24320;&#25918;&#38598;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#24320;&#25918;&#38598;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;$\mathcal{G}^2Pxy$&#65292;&#23427;&#36981;&#24490;&#26356;&#20005;&#26684;&#30340;&#24402;&#32435;&#23398;&#20064;&#35774;&#32622;&#65292;&#22312;&#35757;&#32451;&#21644;&#39564;&#35777;&#36807;&#31243;&#20013;&#27809;&#26377;&#26410;&#30693;&#31867;&#21035;&#30340;&#20449;&#24687;&#21487;&#29992;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;&#20195;&#29702;&#26410;&#30693;&#33410;&#28857;&#65292;&#21363;&#31867;&#38388;&#26410;&#30693;&#20195;&#29702;&#21644;&#22806;&#37096;&#26410;&#30693;&#20195;&#29702;&#65292;&#23558;&#34987;&#24341;&#20837;&#35813;&#26041;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node classification is the task of predicting the labels of unlabeled nodes in a graph. State-of-the-art methods based on graph neural networks achieve excellent performance when all labels are available during training. But in real-life, models are often applied on data with new classes, which can lead to massive misclassification and thus significantly degrade performance. Hence, developing open-set classification methods is crucial to determine if a given sample belongs to a known class. Existing methods for open-set node classification generally use transductive learning with part or all of the features of real unseen class nodes to help with open-set classification. In this paper, we propose a novel generative open-set node classification method, i.e. $\mathcal{G}^2Pxy$, which follows a stricter inductive learning setting where no information about unknown classes is available during training and validation. Two kinds of proxy unknown nodes, inter-class unknown proxies and externa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#36848;&#20102;&#26417;&#36842;&#20122;&#183;&#29632;&#23572;&#30340;&#12298;&#22240;&#26524;&#65306;&#27169;&#22411;&#12289;&#25512;&#29702;&#19982;&#25512;&#26029;&#12299;&#31532;&#20108;&#29256;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#20854;&#20013;&#30340;&#26356;&#26032;&#20869;&#23481;&#20197;&#21450;&#19968;&#31181;&#26131;&#20110;&#36319;&#38543;&#30340;&#39044;&#27979;&#22330;&#26223;&#19979;&#30340;&#22240;&#26524;&#25512;&#26029;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25152;&#38754;&#20020;&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.05451</link><description>&lt;p&gt;
&#12298;&#22240;&#26524;&#65306;&#27169;&#22411;&#12289;&#25512;&#29702;&#19982;&#25512;&#26029;&#12299;&#31532;&#20108;&#29256;&#30340;&#19968;&#20301;&#39044;&#27979;&#21592;&#30340;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Forecaster's Review of Judea Pearl's Causality: Models, Reasoning and Inference, Second Edition, 2009. (arXiv:2308.05451v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#36848;&#20102;&#26417;&#36842;&#20122;&#183;&#29632;&#23572;&#30340;&#12298;&#22240;&#26524;&#65306;&#27169;&#22411;&#12289;&#25512;&#29702;&#19982;&#25512;&#26029;&#12299;&#31532;&#20108;&#29256;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#20854;&#20013;&#30340;&#26356;&#26032;&#20869;&#23481;&#20197;&#21450;&#19968;&#31181;&#26131;&#20110;&#36319;&#38543;&#30340;&#39044;&#27979;&#22330;&#26223;&#19979;&#30340;&#22240;&#26524;&#25512;&#26029;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25152;&#38754;&#20020;&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26417;&#36842;&#20122;&#183;&#29632;&#23572;&#21407;&#33879;&#12298;&#22240;&#26524;&#12299;&#30340;&#24040;&#22823;&#26222;&#21450;&#21644;&#25104;&#21151;&#65292;&#26412;&#35780;&#36848;&#20171;&#32461;&#20102;2009&#24180;&#31532;&#20108;&#29256;&#26356;&#26032;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26131;&#20110;&#36319;&#38543;&#30340;&#39044;&#27979;&#22330;&#26223;&#19979;&#30340;&#22240;&#26524;&#25512;&#26029;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#22312;&#24314;&#27169;&#21453;&#20107;&#23454;&#12289;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#34701;&#20837;&#20808;&#21069;&#30693;&#35782;&#20197;&#22312;&#19981;&#21516;&#39044;&#27979;&#22330;&#26223;&#19979;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26102;&#65292;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#21487;&#33021;&#24102;&#26469;&#30340;&#19968;&#20123;&#28508;&#22312;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the big popularity and success of Judea Pearl's original causality book, this review covers the main topics updated in the second edition in 2009 and illustrates an easy-to-follow causal inference strategy in a forecast scenario. It further discusses some potential benefits and challenges for causal inference with time series forecasting when modeling the counterfactuals, estimating the uncertainty and incorporating prior knowledge to estimate causal effects in different forecasting scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#25506;&#35752;&#20102;&#21307;&#23398;&#20915;&#31574;&#25903;&#25345;&#20013;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#32467;&#26524;&#21457;&#29616;&#36890;&#29992;&#27169;&#22411;&#30340;XAI&#25216;&#26415;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#26356;&#22810;&#20351;&#29992;&#65292;&#35299;&#37322;&#24615;&#34987;&#24212;&#29992;&#20110;&#25552;&#39640;&#20449;&#20219;&#65292;&#20294;&#21307;&#29983;&#30340;&#21442;&#19982;&#20173;&#36739;&#23569;&#25253;&#36947;&#12290;</title><link>http://arxiv.org/abs/2308.05411</link><description>&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explainable AI applications in the Medical Domain: a systematic review. (arXiv:2308.05411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#25506;&#35752;&#20102;&#21307;&#23398;&#20915;&#31574;&#25903;&#25345;&#20013;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#32467;&#26524;&#21457;&#29616;&#36890;&#29992;&#27169;&#22411;&#30340;XAI&#25216;&#26415;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#26356;&#22810;&#20351;&#29992;&#65292;&#35299;&#37322;&#24615;&#34987;&#24212;&#29992;&#20110;&#25552;&#39640;&#20449;&#20219;&#65292;&#20294;&#21307;&#29983;&#30340;&#21442;&#19982;&#20173;&#36739;&#23569;&#25253;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#24433;&#20687;&#12289;&#24739;&#32773;&#25252;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#24212;&#29992;&#22312;&#22238;&#39038;&#24615;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#24456;&#23569;&#26377;&#24212;&#29992;&#12290;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38754;&#20020;&#30528;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#12289;&#36981;&#23432;&#27861;&#35268;&#12289;&#21512;&#29702;&#20351;&#29992;&#25968;&#25454;&#31561;&#21508;&#31181;&#25361;&#25112;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#20351;&#20154;&#31867;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#24182;&#20449;&#20219;&#20854;&#32467;&#26524;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#20960;&#24180;&#21457;&#34920;&#30340;198&#31687;&#25991;&#31456;&#65292;&#23545;&#21307;&#23398;&#20915;&#31574;&#25903;&#25345;&#30340;XAI&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;&#30456;&#20851;&#25991;&#31456;&#30340;&#31995;&#32479;&#32508;&#21512;&#20135;&#29983;&#20102;&#20960;&#20010;&#21457;&#29616;&#65306;&#65288;1&#65289;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#37319;&#29992;&#20102;&#36890;&#29992;&#27169;&#22411;&#30340;XAI&#25216;&#26415;&#65292;&#65288;2&#65289;&#30456;&#27604;&#20854;&#20182;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#26356;&#22810;&#22320;&#20351;&#29992;&#65292;&#65288;3&#65289;&#35299;&#37322;&#24615;&#34987;&#24212;&#29992;&#20110;&#25552;&#39640;&#20449;&#20219;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#25253;&#36947;&#20102;&#21307;&#29983;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in Medicine has made significant progress with emerging applications in medical imaging, patient care, and other areas. While these applications have proven successful in retrospective studies, very few of them were applied in practice.The field of Medical AI faces various challenges, in terms of building user trust, complying with regulations, using data ethically.Explainable AI (XAI) aims to enable humans understand AI and trust its results. This paper presents a literature review on the recent developments of XAI solutions for medical decision support, based on a representative sample of 198 articles published in recent years. The systematic synthesis of the relevant articles resulted in several findings. (1) model-agnostic XAI techniques were mostly employed in these solutions, (2) deep learning models are utilized more than other types of machine learning models, (3) explainability was applied to promote trust, but very few works reported the physicians par
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#35780;&#20272;&#20102;&#22810;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#22312;&#20892;&#20316;&#29289;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#30340;&#34701;&#21512;&#26041;&#27861;&#20248;&#20110;&#21333;&#19968;&#35270;&#35282;&#21644;&#20808;&#21069;&#26041;&#27861;&#65292;&#26681;&#25454;&#27979;&#35797;&#21306;&#22495;&#36873;&#25321;&#26368;&#20339;&#34701;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05407</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#29992;&#20110;&#20892;&#20316;&#29289;&#20998;&#31867;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comparative Assessment of Multi-view fusion learning for Crop Classification. (arXiv:2308.05407v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#35780;&#20272;&#20102;&#22810;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#22312;&#20892;&#20316;&#29289;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#30340;&#34701;&#21512;&#26041;&#27861;&#20248;&#20110;&#21333;&#19968;&#35270;&#35282;&#21644;&#20808;&#21069;&#26041;&#27861;&#65292;&#26681;&#25454;&#27979;&#35797;&#21306;&#22495;&#36873;&#25321;&#26368;&#20339;&#34701;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36965;&#24863;&#25968;&#25454;&#28304;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#24555;&#36895;&#22686;&#21152;&#65292;&#22810;&#35270;&#35282;&#23398;&#20064;&#24314;&#27169;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36965;&#24863;&#25968;&#25454;&#30340;&#20998;&#36776;&#29575;&#12289;&#24133;&#24230;&#21644;&#22122;&#22768;&#24046;&#24322;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#22312;&#36755;&#20837;&#32423;&#21035;&#36827;&#34892;&#34701;&#21512;&#65292;&#20294;&#20854;&#20182;&#26356;&#39640;&#32423;&#30340;&#34701;&#21512;&#31574;&#30053;&#21487;&#33021;&#20250;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;CropHarvest&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#20892;&#20316;&#29289;&#20998;&#31867;&#30340;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#34701;&#21512;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#21333;&#20010;&#35270;&#35282;&#21644;&#20808;&#21069;&#30340;&#34701;&#21512;&#26041;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#34701;&#21512;&#26041;&#27861;&#19968;&#30452;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22810;&#35270;&#35282;&#34701;&#21512;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#65292;&#22312;&#27979;&#35797;&#21306;&#22495;&#19981;&#21516;&#30340;&#26041;&#27861;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#36873;&#25321;&#34701;&#21512;&#26041;&#27861;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a rapidly increasing amount and diversity of remote sensing (RS) data sources, there is a strong need for multi-view learning modeling. This is a complex task when considering the differences in resolution, magnitude, and noise of RS data. The typical approach for merging multiple RS sources has been input-level fusion, but other - more advanced - fusion strategies may outperform this traditional approach. This work assesses different fusion strategies for crop classification in the CropHarvest dataset. The fusion methods proposed in this work outperform models based on individual views and previous fusion methods. We do not find one single fusion method that consistently outperforms all other approaches. Instead, we present a comparison of multi-view fusion methods for three different datasets and show that, depending on the test region, different methods obtain the best performance. Despite this, we suggest a preliminary criterion for the selection of fusion methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#20135;&#21697;&#35780;&#35770;&#22270;&#29255;&#25490;&#24207;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#33021;&#22815;&#23558;&#26368;&#30456;&#20851;&#30340;&#22270;&#29255;&#26174;&#31034;&#22312;&#21069;&#38754;&#65292;&#23545;&#29992;&#25143;&#30340;&#22312;&#32447;&#36141;&#29289;&#36873;&#25321;&#21644;&#34892;&#20026;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.05390</link><description>&lt;p&gt;
&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20135;&#21697;&#35780;&#35770;&#22270;&#29255;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Product Review Image Ranking for Fashion E-commerce. (arXiv:2308.05390v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#20135;&#21697;&#35780;&#35770;&#22270;&#29255;&#25490;&#24207;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#33021;&#22815;&#23558;&#26368;&#30456;&#20851;&#30340;&#22270;&#29255;&#26174;&#31034;&#22312;&#21069;&#38754;&#65292;&#23545;&#29992;&#25143;&#30340;&#22312;&#32447;&#36141;&#29289;&#36873;&#25321;&#21644;&#34892;&#20026;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#65292;&#39038;&#23458;&#26080;&#27861;&#20146;&#33258;&#26816;&#26597;&#20135;&#21697;&#65292;&#22240;&#27492;&#33021;&#22815;&#30475;&#21040;&#20854;&#20182;&#39038;&#23458;&#23545;&#20135;&#21697;&#30340;&#25991;&#23383;&#21644;&#22270;&#29255;&#35780;&#35770;&#22312;&#20570;&#36141;&#20080;&#20915;&#31574;&#26102;&#38750;&#24120;&#37325;&#35201;&#12290;&#38543;&#30528;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#22686;&#21152;&#65292;&#23458;&#25143;&#22270;&#20687;&#30340;&#25968;&#37327;&#20063;&#30456;&#24212;&#22686;&#21152;&#65292;&#22240;&#27492;&#23558;&#26368;&#30456;&#20851;&#30340;&#22270;&#29255;&#26174;&#31034;&#22312;&#21069;&#38754;&#23545;&#20110;&#29992;&#25143;&#30340;&#22312;&#32447;&#36141;&#29289;&#36873;&#25321;&#21644;&#34892;&#20026;&#21487;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#29992;&#20110;&#25490;&#21517;&#39038;&#23458;&#22270;&#20687;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21360;&#24230;&#20027;&#35201;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;Myntra&#30340;&#24037;&#20316;&#23460;&#24086;&#23376;&#21644;&#39640;&#24230;&#21442;&#19982;&#65288;&#39030;/&#36393;&#65289;&#30340;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#22270;&#20687;&#65292;&#24182;&#23545;&#19978;&#36848;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#20351;&#29992;&#20102;&#36873;&#25321;&#30340;&#25197;&#26354;&#25216;&#26415;&#65292;&#20351;&#23427;&#20204;&#30340;&#36136;&#37327;&#36798;&#21040;&#19982;&#20302;&#36136;&#37327;&#30340;UGC&#22270;&#20687;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a fashion e-commerce platform where customers can't physically examine the products on their own, being able to see other customers' text and image reviews of the product is critical while making purchase decisions. Given the high reliance on these reviews, over the years we have observed customers proactively sharing their reviews. With an increase in the coverage of User Generated Content (UGC), there has been a corresponding increase in the number of customer images. It is thus imperative to display the most relevant images on top as it may influence users' online shopping choices and behavior. In this paper, we propose a simple yet effective training procedure for ranking customer images. We created a dataset consisting of Myntra (A Major Indian Fashion e-commerce company) studio posts and highly engaged (upvotes/downvotes) UGC images as our starting point and used selected distortion techniques on the images of the above dataset to bring their quality at par with those of bad U
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#31561;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.05374</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;LLMs&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#35843;&#26597;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. (arXiv:2308.05374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#31561;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20043;&#21069;&#65292;&#30830;&#20445;&#23545;&#40784;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26469;&#35780;&#20272;LLMs&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#12289;&#20215;&#20540;&#35266;&#21644;&#27861;&#35268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#26102;&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#32500;&#24230;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;LLM&#21487;&#20449;&#24230;&#30340;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#12290;&#27599;&#20010;&#20027;&#35201;&#31867;&#21035;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#33509;&#24178;&#23376;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexiCubes&#30340;&#28789;&#27963;&#31561;&#20540;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#32593;&#26684;&#20248;&#21270;&#65292;&#20801;&#35768;&#23545;&#25552;&#21462;&#30340;&#32593;&#26684;&#20960;&#20309;&#21644;&#36830;&#36890;&#24615;&#36827;&#34892;&#28789;&#27963;&#35843;&#25972;&#65292;&#20174;&#32780;&#39640;&#36136;&#37327;&#22320;&#20445;&#30041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.05371</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#32593;&#26684;&#20248;&#21270;&#30340;&#28789;&#27963;&#31561;&#20540;&#38754;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Flexible Isosurface Extraction for Gradient-Based Mesh Optimization. (arXiv:2308.05371v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexiCubes&#30340;&#28789;&#27963;&#31561;&#20540;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#32593;&#26684;&#20248;&#21270;&#65292;&#20801;&#35768;&#23545;&#25552;&#21462;&#30340;&#32593;&#26684;&#20960;&#20309;&#21644;&#36830;&#36890;&#24615;&#36827;&#34892;&#28789;&#27963;&#35843;&#25972;&#65292;&#20174;&#32780;&#39640;&#36136;&#37327;&#22320;&#20445;&#30041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#32593;&#26684;&#20248;&#21270;&#65292;&#36890;&#36807;&#23558;&#20854;&#34920;&#31034;&#20026;&#26631;&#37327;&#22330;&#30340;&#31561;&#20540;&#38754;&#26469;&#36845;&#20195;&#22320;&#20248;&#21270;&#19977;&#32500;&#34920;&#38754;&#32593;&#26684;&#65292;&#36825;&#22312;&#25668;&#24433;&#27979;&#37327;&#12289;&#29983;&#25104;&#24314;&#27169;&#21644;&#21453;&#21521;&#29289;&#29702;&#31561;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#29616;&#26377;&#30340;&#23454;&#29616;&#26041;&#24335;&#26159;&#23558;&#32463;&#20856;&#30340;&#31561;&#20540;&#38754;&#25552;&#21462;&#31639;&#27861;&#65288;&#22914;Marching Cubes&#25110;Dual Contouring&#65289;&#36827;&#34892;&#35843;&#25972;&#65307;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#25216;&#26415;&#32570;&#20047;&#34920;&#31034;&#39640;&#36136;&#37327;&#20445;&#30041;&#29305;&#24449;&#30340;&#32593;&#26684;&#30340;&#33258;&#30001;&#24230;&#65292;&#25110;&#32773;&#20250;&#36973;&#21463;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FlexiCubes&#65292;&#19968;&#31181;&#19987;&#20026;&#20248;&#21270;&#26410;&#30693;&#32593;&#26684;&#32780;&#35774;&#35745;&#30340;&#31561;&#20540;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#20415;&#20110;&#20960;&#20309;&#12289;&#35270;&#35273;&#29978;&#33267;&#29289;&#29702;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#24341;&#20837;&#39069;&#22806;&#30340;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#21040;&#34920;&#31034;&#26041;&#27861;&#20013;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#25552;&#21462;&#30340;&#32593;&#26684;&#20960;&#20309;&#21644;&#36830;&#36890;&#24615;&#36827;&#34892;&#26412;&#22320;&#28789;&#27963;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work considers gradient-based mesh optimization, where we iteratively optimize for a 3D surface mesh by representing it as the isosurface of a scalar field, an increasingly common paradigm in applications including photogrammetry, generative modeling, and inverse physics. Existing implementations adapt classic isosurface extraction algorithms like Marching Cubes or Dual Contouring; these techniques were designed to extract meshes from fixed, known fields, and in the optimization setting they lack the degrees of freedom to represent high-quality feature-preserving meshes, or suffer from numerical instabilities. We introduce FlexiCubes, an isosurface representation specifically designed for optimizing an unknown mesh with respect to geometric, visual, or even physical objectives. Our main insight is to introduce additional carefully-chosen parameters into the representation, which allow local flexible adjustments to the extracted mesh geometry and connectivity. These parameters are 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#35745;&#31639;&#26426;&#26550;&#26500;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#24555;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;CNN&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#21151;&#32791;&#21644;&#24615;&#33021;&#65292;&#24110;&#21161;&#35745;&#31639;&#26426;&#26550;&#26500;&#24072;&#22312;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#20272;&#35745;&#65292;&#20174;&#32780;&#20943;&#23569;&#24320;&#21457;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2308.05364</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#30340;&#35745;&#31639;&#26426;&#26550;&#26500;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Machine Learning aided Computer Architecture Design for CNN Inferencing Systems. (arXiv:2308.05364v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#35745;&#31639;&#26426;&#26550;&#26500;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#24555;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;CNN&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#21151;&#32791;&#21644;&#24615;&#33021;&#65292;&#24110;&#21161;&#35745;&#31639;&#26426;&#26550;&#26500;&#24072;&#22312;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#20272;&#35745;&#65292;&#20174;&#32780;&#20943;&#23569;&#24320;&#21457;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21644;&#21450;&#26102;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#12289;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#21644;&#36793;&#32536;&#35745;&#31639;&#31561;&#26032;&#20852;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#20027;&#35201;ML&#31639;&#27861;&#20043;&#19968;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#23427;&#38656;&#35201;&#39640;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#28385;&#36275;&#35774;&#35745;&#32422;&#26463;&#65292;&#20154;&#20204;&#20351;&#29992;ML&#21152;&#36895;&#22120;&#22914;GPGPUs&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#21152;&#36895;&#22120;&#36890;&#24120;&#28041;&#21450;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65288;DSE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#38656;&#35201;&#22823;&#37327;&#25163;&#24037;&#21162;&#21147;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#24555;DSE&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#36866;&#21512;CNN&#25512;&#29702;&#31995;&#32479;&#30340;GPGPU&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;CNN&#21151;&#32791;&#21644;&#24615;&#33021;&#39044;&#27979;&#65292;MAPE&#20998;&#21035;&#20026;5.03%&#21644;5.94%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#35745;&#31639;&#26426;&#26550;&#26500;&#24072;&#33021;&#22815;&#22312;&#24320;&#21457;&#30340;&#26089;&#26399;&#20272;&#35745;&#21151;&#32791;&#21644;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#23569;&#24320;&#21457;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FINER&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#21487;&#29702;&#35299;&#24615;&#35299;&#37322;&#30340;&#39118;&#38505;&#26816;&#27979;&#20998;&#31867;&#22120;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.05362</link><description>&lt;p&gt;
FINER:&#21033;&#29992;&#29305;&#24449;&#24402;&#22240;&#22686;&#24378;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#20197;&#20419;&#36827;&#23433;&#20840;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis. (arXiv:2308.05362v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FINER&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#21487;&#29702;&#35299;&#24615;&#35299;&#37322;&#30340;&#39118;&#38505;&#26816;&#27979;&#20998;&#31867;&#22120;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#22312;&#21508;&#31181;&#39118;&#38505;&#26816;&#27979;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#25506;&#32034;&#20016;&#23500;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#19988;&#24212;&#35813;&#33258;&#21160;&#21457;&#29616;&#39118;&#38505;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#34892;&#20026;&#35821;&#20041;&#26080;&#27861;&#20256;&#36798;&#32473;&#19979;&#28216;&#23433;&#20840;&#19987;&#23478;&#65292;&#20197;&#20943;&#23569;&#20182;&#20204;&#22312;&#23433;&#20840;&#20998;&#26512;&#20013;&#30340;&#32321;&#37325;&#24037;&#20316;&#37327;&#12290;&#34429;&#28982;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#65292;&#20294;&#22522;&#30784;&#20998;&#31867;&#22120;&#20173;&#28982;&#19981;&#30693;&#36947;&#21738;&#20123;&#34892;&#20026;&#26159;&#21487;&#30097;&#30340;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#35299;&#37322;&#19981;&#33021;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#23548;&#33268;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FINER&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;&#39118;&#38505;&#26816;&#27979;&#20998;&#31867;&#22120;&#29983;&#25104;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#21487;&#29702;&#35299;&#24615;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#39640;&#23618;&#27425;&#30340;&#24605;&#36335;&#26159;&#27719;&#38598;&#27169;&#22411;&#24320;&#21457;&#32773;&#12289;&#29305;&#24449;&#24402;&#22240;&#35774;&#35745;&#24072;&#21644;&#23433;&#20840;&#19987;&#23478;&#30340;&#35299;&#37322;&#24037;&#20316;&#12290;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#35299;&#37322;&#24341;&#23548;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning classifiers achieve state-of-the-art performance in various risk detection applications. They explore rich semantic representations and are supposed to automatically discover risk behaviors. However, due to the lack of transparency, the behavioral semantics cannot be conveyed to downstream security experts to reduce their heavy workload in security analysis. Although feature attribution (FA) methods can be used to explain deep learning, the underlying classifier is still blind to what behavior is suspicious, and the generated explanation cannot adapt to downstream tasks, incurring poor explanation fidelity and intelligibility. In this paper, we propose FINER, the first framework for risk detection classifiers to generate high-fidelity and high-intelligibility explanations. The high-level idea is to gather explanation efforts from model developer, FA designer, and security experts. To improve fidelity, we fine-tune the classifier with an explanation-guided multi-task learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PreAttacK&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#21069;&#26816;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20266;&#36896;&#36134;&#21495;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#20266;&#36896;&#36134;&#21495;&#22312;&#21152;&#20837;&#32593;&#32476;&#21518;&#30340;&#21021;&#22987;&#26379;&#21451;&#35831;&#27714;&#34892;&#20026;&#65292;&#36890;&#36807;&#22810;&#31867;&#25193;&#23637;&#20559;&#22909;&#36830;&#25509;&#27169;&#22411;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.05353</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#31867;&#20559;&#22909;&#36830;&#25509;&#20998;&#31867;&#22120;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#25552;&#21069;&#26816;&#27979;&#20266;&#36896;&#36134;&#21495;
&lt;/p&gt;
&lt;p&gt;
Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers. (arXiv:2308.05353v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PreAttacK&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#21069;&#26816;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20266;&#36896;&#36134;&#21495;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#20266;&#36896;&#36134;&#21495;&#22312;&#21152;&#20837;&#32593;&#32476;&#21518;&#30340;&#21021;&#22987;&#26379;&#21451;&#35831;&#27714;&#34892;&#20026;&#65292;&#36890;&#36807;&#22810;&#31867;&#25193;&#23637;&#20559;&#22909;&#36830;&#25509;&#27169;&#22411;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PreAttacK&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20266;&#36896;&#36134;&#21495;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#31639;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#25152;&#20381;&#36182;&#30340;&#20449;&#24687;&#26159;&#20851;&#20110;&#20266;&#36896;&#36134;&#21495;&#30340;&#21451;&#35850;&#20851;&#31995;&#25110;&#20854;&#19982;&#20182;&#20154;&#20998;&#20139;&#30340;&#20869;&#23481;&#65292;&#32780;&#36825;&#27491;&#26159;&#25105;&#20204;&#24819;&#35201;&#38450;&#27490;&#30340;&#12290;PreAttacK&#19982;&#36825;&#20123;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#19981;&#21516;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#20851;&#20110;&#26032;&#30340;&#20266;&#36896;&#65288;&#21644;&#30495;&#23454;&#65289;&#36134;&#21495;&#22312;&#21152;&#20837;&#20027;&#35201;&#32593;&#32476;&#65288;Facebook&#65289;&#21518;&#23581;&#35797;&#35831;&#27714;&#26379;&#21451;&#30340;&#35814;&#32454;&#20998;&#24067;&#20998;&#26512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#26032;&#36134;&#21495;&#27809;&#26377;&#20132;&#26379;&#21451;&#25110;&#20998;&#20139;&#20869;&#23481;&#20043;&#21069;&#65292;&#36825;&#20123;&#21021;&#22987;&#26379;&#21451;&#35831;&#27714;&#34892;&#20026;&#20250;&#24341;&#21457;&#31038;&#20132;&#32593;&#32476;&#22686;&#38271;&#30340;&#32463;&#20856;&#20559;&#22909;&#36830;&#25509;&#27169;&#22411;&#30340;&#33258;&#28982;&#22810;&#31867;&#25193;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;PreAttacK&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#30456;&#20851;&#38382;&#39064;&#23454;&#20363;&#20013;&#65292;PreAttacK&#33021;&#22815;&#36817;&#20284;&#26368;&#20248;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe a new algorithm called Preferential Attachment k-class Classifier (PreAttacK) for detecting fake accounts in a social network. Recently, several algorithms have obtained high accuracy on this problem. However, they have done so by relying on information about fake accounts' friendships or the content they share with others--the very things we seek to prevent.  PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth.  We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the 
&lt;/p&gt;</description></item><item><title>RTLLM&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#20013;&#30446;&#26631;&#35774;&#35745;&#31616;&#21333;&#19988;&#35268;&#27169;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35774;&#35745;&#36136;&#37327;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.05345</link><description>&lt;p&gt;
RTLLM&#65306;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;RTL&#29983;&#25104;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model. (arXiv:2308.05345v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05345
&lt;/p&gt;
&lt;p&gt;
RTLLM&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#20013;&#30446;&#26631;&#35774;&#35745;&#31616;&#21333;&#19988;&#35268;&#27169;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35774;&#35745;&#36136;&#37327;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#37319;&#29992;LLMs&#36827;&#34892;&#25935;&#25463;&#30828;&#20214;&#35774;&#35745;&#65292;&#20363;&#22914;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#24037;&#20316;&#20013;&#65292;&#30446;&#26631;&#35774;&#35745;&#37117;&#30456;&#23545;&#31616;&#21333;&#19988;&#35268;&#27169;&#36739;&#23567;&#65292;&#24182;&#30001;&#20316;&#32773;&#33258;&#24049;&#25552;&#20986;&#65292;&#36825;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;LLMs&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#21482;&#20851;&#27880;&#35774;&#35745;&#30340;&#27491;&#30830;&#24615;&#65292;&#32780;&#27809;&#26377;&#35780;&#20272;&#29983;&#25104;&#30340;&#35774;&#35745;RTL&#30340;&#35774;&#35745;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RTLLM&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#35774;&#35745;RTL&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19977;&#20010;&#28176;&#36827;&#30446;&#26631;&#65292;&#21363;&#35821;&#27861;&#30446;&#26631;&#12289;&#21151;&#33021;&#30446;&#26631;&#21644;&#35774;&#35745;&#36136;&#37327;&#30446;&#26631;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#33258;&#21160;&#25552;&#20379;&#23545;&#20219;&#20309;&#32473;&#23450;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisin
&lt;/p&gt;</description></item><item><title>OpenProteinSet&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;1600&#19975;&#20010;MSAs&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#34507;&#30333;&#36136;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05326</link><description>&lt;p&gt;
OpenProteinSet&#65306;&#22823;&#35268;&#27169;&#32467;&#26500;&#29983;&#29289;&#23398;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
OpenProteinSet: Training data for structural biology at scale. (arXiv:2308.05326v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05326
&lt;/p&gt;
&lt;p&gt;
OpenProteinSet&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;1600&#19975;&#20010;MSAs&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#34507;&#30333;&#36136;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30340;&#22810;&#24207;&#21015;&#27604;&#23545;&#65288;MSAs&#65289;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#29983;&#29289;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#34507;&#30333;&#36136;&#35774;&#35745;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#31561;&#29983;&#29289;&#20449;&#24687;&#23398;&#26041;&#27861;&#20013;&#24050;&#32463;&#25104;&#20026;&#26680;&#24515;&#24037;&#20855;&#22810;&#24180;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#65292;&#22914;AlphaFold2&#30452;&#25509;&#20351;&#29992;transformers&#23545;&#22823;&#37327;&#21407;&#22987;MSAs&#36827;&#34892;&#20851;&#27880;&#65292;&#20877;&#27425;&#32943;&#23450;&#20102;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;MSAs&#30340;&#29983;&#25104;&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#24182;&#19988;&#23578;&#26410;&#21521;&#30740;&#31350;&#30028;&#25552;&#20379;&#19982;AlphaFold2&#35757;&#32451;&#25152;&#29992;&#25968;&#25454;&#30456;&#24403;&#30340;&#25968;&#25454;&#38598;&#65292;&#21046;&#32422;&#20102;&#34507;&#30333;&#36136;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;OpenProteinSet&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21253;&#21547;&#36229;&#36807;1600&#19975;&#20010;MSAs&#12289;&#34507;&#30333;&#25968;&#25454;&#38134;&#34892;&#20013;&#30340;&#32467;&#26500;&#21516;&#28304;&#29289;&#21644;AlphaFold2&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#20808;&#21069;&#24050;&#32463;&#25104;&#21151;&#22320;&#20351;&#29992;OpenProteinSet&#37325;&#26032;&#35757;&#32451;&#20102;AlphaFold2&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#26399;&#26395;OpenProteinSet&#33021;&#22815;&#20316;&#20026;&#35757;&#32451;&#21644;&#39564;&#35777;&#25968;&#25454;&#24191;&#27867;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple sequence alignments (MSAs) of proteins encode rich biological information and have been workhorses in bioinformatic methods for tasks like protein design and protein structure prediction for decades. Recent breakthroughs like AlphaFold2 that use transformers to attend directly over large quantities of raw MSAs have reaffirmed their importance. Generation of MSAs is highly computationally intensive, however, and no datasets comparable to those used to train AlphaFold2 have been made available to the research community, hindering progress in machine learning for proteins. To remedy this problem, we introduce OpenProteinSet, an open-source corpus of more than 16 million MSAs, associated structural homologs from the Protein Data Bank, and AlphaFold2 protein structure predictions. We have previously demonstrated the utility of OpenProteinSet by successfully retraining AlphaFold2 on it. We expect OpenProteinSet to be broadly useful as training and validation data for 1) diverse task
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05309</link><description>&lt;p&gt;
&#22270;&#32858;&#31867;&#30340;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#22270;&#32467;&#26500;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#31232;&#30095;&#24615;&#21644;&#22810;&#26679;&#24615;&#25152;&#22266;&#26377;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22270;&#32467;&#26500;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#36830;&#25509;&#21644;&#21024;&#38500;&#38169;&#35823;&#30340;&#36830;&#25509;&#26469;&#20248;&#21270;&#36755;&#20837;&#22270;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#30417;&#30563;&#30340;&#35774;&#32622;&#19978;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#65292;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#29305;&#23450;&#32858;&#31867;&#20219;&#21153;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#22270;&#32858;&#31867;&#65288;HoLe&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;&#35266;&#23519;&#21040;&#65292;&#24494;&#22937;&#22320;&#22686;&#24378;&#22270;&#32467;&#26500;&#20013;&#30340;&#21516;&#31867;&#24615;&#31243;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;GNNs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve G
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;U-Net&#21450;&#20854;&#21464;&#20307;&#20197;&#21450;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22914;TransUNet&#30340;&#24212;&#29992;&#12290;&#35770;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#29305;&#28857;&#21644;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.05305</link><description>&lt;p&gt;
&#20174;CNN&#21040;Transformer: &#21307;&#30103;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From CNN to Transformer: A Review of Medical Image Segmentation Models. (arXiv:2308.05305v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;U-Net&#21450;&#20854;&#21464;&#20307;&#20197;&#21450;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22914;TransUNet&#30340;&#24212;&#29992;&#12290;&#35770;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#29305;&#28857;&#21644;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#22270;&#20687;&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#29305;&#21035;&#26159;&#20316;&#20026;&#39640;&#25928;&#30142;&#30149;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20851;&#38190;&#21069;&#25552;&#12290;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#24050;&#25104;&#20026;&#19968;&#31181;&#26222;&#36941;&#36235;&#21183;&#12290;&#30446;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;U-Net&#21450;&#20854;&#21464;&#20307;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22914;TransUNet&#65292;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36817;&#24180;&#26469;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#22235;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#30740;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#32467;&#26680;&#30149;&#33016;&#29255;&#21644;&#21365;&#24034;&#32959;&#30244;&#65289;&#19978;&#23450;&#37327;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#24110;&#21161;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#24314;&#31435;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation is an important step in medical image analysis, especially as a crucial prerequisite for efficient disease diagnosis and treatment. The use of deep learning for image segmentation has become a prevalent trend. The widely adopted approach currently is U-Net and its variants. Additionally, with the remarkable success of pre-trained models in natural language processing tasks, transformer-based models like TransUNet have achieved desirable performance on multiple medical image segmentation datasets. In this paper, we conduct a survey of the most representative four medical image segmentation models in recent years. We theoretically analyze the characteristics of these models and quantitatively evaluate their performance on two benchmark datasets (i.e., Tuberculosis Chest X-rays and ovarian tumors). Finally, we discuss the main challenges and future trends in medical image segmentation. Our work can assist researchers in the related field to quickly establish med
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#25955;&#32593;&#32476;&#20013;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#24046;&#20943;&#23569;&#26041;&#27861;&#26469;&#28040;&#38500;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#30340;&#36127;&#38754;&#25928;&#24212;&#65292;&#24182;&#21462;&#24471;&#20102;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#21644;&#23567;&#30340;&#23398;&#20064;&#38169;&#35823;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05292</link><description>&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#29420;&#31435;&#23398;&#20064;&#38169;&#35823;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#20998;&#25955;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error. (arXiv:2308.05292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#25955;&#32593;&#32476;&#20013;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#24046;&#20943;&#23569;&#26041;&#27861;&#26469;&#28040;&#38500;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#30340;&#36127;&#38754;&#25928;&#24212;&#65292;&#24182;&#21462;&#24471;&#20102;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#21644;&#23567;&#30340;&#23398;&#20064;&#38169;&#35823;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#25955;&#32593;&#32476;&#20013;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#20195;&#29702;&#23450;&#26399;&#19982;&#20854;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#20197;&#20132;&#25442;&#26412;&#22320;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26356;&#26032;&#33258;&#24049;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#36827;&#34892;&#25932;&#23545;&#34892;&#20026;&#30340;&#26410;&#30693;&#25968;&#37327;&#30340;&#25308;&#21344;&#24237;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#27809;&#26377;&#29616;&#26377;&#30340;&#24037;&#20316;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#21644;&#23567;&#30340;&#23398;&#20064;&#38169;&#35823;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#23398;&#20064;&#38169;&#35823;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20869;&#22312;&#30340;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#24046;&#20943;&#23569;&#26041;&#27861;&#65292;&#38543;&#26426;&#24179;&#22343;&#26799;&#24230;&#31639;&#27861;&#65288;SAGA&#65289;&#21644;&#26080;&#24490;&#29615;&#38543;&#26426;&#26041;&#24046;&#20943;&#23569;&#26799;&#24230;&#65288;LSVRG&#65289;&#65292;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20998;&#25955;&#38543;&#26426;&#20248;&#21270;&#65292;&#20197;&#28040;&#38500;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#30340;&#36127;&#38754;&#25928;&#24212;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#20004;&#31181;&#26041;&#27861;&#26159;BRAVO-SAGA&#21644;BRAVO-LS&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Byzantine-robust stochastic optimization over a decentralized network, where every agent periodically communicates with its neighbors to exchange local models, and then updates its own local model by stochastic gradient descent (SGD). The performance of such a method is affected by an unknown number of Byzantine agents, which conduct adversarially during the optimization process. To the best of our knowledge, there is no existing work that simultaneously achieves a linear convergence speed and a small learning error. We observe that the learning error is largely dependent on the intrinsic stochastic gradient noise. Motivated by this observation, we introduce two variance reduction methods, stochastic average gradient algorithm (SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to Byzantine-robust decentralized stochastic optimization for eliminating the negative effect of the stochastic gradient noise. The two resulting methods, BRAVO-SAGA and BRAVO-LS
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05281</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#27169;&#22411;&#30740;&#31350;&#28798;&#23475;&#21709;&#24212;&#65306;&#20197;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season. (arXiv:2308.05281v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#28798;&#23475;&#21709;&#24212;&#23545;&#21463;&#24433;&#21709;&#30340;&#31038;&#21306;&#33267;&#20851;&#37325;&#35201;&#12290;&#24212;&#24613;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#22312;&#28798;&#23475;&#26399;&#38388;&#22312;&#20102;&#35299;&#31038;&#21306;&#25152;&#38754;&#20020;&#38382;&#39064;&#30340;&#21487;&#38752;&#21644;&#21450;&#26102;&#30340;&#25351;&#26631;&#19978;&#23558;&#21463;&#30410;&#20110;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#20016;&#23500;&#25968;&#25454;&#26469;&#28304;&#12290;&#31038;&#20132;&#23186;&#20307;&#21487;&#20197;&#21453;&#26144;&#20844;&#20247;&#20851;&#27880;&#21644;&#38656;&#27714;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#20197;&#20102;&#35299;&#19981;&#26029;&#28436;&#21464;&#30340;&#24773;&#20917;&#24182;&#20248;&#21270;&#36164;&#28304;&#37197;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#65288;BERT&#65289;&#20027;&#39064;&#24314;&#27169;&#23545;Twitter&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#26102;&#38388;-&#31354;&#38388;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;2020&#24180;&#32654;&#22269;&#35199;&#37096;&#28779;&#28798;&#23395;&#26399;&#38388;&#22312;&#19981;&#21516;&#22320;&#21306;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#8220;&#20581;&#24247;&#24433;&#21709;&#8221;&#65292;&#8220;&#25439;&#22833;&#8221;&#65292;&#8220;&#25764;&#31163;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#29702;&#35770;&#26469;&#25506;&#32034;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;&#32467;&#26524;&#28165;&#26224;&#22320;&#26174;&#31034;&#20102;&#20027;&#39064;&#20256;&#25773;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear re
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#20803;&#27169;&#24335;&#21644;&#20351;&#29992;&#22810;&#35270;&#22270;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#33719;&#24322;&#36136;&#20449;&#24687;&#24182;&#23398;&#20064;&#36328;&#24322;&#36136;&#22270;&#30340;&#20803;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.05275</link><description>&lt;p&gt;
&#36328;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-heterogeneity Graph Few-shot Learning. (arXiv:2308.05275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05275
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#20803;&#27169;&#24335;&#21644;&#20351;&#29992;&#22810;&#35270;&#22270;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#33719;&#24322;&#36136;&#20449;&#24687;&#24182;&#23398;&#20064;&#36328;&#24322;&#36136;&#22270;&#30340;&#20803;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#24322;&#36136;&#22270;&#20013;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#65292;&#24322;&#36136;&#22270;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#20174;&#28304;&#24322;&#36136;&#22270;&#20013;&#20016;&#23500;&#26631;&#35760;&#31867;&#25552;&#21462;&#30340;&#27867;&#21270;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#24322;&#36136;&#22270;&#20013;&#30340;&#23569;&#26631;&#35760;&#31867;&#26469;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#32771;&#34385;&#28304;&#21644;&#30446;&#26631;&#24322;&#36136;&#22270;&#20849;&#20139;&#19968;&#32452;&#22266;&#23450;&#30340;&#33410;&#28857;/&#36793;&#31867;&#22411;&#30340;&#24773;&#20917;&#65292;&#24573;&#30053;&#20102;&#26356;&#19968;&#33324;&#30340;&#36328;&#24322;&#36136;&#22270;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#24322;&#36136;&#22270;&#21487;&#20197;&#26377;&#19968;&#20010;&#19981;&#21516;&#30340;&#12289;&#38750;&#22266;&#23450;&#30340;&#33410;&#28857;/&#36793;&#31867;&#22411;&#38598;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26410;&#34987;&#25506;&#32034;&#30340;&#36328;&#24322;&#36136;&#22270;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;CGFL&#12290;&#22312;CGFL&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#21462;&#20803;&#27169;&#24335;&#26469;&#25429;&#33719;&#24322;&#36136;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#22270;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MHGN&#65289;&#26469;&#23398;&#20064;&#36328;&#24322;&#36136;&#22270;&#30340;&#20803;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33539;&#22260;&#23376;..
&lt;/p&gt;
&lt;p&gt;
In recent years, heterogeneous graph few-shot learning has been proposed to address the label sparsity issue in heterogeneous graphs (HGs), which contain various types of nodes and edges. The existing methods have achieved good performance by transferring generalized knowledge extracted from rich-labeled classes in source HG(s) to few-labeled classes in a target HG. However, these methods only consider the single-heterogeneity scenario where the source and target HGs share a fixed set of node/edge types, ignoring the more general scenario of cross-heterogeneity, where each HG can have a different and non-fixed set of node/edge types. To this end, we focus on the unexplored cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity Graph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patterns to capture heterogeneous information and propose a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose a sco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;DGGI&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#27169;&#25311;&#20114;&#32852;&#32593;&#20013;&#33258;&#27835;&#31995;&#32479;&#20869;&#30340;&#22270;&#30340;&#23646;&#24615;&#65292;&#22914;&#20013;&#24515;&#24615;&#12289;&#32858;&#31867;&#24615;&#12289;&#21516;&#36136;&#24615;&#20197;&#21450;&#33410;&#28857;&#24230;&#37327;&#12290;&#35813;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#20114;&#32852;&#32593;&#25299;&#25169;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.05254</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#27835;&#31995;&#32479;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Data-driven Intra-Autonomous Systems Graph Generator. (arXiv:2308.05254v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;DGGI&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#27169;&#25311;&#20114;&#32852;&#32593;&#20013;&#33258;&#27835;&#31995;&#32479;&#20869;&#30340;&#22270;&#30340;&#23646;&#24615;&#65292;&#22914;&#20013;&#24515;&#24615;&#12289;&#32858;&#31867;&#24615;&#12289;&#21516;&#36136;&#24615;&#20197;&#21450;&#33410;&#28857;&#24230;&#37327;&#12290;&#35813;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#20114;&#32852;&#32593;&#25299;&#25169;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;DGGI&#65292;&#29992;&#20110;&#34920;&#31034;&#20114;&#32852;&#32593;&#20013;&#33258;&#27835;&#31995;&#32479;&#65288;AS&#65289;&#20869;&#30340;&#22270;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;Internet Topology Data Kit&#65288;ITDK&#65289;&#39033;&#30446;&#30340;&#30495;&#23454;&#33258;&#27835;&#31995;&#32479;&#22270;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;Internet Graphs&#65288;IGraphs&#65289;&#12290;&#21019;&#24314;IGraphs&#37319;&#29992;&#20102;Filtered Recurrent Multi-level&#65288;FRM&#65289;&#31639;&#27861;&#36827;&#34892;&#31038;&#21306;&#25552;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DGGI&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#21487;&#20197;&#20934;&#30830;&#22320;&#20877;&#29616;&#20013;&#24515;&#24615;&#12289;&#32858;&#31867;&#24615;&#12289;&#21516;&#36136;&#24615;&#21644;&#33410;&#28857;&#24230;&#37327;&#30340;&#29305;&#24615;&#12290;DGGI&#29983;&#25104;&#22120;&#20248;&#20110;&#29616;&#26377;&#30340;&#20114;&#32852;&#32593;&#25299;&#25169;&#29983;&#25104;&#22120;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#23545;&#20110;&#21516;&#36136;&#24615;&#12289;&#20013;&#20171;&#24230;&#12289;&#32858;&#31867;&#24615;&#21644;&#33410;&#28857;&#24230;&#37327;&#65292;DGGI&#22312;&#26368;&#22823;&#22343;&#21248;&#24046;&#24322;&#24230;&#65288;MMD&#65289;&#25351;&#26631;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;84.4%&#12289;95.1%&#12289;97.9%&#21644;94.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep-learning based generator of synthetic graphs that represent intra-Autonomous System (AS) in the Internet, named Deep-generative graphs for the Internet (DGGI). It also presents a novel massive dataset of real intra-AS graphs extracted from the project Internet Topology Data Kit (ITDK), called Internet Graphs (IGraphs). To create IGraphs, the Filtered Recurrent Multi-level (FRM) algorithm for community extraction was developed. It is shown that DGGI creates synthetic graphs which accurately reproduce the properties of centrality, clustering, assortativity, and node degree. The DGGI generator overperforms existing Internet topology generators. On average, DGGI improves the Maximum Mean Discrepancy (MMD) metric 84.4%, 95.1%, 97.9%, and 94.7% for assortativity, betweenness, clustering, and node degree, respectively.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.05239</link><description>&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS).
&lt;/p&gt;
&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS). (arXiv:2308.05239v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05239
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#36719;&#20214;&#12289;&#31995;&#32479;&#21644;&#20225;&#19994;&#30340;&#26550;&#26500;&#26694;&#26550;&#12290;&#23427;&#20204;&#35782;&#21035;&#20102;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#24182;&#23450;&#20041;&#20102;&#26550;&#26500;&#30340;&#35266;&#28857;&#21644;&#35270;&#22270;&#65292;&#20197;&#26694;&#26550;&#21644;&#35299;&#20915;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20851;&#27880;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#26550;&#26500;&#26694;&#26550;&#20013;&#65292;&#23578;&#26410;&#21253;&#25324;&#19982;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#25968;&#25454;&#24037;&#31243;&#24072;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26410;&#33021;&#35299;&#20915;&#21709;&#24212;&#25968;&#25454;&#31185;&#23398;&#31038;&#21306;&#20851;&#27880;&#30340;&#26550;&#26500;&#35270;&#28857;&#21644;&#35270;&#22270;&#12290;&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#36866;&#29992;&#20110;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#30340;&#26550;&#26500;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#26222;&#36941;&#23384;&#22312;&#19988;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;CPS&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#32452;&#36866;&#24212;CPS&#39640;&#25928;&#24320;&#21457;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#20248;&#28857;&#26631;&#20934;&#65292;&#21363;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;CPS&#30340;&#26631;&#20934;&#65292;
&lt;/p&gt;
&lt;p&gt;
Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#22235;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#27169;&#22411;&#22312;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#26368;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#27450;&#35784;&#26816;&#27979;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.05237</link><description>&lt;p&gt;
&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#65306;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Financial Fraud Detection: A Comparative Study of Quantum Machine Learning Models. (arXiv:2308.05237v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#22235;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#27169;&#22411;&#22312;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#26368;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#27450;&#35784;&#26816;&#27979;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#38024;&#23545;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#36827;&#34892;&#20102;&#22235;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#65292;&#27450;&#35784;&#21644;&#38750;&#27450;&#35784;&#31867;&#21035;&#30340;F1&#20998;&#25968;&#22343;&#20026;0.98&#12290;&#20854;&#20182;&#27169;&#22411;&#22914;&#21464;&#20998;&#37327;&#23376;&#20998;&#31867;&#22120;&#12289;&#20272;&#35745;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37319;&#26679;&#22120;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#25512;&#21160;&#20102;QML&#22312;&#37329;&#34701;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#23427;&#20204;&#23384;&#22312;&#19968;&#23450;&#30340;&#38480;&#21046;&#65292;&#20294;&#25152;&#24471;&#21040;&#30340;&#27934;&#23519;&#20026;&#26410;&#26469;&#30340;&#22686;&#24378;&#21644;&#20248;&#21270;&#31574;&#30053;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#23384;&#22312;&#65292;&#21253;&#25324;&#38656;&#35201;&#26356;&#39640;&#25928;&#30340;&#37327;&#23376;&#31639;&#27861;&#21644;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25991;&#31456;&#25552;&#20379;&#20102;&#20811;&#26381;&#24403;&#21069;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#27450;&#35784;&#26816;&#27979;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#23545;&#20854;&#26410;&#26469;&#21457;&#23637;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, a comparative study of four Quantum Machine Learning (QML) models was conducted for fraud detection in finance. We proved that the Quantum Support Vector Classifier model achieved the highest performance, with F1 scores of 0.98 for fraud and non-fraud classes. Other models like the Variational Quantum Classifier, Estimator Quantum Neural Network (QNN), and Sampler QNN demonstrate promising results, propelling the potential of QML classification for financial applications. While they exhibit certain limitations, the insights attained pave the way for future enhancements and optimisation strategies. However, challenges exist, including the need for more efficient Quantum algorithms and larger and more complex datasets. The article provides solutions to overcome current limitations and contributes new insights to the field of Quantum Machine Learning in fraud detection, with important implications for its future development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SGU-MLP&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;MLPs&#21644;&#31354;&#38388;&#38376;&#25511;&#21333;&#20803;&#65288;SGUs&#65289;&#26469;&#31934;&#30830;&#22320;&#36827;&#34892;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#32472;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SGU-MLP&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#20110;CNN&#21644;CNN-ViT&#27169;&#22411;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.05235</link><description>&lt;p&gt;
&#31354;&#38388;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#29992;&#20110;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#32472;&#22270;
&lt;/p&gt;
&lt;p&gt;
Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping. (arXiv:2308.05235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SGU-MLP&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;MLPs&#21644;&#31354;&#38388;&#38376;&#25511;&#21333;&#20803;&#65288;SGUs&#65289;&#26469;&#31934;&#30830;&#22320;&#36827;&#34892;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#32472;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SGU-MLP&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#20110;CNN&#21644;CNN-ViT&#27169;&#22411;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29305;&#24449;&#30340;&#20998;&#23618;&#25552;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#26368;&#36817;&#22312;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#20110;CNN&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#35201;&#21457;&#25381;&#23427;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;ViTs&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#24403;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;&#30446;&#21069;&#30340;&#20808;&#36827;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLPs&#65289;&#21487;&#20197;&#20316;&#20026;&#28145;&#24230;CNN&#21644;ViTs&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SGU-MLP&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#21033;&#29992;MLPs&#21644;&#31354;&#38388;&#38376;&#25511;&#21333;&#20803;&#65288;SGUs&#65289;&#36827;&#34892;&#31934;&#30830;&#22303;&#22320;&#21033;&#29992;&#22303;&#22320;&#35206;&#30422;&#65288;LULC&#65289;&#32472;&#22270;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#24320;&#21457;&#30340;SGU-MLP&#20998;&#31867;&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#20110;CNN&#21644;CNN-ViT&#27169;&#22411;&#65288;&#21253;&#25324;HybridSN&#12289;ResNet&#12289;iFormer&#12289;EfficientFormer&#21644;CoAtNet&#65289;&#19978;&#20248;&#21183;&#26126;&#26174;&#12290;&#25152;&#25552;&#20986;&#30340;SGU-MLP&#31639;&#27861;&#36890;&#36807;&#22312;&#32654;&#22269;&#20241;&#26031;&#39039;&#36827;&#34892;&#30340;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) are models that are utilized extensively for the hierarchical extraction of features. Vision transformers (ViTs), through the use of a self-attention mechanism, have recently achieved superior modeling of global contextual information compared to CNNs. However, to realize their image classification strength, ViTs require substantial training datasets. Where the available training data are limited, current advanced multi-layer perceptrons (MLPs) can provide viable alternatives to both deep CNNs and ViTs. In this paper, we developed the SGU-MLP, a learning algorithm that effectively uses both MLPs and spatial gating units (SGUs) for precise land use land cover (LULC) mapping. Results illustrated the superiority of the developed SGU-MLP classification algorithm over several CNN and CNN-ViT-based models, including HybridSN, ResNet, iFormer, EfficientFormer and CoAtNet. The proposed SGU-MLP algorithm was tested through three experiments in Houston, USA, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21033;&#29992;&#36793;&#32536;&#21644;&#20113;&#31471;&#25216;&#26415;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#12290;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#19981;&#21516;&#30340;&#22806;&#37096;&#21270;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#30828;&#20214;&#21644;&#32593;&#32476;&#27169;&#25311;&#36827;&#34892;&#27604;&#36739;&#65292;&#25214;&#21040;&#20102;&#26435;&#34913;&#39044;&#27979;&#36136;&#37327;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05234</link><description>&lt;p&gt;
&#21033;&#29992;&#36793;&#32536;&#21644;&#20113;&#31471;&#25216;&#26415;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving. (arXiv:2308.05234v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21033;&#29992;&#36793;&#32536;&#21644;&#20113;&#31471;&#25216;&#26415;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#12290;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#19981;&#21516;&#30340;&#22806;&#37096;&#21270;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#30828;&#20214;&#21644;&#32593;&#32476;&#27169;&#25311;&#36827;&#34892;&#27604;&#36739;&#65292;&#25214;&#21040;&#20102;&#26435;&#34913;&#39044;&#27979;&#36136;&#37327;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#24863;&#30693;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#22240;&#20026;&#24863;&#30693;&#27169;&#22359;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#20250;&#24433;&#21709;&#26680;&#24515;&#39550;&#39542;&#20915;&#31574;&#12290;&#23454;&#26102;&#24863;&#30693;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#19968;&#22823;&#25361;&#25112;&#22312;&#20110;&#22312;&#26816;&#27979;&#36136;&#37327;&#21644;&#24310;&#36831;&#20043;&#38388;&#25214;&#21040;&#26368;&#20339;&#26435;&#34913;&#12290;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23454;&#26102;&#24863;&#30693;&#65292;&#24517;&#39035;&#32771;&#34385;&#21040;&#35745;&#31639;&#21644;&#21151;&#32791;&#26041;&#38754;&#30340;&#20027;&#35201;&#38480;&#21046;&#12290;&#36739;&#22823;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#24448;&#24448;&#33021;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#65292;&#20294;&#22312;&#36816;&#34892;&#26102;&#20063;&#26356;&#24930;&#12290;&#30001;&#20110;&#26368;&#20934;&#30830;&#30340;&#26816;&#27979;&#22120;&#26080;&#27861;&#22312;&#26412;&#22320;&#23454;&#26102;&#36816;&#34892;&#65292;&#25105;&#20204;&#30740;&#31350;&#23558;&#35745;&#31639;&#22806;&#37096;&#21270;&#21040;&#36793;&#32536;&#21644;&#20113;&#24179;&#21488;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#24179;&#21488;&#36164;&#28304;&#21463;&#38480;&#36739;&#23569;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#19981;&#21516;&#30340;&#22806;&#37096;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#30828;&#20214;&#21644;&#32593;&#32476;&#27169;&#25311;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#39044;&#27979;&#36136;&#37327;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#30001;&#20110;&#36890;&#36807;&#32593;&#32476;&#20256;&#36865;&#21407;&#22987;&#24103;&#30340;&#23454;&#29616;&#23384;&#22312;&#22256;&#38590;&#21644;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21387;&#32553;&#25216;&#26415;&#26469;&#38477;&#20302;&#25968;&#25454;&#20256;&#36755;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Environmental perception is a key element of autonomous driving because the information received from the perception module influences core driving decisions. An outstanding challenge in real-time perception for autonomous driving lies in finding the best trade-off between detection quality and latency. Major constraints on both computation and power have to be taken into account for real-time perception in autonomous vehicles. Larger object detection models tend to produce the best results, but are also slower at runtime. Since the most accurate detectors cannot run in real-time locally, we investigate the possibility of offloading computation to edge and cloud platforms, which are less resource-constrained. We create a synthetic dataset to train object detection models and evaluate different offloading strategies. Using real hardware and network simulations, we compare different trade-offs between prediction quality and end-to-end delay. Since sending raw frames over the network impl
&lt;/p&gt;</description></item><item><title>SegMatch&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#20266;&#26631;&#31614;&#26469;&#20943;&#23569;&#26114;&#36149;&#30340;&#27880;&#37322;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#24369;&#22686;&#24378;&#21644;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#23454;&#29616;&#26080;&#30417;&#30563;&#25439;&#22833;&#30340;&#26045;&#21152;&#12290;</title><link>http://arxiv.org/abs/2308.05232</link><description>&lt;p&gt;
SegMatch: &#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SegMatch: A semi-supervised learning method for surgical instrument segmentation. (arXiv:2308.05232v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05232
&lt;/p&gt;
&lt;p&gt;
SegMatch&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#20266;&#26631;&#31614;&#26469;&#20943;&#23569;&#26114;&#36149;&#30340;&#27880;&#37322;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#24369;&#22686;&#24378;&#21644;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#23454;&#29616;&#26080;&#30417;&#30563;&#25439;&#22833;&#30340;&#26045;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#34987;&#35748;&#20026;&#26159;&#25552;&#20379;&#20808;&#36827;&#25163;&#26415;&#36741;&#21161;&#21644;&#25913;&#21892;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#30340;&#20851;&#38190;&#25163;&#27573;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SegMatch&#65292;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#26114;&#36149;&#27880;&#37322;&#23545;&#33145;&#33108;&#38236;&#21644;&#26426;&#22120;&#20154;&#25163;&#26415;&#22270;&#20687;&#30340;&#38656;&#27714;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;SegMatch&#22522;&#20110;FixMatch&#65292;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#27969;&#31243;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#20026;&#20998;&#21106;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;SegMatch&#20013;&#65292;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#36827;&#34892;&#24369;&#22686;&#24378;&#65292;&#24182;&#36890;&#36807;&#20998;&#21106;&#27169;&#22411;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#20197;&#23545;&#39640;&#32622;&#20449;&#24230;&#20687;&#32032;&#19978;&#30340;&#23545;&#25239;&#22686;&#24378;&#22270;&#20687;&#30340;&#27169;&#22411;&#36755;&#20986;&#26045;&#21152;&#26080;&#30417;&#30563;&#25439;&#22833;&#12290;&#25105;&#20204;&#38024;&#23545;&#20998;&#21106;&#20219;&#21153;&#30340;&#35843;&#25972;&#36824;&#21253;&#25324;&#20180;&#32454;&#32771;&#34385;&#25152;&#20381;&#36182;&#30340;&#22686;&#24378;&#20989;&#25968;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical instrument segmentation is recognised as a key enabler to provide advanced surgical assistance and improve computer assisted interventions. In this work, we propose SegMatch, a semi supervised learning method to reduce the need for expensive annotation for laparoscopic and robotic surgical images. SegMatch builds on FixMatch, a widespread semi supervised classification pipeline combining consistency regularization and pseudo labelling, and adapts it for the purpose of segmentation. In our proposed SegMatch, the unlabelled images are weakly augmented and fed into the segmentation model to generate a pseudo-label to enforce the unsupervised loss against the output of the model for the adversarial augmented image on the pixels with a high confidence score. Our adaptation for segmentation tasks includes carefully considering the equivariance and invariance properties of the augmentation functions we rely on. To increase the relevance of our augmentations, we depart from using only
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#21644;&#36890;&#29992;&#30340;&#26041;&#26696;&#65292;&#20351;&#29992;&#39281;&#21644;&#21560;&#25910;&#20307;&#20316;&#20026;&#28608;&#27963;&#21333;&#20803;&#30340;&#20809;&#23398;&#20803;&#20214;&#26469;&#23454;&#29616;&#20809;&#23398;&#21453;&#21521;&#20256;&#25773;&#65292;&#20197;&#35299;&#20915;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05226</link><description>&lt;p&gt;
&#20351;&#29992;&#31471;&#21040;&#31471;&#20809;&#32972;&#21521;&#20256;&#25773;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training neural networks with end-to-end optical backpropagation. (arXiv:2308.05226v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#21644;&#36890;&#29992;&#30340;&#26041;&#26696;&#65292;&#20351;&#29992;&#39281;&#21644;&#21560;&#25910;&#20307;&#20316;&#20026;&#28608;&#27963;&#21333;&#20803;&#30340;&#20809;&#23398;&#20803;&#20214;&#26469;&#23454;&#29616;&#20809;&#23398;&#21453;&#21521;&#20256;&#25773;&#65292;&#20197;&#35299;&#20915;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#26159;&#19979;&#19968;&#20195;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#30828;&#20214;&#30340;&#20196;&#20154;&#20852;&#22859;&#30340;&#36884;&#24452;&#65292;&#25215;&#35834;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#33021;&#37327;&#25928;&#29575;&#19978;&#25552;&#20379;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#25512;&#29702;&#65292;&#32780;&#19988;&#35757;&#32451;&#30340;&#35745;&#31639;&#20063;&#38656;&#35201;&#20197;&#20809;&#23398;&#26041;&#24335;&#23454;&#29616;&#12290;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20027;&#35201;&#31639;&#27861;&#26159;&#21453;&#21521;&#20256;&#25773;&#65292;&#20854;&#35745;&#31639;&#39034;&#24207;&#19982;&#25512;&#29702;&#30340;&#20449;&#24687;&#27969;&#30456;&#21453;&#12290;&#23613;&#31649;&#22312;&#25968;&#23383;&#35745;&#31639;&#26426;&#20013;&#24456;&#31616;&#21333;&#65292;&#20294;&#20809;&#23398;&#23454;&#29616;&#21453;&#21521;&#20256;&#25773;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#23454;&#29616;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20809;&#23398;&#20803;&#20214;&#30340;&#20914;&#31361;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#29992;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#21644;&#36890;&#29992;&#30340;&#26041;&#26696;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#12290;&#39281;&#21644;&#21560;&#25910;&#20307;&#34987;&#29992;&#20110;&#25198;&#28436;&#28608;&#27963;&#21333;&#20803;&#30340;&#35282;&#33394;&#65292;&#24182;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optics is an exciting route for the next generation of computing hardware for machine learning, promising several orders of magnitude enhancement in both computational speed and energy efficiency. However, to reach the full capacity of an optical neural network it is necessary that the computing not only for the inference, but also for the training be implemented optically. The primary algorithm for training a neural network is backpropagation, in which the calculation is performed in the order opposite to the information flow for inference. While straightforward in a digital computer, optical implementation of backpropagation has so far remained elusive, particularly because of the conflicting requirements for the optical element that implements the nonlinear activation function. In this work, we address this challenge for the first time with a surprisingly simple and generic scheme. Saturable absorbers are employed for the role of the activation units, and the required properties are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#36716;&#25442;&#22120;&#20013;&#35782;&#21035;&#25991;&#26412;&#26174;&#33879;&#24615;&#30340;&#31574;&#30053;&#65292;&#24182;&#36866;&#24212;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.05219</link><description>&lt;p&gt;
&#35821;&#35328;&#36716;&#25442;&#22120;&#20013;&#30340;&#23618;&#26174;&#33879;&#24615;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding Layer Saliency in Language Transformers. (arXiv:2308.05219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#36716;&#25442;&#22120;&#20013;&#35782;&#21035;&#25991;&#26412;&#26174;&#33879;&#24615;&#30340;&#31574;&#30053;&#65292;&#24182;&#36866;&#24212;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#20013;&#35782;&#21035;&#25991;&#26412;&#26174;&#33879;&#24615;&#30340;&#31574;&#30053;&#12290;&#22312;&#35270;&#35273;&#32593;&#32476;&#20013;&#65292;&#26174;&#33879;&#24615;&#24448;&#24448;&#36890;&#36807;&#21367;&#31215;&#23618;&#33258;&#28982;&#22320;&#36827;&#34892;&#23450;&#20301;&#65292;&#28982;&#32780;&#65292;&#22312;&#29992;&#20110;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#30340;&#29616;&#20195;transformer-stack&#32593;&#32476;&#20013;&#65292;&#24182;&#38750;&#22914;&#27492;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#32593;&#32476;&#36866;&#24212;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#27599;&#23618;&#35821;&#20041;&#19968;&#33268;&#24615;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#22810;&#31181;&#25991;&#26412;&#26174;&#33879;&#24615;&#26041;&#27861;&#30456;&#27604;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#35775;&#38382;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#19988;&#35745;&#31639;&#25928;&#29575;&#30456;&#23545;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Conformer&#30340;&#21333;&#22768;&#36947;&#38899;&#39057;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;TS-ASR&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#23884;&#20837;&#27169;&#22359;&#12289;&#25513;&#27169;&#27169;&#22359;&#21644;ASR&#27169;&#22359;&#23454;&#29616;&#23545;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35782;&#21035;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;TS-ASR&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.05218</link><description>&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#21333;&#22768;&#36947;&#38899;&#39057;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Conformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio. (arXiv:2308.05218v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Conformer&#30340;&#21333;&#22768;&#36947;&#38899;&#39057;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;TS-ASR&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#23884;&#20837;&#27169;&#22359;&#12289;&#25513;&#27169;&#27169;&#22359;&#21644;ASR&#27169;&#22359;&#23454;&#29616;&#23545;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35782;&#21035;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;TS-ASR&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CONF-TSASR&#65292;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#31471;&#21040;&#31471;&#26102;&#38388;-&#39057;&#29575;&#22495;&#26550;&#26500;&#65292;&#29992;&#20110;&#21333;&#22768;&#36947;&#30446;&#26631;&#35828;&#35805;&#32773;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;TS-ASR&#65289;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#22522;&#20110;TitaNet&#30340;&#35828;&#35805;&#32773;&#23884;&#20837;&#27169;&#22359;&#65292;&#22522;&#20110;Conformer&#30340;&#25513;&#27169;&#20197;&#21450;ASR&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21516;&#26102;&#36827;&#34892;&#65292;&#20197;&#36716;&#24405;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#65292;&#21516;&#26102;&#24573;&#30053;&#20854;&#20182;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23610;&#24230;&#19981;&#21464;&#30340;&#35889;&#22270;&#37325;&#26500;&#25439;&#22833;&#65292;&#20197;&#20419;&#20351;&#27169;&#22411;&#26356;&#22909;&#22320;&#23558;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35889;&#22270;&#19982;&#28151;&#21512;&#22768;&#38899;&#20998;&#31163;&#24320;&#26469;&#12290;&#22312;WSJ0-2mix-extr&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#35789;&#38169;&#35823;&#29575;&#65288;TS-WER&#65289;&#65288;4.2%&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#39318;&#27425;&#25253;&#21578;&#20102;WSJ0-3mix-extr&#65288;12.4%&#65289;&#12289;LibriSpeech2Mix&#65288;4.2%&#65289;&#21644;LibriSpeech3Mix&#65288;7.6%&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;TS-WER&#65292;&#24314;&#31435;&#20102;TS-ASR&#30340;&#26032;&#22522;&#20934;&#12290;&#25105;&#20204;&#23558;&#36890;&#36807;NVIDIA NeMo&#24037;&#20855;&#21253;&#24320;&#28304;&#25552;&#20379;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose CONF-TSASR, a non-autoregressive end-to-end time-frequency domain architecture for single-channel target-speaker automatic speech recognition (TS-ASR). The model consists of a TitaNet based speaker embedding module, a Conformer based masking as well as ASR modules. These modules are jointly optimized to transcribe a target-speaker, while ignoring speech from other speakers. For training we use Connectionist Temporal Classification (CTC) loss and introduce a scale-invariant spectrogram reconstruction loss to encourage the model better separate the target-speaker's spectrogram from mixture. We obtain state-of-the-art target-speaker word error rate (TS-WER) on WSJ0-2mix-extr (4.2%). Further, we report for the first time TS-WER on WSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%) datasets, establishing new benchmarks for TS-ASR. The proposed model will be open-sourced through NVIDIA NeMo toolkit.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#27169;&#22411;&#22312;&#29983;&#25104;&#21333;&#20010;&#36712;&#36857;&#26102;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#26576;&#20123;&#36890;&#24120;&#34987;&#35748;&#20026;&#26377;&#29992;&#30340;&#29305;&#24449;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.05194</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving. (arXiv:2308.05194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#27169;&#22411;&#22312;&#29983;&#25104;&#21333;&#20010;&#36712;&#36857;&#26102;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#26576;&#20123;&#36890;&#24120;&#34987;&#35748;&#20026;&#26377;&#29992;&#30340;&#29305;&#24449;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#19982;&#24120;&#36895;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#35780;&#20272;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;ETH/UCY&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#25253;&#21578;&#20102;&#24179;&#22343;&#20301;&#31227;&#35823;&#24046;&#65288;ADE&#65289;&#21644;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#12290;&#20026;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#30340;&#35201;&#27714;&#65292;&#23545;&#21021;&#22987;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#21382;&#21490;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24314;&#31435;&#26356;&#22909;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#36824;&#27979;&#37327;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#20197;&#35780;&#20272;&#38754;&#23545;&#19981;&#21516;&#25968;&#37327;&#20195;&#29702;&#26102;&#27599;&#20010;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#21333;&#20010;&#36712;&#36857;&#26102;&#65292;&#31616;&#21333;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#26576;&#20123;&#36890;&#24120;&#34987;&#35748;&#20026;&#26377;&#29992;&#30340;&#29305;&#24449;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the state of the art in the field of pedestrian trajectory prediction is evaluated alongside the constant velocity model (CVM) with respect to its applicability in autonomous vehicles. The evaluation is conducted on the widely-used ETH/UCY dataset where the Average Displacement Error (ADE) and the Final Displacement Error (FDE) are reported. To align with requirements in real-world applications, modifications are made to the input features of the initially proposed models. An ablation study is conducted to examine the influence of the observed motion history on the prediction performance, thereby establishing a better understanding of its impact. Additionally, the inference time of each model is measured to evaluate the scalability of each model when confronted with varying amounts of agents. The results demonstrate that simple models remain competitive when generating single trajectories, and certain features commonly thought of as useful have little impact on the overa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#34920;&#31034;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;&#65292;&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#20197;&#21450;&#29992;&#20110;&#24314;&#27169;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#21644;&#26102;&#38388;&#39046;&#22495;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.05189</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#21270;&#34920;&#31034;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding. (arXiv:2308.05189v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#34920;&#31034;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;&#65292;&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#20197;&#21450;&#29992;&#20110;&#24314;&#27169;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#21644;&#26102;&#38388;&#39046;&#22495;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#21338;&#22763;&#35770;&#25991;&#30740;&#31350;&#21644;&#24320;&#21457;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#34920;&#31034;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#24314;&#27169;&#19978;&#34892;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#24182;&#26368;&#32456;&#29992;&#20110;&#24314;&#27169;&#26102;&#38388;&#39046;&#22495;&#20013;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This PhD. Thesis concerns the study and development of hierarchical representations for spatio-temporal visual attention modeling and understanding in video sequences. More specifically, we propose two computational models for visual attention. First, we present a generative probabilistic model for context-aware visual attention modeling and understanding. Secondly, we develop a deep network architecture for visual attention modeling, which first estimates top-down spatio-temporal visual attention, and ultimately serves for modeling attention in the temporal domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;EEG&#25968;&#25454;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#20116;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#36890;&#36807;&#24212;&#29992;&#22810;&#26679;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#20026;&#30315;&#30187;&#30149;&#20154;&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#25252;&#29702;&#25552;&#20379;&#20102;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.05176</link><description>&lt;p&gt;
&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#27604;&#36739;&#20998;&#26512;&#65306;&#25506;&#32034;&#22810;&#26679;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models. (arXiv:2308.05176v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;EEG&#25968;&#25454;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#20116;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#36890;&#36807;&#24212;&#29992;&#22810;&#26679;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#20026;&#30315;&#30187;&#30149;&#20154;&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#25252;&#29702;&#25552;&#20379;&#20102;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#20854;&#29305;&#28857;&#26159;&#21453;&#22797;&#21457;&#20316;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#30315;&#30187;&#21457;&#20316;&#65292;&#38656;&#35201;&#20934;&#30830;&#30340;&#39044;&#27979;&#20197;&#23454;&#26045;&#26377;&#25928;&#30340;&#31649;&#29702;&#21644;&#24739;&#32773;&#25252;&#29702;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#65292;&#20197;&#21450;&#20854;&#22312;&#30315;&#30187;&#21457;&#20316;&#26399;&#38388;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#33041;&#27963;&#21160;&#27934;&#23519;&#30340;&#33021;&#21147;&#65292;&#20351;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#25104;&#20026;&#30456;&#20851;&#30740;&#31350;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;EEG&#25968;&#25454;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#20116;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#20998;&#26512; - &#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#65292;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#65292;&#26497;&#31471;&#26862;&#26519;&#65288;ET&#65289;&#65292;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#21644;&#26799;&#24230;&#25552;&#21319;&#65288;GB&#65289;&#12290;&#25968;&#25454;&#38598;&#32463;&#36807;&#20102;&#32454;&#33268;&#30340;&#39044;&#22788;&#29702;&#65292; &#21253;&#25324;&#28165;&#29702;&#12289;&#24402;&#19968;&#21270;&#12289;&#24322;&#24120;&#20540;&#22788;&#29702;&#21644;&#36807;&#37319;&#26679;&#65292;&#20197;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#21644;&#20415;&#20110;&#20934;&#30830;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#20123;&#39044;&#22788;&#29702;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is a prevalent neurological disorder characterized by recurrent and unpredictable seizures, necessitating accurate prediction for effective management and patient care. Application of machine learning (ML) on electroencephalogram (EEG) recordings, along with its ability to provide valuable insights into brain activity during seizures, is able to make accurate and robust seizure prediction an indispensable component in relevant studies. In this research, we present a comprehensive comparative analysis of five machine learning models - Random Forest (RF), Decision Tree (DT), Extra Trees (ET), Logistic Regression (LR), and Gradient Boosting (GB) - for the prediction of epileptic seizures using EEG data. The dataset underwent meticulous preprocessing, including cleaning, normalization, outlier handling, and oversampling, ensuring data quality and facilitating accurate model training. These preprocessing techniques played a crucial role in enhancing the models' performance. The res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24369;&#26631;&#31614;&#36827;&#34892;&#25193;&#23637;&#23556;&#30005;&#26143;&#31995;&#24418;&#24577;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#38477;&#20302;&#20102;&#26631;&#27880;&#25104;&#26412;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05166</link><description>&lt;p&gt;
&#20351;&#29992;&#24369;&#26631;&#31614;&#36827;&#34892;&#25193;&#23637;&#23556;&#30005;&#26143;&#31995;&#24418;&#24577;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels. (arXiv:2308.05166v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24369;&#26631;&#31614;&#36827;&#34892;&#25193;&#23637;&#23556;&#30005;&#26143;&#31995;&#24418;&#24577;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#38477;&#20302;&#20102;&#26631;&#27880;&#25104;&#26412;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20351;&#29992;&#65292;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#23545;&#20855;&#26377;&#22810;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#22797;&#26434;&#23556;&#30005;&#26143;&#31995;&#36827;&#34892;&#20687;&#32032;&#32423;&#26631;&#31614;&#26631;&#27880;&#30340;&#25104;&#26412;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#23556;&#30005;&#26143;&#31995;&#30340;&#24369;&#31867;&#21035;&#32423;&#21035;&#26631;&#31614;&#35757;&#32451;&#24471;&#21040;&#31867;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#12290;&#28982;&#21518;&#20351;&#29992;&#20687;&#32032;&#38388;&#20851;&#31995;&#32593;&#32476;&#65288;IRNet&#65289;&#36827;&#19968;&#27493;&#20248;&#21270;&#36825;&#20123;CAMs&#65292;&#24471;&#21040;&#23556;&#30005;&#26143;&#31995;&#30340;&#23454;&#20363;&#20998;&#21106;&#25513;&#33180;&#20197;&#21450;&#23427;&#20204;&#30340;&#32418;&#22806;&#20027;&#26426;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;&#28595;&#22823;&#21033;&#20122;&#24179;&#26041;&#20844;&#37324;&#38453;&#65288;ASKAP&#65289;&#26395;&#36828;&#38236;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#28436;&#21270;&#23431;&#23449;&#22270;&#65288;EMU&#65289;&#35797;&#39564;&#65292;&#35813;&#35797;&#39564;&#35206;&#30422;&#20102;270&#24179;&#26041;&#24230;&#30340;&#22825;&#21306;&#65292;RMS&#28789;&#25935;&#24230;&#20026;25-35&#24494;Jy/beam&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24369;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22312;&#39044;&#27979;&#20687;&#32032;&#32423;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#25193;&#23637;&#23556;&#30005;&#36752;&#23556;&#30340;&#25513;&#33180;&#21644;&#32418;&#22806;&#20027;&#26426;&#26143;&#31995;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present work discusses the use of a weakly-supervised deep learning algorithm that reduces the cost of labelling pixel-level masks for complex radio galaxies with multiple components. The algorithm is trained on weak class-level labels of radio galaxies to get class activation maps (CAMs). The CAMs are further refined using an inter-pixel relations network (IRNet) to get instance segmentation masks over radio galaxies and the positions of their infrared hosts. We use data from the Australian Square Kilometre Array Pathfinder (ASKAP) telescope, specifically the Evolutionary Map of the Universe (EMU) Pilot Survey, which covered a sky area of 270 square degrees with an RMS sensitivity of 25-35 $\mu$Jy/beam. We demonstrate that weakly-supervised deep learning algorithms can achieve high accuracy in predicting pixel-level information, including masks for the extended radio emission encapsulating all galaxy components and the positions of the infrared host galaxies. We evaluate the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#22120;&#32593;&#32476;&#26469;&#27169;&#25311;&#22312;&#30495;&#23454;&#30340;3D&#22330;&#26223;&#20013;&#24102;&#26377;&#31227;&#21160;&#28304;&#30340;&#22768;&#38899;&#20256;&#25773;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#22768;&#38899;&#20256;&#25773;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#33033;&#20914;&#21709;&#24212;&#30340;&#31163;&#32447;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2308.05141</link><description>&lt;p&gt;
&#22312;&#21442;&#25968;&#21270;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#22120;&#22312;&#30495;&#23454;&#30340;&#20132;&#20114;&#24335;3D&#22330;&#26223;&#20013;&#27169;&#25311;&#22768;&#38899;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators. (arXiv:2308.05141v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#22120;&#32593;&#32476;&#26469;&#27169;&#25311;&#22312;&#30495;&#23454;&#30340;3D&#22330;&#26223;&#20013;&#24102;&#26377;&#31227;&#21160;&#28304;&#30340;&#22768;&#38899;&#20256;&#25773;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#22768;&#38899;&#20256;&#25773;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#33033;&#20914;&#21709;&#24212;&#30340;&#31163;&#32447;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#31227;&#21160;&#28304;&#30340;&#19977;&#32500;&#34394;&#25311;&#25151;&#38388;&#20013;&#36827;&#34892;&#22768;&#38899;&#20256;&#25773;&#27169;&#25311;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#34394;&#25311;/&#22686;&#24378;&#29616;&#23454;&#12289;&#28216;&#25103;&#38899;&#39057;&#21644;&#31354;&#38388;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#12290;&#36890;&#36807;&#27714;&#35299;&#27874;&#21160;&#26041;&#31243;&#65292;&#21487;&#20197;&#25551;&#36848;&#34893;&#23556;&#21644;&#24178;&#28041;&#31561;&#27874;&#21160;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#25968;&#20540;&#31163;&#25955;&#21270;&#26041;&#27861;&#27169;&#25311;&#28041;&#21450;&#25968;&#30334;&#20010;&#28304;&#21644;&#25509;&#25910;&#22120;&#20301;&#32622;&#30340;&#27874;&#21160;&#26041;&#31243;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20351;&#24471;&#20351;&#29992;&#31227;&#21160;&#28304;&#21050;&#28608;&#22768;&#22330;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#25805;&#20316;&#22120;&#32593;&#32476;&#26469;&#36924;&#36817;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#25805;&#20316;&#22120;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#22312;&#20855;&#26377;&#31227;&#21160;&#28304;&#30340;&#30495;&#23454;&#19977;&#32500;&#22768;&#23398;&#22330;&#26223;&#20013;&#30340;&#22768;&#38899;&#20256;&#25773;&#65292;&#36798;&#21040;&#27627;&#31186;&#32423;&#30340;&#35745;&#31639;&#12290;&#36890;&#36807;&#23398;&#20064;&#32039;&#20945;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#20026;&#25152;&#26377;&#30456;&#20851;&#30340;&#28304;/&#21548;&#32773;&#23545;&#35745;&#31639;&#21644;&#23384;&#20648;&#33033;&#20914;&#21709;&#24212;&#30340;&#31163;&#32447;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#21547;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20960;&#20309;&#24418;&#29366;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#21442;&#32771;&#32467;&#26524;&#36798;&#25104;&#20102;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of sound propagation simulations in $3$D virtual rooms with moving sources, which have applications in virtual/augmented reality, game audio, and spatial computing. Solutions to the wave equation can describe wave phenomena such as diffraction and interference. However, simulating them using conventional numerical discretization methods with hundreds of source and receiver positions is intractable, making stimulating a sound field with moving sources impractical. To overcome this limitation, we propose using deep operator networks to approximate linear wave-equation operators. This enables the rapid prediction of sound propagation in realistic 3D acoustic scenes with moving sources, achieving millisecond-scale computations. By learning a compact surrogate model, we avoid the offline calculation and storage of impulse responses for all relevant source/listener pairs. Our experiments, including various complex scene geometries, show good agreement with reference 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#25968;&#25454;&#27745;&#26579;&#23545;&#31934;&#31070;&#38556;&#30861;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#27745;&#26579;&#21487;&#33021;&#23548;&#33268;&#23545;&#30446;&#26631;&#31934;&#31070;&#38556;&#30861;&#30340;&#20998;&#31867;&#22120;&#35757;&#32451;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.05133</link><description>&lt;p&gt;
&#20998;&#26512;&#25968;&#25454;&#27745;&#26579;&#23545;&#31934;&#31070;&#38556;&#30861;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Effect of Data Impurity on the Detection Performances of Mental Disorders. (arXiv:2308.05133v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#25968;&#25454;&#27745;&#26579;&#23545;&#31934;&#31070;&#38556;&#30861;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#27745;&#26579;&#21487;&#33021;&#23548;&#33268;&#23545;&#30446;&#26631;&#31934;&#31070;&#38556;&#30861;&#30340;&#20998;&#31867;&#22120;&#35757;&#32451;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35782;&#21035;&#31934;&#31070;&#38556;&#30861;&#30340;&#20027;&#35201;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#20351;&#29992;&#20108;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#26159;&#20351;&#29992;&#20174;&#35775;&#35848;&#35774;&#32622;&#20013;&#33719;&#24471;&#30340;&#34892;&#20026;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#36825;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26469;&#33258;&#20855;&#20307;&#38556;&#30861;&#30340;&#20010;&#20307;&#30340;&#25968;&#25454;&#34987;&#24402;&#31867;&#20026;&#38451;&#24615;&#31867;&#65292;&#32780;&#25152;&#26377;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#21017;&#26500;&#25104;&#38452;&#24615;&#31867;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#20204;&#26222;&#36941;&#25215;&#35748;&#26576;&#20123;&#31934;&#31070;&#38556;&#30861;&#20855;&#26377;&#30456;&#20284;&#30340;&#30151;&#29366;&#65292;&#23548;&#33268;&#25910;&#38598;&#21040;&#30340;&#34892;&#20026;&#25968;&#25454;&#21253;&#21547;&#19982;&#22810;&#31181;&#38556;&#30861;&#30456;&#20851;&#30340;&#21508;&#31181;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#19982;&#30446;&#26631;&#31934;&#31070;&#38556;&#30861;&#30456;&#20851;&#30340;&#23646;&#24615;&#20063;&#21487;&#33021;&#23384;&#22312;&#20110;&#38452;&#24615;&#31867;&#20013;&#12290;&#36825;&#31181;&#25968;&#25454;&#27745;&#26579;&#21487;&#33021;&#23548;&#33268;&#23545;&#25152;&#20851;&#27880;&#30340;&#31934;&#31070;&#38556;&#30861;&#30340;&#20998;&#31867;&#22120;&#35757;&#32451;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#37325;&#24230;&#25233;&#37057;&#30151;&#65288;MDD&#65289;&#21644;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#65288;PTSD&#65289;&#30340;&#26816;&#27979;&#39046;&#22495;&#20013;&#35843;&#26597;&#20102;&#36825;&#20010;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary method for identifying mental disorders automatically has traditionally involved using binary classifiers. These classifiers are trained using behavioral data obtained from an interview setup. In this training process, data from individuals with the specific disorder under consideration are categorized as the positive class, while data from all other participants constitute the negative class. In practice, it is widely recognized that certain mental disorders share similar symptoms, causing the collected behavioral data to encompass a variety of attributes associated with multiple disorders. Consequently, attributes linked to the targeted mental disorder might also be present within the negative class. This data impurity may lead to sub-optimal training of the classifier for a mental disorder of interest. In this study, we investigate this hypothesis in the context of major depressive disorder (MDD) and post-traumatic stress disorder detection (PTSD). The results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20083;&#33146;&#32452;&#32455;&#23548;&#33268;&#32954;&#37096;&#26333;&#20809;&#19981;&#36275;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#35760;&#24405;&#20998;&#24067;&#20559;&#26012;&#21644;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05129</link><description>&lt;p&gt;
&#24615;&#21035;&#29983;&#29702;&#24046;&#24322;&#26159;&#23548;&#33268;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#24615;&#21035;&#20559;&#35265;&#30340;&#21407;&#22240;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Sex-based Physiological Differences the Cause of Gender Bias for Chest X-ray Diagnosis?. (arXiv:2308.05129v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20083;&#33146;&#32452;&#32455;&#23548;&#33268;&#32954;&#37096;&#26333;&#20809;&#19981;&#36275;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#35760;&#24405;&#20998;&#24067;&#20559;&#26012;&#21644;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#35780;&#20272;&#20102;&#21307;&#23398;&#39046;&#22495;AI&#31639;&#27861;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#26159;&#39044;&#27979;&#24615;&#33021;&#24046;&#24322;&#30340;&#21407;&#22240;&#24448;&#24448;&#26159;&#26410;&#30693;&#30340;&#12290;&#23545;&#20559;&#35265;&#21407;&#22240;&#32570;&#20047;&#20102;&#35299;&#22952;&#30861;&#20102;&#20559;&#35265;&#32531;&#35299;&#30340;&#26377;&#25928;&#24615;&#65292;&#31616;&#21333;&#30340;&#25968;&#25454;&#38598;&#24179;&#34913;&#36890;&#24120;&#26159;&#20943;&#23567;&#24615;&#33021;&#24046;&#36317;&#30340;&#26368;&#22909;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#35299;&#20915;&#25152;&#26377;&#24615;&#33021;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#21407;&#22240;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20083;&#33146;&#32452;&#32455;&#23548;&#33268;&#32954;&#37096;&#26333;&#20809;&#19981;&#36275;&#24182;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#38477;&#20302;&#30340;&#20551;&#35774;&#12290;&#22312;&#26041;&#27861;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#35760;&#24405;&#27599;&#20010;&#24739;&#32773;&#24378;&#28872;&#20559;&#26012;&#30340;&#20998;&#24067;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#20102;&#26631;&#31614;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#30142;&#30149;&#12289;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#38598;&#20013;&#24615;&#21035;&#34920;&#31034;&#26041;&#38754;&#23545;&#24615;&#21035;&#24046;&#24322;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
While many studies have assessed the fairness of AI algorithms in the medical field, the causes of differences in prediction performance are often unknown. This lack of knowledge about the causes of bias hampers the efficacy of bias mitigation, as evidenced by the fact that simple dataset balancing still often performs best in reducing performance gaps but is unable to resolve all performance differences. In this work, we investigate the causes of gender bias in machine learning-based chest X-ray diagnosis. In particular, we explore the hypothesis that breast tissue leads to underexposure of the lungs and causes lower model performance. Methodologically, we propose a new sampling method which addresses the highly skewed distribution of recordings per patient in two widely used public datasets, while at the same time reducing the impact of label errors. Our comprehensive analysis of gender differences across diseases, datasets, and gender representations in the training set shows that d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#36827;&#34892;&#26080;&#25968;&#25454;&#27169;&#22411;&#25552;&#21462;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22120;&#29983;&#25104;&#29305;&#27530;&#26597;&#35810;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#30446;&#26631;&#27169;&#22411;&#30340;&#31363;&#21462;&#12290;&#36890;&#36807;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#20351;&#29992;&#26032;&#39062;&#30340;&#29983;&#25104;&#22120;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#30028;&#26694;&#22352;&#26631;&#30340;&#39044;&#27979;&#38382;&#39064;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2308.05127</link><description>&lt;p&gt;
&#22312;&#30446;&#26631;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#26080;&#25968;&#25454;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Data-Free Model Extraction Attacks in the Context of Object Detection. (arXiv:2308.05127v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#36827;&#34892;&#26080;&#25968;&#25454;&#27169;&#22411;&#25552;&#21462;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22120;&#29983;&#25104;&#29305;&#27530;&#26597;&#35810;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#30446;&#26631;&#27169;&#22411;&#30340;&#31363;&#21462;&#12290;&#36890;&#36807;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#20351;&#29992;&#26032;&#39062;&#30340;&#29983;&#25104;&#22120;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#30028;&#26694;&#22352;&#26631;&#30340;&#39044;&#27979;&#38382;&#39064;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#36890;&#36807;&#20351;&#29992;&#38024;&#23545;&#30446;&#26631;&#27169;&#22411;&#30340;&#29305;&#27530;&#26597;&#35810;&#26469;&#31363;&#21462;&#27169;&#22411;&#30340;&#12290;&#36825;&#39033;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#25110;&#20195;&#29702;&#25968;&#25454;&#38598;&#26469;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#35757;&#32451;&#19968;&#20010;&#27169;&#20223;&#30446;&#26631;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#26469;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#27169;&#22411;&#26159;&#22312;&#23545;&#25163;&#26080;&#27861;&#35775;&#38382;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#26080;&#25968;&#25454;&#27169;&#22411;&#25552;&#21462;&#25216;&#26415;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#20351;&#29992;&#31867;&#20284;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#20351;&#29992;&#30340;&#29983;&#25104;&#22120;&#26469;&#20154;&#24037;&#29983;&#25104;&#26597;&#35810;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#29992;&#20110;&#39044;&#27979;&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#36793;&#30028;&#26694;&#22352;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#20351;&#29992;&#26032;&#39062;&#30340;&#29983;&#25104;&#22120;&#35774;&#32622;&#26159;&#25552;&#21462;&#30446;&#26631;&#27169;&#22411;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant number of machine learning models are vulnerable to model extraction attacks, which focus on stealing the models by using specially curated queries against the target model. This task is well accomplished by using part of the training data or a surrogate dataset to train a new model that mimics a target model in a white-box environment. In pragmatic situations, however, the target models are trained on private datasets that are inaccessible to the adversary. The data-free model extraction technique replaces this problem when it comes to using queries artificially curated by a generator similar to that used in Generative Adversarial Nets. We propose for the first time, to the best of our knowledge, an adversary black box attack extending to a regression problem for predicting bounding box coordinates in object detection. As part of our study, we found that defining a loss function and using a novel generator setup is one of the key aspects in extracting the target model. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#20004;&#31181;&#26032;&#31639;&#27861;&#21644;&#22235;&#31181;&#32463;&#20856;&#31639;&#27861;&#65292;&#25104;&#21151;&#25581;&#31034;&#20102;Omicron&#30149;&#27602;&#21464;&#20307;&#20013;&#30340;&#31038;&#21306;&#32467;&#26500;&#65292;&#20026;&#36827;&#19968;&#27493;&#20102;&#35299;&#30142;&#30149;&#21457;&#30149;&#26426;&#21046;&#21644;&#25512;&#36827;&#33647;&#29289;&#21457;&#29616;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;</title><link>http://arxiv.org/abs/2308.05125</link><description>&lt;p&gt;
&#20004;&#31181;&#26032;&#39062;&#26041;&#27861;&#26816;&#27979;&#31038;&#21306;&#65306;Omicron Lineage&#21464;&#20307;PPI&#32593;&#32476;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Two Novel Approaches to Detect Community: A Case Study of Omicron Lineage Variants PPI Network. (arXiv:2308.05125v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#20004;&#31181;&#26032;&#31639;&#27861;&#21644;&#22235;&#31181;&#32463;&#20856;&#31639;&#27861;&#65292;&#25104;&#21151;&#25581;&#31034;&#20102;Omicron&#30149;&#27602;&#21464;&#20307;&#20013;&#30340;&#31038;&#21306;&#32467;&#26500;&#65292;&#20026;&#36827;&#19968;&#27493;&#20102;&#35299;&#30142;&#30149;&#21457;&#30149;&#26426;&#21046;&#21644;&#25512;&#36827;&#33647;&#29289;&#21457;&#29616;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35782;&#21035;&#21644;&#20998;&#26512;&#34507;&#30333;&#36136;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21450;&#20854;&#20869;&#37096;&#27169;&#22359;&#21270;&#32452;&#32455;&#30340;&#33021;&#21147;&#65292;&#22312;&#20102;&#35299;&#20998;&#23376;&#27700;&#24179;&#19978;&#29983;&#29289;&#36807;&#31243;&#30340;&#22797;&#26434;&#26426;&#21046;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#20102;&#35299;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26500;&#21644;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#35782;&#21035;&#32593;&#32476;&#31038;&#21306;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#36827;&#23545;&#30142;&#30149;&#21457;&#30149;&#26426;&#21046;&#30340;&#29983;&#29289;&#23398;&#26681;&#28304;&#30340;&#29702;&#35299;&#12290;&#32780;&#36825;&#20123;&#30693;&#35782;&#21017;&#22312;&#25512;&#21160;&#33647;&#29289;&#21457;&#29616;&#30340;&#36827;&#27493;&#21644;&#20419;&#36827;&#20010;&#20307;&#21270;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#20004;&#31181;&#25552;&#20986;&#30340;&#26032;&#31639;&#27861;&#65288;ABCDE&#21644;ALCDE&#65289;&#20197;&#21450;&#22235;&#31181;&#24191;&#27867;&#35748;&#21487;&#30340;&#31639;&#27861;&#65288;Girvan-Newman&#65292;Louvain&#65292;Leiden&#21644;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#65289;&#25581;&#31034;&#21464;&#20307;B.1.1.529&#65288;Omicron&#30149;&#27602;&#65289;&#20013;&#30340;&#31038;&#21306;&#12290;&#27599;&#20010;&#31639;&#27861;&#22312;&#35813;&#39046;&#22495;&#20013;&#37117;&#26377;&#26174;&#30528;&#30340;&#22320;&#20301;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35782;&#21035;&#31038;&#21306;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capacity to identify and analyze protein-protein interactions, along with their internal modular organization, plays a crucial role in comprehending the intricate mechanisms underlying biological processes at the molecular level. We can learn a lot about the structure and dynamics of these interactions by using network analysis. We can improve our understanding of the biological roots of disease pathogenesis by recognizing network communities. This knowledge, in turn, holds significant potential for driving advancements in drug discovery and facilitating personalized medicine approaches for disease treatment. In this study, we aimed to uncover the communities within the variant B.1.1.529 (Omicron virus) using two proposed novel algorithm (ABCDE and ALCDE) and four widely recognized algorithms: Girvan-Newman, Louvain, Leiden, and Label Propagation algorithm. Each of these algorithms has established prominence in the field and offers unique perspectives on identifying communities wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#22312;&#32467;&#21512;&#36951;&#20256;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#39044;&#27979;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#22240;&#22411;&#25968;&#25454;&#26469;&#25351;&#23548;&#23545;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#20851;&#27880;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#29305;&#24449;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05122</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#22240;&#22411;&#21644;&#33041;&#24433;&#20687;&#25968;&#25454;&#30340;&#33258;&#27880;&#24847;&#21147;&#32852;&#21512;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Copy Number Variation Informs fMRI-based Prediction of Autism Spectrum Disorder. (arXiv:2308.05122v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#22312;&#32467;&#21512;&#36951;&#20256;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#39044;&#27979;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#22240;&#22411;&#25968;&#25454;&#26469;&#25351;&#23548;&#23545;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#20851;&#27880;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#29305;&#24449;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861; (ASD) &#30340;&#22810;&#22240;&#32032;&#30149;&#22240;&#34920;&#26126;&#65292;&#20174;&#31070;&#32463;&#24433;&#20687;&#23398;&#65292;&#36951;&#20256;&#23398;&#21644;&#20020;&#24202;&#34920;&#24449;&#31561;&#24191;&#27867;&#30340;&#24179;&#21488;&#25968;&#25454;&#20013;&#32467;&#21512;&#22810;&#27169;&#24577;&#26041;&#27861;&#21487;&#20197;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#23545;&#20854;&#30340;&#30740;&#31350;&#12290;&#20808;&#21069;&#30340;&#31070;&#32463;&#24433;&#20687;&#23398;&#21644;&#36951;&#20256;&#23398;&#20998;&#26512;&#26041;&#27861;&#36890;&#24120;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#24037;&#20316;&#20013;&#24212;&#29992;&#31616;&#21333;&#30340;&#29305;&#24449;&#25340;&#25509;&#26041;&#27861;&#65292;&#25110;&#32773;&#20351;&#29992;&#19968;&#31181;&#27169;&#24577;&#30340;&#21457;&#29616;&#26469;&#25351;&#23548;&#21478;&#19968;&#31181;&#30340;&#21518;&#32493;&#20998;&#26512;&#65292;&#38169;&#22833;&#20102;&#23545;&#37197;&#23545;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#30495;&#27491;&#32479;&#19968;&#30340;&#20998;&#26512;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26356;&#32508;&#21512;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#32467;&#21512;&#36951;&#20256;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#12290;&#21463;&#21040;&#22522;&#22240;&#22411;&#23545;&#34920;&#22411;&#30340;&#24433;&#21709;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36951;&#20256;&#25968;&#25454;&#25351;&#23548;&#23545;&#27169;&#22411;&#39044;&#27979;&#21487;&#33021;&#37325;&#35201;&#30340;&#31070;&#32463;&#24433;&#20687;&#29305;&#24449;&#30340;&#20851;&#27880;&#12290;&#36951;&#20256;&#25968;&#25454;&#26469;&#33258;&#22797;&#21046;&#25968;&#21464;&#24322;&#21442;&#25968;&#65292;&#32780;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#26469;&#33258;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multifactorial etiology of autism spectrum disorder (ASD) suggests that its study would benefit greatly from multimodal approaches that combine data from widely varying platforms, e.g., neuroimaging, genetics, and clinical characterization. Prior neuroimaging-genetic analyses often apply naive feature concatenation approaches in data-driven work or use the findings from one modality to guide posthoc analysis of another, missing the opportunity to analyze the paired multimodal data in a truly unified approach. In this paper, we develop a more integrative model for combining genetic, demographic, and neuroimaging data. Inspired by the influence of genotype on phenotype, we propose using an attention-based approach where the genetic data guides attention to neuroimaging features of importance for model prediction. The genetic data is derived from copy number variation parameters, while the neuroimaging data is from functional magnetic resonance imaging. We evaluate the proposed approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#32467;&#21512;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26469;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#31995;&#32479;&#21487;&#20449;&#24230;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05120</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20202;&#22120;&#19982;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#21160;&#24577;&#27169;&#22411;&#19981;&#21487;&#30693;&#21487;&#38752;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dynamic Model Agnostic Reliability Evaluation of Machine-Learning Methods Integrated in Instrumentation &amp; Control Systems. (arXiv:2308.05120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#32467;&#21512;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26469;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#31995;&#32479;&#21487;&#20449;&#24230;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20202;&#22120;&#19982;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#25805;&#20316;&#29615;&#22659;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#31995;&#32479;&#30340;&#25925;&#38556;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#65307;&#32570;&#20047;&#20840;&#38754;&#30340;&#39118;&#38505;&#24314;&#27169;&#21487;&#33021;&#20250;&#38477;&#20302;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;&#26368;&#36817;&#22269;&#23478;&#26631;&#20934;&#19982;&#25216;&#26415;&#30740;&#31350;&#25152;&#30340;&#25253;&#21578;&#25351;&#20986;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#20449;&#24230;&#26159;&#37319;&#29992;&#30340;&#20851;&#38190;&#38556;&#30861;&#65292;&#24182;&#19988;&#23558;&#22312;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#21487;&#36861;&#28335;&#36816;&#34892;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#31181;&#23454;&#26102;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#32467;&#21512;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26469;&#35780;&#20272;ML&#39044;&#27979;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#24050;&#26377;&#25991;&#29486;&#35777;&#26126;&#65292;ML&#31639;&#27861;&#22312;&#25554;&#20540;&#65288;&#25110;&#25509;&#36817;&#25554;&#20540;&#65289;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20250;&#26126;&#26174;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of data-driven neural network-based machine learning (ML) algorithms has grown significantly and spurred research in its applicability to instrumentation and control systems. While they are promising in operational contexts, the trustworthiness of such algorithms is not adequately assessed. Failures of ML-integrated systems are poorly understood; the lack of comprehensive risk modeling can degrade the trustworthiness of these systems. In recent reports by the National Institute for Standards and Technology, trustworthiness in ML is a critical barrier to adoption and will play a vital role in intelligent systems' safe and accountable operation. Thus, in this work, we demonstrate a real-time model-agnostic method to evaluate the relative reliability of ML predictions by incorporating out-of-distribution detection on the training dataset. It is well documented that ML algorithms excel at interpolation (or near-interpolation) tasks but significantly degrade at ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#30456;&#20284;&#24615;&#21644;&#19978;&#19979;&#25991;&#30340;&#21521;&#37327;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;cDNA&#25991;&#24211;&#30340;&#21387;&#32553;&#12289;&#30456;&#20284;&#24615;&#25628;&#32034;&#12289;&#32858;&#31867;&#12289;&#32452;&#32455;&#21644;&#25805;&#20316;&#12290;&#36890;&#36807;&#23558;&#24207;&#21015;&#36716;&#25442;&#20026;&#21521;&#37327;&#23884;&#20837;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#32858;&#31867;&#24182;&#25552;&#39640;&#21387;&#32553;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22522;&#20110;&#27688;&#22522;&#37240;&#24615;&#36136;&#36827;&#34892;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.05118</link><description>&lt;p&gt;
&#22522;&#20110;&#24207;&#21015;&#30456;&#20284;&#24615;&#21644;&#19978;&#19979;&#25991;&#30340;&#21521;&#37327;&#23884;&#20837;&#65292;&#29992;&#20110;&#25913;&#36827;cDNA&#25991;&#24211;&#30340;&#21387;&#32553;&#12289;&#30456;&#20284;&#24615;&#25628;&#32034;&#12289;&#32858;&#31867;&#12289;&#32452;&#32455;&#21644;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Vector Embeddings by Sequence Similarity and Context for Improved Compression, Similarity Search, Clustering, Organization, and Manipulation of cDNA Libraries. (arXiv:2308.05118v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#30456;&#20284;&#24615;&#21644;&#19978;&#19979;&#25991;&#30340;&#21521;&#37327;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;cDNA&#25991;&#24211;&#30340;&#21387;&#32553;&#12289;&#30456;&#20284;&#24615;&#25628;&#32034;&#12289;&#32858;&#31867;&#12289;&#32452;&#32455;&#21644;&#25805;&#20316;&#12290;&#36890;&#36807;&#23558;&#24207;&#21015;&#36716;&#25442;&#20026;&#21521;&#37327;&#23884;&#20837;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#32858;&#31867;&#24182;&#25552;&#39640;&#21387;&#32553;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22522;&#20110;&#27688;&#22522;&#37240;&#24615;&#36136;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22522;&#22240;&#30340;&#26377;&#32452;&#32455;&#30340;&#25968;&#20540;&#34920;&#24449;&#22312;&#30740;&#31350;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#25153;&#24179;&#23383;&#31526;&#20018;&#22522;&#22240;&#26684;&#24335;&#65288;&#20363;&#22914;FASTA/FASTQ&#65289;&#12290;FASTA/FASTQ&#25991;&#20214;&#30446;&#21069;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#25991;&#20214;&#22823;&#23567;&#22823;&#12289;&#26144;&#23556;&#21644;&#27604;&#23545;&#30340;&#22788;&#29702;&#36895;&#24230;&#24930;&#20197;&#21450;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#12290;&#36825;&#20123;&#25361;&#25112;&#20005;&#37325;&#22952;&#30861;&#20102;&#28041;&#21450;&#26597;&#25214;&#30456;&#20284;&#24207;&#21015;&#30340;&#35843;&#26597;&#21644;&#20219;&#21153;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#24207;&#21015;&#36716;&#25442;&#20026;&#19968;&#31181;&#26367;&#20195;&#34920;&#31034;&#27861;&#65292;&#20415;&#20110;&#23558;&#20854;&#32858;&#31867;&#21040;&#19982;&#21407;&#22987;&#24207;&#21015;&#30456;&#27604;&#26356;&#30456;&#20284;&#30340;&#32452;&#20013;&#12290;&#36890;&#36807;&#32473;&#27599;&#20010;&#30701;&#24207;&#21015;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#21521;&#37327;&#23884;&#20837;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#32858;&#31867;&#65292;&#24182;&#25913;&#21892;cDNA&#25991;&#24211;&#23383;&#31526;&#20018;&#34920;&#31034;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23398;&#20064;&#22522;&#20110;&#23494;&#30721;&#23376;&#19977;&#32858;&#20307;&#19978;&#19979;&#25991;&#30340;&#26367;&#20195;&#22352;&#26631;&#21521;&#37327;&#23884;&#20837;&#65292;&#21487;&#20197;&#22522;&#20110;&#27688;&#22522;&#37240;&#24615;&#36136;&#36827;&#34892;&#32858;&#31867;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#36825;&#31181;&#24207;&#21015;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#25928;&#26524;&#65292;&#29992;&#20110;&#25913;&#36827;cDNA&#25991;&#24211;&#30340;&#21387;&#32553;&#12289;&#30456;&#20284;&#24615;&#25628;&#32034;&#12289;&#32858;&#31867;&#12289;&#32452;&#32455;&#21644;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates the utility of organized numerical representations of genes in research involving flat string gene formats (i.e., FASTA/FASTQ5). FASTA/FASTQ files have several current limitations, such as their large file sizes, slow processing speeds for mapping and alignment, and contextual dependencies. These challenges significantly hinder investigations and tasks that involve finding similar sequences. The solution lies in transforming sequences into an alternative representation that facilitates easier clustering into similar groups compared to the raw sequences themselves. By assigning a unique vector embedding to each short sequence, it is possible to more efficiently cluster and improve upon compression performance for the string representations of cDNA libraries. Furthermore, through learning alternative coordinate vector embeddings based on the contexts of codon triplets, we can demonstrate clustering based on amino acid properties. Finally, using this sequence embed
&lt;/p&gt;</description></item><item><title>PTransIPs&#26159;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27688;&#22522;&#37240;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#32467;&#21512;&#22823;&#22411;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#35782;&#21035;&#30967;&#37240;&#21270;&#20301;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.05115</link><description>&lt;p&gt;
&#22522;&#20110;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;Transformer&#30340;&#30967;&#37240;&#21270;&#20301;&#28857;&#35782;&#21035;&#26041;&#27861;(PTransIPs)
&lt;/p&gt;
&lt;p&gt;
PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer. (arXiv:2308.05115v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05115
&lt;/p&gt;
&lt;p&gt;
PTransIPs&#26159;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27688;&#22522;&#37240;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#32467;&#21512;&#22823;&#22411;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#35782;&#21035;&#30967;&#37240;&#21270;&#20301;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30967;&#37240;&#21270;&#26159;&#35768;&#22810;&#22522;&#30784;&#32454;&#32990;&#36807;&#31243;&#30340;&#26680;&#24515;&#65292;&#24433;&#21709;&#30528;&#21508;&#31181;&#30142;&#30149;&#30340;&#21457;&#29983;&#21644;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#30967;&#37240;&#21270;&#20301;&#28857;&#30340;&#35782;&#21035;&#26159;&#29702;&#35299;&#32454;&#32990;&#21644;&#30149;&#27602;&#24863;&#26579;&#30340;&#20998;&#23376;&#26426;&#21046;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#21487;&#33021;&#20026;&#26032;&#30340;&#27835;&#30103;&#38774;&#28857;&#25552;&#20379;&#22522;&#30784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTransIPs&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#30967;&#37240;&#21270;&#20301;&#28857;&#12290;PTransIPs&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27688;&#22522;&#37240;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#26681;&#25454;&#24207;&#21015;&#20013;&#27688;&#22522;&#37240;&#30340;&#31867;&#22411;&#21644;&#20301;&#32622;&#25552;&#21462;&#29420;&#29305;&#30340;&#32534;&#30721;&#12290;&#23427;&#36824;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#23884;&#20837;&#20316;&#20026;&#39069;&#22806;&#30340;&#25968;&#25454;&#36755;&#20837;&#12290;PTransIPs&#36827;&#19968;&#27493;&#36890;&#36807;&#32467;&#21512;&#20855;&#26377;&#27531;&#24046;&#36830;&#25509;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#65292;&#37197;&#22791;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20840;&#36830;&#25509;&#23618;&#36755;&#20986;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phosphorylation is central to numerous fundamental cellular processes, influencing the onset and progression of a variety of diseases. Identification of phosphorylation sites is thus an important step for understanding the molecular mechanisms of cells and virus infection, which potentially leads to new therapeutic targets. In this study, we present PTransIPs, a novel deep learning model for the identification of phosphorylation sites. PTransIPs treats amino acids in protein sequences as words in natural language, extracting unique encodings based on the types along with position of amino acids in the sequence. It also incorporates embeddings from large pre-trained protein models as additional data inputs. PTransIPS is further trained on a combination model of convolutional neural network with residual connections and Transformer model equipped with multi-head attention mechanisms. At last, the model outputs classification results through a fully connected layer. The results of indepen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#26089;&#26399;&#20013;&#39118;&#27515;&#20129;&#29575;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35780;&#20998;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05110</link><description>&lt;p&gt;
&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#37322;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#27515;&#20129;&#29575;&#39044;&#27979;&#20219;&#21153;&#65306;&#20197;&#20986;&#34880;&#24615;&#20013;&#39118;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke. (arXiv:2308.05110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#26089;&#26399;&#20013;&#39118;&#27515;&#20129;&#29575;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35780;&#20998;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#39118;&#26159;&#33268;&#27515;&#29575;&#21644;&#33268;&#27531;&#29575;&#26174;&#33879;&#30340;&#22240;&#32032;&#65292;&#38656;&#35201;&#26089;&#26399;&#39044;&#27979;&#31574;&#30053;&#26469;&#38477;&#20302;&#39118;&#38505;&#12290;&#20256;&#32479;&#35780;&#20272;&#24739;&#32773;&#30340;&#26041;&#27861;&#65292;&#22914;&#24613;&#24615;&#29983;&#29702;&#21644;&#24930;&#24615;&#20581;&#24247;&#35780;&#20272;&#65288;APACHE II&#12289;IV&#65289;&#21644;&#31616;&#21270;&#24613;&#24615;&#29983;&#29702;&#35780;&#20998;III&#65288;SAPS III&#65289;&#65292;&#20855;&#26377;&#26377;&#38480;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#26089;&#26399;&#20013;&#39118;&#27515;&#20129;&#29575;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#35299;&#20915;&#20043;&#21069;&#39044;&#27979;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#35299;&#37322;&#24615;&#65288;&#28165;&#26224;&#12289;&#26131;&#25026;&#22320;&#35299;&#37322;&#27169;&#22411;&#65289;&#21644;&#24544;&#23454;&#24230;&#65288;&#20174;&#36755;&#20837;&#21040;&#36755;&#20986;&#25552;&#20379;&#30495;&#23454;&#30340;&#27169;&#22411;&#21160;&#24577;&#35299;&#37322;&#65289;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#32034;&#21644;&#27604;&#36739;&#20351;&#29992;Shapley&#20540;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35780;&#20998;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#20998;&#25968;&#12290;&#30740;&#31350;&#30446;&#26631;&#21253;&#25324;&#35774;&#35745;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stroke is a significant cause of mortality and morbidity, necessitating early predictive strategies to minimize risks. Traditional methods for evaluating patients, such as Acute Physiology and Chronic Health Evaluation (APACHE II, IV) and Simplified Acute Physiology Score III (SAPS III), have limited accuracy and interpretability. This paper proposes a novel approach: an interpretable, attention-based transformer model for early stroke mortality prediction. This model seeks to address the limitations of previous predictive models, providing both interpretability (providing clear, understandable explanations of the model) and fidelity (giving a truthful explanation of the model's dynamics from input to output). Furthermore, the study explores and compares fidelity and interpretability scores using Shapley values and attention-based scores to improve model explainability. The research objectives include designing an interpretable attention-based transformer model, evaluating its performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#35270;&#39057;&#26292;&#21147;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#23427;&#20204;&#30340;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#35757;&#32451;&#26368;&#20339;&#26292;&#21147;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05106</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#65306;&#26292;&#21147;&#26816;&#27979;&#22312;&#30417;&#25511;&#35270;&#39057;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures. (arXiv:2308.05106v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#35270;&#39057;&#26292;&#21147;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#23427;&#20204;&#30340;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#35757;&#32451;&#26368;&#20339;&#26292;&#21147;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#35270;&#39057;&#26292;&#21147;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#23427;&#20204;&#30340;&#36866;&#24212;&#24615;&#12290;&#30740;&#31350;&#21253;&#25324;&#23545;&#22522;&#20934;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#26102;&#31354;&#29305;&#24449;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#26412;&#30340;&#8220;Flow-Gated&#8221;&#26550;&#26500;&#65292;&#31216;&#20026;&#8220;Diff-Gated&#8221;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#36229;&#25910;&#25947;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23558;&#38598;&#20013;&#24335;&#25968;&#25454;&#38598;&#36866;&#24212;&#21040;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#35757;&#32451;&#26368;&#20339;&#26292;&#21147;&#26816;&#27979;&#27169;&#22411;&#65292;&#30740;&#31350;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an investigation into machine learning techniques for violence detection in videos and their adaptation to a federated learning context. The study includes experiments with spatio-temporal features extracted from benchmark video datasets, comparison of different methods, and proposal of a modified version of the "Flow-Gated" architecture called "Diff-Gated." Additionally, various machine learning techniques, including super-convergence and transfer learning, are explored, and a method for adapting centralized datasets to a federated learning context is developed. The research achieves better accuracy results compared to state-of-the-art models by training the best violence detection model in a federated learning context.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#20869;&#37096;&#31867;&#21035;&#30340;&#22825;&#25991;&#23398;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#22810;&#31867;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;(MCDSVDD)&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#36229;&#29699;&#20307;&#20013;&#65292;&#27599;&#20010;&#36229;&#29699;&#20307;&#20195;&#34920;&#19968;&#20010;&#29305;&#23450;&#30340;&#20869;&#37096;&#31867;&#21035;&#65292;&#24182;&#35745;&#31639;&#26679;&#26412;&#19982;&#36229;&#29699;&#20307;&#20013;&#24515;&#30340;&#36317;&#31163;&#20197;&#30830;&#23450;&#24322;&#24120;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.05011</link><description>&lt;p&gt;
&#22810;&#31867;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#65306;&#19968;&#31181;&#29992;&#20110;&#22825;&#25991;&#23398;&#20013;&#20855;&#26377;&#19981;&#21516;&#20869;&#37096;&#31867;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories. (arXiv:2308.05011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05011
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#20869;&#37096;&#31867;&#21035;&#30340;&#22825;&#25991;&#23398;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#22810;&#31867;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;(MCDSVDD)&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#36229;&#29699;&#20307;&#20013;&#65292;&#27599;&#20010;&#36229;&#29699;&#20307;&#20195;&#34920;&#19968;&#20010;&#29305;&#23450;&#30340;&#20869;&#37096;&#31867;&#21035;&#65292;&#24182;&#35745;&#31639;&#26679;&#26412;&#19982;&#36229;&#29699;&#20307;&#20013;&#24515;&#30340;&#36317;&#31163;&#20197;&#30830;&#23450;&#24322;&#24120;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#21208;&#27979;&#26395;&#36828;&#38236;&#20135;&#29983;&#30340;&#22825;&#25991;&#25968;&#25454;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#33258;&#21160;&#21270;&#27969;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#20026;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21644;&#25552;&#21462;&#30693;&#35782;&#30340;&#20851;&#38190;&#12290;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#25968;&#25454;&#20013;&#35782;&#21035;&#19981;&#35268;&#21017;&#25110;&#24847;&#22806;&#27169;&#24335;&#30340;&#20219;&#21153;&#65292;&#26159;&#22825;&#25991;&#23398;&#20013;&#30340;&#19968;&#20010;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31867;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;(MCDSVDD)&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#23545;&#29616;&#26377;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;One-Class Deep SVDD&#36827;&#34892;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#21516;&#20869;&#37096;&#31867;&#21035;&#12290;MCDSVDD&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#36229;&#29699;&#20307;&#20013;&#65292;&#20854;&#20013;&#27599;&#20010;&#36229;&#29699;&#20307;&#20195;&#34920;&#19968;&#20010;&#29305;&#23450;&#30340;&#20869;&#37096;&#31867;&#21035;&#12290;&#26679;&#26412;&#36317;&#31163;&#36825;&#20123;&#36229;&#29699;&#20307;&#20013;&#24515;&#30340;&#36317;&#31163;&#20915;&#23450;&#20102;&#24322;&#24120;&#20998;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MCDSVDD&#30340;&#24615;&#33021;&#19982;&#22810;&#20010;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#22825;&#25991;&#20809;&#21464;&#26354;&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;MCDSVDD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing volume of astronomical data generated by modern survey telescopes, automated pipelines and machine learning techniques have become crucial for analyzing and extracting knowledge from these datasets. Anomaly detection, i.e. the task of identifying irregular or unexpected patterns in the data, is a complex challenge in astronomy. In this paper, we propose Multi-Class Deep Support Vector Data Description (MCDSVDD), an extension of the state-of-the-art anomaly detection algorithm One-Class Deep SVDD, specifically designed to handle different inlier categories with distinct data distributions. MCDSVDD uses a neural network to map the data into hyperspheres, where each hypersphere represents a specific inlier category. The distance of each sample from the centers of these hyperspheres determines the anomaly score. We evaluate the effectiveness of MCDSVDD by comparing its performance with several anomaly detection algorithms on a large dataset of astronomical light-curves 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#22411;&#29305;&#24449;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;PDF&#24694;&#24847;&#36719;&#20214;&#65292;&#26080;&#38656;&#22826;&#22810;&#39046;&#22495;&#30693;&#35782;&#12290;&#22312;&#20845;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;Random Forest&#27169;&#22411;&#26102;&#21487;&#20197;&#36798;&#21040;99.75%&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20010;&#20165;&#21253;&#21547;12&#20010;&#29305;&#24449;&#30340;&#29305;&#24449;&#38598;&#26159;&#26368;&#31616;&#27905;&#30340;&#20043;&#19968;&#65292;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#32467;&#26524;&#21487;&#19982;&#37319;&#29992;&#26356;&#22823;&#29305;&#24449;&#38598;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.04704</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;PDF&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#23567;&#23610;&#23544;&#29305;&#24449;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Feature Set of Small Size for the PDF Malware Detection. (arXiv:2308.04704v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#22411;&#29305;&#24449;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;PDF&#24694;&#24847;&#36719;&#20214;&#65292;&#26080;&#38656;&#22826;&#22810;&#39046;&#22495;&#30693;&#35782;&#12290;&#22312;&#20845;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;Random Forest&#27169;&#22411;&#26102;&#21487;&#20197;&#36798;&#21040;99.75%&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20010;&#20165;&#21253;&#21547;12&#20010;&#29305;&#24449;&#30340;&#29305;&#24449;&#38598;&#26159;&#26368;&#31616;&#27905;&#30340;&#20043;&#19968;&#65292;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#32467;&#26524;&#21487;&#19982;&#37319;&#29992;&#26356;&#22823;&#29305;&#24449;&#38598;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;&#30340;&#22686;&#21152;&#21644;&#26356;&#21152;&#22797;&#26434;&#21270;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;PDF&#25991;&#20214;&#36890;&#24120;&#34987;&#29992;&#20316;&#38035;&#40060;&#25915;&#20987;&#30340;&#30690;&#37327;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#21487;&#20449;&#30340;&#25968;&#25454;&#36164;&#28304;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#24179;&#21488;&#19978;&#37117;&#21487;&#20197;&#35775;&#38382;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;PDF&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#26816;&#27979;PDF&#24694;&#24847;&#36719;&#20214;&#30340;&#24615;&#33021;&#26377;&#24456;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23567;&#22411;&#29305;&#24449;&#38598;&#65292;&#19981;&#38656;&#35201;&#22826;&#22810;&#20851;&#20110;PDF&#25991;&#20214;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#29305;&#24449;&#12290;&#22312;&#20351;&#29992;Random Forest&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#26368;&#39640;99.75%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20165;&#21253;&#21547;12&#20010;&#29305;&#24449;&#30340;&#29305;&#24449;&#38598;&#26159;PDF&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#20013;&#26368;&#31616;&#27905;&#30340;&#20043;&#19968;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#37319;&#29992;&#26356;&#22823;&#29305;&#24449;&#38598;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML)-based malware detection systems are becoming increasingly important as malware threats increase and get more sophisticated. PDF files are often used as vectors for phishing attacks because they are widely regarded as trustworthy data resources, and are accessible across different platforms. Therefore, researchers have developed many different PDF malware detection methods. Performance in detecting PDF malware is greatly influenced by feature selection. In this research, we propose a small features set that don't require too much domain knowledge of the PDF file. We evaluate proposed features with six different machine learning models. We report the best accuracy of 99.75% when using Random Forest model. Our proposed feature set, which consists of just 12 features, is one of the most conciseness in the field of PDF malware detection. Despite its modest size, we obtain comparable results to state-of-the-art that employ a much larger set of features.
&lt;/p&gt;</description></item><item><title>SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04365</link><description>&lt;p&gt;
SLEM&#65306;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#36335;&#24452;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04365
&lt;/p&gt;
&lt;p&gt;
SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#31185;&#23398;&#30340;&#20851;&#38190;&#30446;&#26631;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24471;&#20986;&#20851;&#20110;&#23545;&#20551;&#23450;&#24178;&#39044;&#30340;&#39044;&#27979;&#30340;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#36335;&#24452;&#27169;&#22411;&#12289;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#33021;&#22815;&#26126;&#30830;&#22320;&#25351;&#23450;&#20851;&#20110;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#20551;&#35774;&#12290;&#19982;DAGs&#19981;&#21516;&#65292;SEMs&#20551;&#35774;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20989;&#25968;&#38169;&#35823;&#35268;&#33539;&#65292;&#20174;&#32780;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#21487;&#38752;&#30340;&#25928;&#26524;&#22823;&#23567;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;&#65288;SLEM&#65289;&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#38598;&#25104;&#30340;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;SLEM&#33021;&#22815;&#25552;&#20379;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#22312;&#19982;SEMs&#36827;&#34892;&#32447;&#24615;&#27169;&#22411;&#27604;&#36739;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#20248;&#20110;SEMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21516;&#26102;&#32553;&#25918;&#25968;&#25454;&#35268;&#27169;&#12289;&#27169;&#22411;&#35268;&#27169;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#20197;&#38750;&#20154;&#31867;&#23610;&#24230;&#30340;&#26465;&#20214;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#29289;&#20307;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03712</link><description>&lt;p&gt;
&#36890;&#36807;&#32553;&#25918;&#21487;&#33021;&#21487;&#20197;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#35270;&#35273;&#32463;&#39564;&#21644;&#20154;&#31867;&#32423;&#21035;&#30340;&#29289;&#20307;&#35782;&#21035;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience. (arXiv:2308.03712v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21516;&#26102;&#32553;&#25918;&#25968;&#25454;&#35268;&#27169;&#12289;&#27169;&#22411;&#35268;&#27169;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#20197;&#38750;&#20154;&#31867;&#23610;&#24230;&#30340;&#26465;&#20214;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#29289;&#20307;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21069;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#36275;&#22815;&#30340;&#32553;&#25918;&#26469;&#36798;&#21040;&#19982;&#20154;&#31867;&#23398;&#20064;&#30456;&#21516;&#31867;&#22411;&#21644;&#25968;&#37327;&#30340;&#35270;&#35273;&#32463;&#39564;&#30340;&#20154;&#31867;&#32423;&#21035;&#35270;&#35273;&#29289;&#20307;&#35782;&#21035;&#33021;&#21147;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21482;&#32771;&#34385;&#20102;&#25968;&#25454;&#35268;&#27169;&#30340;&#32553;&#25918;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#25968;&#25454;&#35268;&#27169;&#12289;&#27169;&#22411;&#35268;&#27169;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#21516;&#26102;&#32553;&#25918;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#27425;&#32553;&#25918;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#26368;&#22810;633M&#21442;&#25968;&#35268;&#27169;&#65288;ViT-H/14&#65289;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#20197;&#38271;&#36798;5K&#23567;&#26102;&#30340;&#31867;&#20154;&#35270;&#39057;&#25968;&#25454;&#65288;&#38271;&#26102;&#38388;&#36830;&#32493;&#12289;&#20027;&#35201;&#20026;&#33258;&#25105;&#20013;&#24515;&#30340;&#35270;&#39057;&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#22270;&#20687;&#20998;&#36776;&#29575;&#39640;&#36798;476x476&#20687;&#32032;&#12290;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#30340;&#39640;&#25928;&#24615;&#20351;&#24471;&#21487;&#20197;&#22312;&#26222;&#36890;&#30340;&#23398;&#26415;&#39044;&#31639;&#19979;&#36827;&#34892;&#36825;&#31181;&#32553;&#25918;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#21516;&#26102;&#32553;&#25918;&#36825;&#20123;&#22240;&#32032;&#65292;&#21363;&#20351;&#22312;&#20122;&#20154;&#31867;&#23610;&#24230;&#30340;&#27169;&#22411;&#35268;&#27169;&#12289;&#25968;&#25454;&#35268;&#27169;&#21644;&#22270;&#20687;&#23610;&#23544;&#19979;&#65292;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#29289;&#20307;&#35782;&#21035;&#33021;&#21147;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper asks whether current self-supervised learning methods, if sufficiently scaled up, would be able to reach human-level visual object recognition capabilities with the same type and amount of visual experience humans learn from. Previous work on this question only considered the scaling of data size. Here, we consider the simultaneous scaling of data size, model size, and image resolution. We perform a scaling experiment with vision transformers up to 633M parameters in size (ViT-H/14) trained with up to 5K hours of human-like video data (long, continuous, mostly egocentric videos) with image resolutions of up to 476x476 pixels. The efficiency of masked autoencoders (MAEs) as a self-supervised learning algorithm makes it possible to run this scaling experiment on an unassuming academic budget. We find that it is feasible to reach human-level object recognition capacity at sub-human scales of model size, data size, and image size, if these factors are scaled up simultaneously. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28151;&#21512;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;U&#22359;&#30340;&#21452;&#20998;&#25903;&#32593;&#32476;&#65292;&#29992;&#20110;&#22686;&#24378;&#32454;&#32990;&#26680;&#20998;&#21106;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#32454;&#32990;&#26680;&#30340;&#22823;&#23567;&#21464;&#21270;&#12289;&#27169;&#31946;&#36718;&#24275;&#12289;&#26579;&#33394;&#19981;&#22343;&#21248;&#12289;&#32454;&#32990;&#22242;&#32858;&#21644;&#37325;&#21472;&#30340;&#32454;&#32990;&#26680;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.03382</link><description>&lt;p&gt;
&#20351;&#29992;HARU-Net&#22686;&#24378;&#32454;&#32990;&#26680;&#20998;&#21106;: &#19968;&#20010;&#28151;&#21512;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;U&#22359;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network. (arXiv:2308.03382v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28151;&#21512;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;U&#22359;&#30340;&#21452;&#20998;&#25903;&#32593;&#32476;&#65292;&#29992;&#20110;&#22686;&#24378;&#32454;&#32990;&#26680;&#20998;&#21106;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#32454;&#32990;&#26680;&#30340;&#22823;&#23567;&#21464;&#21270;&#12289;&#27169;&#31946;&#36718;&#24275;&#12289;&#26579;&#33394;&#19981;&#22343;&#21248;&#12289;&#32454;&#32990;&#22242;&#32858;&#21644;&#37325;&#21472;&#30340;&#32454;&#32990;&#26680;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#26680;&#22270;&#20687;&#20998;&#21106;&#26159;&#20998;&#26512;&#12289;&#30149;&#29702;&#35786;&#26029;&#21644;&#20998;&#31867;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#32454;&#32990;&#26680;&#20998;&#21106;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#32454;&#32990;&#26680;&#23610;&#23544;&#30340;&#21464;&#21270;&#12289;&#27169;&#31946;&#30340;&#32454;&#32990;&#26680;&#36718;&#24275;&#12289;&#26579;&#33394;&#19981;&#22343;&#21248;&#12289;&#32454;&#32990;&#22242;&#32858;&#21644;&#32454;&#32990;&#37325;&#21472;&#31561;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#32454;&#32990;&#26680;&#20998;&#21106;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26680;&#24418;&#24577;&#23398;&#25110;&#22522;&#20110;&#36718;&#24275;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#26680;&#24418;&#24577;&#23398;&#30340;&#26041;&#27861;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23545;&#20110;&#39044;&#27979;&#19981;&#35268;&#21017;&#24418;&#29366;&#30340;&#32454;&#32990;&#26680;&#25928;&#26524;&#26377;&#38480;&#65292;&#32780;&#22522;&#20110;&#36718;&#24275;&#25552;&#21462;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#20998;&#21106;&#37325;&#21472;&#30340;&#32454;&#32990;&#26680;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28151;&#21512;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;U&#22359;&#30340;&#21452;&#20998;&#25903;&#32593;&#32476;&#65292;&#29992;&#20110;&#32454;&#32990;&#26680;&#23454;&#20363;&#20998;&#21106;&#12290;&#35813;&#32593;&#32476;&#21516;&#26102;&#39044;&#27979;&#30446;&#26631;&#20449;&#24687;&#21644;&#30446;&#26631;&#36718;&#24275;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#20449;&#24687;&#19982;&#30446;&#26631;&#36718;&#24275;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nucleus image segmentation is a crucial step in the analysis, pathological diagnosis, and classification, which heavily relies on the quality of nucleus segmentation. However, the complexity of issues such as variations in nucleus size, blurred nucleus contours, uneven staining, cell clustering, and overlapping cells poses significant challenges. Current methods for nucleus segmentation primarily rely on nuclear morphology or contour-based approaches. Nuclear morphology-based methods exhibit limited generalization ability and struggle to effectively predict irregular-shaped nuclei, while contour-based extraction methods face challenges in accurately segmenting overlapping nuclei. To address the aforementioned issues, we propose a dual-branch network using hybrid attention based residual U-blocks for nucleus instance segmentation. The network simultaneously predicts target information and target contours. Additionally, we introduce a post-processing method that combines the target infor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AI-GOMS&#65292;&#19968;&#20010;&#22823;&#22411;AI&#39537;&#21160;&#30340;&#20840;&#29699;&#28023;&#27915;&#27169;&#25311;&#31995;&#32479;&#65292;&#29992;&#20110;&#20934;&#30830;&#39640;&#25928;&#30340;&#20840;&#29699;&#28023;&#27915;&#27599;&#26085;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.03152</link><description>&lt;p&gt;
AI-GOMS: &#22823;&#22411;AI&#39537;&#21160;&#30340;&#20840;&#29699;&#28023;&#27915;&#27169;&#25311;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AI-GOMS: Large AI-Driven Global Ocean Modeling System. (arXiv:2308.03152v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03152
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AI-GOMS&#65292;&#19968;&#20010;&#22823;&#22411;AI&#39537;&#21160;&#30340;&#20840;&#29699;&#28023;&#27915;&#27169;&#25311;&#31995;&#32479;&#65292;&#29992;&#20110;&#20934;&#30830;&#39640;&#25928;&#30340;&#20840;&#29699;&#28023;&#27915;&#27599;&#26085;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#27169;&#25311;&#26159;&#27169;&#25311;&#28023;&#27915;&#30340;&#29289;&#29702;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#36807;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26159;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#21644;&#36816;&#33829;&#28023;&#27915;&#23398;&#30340;&#22522;&#30784;&#12290;&#29616;&#20195;&#25968;&#20540;&#28023;&#27915;&#27169;&#25311;&#20027;&#35201;&#21253;&#25324;&#25511;&#21046;&#26041;&#31243;&#21644;&#25968;&#20540;&#31639;&#27861;&#12290;&#38750;&#32447;&#24615;&#19981;&#31283;&#23450;&#24615;&#12289;&#35745;&#31639;&#24320;&#38144;&#12289;&#20302;&#21487;&#37325;&#29992;&#25928;&#29575;&#21644;&#39640;&#32806;&#21512;&#25104;&#26412;&#36880;&#28176;&#25104;&#20026;&#25968;&#20540;&#28023;&#27915;&#27169;&#25311;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31185;&#23398;&#35745;&#31639;&#27169;&#22411;&#23637;&#31034;&#20102;&#25968;&#23383;&#23402;&#29983;&#21644;&#31185;&#23398;&#27169;&#25311;&#30340;&#38761;&#21629;&#28508;&#21147;&#65292;&#20294;&#25968;&#20540;&#28023;&#27915;&#27169;&#25311;&#30340;&#29942;&#39048;&#23578;&#26410;&#24471;&#21040;&#36827;&#19968;&#27493;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AI-GOMS&#65292;&#19968;&#20010;&#22823;&#22411;AI&#39537;&#21160;&#30340;&#20840;&#29699;&#28023;&#27915;&#27169;&#25311;&#31995;&#32479;&#65292;&#29992;&#20110;&#20934;&#30830;&#39640;&#25928;&#30340;&#20840;&#29699;&#28023;&#27915;&#27599;&#26085;&#39044;&#27979;&#12290;AI-GOMS&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#39592;&#24178;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#26412;&#28023;&#27915;&#21464;&#37327;&#39044;&#27979;&#21644;&#36731;&#37327;&#32423;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean modeling is a powerful tool for simulating the physical, chemical, and biological processes of the ocean, which is the foundation for marine science research and operational oceanography. Modern numerical ocean modeling mainly consists of governing equations and numerical algorithms. Nonlinear instability, computational expense, low reusability efficiency and high coupling costs have gradually become the main bottlenecks for the further development of numerical ocean modeling. Recently, artificial intelligence-based modeling in scientific computing has shown revolutionary potential for digital twins and scientific simulations, but the bottlenecks of numerical ocean modeling have not been further solved. Here, we present AI-GOMS, a large AI-driven global ocean modeling system, for accurate and efficient global ocean daily prediction. AI-GOMS consists of a backbone model with the Fourier-based Masked Autoencoder structure for basic ocean variable prediction and lightweight fine-tun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#20840;&#38754;&#30340;&#28857;&#20113;&#23545;&#25239;&#40065;&#26834;&#24615;&#22522;&#20934;&#12289;&#25910;&#38598;&#24050;&#26377;&#30340;&#38450;&#24481;&#25216;&#24039;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#24635;&#32467;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28857;&#20113;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16361</link><description>&lt;p&gt;
&#23545;&#24378;&#20581;&#30340;&#28857;&#20113;&#35782;&#21035;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#20998;&#26512;&#65306;&#25269;&#24481;&#23545;&#25239;&#26679;&#26412;&#30340;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples. (arXiv:2307.16361v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#20840;&#38754;&#30340;&#28857;&#20113;&#23545;&#25239;&#40065;&#26834;&#24615;&#22522;&#20934;&#12289;&#25910;&#38598;&#24050;&#26377;&#30340;&#38450;&#24481;&#25216;&#24039;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#24635;&#32467;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28857;&#20113;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#28857;&#20113;&#35782;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23545;&#23545;&#25239;&#26679;&#26412;&#20855;&#26377;&#24369;&#28857;&#65292;&#23041;&#32961;&#30528;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;3D&#28857;&#20113;&#19978;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#20351;&#20854;&#27604;2D&#22270;&#20687;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#12289;&#31227;&#21160;&#25110;&#21024;&#38500;&#28857;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#24456;&#38590;&#23545;&#20184;&#26410;&#30693;&#30340;&#28857;&#20113;&#23545;&#25239;&#26679;&#26412;&#12290;&#26412;&#25991;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#19988;&#20005;&#35880;&#30340;&#28857;&#20113;&#23545;&#25239;&#40065;&#26834;&#24615;&#22522;&#20934;&#65292;&#35780;&#20272;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#35814;&#32454;&#20102;&#35299;&#38450;&#24481;&#21644;&#25915;&#20987;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#28982;&#21518;&#25910;&#38598;&#28857;&#20113;&#23545;&#25239;&#38450;&#24481;&#20013;&#24050;&#26377;&#30340;&#25216;&#24039;&#65292;&#24182;&#36827;&#34892;&#24191;&#27867;&#31995;&#32479;&#30340;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#36825;&#20123;&#25216;&#24039;&#30340;&#26377;&#25928;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to adversarial examples, threatening their practical deployment. Despite the many research endeavors have been made to tackle this issue in recent years, the diversity of adversarial examples on 3D point clouds makes them more challenging to defend against than those on 2D images. For examples, attackers can generate adversarial examples by adding, shifting, or removing points. Consequently, existing defense strategies are hard to counter unseen point cloud adversarial examples. In this paper, we first establish a comprehensive, and rigorous point cloud adversarial robustness benchmark to evaluate adversarial robustness, which can provide a detailed understanding of the effects of the defense and attack methods. We then collect existing defense tricks in point cloud adversarial defenses and then perform extensive and systematic experiments to identify an effective combination of these tricks. Furthermore, we prop
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;</title><link>http://arxiv.org/abs/2307.15644</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25968;&#25454;&#29983;&#25104;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#35270;&#35273;&#23548;&#33322;&#30740;&#31350;&#20013;&#65292;&#23545;&#20110;&#36941;&#21382;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#21487;&#27867;&#21270;&#20195;&#29702;&#30340;&#30417;&#30563;&#25968;&#37327;&#26377;&#20102;&#26126;&#26174;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;HM3D&#21644;Gibson&#25968;&#25454;&#38598;&#20013;&#30340;1200&#22810;&#20010;&#36924;&#30495;&#30340;&#29615;&#22659;&#65292;&#24182;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#36164;&#28304;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#33539;&#24335;&#20013;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#24688;&#24403;&#22320;&#24212;&#29992;&#25193;&#22686;&#25968;&#25454;&#26469;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20195;&#29702;&#12290;&#24471;&#30410;&#20110;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#29616;&#26377;&#20195;&#29702;&#30340;&#24615;&#33021;&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#65288;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#32477;&#23545;&#20540;&#22686;&#21152;&#20102;11%&#65289;&#65292;&#22312;R2R&#27979;&#35797;&#38598;&#20013;&#21333;&#27425;&#36816;&#34892;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#33618;&#37326;&#25628;&#25937;&#20013;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;EfficientDET&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14527</link><description>&lt;p&gt;
&#29992;&#20110;&#33618;&#37326;SAR&#21644;&#23547;&#25214;Patricia Wu-Murad&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#33618;&#37326;&#25628;&#25937;&#20013;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;EfficientDET&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#23558;&#20004;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;EfficientDET&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#26469;&#33258;&#26085;&#26412;Wu-Murad&#37326;&#22806;&#25628;&#25937;&#65288;WSAR&#65289;&#21162;&#21147;&#30340;98.9 GB&#26080;&#20154;&#26426;&#22270;&#20687;&#30340;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;3&#20010;&#26041;&#21521;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#33267;&#23569;19&#31181;&#26041;&#27861;&#21644;3&#20010;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22312;&#26080;&#20154;&#26426;&#22270;&#20687;&#20013;&#23450;&#20301;&#22833;&#36394;&#20154;&#21592;&#65292;&#20294;&#21482;&#26377;3&#31181;&#26041;&#27861;&#65288;2&#31181;&#26080;&#30417;&#30563;&#21644;1&#31181;&#26410;&#30693;&#32467;&#26500;&#65289;&#22312;&#25991;&#29486;&#20013;&#34987;&#24341;&#29992;&#20026;&#23454;&#38469;WSAR&#25805;&#20316;&#20013;&#20351;&#29992;&#36807;&#12290;&#22312;&#36825;&#20123;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;EfficientDET&#26550;&#26500;&#21644;&#26080;&#30417;&#30563;&#30340;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#34987;&#36873;&#25321;&#20026;&#26368;&#36866;&#21512;&#27492;&#24773;&#26223;&#30340;&#26041;&#27861;&#12290;EfficientDET&#27169;&#22411;&#24212;&#29992;&#20110;HERIDAL&#25968;&#25454;&#38598;&#65292;&#23613;&#31649;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#26032;&#25216;&#26415;&#30456;&#24403;&#30340;&#27700;&#24179;&#65292;&#20294;&#27169;&#22411;&#22312;&#20551;&#38451;&#24615;&#26041;&#38754;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26377;&#25928;&#35782;&#21035;&#65288;&#20363;&#22914;&#65292;&#35782;&#21035;&#26641;&#26525;&#21644;&#23721;&#30707;&#65289;
&lt;/p&gt;
&lt;p&gt;
This paper details the challenges in applying two computer vision systems, an EfficientDET supervised learning model and the unsupervised RX spectral classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and rescue (WSAR) effort in Japan and identifies 3 directions for future research. There have been at least 19 proposed approaches and 3 datasets aimed at locating missing persons in drone imagery, but only 3 approaches (2 unsupervised and 1 of an unknown structure) are referenced in the literature as having been used in an actual WSAR operation. Of these proposed approaches, the EfficientDET architecture and the unsupervised spectral RX classifier were selected as the most appropriate for this setting. The EfficientDET model was applied to the HERIDAL dataset and despite achieving performance that is statistically equivalent to the state-of-the-art, the model fails to translate to the real world in terms of false positives (e.g., identifying tree limbs and rocks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.07944</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#26469;&#37325;&#26032;&#23457;&#35270;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#19982;&#20266;&#26631;&#31614;&#25216;&#26415;&#30340;&#36741;&#21161;&#24050;&#32463;&#25104;&#20026;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#65292;&#21407;&#22240;&#26159;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#20302;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#20849;&#23384;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21516;&#26102;&#23398;&#20064;&#26816;&#27979;&#25152;&#26377;&#31867;&#21035;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#24341;&#23548;&#19981;&#21516;&#20998;&#24067;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#29615;&#22659;&#24046;&#24322;&#65288;&#20363;&#22914;&#65292;&#20809;&#26463;&#25968;&#37327;&#65289;&#24102;&#26469;&#30340;&#24178;&#25200;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#22495;&#26816;&#26597;&#65288;CDE&#65289;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#23454;&#20363;&#22797;&#21046;&#31896;&#36148;&#21040;&#28304;&#29615;&#22659;&#20013;&#24182;&#27979;&#37327;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#21644;&#32531;&#35299;&#29289;&#20307;&#30340;&#36716;&#31227;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22343;&#20540;&#22330;&#25511;&#21046;&#26694;&#26550;&#65292;&#20174;NeurODEs&#21040;AutoencODEs&#65292;&#29992;&#20110;&#27169;&#25311;&#23485;&#24230;&#21487;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20462;&#25913;&#39537;&#21160;&#21160;&#24577;&#30340;&#25511;&#21046;&#22330;&#65292;&#25193;&#23637;&#20102;&#21407;&#22987;NeurODEs&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#24182;&#22788;&#29702;&#20102;&#20302;Tikhonov&#27491;&#21017;&#21270;&#24773;&#20917;&#19979;&#21487;&#33021;&#30340;&#38750;&#20984;&#25104;&#26412;&#26223;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.02279</link><description>&lt;p&gt;
&#20174;NeurODEs&#21040;AutoencODEs&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#23485;&#24230;&#21464;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#20540;&#22330;&#25511;&#21046;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From NeurODEs to AutoencODEs: a mean-field control framework for width-varying Neural Networks. (arXiv:2307.02279v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22343;&#20540;&#22330;&#25511;&#21046;&#26694;&#26550;&#65292;&#20174;NeurODEs&#21040;AutoencODEs&#65292;&#29992;&#20110;&#27169;&#25311;&#23485;&#24230;&#21487;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20462;&#25913;&#39537;&#21160;&#21160;&#24577;&#30340;&#25511;&#21046;&#22330;&#65292;&#25193;&#23637;&#20102;&#21407;&#22987;NeurODEs&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#24182;&#22788;&#29702;&#20102;&#20302;Tikhonov&#27491;&#21017;&#21270;&#24773;&#20917;&#19979;&#21487;&#33021;&#30340;&#38750;&#20984;&#25104;&#26412;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#65288;ResNets&#65289;&#19982;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#31995;&#32479;&#65288;&#31216;&#20026;NeurODEs&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#23548;&#33268;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#20026;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#32467;&#26524;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26500;&#36896;&#30340;&#20851;&#31995;&#65292;NeurODEs&#20165;&#33021;&#25551;&#36848;&#23485;&#24230;&#24658;&#23450;&#30340;&#23618;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#29992;&#20110;&#27169;&#25311;&#23485;&#24230;&#21487;&#21464;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#33258;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;AutoencODE&#65292;&#23427;&#22522;&#20110;&#39537;&#21160;&#21160;&#24577;&#30340;&#25511;&#21046;&#22330;&#30340;&#20462;&#25913;&#12290;&#36825;&#31181;&#25913;&#36827;&#20351;&#24471;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#24120;&#35268;NeurODEs&#30340;&#22343;&#20540;&#22330;&#25511;&#21046;&#26694;&#26550;&#24471;&#20197;&#25193;&#23637;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#22788;&#29702;&#20102;Tikhonov&#27491;&#21017;&#21270;&#36739;&#20302;&#30340;&#24773;&#20917;&#65292;&#23548;&#33268;&#21487;&#33021;&#38750;&#20984;&#30340;&#25104;&#26412;&#26223;&#35266;&#12290;&#34429;&#28982;&#23545;&#20110;&#36739;&#39640;&#30340;Tikhonov&#27491;&#21017;&#21270;&#65292;&#20840;&#23616;&#24471;&#21040;&#30340;&#32467;&#26524;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#24456;&#22810;&#32467;&#26524;&#22312;&#25439;&#22833;&#20989;&#25968;&#37096;&#20998;&#30340;&#21306;&#22495;&#21487;&#34987;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The connection between Residual Neural Networks (ResNets) and continuous-time control systems (known as NeurODEs) has led to a mathematical analysis of neural networks which has provided interesting results of both theoretical and practical significance. However, by construction, NeurODEs have been limited to describing constant-width layers, making them unsuitable for modeling deep learning architectures with layers of variable width. In this paper, we propose a continuous-time Autoencoder, which we call AutoencODE, based on a modification of the controlled field that drives the dynamics. This adaptation enables the extension of the mean-field control framework originally devised for conventional NeurODEs. In this setting, we tackle the case of low Tikhonov regularization, resulting in potentially non-convex cost landscapes. While the global results obtained for high Tikhonov regularization may not hold globally, we show that many of them can be recovered in regions where the loss fun
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16021</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32467;&#26500;&#65306;&#35843;&#26597;&#19982;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Structure in Reinforcement Learning: A Survey and Open Problems. (arXiv:2306.16021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16021
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#20989;&#25968;&#36924;&#36817;&#26041;&#38754;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#23545;&#22810;&#26679;&#19988;&#19981;&#21487;&#39044;&#27979;&#30340;&#21160;&#24577;&#12289;&#22024;&#26434;&#20449;&#21495;&#20197;&#21450;&#24222;&#22823;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#31561;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#26102;&#65292;&#20854;&#23454;&#29992;&#24615;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;&#35832;&#22914;&#25968;&#25454;&#25928;&#29575;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12289;&#32570;&#23569;&#23433;&#20840;&#20445;&#35777;&#21644;&#19981;&#21487;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#22312;&#36825;&#20123;&#20851;&#38190;&#25351;&#26631;&#19978;&#25552;&#39640;&#24615;&#33021;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#23558;&#38382;&#39064;&#30340;&#38468;&#21152;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#21508;&#20010;&#23376;&#39046;&#22495;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#32435;&#20837;&#36825;&#26679;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#19979;&#65292;&#25581;&#31034;&#32467;&#26500;&#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing a wide range of real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20419;&#38144;&#27963;&#21160;&#21333;&#20301;&#32463;&#27982;&#25928;&#29575;&#30340;&#22686;&#20540;&#24230;&#37327;&#26041;&#27861;IPC&#65292;&#36890;&#36807;&#21709;&#24212;&#36716;&#21270;&#26041;&#27861;&#35299;&#20915;&#21709;&#24212;&#30456;&#20851;&#25104;&#26412;&#22686;&#20540;&#27169;&#22411;&#20013;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#25110;&#35745;&#31639;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#36716;&#25442;&#36807;&#30340;&#25968;&#25454;&#12289;&#20854;&#20542;&#21521;&#24615;&#21644;&#19968;&#20010;&#20272;&#35745;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#20419;&#38144;&#27963;&#21160;&#33719;&#21033;&#20272;&#35745;&#65292;&#26159;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20419;&#38144;&#25928;&#29575;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.13759</link><description>&lt;p&gt;
&#22686;&#37327;&#36716;&#21270;&#25910;&#30410;&#65306;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20419;&#38144;&#22686;&#20540;&#27169;&#22411;&#30340;&#21709;&#24212;&#36716;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incremental Profit per Conversion: a Response Transformation for Uplift Modeling in E-Commerce Promotions. (arXiv:2306.13759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20419;&#38144;&#27963;&#21160;&#21333;&#20301;&#32463;&#27982;&#25928;&#29575;&#30340;&#22686;&#20540;&#24230;&#37327;&#26041;&#27861;IPC&#65292;&#36890;&#36807;&#21709;&#24212;&#36716;&#21270;&#26041;&#27861;&#35299;&#20915;&#21709;&#24212;&#30456;&#20851;&#25104;&#26412;&#22686;&#20540;&#27169;&#22411;&#20013;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#25110;&#35745;&#31639;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#36716;&#25442;&#36807;&#30340;&#25968;&#25454;&#12289;&#20854;&#20542;&#21521;&#24615;&#21644;&#19968;&#20010;&#20272;&#35745;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#20419;&#38144;&#27963;&#21160;&#33719;&#21033;&#20272;&#35745;&#65292;&#26159;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20419;&#38144;&#25928;&#29575;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20419;&#38144;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#37319;&#29992;&#21508;&#31181;&#25104;&#26412;&#32467;&#26500;&#26469;&#25512;&#21160;&#29992;&#25143;&#21442;&#19982;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#20855;&#26377;&#21709;&#24212;&#30456;&#20851;&#25104;&#26412;&#30340;&#20419;&#38144;&#65292;&#21482;&#26377;&#24403;&#36141;&#20080;&#21457;&#29983;&#26102;&#25165;&#20250;&#20135;&#29983;&#36153;&#29992;&#12290;&#36825;&#20123;&#20419;&#38144;&#21253;&#25324;&#25240;&#25187;&#21644;&#20248;&#24800;&#21048;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22686;&#20540;&#27169;&#22411;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#22914;&#20803;&#23398;&#20064;&#22120;&#65292;&#25110;&#32773;&#30001;&#20110;&#38750;&#36716;&#25442;&#20010;&#20307;&#30340;&#38646;&#25104;&#26412;&#21644;&#21033;&#28070;&#24341;&#36215;&#30340;&#38646;&#33192;&#32960;&#20540;&#32780;&#20272;&#31639;&#21033;&#28070;&#26102;&#36935;&#21040;&#22797;&#26434;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#37327;&#36716;&#21270;&#25910;&#30410;&#65288;IPC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#20419;&#38144;&#27963;&#21160;&#21333;&#20301;&#32463;&#27982;&#25928;&#29575;&#30340;&#22686;&#20540;&#24230;&#37327;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21709;&#24212;&#36716;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;IPC&#20165;&#38656;&#35201;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#12289;&#20854;&#20542;&#21521;&#24615;&#21644;&#19968;&#20010;&#35201;&#20272;&#35745;&#30340;&#27169;&#22411;&#21363;&#21487;&#12290;&#32467;&#26524;&#65292;IPC&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#36890;&#24120;&#19982;&#21709;&#24212;&#30456;&#20851;&#25104;&#26412;&#26377;&#20851;&#30340;&#22122;&#22768;&#12290;&#23545;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;IPC&#25552;&#20379;&#20102;&#23545;&#20419;&#38144;&#27963;&#21160;&#33719;&#21033;&#33021;&#21147;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#23450;&#30340;&#20272;&#35745;&#65292;&#26159;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20419;&#38144;&#23450;&#21521;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Promotions play a crucial role in e-commerce platforms, and various cost structures are employed to drive user engagement. This paper focuses on promotions with response-dependent costs, where expenses are incurred only when a purchase is made. Such promotions include discounts and coupons. While existing uplift model approaches aim to address this challenge, these approaches often necessitate training multiple models, like meta-learners, or encounter complications when estimating profit due to zero-inflated values stemming from non-converted individuals with zero cost and profit.  To address these challenges, we introduce Incremental Profit per Conversion (IPC), a novel uplift measure of promotional campaigns' efficiency in unit economics. Through a proposed response transformation, we demonstrate that IPC requires only converted data, its propensity, and a single model to be estimated. As a result, IPC resolves the issues mentioned above while mitigating the noise typically associate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FALL-E&#8221;&#30340;&#20315;&#21033;&#38899;&#25928;&#21512;&#25104;&#31995;&#32479;&#21450;&#20854;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26465;&#20214;&#35757;&#32451;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#20102;&#35299;&#22768;&#38899;&#36136;&#37327;&#21644;&#24405;&#38899;&#29615;&#22659;&#65292;&#24182;&#22312;DCASE 2023&#25361;&#25112;&#36187;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#22810;&#26679;&#24615;&#19978;&#24471;&#20998;&#26368;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.09807</link><description>&lt;p&gt;
FALL-E&#65306;&#19968;&#31181;&#20315;&#21033;&#38899;&#25928;&#21512;&#25104;&#27169;&#22411;&#21450;&#20854;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
FALL-E: A Foley Sound Synthesis Model and Strategies. (arXiv:2306.09807v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FALL-E&#8221;&#30340;&#20315;&#21033;&#38899;&#25928;&#21512;&#25104;&#31995;&#32479;&#21450;&#20854;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26465;&#20214;&#35757;&#32451;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#20102;&#35299;&#22768;&#38899;&#36136;&#37327;&#21644;&#24405;&#38899;&#29615;&#22659;&#65292;&#24182;&#22312;DCASE 2023&#25361;&#25112;&#36187;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#22810;&#26679;&#24615;&#19978;&#24471;&#20998;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FALL-E&#8221;&#30340;&#20315;&#21033;&#38899;&#25928;&#21512;&#25104;&#31995;&#32479;&#21450;&#20854;&#35757;&#32451;/&#25512;&#26029;&#31574;&#30053;&#12290;FALL-E&#27169;&#22411;&#37319;&#29992;&#32423;&#32852;&#26041;&#27861;&#65292;&#21253;&#25324;&#20302;&#20998;&#36776;&#29575;&#39057;&#35889;&#22270;&#29983;&#25104;&#12289;&#39057;&#35889;&#22270;&#36229;&#20998;&#36776;&#29575;&#21644;&#22768;&#30721;&#22120;&#12290;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#27599;&#20010;&#19982;&#22768;&#38899;&#30456;&#20851;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#29305;&#23450;&#30340;&#25991;&#26412;&#23558;&#27169;&#22411;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#20102;&#35299;&#22768;&#38899;&#36136;&#37327;&#21644;&#24405;&#38899;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#36827;&#34892;&#20102;&#36136;&#37327;&#12289;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;FALL-E&#22312;DCASE 2023&#25361;&#25112;&#36187;&#20219;&#21153;7&#20013;&#36827;&#34892;&#20102;&#23458;&#35266;&#35780;&#20272;&#21644;&#21548;&#21147;&#27979;&#35797;&#12290;&#25552;&#20132;&#32467;&#26524;&#22312;&#24179;&#22343;&#24471;&#20998;&#19978;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#21516;&#26102;&#22312;&#22810;&#26679;&#24615;&#19978;&#24471;&#20998;&#26368;&#39640;&#65292;&#22312;&#38899;&#39057;&#36136;&#37327;&#19978;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#22312;&#31867;&#36866;&#24212;&#24615;&#19978;&#33719;&#24471;&#31532;&#19977;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FALL-E, a foley synthesis system and its training/inference strategies. The FALL-E model employs a cascaded approach comprising low-resolution spectrogram generation, spectrogram super-resolution, and a vocoder. We trained every sound-related model from scratch using our extensive datasets, and utilized a pre-trained language model. We conditioned the model with dataset-specific texts, enabling it to learn sound quality and recording environment based on text input. Moreover, we leveraged external language models to improve text descriptions of our datasets and performed prompt engineering for quality, coherence, and diversity. FALL-E was evaluated by an objective measure as well as listening tests in the DCASE 2023 challenge Task 7. The submission achieved the second place on average, while achieving the best score for diversity, second place for audio quality, and third place for class fitness.
&lt;/p&gt;</description></item><item><title>&#36229;&#32593;&#32476;&#26159;&#19968;&#31181;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21160;&#24577;&#24615;&#12289;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12289;&#20449;&#24687;&#20849;&#20139;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#20248;&#28857;&#12290;&#23427;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#20013;&#26174;&#31034;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#25345;&#32493;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#26435;&#37325;&#21098;&#26525;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2306.06955</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36229;&#32593;&#32476;&#31616;&#35201;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Brief Review of Hypernetworks in Deep Learning. (arXiv:2306.06955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06955
&lt;/p&gt;
&lt;p&gt;
&#36229;&#32593;&#32476;&#26159;&#19968;&#31181;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21160;&#24577;&#24615;&#12289;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12289;&#20449;&#24687;&#20849;&#20139;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#20248;&#28857;&#12290;&#23427;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#20013;&#26174;&#31034;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#25345;&#32493;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#26435;&#37325;&#21098;&#26525;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32593;&#32476;&#65288;Hypernetworks&#65289;&#26159;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#30446;&#26631;&#32593;&#32476;&#65289;&#26435;&#37325;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#20204;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20986;&#29616;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21160;&#24577;&#24615;&#12289;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12289;&#20449;&#24687;&#20849;&#20139;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#12290;&#36229;&#32593;&#32476;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#20013;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#25345;&#32493;&#23398;&#20064;&#12289;&#22240;&#26524;&#25512;&#26029;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#26435;&#37325;&#21098;&#26525;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#12290;&#23613;&#31649;&#36229;&#32593;&#32476;&#22312;&#19981;&#21516;&#30340;&#38382;&#39064;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#21487;&#29992;&#30340;&#32508;&#36848;&#26469;&#21578;&#30693;&#30740;&#31350;&#20154;&#21592;&#26377;&#20851;&#20854;&#21457;&#23637;&#24773;&#20917;&#24182;&#24110;&#21161;&#21033;&#29992;&#36229;&#32593;&#32476;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36229;&#32593;&#32476;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20363;&#23376;&#26469;&#35828;&#26126;&#22914;&#20309;&#20351;&#29992;&#36229;&#32593;&#32476;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20116;&#20010;&#35774;&#35745;&#20934;&#21017;&#23545;&#36229;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypernetworks, or hypernets in short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression etc. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning etc. Despite their success across different problem settings, currently, there is no review available to inform the researchers about the developments and to help in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example to train deep neural networks using hypernets and propose categorizing hypernets based on five design criteria as inputs, outputs, variabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30740;&#31350;&#20102;&#20851;&#38190;&#31232;&#30095;&#24615;&#65292;&#21457;&#29616;&#22312;&#21435;&#38500;&#20855;&#26377;&#26368;&#23567;&#24133;&#24230;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19979;&#38477;&#36895;&#24230;&#19982;&#31232;&#30095;&#31243;&#24230;&#30340;&#22686;&#21152;&#21576;&#29616;&#20986;&#25296;&#28857;&#65292;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.03805</link><description>&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#31232;&#30095;&#24615;&#30340;&#20986;&#29616;&#65306;&#37325;&#35201;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter. (arXiv:2306.03805v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30740;&#31350;&#20102;&#20851;&#38190;&#31232;&#30095;&#24615;&#65292;&#21457;&#29616;&#22312;&#21435;&#38500;&#20855;&#26377;&#26368;&#23567;&#24133;&#24230;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19979;&#38477;&#36895;&#24230;&#19982;&#31232;&#30095;&#31243;&#24230;&#30340;&#22686;&#21152;&#21576;&#29616;&#20986;&#25296;&#28857;&#65292;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22240;&#27492;&#29702;&#35299;&#23427;&#20204;&#20869;&#37096;&#23384;&#22312;&#30340;&#31616;&#21270;&#27169;&#24335;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#36807;&#36845;&#20195;&#37327;&#21270;&#21098;&#26525;&#65288;IMP&#65289;&#30340;&#8220;&#25277;&#22870;&#31080;&#25454;&#20551;&#35774;&#8221;&#65288;LTH&#65289;&#21450;&#20854;&#21464;&#20307;&#24050;&#32463;&#22833;&#21435;&#20102;&#31232;&#30095;&#21270;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#65292;&#22240;&#20026;&#37325;&#22797;&#30340;&#35757;&#32451;-&#21098;&#26525;-&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#23548;&#33268;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#29942;&#39048;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#38382;&#39064;&#22312;&#27169;&#22411;&#35268;&#27169;&#22686;&#22823;&#26102;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#22312;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#21464;&#21387;&#22120;&#20013;&#24341;&#21457;&#30340;&#31232;&#30095;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#31232;&#30095;&#24615;&#30340;&#23384;&#22312;&#65292;&#21363;&#24403;&#25105;&#20204;&#19968;&#27425;&#24615;&#30452;&#25509;&#21024;&#38500;&#20855;&#26377;&#26368;&#23567;&#24133;&#24230;&#30340;&#26435;&#37325;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#36895;&#24230;&#38543;&#30528;&#31232;&#30095;&#31243;&#24230;&#30340;&#22686;&#21152;&#32780;&#21152;&#24555;&#65292;&#23384;&#22312;&#19968;&#20010;&#25296;&#28857;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20851;&#38190;&#31232;&#30095;&#24615;&#22312;N:M&#31232;&#30095;&#27169;&#24335;&#20197;&#21450;&#29616;&#20195;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained transformers are show-stealer in modern-day deep learning, and it becomes crucial to comprehend the parsimonious patterns that exist within them as they grow in scale. With exploding parameter counts, Lottery Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in sparsifying them due to high computation and memory bottleneck of repetitive train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens with increasing model size. This paper comprehensively studies induced sparse patterns across multiple large pre-trained vision and language transformers. We propose the existence of -- essential sparsity defined with a sharp dropping point beyond which the performance declines much faster w.r.t the rise of sparsity level, when we directly remove weights with the smallest magnitudes in one-shot without re-training. We also find essential sparsity to hold valid for N:M sparsity patterns as well as on modern-scale large language models (Vicu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#21521;-&#21069;&#21521;&#31639;&#27861;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#65292;&#35299;&#20915;&#20102;&#20809;&#23398;&#24179;&#21488;&#35757;&#32451;&#22810;&#20010;&#21487;&#35757;&#32451;&#23618;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19170</link><description>&lt;p&gt;
&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Forward-Forward Training of an Optical Neural Network. (arXiv:2305.19170v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19170
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#21521;-&#21069;&#21521;&#31639;&#27861;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#65292;&#35299;&#20915;&#20102;&#20809;&#23398;&#24179;&#21488;&#35757;&#32451;&#22810;&#20010;&#21487;&#35757;&#32451;&#23618;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23494;&#38598;&#30340;&#35745;&#31639;&#24615;&#36136;&#35201;&#27714;&#26356;&#24555;&#36895;&#21644;&#26356;&#33410;&#33021;&#30340;&#30828;&#20214;&#23454;&#29616;&#12290;&#22522;&#20110;&#20809;&#23398;&#30340;&#24179;&#21488;&#65292;&#21033;&#29992;&#30789;&#20809;&#23376;&#23398;&#21644;&#31354;&#38388;&#20809;&#35843;&#21046;&#22120;&#31561;&#25216;&#26415;&#65292;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#20123;&#29289;&#29702;&#31995;&#32479;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#21487;&#35757;&#32451;&#30340;&#23618;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#23436;&#20840;&#34920;&#24449;&#24182;&#29992;&#21487;&#24494;&#20989;&#25968;&#36827;&#34892;&#25551;&#36848;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#20351;&#29992;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#21069;&#21521;-&#21069;&#21521;&#31639;&#27861;&#65288;FFA&#65289;&#28040;&#38500;&#20102;&#23545;&#23398;&#20064;&#31995;&#32479;&#23436;&#32654;&#34920;&#24449;&#30340;&#38656;&#27714;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#22823;&#37327;&#21487;&#32534;&#31243;&#21442;&#25968;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;FFA&#19981;&#38656;&#35201;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#35823;&#24046;&#20449;&#21495;&#26469;&#26356;&#26032;&#26435;&#37325;&#65292;&#32780;&#26159;&#20165;&#36890;&#36807;&#21333;&#21521;&#20256;&#36882;&#20449;&#24687;&#26469;&#26356;&#26032;&#26435;&#37325;&#12290;&#23545;&#20110;&#27599;&#32452;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#65292;&#26412;&#22320;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#20248;&#21270;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NN) have demonstrated remarkable capabilities in various tasks, but their computation-intensive nature demands faster and more energy-efficient hardware implementations. Optics-based platforms, using technologies such as silicon photonics and spatial light modulators, offer promising avenues for achieving this goal. However, training multiple trainable layers in tandem with these physical systems poses challenges, as they are difficult to fully characterize and describe with differentiable functions, hindering the use of error backpropagation algorithm. The recently introduced Forward-Forward Algorithm (FFA) eliminates the need for perfect characterization of the learning system and shows promise for efficient training with large numbers of programmable parameters. The FFA does not require backpropagating an error signal to update the weights, rather the weights are updated by only sending information in one direction. The local loss function for each set of trainable 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31232;&#30095;&#21452;&#20998;&#22270;&#32467;&#26500;&#22270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;GraFITi&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#20855;&#26377;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#32570;&#22833;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.12932</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#24418;&#39044;&#27979;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Forecasting Irregularly Sampled Time Series using Graphs. (arXiv:2305.12932v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12932
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#21452;&#20998;&#22270;&#32467;&#26500;&#22270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;GraFITi&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#20855;&#26377;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#32570;&#22833;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20581;&#24247;&#12289;&#22825;&#25991;&#21644;&#27668;&#20505;&#31185;&#23398;&#31561;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#32570;&#22833;&#20540;&#38382;&#39064;&#65292;&#20934;&#30830;&#39044;&#27979;&#20855;&#26377;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#32570;&#22833;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#30446;&#21069;&#38024;&#23545;&#27492;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#20381;&#36182;&#20110;&#24050;&#30693;&#36895;&#24230;&#24930;&#19988;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#29305;&#24449;&#26469;&#22788;&#29702;&#32570;&#22833;&#20540;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;GraFITi&#65292;&#23427;&#20351;&#29992;&#22270;&#24418;&#26469;&#39044;&#27979;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#12290;GraFITi&#39318;&#20808;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#21270;&#20026;&#31232;&#30095;&#21452;&#20998;&#22270;&#32467;&#26500;&#22270;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22270;&#20013;&#36793;&#26435;&#39044;&#27979;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21147;&#37327;&#26469;&#23398;&#20064;&#22270;&#24182;&#39044;&#27979;&#30446;&#26631;&#36793;&#26435;&#12290;GraFITi&#24050;&#32463;&#22312;3&#20010;&#30495;&#23454;&#19990;&#30028;&#21644;1&#20010;&#21512;&#25104;&#30340;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#28436;&#31034;&#20102;GraFITi&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting irregularly sampled time series with missing values is a crucial task for numerous real-world applications such as healthcare, astronomy, and climate sciences. State-of-the-art approaches to this problem rely on Ordinary Differential Equations (ODEs) which are known to be slow and often require additional features to handle missing values. To address this issue, we propose a novel model using Graphs for Forecasting Irregularly Sampled Time Series with missing values which we call GraFITi. GraFITi first converts the time series to a Sparsity Structure Graph which is a sparse bipartite graph, and then reformulates the forecasting problem as the edge weight prediction task in the graph. It uses the power of Graph Neural Networks to learn the graph and predict the target edge weights. GraFITi has been tested on 3 real-world and 1 synthetic irregularly sampled time series dataset with missing values and compared with various state-of-the-art models. The experimental results demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.11509</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#25628;&#32034;&#21040;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25628;&#32034;&#26159;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#20196;&#20154;&#24778;&#21497;&#65292;&#20294;&#24456;&#23569;&#26377;&#38750;&#21551;&#21457;&#24335;&#30340;&#29702;&#35770;&#29992;&#20110;&#25551;&#36848;&#20854;&#24037;&#20316;&#26426;&#21046;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#20851;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#24182;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#29615;&#22659;&#27809;&#26377;&#22122;&#22768;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#20854;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $&#65292;&#20854;&#20013;$ d_s \ge 0 $&#26159;&#24213;&#23618;&#20989;&#25968;&#30340;&#25955;&#23556;&#32500;&#24230;&#12290;&#24403;&#35266;&#23519;&#21040;&#30340;&#20989;&#25968;&#20540;&#21463;&#21040;&#26377;&#30028;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#24433;&#21709;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{2}{2+d_s} } \right) $&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \rig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#33258;&#20027;&#28293;&#23556;&#27785;&#31215;&#34180;&#33180;&#30340;&#20202;&#22120;&#65292;&#21033;&#29992;Python&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#25511;&#21046;&#34180;&#33180;&#32452;&#25104;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#27493;&#20240;&#12290;</title><link>http://arxiv.org/abs/2305.11122</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#20809;&#23398;&#31561;&#31163;&#23376;&#20307;&#21457;&#23556;&#30340;&#33258;&#20027;&#28293;&#23556;&#21512;&#25104;&#34180;&#33180;&#27694;&#21270;&#29289;
&lt;/p&gt;
&lt;p&gt;
Autonomous sputter synthesis of thin film nitrides with composition controlled by Bayesian optimization of optical plasma emission. (arXiv:2305.11122v2 [physics.app-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#33258;&#20027;&#28293;&#23556;&#27785;&#31215;&#34180;&#33180;&#30340;&#20202;&#22120;&#65292;&#21033;&#29992;Python&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#25511;&#21046;&#34180;&#33180;&#32452;&#25104;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#27493;&#20240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#23454;&#39564;&#24050;&#25104;&#20026;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#27493;&#20240;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#34429;&#28982;&#33258;&#20027;&#21512;&#25104;&#30340;&#20202;&#22120;&#22312;&#20998;&#23376;&#21644;&#32858;&#21512;&#29289;&#31185;&#23398;&#20013;&#24050;&#32463;&#24456;&#27969;&#34892;&#65292;&#20294;&#26159;&#29992;&#20110;&#28151;&#21512;&#26448;&#26009;&#21644;&#32435;&#31859;&#39063;&#31890;&#30340;&#28342;&#28082;&#22788;&#29702;&#65292;&#29289;&#29702;&#27668;&#30456;&#27785;&#31215;&#30340;&#33258;&#20027;&#24037;&#20855;&#21364;&#24456;&#23569;&#35265;&#65292;&#20294;&#23545;&#20110;&#21322;&#23548;&#20307;&#34892;&#19994;&#32780;&#35328;&#21364;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#33258;&#20027;&#28293;&#23556;&#27785;&#31215;&#34180;&#33180;&#30340;&#20202;&#22120;&#35774;&#35745;&#21644;&#23454;&#26045;&#65292;&#21033;&#29992;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;&#28293;&#23556;&#21453;&#24212;&#22120;&#12289;Python&#12289;&#20809;&#30005;&#21457;&#23556;&#20809;&#35889;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#25511;&#21046;&#34180;&#33180;&#32452;&#25104;&#12290;&#25105;&#20204;&#23558;&#30001;&#20803;&#32032;&#38156;&#21644;&#38043;&#22312;&#27694;&#27668;&#27668;&#27675;&#19979;&#20849;&#28293;&#23556;&#26102;&#30417;&#27979;&#21040;&#30340;&#21457;&#23556;&#32447;&#20316;&#20026;&#20809;&#35889;&#25968;&#25454;&#65292;&#24182;&#23558;&#34180;&#33180;&#25104;&#20998;&#65288;&#30001;X&#33639;&#20809;&#27861;&#27979;&#37327;&#65289;&#24314;&#27169;&#20026;&#21457;&#23556;&#32447;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#30001;OES&#25552;&#20379;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#25511;&#21046;&#31639;&#27861;&#22312;&#28293;&#23556;&#21151;&#29575;&#30340;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#20197;&#21046;&#36896;&#31526;&#21512;&#29992;&#25143;&#23450;&#20041;&#30340;&#32452;&#25104;&#30340;&#34180;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous experimentation has emerged as an efficient approach to accelerate the pace of materials discovery. Although instruments for autonomous synthesis have become popular in molecular and polymer science, solution processing of hybrid materials and nanoparticles, examples of autonomous tools for physical vapour deposition are scarce yet important for the semiconductor industry. Here, we report the design and implementation of an autonomous instrument for sputter deposition of thin films with controlled composition, leveraging a highly automated sputtering reactor custom-controlled by Python, optical emission spectroscopy (OES), and Bayesian optimization algorithm. We modeled film composition, measured by x-ray fluorescence, as a linear function of emission lines monitored during the co-sputtering from elemental Zn and Ti targets in N$_2$ atmosphere. A Bayesian control algorithm, informed by OES, navigates the space of sputtering power to fabricate films with user-defined composit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#22122;&#22768;&#22810;&#30340;&#21307;&#30103;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03829</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22240;&#26524;&#27169;&#22411;&#25552;&#39640;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;
&lt;/p&gt;
&lt;p&gt;
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#22122;&#22768;&#22810;&#30340;&#21307;&#30103;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#26088;&#22312;&#26681;&#25454;&#20010;&#20307;&#30340;&#29420;&#29305;&#25104;&#20687;&#29305;&#24449;&#20010;&#24615;&#21270;&#35786;&#30103;&#20915;&#31574;&#65292;&#20197;&#25913;&#21892;&#20854;&#20020;&#24202;&#32467;&#26524;&#12290;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20316;&#20026;&#27835;&#30103;&#24314;&#35758;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23558;&#26356;&#21152;&#23433;&#20840;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#22312;&#31934;&#20934;&#21307;&#30103;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#21644;&#39564;&#35777;&#25351;&#26631;&#12290;&#26412;&#25991;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#20272;&#35745;&#27599;&#31181;&#27835;&#30103;&#36873;&#39033;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#20219;&#24847;&#20004;&#31181;&#27835;&#30103;&#20043;&#38388;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#25105;&#20204;&#23545;&#24739;&#26377;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30340;&#24739;&#32773;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20197;&#39044;&#27979;&#26032;&#30340;&#21644;&#25193;&#22823;&#30340;T2&#30149;&#21464;&#25968;&#37327;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based precision medicine aims to personalize treatment decisions based on an individual's unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of 
&lt;/p&gt;</description></item><item><title>{\Pi}-ML&#26159;&#19968;&#31181;&#22522;&#20110;&#23610;&#23544;&#20998;&#26512;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#27668;&#34920;&#23618;&#20809;&#23398;&#28237;&#27969;&#30340;&#24378;&#24230;($C_n^2$)&#12290; &#29289;&#29702;&#23646;&#24615;&#30340;&#29305;&#24449;&#20998;&#26512;&#34920;&#26126;&#65292;&#28508;&#22312;&#28201;&#24230;&#30340;&#24402;&#19968;&#21270;&#26041;&#24046;&#26159;&#39044;&#27979; $C_n^2$ &#30340;&#20851;&#38190;&#29305;&#24449;&#12290; &#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#38598;&#21512;&#65292;&#25105;&#20204;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12177</link><description>&lt;p&gt;
{\Pi}-ML&#65306;&#22522;&#20110;&#23610;&#23544;&#20998;&#26512;&#30340;&#22823;&#27668;&#34920;&#23618;&#20809;&#23398;&#28237;&#27969;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
{\Pi}-ML: A dimensional analysis-based machine learning parameterization of optical turbulence in the atmospheric surface layer. (arXiv:2304.12177v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12177
&lt;/p&gt;
&lt;p&gt;
{\Pi}-ML&#26159;&#19968;&#31181;&#22522;&#20110;&#23610;&#23544;&#20998;&#26512;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#27668;&#34920;&#23618;&#20809;&#23398;&#28237;&#27969;&#30340;&#24378;&#24230;($C_n^2$)&#12290; &#29289;&#29702;&#23646;&#24615;&#30340;&#29305;&#24449;&#20998;&#26512;&#34920;&#26126;&#65292;&#28508;&#22312;&#28201;&#24230;&#30340;&#24402;&#19968;&#21270;&#26041;&#24046;&#26159;&#39044;&#27979; $C_n^2$ &#30340;&#20851;&#38190;&#29305;&#24449;&#12290; &#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#38598;&#21512;&#65292;&#25105;&#20204;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#25240;&#23556;&#29575;&#30340;&#28237;&#27969;&#27874;&#21160;&#65292;&#21363;&#20809;&#23398;&#28237;&#27969;&#65292;&#21487;&#20197;&#23548;&#33268;&#28608;&#20809;&#26463;&#30340;&#26174;&#33879;&#25197;&#26354;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#27169;&#22411;&#26469;&#20272;&#35745;&#36825;&#20123;&#27874;&#21160;&#30340;&#24378;&#24230; ($C_n^2$) &#23545;&#20110;&#25104;&#21151;&#24320;&#21457;&#21644;&#37096;&#32626;&#26410;&#26469;&#33258;&#30001;&#31354;&#38388;&#20809;&#36890;&#20449;&#38142;&#36335;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23610;&#23544;&#20998;&#26512;&#21644;&#26799;&#24230;&#25552;&#21319;&#30340;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064; (ML) &#26041;&#27861; $\Pi$-ML &#26469;&#20272;&#35745; $C_n^2$&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28508;&#22312;&#28201;&#24230;&#24402;&#19968;&#21270;&#26041;&#24046;&#20316;&#20026;&#39044;&#27979; $C_n^2$ &#30340;&#20027;&#35201;&#29305;&#24449;&#12290;&#20026;&#20102;&#20445;&#35777;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#38598;&#21512;&#65292;&#35813;&#38598;&#21512;&#22312; $R^2=0.958\pm0.001$ &#30340;&#26679;&#26412;&#22806;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turbulent fluctuations of the atmospheric refraction index, so-called optical turbulence, can significantly distort propagating laser beams. Therefore, modeling the strength of these fluctuations ($C_n^2$) is highly relevant for the successful development and deployment of future free-space optical communication links. In this letter, we propose a physics-informed machine learning (ML) methodology, $\Pi$-ML, based on dimensional analysis and gradient boosting to estimate $C_n^2$. Through a systematic feature importance analysis, we identify the normalized variance of potential temperature as the dominating feature for predicting $C_n^2$. For statistical robustness, we train an ensemble of models which yields high performance on the out-of-sample data of $R^2=0.958\pm0.001$.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; C-qGAN &#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#32467;&#26500;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#65292;&#21487;&#20197;&#21033;&#29992;&#35813;&#26041;&#27861;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#23450;&#20215;&#30340;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.10382</link><description>&lt;p&gt;
&#23398;&#20064;&#38543;&#26426;&#36807;&#31243;&#30340;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models for Learning Stochastic Processes. (arXiv:2304.10382v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; C-qGAN &#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#32467;&#26500;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#65292;&#21487;&#20197;&#21033;&#29992;&#35813;&#26041;&#27861;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#23450;&#20215;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26465;&#20214;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;C-qGAN&#65289;&#12290;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20005;&#26684;&#37319;&#29992;&#37327;&#23376;&#30005;&#36335;&#65292;&#22240;&#27492;&#34987;&#35777;&#26126;&#33021;&#22815;&#27604;&#24403;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#28508;&#21147;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#23637;&#31034;&#20102;&#32593;&#32476;&#22312;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21518;&#65292;&#23558;&#35813;&#25216;&#26415;&#24212;&#29992;&#20110;&#23450;&#20215;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#20854;&#20182;&#36335;&#24452;&#30456;&#20851;&#26399;&#26435;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A framework to learn a multi-modal distribution is proposed, denoted as the Conditional Quantum Generative Adversarial Network (C-qGAN). The neural network structure is strictly within a quantum circuit and, as a consequence, is shown to represents a more efficient state preparation procedure than current methods. This methodology has the potential to speed-up algorithms, such as Monte Carlo analysis. In particular, after demonstrating the effectiveness of the network in the learning task, the technique is applied to price Asian option derivatives, providing the foundation for further research on other path-dependent options.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.05874</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;EEG&#25968;&#25454;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21487;&#35299;&#37322;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data. (arXiv:2304.05874v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20998;&#31867;&#33041;&#30005;&#22270;(EEG)&#25968;&#25454;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#35786;&#26029;&#20173;&#28982;&#26159;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;AGGCN&#36890;&#36807;&#23558;&#22522;&#20110;&#21367;&#31215;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#19982;&#22522;&#20110;&#21151;&#33021;&#36830;&#25509;&#24615;&#30340;&#33879;&#21517;&#30456;&#20851;&#24230;&#37327;&#30456;&#32467;&#21512;&#26469;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#38376;&#25511;&#22270;&#21367;&#31215;&#21487;&#20197;&#21160;&#24577;&#22320;&#21152;&#26435;&#32771;&#34385;&#21508;&#31181;&#31354;&#38388;&#23610;&#24230;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38381;&#30524;&#21644;&#30529;&#30524;&#29366;&#24577;&#19979;&#22343;&#33021;&#21462;&#24471;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#32467;&#26524;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;AGGCN&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;AD&#26368;&#21463;&#24433;&#21709;&#30340;&#33041;&#21306;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) models are increasingly being used for the classification of electroencephalography (EEG) data. However, GNN-based diagnosis of neurological disorders, such as Alzheimer's disease (AD), remains a relatively unexplored area of research. Previous studies have relied on functional connectivity methods to infer brain graph structures and used simple GNN architectures for the diagnosis of AD. In this work, we propose a novel adaptive gated graph convolutional network (AGGCN) that can provide explainable predictions. AGGCN adaptively learns graph structures by combining convolution-based node feature enhancement with a well-known correlation-based measure of functional connectivity. Furthermore, the gated graph convolution can dynamically weigh the contribution of various spatial scales. The proposed model achieves high accuracy in both eyes-closed and eyes-open conditions, indicating the stability of learned representations. Finally, we demonstrate that the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;$\ell_2$-norm&#19979;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#21644;&#20855;&#20307;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#21892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.14961</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#36827;&#34892;&#35748;&#35777;&#21644;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection. (arXiv:2303.14961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;$\ell_2$-norm&#19979;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#21644;&#20855;&#20307;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#21892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#19981;&#26029;&#25193;&#23637;&#65292;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#35782;&#21035;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#26469;&#33258;&#35757;&#32451;&#20998;&#24067;&#65292;&#25110;&#32773;&#26159;&#19968;&#20010;&#8220;&#26679;&#26412;&#22806;&#8221;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#23545;&#25163;&#21487;&#20197;&#20197;&#19968;&#31181;&#23548;&#33268;&#20998;&#31867;&#22120;&#20570;&#20986;&#33258;&#20449;&#39044;&#27979;&#30340;&#26041;&#24335;&#25805;&#32437;OOD&#26679;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#30340;L2&#33539;&#22260;&#20869;&#35777;&#26126;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#20197;&#21450;&#19981;&#38656;&#35201;&#29305;&#23450;&#32452;&#20214;&#25110;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;OOD&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#24067;&#26679;&#26412;&#30340;&#39640;&#27700;&#24179;&#30340;&#35748;&#35777;&#21644;&#23545;&#25239;&#30340;&#32467;&#26524;&#12290;&#22312;CIFAR10/100&#30340;&#25152;&#26377;OOD&#26816;&#27979;&#25351;&#26631;&#30340;&#24179;&#22343;&#20540;&#26174;&#31034;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#25552;&#39640;&#20102;&#32422;13&#65285;/ 5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of machine learning continues to expand, the importance of ensuring its safety cannot be overstated. A key concern in this regard is the ability to identify whether a given sample is from the training distribution, or is an "Out-Of-Distribution" (OOD) sample. In addition, adversaries can manipulate OOD samples in ways that lead a classifier to make a confident prediction. In this study, we present a novel approach for certifying the robustness of OOD detection within a $\ell_2$-norm around the input, regardless of network architecture and without the need for specific components or additional training. Further, we improve current techniques for detecting adversarial attacks on OOD samples, while providing high levels of certified and adversarial robustness on in-distribution samples. The average of all OOD detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$ relative to previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992; kernel &#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#33021;&#38553;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#22522;&#24577;&#30340;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#38656;&#35201;&#22810;&#39033;&#24335;&#36164;&#28304;&#23454;&#29616;&#65292;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08902</link><description>&lt;p&gt;
&#29992; Kernel &#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#33021;&#38553;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;
&lt;/p&gt;
&lt;p&gt;
Learning ground states of gapped quantum Hamiltonians with Kernel Methods. (arXiv:2303.08902v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992; kernel &#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#33021;&#38553;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#22522;&#24577;&#30340;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#38656;&#35201;&#22810;&#39033;&#24335;&#36164;&#28304;&#23454;&#29616;&#65292;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#22522;&#24577;&#30340;&#26041;&#27861;&#38656;&#35201;&#35299;&#20915;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992; kernel &#26041;&#27861;&#26469;&#20351;&#20248;&#21270;&#21464;&#24471;&#31616;&#21333;&#30340;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#26159;&#21151;&#29575;&#27861;&#30340;&#19968;&#31181;&#36817;&#20284;&#23454;&#29616;&#65292;&#20854;&#20013;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26469;&#23398;&#20064;&#21151;&#29575;&#36845;&#20195;&#30340;&#19979;&#19968;&#27493;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20551;&#35774;&#30417;&#30563;&#23398;&#20064;&#26159;&#26377;&#25928;&#30340;&#65292;&#37027;&#20040;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#36164;&#28304;&#23454;&#29616;&#23545;&#20219;&#24847;&#20855;&#26377;&#33021;&#38553;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;&#24615;&#36136;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#20351;&#29992; kernel ridge &#22238;&#24402;&#65292;&#36890;&#36807;&#23545;&#19968;&#32500;&#21644;&#20108;&#32500;&#30340;&#20960;&#20010;&#20856;&#22411;&#30456;&#20114;&#20316;&#29992;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#36827;&#34892;&#22522;&#24577;&#30340;&#23547;&#25214;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#20540;&#27169;&#25311;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network approaches to approximate the ground state of quantum hamiltonians require the numerical solution of a highly nonlinear optimization problem. We introduce a statistical learning approach that makes the optimization trivial by using kernel methods. Our scheme is an approximate realization of the power method, where supervised learning is used to learn the next step of the power iteration. We show that the ground state properties of arbitrary gapped quantum hamiltonians can be reached with polynomial resources under the assumption that the supervised learning is efficient. Using kernel ridge regression, we provide numerical evidence that the learning assumption is verified by applying our scheme to find the ground states of several prototypical interacting many-body quantum systems, both in one and two dimensions, showing the flexibility of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#37319;&#29992;&#22810;&#25351;&#26631;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#24615;&#22320;&#35782;&#21035;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.06601</link><description>&lt;p&gt;
&#22810;&#25351;&#26631;&#36866;&#24212;&#24615;&#22320;&#35782;&#21035;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Multi-metrics adaptively identifies backdoors in Federated learning. (arXiv:2303.06601v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#37319;&#29992;&#22810;&#25351;&#26631;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#24615;&#22320;&#35782;&#21035;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#24615;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#25805;&#32437;&#27169;&#22411;&#23545;&#29305;&#23450;&#25932;&#23545;&#36873;&#25321;&#30340;&#36755;&#20837;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#32479;&#35745;&#24046;&#24322;&#30340;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#21482;&#23545;&#29305;&#23450;&#25915;&#20987;&#36215;&#25928;&#65292;&#29305;&#21035;&#26159;&#24403;&#24694;&#24847;&#26799;&#24230;&#19982;&#33391;&#24615;&#26799;&#24230;&#30456;&#20284;&#25110;&#25968;&#25454;&#39640;&#24230;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#36317;&#31163;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#39640;&#32500;&#24230;&#20013;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21464;&#24471;&#26080;&#24847;&#20041;&#65292;&#24182;&#19988;&#21333;&#20010;&#25351;&#26631;&#26080;&#27861;&#35782;&#21035;&#20855;&#26377;&#22810;&#31181;&#29305;&#24449;&#30340;&#24694;&#24847;&#26799;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#20351;&#29992;&#22810;&#25351;&#26631;&#21644;&#21160;&#24577;&#21152;&#26435;&#26469;&#36866;&#24212;&#24615;&#22320;&#35782;&#21035;&#21518;&#38376;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#22411;&#38450;&#24481;&#31574;&#30053;&#19981;&#20381;&#36182;&#20110;&#25915;&#20987;&#35774;&#32622;&#25110;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#65292;&#24182;&#23545;&#33391;&#24615;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The decentralized and privacy-preserving nature of federated learning (FL) makes it vulnerable to backdoor attacks aiming to manipulate the behavior of the resulting model on specific adversary-chosen inputs. However, most existing defenses based on statistical differences take effect only against specific attacks, especially when the malicious gradients are similar to benign ones or the data are highly non-independent and identically distributed (non-IID). In this paper, we revisit the distance-based defense methods and discover that i) Euclidean distance becomes meaningless in high dimensions and ii) malicious gradients with diverse characteristics cannot be identified by a single metric. To this end, we present a simple yet effective defense strategy with multi-metrics and dynamic weighting to identify backdoors adaptively. Furthermore, our novel defense has no reliance on predefined assumptions over attack settings or data distributions and little impact on benign performance. To e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#24555;&#36895;&#22320;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#20197;&#22270;&#20026;&#27169;&#22411;&#30340;&#21452;&#23618;&#38382;&#39064;&#20915;&#31574;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.06024</link><description>&lt;p&gt;
&#19968;&#31181;&#31163;&#25955;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#30340;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;-&#20803;&#21551;&#21457;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A hybrid deep-learning-metaheuristic framework for discrete road network design problems. (arXiv:2303.06024v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#24555;&#36895;&#22320;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#20197;&#22270;&#20026;&#27169;&#22411;&#30340;&#21452;&#23618;&#38382;&#39064;&#20915;&#31574;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#23618;&#26550;&#26500;&#30340;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#20803;&#21551;&#21457;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#65288;NDPs&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35757;&#32451;&#26469;&#36817;&#20284;&#29992;&#25143;&#22343;&#34913;&#65288;UE&#65289;&#20132;&#36890;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#30340;&#25512;&#29702;&#26469;&#35745;&#31639;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#35780;&#20272;&#65292;&#20197;&#36817;&#20284;&#35299;&#20915;NDPs&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;NDP&#21464;&#37327;&#21644;&#19968;&#20010;&#31934;&#30830;&#27714;&#35299;&#22120;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#23569;&#20110;1&#65285;&#30340;&#26102;&#38388;&#20869;&#32473;&#20986;&#20840;&#23616;&#26368;&#20248;&#32467;&#26524;&#30340;5&#65285;&#24038;&#21491;&#30340;&#38388;&#38553;&#20869;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#19987;&#23478;&#31995;&#32479;&#20013;&#20351;&#29992;&#65292;&#29992;&#20110;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#65292;&#20197;&#26234;&#33021;&#22320;&#30830;&#23450;&#26368;&#20339;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#20915;&#31574;&#12290;&#30001;&#20110;&#35813;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#35768;&#22810;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22270;&#19978;&#21452;&#23618;&#38382;&#39064;&#30340;&#20854;&#20182;&#20915;&#31574;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#35768;&#22810;&#26377;&#36259;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a hybrid deep-learning-metaheuristic framework with a bi-level architecture for road network design problems (NDPs). We train a graph neural network (GNN) to approximate the solution of the user equilibrium (UE) traffic assignment problem, and use inferences made by the trained model to calculate fitness function evaluations of a genetic algorithm (GA) to approximate solutions for NDPs. Using two NDP variants and an exact solver as benchmark, we show that our proposed framework can provide solutions within 5% gap of the global optimum results given less than 1% of the time required for finding the optimal results. Our framework can be utilized within an expert system for infrastructure planning to intelligently determine the best infrastructure management decisions. Given the flexibility of the framework, it can easily be adapted to many other decision problems that can be modeled as bi-level problems on graphs. Moreover, we observe many interesting future direction
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#28151;&#21512;&#31867;&#22411;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#28508;&#21147;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#25968;&#25454;&#36136;&#37327;&#12289;&#23454;&#29992;&#24615;&#21644;&#22686;&#24378;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38544;&#31169;&#26041;&#38754;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.14679</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#28151;&#21512;&#31867;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Mixed-type Electronic Health Records using Diffusion Models. (arXiv:2302.14679v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#28151;&#21512;&#31867;&#22411;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#28508;&#21147;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#25968;&#25454;&#36136;&#37327;&#12289;&#23454;&#29992;&#24615;&#21644;&#22686;&#24378;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38544;&#31169;&#26041;&#38754;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#21253;&#21547;&#25935;&#24863;&#30340;&#24739;&#32773;&#20449;&#24687;&#65292;&#22312;&#20849;&#20139;&#27492;&#31867;&#25968;&#25454;&#26102;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26159;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;GANs&#20043;&#19978;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#65292;&#20363;&#22914;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#31283;&#23450;&#30340;&#35757;&#32451;&#20197;&#29983;&#25104;&#21253;&#25324;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#22768;&#38899;&#22312;&#20869;&#30340;&#25968;&#25454;&#27169;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#30495;&#23454;&#28151;&#21512;&#31867;&#22411;&#34920;&#26684;EHRs&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23558;TabDDPM&#27169;&#22411;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#25968;&#25454;&#36136;&#37327;&#12289;&#23454;&#29992;&#24615;&#12289;&#38544;&#31169;&#21644;&#22686;&#24378;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#38500;&#38544;&#31169;&#26041;&#38754;&#22806;&#65292;TabDDPM&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36825;&#35777;&#23454;&#20102;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHRs) contain sensitive patient information, which presents privacy concerns when sharing such data. Synthetic data generation is a promising solution to mitigate these risks, often relying on deep generative models such as Generative Adversarial Networks (GANs). However, recent studies have shown that diffusion models offer several advantages over GANs, such as generation of more realistic synthetic data and stable training in generating data modalities, including image, text, and sound. In this work, we investigate the potential of diffusion models for generating realistic mixed-type tabular EHRs, comparing TabDDPM model with existing methods on four datasets in terms of data quality, utility, privacy, and augmentation. Our experiments demonstrate that TabDDPM outperforms the state-of-the-art models across all evaluation metrics, except for privacy, which confirms the trade-off between privacy and utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;</title><link>http://arxiv.org/abs/2302.09738</link><description>&lt;p&gt;
&#31616;&#21270;&#22522;&#20110;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#22312;&#35745;&#31639;&#19978;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30830;&#20445;&#36845;&#20195;&#20445;&#25345;&#22312;&#23376;&#27969;&#24418;&#19978;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#22256;&#38590;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20223;&#23556;&#19981;&#21464;&#24230;&#37327;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#21160;&#24577;&#22320;&#31616;&#21270;&#20026;&#27431;&#20960;&#37324;&#24471;&#26080;&#32422;&#26463;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#31616;&#21270;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21327;&#26041;&#24046;&#26041;&#27861;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#20013;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#26497;&#38480;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#26525;&#24178;&#25353;&#27604;&#20363;&#32553;&#25918;&#26102;&#65292;&#24471;&#21040;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#26159;&#30456;&#21516;&#30340;&#12290;&#36825;&#19968;&#21457;&#29616;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#21363;&#20351;&#28145;&#24230;&#21644;&#23485;&#24230;&#22788;&#20110;&#30456;&#21516;&#38454;&#25968;&#30340;&#32593;&#32476;&#65292;&#26631;&#20934;&#30340;&#23485;&#24230;&#26080;&#38480;&#12289;&#28982;&#21518;&#28145;&#24230;&#36235;&#21521;&#26080;&#31351;&#30340;&#26041;&#27861;&#20063;&#33021;&#25552;&#20379;&#23454;&#38469;&#27934;&#35265;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#39044;&#28608;&#27963;&#20855;&#26377;&#39640;&#26031;&#20998;&#24067;&#65292;&#36825;&#23545;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20855;&#26377;&#30452;&#25509;&#24212;&#29992;&#12290;&#36890;&#36807;&#22823;&#37327;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#29702;&#35770;&#21457;&#29616;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00453</link><description>&lt;p&gt;
&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#20013;&#23485;&#24230;&#21644;&#28145;&#24230;&#26497;&#38480;&#30340;&#36890;&#34892;
&lt;/p&gt;
&lt;p&gt;
Width and Depth Limits Commute in Residual Networks. (arXiv:2302.00453v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#20013;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#26497;&#38480;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#26525;&#24178;&#25353;&#27604;&#20363;&#32553;&#25918;&#26102;&#65292;&#24471;&#21040;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#26159;&#30456;&#21516;&#30340;&#12290;&#36825;&#19968;&#21457;&#29616;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#21363;&#20351;&#28145;&#24230;&#21644;&#23485;&#24230;&#22788;&#20110;&#30456;&#21516;&#38454;&#25968;&#30340;&#32593;&#32476;&#65292;&#26631;&#20934;&#30340;&#23485;&#24230;&#26080;&#38480;&#12289;&#28982;&#21518;&#28145;&#24230;&#36235;&#21521;&#26080;&#31351;&#30340;&#26041;&#27861;&#20063;&#33021;&#25552;&#20379;&#23454;&#38469;&#27934;&#35265;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#39044;&#28608;&#27963;&#20855;&#26377;&#39640;&#26031;&#20998;&#24067;&#65292;&#36825;&#23545;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20855;&#26377;&#30452;&#25509;&#24212;&#29992;&#12290;&#36890;&#36807;&#22823;&#37327;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#29702;&#35770;&#21457;&#29616;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24403;&#26525;&#24178;&#25353;&#27604;&#20363;$1/\sqrt{depth}$&#32553;&#25918;&#26102;&#65292;&#23558;&#23485;&#24230;&#21644;&#28145;&#24230;&#36235;&#21521;&#26080;&#31351;&#24471;&#21040;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#26159;&#30456;&#21516;&#30340;&#12290;&#36825;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26631;&#20934;&#30340;&#23485;&#24230;&#26080;&#38480;&#12289;&#28982;&#21518;&#28145;&#24230;&#36235;&#21521;&#26080;&#31351;&#30340;&#26041;&#27861;&#23545;&#20110;&#28145;&#24230;&#21644;&#23485;&#24230;&#22788;&#20110;&#30456;&#21516;&#38454;&#25968;&#30340;&#32593;&#32476;&#20063;&#33021;&#25552;&#20379;&#23454;&#38469;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39044;&#28608;&#27963;&#20855;&#26377;&#39640;&#26031;&#20998;&#24067;&#65292;&#36825;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#20855;&#26377;&#30452;&#25509;&#24212;&#29992;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#27169;&#25311;&#23454;&#39564;&#65292;&#32467;&#26524;&#19982;&#29702;&#35770;&#21457;&#29616;&#38750;&#24120;&#21563;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that taking the width and depth to infinity in a deep neural network with skip connections, when branches are scaled by $1/\sqrt{depth}$ (the only nontrivial scaling), result in the same covariance structure no matter how that limit is taken. This explains why the standard infinite-width-then-depth approach provides practical insights even for networks with depth of the same order as width. We also demonstrate that the pre-activations, in this case, have Gaussian distributions which has direct applications in Bayesian deep learning. We conduct extensive simulations that show an excellent match with our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#39044;&#27979;&#32500;&#25252;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20026;&#39044;&#27979;&#32500;&#25252;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2301.10822</link><description>&lt;p&gt;
RobustPdM&#65306;&#35774;&#35745;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#39044;&#27979;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
RobustPdM: Designing Robust Predictive Maintenance against Adversarial Attacks. (arXiv:2301.10822v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#39044;&#27979;&#32500;&#25252;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20026;&#39044;&#27979;&#32500;&#25252;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#32500;&#25252;&#25216;&#26415;&#22312;&#38477;&#20302;&#22797;&#26434;&#26426;&#22120;&#30340;&#32500;&#25252;&#25104;&#26412;&#21644;&#20572;&#26426;&#26102;&#38388;&#65292;&#25552;&#39640;&#25972;&#20307;&#29983;&#20135;&#29575;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36890;&#36807;&#24191;&#27867;&#21033;&#29992;&#29289;&#32852;&#32593;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#37117;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#25935;&#24863;&#24615;&#24050;&#32463;&#34987;&#20844;&#35748;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#32500;&#25252;&#39046;&#22495;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#20219;&#21153;&#30340;&#39044;&#27979;&#32500;&#25252;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#39044;&#27979;&#32500;&#25252;&#31995;&#32479;&#65292;&#36890;&#36807;&#24191;&#27867;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#32500;&#25252;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state-of-the-art predictive maintenance (PdM) techniques have shown great success in reducing maintenance costs and downtime of complicated machines while increasing overall productivity through extensive utilization of Internet-of-Things (IoT) and Deep Learning (DL). Unfortunately, IoT sensors and DL algorithms are both prone to cyber-attacks. For instance, DL algorithms are known for their susceptibility to adversarial examples. Such adversarial attacks are vastly under-explored in the PdM domain. This is because the adversarial attacks in the computer vision domain for classification tasks cannot be directly applied to the PdM domain for multivariate time series (MTS) regression tasks. In this work, we propose an end-to-end methodology to design adversarially robust PdM systems by extensively analyzing the effect of different types of adversarial attacks and proposing a novel adversarial defense technique for DL-enabled PdM models. First, we propose novel MTS Projected Gradient 
&lt;/p&gt;</description></item><item><title>&#21151;&#33021;&#24615;&#31070;&#32463;&#32593;&#32476;&#65288;FNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#20855;&#26377;&#20301;&#31227;&#19981;&#21464;&#24615;&#21644;&#20445;&#25345;&#25968;&#25454;&#24179;&#28369;&#24615;&#30340;&#29305;&#28857;&#12290;&#22312;&#33041;&#30005;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;FNNs&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#24182;&#33021;&#25104;&#21151;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2301.05869</link><description>&lt;p&gt;
&#21151;&#33021;&#24615;&#31070;&#32463;&#32593;&#32476;&#65306;&#29992;&#20110;&#21151;&#33021;&#25968;&#25454;&#30340;&#20301;&#31227;&#19981;&#21464;&#27169;&#22411;&#21450;&#20854;&#22312;&#33041;&#30005;&#22270;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Functional Neural Networks: Shift invariant models for functional data with applications to EEG classification. (arXiv:2301.05869v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05869
&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#31070;&#32463;&#32593;&#32476;&#65288;FNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#20855;&#26377;&#20301;&#31227;&#19981;&#21464;&#24615;&#21644;&#20445;&#25345;&#25968;&#25454;&#24179;&#28369;&#24615;&#30340;&#29305;&#28857;&#12290;&#22312;&#33041;&#30005;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;FNNs&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#24182;&#33021;&#25104;&#21151;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32479;&#35745;&#27169;&#22411;&#26469;&#35828;&#65292;&#29420;&#31435;&#20110;&#20301;&#32622;&#22320;&#26816;&#27979;&#24863;&#20852;&#36259;&#30340;&#20449;&#21495;&#26159;&#24456;&#29702;&#24819;&#30340;&#12290;&#22914;&#26524;&#25968;&#25454;&#30001;&#26576;&#20010;&#24179;&#28369;&#36807;&#31243;&#29983;&#25104;&#65292;&#37027;&#20040;&#36825;&#31181;&#38468;&#21152;&#32467;&#26500;&#24212;&#35813;&#34987;&#32771;&#34385;&#36827;&#21435;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#23427;&#20204;&#20855;&#26377;&#20301;&#31227;&#19981;&#21464;&#24615;&#24182;&#20445;&#25345;&#25968;&#25454;&#30340;&#24179;&#28369;&#24615;&#65306;&#21151;&#33021;&#24615;&#31070;&#32463;&#32593;&#32476;&#65288;FNNs&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#65288;FDA&#65289;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20197;&#36866;&#24212;&#21151;&#33021;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;FDA&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#22320;&#20351;&#29992;FNNs&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is desirable for statistical models to detect signals of interest independently of their position. If the data is generated by some smooth process, this additional structure should be taken into account. We introduce a new class of neural networks that are shift invariant and preserve smoothness of the data: functional neural networks (FNNs). For this, we use methods from functional data analysis (FDA) to extend multi-layer perceptrons and convolutional neural networks to functional data. We propose different model architectures, show that the models outperform a benchmark model from FDA in terms of accuracy and successfully use FNNs to classify electroencephalography (EEG) data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#21046;&#24230;&#21464;&#21270;&#19979;&#23545;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#36827;&#34892;&#25490;&#21517;&#12290;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#24182;&#32467;&#21512;dropout&#25216;&#26415;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#21160;&#24577;&#29305;&#24449;&#20013;&#21644;&#21017;&#26159;&#19968;&#31181;&#39640;&#25928;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2301.00790</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#24207;&#34920;&#26684;&#25968;&#25454;&#30340;&#21160;&#24577;&#29305;&#24449;&#24037;&#31243;&#21644;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#22312;&#21046;&#24230;&#21464;&#21270;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dynamic Feature Engineering and model selection methods for temporal tabular datasets with regime changes. (arXiv:2301.00790v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#21046;&#24230;&#21464;&#21270;&#19979;&#23545;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#36827;&#34892;&#25490;&#21517;&#12290;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#24182;&#32467;&#21512;dropout&#25216;&#26415;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#21160;&#24577;&#29305;&#24449;&#20013;&#21644;&#21017;&#26159;&#19968;&#31181;&#39640;&#25928;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20005;&#37325;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#30340;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#30340;&#27169;&#22411;&#22312;&#21046;&#24230;&#21464;&#21270;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#21046;&#24230;&#21464;&#21270;&#19979;&#23545;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#36827;&#34892;&#25490;&#21517;&#12290;&#31649;&#36947;&#35780;&#20272;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#21644;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#31616;&#21333;&#29305;&#24449;&#24037;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;dropout&#30340;GBDT&#27169;&#22411;&#20855;&#26377;&#39640;&#24615;&#33021;&#12289;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19988;&#30456;&#23545;&#22797;&#26434;&#24230;&#36739;&#20302;&#12289;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#27979;&#21518;&#22788;&#29702;&#20013;&#29992;&#20110;&#22686;&#24378;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#29305;&#24449;&#20013;&#21644;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#25928;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of deep learning algorithms to temporal panel datasets is difficult due to heavy non-stationarities which can lead to over-fitted models that under-perform under regime changes. In this work we propose a new machine learning pipeline for ranking predictions on temporal panel datasets which is robust under regime changes of data. Different machine-learning models, including Gradient Boosting Decision Trees (GBDTs) and Neural Networks with and without simple feature engineering are evaluated in the pipeline with different settings. We find that GBDT models with dropout display high performance, robustness and generalisability with relatively low complexity and reduced computational cost. We then show that online learning techniques can be used in post-prediction processing to enhance the results. In particular, dynamic feature neutralisation, an efficient procedure that requires no retraining of models and can be applied post-prediction to any machine learning model, impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#19982;&#20316;&#32773;&#24341;&#20837;&#30340;&#19968;&#31181;&#26032;&#30340;&#30456;&#20851;&#31995;&#25968;&#30456;&#20851;&#30340;&#20851;&#32852;&#24230;&#27979;&#37327;&#26041;&#27861;&#30340;&#19968;&#20123;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#26631;&#20934;Borel&#31354;&#38388;&#30340;&#30452;&#25509;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2211.04702</link><description>&lt;p&gt;
&#20851;&#32852;&#24230;&#27979;&#37327;&#26041;&#27861;&#30340;&#19968;&#20123;&#26368;&#26032;&#21457;&#23637;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey of some recent developments in measures of association. (arXiv:2211.04702v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#19982;&#20316;&#32773;&#24341;&#20837;&#30340;&#19968;&#31181;&#26032;&#30340;&#30456;&#20851;&#31995;&#25968;&#30456;&#20851;&#30340;&#20851;&#32852;&#24230;&#27979;&#37327;&#26041;&#27861;&#30340;&#19968;&#20123;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#26631;&#20934;Borel&#31354;&#38388;&#30340;&#30452;&#25509;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#19982;&#20316;&#32773;&#24341;&#20837;&#30340;&#19968;&#31181;&#26032;&#30340;&#30456;&#20851;&#31995;&#25968;&#30456;&#20851;&#30340;&#20851;&#32852;&#24230;&#27979;&#37327;&#26041;&#27861;&#30340;&#19968;&#20123;&#26368;&#26032;&#21457;&#23637;&#12290;&#24182;&#22312;&#32508;&#36848;&#30340;&#26368;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#26631;&#20934;Borel&#31354;&#38388;&#65288;&#21253;&#25324;&#25152;&#26377;&#27874;&#20848;&#31354;&#38388;&#65289;&#30340;&#30452;&#25509;&#25512;&#24191;&#65292;&#36825;&#19968;&#25512;&#24191;&#22312;&#25991;&#29486;&#20013;&#36804;&#20170;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys some recent developments in measures of association related to a new coefficient of correlation introduced by the author. A straightforward extension of this coefficient to standard Borel spaces (which includes all Polish spaces), overlooked in the literature so far, is proposed at the end of the survey.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;MVU&#26426;&#21046;&#65292;&#36890;&#36807;&#25968;&#20540;&#26426;&#21046;&#35774;&#35745;&#23454;&#29616;&#38754;&#21521;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#21387;&#32553;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#21644;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#36890;&#20449;&#39640;&#25928;&#30340;&#31169;&#26377;FL&#30340;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.03942</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#21387;&#32553;&#65306;&#36890;&#36807;&#25968;&#20540;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design. (arXiv:2211.03942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;MVU&#26426;&#21046;&#65292;&#36890;&#36807;&#25968;&#20540;&#26426;&#21046;&#35774;&#35745;&#23454;&#29616;&#38754;&#21521;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#21387;&#32553;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#21644;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#36890;&#20449;&#39640;&#25928;&#30340;&#31169;&#26377;FL&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new Interpolated MVU mechanism for privacy-aware compression in federated learning, which achieves a better privacy-utility trade-off and scalability through numerical mechanism design, and provides SOTA results on communication-efficient private FL on a variety of datasets.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#26381;&#21153;&#22120;&#32858;&#21512;&#26469;&#33258;&#22823;&#37327;&#23458;&#25143;&#31471;&#30340;&#24046;&#20998;&#38544;&#31169;&#26356;&#26032;&#65292;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#38544;&#31169;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#36890;&#20449;&#30340;&#20301;&#25968;&#20043;&#38388;&#24179;&#34913;&#38544;&#31169;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#21387;&#32553;&#26426;&#21046;&#65288;&#31216;&#20026;&#26368;&#23567;&#26041;&#24046;&#26080;&#20559;&#65288;MVU&#65289;&#26426;&#21046;&#65289;&#26469;&#23454;&#29616;&#33391;&#22909;&#30340;&#26435;&#34913;&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#26469;&#30830;&#23450;&#26426;&#21046;&#30340;&#21442;&#25968;&#12290;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;&#36807;&#31243;&#65292;&#29992;&#20110;&#25968;&#20540;&#35774;&#35745;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#38544;&#31169;&#20998;&#26512;&#12290;&#32467;&#26524;&#26159;&#26032;&#30340;&#25554;&#20540;MVU&#26426;&#21046;&#65292;&#23427;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#36890;&#20449;&#39640;&#25928;&#30340;&#31169;&#26377;FL&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In private federated learning (FL), a server aggregates differentially private updates from a large number of clients in order to train a machine learning model. The main challenge in this setting is balancing privacy with both classification accuracy of the learnt model as well as the number of bits communicated between the clients and server. Prior work has achieved a good trade-off by designing a privacy-aware compression mechanism, called the minimum variance unbiased (MVU) mechanism, that numerically solves an optimization problem to determine the parameters of the mechanism. This paper builds upon it by introducing a new interpolation procedure in the numerical design process that allows for a far more efficient privacy analysis. The result is the new Interpolated MVU mechanism that is more scalable, has a better privacy-utility trade-off, and provides SOTA results on communication-efficient private FL on a variety of datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#26041;&#27861;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24046;&#20998;&#38544;&#31169;&#23545;&#25239;&#25968;&#25454;&#37325;&#24314;&#25915;&#20987;&#30340;&#29702;&#35770;&#26377;&#25928;&#24615;&#65292;&#20026;&#30456;&#23545;&#36739;&#22823;&#30340;&#38544;&#31169;&#21442;&#25968;&#949;&#20540;&#19979;&#30340;&#23454;&#36341;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2210.13662</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;: Fano&#30340;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Analyzing Privacy Leakage in Machine Learning via Multiple Hypothesis Testing: A Lesson From Fano. (arXiv:2210.13662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#26041;&#27861;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24046;&#20998;&#38544;&#31169;&#23545;&#25239;&#25968;&#25454;&#37325;&#24314;&#25915;&#20987;&#30340;&#29702;&#35770;&#26377;&#25928;&#24615;&#65292;&#20026;&#30456;&#23545;&#36739;&#22823;&#30340;&#38544;&#31169;&#21442;&#25968;&#949;&#20540;&#19979;&#30340;&#23454;&#36341;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;(DP)&#26159;&#30446;&#21069;&#26368;&#24191;&#27867;&#25509;&#21463;&#30340;&#26426;&#22120;&#23398;&#20064;&#38544;&#31169;&#39118;&#38505;&#32531;&#35299;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;&#20102;&#20445;&#25252;&#29305;&#23450;&#38544;&#31169;&#39118;&#38505;&#65292;&#38544;&#31169;&#21442;&#25968;&#949;&#38656;&#35201;&#22810;&#23567;&#20173;&#19981;&#20026;&#20154;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#25955;&#25968;&#25454;&#30340;&#25968;&#25454;&#37325;&#24314;&#25915;&#20987;&#65292;&#24182;&#22312;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#19979;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#21033;&#29992;&#33879;&#21517;&#30340;Fano&#19981;&#31561;&#24335;&#30340;&#19981;&#21516;&#21464;&#20307;&#65292;&#25512;&#23548;&#20986;&#27169;&#22411;&#22312;&#19981;&#21516;ially&#31169;&#26377;&#35757;&#32451;&#26102;&#25968;&#25454;&#37325;&#24314;&#23545;&#25163;&#25512;&#35770;&#33021;&#21147;&#30340;&#19978;&#38480;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#24213;&#23618;&#31169;&#26377;&#25968;&#25454;&#21462;&#20540;&#26469;&#33258;&#22823;&#23567;&#20026;M&#30340;&#38598;&#21512;&#65292;&#37027;&#20040;&#22312;&#23545;&#25163;&#33719;&#24471;&#26174;&#33879;&#25512;&#35770;&#33021;&#21147;&#20043;&#21069;&#65292;&#30446;&#26631;&#38544;&#31169;&#21442;&#25968;&#949;&#21487;&#20197;&#36798;&#21040;O(log M)&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;DP&#22312;&#30456;&#23545;&#36739;&#22823;&#30340;&#949;&#20540;&#19979;&#23545;&#25239;&#25968;&#25454;&#37325;&#24314;&#25915;&#20987;&#30340;&#32463;&#39564;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy (DP) is by far the most widely accepted framework for mitigating privacy risks in machine learning. However, exactly how small the privacy parameter $\epsilon$ needs to be to protect against certain privacy risks in practice is still not well-understood. In this work, we study data reconstruction attacks for discrete data and analyze it under the framework of multiple hypothesis testing. We utilize different variants of the celebrated Fano's inequality to derive upper bounds on the inferential power of a data reconstruction adversary when the model is trained differentially privately. Importantly, we show that if the underlying private data takes values from a set of size $M$, then the target privacy parameter $\epsilon$ can be $O(\log M)$ before the adversary gains significant inferential power. Our analysis offers theoretical evidence for the empirical effectiveness of DP against data reconstruction attacks even at relatively large values of $\epsilon$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#25110;&#27700;&#24179;&#32763;&#36716;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#23376;&#32676;&#23545;&#31216;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2210.04087</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;CNN&#25200;&#21160;&#25915;&#20987;&#30340;&#23545;&#31216;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Symmetry Defense Against CNN Adversarial Perturbation Attacks. (arXiv:2210.04087v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#25110;&#27700;&#24179;&#32763;&#36716;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#23376;&#32676;&#23545;&#31216;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a symmetry defense method to improve adversarial robustness by flipping or horizontally flipping symmetric adversarial samples, and uses subgroup symmetries for classification.
&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65288;CNN&#65289;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#20250;&#25200;&#21160;&#21407;&#22987;&#26679;&#26412;&#20197;&#27450;&#39575;&#20998;&#31867;&#22120;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#36947;&#36335;&#26631;&#24535;&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;CNN&#22312;&#23545;&#31216;&#26679;&#26412;&#30340;&#20998;&#31867;&#20013;&#20063;&#32570;&#20047;&#19981;&#21464;&#24615;&#65292;&#22240;&#20026;CNN&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#23545;&#31216;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#32771;&#34385;&#21040;CNN&#32570;&#20047;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;CNN&#32570;&#20047;&#19981;&#21464;&#24615;&#65292;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#30340;&#20998;&#31867;&#21487;&#33021;&#19982;&#20854;&#38169;&#35823;&#20998;&#31867;&#19981;&#21516;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#23545;&#31216;&#38450;&#24481;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#23545;&#25239;&#32773;&#19981;&#30693;&#36947;&#38450;&#24481;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#32763;&#36716;&#25110;&#27700;&#24179;&#32763;&#36716;&#21518;&#20877;&#36827;&#34892;&#20998;&#31867;&#12290;&#23545;&#20110;&#30693;&#36947;&#38450;&#24481;&#30340;&#23545;&#25163;&#65292;&#38450;&#24481;&#35774;&#35745;&#20102;&#19968;&#20010;Klein&#22235;&#20010;&#23545;&#31216;&#23376;&#32676;&#65292;&#20854;&#20013;&#21253;&#25324;&#27700;&#24179;&#32763;&#36716;&#21644;&#20687;&#32032;&#21453;&#36716;&#23545;&#31216;&#24615;&#12290;&#23545;&#31216;&#38450;&#24481;&#20351;&#29992;&#23376;&#32676;&#23545;&#31216;&#24615;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural network classifiers (CNNs) are susceptible to adversarial attacks that perturb original samples to fool classifiers such as an autonomous vehicle's road sign image classifier. CNNs also lack invariance in the classification of symmetric samples because CNNs can classify symmetric samples differently. Considered together, the CNN lack of adversarial robustness and the CNN lack of invariance mean that the classification of symmetric adversarial samples can differ from their incorrect classification. Could symmetric adversarial samples revert to their correct classification? This paper answers this question by designing a symmetry defense that inverts or horizontally flips adversarial samples before classification against adversaries unaware of the defense. Against adversaries aware of the defense, the defense devises a Klein four symmetry subgroup that includes the horizontal flip and pixel inversion symmetries. The symmetry defense uses the subgroup symmetries in ac
&lt;/p&gt;</description></item><item><title>RALACs&#26159;&#19968;&#31181;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#39062;&#31995;&#32479;&#65292;&#36890;&#36807;&#20132;&#20114;&#32534;&#30721;&#21644;&#20809;&#27969;&#25216;&#26415;&#23558;&#21160;&#20316;&#35782;&#21035;&#24212;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#65292;&#24182;&#24357;&#21512;&#20102;&#20854;&#19982;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2209.14408</link><description>&lt;p&gt;
RALACs: &#20351;&#29992;&#20132;&#20114;&#32534;&#30721;&#21644;&#20809;&#27969;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow. (arXiv:2209.14408v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14408
&lt;/p&gt;
&lt;p&gt;
RALACs&#26159;&#19968;&#31181;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#39062;&#31995;&#32479;&#65292;&#36890;&#36807;&#20132;&#20114;&#32534;&#30721;&#21644;&#20809;&#27969;&#25216;&#26415;&#23558;&#21160;&#20316;&#35782;&#21035;&#24212;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#65292;&#24182;&#24357;&#21512;&#20102;&#20854;&#19982;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21160;&#20316;&#35782;&#21035;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#29615;&#22659;&#21487;&#20197;&#22686;&#24378;&#29615;&#22659;&#27169;&#22411;&#30340;&#24773;&#22659;&#24863;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21160;&#20316;&#35782;&#21035;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#20154;&#31867;&#65292;&#20854;&#23545;&#20110;&#22024;&#26434;&#12289;&#26410;&#21098;&#36753;&#12289;&#21407;&#22987;&#30340;RGB&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#26377;&#38480;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20026;&#25512;&#21160;&#21160;&#20316;&#35782;&#21035;&#22312;AVs&#20013;&#30340;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#38454;&#27573;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;&#65292;&#21629;&#21517;&#20026;RALACs&#12290;RALACs&#23558;&#21160;&#20316;&#35782;&#21035;&#38382;&#39064;&#24212;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#65292;&#24182;&#24357;&#21512;&#20102;&#20854;&#19982;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#26377;&#21161;&#20110;&#32534;&#30721;agent&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#24378;&#35843;&#36825;&#31181;&#26041;&#26696;&#22914;&#20309;&#19982;&#31867;&#21035;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#20026;&#35299;&#20915;&#36947;&#36335;&#19978;agent&#30340;&#21160;&#24577;&#24615;&#65292;RALACs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
When applied to autonomous vehicle (AV) settings, action recognition can enhance an environment model's situational awareness. This is especially prevalent in scenarios where traditional geometric descriptions and heuristics in AVs are insufficient. However, action recognition has traditionally been studied for humans, and its limited adaptability to noisy, un-clipped, un-pampered, raw RGB data has limited its application in other fields. To push for the advancement and adoption of action recognition into AVs, this work proposes a novel two-stage action recognition system, termed RALACs. RALACs formulates the problem of action recognition for road scenes, and bridges the gap between it and the established field of human action recognition. This work shows how attention layers can be useful for encoding the relations across agents, and stresses how such a scheme can be class-agnostic. Furthermore, to address the dynamic nature of agents on the road, RALACs constructs a novel approach to
&lt;/p&gt;</description></item><item><title>MetaMask&#26159;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23398;&#20064;&#30340;&#32500;&#24230;&#36974;&#32617;&#65292;&#29992;&#20110;&#23545;&#25239;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#20887;&#20313;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.07902</link><description>&lt;p&gt;
MetaMask&#65306;&#37325;&#26032;&#24605;&#32771;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#24178;&#25200;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning. (arXiv:2209.07902v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07902
&lt;/p&gt;
&lt;p&gt;
MetaMask&#26159;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23398;&#20064;&#30340;&#32500;&#24230;&#36974;&#32617;&#65292;&#29992;&#20110;&#23545;&#25239;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#20887;&#20313;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25104;&#21151;&#26041;&#27861;&#65292;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#22312;&#36755;&#20837;&#26679;&#26412;&#30340;&#25197;&#26354;&#20043;&#38388;&#20849;&#20139;&#30340;&#19981;&#21464;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#23398;&#20064;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#25345;&#20037;&#30340;&#32570;&#38519;&#65306;&#20219;&#21153;&#26080;&#20851;&#20449;&#24687;&#30340;&#24178;&#25200;&#21644;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#36825;&#19982;&#24179;&#20961;&#24120;&#25968;&#35299;&#30340;&#21453;&#22797;&#23384;&#22312;&#30456;&#20851;&#12290;&#20174;&#32500;&#24230;&#20998;&#26512;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#21457;&#29616;&#32500;&#24230;&#20887;&#20313;&#21644;&#32500;&#24230;&#24178;&#25200;&#26159;&#36825;&#20123;&#29616;&#35937;&#32972;&#21518;&#30340;&#22266;&#26377;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;MetaMask&#65292;&#21363;&#36890;&#36807;&#20803;&#23398;&#20064;&#23398;&#20064;&#30340;&#32500;&#24230;&#36974;&#32617;&#65292;&#20197;&#23545;&#25239;&#32500;&#24230;&#20887;&#20313;&#21644;&#24178;&#25200;&#12290;MetaMask&#37319;&#29992;&#20887;&#20313;&#20943;&#23569;&#25216;&#26415;&#26469;&#35299;&#20915;&#32500;&#24230;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#21019;&#26032;&#22320;&#24341;&#20837;&#20102;&#19968;&#31181;&#32500;&#24230;&#24178;&#25200;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a successful approach to self-supervised learning, contrastive learning aims to learn invariant information shared among distortions of the input sample. While contrastive learning has yielded continuous advancements in sampling strategy and architecture design, it still remains two persistent defects: the interference of task-irrelevant information and sample inefficiency, which are related to the recurring existence of trivial constant solutions. From the perspective of dimensional analysis, we find out that the dimensional redundancy and dimensional confounder are the intrinsic issues behind the phenomena, and provide experimental evidence to support our viewpoint. We further propose a simple yet effective approach MetaMask, short for the dimensional Mask learned by Meta-learning, to learn representations against dimensional redundancy and confounder. MetaMask adopts the redundancy-reduction technique to tackle the dimensional redundancy issue and innovatively introduces a dimens
&lt;/p&gt;</description></item><item><title>VAE&#35299;&#32439;&#32467;&#26524;&#30340;&#25104;&#22240;&#26377;&#24453;&#37325;&#26032;&#23457;&#35270;&#65292;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#19982;&#37325;&#24314;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#35299;&#32806;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#65292;&#26631;&#20934;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26681;&#25454;&#20856;&#22411;&#30340;VAE&#37325;&#24314;&#25439;&#22833;&#19982;&#24863;&#30693;&#36724;&#20043;&#38388;&#23384;&#22312;&#24847;&#22806;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.13341</link><description>&lt;p&gt;
VAE&#35299;&#32439;&#25928;&#26524;&#30340;&#37325;&#24314;&#25439;&#22833;&#34987;&#24573;&#35270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Overlooked Implications of the Reconstruction Loss for VAE Disentanglement. (arXiv:2202.13341v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13341
&lt;/p&gt;
&lt;p&gt;
VAE&#35299;&#32439;&#32467;&#26524;&#30340;&#25104;&#22240;&#26377;&#24453;&#37325;&#26032;&#23457;&#35270;&#65292;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#19982;&#37325;&#24314;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#35299;&#32806;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#65292;&#26631;&#20934;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26681;&#25454;&#20856;&#22411;&#30340;VAE&#37325;&#24314;&#25439;&#22833;&#19982;&#24863;&#30693;&#36724;&#20043;&#38388;&#23384;&#22312;&#24847;&#22806;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#35299;&#32806;&#34920;&#31034;&#36890;&#24120;&#24402;&#22240;&#20110;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#32452;&#20214;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#25968;&#25454;&#19982;&#25439;&#22833;&#30340;&#37325;&#24314;&#39033;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;VAEs&#20013;&#35299;&#32806;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26631;&#20934;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26681;&#25454;&#20856;&#22411;&#30340;VAE&#37325;&#24314;&#25439;&#22833;&#20250;&#23548;&#33268;&#23427;&#20204;&#30340;&#20027;&#35266;&#30495;&#23454;&#22240;&#32032;&#19982;&#25968;&#25454;&#20013;&#30340;&#24863;&#30693;&#36724;&#20043;&#38388;&#23384;&#22312;&#24847;&#22806;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#20026;&#22312;&#32473;&#23450;&#37325;&#24314;&#25439;&#22833;&#19979;&#26500;&#24314;&#23545;&#25239;&#25968;&#25454;&#38598;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#31034;&#20363;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#26368;&#20808;&#36827;&#26694;&#26550;&#20013;&#30340;&#35299;&#32806;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20154;&#31867;&#30452;&#35266;&#30340;&#30495;&#23454;&#22240;&#32032;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#31034;&#20363;&#37325;&#24314;&#25439;&#22833;&#65292;&#20877;&#27425;&#33021;&#22815;&#24863;&#30693;&#21040;&#30495;&#23454;&#22240;&#32032;&#26469;&#37325;&#26032;&#23454;&#29616;&#35299;&#32806;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#35299;&#32806;&#30340;&#20027;&#35266;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning disentangled representations with variational autoencoders (VAEs) is often attributed to the regularisation component of the loss. In this work, we highlight the interaction between data and the reconstruction term of the loss as the main contributor to disentanglement in VAEs. We show that standard benchmark datasets have unintended correlations between their subjective ground-truth factors and perceived axes in the data according to typical VAE reconstruction losses. Our work exploits this relationship to provide a theory for what constitutes an adversarial dataset under a given reconstruction loss. We verify this by constructing an example dataset that prevents disentanglement in state-of-the-art frameworks while maintaining human-intuitive ground-truth factors. Finally, we re-enable disentanglement by designing an example reconstruction loss that is once again able to perceive the ground-truth factors. Our findings demonstrate the subjective nature of disentanglement and t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#36229;&#20869;&#23384;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;CPU/GPU&#26550;&#26500;&#19978;&#23454;&#29616;&#39640;&#25928;&#35745;&#31639;&#12290;&#31639;&#27861;&#36890;&#36807;&#31232;&#30095;&#21644;&#31264;&#23494;&#30697;&#38453;&#25805;&#20316;&#20197;&#21450;&#25209;&#22788;&#29702;/&#24179;&#38138;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;CUDA&#27969;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#21644;&#24322;&#27493;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2202.09518</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;CPU/GPU&#26550;&#26500;&#19978;&#30340;&#36229;&#20869;&#23384;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)
&lt;/p&gt;
&lt;p&gt;
Distributed Out-of-Memory NMF on CPU/GPU Architectures. (arXiv:2202.09518v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09518
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#36229;&#20869;&#23384;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;CPU/GPU&#26550;&#26500;&#19978;&#23454;&#29616;&#39640;&#25928;&#35745;&#31639;&#12290;&#31639;&#27861;&#36890;&#36807;&#31232;&#30095;&#21644;&#31264;&#23494;&#30697;&#38453;&#25805;&#20316;&#20197;&#21450;&#25209;&#22788;&#29702;/&#24179;&#38138;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;CUDA&#27969;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#21644;&#24322;&#27493;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#36229;&#20869;&#23384;&#23454;&#29616;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#31639;&#27861;&#65292;&#29992;&#20110;&#24322;&#26500;&#39640;&#24615;&#33021;&#35745;&#31639;(HPC)&#31995;&#32479;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;NMFk&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#24182;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#21464;&#37327;&#21644;&#27169;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#23545;&#22810;&#33410;&#28857;&#12289;&#22810;GPU&#31995;&#32479;&#30340;&#31264;&#23494;&#21644;&#31232;&#30095;&#30697;&#38453;&#25805;&#20316;&#25903;&#25345;&#65292;&#25193;&#23637;&#20102;NMFk&#12290;&#24471;&#21040;&#30340;&#31639;&#27861;&#38024;&#23545;&#36229;&#20869;&#23384;&#38382;&#39064;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20854;&#20013;&#25152;&#38656;&#20869;&#23384;&#22823;&#20110;&#21487;&#29992;&#30340;GPU&#20869;&#23384;&#26469;&#36827;&#34892;&#30697;&#38453;&#20998;&#35299;&#12290;&#36890;&#36807;&#25209;&#22788;&#29702;/&#24179;&#38138;&#31574;&#30053;&#38477;&#20302;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#24182;&#20351;&#29992;GPU&#26680;&#24515;(&#25110;&#32773;&#21487;&#29992;&#30340;&#24352;&#37327;&#26680;&#24515;)&#26174;&#33879;&#21152;&#36895;&#31232;&#30095;&#21644;&#31264;&#23494;&#30697;&#38453;&#25805;&#20316;&#12290;&#20351;&#29992;CUDA&#27969;&#38544;&#34255;&#20102;&#20027;&#26426;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#25209;&#22788;&#29702;&#22797;&#21046;&#30340;&#36755;&#20837;/&#36755;&#20986;(I/O)&#24310;&#36831;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#20256;&#36755;&#21644;&#24322;&#27493;&#35745;&#31639;&#30340;&#37325;&#21472;&#65292;&#20197;&#21450;&#19982;&#25910;&#38598;&#30456;&#20851;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient distributed out-of-memory implementation of the Non-negative Matrix Factorization (NMF) algorithm for heterogeneous high-performance-computing (HPC) systems. The proposed implementation is based on prior work on NMFk, which can perform automatic model selection and extract latent variables and patterns from data. In this work, we extend NMFk by adding support for dense and sparse matrix operation on multi-node, multi-GPU systems. The resulting algorithm is optimized for out-of-memory (OOM) problems where the memory required to factorize a given matrix is greater than the available GPU memory. Memory complexity is reduced by batching/tiling strategies, and sparse and dense matrix operations are significantly accelerated with GPU cores (or tensor cores when available). Input/Output (I/O) latency associated with batch copies between host and device is hidden using CUDA streams to overlap data transfers and compute asynchronously, and latency associated with collect
&lt;/p&gt;</description></item><item><title>InfoNCE&#30446;&#26631;&#22312;&#35782;&#21035;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#31561;&#21516;&#20110;ELBO&#65292;&#22312;&#23398;&#20064;&#26368;&#20248;&#20808;&#39564;&#26102;&#21464;&#20026;&#20114;&#20449;&#24687;&#65292;&#24182;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;InfoNCE&#30446;&#26631;&#26159;&#23545;&#20114;&#20449;&#24687;&#30340;&#26494;&#25955;&#19979;&#30028;&#65292;&#20197;&#36991;&#20813;&#39640;&#24230;&#32416;&#32544;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2107.02495</link><description>&lt;p&gt;
InfoNCE&#26159;&#35782;&#21035;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
InfoNCE is variational inference in a recognition parameterised model. (arXiv:2107.02495v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.02495
&lt;/p&gt;
&lt;p&gt;
InfoNCE&#30446;&#26631;&#22312;&#35782;&#21035;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#31561;&#21516;&#20110;ELBO&#65292;&#22312;&#23398;&#20064;&#26368;&#20248;&#20808;&#39564;&#26102;&#21464;&#20026;&#20114;&#20449;&#24687;&#65292;&#24182;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;InfoNCE&#30446;&#26631;&#26159;&#23545;&#20114;&#20449;&#24687;&#30340;&#26494;&#25955;&#19979;&#30028;&#65292;&#20197;&#36991;&#20813;&#39640;&#24230;&#32416;&#32544;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;InfoNCE&#30446;&#26631;&#31561;&#21516;&#20110;&#19968;&#31181;&#26032;&#22411;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#35782;&#21035;&#21442;&#25968;&#21270;&#27169;&#22411;&#65288;RPM&#65289;&#20013;&#30340;ELBO&#12290;&#24403;&#25105;&#20204;&#23398;&#20064;&#26368;&#20248;&#20808;&#39564;&#26102;&#65292;RPM ELBO&#21464;&#25104;&#20102;&#20114;&#20449;&#24687;&#65288;MI&#65307;&#38500;&#20102;&#19968;&#20010;&#24120;&#25968;&#65289;&#65292;&#20174;&#32780;&#19982;&#20043;&#21069;&#23384;&#22312;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;InfoNCE&#65289;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;InfoNCE&#26041;&#27861;&#24182;&#19981;&#20351;&#29992;MI&#20316;&#20026;&#30446;&#26631;&#65307;MI&#23545;&#20110;&#20219;&#24847;&#21487;&#36870;&#21464;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;MI&#30446;&#26631;&#21487;&#33021;&#23548;&#33268;&#39640;&#24230;&#32416;&#32544;&#30340;&#34920;&#31034;&#65288;Tschannen et al.&#65292;2019&#65289;&#12290;&#30456;&#21453;&#65292;&#23454;&#38469;&#30340;InfoNCE&#30446;&#26631;&#26159;&#23545;MI&#30340;&#19968;&#20010;&#31616;&#21270;&#19979;&#30028;&#65292;&#21363;&#20351;&#22312;&#26080;&#38480;&#26679;&#26412;&#26497;&#38480;&#19979;&#20063;&#19981;&#32039;&#23494;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#30446;&#26631;&#65288;&#21363;&#23454;&#38469;&#30340;InfoNCE&#30446;&#26631;&#65289;&#20284;&#20046;&#26159;&#23545;&#19968;&#20010;&#26080;&#25928;&#30340;&#30446;&#26631;&#65288;&#21363;&#32473;&#20986;&#20219;&#24847;&#32416;&#32544;&#34920;&#31034;&#30340;&#30495;&#23454;MI&#65289;&#30340;&#26494;&#25955;&#19979;&#30028;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#23454;&#38469;&#30340;InfoNCE&#30446;&#26631;&#30340;&#21478;&#19968;&#31181;&#21160;&#26426;&#12290;&#22312;&#30446;&#24405;&#20013;
&lt;/p&gt;
&lt;p&gt;
Here, we show that the InfoNCE objective is equivalent to the ELBO in a new class of probabilistic generative model, the recognition parameterised model (RPM). When we learn the optimal prior, the RPM ELBO becomes equal to the mutual information (MI; up to a constant), establishing a connection to pre-existing self-supervised learning methods such as InfoNCE. However, practical InfoNCE methods do not use the MI as an objective; the MI is invariant to arbitrary invertible transformations, so using an MI objective can lead to highly entangled representations (Tschannen et al., 2019). Instead, the actual InfoNCE objective is a simplified lower bound on the MI which is loose even in the infinite sample limit. Thus, an objective that works (i.e. the actual InfoNCE objective) appears to be motivated as a loose bound on an objective that does not work (i.e. the true MI which gives arbitrarily entangled representations). We give an alternative motivation for the actual InfoNCE objective. In pa
&lt;/p&gt;</description></item></channel></rss>