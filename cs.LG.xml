<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02476</link><description>&lt;p&gt;
&#29992;&#20110;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Traveling Purchaser Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#65288;TPP&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#29992;&#20110;&#25429;&#25417;&#24066;&#22330;-&#20135;&#21697;&#20851;&#31995;&#30340;TPP&#30340;&#20108;&#37096;&#22270;&#34920;&#31034;&#65292;&#20197;&#21450;&#20174;&#20108;&#37096;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#39034;&#24207;&#26500;&#24314;&#36335;&#30001;&#30340;&#31574;&#30053;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01712</link><description>&lt;p&gt;
&#36890;&#36807;&#20813;Hessian&#37325;&#26032;&#25972;&#21512;&#20010;&#20307;&#25968;&#25454;&#32479;&#35745;&#23454;&#29616;&#39640;&#25928;&#22312;&#32447;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26088;&#22312;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#24536;&#35760;&#29305;&#23450;&#25968;&#25454;&#26469;&#32500;&#25252;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#34987;&#36951;&#24536;&#26435;&#21033;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#19968;&#31181;&#25968;&#25454;&#36951;&#24536;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;&#25658;&#24102;&#20108;&#38454;&#20449;&#24687;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#33499;&#21051;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#35745;&#31639;/&#23384;&#20648;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#24212;&#29992;&#21040;&#22823;&#22810;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;Hessian&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#32500;&#25252;&#19968;&#20010;&#32479;&#35745;&#21521;&#37327;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#20223;&#23556;&#38543;&#26426;&#36882;&#24402;&#36924;&#36817;&#26469;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;&#22522;&#20110;&#37325;&#26032;&#25910;&#38598;&#36951;&#24536;&#25968;&#25454;&#32479;&#35745;&#30340;&#31574;&#30053;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01712v1 Announce Type: cross  Abstract: Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32858;&#28966;&#20110;&#21033;&#29992;&#22810;&#31890;&#24230;&#25200;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36890;&#36807;&#36716;&#21270;&#20026;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#26469;&#35299;&#20915;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01574</link><description>&lt;p&gt;
&#30446;&#26631;&#40657;&#30418;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Multi-granular Adversarial Attacks against Black-box Neural Ranking Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32858;&#28966;&#20110;&#21033;&#29992;&#22810;&#31890;&#24230;&#25200;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36890;&#36807;&#36716;&#21270;&#20026;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#26469;&#35299;&#20915;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25490;&#24207;&#25915;&#20987;&#30001;&#20110;&#22312;&#21457;&#29616;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#24182;&#22686;&#24378;&#20854;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#25915;&#20987;&#26041;&#27861;&#20165;&#22312;&#21333;&#19968;&#31890;&#24230;&#19978;&#36827;&#34892;&#25200;&#21160;&#65292;&#20363;&#22914;&#21333;&#35789;&#32423;&#25110;&#21477;&#23376;&#32423;&#65292;&#23545;&#30446;&#26631;&#25991;&#26723;&#36827;&#34892;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#23558;&#25200;&#21160;&#38480;&#21046;&#22312;&#21333;&#19968;&#31890;&#24230;&#19978;&#21487;&#33021;&#20250;&#20943;&#23569;&#21019;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#28789;&#27963;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#25915;&#20987;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#31890;&#24230;&#30340;&#25200;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#28041;&#21450;&#35299;&#20915;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#65292;&#38656;&#35201;&#35782;&#21035;&#20986;&#36328;&#25152;&#26377;&#21487;&#33021;&#30340;&#31890;&#24230;&#12289;&#20301;&#32622;&#21644;&#25991;&#26412;&#29255;&#27573;&#30340;&#26368;&#20339;&#32452;&#21512;&#25200;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#22810;&#31890;&#24230;&#23545;&#25239;&#25915;&#20987;&#36716;&#21270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01574v1 Announce Type: cross  Abstract: Adversarial ranking attacks have gained increasing attention due to their success in probing vulnerabilities, and, hence, enhancing the robustness, of neural ranking models. Conventional attack methods employ perturbations at a single granularity, e.g., word-level or sentence-level, to a target document. However, limiting perturbations to a single level of granularity may reduce the flexibility of creating adversarial examples, thereby diminishing the potential threat of the attack. Therefore, we focus on generating high-quality adversarial examples by incorporating multi-granular perturbations. Achieving this objective involves tackling a combinatorial explosion problem, which requires identifying an optimal combination of perturbations across all possible levels of granularity, positions, and textual pieces. To address this challenge, we transform the multi-granular adversarial attack into a sequential decision-making process, where 
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#24037;&#20855;&#22791;&#21463;&#20851;&#27880;&#65292;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;GNN&#22312;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.19852</link><description>&lt;p&gt;
&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Graph Neural Networks in Epidemic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19852
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#24037;&#20855;&#22791;&#21463;&#20851;&#27880;&#65292;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;GNN&#22312;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26032;&#20896;&#30123;&#24773;&#29190;&#21457;&#20197;&#26469;&#65292;&#20154;&#20204;&#23545;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20256;&#32479;&#30340;&#26426;&#26800;&#27169;&#22411;&#25968;&#23398;&#25551;&#36848;&#20102;&#20256;&#26579;&#30149;&#30340;&#20256;&#25773;&#26426;&#21046;&#65292;&#20294;&#22312;&#38754;&#23545;&#24403;&#20170;&#19981;&#26029;&#22686;&#38271;&#30340;&#25361;&#25112;&#26102;&#24448;&#24448;&#21147;&#19981;&#20174;&#24515;&#12290;&#22240;&#27492;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#35797;&#22270;&#20840;&#38754;&#22238;&#39038;GNN&#22312;&#27969;&#34892;&#30149;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#28508;&#22312;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20026;&#27969;&#34892;&#30149;&#20219;&#21153;&#21644;&#26041;&#27861;&#35770;&#21508;&#24341;&#20837;&#20102;&#20998;&#23618;&#20998;&#31867;&#27861;&#65292;&#20026;&#35813;&#39046;&#22495;&#20869;&#30340;&#21457;&#23637;&#36712;&#36857;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#23545;&#20110;&#27969;&#34892;&#30149;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#27969;&#34892;&#30149;&#39046;&#22495;&#36890;&#24120;&#24212;&#29992;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;&#23545;&#20110;&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#20998;&#20026;&#8220;&#31070;&#32463;&#27169;&#22411;&#8221;&#21644;&#8220;&#28151;&#21512;&#27169;&#22411;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19852v1 Announce Type: new  Abstract: Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into \textit{Neural Models} and \textit{Hybrid Models}. Following this, we perform an exha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19837</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Concept-based Analysis of Neural Networks via Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#38750;&#24120;&#21487;&#21462;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#34920;&#36798;&#35270;&#35273;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#20197;&#21450;&#32570;&#20047;&#39640;&#25928;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#36825;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#12289;&#35270;&#35273;&#35821;&#35328;&#12289;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20854;&#21487;&#20197;&#25512;&#29702;&#35270;&#35273;&#27169;&#22411;&#30340;&#36879;&#38236;&#12290;VLMs&#24050;&#32463;&#22312;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#27492;&#38544;&#24335;&#22320;&#20102;&#35299;&#25551;&#36848;&#36825;&#20123;&#22270;&#20687;&#30340;&#39640;&#23618;&#27425;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Con}_{\texttt{spec}}$&#30340;&#36923;&#36753;&#35268;&#33539;&#35821;&#35328;&#65292;&#26088;&#22312;&#20415;&#20110;&#25353;&#29031;&#36825;&#20123;&#27010;&#24565;&#32534;&#20889;&#35268;&#33539;&#12290;&#20026;&#20102;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#26816;&#26597;$\texttt{Con}_{\texttt{spec}}$&#35268;&#33539;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;VLM&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#21644;&#39640;&#25928;&#26816;&#26597;&#35270;&#35273;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;&#19968;&#20221;&#21253;&#25324; 100 &#31687;&#25991;&#26723;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598; OBSINFOX&#65292;&#30740;&#31350;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#22312;&#35748;&#23450;&#20551;&#26032;&#38395;&#29305;&#24449;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#26009;&#24211;&#20013;&#35773;&#21050;&#25991;&#26412;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2403.16099</link><description>&lt;p&gt;
&#19968;&#20221;&#27861;&#22269;&#20551;&#26032;&#38395;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65306;&#20154;&#31867;&#19982;&#26426;&#22120;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Multi-Label Dataset of French Fake News: Human and Machine Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;&#19968;&#20221;&#21253;&#25324; 100 &#31687;&#25991;&#26723;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598; OBSINFOX&#65292;&#30740;&#31350;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#22312;&#35748;&#23450;&#20551;&#26032;&#38395;&#29305;&#24449;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#26009;&#24211;&#20013;&#35773;&#21050;&#25991;&#26412;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001; 8 &#20301;&#27880;&#37322;&#32773;&#20351;&#29992; 11 &#20010;&#26631;&#31614;&#27880;&#37322;&#30340;&#26469;&#33258; 17 &#20010;&#27861;&#22269;&#34987;&#19987;&#23478;&#26426;&#26500;&#35748;&#20026;&#19981;&#21487;&#38752;&#30340;&#26032;&#38395;&#26469;&#28304;&#36873;&#21462;&#30340; 100 &#31687;&#25991;&#26723;&#30340;&#35821;&#26009;&#24211; OBSINFOX&#12290;&#36890;&#36807;&#25910;&#38598;&#27604;&#36890;&#24120;&#26356;&#22810;&#30340;&#26631;&#31614;&#21644;&#27880;&#37322;&#32773;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20154;&#31867;&#35748;&#20026;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#20551;&#26032;&#38395;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#21160;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992; Gate Cloud &#36827;&#34892;&#20027;&#39064;&#21644;&#20307;&#35009;&#20998;&#26512;&#65292;&#36825;&#34920;&#26126;&#35821;&#26009;&#24211;&#20013;&#31867;&#20284;&#35773;&#21050;&#30340;&#25991;&#26412;&#26222;&#36941;&#23384;&#22312;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992; VAGO &#20027;&#35266;&#24615;&#20998;&#26512;&#22120;&#21450;&#20854;&#31070;&#32463;&#29256;&#26412;&#65292;&#20197;&#28548;&#28165;&#26631;&#31614;&#8220;&#20027;&#35266;&#8221;&#19982;&#26631;&#31614;&#8220;&#20551;&#26032;&#38395;&#8221;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#24102;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21487;&#36890;&#36807;&#20197;&#19979;&#32593;&#22336;&#22312;&#32447;&#33719;&#21462;&#65306;https://github.com/obs-info/obsinfox
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16099v1 Announce Type: new  Abstract: We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press
&lt;/p&gt;</description></item><item><title>KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15304</link><description>&lt;p&gt;
KTbench&#65306;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15304
&lt;/p&gt;
&lt;p&gt;
KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#28041;&#21450;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#29983;&#23545;&#23398;&#20064;&#39033;&#30446;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23398;&#20064;&#39033;&#30446;&#34987;&#26631;&#35760;&#20026;&#31216;&#20026;&#30693;&#35782;&#27010;&#24565;&#65288;KCs&#65289;&#30340;&#25216;&#33021;&#26631;&#31614;&#12290;&#35768;&#22810;KT&#27169;&#22411;&#36890;&#36807;&#29992;&#26500;&#25104;KC&#30340;&#23398;&#20064;&#39033;&#30446;&#21462;&#20195;&#23398;&#20064;&#39033;&#30446;&#26469;&#23558;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#25193;&#23637;&#20026;KC-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31232;&#30095;&#30340;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#38382;&#39064;&#24182;&#26368;&#23567;&#21270;&#20102;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#27169;&#22411;&#23398;&#20064;&#21516;&#19968;&#39033;&#30446;&#20869;&#30340;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22522;&#26412;&#20107;&#23454;&#26631;&#31614;&#30340;&#27844;&#28431;&#24182;&#38459;&#30861;&#27169;&#22411;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#29616;&#26377;&#30340;&#22522;&#20934;&#23454;&#29616;&#24573;&#30053;&#20102;&#35745;&#25968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15304v1 Announce Type: cross  Abstract: Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.   The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;</title><link>https://arxiv.org/abs/2403.13041</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#31169;&#23494;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Provable Privacy with Non-Private Pre-Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#24046;&#20998;&#31169;&#23494;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#25968;&#25454;&#30456;&#20851;&#30340;&#39044;&#22788;&#29702;&#30340;&#28508;&#22312;&#38544;&#31169;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#25216;&#26415;&#27010;&#24565;&#24314;&#31435;&#20102;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;&#65306;&#19968;&#31181;&#31216;&#20026;&#24179;&#28369;DP&#30340;DP&#21464;&#20307;&#20197;&#21450;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
&lt;/p&gt;</description></item><item><title>S^2MVTC&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#22810;&#35270;&#22270;&#24352;&#37327;&#32858;&#31867;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#23398;&#20064;&#36328;&#35270;&#22270;&#20869;&#37096;&#21644;&#35270;&#22270;&#38388;&#30340;&#23884;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09107</link><description>&lt;p&gt;
S^2MVTC&#65306;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#22810;&#35270;&#22270;&#24352;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09107
&lt;/p&gt;
&lt;p&gt;
S^2MVTC&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#22810;&#35270;&#22270;&#24352;&#37327;&#32858;&#31867;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#23398;&#20064;&#36328;&#35270;&#22270;&#20869;&#37096;&#21644;&#35270;&#22270;&#38388;&#30340;&#23884;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09107v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#35268;&#27169;&#22810;&#35270;&#22270;&#32858;&#31867;&#22240;&#20854;&#22312;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#38598;&#26102;&#30340;&#26377;&#25928;&#24615;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#25506;&#32034;&#38170;&#22270;&#25110;&#25237;&#24433;&#30697;&#38453;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#26469;&#23547;&#27714;&#29992;&#20110;&#32858;&#31867;&#30340;&#20849;&#35782;&#23884;&#20837;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#22810;&#35270;&#22270;&#24352;&#37327;&#32858;&#31867;&#65288;S^2MVTC&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#23398;&#20064;&#36328;&#35270;&#22270;&#20869;&#37096;&#21644;&#35270;&#22270;&#38388;&#30340;&#23884;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#19981;&#21516;&#35270;&#22270;&#30340;&#23884;&#20837;&#29305;&#24449;&#21472;&#21152;&#25104;&#24352;&#37327;&#24182;&#23545;&#20854;&#36827;&#34892;&#26059;&#36716;&#26469;&#26500;&#24314;&#23884;&#20837;&#29305;&#24449;&#24352;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#20302;&#39057;&#36817;&#20284;&#65288;TLFA&#65289;&#31639;&#23376;&#65292;&#23558;&#22270;&#30456;&#20284;&#24615;&#34701;&#20837;&#21040;&#23884;&#20837;&#29305;&#24449;&#23398;&#20064;&#20013;&#65292;&#39640;&#25928;&#22320;&#23454;&#29616;&#20102;&#19981;&#21516;&#35270;&#22270;&#20869;&#37096;&#23884;&#20837;&#29305;&#24449;&#30340;&#24179;&#28369;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#19968;&#33268;&#24615;&#32422;&#26463;&#34987;&#24212;&#29992;&#20110;&#23884;&#20837;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09107v1 Announce Type: new  Abstract: Anchor-based large-scale multi-view clustering has attracted considerable attention for its effectiveness in handling massive datasets. However, current methods mainly seek the consensus embedding feature for clustering by exploring global correlations between anchor graphs or projection matrices.In this paper, we propose a simple yet efficient scalable multi-view tensor clustering (S^2MVTC) approach, where our focus is on learning correlations of embedding features within and across views. Specifically, we first construct the embedding feature tensor by stacking the embedding features of different views into a tensor and rotating it. Additionally, we build a novel tensor low-frequency approximation (TLFA) operator, which incorporates graph similarity into embedding feature learning, efficiently achieving smooth representation of embedding features within different views. Furthermore, consensus constraints are applied to embedding featur
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.04814</link><description>&lt;p&gt;
&#22312;&#21477;&#27861;&#24863;&#30693;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Syntax-Aware Fill-In-the-Middle&#65288;SAFIM&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#22635;&#31354;&#65288;FIM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#20391;&#37325;&#20110;&#31243;&#24207;&#32467;&#26500;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#65292;&#22914;&#20195;&#30721;&#22359;&#21644;&#26465;&#20214;&#34920;&#36798;&#24335;&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;17,720&#20010;&#31034;&#20363;&#65292;&#26469;&#28304;&#20110;2022&#24180;4&#26376;&#20043;&#21518;&#30340;&#26368;&#26032;&#20195;&#30721;&#25552;&#20132;&#65292;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#12290; SAFIM&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#21644;&#26032;&#39062;&#30340;&#21477;&#27861;&#24863;&#30693;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;15&#20010;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#21319;&#20102;FIM&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#36824;&#25913;&#36827;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#65288;L2R&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;SAFIM&#20026;&#26410;&#26469;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#20110;&#19968;&#33324;&#21270;&#30340;log-Sobolev&#21644;Polyak-Lojasiewicz&#19981;&#31561;&#24335;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20197;&#21450;&#25512;&#24191;&#20102;Bakry-Emery&#23450;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.02004</link><description>&lt;p&gt;
&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#20197;&#21450;log-Sobolev&#21644;Talagrand&#19981;&#31561;&#24335;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02004
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#20110;&#19968;&#33324;&#21270;&#30340;log-Sobolev&#21644;Polyak-Lojasiewicz&#19981;&#31561;&#24335;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20197;&#21450;&#25512;&#24191;&#20102;Bakry-Emery&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;(PGD)~(Kuntz&#31561;&#20154;&#65292;2023)&#30340;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#38480;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#21270;&#33258;&#30001;&#33021;&#26799;&#24230;&#27969;&#33719;&#24471;&#30340;&#22823;&#22411;&#28508;&#21464;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#23545;&#20110;&#28385;&#36275;&#19968;&#33324;&#21270;log-Sobolev&#21644;Polyak-Lojasiewicz&#19981;&#31561;&#24335;&#65288;LSI&#21644;PLI&#65289;&#30340;&#27169;&#22411;&#65292;&#27969;&#20197;&#25351;&#25968;&#36895;&#24230;&#25910;&#25947;&#21040;&#33258;&#30001;&#33021;&#30340;&#26497;&#23567;&#21270;&#38598;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26368;&#20248;&#36755;&#36816;&#25991;&#29486;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#32467;&#26524;&#65288;LSI&#24847;&#21619;&#30528;Talagrand&#19981;&#31561;&#24335;&#65289;&#21450;&#20854;&#22312;&#20248;&#21270;&#25991;&#29486;&#20013;&#30340;&#23545;&#24212;&#29289;&#65288;PLI&#24847;&#21619;&#30528;&#25152;&#35859;&#30340;&#20108;&#27425;&#22686;&#38271;&#26465;&#20214;&#65289;&#25193;&#23637;&#24182;&#24212;&#29992;&#21040;&#25105;&#20204;&#30340;&#26032;&#35774;&#32622;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#25512;&#24191;&#20102;Bakry-Emery&#23450;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#24378;&#20985;&#23545;&#25968;&#20284;&#28982;&#30340;&#27169;&#22411;&#65292;LSI/PLI&#30340;&#27010;&#25324;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02004v1 Announce Type: new  Abstract: We prove non-asymptotic error bounds for particle gradient descent (PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that, for models satisfying a condition generalizing both the log-Sobolev and the Polyak--{\L}ojasiewicz inequalities (LSI and P{\L}I, respectively), the flow converges exponentially fast to the set of minimizers of the free energy. We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the P{\L}I implies the so-called quadratic growth condition), and applying it to our new setting. We also generalize the Bakry--\'Emery Theorem and show that the LSI/P{\L}I generalization holds for models with strongly concave log-likelihoods. For such m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOGA&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#30005;&#36335;&#20013;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#65292;&#36890;&#36807;&#36339;&#25968;&#29305;&#24449;&#21644;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#30005;&#36335;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.01317</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#38754;&#21521;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#23398;&#20064;&#30340;&#36339;&#25968;&#22270;&#27880;&#24847;&#21147;&#22312;&#30005;&#36335;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOGA&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#30005;&#36335;&#20013;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#65292;&#36890;&#36807;&#36339;&#25968;&#29305;&#24449;&#21644;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#30005;&#36335;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#20219;&#21153;&#20013;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#26041;&#38754;&#21464;&#24471;&#27969;&#34892;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#22823;&#22270;&#26102;&#65292;&#23427;&#20204;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#24182;&#19988;&#23545;&#26032;&#35774;&#35745;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#36825;&#20123;&#38480;&#21046;&#20351;&#23427;&#20204;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22797;&#26434;&#30005;&#36335;&#38382;&#39064;&#26102;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOGA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#12290;HOGA&#39318;&#20808;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#38024;&#23545;&#27599;&#20010;&#33410;&#28857;&#35745;&#31639;&#36339;&#25968;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#36339;&#25968;&#29305;&#24449;&#20165;&#29992;&#20110;&#36890;&#36807;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#29983;&#25104;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#27169;&#22359;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#19981;&#21516;&#36339;&#25968;&#20043;&#38388;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#32780;&#19981;&#28041;&#21450;&#22270;&#25299;&#25169;&#12290;&#22240;&#27492;&#65292;HOGA&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30005;&#36335;&#20043;&#38388;&#30340;&#21508;&#31181;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e
&lt;/p&gt;</description></item><item><title>GEM3D&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31070;&#32463;&#39592;&#26550;&#32534;&#30721;&#20102;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#34920;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.16994</link><description>&lt;p&gt;
GEM3D&#65306;&#19977;&#32500;&#24418;&#29366;&#21512;&#25104;&#30340;&#29983;&#25104;&#23186;&#20307;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16994
&lt;/p&gt;
&lt;p&gt;
GEM3D&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31070;&#32463;&#39592;&#26550;&#32534;&#30721;&#20102;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;GEM3D&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#22522;&#20110;&#31070;&#32463;&#39592;&#26550;&#30340;&#34920;&#31034;&#65292;&#32534;&#30721;&#20102;&#20851;&#20110;&#24418;&#29366;&#25299;&#25169;&#21644;&#20960;&#20309;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#19968;&#20010;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#36981;&#24490;&#20013;&#36724;&#21464;&#25442;&#65288;MAT&#65289;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#34920;&#38754;&#12290;&#31070;&#32463;&#38544;&#24335;&#32771;&#34385;&#20102;&#22312;&#29983;&#25104;&#30340;&#39592;&#26550;&#34920;&#31034;&#20013;&#23384;&#20648;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#20135;&#29983;&#30340;&#34920;&#38754;&#19982;&#20043;&#21069;&#30340;&#31070;&#32463;&#22330;&#20844;&#24335;&#30456;&#27604;&#26356;&#21152;&#25299;&#25169;&#21644;&#20960;&#20309;&#20934;&#30830;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24418;&#29366;&#21512;&#25104;&#21644;&#28857;&#20113;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#26356;&#20026;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#24418;&#29366;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16994v1 Announce Type: cross  Abstract: We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23618;&#19981;&#21464;&#24615;&#26500;&#24314;&#31283;&#20581;&#19988;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#19981;&#21464;&#24615;&#34920;&#31034;&#22312;&#36739;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21487;&#21306;&#20998;&#24615;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15430</link><description>&lt;p&gt;
&#36739;&#22823;&#23610;&#24230;&#19979;&#29992;&#20110;&#31283;&#20581;&#19988;&#21487;&#35299;&#37322;&#35270;&#35273;&#20219;&#21153;&#30340;&#20998;&#23618;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#19981;&#21464;&#24615;&#26500;&#24314;&#31283;&#20581;&#19988;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#19981;&#21464;&#24615;&#34920;&#31034;&#22312;&#36739;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21487;&#21306;&#20998;&#24615;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#31283;&#20581;&#19988;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#31995;&#32479;&#26159;&#36808;&#21521;&#21487;&#38752;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#26159;&#32771;&#34385;&#22312;&#22522;&#26412;&#22270;&#20687;&#34920;&#31034;&#20013;&#23884;&#20837;&#20219;&#21153;&#25152;&#38656;&#30340;&#19981;&#21464;&#32467;&#26500;&#65292;&#20363;&#22914;&#20960;&#20309;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19981;&#21464;&#30340;&#34920;&#31034;&#36890;&#24120;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#21487;&#38752;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#38024;&#23545;&#36825;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#12289;&#23454;&#36341;&#21644;&#24212;&#29992;&#30340;&#35282;&#24230;&#23545;&#20998;&#23618;&#19981;&#21464;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#20307;&#31995;&#32467;&#26500;&#20294;&#20197;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26500;&#24314;&#36229;&#23436;&#22791;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36890;&#29992;&#34013;&#22270;&#12289;&#20855;&#20307;&#23450;&#20041;&#12289;&#19981;&#21464;&#29305;&#24615;&#21644;&#25968;&#20540;&#23454;&#29616;&#12290;&#22312;&#23454;&#36341;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23450;&#21046;&#19981;&#21464;&#24615;&#20197;&#36866;&#24212;&#26356;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15430v1 Announce Type: cross  Abstract: Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence. In this regard, a promising paradigm considers embedding task-required invariant structures, e.g., geometric invariance, in the fundamental image representation. However, such invariant representations typically exhibit limited discriminability, limiting their applications in larger-scale trustworthy vision tasks. For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives. At the theoretical level, we show how to construct over-complete invariants with a Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a fully interpretable manner. The general blueprint, specific definitions, invariant properties, and numerical implementations are provided. At the practical level, we discuss how to customize 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#38656;&#35201;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#20102;&#28382;&#21518;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.13989</link><description>&lt;p&gt;
FedADMM-InSa: &#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#23398;&#20064;ADMM
&lt;/p&gt;
&lt;p&gt;
FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#38656;&#35201;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#20102;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#23398;&#20064;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#12290;&#26377;&#25928;&#30340;FL&#31639;&#27861;&#30340;&#21457;&#23637;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#24322;&#26500;&#25968;&#25454;&#21644;&#31995;&#32479;&#12289;&#36890;&#20449;&#33021;&#21147;&#26377;&#38480;&#20197;&#21450;&#21463;&#38480;&#30340;&#26412;&#22320;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;FedADMM&#26041;&#27861;&#23545;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#38887;&#24615;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36229;&#21442;&#25968;&#27809;&#26377;&#32463;&#36807;&#31934;&#24515;&#35843;&#25972;&#65292;&#23427;&#20204;&#20173;&#28982;&#20250;&#36973;&#21463;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#21517;&#20026;FedADMM-InSa&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#20197;&#28040;&#38500;&#24517;&#39035;&#26681;&#25454;&#32463;&#39564;&#35774;&#32622;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24615;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#21487;&#20197;&#30001;&#27599;&#20010;&#23458;&#25143;&#31471;&#29420;&#31435;&#22320;&#26681;&#25454;&#20854;&#29420;&#29305;&#26465;&#20214;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#38477;&#20302;&#26412;&#22320;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#19981;&#33391;&#30340;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13989v1 Announce Type: new  Abstract: Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle e
&lt;/p&gt;</description></item><item><title>LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.08170</link><description>&lt;p&gt;
LLaGA: &#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
LLaGA: Large Language and Graph Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08170
&lt;/p&gt;
&lt;p&gt;
LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25512;&#21160;&#20102;&#22270;&#32467;&#26500;&#25968;&#25454;&#20998;&#26512;&#30340;&#36827;&#27493;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#30340;&#23835;&#36215;&#39044;&#31034;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#36824;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#25991;&#26412;&#30340;&#22266;&#26377;&#38590;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;&#65288;LLaGA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;LLM&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;LLaGA&#20445;&#30041;&#20102;LLM&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#23558;&#22270;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;LLM&#36755;&#20837;&#20860;&#23481;&#30340;&#26684;&#24335;&#12290;LLaGA&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
&lt;/p&gt;</description></item><item><title>Minusformer&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#27531;&#24046;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#23558;Transformer&#27169;&#22411;&#20013;&#20449;&#24687;&#32858;&#21512;&#26426;&#21046;&#20174;&#21152;&#27861;&#25913;&#20026;&#20943;&#27861;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;&#36755;&#20986;&#20998;&#25903;&#36880;&#23618;&#23398;&#20064;&#30417;&#30563;&#20449;&#21495;&#30340;&#27531;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02332</link><description>&lt;p&gt;
Minusformer: &#36890;&#36807;&#28176;&#36827;&#23398;&#20064;&#27531;&#24046;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Minusformer: Improving Time Series Forecasting by Progressively Learning Residuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02332
&lt;/p&gt;
&lt;p&gt;
Minusformer&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#27531;&#24046;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#23558;Transformer&#27169;&#22411;&#20013;&#20449;&#24687;&#32858;&#21512;&#26426;&#21046;&#20174;&#21152;&#27861;&#25913;&#20026;&#20943;&#27861;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;&#36755;&#20986;&#20998;&#25903;&#36880;&#23618;&#23398;&#20064;&#30417;&#30563;&#20449;&#21495;&#30340;&#27531;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26222;&#36941;&#23384;&#22312;&#30340;&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#39044;&#27979;&#27169;&#22411;&#23481;&#26131;&#20005;&#37325;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#21435;&#20887;&#20313;&#30340;&#26041;&#27861;&#36880;&#27493;&#24674;&#22797;TS&#30340;&#20869;&#22312;&#20215;&#20540;&#20197;&#29992;&#20110;&#26410;&#26469;&#30340;&#26102;&#38388;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20449;&#24687;&#32858;&#21512;&#26426;&#21046;&#20174;&#21152;&#27861;&#36716;&#21464;&#20026;&#20943;&#27861;&#26469;&#25913;&#36827;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#21407;&#27169;&#22411;&#30340;&#27599;&#20010;&#27169;&#22359;&#20013;&#21152;&#20837;&#19968;&#20010;&#36741;&#21161;&#36755;&#20986;&#20998;&#25903;&#65292;&#26500;&#24314;&#19968;&#26465;&#36890;&#24448;&#26368;&#32456;&#39044;&#27979;&#30340;&#39640;&#36895;&#20844;&#36335;&#12290;&#35813;&#20998;&#25903;&#20013;&#21518;&#32493;&#27169;&#22359;&#30340;&#36755;&#20986;&#23558;&#20943;&#21435;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#32467;&#26524;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36880;&#23618;&#23398;&#20064;&#30417;&#30563;&#20449;&#21495;&#30340;&#27531;&#24046;&#12290;&#36825;&#31181;&#35774;&#35745;&#20419;&#36827;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#30340;&#36880;&#27493;&#23398;&#20064;&#39537;&#21160;&#38544;&#24335;&#20998;&#35299;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#38887;&#24615;&#12290;&#30001;&#20110;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#32858;&#21512;&#37117;&#26159;&#20943;&#21495;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;Minusformer&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we find that ubiquitous time series (TS) forecasting models are prone to severe overfitting. To cope with this problem, we embrace a de-redundancy approach to progressively reinstate the intrinsic values of TS for future intervals. Specifically, we renovate the vanilla Transformer by reorienting the information aggregation mechanism from addition to subtraction. Then, we incorporate an auxiliary output branch into each block of the original model to construct a highway leading to the ultimate prediction. The output of subsequent modules in this branch will subtract the previously learned results, enabling the model to learn the residuals of the supervision signal, layer by layer. This designing facilitates the learning-driven implicit progressive decomposition of the input and output streams, empowering the model with heightened versatility, interpretability, and resilience against overfitting. Since all aggregations in the model are minus signs, which is called Minusfor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#20799;&#31461;&#21644;&#25945;&#24072;&#20329;&#25140;&#30340;&#24405;&#38899;&#35774;&#22791;&#35760;&#24405;&#35328;&#35821;&#65292;&#23454;&#29616;&#35828;&#35805;&#32773;&#20998;&#31867;&#21644;&#36716;&#24405;&#65292;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#27604;&#32467;&#26524;&#34920;&#26126;&#65292;&#26694;&#26550;&#25972;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;0.76&#65292;&#20026;&#23398;&#21069;&#25945;&#23460;&#35328;&#35821;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2401.07342</link><description>&lt;p&gt;
&#35841;&#35828;&#20102;&#20160;&#20040;&#65311;&#20998;&#26512;&#23398;&#21069;&#35838;&#22530;&#35328;&#35821;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Who Said What? An Automated Approach to Analyzing Speech in Preschool Classrooms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07342
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#20799;&#31461;&#21644;&#25945;&#24072;&#20329;&#25140;&#30340;&#24405;&#38899;&#35774;&#22791;&#35760;&#24405;&#35328;&#35821;&#65292;&#23454;&#29616;&#35828;&#35805;&#32773;&#20998;&#31867;&#21644;&#36716;&#24405;&#65292;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#27604;&#32467;&#26524;&#34920;&#26126;&#65292;&#26694;&#26550;&#25972;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;0.76&#65292;&#20026;&#23398;&#21069;&#25945;&#23460;&#35328;&#35821;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24180;&#24188;&#20799;&#31461;&#22312;&#21927;&#38393;&#30340;&#23398;&#21069;&#35838;&#22530;&#20013;&#24230;&#36807;&#22823;&#37096;&#20998;&#28165;&#37266;&#26102;&#38388;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#23401;&#23376;&#19982;&#25945;&#24072;&#20043;&#38388;&#30340;&#35328;&#35821;&#20114;&#21160;&#23545;&#20182;&#20204;&#30340;&#35821;&#35328;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25163;&#21160;&#36716;&#24405;&#36825;&#20123;&#20114;&#21160;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#20799;&#31461;&#21644;&#25945;&#24072;&#20329;&#25140;&#30340;&#24405;&#38899;&#35774;&#22791;&#24405;&#38899;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;&#23545;&#35828;&#35805;&#32773;&#36827;&#34892;&#20998;&#31867;&#65288;ALICE&#65289;&#65292;&#24182;&#36716;&#24405;&#20182;&#20204;&#30340;&#35805;&#35821;&#65288;Whisper&#65289;&#12290;&#25105;&#20204;&#23558;&#26694;&#26550;&#30340;&#32467;&#26524;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;110&#20998;&#38047;&#25945;&#23460;&#24405;&#38899;&#65288;&#21253;&#25324;&#26469;&#33258;&#20799;&#31461;&#35805;&#31570;&#30340;85&#20998;&#38047;&#65288;4&#21517;&#20799;&#31461;&#65289;&#21644;&#26469;&#33258;&#25945;&#24072;&#35805;&#31570;&#30340;25&#20998;&#38047;&#65288;2&#21517;&#25945;&#24072;&#65289;&#65289;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#24635;&#20307;&#19968;&#33268;&#27604;&#20363;&#65292;&#21363;&#27491;&#30830;&#20998;&#31867;&#30340;&#25945;&#24072;&#21644;&#20799;&#31461;&#35805;&#35821;&#30340;&#27604;&#20363;&#20026;0.76&#65292;&#30699;&#27491;&#30340;kappa&#20026;0.50&#65292;&#21152;&#26435;F1&#20026;0.76&#12290;&#25945;&#24072;&#21644;&#20799;&#31461;&#30340;&#35805;&#35821;&#30340;&#35789;&#35823;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07342v2 Announce Type: replace-cross  Abstract: Young children spend substantial portions of their waking hours in noisy preschool classrooms. In these environments, children's vocal interactions with teachers are critical contributors to their language outcomes, but manually transcribing these interactions is prohibitive. Using audio from child- and teacher-worn recorders, we propose an automated framework that uses open source software both to classify speakers (ALICE) and to transcribe their utterances (Whisper). We compare results from our framework to those from a human expert for 110 minutes of classroom recordings, including 85 minutes from child-word microphones (n=4 children) and 25 minutes from teacher-worn microphones (n=2 teachers). The overall proportion of agreement, that is, the proportion of correctly classified teacher and child utterances, was .76, with an error-corrected kappa of .50 and a weighted F1 of .76. The word error rate for both teacher and child 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#20026;&#33719;&#24471;&#24494;&#35843;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.01957</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#32463;&#36807;&#31934;&#28860;&#30340;LLMs&#33258;&#25105;&#25209;&#35780;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#20026;&#33719;&#24471;&#24494;&#35843;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;(dSC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;Gibbs&#37319;&#26679;&#22120;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#28982;&#21518;&#23558;&#20854;&#33976;&#39311;&#25104;&#19968;&#20010;&#24494;&#35843;&#27169;&#22411;&#12290;&#21482;&#38656;&#35201;&#21512;&#25104;&#25968;&#25454;&#65292;dSC&#22312;&#28041;&#21450;&#23433;&#20840;&#24615;&#12289;&#24773;&#24863;&#21644;&#38544;&#31169;&#25511;&#21046;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#20316;&#20026;&#23545;&#40784;LLMs&#30340;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20195;&#30721;&#22312;\url{https://github.com/vicgalle/distilled-self-critique}&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01957v2 Announce Type: replace  Abstract: This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \url{https://github.com/vicgalle/distilled-self-critique}.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30340;&#21327;&#35843;&#21644;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.09852</link><description>&lt;p&gt;
&#30701;&#26399;&#19982;&#38271;&#26399;&#26080;&#20154;&#26426;&#21327;&#35843;&#65306;&#20998;&#24067;&#24335;&#20248;&#21270;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#27719;
&lt;/p&gt;
&lt;p&gt;
Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning. (arXiv:2311.09852v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09852
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30340;&#21327;&#35843;&#21644;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#22478;&#24066;&#20013;&#65292;&#25903;&#25345;&#20805;&#30005;&#25216;&#26415;&#30340;&#33258;&#20027;&#20132;&#20114;&#24335;&#26080;&#20154;&#26426;&#32676;&#21487;&#20197;&#25552;&#20379;&#24341;&#20154;&#27880;&#30446;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20363;&#22914;&#20132;&#36890;&#30417;&#27979;&#21644;&#28798;&#38590;&#21709;&#24212;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#26088;&#22312;&#21327;&#35843;&#26080;&#20154;&#26426;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#39640;&#36136;&#37327;&#30340;&#23548;&#33322;&#12289;&#24863;&#30693;&#21644;&#20805;&#30005;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65306;&#30701;&#26399;&#20248;&#21270;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24847;&#22806;&#21464;&#21270;&#19979;&#24182;&#19981;&#26377;&#25928;&#65292;&#32780;&#38271;&#26399;&#23398;&#20064;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#12289;&#38887;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#22522;&#20110;DRL&#30340;&#38271;&#26399;&#39134;&#34892;&#26041;&#21521;&#30340;&#25112;&#30053;&#35843;&#24230;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23545;&#20174;&#29616;&#23454;&#22478;&#24066;&#31227;&#21160;&#20013;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarms of autonomous interactive drones, with the support of recharging technology, can provide compelling sensing capabilities in Smart Cities, such as traffic monitoring and disaster response. Existing approaches, including distributed optimization and deep reinforcement learning (DRL), aim to coordinate drones to achieve cost-effective, high-quality navigation, sensing, and charging. However, they face grand challenges: short-term optimization is not effective in dynamic environments with unanticipated changes, while long-term learning lacks scalability, resilience, and flexibility. To bridge this gap, this paper introduces a new progressive approach that combines short-term plan generation and selection based on distributed optimization with a DRL-based long-term strategic scheduling of flying direction. Extensive experimentation with datasets generated from realistic urban mobility underscores an outstanding performance of the proposed solution compared to state-of-the-art. We als
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2311.00445</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;&#31995;&#32479;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00445
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#34892;&#20026;&#30340;&#19968;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#36923;&#36753;&#25512;&#29702;&#65306;&#30830;&#23450;&#21738;&#20123;&#32467;&#35770;&#21487;&#20197;&#20174;&#19968;&#32452;&#21069;&#25552;&#20013;&#24471;&#20986;&#12290;&#24515;&#29702;&#23398;&#23478;&#24050;&#32463;&#35760;&#24405;&#19979;&#20154;&#31867;&#25512;&#29702;&#19982;&#36923;&#36753;&#35268;&#21017;&#19981;&#31526;&#30340;&#20960;&#31181;&#26041;&#24335;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#36825;&#20123;&#20559;&#24046;&#65292;&#25110;&#32773;&#23427;&#20204;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#24046;&#65311;&#25105;&#20204;&#20851;&#27880;&#19977;&#27573;&#35770;&#30340;&#24773;&#20917; - &#20174;&#20004;&#20010;&#31616;&#21333;&#21069;&#25552;&#20013;&#25512;&#23548;&#20986;&#30340;&#25512;&#29702;&#65292;&#36825;&#22312;&#24515;&#29702;&#23398;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350; - &#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#27169;&#22411;&#27604;&#36739;&#21512;&#36923;&#36753;&#65292;&#32780;&#19988;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#19968;&#20123;&#38169;&#35823;&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;&#20559;&#35265;&#30456;&#20284;&#65292;&#20363;&#22914;&#25490;&#24207;&#25928;&#24212;&#21644;&#36923;&#36753;&#35884;&#35823;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#27169;&#20223;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29615;&#22659;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#20197;&#20272;&#35745;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26679;&#26412;&#23481;&#37327;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.16646</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#20540;&#20272;&#35745;&#29992;&#20110;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model predictive control-based value estimation for efficient reinforcement learning. (arXiv:2310.16646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16646
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29615;&#22659;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#20197;&#20272;&#35745;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26679;&#26412;&#23481;&#37327;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#30528;&#19982;&#34394;&#25311;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#27425;&#25968;&#25152;&#24102;&#26469;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23545;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#12290;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#29615;&#22659;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#20197;&#20272;&#35745;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#20197;&#25509;&#36817;&#26368;&#20248;&#20540;&#65292;&#24182;&#19988;&#22312;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#38656;&#35201;&#26356;&#23569;&#30340;&#26679;&#26412;&#23481;&#37327;&#12290;&#22312;&#32463;&#20856;&#25968;&#25454;&#24211;&#21644;&#26080;&#20154;&#26426;&#21160;&#24577;&#36991;&#38556;&#22330;&#26223;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning suffers from limitations in real practices primarily due to the numbers of required interactions with virtual environments. It results in a challenging problem that we are implausible to obtain an optimal strategy only with a few attempts for many learning method. Hereby, we design an improved reinforcement learning method based on model predictive control that models the environment through a data-driven approach. Based on learned environmental model, it performs multi-step prediction to estimate the value function and optimize the policy. The method demonstrates higher learning efficiency, faster convergent speed of strategies tending to the optimal value, and fewer sample capacity space required by experience replay buffers. Experimental results, both in classic databases and in a dynamic obstacle avoidance scenario for unmanned aerial vehicle, validate the proposed approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#25104;&#26412;&#36807;&#31243;&#20013;&#20272;&#35745;&#26080;&#38480;&#26102;&#38388;&#25240;&#29616;&#25104;&#26412;&#30340;VaR&#21644;CVaR&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#26368;&#23567;&#26368;&#22823;&#19979;&#30028;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#26102;&#38388;&#25130;&#26029;&#26041;&#26696;&#24471;&#20986;&#20102;&#19978;&#30028;&#12290;&#36825;&#26159;&#22312;&#39532;&#23572;&#21487;&#22827;&#35774;&#32622;&#20013;&#39318;&#27425;&#25552;&#20379;&#20219;&#20309;&#39118;&#38505;&#24230;&#37327;&#20272;&#35745;&#35823;&#24046;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.11389</link><description>&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#25104;&#26412;&#36807;&#31243;&#20013;&#30340;VaR&#21644;CVaR&#20272;&#35745;&#65306;&#19979;&#30028;&#21644;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
VaR\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds. (arXiv:2310.11389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#25104;&#26412;&#36807;&#31243;&#20013;&#20272;&#35745;&#26080;&#38480;&#26102;&#38388;&#25240;&#29616;&#25104;&#26412;&#30340;VaR&#21644;CVaR&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#26368;&#23567;&#26368;&#22823;&#19979;&#30028;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#26102;&#38388;&#25130;&#26029;&#26041;&#26696;&#24471;&#20986;&#20102;&#19978;&#30028;&#12290;&#36825;&#26159;&#22312;&#39532;&#23572;&#21487;&#22827;&#35774;&#32622;&#20013;&#39318;&#27425;&#25552;&#20379;&#20219;&#20309;&#39118;&#38505;&#24230;&#37327;&#20272;&#35745;&#35823;&#24046;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#25104;&#26412;&#36807;&#31243;&#20013;&#20272;&#35745;&#26080;&#38480;&#26102;&#38388;&#25240;&#29616;&#25104;&#26412;&#30340;&#39118;&#38505;&#20215;&#20540;&#65288;Value-at-Risk&#65292;VaR&#65289;&#21644;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65288;Conditional Value-at-Risk&#65292;CVaR&#65289;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#26368;&#23567;&#26368;&#22823;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#22312;&#26399;&#26395;&#24847;&#20041;&#21644;&#27010;&#29575;&#24847;&#20041;&#19979;&#37117;&#25104;&#31435;&#65292;&#20854;&#35823;&#24046;&#30028;&#20026;$\Omega(1/\sqrt{n})$&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#26377;&#38480;&#26102;&#38388;&#25130;&#26029;&#26041;&#26696;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;CVaR&#20272;&#35745;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#19982;&#25105;&#20204;&#30340;&#19979;&#30028;&#21305;&#37197;&#65292;&#21482;&#26377;&#24120;&#25968;&#22240;&#23376;&#30340;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#26041;&#26696;&#30340;&#25193;&#23637;&#65292;&#28085;&#30422;&#20102;&#26356;&#36890;&#29992;&#30340;&#28385;&#36275;&#19968;&#23450;&#36830;&#32493;&#24615;&#20934;&#21017;&#30340;&#39118;&#38505;&#24230;&#37327;&#65292;&#20363;&#22914;&#35889;&#39118;&#38505;&#24230;&#37327;&#21644;&#22522;&#20110;&#25928;&#29992;&#30340;&#32570;&#21475;&#39118;&#38505;&#24230;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#22312;&#39532;&#23572;&#21487;&#22827;&#35774;&#32622;&#20013;&#20026;&#20219;&#20309;&#39118;&#38505;&#24230;&#37327;&#25552;&#20379;&#20272;&#35745;&#35823;&#24046;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25105;&#20204;&#30340;&#19979;&#30028;&#20063;&#21487;&#25193;&#23637;&#21040;&#26080;&#38480;&#26102;&#38388;&#25240;&#29616;&#25104;&#26412;&#30340;&#22343;&#20540;&#12290;&#21363;&#20351;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;$\Omega(1/\sqrt{n})$&#20063;&#20248;&#20110;&#29616;&#26377;&#32467;&#26524;$\Omega(
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of estimating the Value-at-Risk (VaR) and the Conditional Value-at-Risk (CVaR) of the infinite-horizon discounted cost within a Markov cost process. First, we derive a minimax lower bound of $\Omega(1/\sqrt{n})$ that holds both in an expected and in a probabilistic sense. Then, using a finite-horizon truncation scheme, we derive an upper bound for the error in CVaR estimation, which matches our lower bound up to constant factors. Finally, we discuss an extension of our estimation scheme that covers more general risk measures satisfying a certain continuity criterion, e.g., spectral risk measures, utility-based shortfall risk. To the best of our knowledge, our work is the first to provide lower and upper bounds on the estimation error for any risk measure within Markovian settings. We remark that our lower bounds also extend to the infinite-horizon discounted costs' mean. Even in that case, our result $\Omega(1/\sqrt{n}) $ improves upon the existing result $\Omega(
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#27934;&#23519;&#29616;&#35937;&#21487;&#33021;&#26159;&#30001;&#31070;&#32463;&#32593;&#32476;&#20174;&#25042;&#24816;&#35757;&#32451;&#21160;&#24577;&#36807;&#28193;&#21040;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#36319;&#36394;&#36275;&#22815;&#30340;&#32479;&#35745;&#37327;&#65292;&#21457;&#29616;&#27934;&#23519;&#26159;&#22312;&#32593;&#32476;&#39318;&#20808;&#23581;&#35797;&#25311;&#21512;&#26680;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#21518;&#65292;&#36827;&#34892;&#21518;&#26399;&#29305;&#24449;&#23398;&#20064;&#25214;&#21040;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#20043;&#21518;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06110</link><description>&lt;p&gt;
&#20174;&#25042;&#24816;&#21040;&#20016;&#23500;&#35757;&#32451;&#21160;&#24577;&#30340;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Grokking as the Transition from Lazy to Rich Training Dynamics. (arXiv:2310.06110v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06110
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#27934;&#23519;&#29616;&#35937;&#21487;&#33021;&#26159;&#30001;&#31070;&#32463;&#32593;&#32476;&#20174;&#25042;&#24816;&#35757;&#32451;&#21160;&#24577;&#36807;&#28193;&#21040;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#36319;&#36394;&#36275;&#22815;&#30340;&#32479;&#35745;&#37327;&#65292;&#21457;&#29616;&#27934;&#23519;&#26159;&#22312;&#32593;&#32476;&#39318;&#20808;&#23581;&#35797;&#25311;&#21512;&#26680;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#21518;&#65292;&#36827;&#34892;&#21518;&#26399;&#29305;&#24449;&#23398;&#20064;&#25214;&#21040;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#20043;&#21518;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#27934;&#23519;&#29616;&#35937;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25439;&#22833;&#22312;&#27979;&#35797;&#25439;&#22833;&#20043;&#21069;&#22823;&#24133;&#19979;&#38477;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#20174;&#25042;&#24816;&#30340;&#35757;&#32451;&#21160;&#24577;&#36716;&#21464;&#20026;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#26426;&#21046;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;Vanilla&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#22810;&#39033;&#24335;&#22238;&#24402;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#35813;&#35757;&#32451;&#23637;&#29616;&#20102;&#26080;&#27861;&#29992;&#29616;&#26377;&#29702;&#35770;&#35299;&#37322;&#30340;&#27934;&#23519;&#29616;&#35937;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#35813;&#32593;&#32476;&#27979;&#35797;&#25439;&#22833;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#36319;&#36394;&#36825;&#20123;&#32479;&#35745;&#37327;&#25581;&#31034;&#20102;&#27934;&#23519;&#29616;&#35937;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#39318;&#20808;&#23581;&#35797;&#20351;&#29992;&#21021;&#22987;&#29305;&#24449;&#25311;&#21512;&#26680;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#65292;&#25509;&#30528;&#22312;&#35757;&#32451;&#25439;&#22833;&#24050;&#32463;&#24456;&#20302;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21518;&#26399;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#25214;&#21040;&#20102;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27934;&#23519;&#20135;&#29983;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#29305;&#24449;&#23398;&#20064;&#30340;&#36895;&#29575;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#32553;&#25918;&#32593;&#32476;&#21442;&#25968;&#26469;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning -- which can be controlled precisely by parameters that scale the ne
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02391</link><description>&lt;p&gt;
SE(3)-&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#20013;&#30340;&#38543;&#26426;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#19977;&#32500;&#21018;&#20307;&#36816;&#21160;&#65288;&#21363;SE(3)&#32676;&#65289;&#30340;&#27969;&#21305;&#37197;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#26029;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65306;FoldFlow&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#20027;&#38142;&#30340;&#20934;&#30830;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FoldFlow-Base&#65292;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#23398;&#20064;&#30830;&#23450;&#24615;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#21305;&#37197;&#19981;&#21464;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Riemannian&#26368;&#20248;&#20256;&#36755;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;FoldFlow-OT&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#26356;&#31616;&#21333;&#21644;&#31283;&#23450;&#30340;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;FoldFlow-SFM&#65292;&#23558;Riemannian&#26368;&#20248;&#20256;&#36755;&#21644;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;SE(3)&#19978;&#30340;&#38543;&#26426;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;FoldFlow&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#12290;&#36890;&#36807;&#23545;&#22238;&#25253;&#26223;&#35266;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#22833;&#36133;&#21306;&#22495;&#21644;&#31574;&#30053;&#21697;&#36136;&#30340;&#38544;&#34255;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14597</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#25511;&#21046;&#20013;&#30340;&#22122;&#22768;&#37051;&#22495;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control. (arXiv:2309.14597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#12290;&#36890;&#36807;&#23545;&#22238;&#25253;&#26223;&#35266;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#22833;&#36133;&#21306;&#22495;&#21644;&#31574;&#30053;&#21697;&#36136;&#30340;&#38544;&#34255;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#24615;&#33021;&#19978;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#20174;&#30740;&#31350;&#31574;&#30053;&#21644;&#22238;&#25253;&#20043;&#38388;&#30340;&#26144;&#23556;&#21363;&#22238;&#25253;&#26223;&#35266;&#30340;&#35282;&#24230;&#20026;&#36825;&#20123;&#34892;&#20026;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27969;&#34892;&#30340;&#31639;&#27861;&#22312;&#36825;&#20010;&#26223;&#35266;&#30340;&#22122;&#22768;&#37051;&#22495;&#20013;&#31359;&#34892;&#65292;&#19968;&#20010;&#31574;&#30053;&#21442;&#25968;&#30340;&#21333;&#27425;&#26356;&#26032;&#20250;&#23548;&#33268;&#22238;&#25253;&#22312;&#24456;&#22823;&#33539;&#22260;&#20869;&#21464;&#21270;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#22238;&#25253;&#36827;&#34892;&#20998;&#24067;&#22788;&#29702;&#65292;&#25105;&#20204;&#23545;&#26223;&#35266;&#36827;&#34892;&#20102;&#26144;&#23556;&#65292;&#25551;&#36848;&#20102;&#31574;&#30053;&#31354;&#38388;&#20013;&#23481;&#26131;&#20135;&#29983;&#22833;&#36133;&#30340;&#21306;&#22495;&#65292;&#24182;&#25581;&#31034;&#20102;&#31574;&#30053;&#21697;&#36136;&#30340;&#38544;&#34255;&#32500;&#24230;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26223;&#35266;&#30340;&#24778;&#20154;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#25214;&#21040;&#31616;&#21333;&#30340;&#36335;&#24452;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#24320;&#22122;&#22768;&#37051;&#22495;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20248;&#21270;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time. In this work, we provide a fresh perspective on these behaviors by studying the return landscape: the mapping between a policy and a return. We find that popular algorithms traverse noisy neighborhoods of this landscape, in which a single update to the policy parameters leads to a wide range of returns. By taking a distributional view of these returns, we map the landscape, characterizing failure-prone regions of policy space and revealing a hidden dimension of policy quality. We show that the landscape exhibits surprising structure by finding simple paths in parameter space which improve the stability of a policy. To conclude, we develop a distribution-aware procedure which finds such paths, navigating away from noisy neighborhoods in order to improve the robustness of a policy. Taken together, our results provide new insight into the optimization, eva
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;13&#31181;&#32852;&#37030;DG&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#37327;&#23458;&#25143;&#31471;&#21644;&#39640;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;DG&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.04942</link><description>&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#38024;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Algorithms for Federated Domain Generalization. (arXiv:2307.04942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;13&#31181;&#32852;&#37030;DG&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#37327;&#23458;&#25143;&#31471;&#21644;&#39640;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;DG&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20808;&#21069;&#30340;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#22522;&#20934;&#32771;&#34385;&#20102;&#35757;&#32451;-&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#29305;&#23450;&#25361;&#25112;&#30340;&#32852;&#37030;DG&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23458;&#25143;&#31471;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#25506;&#32034;&#22522;&#20110;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;-&#19968;&#20010;&#29616;&#23454;&#30340;&#32852;&#37030;DG&#22330;&#26223;&#12290;&#20808;&#21069;&#30340;&#32852;&#37030;DG&#35780;&#20272;&#22312;&#23458;&#25143;&#31471;&#25968;&#37327;&#25110;&#24322;&#36136;&#24615;&#20197;&#21450;&#25968;&#25454;&#38598;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;DG&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#21644;&#24322;&#36136;&#24615;&#65292;&#24182;&#25552;&#20379;&#25968;&#25454;&#38598;&#38590;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;13&#31181;&#32852;&#37030;DG&#26041;&#27861;&#65292;&#21253;&#25324;&#36866;&#24212;FL&#29615;&#22659;&#30340;&#38598;&#20013;DG&#26041;&#27861;&#12289;&#22788;&#29702;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;FL&#26041;&#27861;&#20197;&#21450;&#19987;&#20026;&#32852;&#37030;DG&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#22823;&#37327;&#23458;&#25143;&#31471;&#21644;&#39640;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;DG&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
While prior domain generalization (DG) benchmarks consider train-test dataset heterogeneity, we evaluate Federated DG which introduces federated learning (FL) specific challenges. Additionally, we explore domain-based heterogeneity in clients' local datasets - a realistic Federated DG scenario. Prior Federated DG evaluations are limited in terms of the number or heterogeneity of clients and dataset diversity. To address this gap, we propose an Federated DG benchmark methodology that enables control of the number and heterogeneity of clients and provides metrics for dataset difficulty. We then apply our methodology to evaluate 13 Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG. Our results suggest that despite some progress, there remain significant performance gaps in Federated DG particularly when evaluating with a large number of clients, high client h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#24076;&#23572;&#20271;&#29305;&#38454;&#26799;(NHL)&#30340;&#27010;&#24565;&#65292;&#23427;&#23558;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#20989;&#25968;&#31354;&#38388;&#20869;&#30340;&#24615;&#36136;&#21644;&#24212;&#29992;&#12290;&#36890;&#36807;&#35777;&#26126;&#19981;&#21516;&#23618;&#27425;&#30340;NHL&#19982;&#22810;&#23618;NNs&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;NHL&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;NHL&#30340;&#29305;&#24449;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#22312;ReLU&#21644;&#20108;&#27425;&#28608;&#27963;&#20989;&#25968;&#19979;&#23637;&#31034;&#20102;NHLs&#20013;&#30340;&#28145;&#24230;&#20998;&#31163;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.01177</link><description>&lt;p&gt;
&#31070;&#32463;&#24076;&#23572;&#20271;&#29305;&#38454;&#26799;&#65306;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space. (arXiv:2307.01177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#24076;&#23572;&#20271;&#29305;&#38454;&#26799;(NHL)&#30340;&#27010;&#24565;&#65292;&#23427;&#23558;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#20989;&#25968;&#31354;&#38388;&#20869;&#30340;&#24615;&#36136;&#21644;&#24212;&#29992;&#12290;&#36890;&#36807;&#35777;&#26126;&#19981;&#21516;&#23618;&#27425;&#30340;NHL&#19982;&#22810;&#23618;NNs&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;NHL&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;NHL&#30340;&#29305;&#24449;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#22312;ReLU&#21644;&#20108;&#27425;&#28608;&#27963;&#20989;&#25968;&#19979;&#23637;&#31034;&#20102;NHLs&#20013;&#30340;&#28145;&#24230;&#20998;&#31163;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;(NNs)&#25152;&#25506;&#32034;&#30340;&#20989;&#25968;&#31354;&#38388;&#30340;&#29305;&#24449;&#21270;&#26159;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#26412;&#25991;&#23558;&#20855;&#26377;&#20219;&#24847;&#23485;&#24230;&#30340;&#22810;&#23618;NN&#35270;&#20026;&#23450;&#20041;&#29305;&#23450;&#23618;&#27425;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#30340;&#31070;&#32463;&#24076;&#23572;&#20271;&#29305;&#38454;&#26799;(NHL)&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23450;&#20041;&#19968;&#20010;&#20989;&#25968;&#31354;&#38388;&#21644;&#19968;&#20010;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#35813;&#24230;&#37327;&#25512;&#24191;&#20102;&#27973;&#23618;NNs&#30340;&#20808;&#21069;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#20960;&#20010;&#26041;&#38754;&#30340;&#29702;&#35770;&#29305;&#24615;&#21644;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;L&#23618;NNs&#34920;&#31034;&#30340;&#20989;&#25968;&#19982;&#23646;&#20110;L&#23618;NHLs&#30340;&#20989;&#25968;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#20855;&#26377;&#21463;&#25511;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;NHL&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;&#31532;&#19977;&#65292;&#23545;&#24212;&#20110;&#22312;&#26080;&#31351;&#23485;&#22343;&#22330;&#26497;&#38480;&#19979;&#35757;&#32451;&#22810;&#23618;NNs&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;NHL&#30340;&#29305;&#24449;&#21160;&#21147;&#23398;&#65292;&#35813;&#21160;&#21147;&#23398;&#34987;&#25551;&#36848;&#20026;&#22810;&#20010;&#38543;&#26426;&#22330;&#30340;&#28436;&#21270;&#12290;&#31532;&#22235;&#65292;&#22312;ReLU&#21644;&#20108;&#27425;&#28608;&#27963;&#20989;&#25968;&#19979;&#23637;&#31034;&#20102;NHLs&#20013;&#30340;&#28145;&#24230;&#20998;&#31163;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The characterization of the functions spaces explored by neural networks (NNs) is an important aspect of deep learning theory. In this work, we view a multi-layer NN with arbitrary width as defining a particular hierarchy of reproducing kernel Hilbert spaces (RKHSs), named a Neural Hilbert Ladder (NHL). This allows us to define a function space and a complexity measure that generalize prior results for shallow NNs, and we then examine their theoretical properties and implications in several aspects. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning an NHL with the complexity measure controlled. Third, corresponding to the training of multi-layer NNs in the infinite-width mean-field limit, we derive an evolution of the NHL characterized as the dynamics of multiple random fields. Fourth, we show examples of depth separation in NHLs under ReLU and quadratic activation fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28176;&#36827;&#24335;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26041;&#27861;&#65288;IRS&#65289;&#65292;&#21487;&#36890;&#36807;&#37325;&#29992;&#21407;&#22987;&#24179;&#28369;&#27169;&#22411;&#30340;&#35748;&#35777;&#20445;&#35777;&#26469;&#35748;&#35777;&#36817;&#20284;&#27169;&#22411;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#35748;&#35777;&#20462;&#25913;DNN&#30340;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19521</link><description>&lt;p&gt;
&#28176;&#36827;&#24335;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental Randomized Smoothing Certification. (arXiv:2305.19521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28176;&#36827;&#24335;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26041;&#27861;&#65288;IRS&#65289;&#65292;&#21487;&#36890;&#36807;&#37325;&#29992;&#21407;&#22987;&#24179;&#28369;&#27169;&#22411;&#30340;&#35748;&#35777;&#20445;&#35777;&#26469;&#35748;&#35777;&#36817;&#20284;&#27169;&#22411;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#35748;&#35777;&#20462;&#25913;DNN&#30340;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;DNN&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#25277;&#26679;&#26469;&#35777;&#26126;&#20854;&#40065;&#26834;&#24615;&#65292;&#20294;&#35745;&#31639;&#20195;&#20215;&#36739;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#20351;&#29992;&#22823;&#37327;&#26679;&#26412;&#36827;&#34892;&#35777;&#26126;&#26102;&#12290;&#27492;&#22806;&#65292;&#24403;&#20462;&#25913;&#24179;&#28369;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#37327;&#21270;&#25110;&#20462;&#21098;&#65289;&#26102;&#65292;&#35748;&#35777;&#20445;&#35777;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#20462;&#25913;&#30340;DNN&#65292;&#24182;&#19988;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35748;&#35777;&#21487;&#33021;&#20195;&#20215;&#22826;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#28176;&#36827;&#24335;&#40065;&#26834;&#24615;&#35748;&#35777;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65288;IRS&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#24179;&#28369;&#27169;&#22411;&#30340;&#35748;&#35777;&#20445;&#35777;&#65292;&#20197;&#21033;&#29992;&#24456;&#23569;&#30340;&#26679;&#26412;&#35748;&#35777;&#36817;&#20284;&#27169;&#22411;&#12290;IRS&#26174;&#33879;&#38477;&#20302;&#20102;&#35748;&#35777;&#20462;&#25913;DNN&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;IRS&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive.  We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BEBE&#30340;&#21160;&#29289;&#34892;&#20026;&#35745;&#31639;&#20998;&#26512;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;1654&#23567;&#26102;&#30340;&#21160;&#29289;&#29983;&#24577;&#29983;&#29702;&#23398;&#25968;&#25454;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#12289;&#26368;&#20855;&#20998;&#31867;&#22810;&#26679;&#24615;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#65292;&#20316;&#32773;&#20351;&#29992;&#20102;&#21313;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10740</link><description>&lt;p&gt;
&#19968;&#31181;&#21160;&#29289;&#34892;&#20026;&#35745;&#31639;&#20998;&#26512;&#30340;&#22522;&#20934;&#65292;&#20351;&#29992;&#21160;&#29289;&#25658;&#24102;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
A benchmark for computational analysis of animal behavior, using animal-borne tags. (arXiv:2305.10740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BEBE&#30340;&#21160;&#29289;&#34892;&#20026;&#35745;&#31639;&#20998;&#26512;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;1654&#23567;&#26102;&#30340;&#21160;&#29289;&#29983;&#24577;&#29983;&#29702;&#23398;&#25968;&#25454;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#12289;&#26368;&#20855;&#20998;&#31867;&#22810;&#26679;&#24615;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#65292;&#20316;&#32773;&#20351;&#29992;&#20102;&#21313;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#25658;&#24102;&#30340;&#20256;&#24863;&#22120;&#65288;&#8220;&#29983;&#29289;&#35760;&#24405;&#22120;&#8221;&#65289;&#21487;&#20197;&#35760;&#24405;&#19968;&#31995;&#21015;&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#25968;&#25454;&#65292;&#25581;&#31034;&#21160;&#29289;&#29983;&#24577;&#29983;&#29702;&#23398;&#24182;&#25913;&#21892;&#20445;&#25252;&#24037;&#20316;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#20110;&#35299;&#37322;&#29983;&#29289;&#35760;&#24405;&#22120;&#35760;&#24405;&#30340;&#22823;&#37327;&#25968;&#25454;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#27809;&#26377;&#26631;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bio-logger Ethogram Benchmark&#65288;BEBE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#34892;&#20026;&#27880;&#37322;&#65292;&#26631;&#20934;&#21270;&#24314;&#27169;&#20219;&#21153;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;BEBE&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#12289;&#26368;&#20855;&#20998;&#31867;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#36825;&#31181;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#20061;&#20010;&#20998;&#31867;&#21333;&#20803;&#20013;149&#20010;&#20010;&#20307;&#25910;&#38598;&#30340;1654&#23567;&#26102;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;BEBE&#19978;&#35780;&#20272;&#20102;&#21313;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#22312;https://github.com/earthspecies/BEBE&#65292;&#20197;&#20415;&#31038;&#21306;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animal-borne sensors ('bio-loggers') can record a suite of kinematic and environmental data, which can elucidate animal ecophysiology and improve conservation efforts. Machine learning techniques are useful for interpreting the large amounts of data recorded by bio-loggers, but there exists no standard for comparing the different machine learning techniques in this domain. To address this, we present the Bio-logger Ethogram Benchmark (BEBE), a collection of datasets with behavioral annotations, standardized modeling tasks, and evaluation metrics. BEBE is to date the largest, most taxonomically diverse, publicly available benchmark of this type, and includes 1654 hours of data collected from 149 individuals across nine taxa. We evaluate the performance of ten different machine learning methods on BEBE, and identify key challenges to be addressed in future work. Datasets, models, and evaluation code are made publicly available at https://github.com/earthspecies/BEBE, to enable community 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10738</link><description>&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Graph Clustering. (arXiv:2305.10738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#22270;&#32858;&#31867;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36866;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861; - &#21487;&#20197;&#25429;&#33719;&#20851;&#38190;&#30340;&#21160;&#24577;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#35768;&#22810;&#38754;&#21521;&#32858;&#31867;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#26469;&#22788;&#29702;&#12290;&#36825;&#19981;&#20165;&#23548;&#33268;&#20102;&#21160;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20063;&#24341;&#21457;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TGC&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#22270;&#28145;&#24230;&#32858;&#31867;&#65292;&#23427;&#35843;&#25972;&#20102;&#28145;&#24230;&#32858;&#31867;&#25216;&#26415;&#65288;&#32858;&#31867;&#20998;&#37197;&#20998;&#24067;&#21644;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#65289;&#65292;&#20197;&#36866;&#24212;&#26102;&#38388;&#22270;&#22522;&#20110;&#20132;&#20114;&#24207;&#21015;&#30340;&#25209;&#22788;&#29702;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#20102;&#26102;&#38388;&#22270;&#32858;&#31867;&#19982;&#29616;&#26377;&#38745;&#24577;&#22270;&#32858;&#31867;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;TGC&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33647;&#29289;&#20998;&#23376;&#21103;&#20316;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10473</link><description>&lt;p&gt;
&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33647;&#29289;&#20998;&#23376;&#30340;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predicting Side Effect of Drug Molecules using Recurrent Neural Networks. (arXiv:2305.10473v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33647;&#29289;&#20998;&#23376;&#21103;&#20316;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#23450;&#21644;&#39564;&#35777;&#20998;&#23376;&#23646;&#24615;&#65292;&#22914;&#21103;&#20316;&#29992;&#65292;&#26159;&#20998;&#23376;&#21512;&#25104;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#21644;&#32791;&#26102;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36827;&#34892;&#21103;&#20316;&#29992;&#39044;&#27979;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#22797;&#26434;&#27169;&#22411;&#30340;&#38382;&#39064;&#24182;&#22823;&#22823;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification and verification of molecular properties such as side effects is one of the most important and time-consuming steps in the process of molecule synthesis. For example, failure to identify side effects before submission to regulatory groups can cost millions of dollars and months of additional research to the companies. Failure to identify side effects during the regulatory review can also cost lives. The complexity and expense of this task have made it a candidate for a machine learning-based solution. Prior approaches rely on complex model designs and excessive parameter counts for side effect predictions. We believe reliance on complex models only shifts the difficulty away from chemists rather than alleviating the issue. Implementing large models is also expensive without prior access to high-performance computers. We propose a heuristic approach that allows for the utilization of simple neural networks, specifically the recurrent neural network, with a 98+% reduction 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20854;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.13356</link><description>&lt;p&gt;
&#34920;&#29616;&#19981;&#36275;&#20197;&#20026;&#30408;&#65292;&#28145;&#31350;Rashomon&#30340;&#22235;&#37325;&#22863;
&lt;/p&gt;
&lt;p&gt;
Performance is not enough: a story of the Rashomon's quartet. (arXiv:2302.13356v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20854;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24314;&#27169;&#36890;&#24120;&#34987;&#31616;&#21270;&#20026;&#23547;&#25214;&#26368;&#20248;&#27169;&#22411;&#26469;&#20248;&#21270;&#36873;&#23450;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#20294;&#22914;&#26524;&#31532;&#20108;&#20248;&#27169;&#22411;&#33021;&#22815;&#20197;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#21516;&#26679;&#25551;&#36848;&#25968;&#25454;&#21602;&#65311;&#31532;&#19977;&#20010;&#27169;&#22411;&#21602;&#65311;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#20250;&#23398;&#21040;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#20851;&#31995;&#21527;&#65311;&#21463;&#21040;Anscombe&#22235;&#37325;&#22863;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;&#24341;&#20837;&#30340;&#31616;&#21333;&#31034;&#20363;&#26088;&#22312;&#36827;&#19968;&#27493;&#20419;&#36827;&#21487;&#35270;&#21270;&#20316;&#20026;&#27604;&#36739;&#39044;&#27979;&#27169;&#22411;&#36229;&#36234;&#24615;&#33021;&#30340;&#24517;&#35201;&#24037;&#20855;&#12290;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#23500;&#26377;&#27934;&#23519;&#21147;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive modelling is often reduced to finding the best model that optimizes a selected performance measure. But what if the second-best model describes the data equally well but in a completely different way? What about the third? Is it possible that the most effective models learn completely different relationships in the data? Inspired by Anscombe's quartet, this paper introduces Rashomon's quartet, a synthetic dataset for which four models from different classes have practically identical predictive performance. However, their visualization reveals drastically distinct ways of understanding the correlation structure in data. The introduced simple illustrative example aims to further facilitate visualization as a mandatory tool to compare predictive models beyond their performance. We need to develop insightful techniques for the explanatory analysis of model sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26500;&#24314;&#20551;&#35774;&#30340;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#26368;&#30701;&#30340;&#20551;&#35774;&#19981;&#22914;&#36873;&#25321;&#26368;&#24369;&#30340;&#20551;&#35774;&#65292;&#32780;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2301.12987</link><description>&lt;p&gt;
&#20551;&#35774;&#30340;&#26368;&#20339;&#36873;&#25321;&#26159;&#26368;&#24369;&#30340;&#32780;&#19981;&#26159;&#26368;&#30701;&#30340;
&lt;/p&gt;
&lt;p&gt;
The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest. (arXiv:2301.12987v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26500;&#24314;&#20551;&#35774;&#30340;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#26368;&#30701;&#30340;&#20551;&#35774;&#19981;&#22914;&#36873;&#25321;&#26368;&#24369;&#30340;&#20551;&#35774;&#65292;&#32780;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;$A$&#21644;$B$&#26159;&#36825;&#26679;&#30340;&#38598;&#21512;&#65292;&#21363;$A \subset B$&#65292;&#37027;&#20040;&#19968;&#33324;&#21270;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20174;$A$&#25512;&#26029;&#20986;&#19968;&#20010;&#36275;&#20197;&#26500;&#24314;$B$&#30340;&#20551;&#35774;&#12290;&#21487;&#20197;&#20174;$A$&#25512;&#26029;&#20986;&#20219;&#24847;&#25968;&#37327;&#30340;&#20551;&#35774;&#65292;&#20294;&#21482;&#26377;&#20854;&#20013;&#30340;&#19968;&#20123;&#21487;&#20197;&#25512;&#24191;&#21040;$B$&#12290;&#24590;&#26679;&#30693;&#36947;&#21738;&#20123;&#20551;&#35774;&#21487;&#33021;&#25512;&#24191;&#65311;&#19968;&#31181;&#31574;&#30053;&#26159;&#36873;&#25321;&#26368;&#30701;&#30340;&#65292;&#23558;&#21387;&#32553;&#20449;&#24687;&#30340;&#33021;&#21147;&#19982;&#25512;&#24191;&#30340;&#33021;&#21147;&#65288;&#26234;&#33021;&#30340;&#20195;&#29702;&#65289;&#31561;&#21516;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;&#20027;&#21160;&#35748;&#30693;&#30340;&#25968;&#23398;&#24418;&#24335;&#20027;&#20041;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#21387;&#32553;&#26082;&#19981;&#26159;&#26368;&#22823;&#21270;&#34920;&#29616;&#65288;&#29992;&#20551;&#35774;&#25512;&#24191;&#30340;&#27010;&#29575;&#34913;&#37327;&#65289;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20063;&#19981;&#26159;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#19982;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26080;&#20851;&#30340;&#20195;&#29702;&#65292;&#31216;&#20026;&#24369;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20219;&#21153;&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#65292;&#21017;&#19981;&#23384;&#22312;&#20219;&#20309;&#20195;&#29702;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#33267;&#23569;&#19982;&#24369;&#28857;&#26368;&#22823;&#21270;&#30340;&#34920;&#29616;&#30456;&#21516;&#65292;&#21516;&#26102;&#22312;&#33267;&#23569;&#19968;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#27604;&#36739;&#26368;&#22823;&#24369;&#28857;&#21644;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;(MDL)&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#24369;&#28857;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;MDL&#12290;&#25105;&#20204;&#35748;&#20026;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
If $A$ and $B$ are sets such that $A \subset B$, generalisation may be understood as the inference from $A$ of a hypothesis sufficient to construct $B$. One might infer any number of hypotheses from $A$, yet only some of those may generalise to $B$. How can one know which are likely to generalise? One strategy is to choose the shortest, equating the ability to compress information with the ability to generalise (a proxy for intelligence). We examine this in the context of a mathematical formalism of enactive cognition. We show that compression is neither necessary nor sufficient to maximise performance (measured in terms of the probability of a hypothesis generalising). We formulate a proxy unrelated to length or simplicity, called weakness. We show that if tasks are uniformly distributed, then there is no choice of proxy that performs at least as well as weakness maximisation in all tasks while performing strictly better in at least one. In experiments comparing maximum weakness and m
&lt;/p&gt;</description></item></channel></rss>