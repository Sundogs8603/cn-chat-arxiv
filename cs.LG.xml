<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#26174;&#31034;&#20102;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#26080;&#26816;&#32034;&#22522;&#32447;&#30456;&#27604;&#20960;&#20046;&#27809;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#20294;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35299;&#38145;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14338</link><description>&lt;p&gt;
TabR&#65306;&#35299;&#38145;&#26816;&#32034;&#22686;&#24378;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning. (arXiv:2307.14338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26174;&#31034;&#20102;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#26080;&#26816;&#32034;&#22522;&#32447;&#30456;&#27604;&#20960;&#20046;&#27809;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#20294;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35299;&#38145;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#32780;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#30340;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#20854;&#20182;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#26816;&#32034;&#27169;&#22411;&#20174;&#21487;&#29992;&#30340;&#65288;&#35757;&#32451;&#65289;&#25968;&#25454;&#20013;&#26816;&#32034;&#20854;&#20182;&#30456;&#20851;&#23545;&#35937;&#65292;&#20363;&#22914;&#26368;&#36817;&#37051;&#23621;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#30340;&#29305;&#24449;&#29978;&#33267;&#26631;&#31614;&#26469;&#36827;&#34892;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#36866;&#24403;&#35843;&#25972;&#30340;&#31616;&#21333;&#26080;&#26816;&#32034;&#22522;&#32447;&#30456;&#27604;&#65292;&#20960;&#20046;&#27809;&#26377;&#25110;&#32773;&#21482;&#26377;&#24494;&#23567;&#30340;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#26159;&#21542;&#20540;&#24471;&#22312;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#32487;&#32493;&#25506;&#32034;&#36824;&#19981;&#28165;&#26970;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution. Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed. For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction. However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines. Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL.  In this work, we give a strong positive answer to this question. We start by incrementally augmenting a simple feed-forward architecture wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#28857;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#36335;&#24452;&#28857;&#26469;&#20943;&#23569;&#34892;&#20026;&#20811;&#38534;&#20013;&#32047;&#31215;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14326</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#28857;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Waypoint-Based Imitation Learning for Robotic Manipulation. (arXiv:2307.14326v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14326
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#28857;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#36335;&#24452;&#28857;&#26469;&#20943;&#23569;&#34892;&#20026;&#20811;&#38534;&#20013;&#32047;&#31215;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#20877;&#27425;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#20173;&#28982;&#22256;&#25200;&#30528;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#12290;&#36335;&#24452;&#28857;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;BC&#23398;&#20064;&#38382;&#39064;&#30340;&#35270;&#37326;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#20943;&#23569;&#38543;&#26102;&#38388;&#32047;&#31215;&#30340;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#36335;&#24452;&#28857;&#26631;&#35760;&#26159;&#19981;&#20805;&#20998;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22914;&#26524;&#19968;&#20010;&#36712;&#36857;&#27573;&#21487;&#20197;&#29992;&#32447;&#24615;&#36816;&#21160;&#36817;&#20284;&#65292;&#37027;&#20040;&#31471;&#28857;&#21487;&#20197;&#29992;&#20316;&#36335;&#24452;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#36335;&#24452;&#28857;&#25552;&#21462;&#65288;AWE&#65289;&#30340;&#27169;&#20223;&#23398;&#20064;&#39044;&#22788;&#29702;&#27169;&#22359;&#65292;&#23558;&#28436;&#31034;&#20998;&#35299;&#20026;&#19968;&#32452;&#26368;&#23567;&#30340;&#36335;&#24452;&#28857;&#65292;&#32447;&#24615;&#25554;&#20540;&#21487;&#20197;&#36817;&#20284;&#21040;&#25351;&#23450;&#30340;&#35823;&#24046;&#38408;&#20540;&#12290;AWE&#21487;&#20197;&#19982;&#20219;&#20309;BC&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#25105;&#20204;&#21457;&#29616;AWE&#33021;&#22815;&#25552;&#39640;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;&#36827;&#34892;&#35780;&#20272;&#30340;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22823;&#35268;&#27169;&#35843;&#26597;&#20102;&#35299;&#19981;&#21516;LLMs&#20013;&#30340;&#36947;&#24503;&#20449;&#24565;&#65292;&#22312;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20542;&#21521;&#20110;&#19982;&#20154;&#31867;&#30340;&#36947;&#24503;&#30452;&#35273;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#22238;&#31572;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14324</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Moral Beliefs Encoded in LLMs. (arXiv:2307.14324v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;&#36827;&#34892;&#35780;&#20272;&#30340;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22823;&#35268;&#27169;&#35843;&#26597;&#20102;&#35299;&#19981;&#21516;LLMs&#20013;&#30340;&#36947;&#24503;&#20449;&#24565;&#65292;&#22312;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20542;&#21521;&#20110;&#19982;&#20154;&#31867;&#30340;&#36947;&#24503;&#30452;&#35273;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#22238;&#31572;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#35774;&#35745;&#12289;&#31649;&#29702;&#12289;&#21518;&#22788;&#29702;&#21644;&#35780;&#20272;&#35843;&#26597;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;(1) &#19968;&#31181;&#29992;&#20110;&#33719;&#21462;LLMs&#20013;&#32534;&#30721;&#30340;&#20449;&#24565;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#32479;&#35745;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;LLM&#8220;&#20570;&#20986;&#36873;&#25321;&#8221;&#30340;&#27010;&#29575;&#12289;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#12290;(b) &#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#30740;&#31350;&#19981;&#21516;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#30830;&#36873;&#25321;&#19981;&#26126;&#26174;&#30340;&#27169;&#31946;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35843;&#26597;&#65292;&#20854;&#20013;&#21253;&#25324;680&#20010;&#39640;&#27169;&#31946;&#24230;&#30340;&#36947;&#24503;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#24212;&#35813;&#25746;&#19968;&#20010;&#21892;&#24847;&#30340;&#35854;&#35328;&#21527;&#65311;&#8221;&#65289;&#21644;687&#20010;&#20302;&#27169;&#31946;&#24230;&#30340;&#36947;&#24503;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#24212;&#35813;&#20026;&#36335;&#19978;&#30340;&#34892;&#20154;&#20572;&#19979;&#26469;&#21527;&#65311;&#8221;&#65289;&#12290;&#27599;&#20010;&#22330;&#26223;&#21253;&#25324;&#19968;&#20010;&#25551;&#36848;&#12289;&#20004;&#20010;&#21487;&#33021;&#30340;&#34892;&#21160;&#20197;&#21450;&#25351;&#31034;&#36829;&#21453;&#35268;&#21017;&#30340;&#36741;&#21161;&#26631;&#31614;&#65288;&#20363;&#22914;&#65292;&#8220;&#19981;&#35201;&#26432;&#20154;&#8221;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#35843;&#26597;&#24212;&#29992;&#20110;28&#20010;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;(b) &#22312;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs tend to align with human moral intuitions, but in ambiguous scenarios, their responses vary and may exhibit biases and inconsistencies.
&lt;/p&gt;
&lt;p&gt;
This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM "making a choice", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a white lie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for a pedestrian on the road?"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., "do not kill"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#23433;&#20840;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22312;&#27809;&#26377;&#22870;&#21169;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#23433;&#20840;&#25506;&#32034;&#65292;&#28982;&#21518;&#21033;&#29992;&#20854;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#19988;&#20511;&#37492;&#36801;&#31227;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#28040;&#38500;&#24341;&#23548;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.14316</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#23433;&#20840;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning by Guided Safe Exploration. (arXiv:2307.14316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#23433;&#20840;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22312;&#27809;&#26377;&#22870;&#21169;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#23433;&#20840;&#25506;&#32034;&#65292;&#28982;&#21518;&#21033;&#29992;&#20854;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#19988;&#20511;&#37492;&#36801;&#31227;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#28040;&#38500;&#24341;&#23548;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#23545;&#20110;&#25193;&#22823;&#24378;&#21270;&#23398;&#20064;(RL)&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#32463;&#24120;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#26234;&#33021;&#20307;&#65292;&#22914;&#23454;&#39564;&#23460;&#65292;&#28982;&#21518;&#20877;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#30446;&#26631;&#20219;&#21153;&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#12290;&#26080;&#22870;&#21169;RL&#22312;&#27809;&#26377;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#20197;&#20415;&#22312;&#22870;&#21169;&#25581;&#31034;&#21518;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26377;&#32422;&#26463;&#30340;&#26080;&#22870;&#21169;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#65288;&#23548;&#24341;&#32773;&#65289;&#22312;&#27809;&#26377;&#22870;&#21169;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#23433;&#20840;&#25506;&#32034;&#12290;&#35813;&#20195;&#29702;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#25509;&#21463;&#35757;&#32451;&#65292;&#21487;&#20197;&#36827;&#34892;&#19981;&#23433;&#20840;&#30340;&#20132;&#20114;&#20294;&#20173;&#25552;&#20379;&#23433;&#20840;&#20449;&#21495;&#12290;&#24403;&#30446;&#26631;&#20219;&#21153;&#34987;&#25581;&#31034;&#21518;&#65292;&#19981;&#20801;&#35768;&#36829;&#21453;&#23433;&#20840;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#23548;&#24341;&#32773;&#34987;&#21033;&#29992;&#26469;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#30340;&#34892;&#20026;&#31574;&#30053;&#12290;&#21463;&#21040;&#36801;&#31227;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36824;&#23558;&#30446;&#26631;&#31574;&#30053;&#65288;&#23398;&#29983;&#65289;&#21521;&#23548;&#24341;&#32773;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#23398;&#29983;&#22312;&#21487;&#38752;&#24615;&#19978;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#24182;&#36880;&#28176;&#28040;&#38500;&#23548;&#24341;&#32773;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24433;&#21709;&#12290;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32422;&#26463;&#25191;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#35843;&#24230;&#20013;&#30340;&#25805;&#20316;&#32422;&#26463;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#32422;&#26463;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2307.14304</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26368;&#20248;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#35843;&#24230;&#30340;&#32422;&#26463;&#25191;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch. (arXiv:2307.14304v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14304
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32422;&#26463;&#25191;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#35843;&#24230;&#20013;&#30340;&#25805;&#20316;&#32422;&#26463;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#32422;&#26463;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21160;&#24577;&#20215;&#26684;&#27874;&#21160;&#12289;&#38656;&#27714;&#28040;&#32791;&#21644;&#22522;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#33021;&#37327;&#29983;&#25104;&#24341;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;(ESSs)&#30340;&#26368;&#20248;&#35843;&#24230;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#36866;&#24212;&#37197;&#30005;&#32593;&#32476;&#38543;&#26426;&#24615;&#30340;&#39640;&#36136;&#37327;&#25511;&#21046;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;DRL&#31639;&#27861;&#32570;&#20047;&#20005;&#26684;&#25191;&#34892;&#25805;&#20316;&#32422;&#26463;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#24120;&#24120;&#25552;&#20379;&#19981;&#21487;&#34892;&#30340;&#25511;&#21046;&#21160;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;DRL&#26694;&#26550;&#65292;&#22312;&#22312;&#32447;&#25805;&#20316;&#26399;&#38388;&#26377;&#25928;&#22788;&#29702;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#65292;&#21516;&#26102;&#20005;&#26684;&#25191;&#34892;&#29615;&#22659;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25805;&#20316;&#32422;&#26463;&#12290;&#39318;&#20808;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#35757;&#32451;&#19968;&#20010;&#30001;DNNs&#27169;&#25311;&#30340;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;&#38543;&#21518;&#65292;&#35813;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#38382;&#39064;&#65292;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#25805;&#20316;&#32422;&#26463;&#30340;&#20005;&#26684;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal dispatch of energy storage systems (ESSs) presents formidable challenges due to the uncertainty introduced by fluctuations in dynamic prices, demand consumption, and renewable-based energy generation. By exploiting the generalization capabilities of deep neural networks (DNNs), deep reinforcement learning (DRL) algorithms can learn good-quality control models that adaptively respond to distribution networks' stochastic nature. However, current DRL algorithms lack the capabilities to enforce operational constraints strictly, often even providing unfeasible control actions. To address this issue, we propose a DRL framework that effectively handles continuous action spaces while strictly enforcing the environments and action space operational constraints during online operation. Firstly, the proposed framework trains an action-value function modeled using DNNs. Subsequently, this action-value function is formulated as a mixed-integer programming (MIP) formulation enabling the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#24212;&#29992;&#20110;&#37202;&#24215;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;ChatGPT&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#65292;&#32780;&#35828;&#26381;&#25216;&#26415;&#21487;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#24182;&#22686;&#24378;&#25512;&#33616;&#30340;&#35828;&#26381;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14298</link><description>&lt;p&gt;
ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#22312;&#37202;&#24215;&#26381;&#21153;&#39046;&#22495;&#20010;&#24615;&#21270;&#25512;&#33616;&#31649;&#29702;&#21644;&#25552;&#20379;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality. (arXiv:2307.14298v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#24212;&#29992;&#20110;&#37202;&#24215;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;ChatGPT&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#65292;&#32780;&#35828;&#26381;&#25216;&#26415;&#21487;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#24182;&#22686;&#24378;&#25512;&#33616;&#30340;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#37202;&#24215;&#26381;&#21153;&#19994;&#24050;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#20026;&#23458;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#23450;&#21046;&#21270;&#30340;&#20307;&#39564;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20026;&#25552;&#21319;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#26524;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#25972;&#21512;&#21040;&#37202;&#24215;&#26381;&#21153;&#25512;&#33616;&#31995;&#32479;&#20013;&#33258;&#21160;&#21270;&#21644;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;ChatGPT&#30340;&#33021;&#21147;&#65292;&#23427;&#21487;&#20197;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;ChatGPT&#25972;&#21512;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#33021;&#21147;&#65292;&#31361;&#20986;&#20102;&#20854;&#20998;&#26512;&#29992;&#25143;&#20559;&#22909;&#12289;&#20174;&#22312;&#32447;&#35780;&#35770;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#24182;&#26681;&#25454;&#23458;&#20154;&#37197;&#32622;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35828;&#26381;&#25216;&#26415;&#22312;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#21644;&#25552;&#21319;&#37202;&#24215;&#25512;&#33616;&#30340;&#35828;&#26381;&#25928;&#26524;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#27010;&#24565;&#25991;&#31456;&#30740;&#31350;&#20102;&#21508;&#31181;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#30005;&#26426;&#27979;&#35797;&#21488;&#21644;&#28082;&#20307;&#20013;&#30340;&#31890;&#23376;&#36319;&#36394;&#31561;&#23454;&#20363;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.14294</link><description>&lt;p&gt;
&#25581;&#31034;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65306;&#35299;&#20915;&#35270;&#39057;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis. (arXiv:2307.14294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27010;&#24565;&#25991;&#31456;&#30740;&#31350;&#20102;&#21508;&#31181;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#30005;&#26426;&#27979;&#35797;&#21488;&#21644;&#28082;&#20307;&#20013;&#30340;&#31890;&#23376;&#36319;&#36394;&#31561;&#23454;&#20363;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25968;&#25454;&#30340;&#25286;&#20998;&#65292;&#22914;&#35270;&#39057;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#26159;&#21508;&#31181;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#21253;&#25324;&#30446;&#26631;&#36319;&#36394;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#21518;&#32493;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#27010;&#24565;&#25991;&#31456;&#30740;&#31350;&#20102;&#19982;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#33719;&#21462;&#12289;&#25968;&#25454;&#34920;&#31034;&#12289;&#25286;&#20998;&#27604;&#20363;&#36873;&#25321;&#12289;&#24314;&#31435;&#36136;&#37327;&#26631;&#20934;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#36873;&#25321;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#30005;&#26426;&#27979;&#35797;&#21488;&#21644;&#28082;&#20307;&#20013;&#30340;&#31890;&#23376;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Splitting of sequential data, such as videos and time series, is an essential step in various data analysis tasks, including object tracking and anomaly detection. However, splitting sequential data presents a variety of challenges that can impact the accuracy and reliability of subsequent analyses. This concept article examines the challenges associated with splitting sequential data, including data acquisition, data representation, split ratio selection, setting up quality criteria, and choosing suitable selection strategies. We explore these challenges through two real-world examples: motor test benches and particle tracking in liquids.
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#30340;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14283</link><description>&lt;p&gt;
&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#65306;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#24320;&#25918;&#25361;&#25112;&#21644;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications. (arXiv:2307.14283v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#30340;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#37117;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#21644;&#26377;&#38480;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#22330;&#26223;&#38656;&#35201;&#26356;&#36890;&#29992;&#30340;AI&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#32780;&#19981;&#38656;&#35201;&#19987;&#38376;&#20026;&#23427;&#20204;&#35774;&#35745;&#12290;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#36825;&#20010;&#26415;&#35821;&#34987;&#23450;&#20041;&#20026;&#25351;&#20195;&#36825;&#20123;AI&#31995;&#32479;&#12290;&#23613;&#31649;&#36804;&#20170;&#20026;&#27490;&#65292;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#36275;&#22815;&#24378;&#22823;&#20197;&#27169;&#25311;&#20154;&#31867;&#24182;&#25913;&#36827;&#21508;&#31181;&#26234;&#21147;&#20219;&#21153;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#24895;&#26395;&#12289;&#34394;&#26500;&#30340;&#27010;&#24565;&#65292;&#24182;&#34987;&#35748;&#20026;&#23545;&#25105;&#20204;&#31038;&#20250;&#26500;&#25104;&#39118;&#38505;&#12290;&#34429;&#28982;&#25105;&#20204;&#31163;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#21487;&#33021;&#36824;&#24456;&#36965;&#36828;&#65292;&#20294;GPAIS&#26159;&#29616;&#23454;&#23384;&#22312;&#24182;&#20301;&#23621;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#21069;&#27839;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29616;&#26377;GPAIS&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#23553;&#38381;&#19990;&#30028;&#21644;&#24320;&#25918;&#19990;&#30028;&#30340;GPAIS&#65292;&#25551;&#36848;&#20854;&#33258;&#20027;&#31243;&#24230;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.  This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#29983;&#25104;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#21106;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.14273</link><description>&lt;p&gt;
&#20026;&#25552;&#39640;&#33041;&#32959;&#30244;&#20998;&#21106;&#32780;&#36827;&#34892;&#30340;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deepfake Image Generation for Improved Brain Tumor Segmentation. (arXiv:2307.14273v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#29983;&#25104;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#21106;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#22312;&#25216;&#26415;&#21644;&#20581;&#24247;&#26041;&#38754;&#30340;&#36827;&#27493;&#65292;&#36890;&#36807;&#25581;&#31034;&#26080;&#30151;&#29366;&#30340;&#36857;&#35937;&#26469;&#25552;&#39640;&#23545;&#30142;&#30149;&#30340;&#35748;&#35782;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21450;&#26089;&#26816;&#27979;&#21644;&#27835;&#30103;&#32959;&#30244;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#21361;&#21450;&#29983;&#21629;&#12290;&#35745;&#31639;&#26426;&#36741;&#21161;&#25216;&#26415;&#34987;&#29992;&#26469;&#20811;&#26381;&#30142;&#30149;&#35786;&#26029;&#20013;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#65292;&#32780;&#33041;&#32959;&#30244;&#20998;&#21106;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#22810;&#27169;&#24577;&#25968;&#25454;&#26102;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#25968;&#25454;&#21644;&#23545;&#24212;&#26631;&#27880;&#30340;&#32570;&#20047;&#23548;&#33268;&#30340;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#26377;&#25928;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#20197;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;U-Net&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#35757;&#32451;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#22235;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the world progresses in technology and health, awareness of disease by revealing asymptomatic signs improves. It is important to detect and treat tumors in early stage as it can be life-threatening. Computer-aided technologies are used to overcome lingering limitations facing disease diagnosis, while brain tumor segmentation remains a difficult process, especially when multi-modality data is involved. This is mainly attributed to ineffective training due to lack of data and corresponding labelling. This work investigates the feasibility of employing deep-fake image generation for effective brain tumor segmentation. To this end, a Generative Adversarial Network was used for image-to-image translation for increasing dataset size, followed by image segmentation using a U-Net-based convolutional neural network trained with deepfake images. Performance of the proposed approach is compared with ground truth of four publicly available datasets. Results show improved performance in terms of
&lt;/p&gt;</description></item><item><title>&#33639;&#20809;&#31070;&#32463;&#32454;&#32990;v2&#26159;&#19968;&#32452;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#21450;&#20854;&#30495;&#23454;&#27880;&#37322;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#23398;&#20064;&#20219;&#21153;&#30340;&#27880;&#37322;&#65292;&#24182;&#26377;&#21161;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.14243</link><description>&lt;p&gt;
&#33639;&#20809;&#31070;&#32463;&#32454;&#32990;v2&#65306;&#28145;&#24230;&#23398;&#20064;&#26174;&#24494;&#38236;&#20013;&#30340;&#22810;&#20219;&#21153;&#12289;&#22810;&#26684;&#24335;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy. (arXiv:2307.14243v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14243
&lt;/p&gt;
&lt;p&gt;
&#33639;&#20809;&#31070;&#32463;&#32454;&#32990;v2&#26159;&#19968;&#32452;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#21450;&#20854;&#30495;&#23454;&#27880;&#37322;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#23398;&#20064;&#20219;&#21153;&#30340;&#27880;&#37322;&#65292;&#24182;&#26377;&#21161;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33639;&#20809;&#31070;&#32463;&#32454;&#32990;v2&#26159;&#19968;&#32452;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#30495;&#23454;&#27880;&#37322;&#65292;&#26088;&#22312;&#20419;&#36827;&#29983;&#21629;&#31185;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#21019;&#26032;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#19977;&#20010;&#22270;&#20687;&#25910;&#38598;&#65292;&#20854;&#20013;&#21870;&#40831;&#31867;&#31070;&#32463;&#32454;&#32990;&#30340;&#32454;&#32990;&#26680;&#21644;&#32454;&#32990;&#36136;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#35760;&#26579;&#33394;&#20197;&#31361;&#20986;&#20854;&#35299;&#21078;&#25110;&#21151;&#33021;&#29305;&#24449;&#12290;&#38500;&#22270;&#20687;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22810;&#20010;&#23398;&#20064;&#20219;&#21153;&#30340;&#30495;&#23454;&#27880;&#37322;&#65292;&#21253;&#25324;&#35821;&#20041;&#20998;&#21106;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#12290;&#35813;&#30740;&#31350;&#25104;&#26524;&#26377;&#20004;&#20010;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#37492;&#20110;&#27880;&#37322;&#30340;&#22810;&#26679;&#24615;&#21644;&#20854;&#21487;&#35775;&#38382;&#30340;&#26684;&#24335;&#65292;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#26377;&#21161;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#22312;&#20998;&#21106;&#12289;&#26816;&#27979;&#12289;&#29305;&#24449;&#23398;&#20064;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#26041;&#27861;&#35770;&#36827;&#23637;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#20419;&#36827;&#24191;&#27867;&#25506;&#32034;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#24076;&#26395;&#33639;&#20809;&#31070;&#32463;&#32454;&#32990;v2&#33021;&#22815;&#20652;&#21270;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning. This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics. Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting. The contribution is two-fold. First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas. Second, by enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal Cells v2 will catalyze bre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#28436;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#25511;&#21046;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#27599;&#20010;&#26426;&#22120;&#20154;&#65292;&#24182;&#19988;&#21487;&#22312;&#20855;&#26377;&#26356;&#22810;&#26426;&#22120;&#20154;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25193;&#23637;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.14237</link><description>&lt;p&gt;
&#20026;&#26426;&#22120;&#20154;&#32676;&#20307;&#28436;&#21270;&#22810;&#30446;&#26631;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evolving Multi-Objective Neural Network Controllers for Robot Swarms. (arXiv:2307.14237v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#28436;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#25511;&#21046;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#27599;&#20010;&#26426;&#22120;&#20154;&#65292;&#24182;&#19988;&#21487;&#22312;&#20855;&#26377;&#26356;&#22810;&#26426;&#22120;&#20154;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25193;&#23637;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32676;&#20307;&#26426;&#22120;&#20154;&#20219;&#21153;&#30001;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#30446;&#26631;&#32452;&#25104;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#28436;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#25511;&#21046;&#22120;&#12290;&#32676;&#20307;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#22312;&#20302;&#20445;&#30495;&#30340;Python&#27169;&#25311;&#22120;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#39640;&#20445;&#30495;&#30340;Webots&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#36890;&#36807;&#36827;&#34892;&#27169;&#25311;&#23454;&#39564;&#65292;&#27979;&#35797;&#20102;&#28436;&#21270;&#20986;&#30340;&#22810;&#30446;&#26631;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#22312;&#20855;&#26377;&#26356;&#22810;&#26426;&#22120;&#20154;&#30340;&#29615;&#22659;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#27599;&#20010;&#26426;&#22120;&#20154;&#12290;&#26426;&#22120;&#20154;&#32676;&#20307;&#22312;&#35843;&#25972;&#27599;&#20010;&#30446;&#26631; &#30340;&#26435;&#37325;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#36824;&#35777;&#23454;&#65292;&#22312;&#20302;&#20445;&#30495;&#27169;&#25311;&#22120;&#20013;&#28436;&#21270;&#20986;&#30340;&#22810;&#30446;&#26631;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21487;&#20197;&#20256;&#36755;&#21040;&#39640;&#20445;&#30495;&#27169;&#25311;&#29615;&#22659;&#65292;&#24182;&#19988;&#25511;&#21046;&#22120;&#33021;&#22815;&#22312;&#20855;&#26377;&#26356;&#22810;&#26426;&#22120;&#20154;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25193;&#23637;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many swarm robotics tasks consist of multiple conflicting objectives. This research proposes a multi-objective evolutionary neural network approach to developing controllers for swarms of robots. The swarm robot controllers are trained in a low-fidelity Python simulator and then tested in a high-fidelity simulated environment using Webots. Simulations are then conducted to test the scalability of the evolved multi-objective robot controllers to environments with a larger number of robots. The results presented demonstrate that the proposed approach can effectively control each of the robots. The robot swarm exhibits different behaviours as the weighting for each objective is adjusted. The results also confirm that multi-objective neural network controllers evolved in a low-fidelity simulator can be transferred to high-fidelity simulated environments and that the controllers can scale to environments with a larger number of robots without further retraining needed.
&lt;/p&gt;</description></item><item><title>&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#19982;&#22522;&#20110;&#39033;&#30446;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#30456;&#24403;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#32431;&#22522;&#20110;&#35821;&#35328;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2307.14225</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20919;&#21551;&#21160;&#25512;&#33616;&#31995;&#32479;&#20013;&#19982;&#22522;&#20110;&#35821;&#35328;&#21644;&#22522;&#20110;&#39033;&#30446;&#20559;&#22909;&#31454;&#20105;&#21147;&#30456;&#24403;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences. (arXiv:2307.14225v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14225
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#19982;&#22522;&#20110;&#39033;&#30446;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#30456;&#24403;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#32431;&#22522;&#20110;&#35821;&#35328;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#29992;&#25143;&#30340;&#39033;&#30446;&#20559;&#22909;&#21382;&#21490;&#26469;&#25512;&#33616;&#29992;&#25143;&#21487;&#33021;&#21916;&#27426;&#30340;&#26032;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#23545;&#35805;&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#34920;&#36798;&#22522;&#20110;&#35821;&#35328;&#30340;&#20559;&#22909;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#20559;&#22909;&#36755;&#20837;&#26041;&#24335;&#12290;&#21463;&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#33539;&#24335;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22522;&#20110;&#39033;&#30446;&#21644;&#22522;&#20110;&#35821;&#35328;&#20559;&#22909;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#39033;&#30446;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#30456;&#27604;&#30340;&#25512;&#33616;&#24212;&#29992;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#29992;&#25143;&#37027;&#37324;&#24341;&#21457;&#20986;&#26469;&#30340;&#22522;&#20110;&#39033;&#30446;&#21644;&#22522;&#20110;&#35821;&#35328;&#20559;&#22909;&#65292;&#20197;&#21450;&#20182;&#20204;&#23545;&#21508;&#31181;&#65288;&#26377;&#20559;&#35265;&#30340;&#65289;&#25512;&#33616;&#39033;&#30446;&#21644;&#65288;&#26080;&#20559;&#35265;&#30340;&#65289;&#38543;&#26426;&#39033;&#30446;&#30340;&#35780;&#20998;&#12290;&#22312;&#20247;&#22810;&#23454;&#39564;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#32431;&#22522;&#20110;&#35821;&#35328;&#20559;&#22909;&#65288;&#27809;&#26377;&#39033;&#30446;&#20559;&#22909;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#22312;&#25509;&#36817;&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#19982;&#22522;&#20110;&#39033;&#30446;&#30340;CF&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional recommender systems leverage users' item preference history to recommend novel content that users may like. However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input. Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#24615;&#22320;&#20998;&#37197;&#36164;&#28304;&#65292;&#23454;&#29616;&#23545;&#30456;&#20851;&#36827;&#31243;&#30340;&#30417;&#25511;&#21644;&#21160;&#24577;&#25506;&#32034;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.14208</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#26465;&#20214;&#19979;&#22312;&#32447;&#24314;&#27169;&#21644;&#30417;&#25511;&#30456;&#20851;&#36827;&#31243;
&lt;/p&gt;
&lt;p&gt;
Online Modeling and Monitoring of Dependent Processes under Resource Constraints. (arXiv:2307.14208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#24615;&#22320;&#20998;&#37197;&#36164;&#28304;&#65292;&#23454;&#29616;&#23545;&#30456;&#20851;&#36827;&#31243;&#30340;&#30417;&#25511;&#21644;&#21160;&#24577;&#25506;&#32034;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#30417;&#25511;&#30456;&#20851;&#36827;&#31243;&#30340;&#32676;&#20307;&#23545;&#20110;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#24615;&#22320;&#20998;&#37197;&#36164;&#28304;&#65292;&#23454;&#29616;&#39640;&#39118;&#38505;&#36827;&#31243;&#30340;&#24320;&#21457;&#21033;&#29992;&#21644;&#30456;&#20851;&#21160;&#24577;&#30340;&#25506;&#32034;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring a population of dependent processes under limited resources is critical for abnormal events detection. A novel online collaborative learning method is proposed to adaptively allocate the resources for exploitation of high-risk processes and exploration of dependent dynamics. Efficiency of the proposed method is proved through theoretical analysis and experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24212;&#29992;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#23545;&#38156;&#21378;&#21387;&#21147;&#36807;&#28388;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#39044;&#27979;&#28388;&#39292;&#28287;&#24230;&#21644;&#25552;&#39640;&#38156;&#30340;&#22238;&#25910;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14199</link><description>&lt;p&gt;
&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#23545;&#21387;&#21147;&#36807;&#28388;&#24615;&#33021;&#36827;&#34892;&#35843;&#26597;&#30340;&#24212;&#29992;&#65292;&#19968;&#31181;&#38156;&#21378;&#28388;&#39292;&#24314;&#27169;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling. (arXiv:2307.14199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14199
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24212;&#29992;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#23545;&#38156;&#21378;&#21387;&#21147;&#36807;&#28388;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#39044;&#27979;&#28388;&#39292;&#28287;&#24230;&#21644;&#25552;&#39640;&#38156;&#30340;&#22238;&#25910;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38156;&#20918;&#37329;&#29983;&#20135;&#26041;&#27861;&#28041;&#21450;&#20174;&#30719;&#30707;&#20013;&#28024;&#20986;&#38156;&#65292;&#28982;&#21518;&#36890;&#36807;&#21387;&#21147;&#36807;&#28388;&#23558;&#22266;&#20307;&#27531;&#28195;&#19982;&#28082;&#20307;&#28342;&#28082;&#20998;&#31163;&#12290;&#36825;&#20010;&#20998;&#31163;&#36807;&#31243;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#22266;&#20307;&#27531;&#28195;&#20013;&#21547;&#26377;&#19968;&#23450;&#30340;&#28287;&#24230;&#65292;&#36825;&#20250;&#38477;&#20302;&#38156;&#30340;&#22238;&#25910;&#37327;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#23545;&#21387;&#21147;&#36807;&#28388;&#36807;&#31243;&#36827;&#34892;&#20102;&#24314;&#27169;&#12290;&#27169;&#22411;&#20197;&#23454;&#39564;&#23460;&#26679;&#21697;&#30340;&#36830;&#32493;&#21464;&#37327;&#65288;&#25552;&#21462;&#29305;&#24449;&#65289;&#20316;&#20026;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#36873;&#25321;&#20102;&#22238;&#24402;&#27169;&#22411;&#65292;&#21363;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#65288;RFR&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#12290;&#22312;&#20004;&#20010;&#26465;&#20214;&#19979;&#65288;&#32858;&#19993;&#28911;&#21644;&#28068;&#32438;&#32455;&#29289;&#65289;&#65292;&#24471;&#21040;&#20102;&#21387;&#21147;&#36807;&#28388;&#36807;&#31243;&#30340;&#23436;&#25972;&#25968;&#25454;&#38598;&#12290;&#29992;&#20110;&#39044;&#27979;&#28388;&#39292;&#28287;&#24230;&#30340;&#22240;&#32032;&#21253;&#25324;&#22266;&#20307;&#27987;&#24230;&#12289;&#28201;&#24230;&#12289;pH&#20540;&#12289;&#21387;&#21147;&#12289;&#28388;&#39292;&#21402;&#24230;&#12289;&#21561;&#27668;&#26102;&#38388;&#21644;&#36807;&#28388;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The hydrometallurgical method of zinc production involves leaching zinc from ore and then separating the solid residue from the liquid solution by pressure filtration. This separation process is very important since the solid residue contains some moisture that can reduce the amount of zinc recovered. This study modeled the pressure filtration process through Random Forest (RF) and Support Vector Machine (SVM). The models take continuous variables (extracted features) from the lab samples as inputs. Thus, regression models namely Random Forest Regression (RFR) and Support Vector Regression (SVR) were chosen. A total dataset was obtained during the pressure filtration process in two conditions: 1) Polypropylene (S1) and 2) Polyester fabrics (S2). To predict the cake moisture, solids concentration (0.2 and 0.38), temperature (35 and 65 centigrade), pH (2, 3.5, and 5), pressure, cake thickness (14, 20, 26, and 34 mm), air-blow time (2, 10 and 15 min) and filtration time were applied as in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#31163;&#25955;&#32452;&#20214;&#30340;&#38543;&#26426;&#35745;&#31639;&#22270;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31574;&#30053;:&#22686;&#21152;Gumbel&#22122;&#22768;&#25200;&#21160;&#30340;&#23610;&#24230;&#21442;&#25968;&#21644;&#20351;&#29992;&#22810;&#20010;&#31163;&#25955;&#32452;&#20214;&#12290;&#36825;&#20123;&#31574;&#30053;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14193</link><description>&lt;p&gt;
&#39640;&#25928;&#23398;&#20064;&#31163;&#25955;-&#36830;&#32493;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Discrete-Continuous Computation Graphs. (arXiv:2307.14193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#31163;&#25955;&#32452;&#20214;&#30340;&#38543;&#26426;&#35745;&#31639;&#22270;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31574;&#30053;:&#22686;&#21152;Gumbel&#22122;&#22768;&#25200;&#21160;&#30340;&#23610;&#24230;&#21442;&#25968;&#21644;&#20351;&#29992;&#22810;&#20010;&#31163;&#25955;&#32452;&#20214;&#12290;&#36825;&#20123;&#31574;&#30053;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#32452;&#21512;&#24212;&#29992;&#24191;&#27867;&#12290;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#31163;&#25955;-&#36830;&#32493;&#27169;&#22411;&#20855;&#26377;&#32452;&#21512;&#24615;&#12289;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26356;&#21487;&#35299;&#37322;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#35745;&#31639;&#22270;&#20013;&#27599;&#20010;&#25191;&#34892;&#36335;&#24452;&#19978;&#21482;&#26377;&#19968;&#20010;&#31163;&#25955;&#32452;&#20214;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20998;&#26512;&#20855;&#26377;&#22810;&#20010;&#36830;&#32493;&#31163;&#25955;&#32452;&#20214;&#30340;&#22797;&#26434;&#38543;&#26426;&#35745;&#31639;&#22270;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#26799;&#24230;&#36739;&#23567;&#21644;&#23616;&#37096;&#26368;&#23567;&#20540;&#38382;&#39064;&#65292;&#20248;&#21270;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26032;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;Gumbel&#22122;&#22768;&#25200;&#21160;&#30340;&#23610;&#24230;&#21442;&#25968;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Seco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#20808;&#21069;&#26367;&#20195;&#27169;&#22411;&#19982;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;LSTM&#21644;GRU&#65289;&#22312;&#24343;&#21513;&#23612;&#20122;&#35834;&#31119;&#20811;&#24066;&#34903;&#36947;&#27946;&#27700;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#25903;&#25345;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20256;&#36798;&#21644;&#26377;&#25928;&#38598;&#25104;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#27169;&#22411;&#26550;&#26500;&#21313;&#20998;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2307.14185</link><description>&lt;p&gt;
&#12298;&#24343;&#21513;&#23612;&#20122;&#35834;&#31119;&#20811;&#24066;&#34903;&#36947;&#27946;&#27700;&#30340;&#26426;&#22120;&#23398;&#20064;&#26367;&#20195;&#27169;&#22411;&#27604;&#36739;&#12299;
&lt;/p&gt;
&lt;p&gt;
A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia. (arXiv:2307.14185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#20808;&#21069;&#26367;&#20195;&#27169;&#22411;&#19982;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;LSTM&#21644;GRU&#65289;&#22312;&#24343;&#21513;&#23612;&#20122;&#35834;&#31119;&#20811;&#24066;&#34903;&#36947;&#27946;&#27700;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#25903;&#25345;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20256;&#36798;&#21644;&#26377;&#25928;&#38598;&#25104;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#27169;&#22411;&#26550;&#26500;&#21313;&#20998;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#27964;&#30340;&#27839;&#28023;&#22478;&#24066;&#65292;&#22914;&#24343;&#21513;&#23612;&#20122;&#35834;&#31119;&#20811;&#24066;&#65292;&#38754;&#20020;&#30528;&#30001;&#38477;&#38632;&#21644;&#28526;&#27728;&#24341;&#36215;&#30340;&#34903;&#36947;&#27946;&#27700;&#30340;&#25361;&#25112;&#65292;&#36825;&#32473;&#20132;&#36890;&#21644;&#19979;&#27700;&#36947;&#31995;&#32479;&#36896;&#25104;&#20102;&#21387;&#21147;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36130;&#20135;&#25439;&#22833;&#12290;&#34429;&#28982;&#39640;&#20445;&#30495;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#25311;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#22478;&#24066;&#26292;&#38632;&#27946;&#27700;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;2016&#24180;&#33267;2018&#24180;&#38388;&#24343;&#21513;&#23612;&#20122;&#35834;&#31119;&#20811;&#24066;&#26292;&#38632;&#20107;&#20214;&#30340;&#25968;&#25454;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#20808;&#21069;&#26367;&#20195;&#27169;&#22411;&#19982;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#38271;&#30701;&#26102;&#35760;&#24518;LSTM&#21644;&#38376;&#24335;&#24490;&#29615;&#21333;&#20803;GRU&#65289;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#20351;&#29992;&#25903;&#25345;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20256;&#36798;&#21644;&#26377;&#25928;&#38598;&#25104;&#30456;&#20851;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-lying coastal cities, exemplified by Norfolk, Virginia, face the challenge of street flooding caused by rainfall and tides, which strain transportation and sewer systems and can lead to property damage. While high-fidelity, physics-based simulations provide accurate predictions of urban pluvial flooding, their computational complexity renders them unsuitable for real-time applications. Using data from Norfolk rainfall events between 2016 and 2018, this study compares the performance of a previous surrogate model based on a random forest algorithm with two deep learning models: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). This investigation underscores the importance of using a model architecture that supports the communication of prediction uncertainty and the effective integration of relevant, multi-modal features.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26367;&#25442;&#26631;&#20934;&#30340;&#39640;&#26031;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#31867;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#21457;&#29616;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#24213;&#23618;&#32593;&#26684;&#32467;&#26500;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#35299;&#31163;&#34920;&#31034;&#20013;&#30340;&#26059;&#36716;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#21270;&#35299;&#31163;&#34920;&#31034;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.14151</link><description>&lt;p&gt;
&#23398;&#20064;&#35299;&#31163;&#30340;&#31163;&#25955;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Discrete Representations. (arXiv:2307.14151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14151
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26367;&#25442;&#26631;&#20934;&#30340;&#39640;&#26031;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#31867;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#21457;&#29616;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#24213;&#23618;&#32593;&#26684;&#32467;&#26500;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#35299;&#31163;&#34920;&#31034;&#20013;&#30340;&#26059;&#36716;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#21270;&#35299;&#31163;&#34920;&#31034;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#22522;&#20110;&#27169;&#22411;&#30340;&#22686;&#24378;&#23398;&#20064;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#20123;&#37117;&#35777;&#26126;&#20102;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#30340;&#32463;&#39564;&#20248;&#21183;&#65292;&#23613;&#31649;&#20854;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26631;&#20934;&#30340;&#39640;&#26031;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26367;&#25442;&#20026;&#23450;&#21046;&#30340;&#20998;&#31867;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#25506;&#32034;&#20102;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#21644;&#35299;&#31163;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#26174;&#31034;&#20998;&#31867;&#20998;&#24067;&#30340;&#24213;&#23618;&#32593;&#26684;&#32467;&#26500;&#20943;&#36731;&#20102;&#19982;&#22810;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;&#30456;&#20851;&#30340;&#26059;&#36716;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#20316;&#20026;&#35299;&#31163;&#34920;&#31034;&#30340;&#39640;&#25928;&#24402;&#32435;&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#31163;&#25955;VAE&#22312;&#23398;&#20064;&#35299;&#31163;&#34920;&#31034;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;&#35299;&#31163;&#34920;&#31034;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear. We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder. We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations. We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations. Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#25903;&#25345;&#38750;&#19987;&#23478;&#24037;&#31243;&#24072;&#24320;&#21457;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#36719;&#20214;&#24037;&#20855;&#31665;&#65292;&#20197;&#23454;&#29616;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#26377;&#25928;&#20195;&#29702;&#12290;&#26088;&#22312;&#21152;&#36895;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#27665;&#20027;&#21270;&#36827;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.14145</link><description>&lt;p&gt;
&#36890;&#36807;&#20961;&#20154;&#35774;&#35745;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Toward Design of Synthetic Active Inference Agents by Mere Mortals. (arXiv:2307.14145v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#25903;&#25345;&#38750;&#19987;&#23478;&#24037;&#31243;&#24072;&#24320;&#21457;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#36719;&#20214;&#24037;&#20855;&#31665;&#65292;&#20197;&#23454;&#29616;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#26377;&#25928;&#20195;&#29702;&#12290;&#26088;&#22312;&#21152;&#36895;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#27665;&#20027;&#21270;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#29702;&#35770;&#29305;&#24615;&#26159;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#65292;&#20294;&#26159;&#25105;&#20204;&#22914;&#20309;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#26377;&#25928;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#20195;&#29702;&#21602;&#65311;&#36825;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#31574;&#30053;&#25506;&#32034;&#30340;&#35745;&#31639;&#36127;&#33655;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#36793;&#32536;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20010;&#25903;&#25345;&#38750;&#19987;&#23478;&#24037;&#31243;&#24072;&#24320;&#21457;&#26377;&#25928;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#36719;&#20214;&#24037;&#20855;&#31665;&#25152;&#24517;&#38656;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#27491;&#22312;&#24320;&#21457;&#20013;&#30340;&#24037;&#20855;&#31665;&#65292;&#26088;&#22312;&#21152;&#36895;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#27665;&#20027;&#21270;&#36827;&#31243;&#65292;&#23601;&#20687;TensorFlow&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#19968;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical properties of active inference agents are impressive, but how do we realize effective agents in working hardware and software on edge devices? This is an interesting problem because the computational load for policy exploration explodes exponentially, while the computational resources are very limited for edge devices. In this paper, we discuss the necessary features for a software toolbox that supports a competent non-expert engineer to develop working active inference agents. We introduce a toolbox-in-progress that aims to accelerate the democratization of active inference agents in a similar way as TensorFlow propelled applications of deep learning technology.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#22870;&#21169;&#30340;&#20998;&#27573;&#31283;&#23450;&#32452;&#21512;&#21322;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19978;&#30028;&#32622;&#20449;&#24230;&#31639;&#27861;&#20197;&#24212;&#23545;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#32452;&#37325;&#21551;&#30340;&#27010;&#24565;&#20316;&#20026;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#22791;&#20221;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.14138</link><description>&lt;p&gt;
&#20998;&#27573;&#31283;&#23450;&#32452;&#21512;&#21322;&#24378;&#30423;&#38382;&#39064;&#21450;&#22240;&#26524;&#20851;&#31995;&#22870;&#21169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.14138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#22870;&#21169;&#30340;&#20998;&#27573;&#31283;&#23450;&#32452;&#21512;&#21322;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19978;&#30028;&#32622;&#20449;&#24230;&#31639;&#27861;&#20197;&#24212;&#23545;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#32452;&#37325;&#21551;&#30340;&#27010;&#24565;&#20316;&#20026;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#22791;&#20221;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#22870;&#21169;&#30340;&#20998;&#27573;&#31283;&#23450;&#32452;&#21512;&#21322;&#24378;&#30423;&#38382;&#39064;&#12290;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#65292;&#22522;&#26412;&#33218;&#30340;&#20998;&#24067;&#21464;&#21270;&#12289;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25110;&#32773;&#20004;&#32773;&#21516;&#26102;&#25913;&#21464;&#22870;&#21169;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#30340;&#20915;&#31574;&#32773;&#24517;&#39035;&#36319;&#38543;&#36825;&#20004;&#20010;&#21464;&#21270;&#28304;&#65292;&#24182;&#30456;&#24212;&#22320;&#36827;&#34892;&#36866;&#24212;&#12290;&#22312;&#32452;&#21512;&#21322;&#24378;&#30423;&#35774;&#32622;&#20013;&#65292;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#20915;&#31574;&#32773;&#21482;&#35266;&#23519;&#21040;&#25152;&#36873;&#33218;&#32452;&#21512;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#26680;&#24515;&#26159;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;Upper Confidence Bound, UCB&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#20195;&#29702;&#20381;&#38752;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#21464;&#28857;&#26816;&#27979;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#37325;&#21551;&#30340;&#27010;&#24565;&#20316;&#20026;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26032;&#22411;&#22791;&#20221;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25972;&#21512;&#20102;&#19968;&#20010;&#36319;&#36394;&#26426;&#21046;&#20197;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards. In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process. In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly. The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms. The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We assume the agent relies on an adaptive approach to overcome the challenge. More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio (GLR) test. Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments. Finally, our algorithm integrates a mechanism to trace the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#23567;&#22411;&#21040;&#20013;&#22411;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#65292;&#22312;&#22635;&#34917;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#26041;&#38754;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#25104;&#26524;&#65292;&#24182;&#20026;&#24320;&#21457;&#21644;&#24212;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.14134</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#35780;&#20272;&#23567;&#22411;&#21040;&#20013;&#22411;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models. (arXiv:2307.14134v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#23567;&#22411;&#21040;&#20013;&#22411;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#65292;&#22312;&#22635;&#34917;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#26041;&#38754;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#25104;&#26524;&#65292;&#24182;&#20026;&#24320;&#21457;&#21644;&#24212;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#23567;&#22411;&#12289;&#36855;&#20320;&#22411;&#12289;&#23567;&#22411;&#21644;&#20013;&#22411;&#30340;&#26080;&#22823;&#23567;&#20889;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#65292;&#26088;&#22312;&#22635;&#34917;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;75GB&#20197;&#19978;&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#25513;&#30721;&#39044;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#26032;&#38395;&#20998;&#31867;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26356;&#24555;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#22303;&#32819;&#20854;&#35821;&#22659;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces and evaluates tiny, mini, small, and medium-sized uncased Turkish BERT models, aiming to bridge the research gap in less-resourced languages. We trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and, zero-shot classification. Despite their smaller size, our models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times. Our findings provide valuable insights into the development and application of smaller language models, especially in the context of the Turkish language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GraphRNN&#36827;&#34892;&#20102;&#22797;&#29616;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;You&#31561;&#20154;&#24314;&#35758;&#30340;BFS&#36941;&#21382;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;BFS&#36941;&#21382;&#26367;&#25442;&#20026;&#25299;&#25169;&#25490;&#24207;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;GraphRNN&#20197;&#29983;&#25104;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.14109</link><description>&lt;p&gt;
GraphRNN&#20877;&#25506;&#65306;&#28040;&#34701;&#30740;&#31350;&#21644;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs. (arXiv:2307.14109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GraphRNN&#36827;&#34892;&#20102;&#22797;&#29616;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;You&#31561;&#20154;&#24314;&#35758;&#30340;BFS&#36941;&#21382;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;BFS&#36941;&#21382;&#26367;&#25442;&#20026;&#25299;&#25169;&#25490;&#24207;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;GraphRNN&#20197;&#29983;&#25104;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GraphRNN&#26159;&#30001;You&#31561;&#20154;&#25552;&#20986;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#37325;&#26032;&#23454;&#29616;&#30340;GraphRNN&#26550;&#26500;&#22797;&#29616;&#20102;You&#31561;&#20154;&#30340;&#32467;&#26524;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#25351;&#26631;&#23558;&#20854;&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;You&#31561;&#20154;&#24314;&#35758;&#30340;BFS&#36941;&#21382;&#20197;&#21512;&#24182;&#21516;&#26500;&#22270;&#30340;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26367;&#25442;BFS&#36941;&#21382;&#20026;&#25299;&#25169;&#25490;&#24207;&#65292;&#23558;GraphRNN&#25193;&#23637;&#20026;&#29983;&#25104;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;GraphRNN&#30340;&#26377;&#21521;&#22810;&#31867;&#21035;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
GraphRNN is a deep learning-based architecture proposed by You et al. for learning generative models for graphs. We replicate the results of You et al. using a reproduced implementation of the GraphRNN architecture and evaluate this against baseline models using new metrics. Through an ablation study, we find that the BFS traversal suggested by You et al. to collapse representations of isomorphic graphs contributes significantly to model performance. Additionally, we extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. We demonstrate that this method improves significantly over a directed-multiclass variant of GraphRNN on a real-world dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37327;&#21270;&#26031;&#22374;&#20811;&#20271;&#26684;&#22343;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30465;&#26679;&#26412;&#37327;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25512;&#26029;&#36861;&#38543;&#32773;&#30340;&#34892;&#21160;&#26469;&#23398;&#20064;&#37327;&#21270;&#21709;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14085</link><description>&lt;p&gt;
&#34892;&#21160;&#32988;&#20110;&#35328;&#36766;&#65306;&#35777;&#26126;&#20102;&#20174;&#31574;&#30053;&#21453;&#39304;&#20013;&#30465;&#26679;&#26412;&#37327;&#30340;&#37327;&#21270;&#26031;&#22374;&#20811;&#20271;&#26684;&#22343;&#34913;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks. (arXiv:2307.14085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37327;&#21270;&#26031;&#22374;&#20811;&#20271;&#26684;&#22343;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30465;&#26679;&#26412;&#37327;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25512;&#26029;&#36861;&#38543;&#32773;&#30340;&#34892;&#21160;&#26469;&#23398;&#20064;&#37327;&#21270;&#21709;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20855;&#26377;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#32467;&#26500;&#30340;&#24773;&#22659;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#23398;&#20064;&#37327;&#21270;&#26031;&#22374;&#20811;&#20271;&#26684;&#22343;&#34913;&#65288;QSE&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#22312;&#28216;&#25103;&#24320;&#22987;&#26102;&#65292;&#39046;&#23548;&#32773;&#23459;&#24067;&#22905;&#30340;&#31574;&#30053;&#24182;&#25215;&#35834;&#25191;&#34892;&#12290;&#36861;&#38543;&#32773;&#35266;&#23519;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#65292;&#28982;&#21518;&#37319;&#21462;&#37327;&#21270;&#21709;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#35299;&#20915;&#30001;&#39046;&#23548;&#32773;&#31574;&#30053;&#24341;&#21457;&#30340;&#29109;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#26469;&#30830;&#23450;&#12290;&#39046;&#23548;&#32773;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19982;&#36861;&#38543;&#32773;&#30340;&#20132;&#20114;&#24182;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#25214;&#21040;&#33258;&#24049;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#20248;&#30340;&#39044;&#26399;&#24635;&#22238;&#25253;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#39046;&#23548;&#32773;&#26080;&#27861;&#35266;&#23519;&#21040;&#36861;&#38543;&#32773;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#36861;&#38543;&#32773;&#23545;&#25239;&#39046;&#23548;&#32773;&#31574;&#30053;&#30340;&#34892;&#21160;&#20013;&#25512;&#26029;&#20986;&#36861;&#38543;&#32773;&#30340;&#37327;&#21270;&#21709;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20989;&#25968;&#36924;&#36817;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#35774;&#32622;&#30340;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#65288;i&#65289;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#23398;&#20064;&#37327;&#21270;&#21709;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We study reinforcement learning (RL) for learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure. In specific, at the outset of the game, the leader announces her policy to the follower and commits to it. The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy. The goal of the leader is to find her optimal policy, which yields the optimal expected total return, by interacting with the follower and learning from data. A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies. We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation. Our algorithms are based on (i) learning the quantal response model via maximum likelihood 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D3AAMDA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#29305;&#24449;&#20998;&#24067;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#28304;&#22495;&#20869;&#30340;&#23616;&#37096;&#26377;&#21033;&#29305;&#24449;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14068</link><description>&lt;p&gt;
&#21160;&#24577;&#22495;&#24046;&#24322;&#35843;&#25972;&#29992;&#20110;&#20027;&#21160;&#22810;&#39046;&#22495;&#36866;&#24212;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation. (arXiv:2307.14068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14068
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D3AAMDA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#29305;&#24449;&#20998;&#24067;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#28304;&#22495;&#20869;&#30340;&#23616;&#37096;&#26377;&#21033;&#29305;&#24449;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#65288;MUDA&#65289;&#26088;&#22312;&#23558;&#30456;&#20851;&#28304;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#33267;&#26080;&#26631;&#31614;&#30446;&#26631;&#22495;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;MUDA&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#23545;&#40784;&#28304;&#22495;&#20043;&#38388;&#30340;&#25972;&#20307;&#29305;&#24449;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#27599;&#20010;&#22495;&#20869;&#20887;&#20313;&#29305;&#24449;&#32780;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;MUDA&#26041;&#27861;&#19982;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22495;&#24046;&#24322;&#35843;&#25972;&#29992;&#20110;&#20027;&#21160;&#22810;&#39046;&#22495;&#36866;&#24212;&#65288;D3AAMDA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#28304;&#21160;&#24577;&#35843;&#21046;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22522;&#20110;&#28304;&#22495;&#19982;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#31243;&#24230;&#12290;&#35813;&#26426;&#21046;&#25511;&#21046;&#27599;&#20010;&#28304;&#22495;&#19982;&#30446;&#26631;&#22495;&#20043;&#38388;&#29305;&#24449;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#28304;&#22495;&#20869;&#30340;&#23616;&#37096;&#26377;&#21033;&#29305;&#24449;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Multi-source unsupervised domain adaptation (MUDA) aims to transfer knowledge from related source domains to an unlabeled target domain. While recent MUDA methods have shown promising results, most focus on aligning the overall feature distributions across source domains, which can lead to negative effects due to redundant features within each domain. Moreover, there is a significant performance gap between MUDA and supervised methods. To address these challenges, we propose a novel approach called Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation (D3AAMDA). Firstly, we establish a multi-source dynamic modulation mechanism during the training process based on the degree of distribution differences between source and target domains. This mechanism controls the alignment level of features between each source domain and the target domain, effectively leveraging the local advantageous feature information within the source domains. Additionally, we propose a Multi-sou
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20998;&#25955;&#30340;&#20449;&#24687;&#21644;&#32570;&#20047;&#26131;&#20110;&#29702;&#35299;&#30340;&#25991;&#26723;&#65292;&#20351;&#24471;&#20854;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.14067</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#65306;&#29616;&#29366;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions. (arXiv:2307.14067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14067
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20998;&#25955;&#30340;&#20449;&#24687;&#21644;&#32570;&#20047;&#26131;&#20110;&#29702;&#35299;&#30340;&#25991;&#26723;&#65292;&#20351;&#24471;&#20854;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#22788;&#29702;&#33021;&#21147;&#21644;&#23545;&#26131;&#34987;&#24573;&#35270;&#30340;&#38544;&#34255;&#27169;&#24335;&#30340;&#26816;&#27979;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23545;&#20170;&#22825;&#30340;&#21307;&#30103;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#21457;&#29616;&#20102;&#35768;&#22810;ML&#24212;&#29992;&#31243;&#24207;&#65292;&#36824;&#26377;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20173;&#22312;&#30740;&#31350;&#20013;&#65292;&#20294;&#30446;&#21069;&#30340;&#21307;&#30103;&#31995;&#32479;&#21482;&#26377;&#23569;&#25968;&#24212;&#29992;&#20102;&#36825;&#20123;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#22312;&#21307;&#30103;&#31995;&#32479;&#20013;&#23384;&#22312;&#30528;&#24040;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#26426;&#20250;&#65292;&#20294;&#20998;&#25955;&#30340;&#20449;&#24687;&#12289;&#32570;&#20047;&#36866;&#24403;&#23433;&#25490;&#21644;&#26131;&#20110;&#35299;&#37322;&#30340;&#30456;&#20851;&#25991;&#20214;&#26159;&#20351;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38590;&#20197;&#20351;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31616;&#27905;&#12289;&#26356;&#26377;&#25928;&#22320;&#25910;&#38598;&#19981;&#21516;&#39046;&#22495;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#20197;&#20415;&#31435;&#21363;&#35775;&#38382;&#30456;&#20851;&#21442;&#32771;&#36164;&#26009;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#20998;&#20026;&#20116;&#22823;&#32452;&#65306;&#31038;&#21306;&#32423;&#24037;&#20316;&#12289;&#39118;&#38505;&#31649;&#29702;/&#39044;&#38450;&#20445;&#20581;&#12289;&#21307;&#30103;&#36816;&#33829;&#31649;&#29702;&#12289;&#36828;&#31243;&#25252;&#29702;&#21644;&#26089;&#26399;&#26816;&#27979;&#12290;&#23558;&#36825;&#20123;&#32452;&#20998;&#20026;&#23376;&#32452;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system. Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems. As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals. This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references. We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection. Dividing these groups into subgroups, we provided relevant re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26631;&#31614;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#65292;&#31454;&#20105;&#21147;&#24378;&#12290;</title><link>http://arxiv.org/abs/2307.14066</link><description>&lt;p&gt;
&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#30340;&#25193;&#25955;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-Training with Diffusion models for Dental Radiography segmentation. (arXiv:2307.14066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26631;&#31614;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#65292;&#31454;&#20105;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25918;&#23556;&#23398;&#20998;&#21106;&#65292;&#23588;&#20854;&#26159;&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#65292;&#21463;&#21040;&#26631;&#35760;&#25104;&#26412;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#38656;&#35201;&#20855;&#26377;&#29305;&#23450;&#19987;&#19994;&#30693;&#35782;&#21644;&#32321;&#37325;&#30340;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#26631;&#31614;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#39044;&#35757;&#32451;&#21644;&#21518;&#32493;&#20219;&#21153;&#20043;&#38388;&#19981;&#38656;&#35201;&#26550;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#25552;&#35758;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;DDPM&#35757;&#32451;&#30446;&#26631;&#23545;Unet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20998;&#21106;&#20219;&#21153;&#19978;&#23545;&#24471;&#21040;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23545;&#29273;&#31185;&#25918;&#23556;&#29255;&#30340;&#20998;&#21106;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical radiography segmentation, and specifically dental radiography, is highly limited by the cost of labeling which requires specific expertise and labor-intensive annotations. In this work, we propose a straightforward pre-training method for semantic segmentation leveraging Denoising Diffusion Probabilistic Models (DDPM), which have shown impressive results for generative modeling. Our straightforward approach achieves remarkable performance in terms of label efficiency and does not require architectural modifications between pre-training and downstream tasks. We propose to first pre-train a Unet by exploiting the DDPM training objective, and then fine-tune the resulting model on a segmentation task. Our experimental results on the segmentation of dental radiographs demonstrate that the proposed method is competitive with state-of-the-art pre-training methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#27491;&#21017;&#21270;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32597;&#35265;&#36139;&#34880;&#30142;&#30149;&#30340;&#32418;&#32454;&#32990;&#20998;&#31867;&#12290;&#36890;&#36807;&#20174;&#21333;&#20010;&#32418;&#32454;&#32990;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#23610;&#24230;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#36827;&#34892;&#27169;&#22411;&#27491;&#21017;&#21270;&#65292;&#20197;&#20445;&#25345;&#25968;&#25454;&#30340;&#29305;&#24449;&#25299;&#25169;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.14025</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#27491;&#21017;&#21270;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#29992;&#20110;&#32418;&#32454;&#32990;&#30142;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification. (arXiv:2307.14025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#27491;&#21017;&#21270;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32597;&#35265;&#36139;&#34880;&#30142;&#30149;&#30340;&#32418;&#32454;&#32990;&#20998;&#31867;&#12290;&#36890;&#36807;&#20174;&#21333;&#20010;&#32418;&#32454;&#32990;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#23610;&#24230;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#36827;&#34892;&#27169;&#22411;&#27491;&#21017;&#21270;&#65292;&#20197;&#20445;&#25345;&#25968;&#25454;&#30340;&#29305;&#24449;&#25299;&#25169;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26174;&#24494;&#22270;&#20687;&#35786;&#26029;&#32597;&#35265;&#30340;&#36139;&#34880;&#30142;&#30149;&#23545;&#20110;&#29087;&#32451;&#30340;&#19987;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#22312;&#21333;&#20010;&#34880;&#26679;&#20013;&#26377;&#25968;&#21315;&#20010;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#32454;&#32990;&#65292;&#36825;&#26500;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#38382;&#39064;&#12290;&#34429;&#28982;&#32418;&#32454;&#32990;&#30340;&#31354;&#38388;&#37051;&#22495;&#26412;&#36523;&#24182;&#19981;&#37325;&#35201;&#65292;&#20294;&#25972;&#20010;&#34880;&#26679;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#21363;&#25968;&#25454;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#21253;&#21547;&#20102;&#26377;&#30410;&#30340;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#20856;&#22411;&#30340;MIL&#38382;&#39064;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#22312;&#26377;&#38480;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#30340;&#26041;&#27861;&#65292;&#20174;&#21333;&#20010;&#32418;&#32454;&#32990;&#22270;&#20687;&#30340;&#21253;&#20013;&#25552;&#21462;&#22810;&#23610;&#24230;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;&#36825;&#20123;&#25299;&#25169;&#29305;&#24449;&#34987;&#29992;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#24378;&#21046;&#20445;&#25345;&#25968;&#25454;&#30340;&#29305;&#24449;&#25299;&#25169;&#23646;&#24615;&#12290;&#22312;&#21253;&#21547;71&#20010;&#32597;&#35265;&#36139;&#34880;&#30142;&#30149;&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;521&#24352;&#32418;&#32454;&#32990;&#26174;&#24494;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#25299;&#25169;&#27491;&#21017;&#21270;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28548;&#28165;softmax&#20989;&#25968;&#19982;Boltzmann&#31639;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#20855;&#26377;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#33258;&#27880;&#24847;&#21147;&#20855;&#22791;&#23436;&#20840;&#25429;&#33719;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#21333;&#23618;Transformer&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;Transformer&#26159;&#32039;&#20945;&#22495;&#19978;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.14023</link><description>&lt;p&gt;
&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#20351;&#29992;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#26159;&#21542;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?. (arXiv:2307.14023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14023
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28548;&#28165;softmax&#20989;&#25968;&#19982;Boltzmann&#31639;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#20855;&#26377;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#33258;&#27880;&#24847;&#21147;&#20855;&#22791;&#23436;&#20840;&#25429;&#33719;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#21333;&#23618;Transformer&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;Transformer&#26159;&#32039;&#20945;&#22495;&#19978;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;Transformer&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#30340;&#20998;&#26512;&#35201;&#27714;&#36807;&#28145;&#30340;&#23618;&#25968;&#26469;&#23454;&#29616;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#23548;&#33268;&#19982;&#23454;&#38469;&#20351;&#29992;&#30340;Transformer&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#23558;softmax&#20989;&#25968;&#35299;&#37322;&#20026;hardmax&#20989;&#25968;&#30340;&#36924;&#36817;&#12290;&#36890;&#36807;&#28548;&#28165;softmax&#20989;&#25968;&#19982;Boltzmann&#31639;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#20855;&#26377;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#33258;&#27880;&#24847;&#21147;&#20855;&#22791;&#23436;&#20840;&#25429;&#33719;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21333;&#23618;Transformer&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#24182;&#19988;&#30001;&#20004;&#20010;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26500;&#25104;&#30340;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;Transformer&#26159;&#32039;&#20945;&#22495;&#19978;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21508;&#31181;MCMC&#26041;&#27861;&#32467;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#32452;&#21512;&#21644;&#36827;&#34892;&#26356;&#22909;&#30340;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2307.14012</link><description>&lt;p&gt;
MCMC-&#20462;&#27491;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#27169;&#22411;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
MCMC-Correction of Score-Based Diffusion Models for Model Composition. (arXiv:2307.14012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21508;&#31181;MCMC&#26041;&#27861;&#32467;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#32452;&#21512;&#21644;&#36827;&#34892;&#26356;&#22909;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29992;&#24471;&#20998;&#25110;&#33021;&#37327;&#20989;&#25968;&#26469;&#21442;&#25968;&#21270;&#12290;&#33021;&#37327;&#21442;&#25968;&#21270;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#20027;&#35201;&#26159;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#25552;&#35758;&#26679;&#26412;&#20013;&#24635;&#33021;&#37327;&#30340;&#21464;&#21270;&#22522;&#20110;Metropolis-Hastings&#20462;&#27491;&#27493;&#39588;&#26469;&#36827;&#34892;&#25193;&#23637;&#37319;&#26679;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23427;&#20284;&#20046;&#20135;&#29983;&#20102;&#31245;&#24494;&#36739;&#24046;&#30340;&#24615;&#33021;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#30001;&#20110;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26222;&#36941;&#27969;&#34892;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#33021;&#37327;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#21066;&#24369;&#20102;&#27169;&#22411;&#32452;&#21512;&#30340;&#30446;&#30340;&#65292;&#21363;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#32452;&#21512;&#36215;&#26469;&#20174;&#26032;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#24314;&#35758;&#20445;&#30041;&#24471;&#20998;&#21442;&#25968;&#21270;&#65292;&#32780;&#26159;&#36890;&#36807;&#23545;&#24471;&#20998;&#20989;&#25968;&#36827;&#34892;&#32447;&#31215;&#20998;&#26469;&#35745;&#31639;&#22522;&#20110;&#33021;&#37327;&#30340;&#25509;&#21463;&#27010;&#29575;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#21508;&#31181;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models can be parameterised in terms of either a score or an energy function. The energy parameterisation has better theoretical properties, mainly that it enables an extended sampling procedure with a Metropolis--Hastings correction step, based on the change in total energy in the proposed samples. However, it seems to yield slightly worse performance, and more importantly, due to the widespread popularity of score-based diffusion, there are limited availability of off-the-shelf pre-trained energy-based ones. This limitation undermines the purpose of model composition, which aims to combine pre-trained models to sample from new distributions. Our proposal, however, suggests retaining the score parameterization and instead computing the energy-based acceptance probability through line integration of the score function. This allows us to re-use existing diffusion models and still combine the reverse process with various Markov-Chain Monte Carlo (MCMC) methods. We evaluate our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#31639;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#38408;&#20540;&#36882;&#20943;&#31639;&#27861;&#26469;&#26368;&#22823;&#21270;&#28385;&#36275;&#25311;&#38453;&#32422;&#26463;&#30340;k-submodular&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#22312;&#38477;&#20302;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#65292;&#36817;&#20284;&#27604;&#20363;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#12290;&#21478;&#22806;&#65292;&#20316;&#32773;&#36824;&#32473;&#20986;&#20102;&#23545;&#20110;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#24773;&#20917;&#30340;&#19981;&#21516;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#23558;&#24635;&#22823;&#23567;&#32422;&#26463;&#19979;&#30340;&#38382;&#39064;&#20316;&#20026;&#25512;&#35770;&#32473;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.13996</link><description>&lt;p&gt;
&#24555;&#36895;&#31639;&#27861;&#29992;&#20110;&#28385;&#36275;&#19968;&#20010;&#25311;&#38453;&#32422;&#26463;&#30340;k-submodular&#26368;&#22823;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fast algorithms for k-submodular maximization subject to a matroid constraint. (arXiv:2307.13996v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#31639;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#38408;&#20540;&#36882;&#20943;&#31639;&#27861;&#26469;&#26368;&#22823;&#21270;&#28385;&#36275;&#25311;&#38453;&#32422;&#26463;&#30340;k-submodular&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#22312;&#38477;&#20302;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#65292;&#36817;&#20284;&#27604;&#20363;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#12290;&#21478;&#22806;&#65292;&#20316;&#32773;&#36824;&#32473;&#20986;&#20102;&#23545;&#20110;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#24773;&#20917;&#30340;&#19981;&#21516;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#23558;&#24635;&#22823;&#23567;&#32422;&#26463;&#19979;&#30340;&#38382;&#39064;&#20316;&#20026;&#25512;&#35770;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#20010;&#38408;&#20540;&#36882;&#20943;&#31639;&#27861;&#26469;&#26368;&#22823;&#21270;&#28385;&#36275;&#25311;&#38453;&#32422;&#26463;&#30340;k-submodular&#20989;&#25968;&#65292;&#35813;&#31639;&#27861;&#30456;&#36739;&#20110;&#36138;&#23146;&#31639;&#27861;&#20943;&#23569;&#20102;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#22312;&#36817;&#20284;&#27604;&#20363;&#19978;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#21322;&#36817;&#20284;&#31639;&#27861;$(\frac{1}{2} - \epsilon)$&#26469;&#26368;&#22823;&#21270;&#21333;&#35843;&#30340;k-submodular&#20989;&#25968;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;&#21333;&#35843;&#24773;&#20917;&#19979;&#30340;$(\frac{1}{3} - \epsilon)$&#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;$O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$&#65292;&#20854;&#20013;$r$&#34920;&#31034;&#25311;&#38453;&#30340;&#31209;&#65292;$IO, EO$&#20998;&#21035;&#34920;&#31034;&#21028;&#26029;&#19968;&#20010;&#23376;&#38598;&#26159;&#21542;&#20026;&#29420;&#31435;&#38598;&#21644;&#35745;&#31639;&#20989;&#25968;$f$&#20540;&#30340;oracle&#35843;&#29992;&#25968;&#12290;&#30001;&#20110;&#24635;&#22823;&#23567;&#30340;&#32422;&#26463;&#21487;&#20197;&#30475;&#20316;&#19968;&#31181;&#29305;&#27530;&#30340;&#25311;&#38453;&#65292;&#31216;&#20026;&#22343;&#21248;&#25311;&#38453;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#24555;&#36895;&#31639;&#27861;&#20316;&#20026;&#25512;&#35770;&#32473;&#20986;&#65292;&#29992;&#20110;&#28385;&#36275;&#24635;&#22823;&#23567;&#32422;&#26463;&#19979;&#30340;k-submodular&#20989;&#25968;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we apply a Threshold-Decreasing Algorithm to maximize $k$-submodular functions under a matroid constraint, which reduces the query complexity of the algorithm compared to the greedy algorithm with little loss in approximation ratio. We give a $(\frac{1}{2} - \epsilon)$-approximation algorithm for monotone $k$-submodular function maximization, and a $(\frac{1}{3} - \epsilon)$-approximation algorithm for non-monotone case, with complexity $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$, where $r$ denotes the rank of the matroid, and $IO, EO$ denote the number of oracles to evaluate whether a subset is an independent set and to compute the function value of $f$, respectively. Since the constraint of total size can be looked as a special matroid, called uniform matroid, then we present the fast algorithm for maximizing $k$-submodular functions subject to a total size constraint as corollaries. corollaries.
&lt;/p&gt;</description></item><item><title>&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#25968;&#25454;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#20840;&#23616;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#36890;&#29992;&#29305;&#24449;&#21253;&#21547;&#35768;&#22810;&#23545;&#26576;&#20010;&#29305;&#23450;&#23458;&#25143;&#31471;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#30340;&#20302;&#32500;&#24230;&#29305;&#28857;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.13995</link><description>&lt;p&gt;
&#38543;&#20320;&#36873;&#25321;: &#22312;&#20302;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space. (arXiv:2307.13995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13995
&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#25968;&#25454;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#20840;&#23616;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#36890;&#29992;&#29305;&#24449;&#21253;&#21547;&#35768;&#22810;&#23545;&#26576;&#20010;&#29305;&#23450;&#23458;&#25143;&#31471;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#30340;&#20302;&#32500;&#24230;&#29305;&#28857;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#25968;&#25454;&#23646;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#25317;&#26377;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;PFL&#20013;&#19968;&#20010;&#20856;&#22411;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#30001;&#25152;&#26377;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#20840;&#23616;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#36890;&#29992;&#29305;&#24449;&#65292;&#20197;&#21450;&#20351;&#29992;&#23458;&#25143;&#31471;&#26412;&#22320;&#25968;&#25454;&#35757;&#32451;&#30340;&#20010;&#24615;&#21270;&#23618;&#65288;&#22914;&#20998;&#31867;&#22120;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#65288;&#20063;&#31216;&#20026;&#39046;&#22495;&#24046;&#24322;&#65289;&#65292;&#20840;&#23616;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#36890;&#29992;&#29305;&#24449;&#20027;&#35201;&#21253;&#25324;&#35768;&#22810;&#19982;&#26576;&#20010;&#29305;&#23450;&#23458;&#25143;&#31471;&#26412;&#22320;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#20214;&#12290;&#26368;&#36817;&#19968;&#20123;PFL&#26041;&#27861;&#36890;&#36807;&#20010;&#24615;&#21270;&#32534;&#30721;&#22120;&#20013;&#30340;&#29305;&#23450;&#21442;&#25968;&#26469;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#31354;&#38388;&#39640;&#32500;&#24230;&#21644;&#38750;&#32447;&#24615;&#25152;&#24102;&#26469;&#30340;&#24456;&#22823;&#25361;&#25112;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29305;&#24449;&#31354;&#38388;&#20855;&#26377;&#36739;&#20302;&#30340;&#32500;&#24230;&#65292;&#25552;&#20379;&#26356;&#30452;&#35266;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning (PFL) is a popular framework that allows clients to have different models to address application scenarios where clients' data are in different domains. The typical model of a client in PFL features a global encoder trained by all clients to extract universal features from the raw data and personalized layers (e.g., a classifier) trained using the client's local data. Nonetheless, due to the differences between the data distributions of different clients (aka, domain gaps), the universal features produced by the global encoder largely encompass numerous components irrelevant to a certain client's local task. Some recent PFL methods address the above problem by personalizing specific parameters within the encoder. However, these methods encounter substantial challenges attributed to the high dimensionality and non-linearity of neural network parameter space. In contrast, the feature space exhibits a lower dimensionality, providing greater intuitiveness an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#20083;&#29275;&#22312;&#36127;&#24615;&#24773;&#32490;&#19979;&#30340;&#22768;&#38899;&#29305;&#24449;&#65292;&#20026;&#24320;&#21457;&#38750;&#20405;&#20837;&#24615;&#21160;&#29289;&#24773;&#32490;&#29366;&#24577;&#25351;&#26631;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.13994</link><description>&lt;p&gt;
BovineTalk: &#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20083;&#29275;&#36127;&#24615;&#24773;&#32490;&#19979;&#30340;&#22768;&#38899;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States. (arXiv:2307.13994v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13994
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#20083;&#29275;&#22312;&#36127;&#24615;&#24773;&#32490;&#19979;&#30340;&#22768;&#38899;&#29305;&#24449;&#65292;&#20026;&#24320;&#21457;&#38750;&#20405;&#20837;&#24615;&#21160;&#29289;&#24773;&#32490;&#29366;&#24577;&#25351;&#26631;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30044;&#29287;&#21160;&#29289;&#20013;&#65292;&#24320;&#21457;&#21644;&#39564;&#35777;&#38750;&#20405;&#20837;&#24615;&#22522;&#20110;&#21160;&#29289;&#30340;&#24773;&#32490;&#29366;&#24577;&#25351;&#26631;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20197;&#20415;&#23558;&#20854;&#25972;&#21512;&#21040;&#20892;&#22330;&#35780;&#20272;&#21327;&#35758;&#20013;&#65292;&#21487;&#33021;&#36890;&#36807;&#31934;&#23494;&#30044;&#29287;&#19994;&#24037;&#20855;&#26469;&#23454;&#29616;&#12290;&#20854;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22768;&#38899;&#25351;&#26631;&#12290;&#22312;&#37325;&#35201;&#30340;&#30044;&#29287;&#29289;&#31181;&#20013;&#65292;&#22914;&#29482;&#12289;&#39532;&#12289;&#23478;&#31165;&#21644;&#23665;&#32650;&#20013;&#65292;&#22768;&#38899;&#30340;&#22768;&#23398;&#32467;&#26500;&#21644;&#21151;&#33021;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#28982;&#32780;&#22312;&#20083;&#29275;&#20013;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29275;&#34987;&#21457;&#29616;&#20250;&#21457;&#20986;&#20004;&#31181;&#31867;&#22411;&#30340;&#21483;&#22768;&#65306;&#20302;&#39057;&#21483;&#22768;(LF)&#65292;&#29992;&#20110;&#36817;&#36317;&#31163;&#25509;&#35302;&#26102;&#65292;&#22068;&#38381;&#21512;&#25110;&#37096;&#20998;&#38381;&#21512;&#65307;&#39640;&#39057;&#21483;&#22768;(HF)&#65292;&#29992;&#20110;&#36828;&#36317;&#31163;&#20256;&#25773;&#26102;&#65292;&#22068;&#24352;&#24320;&#65292;&#21518;&#32773;&#24448;&#24448;&#19982;&#36127;&#24615;&#24773;&#32490;&#29366;&#24577;&#26377;&#20851;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#20083;&#29275;&#21483;&#22768;&#20013;&#21253;&#21547;&#20102;&#20010;&#20307;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#30340;&#20010;&#20307;&#29305;&#28857;&#65292;&#21253;&#25324;&#36127;&#24615;&#21644;
&lt;/p&gt;
&lt;p&gt;
There is a critical need to develop and validate non-invasive animal-based indicators of affective states in livestock species, in order to integrate them into on-farm assessment protocols, potentially via the use of precision livestock farming (PLF) tools. One such promising approach is the use of vocal indicators. The acoustic structure of vocalizations and their functions were extensively studied in important livestock species, such as pigs, horses, poultry and goats, yet cattle remain understudied in this context to date. Cows were shown to produce two types vocalizations: low-frequency calls (LF), produced with the mouth closed, or partially closed, for close distance contacts and open mouth emitted high-frequency calls (HF), produced for long distance communication, with the latter considered to be largely associated with negative affective states. Moreover, cattle vocalizations were shown to contain information on individuality across a wide range of contexts, both negative and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;METAVerse&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#24418;&#21487;&#34892;&#24615;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;LiDAR&#28857;&#20113;&#29983;&#25104;&#23494;&#38598;&#36830;&#32493;&#20540;&#25104;&#26412;&#22270;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23567;&#22320;&#24418;&#21487;&#34892;&#24615;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13991</link><description>&lt;p&gt;
METAVerse&#65306;&#29992;&#20110;&#36234;&#37326;&#23548;&#33322;&#30340;&#20803;&#23398;&#20064;&#21487;&#34892;&#24615;&#25104;&#26412;&#22270;
&lt;/p&gt;
&lt;p&gt;
METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation. (arXiv:2307.13991v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;METAVerse&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#24418;&#21487;&#34892;&#24615;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;LiDAR&#28857;&#20113;&#29983;&#25104;&#23494;&#38598;&#36830;&#32493;&#20540;&#25104;&#26412;&#22270;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23567;&#22320;&#24418;&#21487;&#34892;&#24615;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#37326;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#23548;&#33322;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#22320;&#24418;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#65292;&#22320;&#24418;&#21487;&#34892;&#24615;&#20272;&#35745;&#21463;&#21040;&#22810;&#20010;&#24433;&#21709;&#36710;&#36742;&#19982;&#22320;&#24418;&#30456;&#20114;&#20316;&#29992;&#30340;&#22240;&#32032;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#33719;&#21462;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#29615;&#22659;&#20013;&#21487;&#34892;&#24615;&#30340;&#21487;&#25512;&#24191;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;METAVerse&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#24418;&#21487;&#34892;&#24615;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#21033;&#29992;&#36710;&#36742;&#19982;&#22320;&#24418;&#30456;&#20114;&#20316;&#29992;&#21453;&#39304;&#26469;&#35757;&#32451;&#21487;&#34892;&#24615;&#39044;&#27979;&#32593;&#32476;&#65292;&#20174;&#31232;&#30095;&#30340;LiDAR&#28857;&#20113;&#29983;&#25104;&#23494;&#38598;&#36830;&#32493;&#20540;&#25104;&#26412;&#22270;&#12290;&#21033;&#29992;&#20803;&#23398;&#20064;&#20174;&#22810;&#20010;&#29615;&#22659;&#25910;&#38598;&#30340;&#39550;&#39542;&#25968;&#25454;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#20943;&#23567;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36827;&#34892;&#22312;&#32447;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#36827;&#34892;&#21542;&#23450;&#24863;&#30693;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;NegBLEURT&#35780;&#20272;&#25351;&#26631;&#20197;&#25552;&#39640;&#23545;&#21542;&#23450;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21542;&#23450;&#21477;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25351;&#26631;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#25200;&#21160;&#19978;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13989</link><description>&lt;p&gt;
&#36825;&#26159;&#19981;&#27491;&#30830;&#30340;&#65281;&#23545;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#36827;&#34892;&#21542;&#23450;&#24863;&#30693;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
This is not correct! Negation-aware Evaluation of Language Generation Systems. (arXiv:2307.13989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#36827;&#34892;&#21542;&#23450;&#24863;&#30693;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;NegBLEURT&#35780;&#20272;&#25351;&#26631;&#20197;&#25552;&#39640;&#23545;&#21542;&#23450;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21542;&#23450;&#21477;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25351;&#26631;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#25200;&#21160;&#19978;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20272;&#20102;&#21542;&#23450;&#23545;&#21477;&#23376;&#21547;&#20041;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23398;&#20064;&#35780;&#20272;&#25351;&#26631;&#23545;&#21542;&#23450;&#19981;&#25935;&#24863;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NegBLEURT&#65292;&#19968;&#31181;&#38024;&#23545;&#21542;&#23450;&#24863;&#30693;&#30340;BLEURT&#35780;&#20272;&#25351;&#26631;&#30340;&#25913;&#36827;&#29256;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21477;&#23376;&#21542;&#23450;&#24037;&#20855;&#65292;&#24182;&#29992;&#23427;&#26469;&#21019;&#24314;&#20102;CANNOT&#21542;&#23450;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#21477;&#23376;&#36716;&#25442;&#22120;&#21644;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#23545;&#21542;&#23450;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#21542;&#23450;&#21477;&#19978;&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23427;&#20204;&#22522;&#30784;&#27169;&#22411;&#22312;&#20854;&#20182;&#25200;&#21160;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations. In this paper, we propose NegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that, we designed a rule-based sentence negation tool and used it to create the CANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a sentence transformer and an evaluation metric to improve their negation sensitivity. Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models' performances on other perturbations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#19982;&#28508;&#22312;&#31354;&#38388; GAN &#38598;&#25104;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#30340;&#22522;&#30784;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.13978</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046; GAN &#30340;&#28508;&#22312;&#31354;&#38388;&#65306;&#22522;&#20110;&#20219;&#21153;&#30340;&#22270;&#20687;&#32763;&#35793;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation. (arXiv:2307.13978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#19982;&#28508;&#22312;&#31354;&#38388; GAN &#38598;&#25104;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#30340;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#36924;&#30495;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046; GAN &#29983;&#25104;&#36807;&#31243;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#19982;&#28508;&#22312;&#31354;&#38388; GAN&#65288;l-GAN&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#25104;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24102;&#26377;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#31574;&#30053;&#30340; actor-critic RL &#20195;&#29702;&#65292;&#20351;&#20854;&#33021;&#22815;&#29087;&#32451;&#22320;&#22312; l-GAN &#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#24182;&#26681;&#25454;&#25351;&#23450;&#30340;&#20219;&#21153;&#29983;&#25104;&#36755;&#20986;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20351;&#29992; MNIST &#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#31639;&#26415;&#21152;&#27861;&#20316;&#20026;&#19968;&#20010;&#35828;&#26126;&#24615;&#20219;&#21153;&#12290;&#36825;&#20123;&#23454;&#39564;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#23558; RL &#20195;&#29702;&#19982; GAN &#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to generate realistic outputs based on training datasets. However, the challenge of exerting control over the generation process of GANs remains a significant hurdle. In this paper, we propose a novel methodology to address this issue by integrating a reinforcement learning (RL) agent with a latent-space GAN (l-GAN), thereby facilitating the generation of desired outputs. More specifically, we have developed an actor-critic RL agent with a meticulously designed reward policy, enabling it to acquire proficiency in navigating the latent space of the l-GAN and generating outputs based on specified tasks. To substantiate the efficacy of our approach, we have conducted a series of experiments employing the MNIST dataset, including arithmetic addition as an illustrative task. The outcomes of these experiments serve to validate our methodology. Our pioneering integration of an RL agent with a GAN model represents a nov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#26469;&#30740;&#31350;&#20854;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#31243;&#24230;&#19982;&#32593;&#32476;&#35757;&#32451;&#24615;&#33021;&#26377;&#21516;&#27493;&#24615;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#28608;&#27963;&#20989;&#25968;&#21644;&#32593;&#32476;&#23610;&#23544;&#23545;&#38544;&#34255;&#23618;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13962</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#34255;&#23618;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Neural Networks via Linear Separability of Hidden Layers. (arXiv:2307.13962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#26469;&#30740;&#31350;&#20854;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#31243;&#24230;&#19982;&#32593;&#32476;&#35757;&#32451;&#24615;&#33021;&#26377;&#21516;&#27493;&#24615;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#28608;&#27963;&#20989;&#25968;&#21644;&#32593;&#32476;&#23610;&#23544;&#23545;&#38544;&#34255;&#23618;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#26469;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#38389;&#21487;&#22827;&#26031;&#22522;&#24046;&#24322;&#30340;&#32447;&#24615;&#21487;&#20998;&#24230;&#37327;&#65288;MD-LSMs&#65289;&#26469;&#35780;&#20272;&#20004;&#20010;&#28857;&#38598;&#30340;&#32447;&#24615;&#21487;&#20998;&#31243;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#31243;&#24230;&#19982;&#32593;&#32476;&#35757;&#32451;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#21516;&#27493;&#24615;&#65292;&#21363;&#22914;&#26524;&#26356;&#26032;&#30340;&#26435;&#37325;&#33021;&#22815;&#25552;&#39640;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#31243;&#24230;&#65292;&#37027;&#20040;&#26356;&#26032;&#21518;&#30340;&#32593;&#32476;&#23558;&#23454;&#29616;&#26356;&#22909;&#30340;&#35757;&#32451;&#24615;&#33021;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#32593;&#32476;&#23610;&#23544;&#65288;&#21253;&#25324;&#23485;&#24230;&#21644;&#28145;&#24230;&#65289;&#23545;&#38544;&#34255;&#23618;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#19968;&#20123;&#27969;&#34892;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#28145;&#24230;&#32622;&#20449;&#32593;&#32476;&#65288;DBN&#65289;&#12289;ResNet&#12289;VGGNet&#12289;AlexNet&#12289;vision tran&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision tran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#31070;&#32463;&#20272;&#35745;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#19979;&#30028;&#26469;&#36817;&#20284;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#29109;&#12290;&#36890;&#36807;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23376;&#38598;&#25277;&#26679;&#31574;&#30053;&#23545;&#27604;&#25968;&#25454;&#38598;&#19981;&#21516;&#35270;&#22270;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#30446;&#26631;&#20248;&#21270;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.13944</link><description>&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#29109;&#31070;&#32463;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Entropy Neural Estimation for Graph Contrastive Learning. (arXiv:2307.13944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#31070;&#32463;&#20272;&#35745;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#19979;&#30028;&#26469;&#36817;&#20284;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#29109;&#12290;&#36890;&#36807;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23376;&#38598;&#25277;&#26679;&#31574;&#30053;&#23545;&#27604;&#25968;&#25454;&#38598;&#19981;&#21516;&#35270;&#22270;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#30446;&#26631;&#20248;&#21270;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#25552;&#21462;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#30340;&#39640;&#23618;&#34920;&#31034;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25968;&#25454;&#38598;&#30340;&#29109;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#19981;&#21516;&#35270;&#22270;&#19979;&#30340;&#20114;&#20449;&#24687;&#19979;&#30028;&#26469;&#36817;&#20284;&#20272;&#35745;&#65292;&#21363;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#29109;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23376;&#38598;&#25277;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#23545;&#27604;&#25968;&#25454;&#38598;&#21508;&#35270;&#22270;&#20043;&#38388;&#30340;&#25104;&#23545;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38543;&#26426;&#20174;&#32473;&#23450;&#30340;&#22270;&#20013;&#25277;&#26679;&#33410;&#28857;&#21644;&#36793;&#26469;&#26500;&#24314;&#35270;&#22270;&#30340;&#36755;&#20837;&#23376;&#38598;&#12290;&#20004;&#20010;&#35270;&#22270;&#34987;&#36755;&#20837;&#21040;&#21442;&#25968;&#20849;&#20139;&#30340;&#36830;&#20307;&#32593;&#32476;&#20013;&#65292;&#20197;&#25552;&#21462;&#39640;&#32500;&#23884;&#20837;&#24182;&#20272;&#35745;&#25972;&#20010;&#22270;&#30340;&#20449;&#24687;&#29109;&#12290;&#23545;&#20110;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#30446;&#26631;&#20248;&#21270;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#36755;&#20837;&#30001;&#27491;&#36127;&#23545;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#23545;&#27604;&#23545;&#31574;&#30053;&#19982;&#20197;&#21069;&#30340;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36234;&#30028;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#32422;&#26463;&#20248;&#21270;&#36807;&#31243;&#65292;&#23454;&#29616;&#39640;&#27867;&#21270;&#32622;&#20449;&#24230;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36234;&#30028;&#27867;&#21270;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13943</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#36234;&#30028;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Topology-aware Robust Optimization for Out-of-distribution Generalization. (arXiv:2307.13943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36234;&#30028;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#32422;&#26463;&#20248;&#21270;&#36807;&#31243;&#65292;&#23454;&#29616;&#39640;&#27867;&#21270;&#32622;&#20449;&#24230;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36234;&#30028;&#27867;&#21270;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#30028;&#27867;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#27867;&#21270;&#32622;&#20449;&#24230;&#20302;&#30340;&#24773;&#20917;&#19979;&#21463;&#21040;&#36807;&#20110;&#24754;&#35266;&#30340;&#24314;&#27169;&#12290;&#30001;&#20110;&#19981;&#21487;&#33021;&#23545;&#20219;&#24847;&#27979;&#35797;&#20998;&#24067;&#36827;&#34892;&#27867;&#21270;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#24320;&#21457;&#24378;&#22823;&#30340;&#36234;&#30028;&#27867;&#21270;&#33021;&#21147;&#26102;&#65292;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#30340;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#65288;TRO&#65289;&#65292;&#23558;&#20998;&#24067;&#30340;&#25299;&#25169;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#19968;&#20010;&#21512;&#29702;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TRO&#35299;&#20915;&#20102;&#20004;&#20010;&#20248;&#21270;&#30446;&#26631;&#65306;&#65288;1&#65289;&#25299;&#25169;&#23398;&#20064;&#65292;&#25506;&#32034;&#25968;&#25454;&#27969;&#24418;&#20197;&#25581;&#31034;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#65307;&#65288;2&#65289;&#25299;&#25169;&#23398;&#20064;&#65292;&#21033;&#29992;&#25299;&#25169;&#32467;&#26500;&#23545;&#40065;&#26834;&#20248;&#21270;&#36827;&#34892;&#32422;&#26463;&#65292;&#20197;&#23454;&#29616;&#32039;&#23494;&#36793;&#30028;&#30340;&#27867;&#21270;&#39118;&#38505;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#32463;&#39564;&#35777;&#26126;&#23427;&#22312;&#24191;&#27867;&#33539;&#22260;&#20869;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide rang
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#21452;&#32423;&#23402;&#29983;&#32467;&#26500;&#32593;&#32476;&#65288;DSSN&#65289;&#65292;&#36890;&#36807;&#20687;&#32032;&#32423;&#23545;&#27604;&#23398;&#20064;&#26469;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#31867;&#21035;&#24863;&#30693;&#20266;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13938</link><description>&lt;p&gt;
&#25552;&#39640;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#25928;&#26524;&#30340;&#21452;&#32423;&#23402;&#29983;&#32467;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network. (arXiv:2307.13938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#21452;&#32423;&#23402;&#29983;&#32467;&#26500;&#32593;&#32476;&#65288;DSSN&#65289;&#65292;&#36890;&#36807;&#20687;&#32032;&#32423;&#23545;&#27604;&#23398;&#20064;&#26469;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#31867;&#21035;&#24863;&#30693;&#20266;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20943;&#23569;&#26631;&#35760;&#35757;&#32451;&#26679;&#20363;&#30340;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;SSS&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#28508;&#21147;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20687;&#32032;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#21452;&#32423;&#23402;&#29983;&#32467;&#26500;&#32593;&#32476;&#65288;DSSN&#65289;&#12290;&#36890;&#36807;&#22312;&#20302;&#32423;&#22270;&#20687;&#31354;&#38388;&#21644;&#39640;&#32423;&#29305;&#24449;&#31354;&#38388;&#20013;&#20351;&#29992;&#24378;&#22686;&#24378;&#35270;&#22270;&#30340;&#20687;&#32032;&#32423;&#23545;&#27604;&#25439;&#22833;&#26469;&#23545;&#40784;&#27491;&#23545;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;DSSN&#26088;&#22312;&#26368;&#22823;&#21270;&#21487;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#21035;&#24863;&#30693;&#20266;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#65292;&#29992;&#20110;&#20174;&#24369;&#21040;&#24378;&#30340;&#30417;&#30563;&#65292;&#35299;&#20915;&#20102;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#19981;&#25191;&#34892;&#36873;&#25321;&#25110;&#20026;&#25152;&#26377;&#31867;&#21035;&#24212;&#29992;&#39044;&#23450;&#20041;&#38408;&#20540;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#36873;&#25321;&#20102;&#27599;&#20010;&#31867;&#21035;&#24369;&#35270;&#22270;&#30340;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised semantic segmentation (SSS) is an important task that utilizes both labeled and unlabeled data to reduce expenses on labeling training examples. However, the effectiveness of SSS algorithms is limited by the difficulty of fully exploiting the potential of unlabeled data. To address this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise contrastive learning. By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space, the proposed DSSN is designed to maximize the utilization of available unlabeled data. Additionally, we introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, which addresses the limitations of most existing methods that do not perform selection or apply a predefined threshold for all classes. Specifically, our strategy selects the top high-confidence prediction of the weak view for each class to generate ps
&lt;/p&gt;</description></item><item><title>trajdata&#26159;&#19968;&#20010;&#32479;&#19968;&#25509;&#21475;&#65292;&#25552;&#20379;&#31616;&#21333;&#12289;&#32479;&#19968;&#21644;&#39640;&#25928;&#30340;&#36712;&#36857;&#21644;&#22320;&#22270;&#25968;&#25454;&#30340;&#34920;&#31034;&#21644;API&#65292;&#22312;&#36712;&#36857;&#39044;&#27979;&#39046;&#22495;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;&#20154;&#31867;&#21644;AV&#36816;&#21160;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#28145;&#20837;&#29702;&#35299;&#21644;&#26410;&#26469;&#25968;&#25454;&#38598;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.13924</link><description>&lt;p&gt;
trajdata&#65306;&#22810;&#20010;&#20154;&#31867;&#36712;&#36857;&#25968;&#25454;&#38598;&#30340;&#32479;&#19968;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
trajdata: A Unified Interface to Multiple Human Trajectory Datasets. (arXiv:2307.13924v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13924
&lt;/p&gt;
&lt;p&gt;
trajdata&#26159;&#19968;&#20010;&#32479;&#19968;&#25509;&#21475;&#65292;&#25552;&#20379;&#31616;&#21333;&#12289;&#32479;&#19968;&#21644;&#39640;&#25928;&#30340;&#36712;&#36857;&#21644;&#22320;&#22270;&#25968;&#25454;&#30340;&#34920;&#31034;&#21644;API&#65292;&#22312;&#36712;&#36857;&#39044;&#27979;&#39046;&#22495;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;&#20154;&#31867;&#21644;AV&#36816;&#21160;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#28145;&#20837;&#29702;&#35299;&#21644;&#26410;&#26469;&#25968;&#25454;&#38598;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21644;&#34892;&#20154;&#36816;&#21160;&#36319;&#36394;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20154;&#31867;&#36712;&#36857;&#25968;&#25454;&#38598;&#30340;&#21457;&#24067;&#65292;&#36712;&#36857;&#39044;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#31038;&#21306;&#26469;&#35828;&#26159;&#19968;&#20010;&#31119;&#38899;&#65292;&#20294;&#23427;&#20204;&#37117;&#20351;&#29992;&#33258;&#23450;&#20041;&#21644;&#29420;&#29305;&#30340;&#25968;&#25454;&#26684;&#24335;&#21644;API&#65292;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#38590;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;trajdata&#65306;&#22810;&#20010;&#20154;&#31867;&#36712;&#36857;&#25968;&#25454;&#38598;&#30340;&#32479;&#19968;&#25509;&#21475;&#12290;trajdata&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#32479;&#19968;&#21644;&#39640;&#25928;&#30340;&#36712;&#36857;&#21644;&#22320;&#22270;&#25968;&#25454;&#30340;&#34920;&#31034;&#21644;API&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#29616;&#26377;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;trajdata&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#25143;&#23545;&#24403;&#21069;&#34892;&#20154;&#21644;AV&#36816;&#21160;&#39044;&#27979;&#30740;&#31350;&#30340;&#25968;&#25454;&#26377;&#20102;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#25968;&#25454;&#38598;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24515;&#34880;&#31649;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#20316;&#20026;&#32479;&#35745;&#25512;&#29702;&#36827;&#34892;&#35299;&#20915;&#65292;&#22312;&#20307;&#22806;&#36827;&#34892;&#20102;&#20116;&#20010;&#29983;&#29289;&#26631;&#35760;&#29289;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#27169;&#25311;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13918</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#29992;&#20110;&#24515;&#34880;&#31649;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simulation-based Inference for Cardiovascular Models. (arXiv:2307.13918v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24515;&#34880;&#31649;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#20316;&#20026;&#32479;&#35745;&#25512;&#29702;&#36827;&#34892;&#35299;&#20915;&#65292;&#22312;&#20307;&#22806;&#36827;&#34892;&#20102;&#20116;&#20010;&#29983;&#29289;&#26631;&#35760;&#29289;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#27169;&#25311;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#34880;&#27969;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;&#19981;&#26029;&#21457;&#23637;&#65292;&#24050;&#25104;&#20026;&#30740;&#31350;&#20307;&#22806;&#24515;&#34880;&#31649;&#31995;&#32479;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#24037;&#20855;&#36890;&#24120;&#29992;&#20110;&#20174;&#29983;&#29702;&#21442;&#25968;&#27169;&#25311;&#20840;&#36523;&#34880;&#27969;&#21160;&#21147;&#23398;&#65292;&#20294;&#35299;&#20915;&#23558;&#27874;&#24418;&#26144;&#23556;&#22238;&#21512;&#29702;&#30340;&#29983;&#29702;&#21442;&#25968;&#30340;&#36870;&#38382;&#39064;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#21463;&#27169;&#25311;&#25512;&#29702;&#65288;SBI&#65289;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36870;&#38382;&#39064;&#20316;&#20026;&#32479;&#35745;&#25512;&#29702;&#26469;&#22788;&#29702;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;SBI&#20026;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#25552;&#20379;&#20102;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20010;&#20307;&#27979;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#22810;&#32500;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20960;&#31181;&#27979;&#37327;&#27169;&#24577;&#26469;&#23637;&#31034;&#36825;&#31181;&#33021;&#21147;&#65292;&#36827;&#34892;&#20102;&#20116;&#20010;&#20020;&#24202;&#24863;&#20852;&#36259;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#20307;&#22806;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#12290;&#38500;&#20102;&#30830;&#35748;&#24050;&#30693;&#20107;&#23454;&#65292;&#27604;&#22914;&#20272;&#35745;&#24515;&#29575;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
Over the past decades, hemodynamics simulators have steadily evolved and have become tools of choice for studying cardiovascular systems in-silico. While such tools are routinely used to simulate whole-body hemodynamics from physiological parameters, solving the corresponding inverse problem of mapping waveforms back to plausible physiological parameters remains both promising and challenging. Motivated by advances in simulation-based inference (SBI), we cast this inverse problem as statistical inference. In contrast to alternative approaches, SBI provides \textit{posterior distributions} for the parameters of interest, providing a \textit{multi-dimensional} representation of uncertainty for \textit{individual} measurements. We showcase this ability by performing an in-silico uncertainty analysis of five biomarkers of clinical interest comparing several measurement modalities. Beyond the corroboration of known facts, such as the feasibility of estimating heart rate, our study highlight
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Bayesian causal discovery&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25512;&#26029;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;DAG&#27491;&#21017;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13917</link><description>&lt;p&gt;
BayesDAG&#65306;&#22522;&#20110;&#26799;&#24230;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#21518;&#39564;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery. (arXiv:2307.13917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Bayesian causal discovery&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25512;&#26029;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;DAG&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26088;&#22312;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#37327;&#21270;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#21644;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#31354;&#38388;&#30340;&#32852;&#21512;&#25512;&#29702;&#32780;&#24102;&#26469;&#20102;&#35745;&#31639;&#25361;&#25112;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;DAG&#19978;&#30340;&#39640;&#25928;&#21518;&#39564;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20165;&#38480;&#20110;&#23545;&#32447;&#24615;&#22240;&#26524;&#27169;&#22411;&#30340;&#33410;&#28857;&#25490;&#21015;&#30697;&#38453;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#23548;&#33268;&#25512;&#26029;&#20934;&#30830;&#24615;&#21463;&#25439;&#65292;&#35201;&#20040;&#26159;&#22312;&#21463;DAG&#27491;&#21017;&#21270;&#32422;&#26463;&#30340;&#37051;&#25509;&#30697;&#38453;&#19978;&#36827;&#34892;&#36830;&#32493;&#26494;&#24347;&#65292;&#32780;&#19981;&#33021;&#30830;&#20445;&#24471;&#21040;&#30340;&#22270;&#26159;DAGs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;SG-MCMC&#65289;&#30340;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#20174;&#21518;&#39564;&#20013;&#37319;&#26679;DAG&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;DAG&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#36824;&#32472;&#21046;&#20989;&#25968;&#21442;&#25968;&#26679;&#26412;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#32479;&#35745;&#23398;&#20013;&#30340;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#25512;&#24191;&#21040;&#22312;&#32447;&#20915;&#31574;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13916</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online learning in bandits with predicted context. (arXiv:2307.13916v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#32479;&#35745;&#23398;&#20013;&#30340;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#25512;&#24191;&#21040;&#22312;&#32447;&#20915;&#31574;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#27599;&#20010;&#26102;&#21051;&#65292;&#20195;&#29702;&#21482;&#33021;&#35775;&#38382;&#21040;&#19978;&#19979;&#25991;&#30340;&#19968;&#20010;&#24102;&#22122;&#22768;&#30340;&#29256;&#26412;&#20197;&#21450;&#35823;&#24046;&#26041;&#24046;&#65288;&#25110;&#32773;&#36825;&#20010;&#26041;&#24046;&#30340;&#19968;&#20010;&#20272;&#35745;&#65289;&#12290;&#36825;&#19968;&#35774;&#32622;&#21463;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#20915;&#31574;&#30340;&#30495;&#23454;&#19978;&#19979;&#25991;&#26159;&#19981;&#21487;&#35266;&#27979;&#30340;&#65292;&#32780;&#21482;&#26377;&#19968;&#20010;&#30001;&#21487;&#33021;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#20986;&#30340;&#19978;&#19979;&#25991;&#12290;&#24403;&#19978;&#19979;&#25991;&#35823;&#24046;&#26159;&#38750;&#34928;&#20943;&#30340;&#26102;&#20505;&#65292;&#32463;&#20856;&#30340;bandit&#31639;&#27861;&#26080;&#27861;&#36798;&#21040;&#27425;&#32447;&#24615;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36825;&#19968;&#35774;&#32622;&#19979;&#65292;&#31532;&#19968;&#20010;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24182;&#19982;&#36866;&#24403;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20851;&#38190;&#30340;&#24605;&#24819;&#26159;&#23558;&#32463;&#20856;&#32479;&#35745;&#23398;&#20013;&#30340;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#25512;&#24191;&#21040;&#22312;&#32447;&#20915;&#31574;&#35774;&#32622;&#20013;&#65292;&#36825;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#22240;&#20026;&#31574;&#30053;&#20381;&#36182;&#20110;&#26377;&#22122;&#22768;&#30340;&#19978;&#19979;&#25991;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#39063;&#31890;&#30772;&#30862;&#24378;&#24230;&#12290;&#36890;&#36807;&#24314;&#27169;&#39063;&#31890;&#30862;&#29255;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25551;&#36848;&#39063;&#31890;&#30772;&#30862;&#30340;&#21147;&#23398;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#22312;&#39063;&#31890;&#30772;&#30862;&#30740;&#31350;&#20013;&#30340;&#36827;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#25968;&#20540;&#27169;&#25311;&#25968;&#25454;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.13909</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#29992;&#20110;&#39044;&#27979;&#39063;&#31890;&#30772;&#30862;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength. (arXiv:2307.13909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#39063;&#31890;&#30772;&#30862;&#24378;&#24230;&#12290;&#36890;&#36807;&#24314;&#27169;&#39063;&#31890;&#30862;&#29255;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25551;&#36848;&#39063;&#31890;&#30772;&#30862;&#30340;&#21147;&#23398;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#22312;&#39063;&#31890;&#30772;&#30862;&#30740;&#31350;&#20013;&#30340;&#36827;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#25968;&#20540;&#27169;&#25311;&#25968;&#25454;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#29992;&#20110;&#22810;&#23398;&#31185;&#20219;&#21153;&#65292;&#20363;&#22914;&#21046;&#33647;&#20998;&#23376;&#20998;&#31867;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#24314;&#27169;&#19981;&#27431;&#20960;&#37324;&#24503;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#39063;&#31890;&#30772;&#30862;&#20316;&#20026;&#22303;&#26408;&#24037;&#31243;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#25551;&#36848;&#20102;&#39063;&#31890;&#26448;&#26009;&#22312;&#25968;&#20540;&#27169;&#25311;&#19979;&#30001;&#20110;&#39063;&#31890;&#30862;&#29255;&#38190;&#30340;&#30772;&#35010;&#32780;&#21457;&#29983;&#30340;&#26029;&#35010;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#36890;&#36807;&#39063;&#31890;&#30862;&#29255;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#36830;&#25509;&#24615;&#26469;&#34920;&#24449;&#39063;&#31890;&#30772;&#30862;&#30340;&#21147;&#23398;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#27979;&#35797;&#25110;&#25968;&#20540;&#27169;&#25311;&#30340;&#26114;&#36149;&#25104;&#26412;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39063;&#31890;&#30772;&#30862;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;45,000&#20010;&#25968;&#20540;&#27169;&#25311;&#21644;900&#31181;&#39063;&#31890;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#39063;&#31890;&#30772;&#30862;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#23637;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks have emerged as an effective machine learning tool for multi-disciplinary tasks such as pharmaceutical molecule classification and chemical reaction prediction, because they can model non-euclidean relationships between different entities. Particle crushing, as a significant field of civil engineering, describes the breakage of granular materials caused by the breakage of particle fragment bonds under the modeling of numerical simulations, which motivates us to characterize the mechanical behaviors of particle crushing through the connectivity of particle fragments with Graph Neural Networks (GNNs). However, there lacks an open-source large-scale particle crushing dataset for research due to the expensive costs of laboratory tests or numerical simulations. Therefore, we firstly generate a dataset with 45,000 numerical simulations and 900 particle types to facilitate the research progress of machine learning for particle crushing. Secondly, we devise a hybrid frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#26143;&#26143;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#21464;&#38271;&#26102;&#38388;&#24207;&#21015;&#36755;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#39046;&#22495;&#30340;SOC&#20272;&#35745;&#21644;RUL&#20272;&#35745;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.13907</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26143;&#26143;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#21464;&#38271;&#26102;&#38388;&#24207;&#21015;&#36755;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input. (arXiv:2307.13907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#26143;&#26143;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#21464;&#38271;&#26102;&#38388;&#24207;&#21015;&#36755;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#39046;&#22495;&#30340;SOC&#20272;&#35745;&#21644;RUL&#20272;&#35745;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#32500;&#25252;&#26159;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;NN&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#36807;&#21435;&#34892;&#20026;&#21644;&#20851;&#38190;&#21442;&#25968;&#65288;&#22914;&#35774;&#22791;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#21644;&#30005;&#27744;&#30340;&#33655;&#30005;&#29366;&#24577;&#65288;SOC&#65289;&#65289;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#36755;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#32463;&#36807;&#20256;&#24863;&#22120;&#26102;&#21487;&#33021;&#36973;&#21463;&#26377;&#24847;&#25110;&#26080;&#24847;&#30340;&#22122;&#22768;&#24178;&#25200;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#20123;NN&#36827;&#34892;&#40065;&#26834;&#24615;&#39564;&#35777;&#21644;&#39564;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;NN&#65288;TSRegNN&#65289;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#26041;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#37319;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#24418;&#24335;&#26041;&#27861;&#12290;&#23427;&#30528;&#37325;&#20110;&#21033;&#29992;&#21464;&#38271;&#36755;&#20837;&#25968;&#25454;&#26469;&#31616;&#21270;&#36755;&#20837;&#25805;&#20316;&#24182;&#22686;&#24378;&#32593;&#32476;&#26550;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#65288;1&#65289;&#38146;&#31163;&#23376;&#30005;&#27744;SOC&#20272;&#35745;&#21644;&#65288;2&#65289;&#28065;&#36718;&#21457;&#21160;&#26426;RUL&#20272;&#35745;&#12290;&#23545;NN&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#26680;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven, neural network (NN) based anomaly detection and predictive maintenance are emerging research areas. NN-based analytics of time-series data offer valuable insights into past behaviors and estimates of critical parameters like remaining useful life (RUL) of equipment and state-of-charge (SOC) of batteries. However, input time series data can be exposed to intentional or unintentional noise when passing through sensors, necessitating robust validation and verification of these NNs. This paper presents a case study of the robustness verification approach for time series regression NNs (TSRegNN) using set-based formal methods. It focuses on utilizing variable-length input data to streamline input manipulation and enhance network architecture generalizability. The method is applied to two data sets in the Prognostics and Health Management (PHM) application areas: (1) SOC estimation of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs' robustness is checke
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2307.13903</link><description>&lt;p&gt;
&#33104;&#36133;&#40065;&#26834;&#30340;Lipschitz&#19978;&#19979;&#25991;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#23398;&#20064;&#32773;&#35797;&#22270;&#23398;&#20064;&#19968;&#20010;&#30001;&#23545;&#25163;&#36873;&#25321;&#30340;Lipschitz&#20989;&#25968;$f$&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#23545;&#25163;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#36873;&#25321;&#19968;&#20010;&#19978;&#19979;&#25991;&#21521;&#37327;$x_t$&#65292;&#23398;&#20064;&#32773;&#23545;&#30495;&#23454;&#20989;&#25968;&#20540;$f(x_t)$&#36827;&#34892;&#29468;&#27979;&#65292;&#24182;&#25509;&#25910;&#19968;&#20010;&#25351;&#31034;&#29468;&#27979;&#26159;&#39640;&#36824;&#26159;&#20302;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;&#22312;&#24635;&#20849;$C$&#36718;&#20013;&#65292;&#20449;&#21495;&#21487;&#33021;&#34987;&#31713;&#25913;&#65292;&#20294;&#23398;&#20064;&#32773;&#19981;&#30693;&#36947;$C$&#30340;&#20540;&#12290;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#36896;&#25104;&#23567;&#30340;&#32047;&#31215;&#25439;&#22833;&#12290;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#32780;&#24378;&#22823;&#30340;&#25216;&#26415;&#39564;&#35777;&#65292;&#23545;&#35774;&#35745;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#35774;&#35745;&#20102;&#19968;&#20123;&#31639;&#27861;&#65288;&#23558;Lipschitz&#21442;&#25968;$L$&#35270;&#20026;&#24120;&#25968;&#65289;&#65306;&#23545;&#20110;&#23545;&#31216;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d=1$&#26102;&#36798;&#21040;&#21518;&#24724;$O(C\log T)$&#65292;&#22312;$d&gt;1$&#26102;&#36798;&#21040;&#21518;&#24724;$O_d(C\log T + T^{(d-1)/d})$&#65307;&#23545;&#20110;&#35745;&#20215;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d/(d+1)$&#26102;&#36798;&#21040;&#21518;&#24724;$\widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$&#12290;
&lt;/p&gt;
&lt;p&gt;
I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d &gt; 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13899</link><description>&lt;p&gt;
&#29992;&#20803;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Regularizing Neural Networks with Meta-Learning Generative Models. (arXiv:2307.13899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#26679;&#26412;&#20316;&#20026;&#23567;&#25968;&#25454;&#38598;&#20998;&#31867;&#30340;&#39069;&#22806;&#25968;&#25454;&#38598;&#12290;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#21512;&#25104;&#25968;&#25454;&#20013;&#21253;&#21547;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26080;&#20449;&#24687;&#26679;&#26412;&#12290;&#36825;&#26159;&#22240;&#20026;&#21512;&#25104;&#26679;&#26412;&#19981;&#33021;&#23436;&#32654;&#22320;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#65292;&#22343;&#21248;&#25277;&#26679;&#20063;&#19981;&#19968;&#23450;&#20026;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#36991;&#20813;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#30340;&#38477;&#32423;&#65292;MGR&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#20132;&#21449;&#29109;&#12290;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#36890;&#36807;&#20803;&#23398;&#20064;&#21160;&#24577;&#30830;&#23450;&#65292;&#20197;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13885</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#39640;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#23545;&#22122;&#22768;&#36755;&#20837;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22122;&#22768;&#65288;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#65289;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#26469;&#25429;&#25417;&#65292;&#21363;&#22312;&#36755;&#20837;&#21608;&#22260;&#30340;&#23616;&#37096;&#21306;&#22495;&#20869;&#27169;&#22411;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#30340;&#35745;&#31639;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#26420;&#32032;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#24320;&#21457;&#20102;&#39318;&#20010;&#20998;&#26512;&#20272;&#35745;&#22120;&#65292;&#20197;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#20272;&#35745;&#22120;&#30340;&#25512;&#23548;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#40065;&#26834;&#24615;&#19982;&#38543;&#26426;&#24179;&#28369;&#21644;softmax&#27010;&#29575;&#31561;&#27010;&#24565;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
&lt;/p&gt;</description></item><item><title>ExeDec&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#21512;&#25104;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#27979;&#25191;&#34892;&#23376;&#30446;&#26631;&#24182;&#22312;&#27599;&#20010;&#27493;&#39588;&#30340;&#31243;&#24207;&#25191;&#34892;&#30340;&#25351;&#23548;&#19979;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#21512;&#25104;&#24615;&#33021;&#21644;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13883</link><description>&lt;p&gt;
ExeDec: &#22312;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20013;&#36827;&#34892;&#25191;&#34892;&#20998;&#35299;&#20197;&#23454;&#29616;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis. (arXiv:2307.13883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13883
&lt;/p&gt;
&lt;p&gt;
ExeDec&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#21512;&#25104;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#27979;&#25191;&#34892;&#23376;&#30446;&#26631;&#24182;&#22312;&#27599;&#20010;&#27493;&#39588;&#30340;&#31243;&#24207;&#25191;&#34892;&#30340;&#25351;&#23548;&#19979;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#21512;&#25104;&#24615;&#33021;&#21644;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32534;&#20889;&#31243;&#24207;&#26102;&#65292;&#20154;&#20204;&#36890;&#36807;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#19988;&#26356;&#29087;&#24713;&#30340;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#12290;&#34429;&#28982;&#34913;&#37327;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#30340;&#33021;&#21147;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#25105;&#20204;&#21487;&#20197;&#34913;&#37327;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#65292;&#21363;&#32463;&#36807;&#35757;&#32451;&#22312;&#36739;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#31243;&#24207;&#21512;&#25104;&#20013;&#24076;&#26395;&#30340;&#20960;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#32452;&#21512;&#27867;&#21270;&#65292;&#24182;&#24418;&#25104;&#19968;&#20010;&#20803;&#22522;&#20934;&#65292;&#29992;&#20110;&#20026;&#20004;&#20010;&#21463;&#27426;&#36814;&#30340;&#25968;&#25454;&#38598;RobustFill&#21644;DeepCoder&#21019;&#24314;&#27867;&#21270;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#35299;&#30340;&#21512;&#25104;&#31574;&#30053;ExeDec&#65292;&#23427;&#36890;&#36807;&#22312;&#27599;&#20010;&#27493;&#39588;&#30340;&#31243;&#24207;&#25191;&#34892;&#30340;&#25351;&#23548;&#19979;&#36880;&#27493;&#39044;&#27979;&#25191;&#34892;&#23376;&#30446;&#26631;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;ExeDec&#20855;&#26377;&#26356;&#20339;&#30340;&#21512;&#25104;&#24615;&#33021;&#21644;&#22823;&#22823;&#25913;&#36827;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21463;&#25968;&#35770;&#26041;&#27861;&#21551;&#21457;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#25554;&#20540;&#28857;&#26469;&#25552;&#39640;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.13869</link><description>&lt;p&gt;
&#20248;&#33391;&#26684;&#35757;&#32451;: &#20511;&#21161;&#25968;&#35770;&#21152;&#36895;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory. (arXiv:2307.13869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21463;&#25968;&#35770;&#26041;&#27861;&#21551;&#21457;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#25554;&#20540;&#28857;&#26469;&#25552;&#39640;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#26041;&#27861;&#12290;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#20110;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#28385;&#36275;&#32473;&#23450;&#28857;&#19978;&#30340;PDE&#65292;&#24182;&#23545;&#35299;&#36827;&#34892;&#36924;&#36817;&#12290;&#28982;&#32780;&#65292;PDE&#30340;&#35299;&#22312;&#26412;&#36136;&#19978;&#26159;&#26080;&#38480;&#32500;&#30340;&#65292;&#24182;&#19988;&#36755;&#20986;&#19982;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#26159;&#23450;&#20041;&#22312;&#25972;&#20010;&#22495;&#19978;&#30340;&#31215;&#20998;&#12290;&#22240;&#27492;&#65292;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#20165;&#25552;&#20379;&#26377;&#38480;&#30340;&#36924;&#36817;&#12290;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#25554;&#20540;&#28857;&#26041;&#38754;&#21017;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23613;&#31649;&#36825;&#19968;&#26041;&#38754;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#20248;&#33391;&#26684;&#35757;&#32451;(GLT)&#65292;&#29992;&#20110;PINNs&#65292;&#21463;&#25968;&#20540;&#20998;&#26512;&#20013;&#30340;&#25968;&#35770;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;GLT&#25552;&#20379;&#20102;&#19968;&#32452;&#21363;&#20351;&#22312;&#23569;&#37327;&#28857;&#21644;&#22810;&#32500;&#31354;&#38388;&#20013;&#20063;&#38750;&#24120;&#26377;&#25928;&#30340;&#25554;&#20540;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GLT&#21482;&#38656;&#35201;2-20&#20493;&#30340;&#28857;&#25968;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 tim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#35266;&#27979;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#20272;&#35745;&#27867;&#21270;&#21040;&#20219;&#24847;&#32500;&#24230;&#25110;&#21487;&#27979;&#31354;&#38388;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21517;&#20041;&#21464;&#37327;&#30340;&#22240;&#26524;&#20559;&#24046;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#31574;&#30053;&#22312;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#21644;&#21151;&#29575;&#26041;&#38754;&#26377;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.13868</link><description>&lt;p&gt;
&#20174;&#39640;&#32500;&#35266;&#27979;&#30740;&#31350;&#20013;&#23398;&#20064;&#21464;&#24322;&#28304;
&lt;/p&gt;
&lt;p&gt;
Learning sources of variability from high-dimensional observational studies. (arXiv:2307.13868v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#35266;&#27979;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#20272;&#35745;&#27867;&#21270;&#21040;&#20219;&#24847;&#32500;&#24230;&#25110;&#21487;&#27979;&#31354;&#38388;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21517;&#20041;&#21464;&#37327;&#30340;&#22240;&#26524;&#20559;&#24046;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#31574;&#30053;&#22312;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#21644;&#21151;&#29575;&#26041;&#38754;&#26377;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#21464;&#37327;&#24433;&#21709;&#35266;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#35832;&#22914;&#8220;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#8221;&#31561;&#37327;&#21270;&#25351;&#26631;&#65292;&#36825;&#19968;&#33539;&#24335;&#22312;&#35768;&#22810;&#29983;&#29289;&#39046;&#22495;&#20013;&#34987;&#37319;&#29992;&#65292;&#20174;&#30123;&#33495;&#21644;&#33647;&#29289;&#24320;&#21457;&#21040;&#25919;&#31574;&#24178;&#39044;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#24120;&#20165;&#38480;&#20110;&#21333;&#21464;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#22240;&#26524;&#20272;&#35745;&#27867;&#21270;&#21040;&#20219;&#24847;&#32500;&#24230;&#25110;&#21487;&#27979;&#31354;&#38388;&#30340;&#32467;&#26524;&#65292;&#24182;&#23558;&#20256;&#32479;&#30340;&#22240;&#26524;&#20272;&#35745;&#24418;&#24335;&#21270;&#20026;&#21517;&#20041;&#21464;&#37327;&#30340;&#22240;&#26524;&#20559;&#24046;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#35843;&#25972;&#19968;&#33268;&#24615;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#27979;&#35797;&#26159;&#19968;&#33268;&#24615;&#22240;&#26524;&#20559;&#24046;&#27979;&#35797;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Causal CDcorr&#22312;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#21644;&#21151;&#29575;&#26041;&#38754;&#22343;&#26377;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37117;&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#22312;github.com/ebridge2/cdcorr&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference studies whether the presence of a variable influences an observed outcome. As measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. Unfortunately, the majority of these methods are often limited to univariate outcomes. Our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. We propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. Numerical experiments illustrate that our method, Causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. Our methods are all open source and available at github.com/ebridge2/cdcorr.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;2.5D&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#39044;&#27979;&#35270;&#32593;&#33180;OCT&#30340;&#30142;&#30149;&#36827;&#23637;&#12290;&#37319;&#29992;&#28151;&#21512;2.5D&#26041;&#27861;&#32467;&#21512;&#20108;&#32500;&#21644;&#19977;&#32500;&#25216;&#26415;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#24615;&#33021;&#21644;&#20869;&#23384;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.13865</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;2.5D&#27169;&#22411;&#29992;&#20110;&#35270;&#32593;&#33180;OCT&#30340;&#39640;&#25928;&#39044;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT. (arXiv:2307.13865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;2.5D&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#39044;&#27979;&#35270;&#32593;&#33180;OCT&#30340;&#30142;&#30149;&#36827;&#23637;&#12290;&#37319;&#29992;&#28151;&#21512;2.5D&#26041;&#27861;&#32467;&#21512;&#20108;&#32500;&#21644;&#19977;&#32500;&#25216;&#26415;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#24615;&#33021;&#21644;&#20869;&#23384;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;3D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26500;&#24314;&#24378;&#22823;&#30340;&#30142;&#30149;&#36827;&#23637;&#39044;&#27979;&#27169;&#22411;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#22312;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;3D&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#28151;&#21512;2.5D&#26041;&#27861;&#20026;&#21033;&#29992;&#20108;&#32500;&#27169;&#22411;&#39640;&#25928;&#22320;&#22788;&#29702;&#19977;&#32500;&#20307;&#31215;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#32467;&#21512;&#20108;&#32500;&#21644;&#19977;&#32500;&#25216;&#26415;&#20026;&#20248;&#21270;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#20869;&#23384;&#35201;&#27714;&#25552;&#20379;&#20102;&#19968;&#26465;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;Transformer&#30340;2.5D&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26368;&#36817;&#30340;&#38750;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#20108;&#32500;&#19978;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;2.5D&#25216;&#26415;&#30340;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression. However, the size of these models presents significant challenges, both in terms of computational resources and data requirements. Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging. To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models. Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements. In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers. In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further. We demonstrate the effectiveness of archi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20223;&#30495;&#25968;&#25454;&#35757;&#32451;&#31995;&#32479;&#26469;&#35774;&#35745;&#28385;&#36275;&#38408;&#20540;&#35268;&#33539;&#30340;&#27169;&#25311;&#30005;&#36335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25104;&#21151;&#29575;&#36229;&#36807;90&#65285;&#12290;</title><link>http://arxiv.org/abs/2307.13861</link><description>&lt;p&gt;
&#23398;&#20064;&#35774;&#35745;&#27169;&#25311;&#30005;&#36335;&#20197;&#28385;&#36275;&#38408;&#20540;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Learning to Design Analog Circuits to Meet Threshold Specifications. (arXiv:2307.13861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20223;&#30495;&#25968;&#25454;&#35757;&#32451;&#31995;&#32479;&#26469;&#35774;&#35745;&#28385;&#36275;&#38408;&#20540;&#35268;&#33539;&#30340;&#27169;&#25311;&#30005;&#36335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25104;&#21151;&#29575;&#36229;&#36807;90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#20174;&#20223;&#30495;&#25968;&#25454;&#20013;&#33258;&#21160;&#35774;&#35745;&#27169;&#25311;&#30005;&#36335;&#21644;&#23556;&#39057;&#30005;&#36335;&#20316;&#20026;&#25163;&#21160;&#19987;&#23478;&#35774;&#35745;&#30340;&#26367;&#20195;&#26041;&#27861;&#24050;&#32463;&#24471;&#21040;&#30740;&#31350;&#12290;&#23545;&#20110;&#35774;&#35745;&#20195;&#29702;&#26469;&#35828;&#65292;&#20174;&#26399;&#26395;&#30340;&#24615;&#33021;&#25351;&#26631;&#21040;&#30005;&#36335;&#21442;&#25968;&#30340;&#36870;&#20989;&#25968;&#23398;&#20064;&#26159;&#30452;&#25509;&#30340;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#36890;&#24120;&#20855;&#26377;&#38408;&#20540;&#24615;&#33021;&#26631;&#20934;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#30340;&#30446;&#26631;&#21521;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20223;&#30495;&#25968;&#25454;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#31995;&#32479;&#26469;&#35774;&#35745;&#28385;&#36275;&#38408;&#20540;&#35268;&#33539;&#30340;&#30005;&#36335;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#36804;&#20170;&#20026;&#27490;&#26368;&#24191;&#27867;&#30340;&#33258;&#21160;&#27169;&#25311;&#30005;&#36335;&#35774;&#35745;&#35780;&#20272;&#65292;&#21253;&#25324;&#22312;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#36335;&#38598;&#21512;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#28085;&#30422;&#32447;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#27835;&#30005;&#36335;&#37197;&#32622;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#22312;5&#65285;&#35823;&#24046;&#36793;&#30028;&#19979;&#33719;&#24471;&#36229;&#36807;90&#65285;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated design of analog and radio-frequency circuits using supervised or reinforcement learning from simulation data has recently been studied as an alternative to manual expert design. It is straightforward for a design agent to learn an inverse function from desired performance metrics to circuit parameters. However, it is more common for a user to have threshold performance criteria rather than an exact target vector of feasible performance measures. In this work, we propose a method for generating from simulation data a dataset on which a system can be trained via supervised learning to design circuits to meet threshold specifications. We moreover perform the to-date most extensive evaluation of automated analog circuit design, including experimenting in a significantly more diverse set of circuits than in prior work, covering linear, nonlinear, and autonomous circuit configurations, and show that our method consistently reaches success rate better than 90% at 5% error margin, w
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35843;&#26597;&#20102;Vision Transformers&#65288;ViTs&#65289;&#22312;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;ViTs&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#24182;&#19981;&#33021;&#25193;&#23637;&#21040;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#65292;&#24182;&#19988;&#36825;&#20123;&#27169;&#22411;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13856</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;transformers&#30340;&#19981;&#21512;&#29702;&#26131;&#21463;&#25915;&#20987;&#24615; -- &#20197;&#21450;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the unreasonable vulnerability of transformers for image restoration -- and an easy fix. (arXiv:2307.13856v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13856
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;Vision Transformers&#65288;ViTs&#65289;&#22312;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;ViTs&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#24182;&#19981;&#33021;&#25193;&#23637;&#21040;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#65292;&#24182;&#19988;&#36825;&#20123;&#27169;&#22411;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#21518;&#65292;Vision Transformers&#65288;ViTs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;ViTs&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;ViTs&#30340;&#25913;&#36827;&#23545;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#26159;&#21542;&#20855;&#26377;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Restormer&#27169;&#22411;&#65292;&#20197;&#21450;NAFNet&#21644;&#8220;&#22522;&#20934;&#32593;&#32476;&#8221;&#65292;&#23427;&#20204;&#37117;&#26159;Restormer&#30340;&#31616;&#21270;&#29256;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#21644;CosPGD&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20687;&#32032;&#39044;&#27979;&#20219;&#21153;&#25552;&#20986;&#30340;&#26368;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;GoPro&#25968;&#25454;&#38598;&#19978;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#22270;&#20687;&#21435;&#27169;&#31946;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;ViTs&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#22768;&#31216;&#30456;&#21453;&#65292;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Following their success in visual recognition tasks, Vision Transformers(ViTs) are being increasingly employed for image restoration. As a few recent works claim that ViTs for image classification also have better robustness properties, we investigate whether the improved adversarial robustness of ViTs extends to image restoration. We consider the recently proposed Restormer model, as well as NAFNet and the "Baseline network" which are both simplified versions of a Restormer. We use Projected Gradient Descent (PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise prediction tasks for our robustness evaluation. Our experiments are performed on real-world images from the GoPro dataset for image deblurring. Our analysis indicates that contrary to as advocated by ViTs in image classification works, these models are highly susceptible to adversarial attacks. We attempt to improve their robustness through adversarial training. While this yields a significant increase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38160;&#21270;&#20313;&#24358;&#30456;&#20284;&#24230;(SCS)&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20316;&#20026;&#21367;&#31215;&#30340;&#26367;&#20195;&#21697;&#30340;&#21487;&#33021;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;SCS&#21487;&#33021;&#19981;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#33021;&#23398;&#20064;&#21040;&#26356;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#31245;&#24494;&#22686;&#21152;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13855</link><description>&lt;p&gt;
&#25506;&#32034;&#38160;&#21270;&#20313;&#24358;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Exploring the Sharpened Cosine Similarity. (arXiv:2307.13855v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13855
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38160;&#21270;&#20313;&#24358;&#30456;&#20284;&#24230;(SCS)&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20316;&#20026;&#21367;&#31215;&#30340;&#26367;&#20195;&#21697;&#30340;&#21487;&#33021;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;SCS&#21487;&#33021;&#19981;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#33021;&#23398;&#20064;&#21040;&#26356;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#31245;&#24494;&#22686;&#21152;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#23618;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#22270;&#20687;&#20998;&#31867;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38160;&#21270;&#20313;&#24358;&#30456;&#20284;&#24230;&#65288;SCS&#65289;&#65292;&#25454;&#29702;&#35770;&#19978;&#35748;&#20026;&#21487;&#33021;&#20316;&#20026;&#26356;&#22909;&#30340;&#29305;&#24449;&#26816;&#27979;&#22120;&#12290;&#34429;&#28982;&#22810;&#20010;&#26469;&#28304;&#25253;&#36947;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#36827;&#34892;&#20840;&#38754;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#35780;&#20272;&#36825;&#20123;&#26032;&#23618;&#22312;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#26041;&#38754;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;SCS&#30340;&#21442;&#25968;&#34892;&#20026;&#21644;&#22312;CIFAR-10&#19978;&#22810;&#20010;CNN&#26550;&#26500;&#20013;&#20316;&#20026;&#21367;&#31215;&#30340;&#26367;&#20195;&#21697;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;SCS&#21487;&#33021;&#19981;&#20250;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#33021;&#23398;&#20064;&#21040;&#26356;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;SCS&#21487;&#33021;&#20250;&#31245;&#24494;&#22686;&#21152;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional layers have long served as the primary workhorse for image classification. Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector. While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new layers. In our work, we explore SCS's parameter behavior and potential as a drop-in replacement for convolutions in multiple CNN architectures benchmarked on CIFAR-10. We find that while SCS may not yield significant increases in accuracy, it may learn more interpretable representations. We also find that, in some circumstances, SCS may confer a slight increase in adversarial robustness.
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SplitFed&#65289;&#22312;&#36890;&#20449;&#38142;&#36335;&#19978;&#23545;&#25968;&#25454;&#21253;&#20002;&#22833;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26356;&#28145;&#30340;&#20999;&#21106;&#28857;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.13851</link><description>&lt;p&gt;
SplitFed&#23545;&#25968;&#25454;&#21253;&#20002;&#22833;&#30340;&#38887;&#24615;&#65306;&#20309;&#22788;&#20999;&#21106;&#65292;&#36825;&#26159;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
SplitFed resilience to packet loss: Where to split, that is the question. (arXiv:2307.13851v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SplitFed&#65289;&#22312;&#36890;&#20449;&#38142;&#36335;&#19978;&#23545;&#25968;&#25454;&#21253;&#20002;&#22833;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26356;&#28145;&#30340;&#20999;&#21106;&#28857;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#28151;&#21512;&#24418;&#24335;&#65292;&#22914;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SplitFed&#25110;SFL&#65289;&#65292;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#33539;&#22260;&#24471;&#21040;&#20102;&#25193;&#23637;&#12290;SFL&#30340;&#30446;&#26631;&#26159;&#38477;&#20302;FL&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#25152;&#38656;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#24182;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;SL&#30340;&#24182;&#34892;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;SFL&#23545;&#36890;&#20449;&#38142;&#36335;&#19978;&#25968;&#25454;&#21253;&#20002;&#22833;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#27169;&#22411;&#30340;&#20004;&#20010;&#20301;&#32622;&#8212;&#8212;&#27973;&#24230;&#20999;&#21106;&#28857;&#21644;&#28145;&#24230;&#20999;&#21106;&#28857;&#8212;&#8212;&#36827;&#34892;&#20999;&#21106;&#65292;&#24182;&#27979;&#35797;&#20999;&#21106;&#28857;&#26159;&#21542;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#32479;&#35745;&#26174;&#33879;&#30340;&#24433;&#21709;&#65292;&#32771;&#23519;&#20102;&#19981;&#21516;&#30340;SFL&#32858;&#21512;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#22312;&#20154;&#31867;&#32986;&#32974;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#19978;&#36827;&#34892;&#65292;&#24182;&#34920;&#26126;&#26356;&#28145;&#30340;&#20999;&#21106;&#28857;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized machine learning has broadened its scope recently with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy. This paper investigates the robustness of SFL against packet loss on communication links. The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model. Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.
&lt;/p&gt;</description></item><item><title>MAEA&#26159;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#20219;&#20309;&#21487;&#24494;&#20998;&#30340;&#22810;&#27169;&#24577;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#30340;&#20840;&#23616;&#24402;&#22240;&#30340;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#26469;&#25490;&#21517;&#21644;&#20998;&#32452;&#25925;&#38556;&#22330;&#26223;&#65292;&#35843;&#26597;&#24314;&#27169;&#21644;&#25968;&#25454;&#38598;&#20559;&#35265;&#65292;&#24182;&#23545;&#22810;&#27169;&#24577;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#30340;&#31283;&#20581;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#36827;&#34892;&#20851;&#38190;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.13850</link><description>&lt;p&gt;
MAEA: &#22522;&#20110;&#22810;&#27169;&#24577;&#30340;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#30340;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
MAEA: Multimodal Attribution for Embodied AI. (arXiv:2307.13850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13850
&lt;/p&gt;
&lt;p&gt;
MAEA&#26159;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#20219;&#20309;&#21487;&#24494;&#20998;&#30340;&#22810;&#27169;&#24577;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#30340;&#20840;&#23616;&#24402;&#22240;&#30340;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#26469;&#25490;&#21517;&#21644;&#20998;&#32452;&#25925;&#38556;&#22330;&#26223;&#65292;&#35843;&#26597;&#24314;&#27169;&#21644;&#25968;&#25454;&#38598;&#20559;&#35265;&#65292;&#24182;&#23545;&#22810;&#27169;&#24577;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#30340;&#31283;&#20581;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#36827;&#34892;&#20851;&#38190;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22810;&#27169;&#24577;&#24863;&#30693;&#23545;&#20110;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#36755;&#20837;&#21487;&#33021;&#21253;&#21547;&#39640;&#24230;&#20114;&#34917;&#21644;&#20887;&#20313;&#30340;&#20449;&#24687;&#29992;&#20110;&#20219;&#21153;&#12290;&#22810;&#27169;&#24577;&#25919;&#31574;&#30340;&#19968;&#20010;&#30456;&#20851;&#26041;&#21521;&#26159;&#29702;&#35299;&#34701;&#21512;&#23618;&#20013;&#27599;&#31181;&#27169;&#24577;&#30340;&#20840;&#23616;&#36235;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;ALFRED&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#19981;&#21516;&#25919;&#31574;&#19978;&#35299;&#24320;&#20102;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#20808;&#21069;&#21160;&#20316;&#36755;&#20837;&#30340;&#24402;&#22240;&#12290;&#24402;&#22240;&#20998;&#26512;&#21487;&#20197;&#29992;&#20110;&#25490;&#21517;&#21644;&#20998;&#32452;&#25925;&#38556;&#22330;&#26223;&#12289;&#35843;&#26597;&#24314;&#27169;&#21644;&#25968;&#25454;&#38598;&#20559;&#35265;&#65292;&#24182;&#22312;&#37096;&#32626;&#20043;&#21069;&#23545;&#22810;&#27169;&#24577;EAI&#25919;&#31574;&#30340;&#31283;&#20581;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#36827;&#34892;&#20851;&#38190;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MAEA&#65292;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#20219;&#20309;&#21487;&#24494;&#20998;&#25919;&#31574;&#30340;&#27599;&#20010;&#27169;&#24577;&#30340;&#20840;&#23616;&#24402;&#22240;&#30340;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24402;&#22240;&#22914;&#20309;&#22312;EAI&#25919;&#31574;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#24402;&#22240;&#20013;&#21551;&#29992;&#26356;&#24213;&#23618;&#30340;&#34892;&#20026;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20013;&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.13831</link><description>&lt;p&gt;
&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#19982;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13831
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20013;&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;&#34429;&#28982;SGD&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#23398;&#20064;&#29575;&#65292;&#22914;&#24120;&#25968;&#25110;&#36882;&#20943;&#30340;&#23398;&#20064;&#29575;&#65292;&#20294;&#20043;&#21069;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;SGD&#20351;&#29992;&#32447;&#25628;&#32034;&#26041;&#27861;&#32473;&#20986;&#30340;&#23398;&#20064;&#29575;&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#32473;&#20986;&#23398;&#20064;&#29575;&#30340;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;SGD&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#27493;&#25968;&#21644;&#25209;&#22823;&#23567;&#37117;&#24456;&#22823;&#26102;&#65292;&#20840;&#26799;&#24230;&#30340;&#24179;&#26041;&#33539;&#25968;&#30340;&#26399;&#26395;&#19978;&#30028;&#21464;&#23567;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#23398;&#20064;&#29575;&#30340;SGD&#26469;&#35828;&#65292;&#38750;&#20984;&#20248;&#21270;&#25152;&#38656;&#30340;&#27493;&#25968;&#26159;&#25209;&#22823;&#23567;&#30340;&#21333;&#35843;&#36882;&#20943;&#20984;&#20989;&#25968;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#38750;&#20984;&#20248;&#21270;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38543;&#26426;&#28779;&#28798;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic fir
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21521;&#34892;&#20026;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#34892;&#20026;&#31574;&#30053;&#26412;&#36523;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#26469;&#22788;&#29702;&#22806;&#25512;&#35823;&#24046;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13824</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22312;&#32447;Q&#20989;&#25968;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with On-Policy Q-Function Regularization. (arXiv:2307.13824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21521;&#34892;&#20026;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#34892;&#20026;&#31574;&#30053;&#26412;&#36523;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#26469;&#22788;&#29702;&#22806;&#25512;&#35823;&#24046;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;Offline RL&#65289;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#22788;&#29702;&#21382;&#21490;&#25968;&#25454;&#38598;&#19982;&#26399;&#26395;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#25152;&#24341;&#36215;&#30340;&#65288;&#28508;&#22312;&#28798;&#38590;&#24615;&#30340;&#65289;&#22806;&#25512;&#35823;&#24046;&#12290;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#36890;&#36807;&#38544;&#24335;/&#26174;&#24335;&#22320;&#23558;&#23398;&#20064;&#31574;&#30053;&#21521;&#34892;&#20026;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#21487;&#38752;&#22320;&#20272;&#35745;&#34892;&#20026;&#31574;&#30053;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21521;&#34892;&#20026;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#21069;&#25552;&#26159;Q&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;SARSA-style&#20272;&#35745;&#26356;&#21487;&#38752;&#19988;&#26356;&#23481;&#26131;&#22320;&#22788;&#29702;&#22806;&#25512;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21033;&#29992;&#20272;&#35745;&#30340;Q&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy. A large portion of prior work tackles this challenge by implicitly/explicitly regularizing the learning policy towards the behavior policy, which is hard to estimate reliably in practice. In this work, we propose to regularize towards the Q-function of the behavior policy instead of the behavior policy itself, under the premise that the Q-function can be estimated more reliably and easily by a SARSA-style estimate and handles the extrapolation error more straightforwardly. We propose two algorithms taking advantage of the estimated Q-function through regularizations, and demonstrate they exhibit strong performance on the D4RL benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;MuReNN&#65289;&#30340;&#31070;&#32463;&#38899;&#39057;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#30340;&#20843;&#24230;&#23376;&#24102;&#19978;&#35757;&#32451;&#21333;&#29420;&#30340;&#21367;&#31215;&#31639;&#23376;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#38754;&#20020;&#30340;&#38750;&#21442;&#25968;&#21644;&#21442;&#25968;&#26041;&#27861;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2307.13821</link><description>&lt;p&gt;
&#29992;&#22810;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;&#25311;&#21512;&#21548;&#35273;&#28388;&#27874;&#22120;&#32452;
&lt;/p&gt;
&lt;p&gt;
Fitting Auditory Filterbanks with Multiresolution Neural Networks. (arXiv:2307.13821v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;MuReNN&#65289;&#30340;&#31070;&#32463;&#38899;&#39057;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#30340;&#20843;&#24230;&#23376;&#24102;&#19978;&#35757;&#32451;&#21333;&#29420;&#30340;&#21367;&#31215;&#31639;&#23376;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#38754;&#20020;&#30340;&#38750;&#21442;&#25968;&#21644;&#21442;&#25968;&#26041;&#27861;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#38754;&#20020;&#38750;&#21442;&#25968;&#21644;&#21442;&#25968;&#26041;&#27861;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;&#19968;&#26041;&#38754;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;convnets&#65289;&#21487;&#20197;&#36817;&#20284;&#20219;&#20309;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65307;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#30340;&#39057;&#29575;&#21709;&#24212;&#38543;&#30528;&#24863;&#21463;&#37326;&#30340;&#22686;&#38271;&#21464;&#24471;&#26356;&#21152;&#19981;&#35268;&#21017;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35832;&#22914;LEAF&#20043;&#31867;&#30340;&#21442;&#25968;&#27169;&#22411;&#20445;&#35777;&#20135;&#29983;Gabor&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#30340;&#26102;&#39057;&#23450;&#20301;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#19981;&#21033;&#20110;&#34920;&#31034;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;&#22810;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;MuReNN&#65289;&#30340;&#31070;&#32463;&#38899;&#39057;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#19968;&#22256;&#22659;&#12290;MuReNN&#32972;&#21518;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#30340;&#20843;&#24230;&#23376;&#24102;&#19978;&#35757;&#32451;&#21333;&#29420;&#30340;&#21367;&#31215;&#31639;&#23376;&#12290;&#30001;&#20110;DWT&#21407;&#23376;&#30340;&#23610;&#24230;&#22312;&#20843;&#24230;&#20043;&#38388;&#25353;&#25351;&#25968;&#22686;&#38271;&#65292;MuReNN&#20013;&#21518;&#32493;&#21487;&#23398;&#20064;&#21367;&#31215;&#30340;&#24863;&#21463;&#37326;&#20063;&#30456;&#24212;&#25193;&#24352;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25311;&#21512;&#20102;&#24133;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Waveform-based deep learning faces a dilemma between nonparametric and parametric approaches. On one hand, convolutional neural networks (convnets) may approximate any linear time-invariant system; yet, in practice, their frequency responses become more irregular as their receptive fields grow. On the other hand, a parametric model such as LEAF is guaranteed to yield Gabor filters, hence an optimal time-frequency localization; yet, this strong inductive bias comes at the detriment of representational capacity. In this paper, we aim to overcome this dilemma by introducing a neural audio model, named multiresolution neural network (MuReNN). The key idea behind MuReNN is to train separate convolutional operators over the octave subbands of a discrete wavelet transform (DWT). Since the scale of DWT atoms grows exponentially between octaves, the receptive fields of the subsequent learnable convolutions in MuReNN are dilated accordingly. For a given real-world dataset, we fit the magnitude r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#38543;&#26426;&#28857;&#31215;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#38750;&#20984;&#20248;&#21270;&#25216;&#26415;&#25913;&#36827;&#20102;&#22312;&#35266;&#23519;&#22270;&#20013;&#20272;&#35745;&#33410;&#28857;&#28508;&#22312;&#21521;&#37327;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#38454;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#35299;&#20915;&#23884;&#20837;&#38382;&#39064;&#65292;&#24182;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#23454;&#29992;&#32593;&#32476;&#23884;&#20837;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.13818</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#38543;&#26426;&#28857;&#31215;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Gradient-Based Spectral Embeddings of Random Dot Product Graphs. (arXiv:2307.13818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#38543;&#26426;&#28857;&#31215;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#38750;&#20984;&#20248;&#21270;&#25216;&#26415;&#25913;&#36827;&#20102;&#22312;&#35266;&#23519;&#22270;&#20013;&#20272;&#35745;&#33410;&#28857;&#28508;&#22312;&#21521;&#37327;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#38454;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#35299;&#20915;&#23884;&#20837;&#38382;&#39064;&#65292;&#24182;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#23454;&#29992;&#32593;&#32476;&#23884;&#20837;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#28857;&#31215;&#22270;&#35889;&#65288;RDPG&#65289;&#26159;&#19968;&#20010;&#20851;&#31995;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#33410;&#28857;&#36890;&#36807;&#22312;&#20302;&#32500;&#27431;&#27663;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#34920;&#31034;&#12290;RDPG&#20851;&#38190;&#22320;&#20551;&#35774;&#36793;&#30340;&#24418;&#25104;&#27010;&#29575;&#30001;&#30456;&#24212;&#30340;&#28508;&#22312;&#20301;&#32622;&#30340;&#28857;&#31215;&#32473;&#20986;&#12290;&#22240;&#27492;&#65292;&#20174;&#35266;&#23519;&#21040;&#30340;&#22270;&#20013;&#20272;&#35745;&#36825;&#20123;&#21521;&#37327;&#30340;&#23884;&#20837;&#20219;&#21153;&#36890;&#24120;&#34987;&#35774;&#23450;&#20026;&#19968;&#20010;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#12290;&#32463;&#20856;&#30340;&#37051;&#25509;&#35889;&#23884;&#20837;&#65288;ASE&#65289;&#20855;&#26377;&#21487;&#38752;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#20294;&#23427;&#22312;&#24418;&#24335;&#19978;&#35299;&#20915;&#30340;&#26159;&#19968;&#20010;&#20195;&#29702;&#38382;&#39064;&#65292;&#24182;&#19988;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#38750;&#20984;&#20248;&#21270;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#23545;RDPG&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20513;&#20351;&#29992;&#19968;&#38454;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#35299;&#20915;&#23884;&#20837;&#38382;&#39064;&#65292;&#24182;&#33258;&#28982;&#22320;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#23454;&#29992;&#32593;&#32476;&#23884;&#20837;&#24212;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35748;&#20026;RDPG&#23884;&#20837;&#26377;&#21521;&#22270;&#22833;&#21435;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#38500;&#38750;...
&lt;/p&gt;
&lt;p&gt;
The Random Dot Product Graph (RDPG) is a generative model for relational data, where nodes are represented via latent vectors in low-dimensional Euclidean space. RDPGs crucially postulate that edge formation probabilities are given by the dot product of the corresponding latent positions. Accordingly, the embedding task of estimating these vectors from an observed graph is typically posed as a low-rank matrix factorization problem. The workhorse Adjacency Spectral Embedding (ASE) enjoys solid statistical properties, but it is formally solving a surrogate problem and can be computationally intensive. In this paper, we bring to bear recent advances in non-convex optimization and demonstrate their impact to RDPG inference. We advocate first-order gradient descent methods to better solve the embedding problem, and to organically accommodate broader network embedding applications of practical relevance. Notably, we argue that RDPG embeddings of directed graphs loose interpretability unless 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2307.13813</link><description>&lt;p&gt;
&#22914;&#20309;&#25193;&#23637;&#24744;&#30340;EMA&#65288;arXiv:2307.13813v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#22312;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#36890;&#24120;&#36890;&#36807;&#19968;&#20010;&#32553;&#25918;&#35268;&#21017;&#26469;&#23454;&#29616;&#65292;&#20363;&#22914;&#65292;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#65292;&#24212;&#35813;&#23558;&#23398;&#20064;&#29575;&#19982;&#25209;&#37327;&#22823;&#23567;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#21478;&#19968;&#20010;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24037;&#20855;&#26159;&#27169;&#22411;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#25509;&#25910;&#26799;&#24230;&#20449;&#24687;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#32780;&#26159;&#20197;&#19968;&#23450;&#30340;&#21160;&#37327;&#36319;&#38543;&#20854;&#30446;&#26631;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;EMA&#21487;&#20197;&#25552;&#39640;&#30417;&#30563;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#31283;&#23450;&#20266;&#26631;&#35760;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;EMA&#19982;&#20248;&#21270;&#20998;&#24320;&#22788;&#29702;&#65292;&#23548;&#33268;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19982;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#33521;&#36229;&#32852;&#36187;&#25968;&#25454;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36229;&#36807;&#21021;&#22987;&#36130;&#23500;135.8%&#30340;&#24778;&#20154;&#21033;&#28070;&#30340;&#20307;&#32946;&#21338;&#24425;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13807</link><description>&lt;p&gt;
&#20307;&#32946;&#21338;&#24425;&#65306;&#31070;&#32463;&#32593;&#32476;&#21644;&#29616;&#20195;&#25237;&#36164;&#32452;&#21512;&#29702;&#35770;&#22312;&#33521;&#36229;&#32852;&#36187;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sports Betting: an application of neural networks and modern portfolio theory to the English Premier League. (arXiv:2307.13807v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19982;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#33521;&#36229;&#32852;&#36187;&#25968;&#25454;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36229;&#36807;&#21021;&#22987;&#36130;&#23500;135.8%&#30340;&#24778;&#20154;&#21033;&#28070;&#30340;&#20307;&#32946;&#21338;&#24425;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20307;&#32946;&#21338;&#24425;&#20013;&#20248;&#21270;&#25237;&#27880;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20911;&#183;&#35834;&#20381;&#26364;-&#33707;&#26681;&#26031;&#29305;&#24681;&#26399;&#26395;&#25928;&#29992;&#29702;&#35770;&#12289;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#20975;&#21033;&#26631;&#20934;&#30340;&#20808;&#36827;&#20844;&#24335;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19982;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;2020/2021&#33521;&#36229;&#32852;&#36187;&#30340;&#21518;&#21322;&#27573;&#30456;&#23545;&#20110;&#21021;&#22987;&#36130;&#23500;&#33719;&#24471;&#20102;&#24778;&#20154;&#30340;&#21033;&#28070;&#65292;&#36798;&#21040;&#20102;135.8%&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23436;&#25972;&#21644;&#21463;&#38480;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#32489;&#25928;&#12289;&#39118;&#38505;&#31649;&#29702;&#21644;&#22810;&#26679;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#27604;&#36187;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#21464;&#37327;&#26377;&#38480;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#20307;&#32946;&#21338;&#24425;&#21644;&#39044;&#27979;&#24314;&#27169;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach for optimizing betting strategies in sports gambling by integrating Von Neumann-Morgenstern Expected Utility Theory, deep learning techniques, and advanced formulations of the Kelly Criterion. By combining neural network models with portfolio optimization, our method achieved remarkable profits of 135.8% relative to the initial wealth during the latter half of the 20/21 season of the English Premier League. We explore complete and restricted strategies, evaluating their performance, risk management, and diversification. A deep neural network model is developed to forecast match outcomes, addressing challenges such as limited variables. Our research provides valuable insights and practical applications in the field of sports betting and predictive modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28304;&#26465;&#20214;&#21452;&#31283;&#20581;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32447;&#24615;&#36870;&#38382;&#39064;&#35299;&#30340;&#32447;&#24615;&#20989;&#25968;&#21442;&#25968;&#65292;&#26080;&#38656;&#30693;&#36947;&#21738;&#20010;&#36870;&#38382;&#39064;&#26356;&#33391;&#22909;&#65292;&#35813;&#26041;&#27861;&#33021;&#30830;&#20445;&#23545;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#36845;&#20195;Tikhonov&#27491;&#21017;&#21270;&#23545;&#25239;&#20272;&#35745;&#22120;&#30340;&#26032;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.13793</link><description>&lt;p&gt;
&#36870;&#38382;&#39064;&#20989;&#25968;&#30340;&#28304;&#26465;&#20214;&#21452;&#31283;&#20581;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Source Condition Double Robust Inference on Functionals of Inverse Problems. (arXiv:2307.13793v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28304;&#26465;&#20214;&#21452;&#31283;&#20581;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32447;&#24615;&#36870;&#38382;&#39064;&#35299;&#30340;&#32447;&#24615;&#20989;&#25968;&#21442;&#25968;&#65292;&#26080;&#38656;&#30693;&#36947;&#21738;&#20010;&#36870;&#38382;&#39064;&#26356;&#33391;&#22909;&#65292;&#35813;&#26041;&#27861;&#33021;&#30830;&#20445;&#23545;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#36845;&#20195;Tikhonov&#27491;&#21017;&#21270;&#23545;&#25239;&#20272;&#35745;&#22120;&#30340;&#26032;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#32447;&#24615;&#36870;&#38382;&#39064;&#35299;&#30340;&#32447;&#24615;&#20989;&#25968;&#21442;&#25968;&#30340;&#20272;&#35745;&#12290;&#20219;&#20309;&#36825;&#26679;&#30340;&#21442;&#25968;&#37117;&#26377;&#19968;&#20010;&#21452;&#31283;&#20581;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#20381;&#36182;&#20110;&#23545;&#20598;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#35299;&#65292;&#20854;&#20013;&#23545;&#20598;&#35299;&#21487;&#20197;&#34987;&#35270;&#20026;&#36870;&#20542;&#21521;&#20989;&#25968;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#28304;&#26465;&#20214;&#21452;&#31283;&#20581;&#25512;&#26029;&#26041;&#27861;&#65292;&#21482;&#35201;&#21407;&#22987;&#25110;&#23545;&#20598;&#36870;&#38382;&#39064;&#36275;&#22815;&#33391;&#22909;&#65292;&#26080;&#38656;&#30693;&#36947;&#21738;&#20010;&#36870;&#38382;&#39064;&#26356;&#33391;&#22909;&#65292;&#23601;&#33021;&#30830;&#20445;&#23545;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#36890;&#36807;&#23545;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#36845;&#20195;Tikhonov&#27491;&#21017;&#21270;&#23545;&#25239;&#20272;&#35745;&#22120;&#22312;&#19968;&#33324;&#20551;&#35774;&#31354;&#38388;&#19978;&#30340;&#26032;&#30340;&#20445;&#35777;&#32780;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#29420;&#31435;&#21457;&#23637;&#30340;&#21033;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider estimation of parameters defined as linear functionals of solutions to linear inverse problems. Any such parameter admits a doubly robust representation that depends on the solution to a dual linear inverse problem, where the dual solution can be thought as a generalization of the inverse propensity function. We provide the first source condition double robust inference method that ensures asymptotic normality around the parameter of interest as long as either the primal or the dual inverse problem is sufficiently well-posed, without knowledge of which inverse problem is the more well-posed one. Our result is enabled by novel guarantees for iterated Tikhonov regularized adversarial estimators for linear inverse problems, over general hypothesis spaces, which are developments of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#26041;&#22270;&#23618;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#27700;&#19979;&#22768;&#32435;&#30446;&#26631;&#35782;&#21035;&#12290;&#36890;&#36807;&#24341;&#20837;&#32479;&#35745;&#32972;&#26223;&#65292;&#35813;&#26041;&#27861;&#22312;&#29305;&#24449;&#23398;&#20064;&#21644;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.13788</link><description>&lt;p&gt;
&#30452;&#26041;&#22270;&#23618;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34987;&#21160;&#22768;&#32435;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Histogram Layer Time Delay Neural Networks for Passive Sonar Classification. (arXiv:2307.13788v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#26041;&#22270;&#23618;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#27700;&#19979;&#22768;&#32435;&#30446;&#26631;&#35782;&#21035;&#12290;&#36890;&#36807;&#24341;&#20837;&#32479;&#35745;&#32972;&#26223;&#65292;&#35813;&#26041;&#27861;&#22312;&#29305;&#24449;&#23398;&#20064;&#21644;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36828;&#31243;&#28023;&#27915;&#24863;&#24212;&#36816;&#20316;&#20013;&#65292;&#27700;&#19979;&#22768;&#32435;&#30446;&#26631;&#26816;&#27979;&#38754;&#20020;&#30528;&#22797;&#26434;&#30340;&#22768;&#27874;&#20256;&#25773;&#25361;&#25112;&#12290;&#23613;&#31649;&#26377;&#21487;&#38752;&#30340;&#22768;&#32435;&#31995;&#32479;&#65292;&#30446;&#26631;&#35782;&#21035;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#21508;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#25913;&#36827;&#30340;&#30446;&#26631;&#35782;&#21035;&#38382;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24456;&#38590;&#35299;&#24320;&#35266;&#27979;&#30446;&#26631;&#24405;&#38899;&#20013;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#21644;&#30452;&#26041;&#22270;&#23618;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#32479;&#35745;&#32972;&#26223;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#27700;&#19979;&#22768;&#32435;&#30446;&#26631;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#24341;&#20837;&#32479;&#35745;&#32972;&#26223;&#23545;&#20110;&#34987;&#21160;&#22768;&#32435;&#30446;&#26631;&#35782;&#21035;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underwater acoustic target detection in remote marine sensing operations is challenging due to complex sound wave propagation. Despite the availability of reliable sonar systems, target recognition remains a difficult problem. Various methods address improved target recognition. However, most struggle to disentangle the high-dimensional, non-linear patterns in the observed target recordings. In this work, a novel method combines a time delay neural network and histogram layer to incorporate statistical contexts for improved feature learning and underwater acoustic target classification. The proposed method outperforms the baseline model, demonstrating the utility in incorporating statistical contexts for passive sonar target recognition. The code for this work is publicly available.
&lt;/p&gt;</description></item><item><title>GANfather&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#38450;&#24481;&#31995;&#32479;&#30340;&#21487;&#25511;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24694;&#24847;&#34892;&#20026;&#23646;&#24615;&#20294;&#19981;&#38656;&#35201;&#26631;&#35760;&#35201;&#27714;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#39640;&#23545;&#38750;&#27861;&#27963;&#21160;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24182;&#20462;&#27491;&#38450;&#24481;&#24615;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.13787</link><description>&lt;p&gt;
GANfather&#65306;&#36890;&#36807;&#29983;&#25104;&#24694;&#24847;&#34892;&#20026;&#26469;&#25913;&#36827;&#38450;&#24481;&#31995;&#32479;&#30340;&#21487;&#25511;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The GANfather: Controllable generation of malicious activity to improve defence systems. (arXiv:2307.13787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13787
&lt;/p&gt;
&lt;p&gt;
GANfather&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#38450;&#24481;&#31995;&#32479;&#30340;&#21487;&#25511;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24694;&#24847;&#34892;&#20026;&#23646;&#24615;&#20294;&#19981;&#38656;&#35201;&#26631;&#35760;&#35201;&#27714;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#39640;&#23545;&#38750;&#27861;&#27963;&#21160;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24182;&#20462;&#27491;&#38450;&#24481;&#24615;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#36741;&#21161;&#38450;&#24481;&#31995;&#32479;&#26816;&#27979;&#24694;&#24847;&#34892;&#20026;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#36825;&#31181;&#26631;&#35760;&#25968;&#25454;&#19981;&#21487;&#33719;&#21462;&#25110;&#19981;&#23436;&#25972;&#12290;&#23454;&#38469;&#19978;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20302;&#26816;&#27979;&#29575;&#21644;&#39640;&#35823;&#25253;&#29575;&#65292;&#20363;&#22914;&#21453;&#27927;&#38065;&#31995;&#32479;&#12290;&#25454;&#20272;&#35745;&#65292;&#27599;&#24180;&#26377;1.7-4&#19975;&#20159;&#27431;&#20803;&#27927;&#38065;&#26410;&#34987;&#21457;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GANfather&#65292;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#24694;&#24847;&#34892;&#20026;&#23646;&#24615;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26631;&#35760;&#35201;&#27714;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#20856;&#22411;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#25439;&#22833;&#19978;&#24341;&#20837;&#39069;&#22806;&#30340;&#30446;&#26631;&#26469;&#22870;&#21169;&#29983;&#25104;&#24694;&#24847;&#26679;&#26412;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36776;&#21035;&#22120;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#19988;&#24378;&#22823;&#30340;&#38450;&#24481;&#31995;&#32479;&#65292;&#25552;&#39640;&#23545;&#38750;&#27861;&#27963;&#21160;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;&#21487;&#36873;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#40723;&#21169;&#29983;&#25104;&#22120;&#36234;&#36807;&#29616;&#26377;&#30340;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#31181;&#35774;&#32622;&#20250;&#26292;&#38706;&#20986;&#36776;&#21035;&#22120;&#30340;&#38450;&#24481;&#24615;&#24369;&#28857;&#20197;&#36827;&#34892;&#20462;&#27491;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods to aid defence systems in detecting malicious activity typically rely on labelled data. In some domains, such labelled data is unavailable or incomplete. In practice this can lead to low detection rates and high false positive rates, which characterise for example anti-money laundering systems. In fact, it is estimated that 1.7--4 trillion euros are laundered annually and go undetected. We propose The GANfather, a method to generate samples with properties of malicious activity, without label requirements. We propose to reward the generation of malicious samples by introducing an extra objective to the typical Generative Adversarial Networks (GANs) loss. Ultimately, our goal is to enhance the detection of illicit activity using the discriminator network as a novel and robust defence system. Optionally, we may encourage the generator to bypass pre-existing detection systems. This setup then reveals defensive weaknesses for the discriminator to correct. We evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28155;&#21152;&#39044;&#35757;&#32451;&#27169;&#22359;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13771</link><description>&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#20013;&#30340;&#20934;&#30830;&#24615;&#22686;&#24378;&#65306;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach. (arXiv:2307.13771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28155;&#21152;&#39044;&#35757;&#32451;&#27169;&#22359;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#20405;&#29359;&#20010;&#20154;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21487;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20445;&#30041;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22359;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20844;&#24320;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#38598;&#21644;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;&#39044;&#35757;&#32451;&#27169;&#22359;&#26174;&#33879;&#25552;&#39640;&#20102;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.
&lt;/p&gt;</description></item><item><title>ClusterSeq&#26159;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20803;&#23398;&#20064;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#25552;&#39640;&#20102;&#29289;&#21697;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20445;&#30041;&#20102;&#27425;&#35201;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#21033;&#29992;&#20102;&#21516;&#19968;&#32858;&#31867;&#20013;&#29992;&#25143;&#30340;&#38598;&#20307;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.13766</link><description>&lt;p&gt;
ClusterSeq: &#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#20803;&#23398;&#20064;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning. (arXiv:2307.13766v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13766
&lt;/p&gt;
&lt;p&gt;
ClusterSeq&#26159;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20803;&#23398;&#20064;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#25552;&#39640;&#20102;&#29289;&#21697;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20445;&#30041;&#20102;&#27425;&#35201;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#21033;&#29992;&#20102;&#21516;&#19968;&#32858;&#31867;&#20013;&#29992;&#25143;&#30340;&#38598;&#20307;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20102;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#36825;&#26159;&#30001;&#20110;&#26377;&#38480;&#30340;&#20132;&#20114;&#20351;&#24471;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#23558;&#20803;&#23398;&#20064;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#20391;&#20449;&#24687;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#21160;&#24577;&#26041;&#38754;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23637;&#29616;&#20986;&#19982;&#26356;&#24120;&#35265;&#25110;&#8220;&#20027;&#35201;&#29992;&#25143;&#8221;&#19981;&#21516;&#20559;&#22909;&#30340;&#8220;&#27425;&#35201;&#29992;&#25143;&#8221;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ClusterSeq&#65292;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20803;&#23398;&#20064;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#12290;ClusterSeq&#21033;&#29992;&#29992;&#25143;&#24207;&#21015;&#20013;&#30340;&#21160;&#24577;&#20449;&#24687;&#26469;&#25552;&#39640;&#29289;&#21697;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#27809;&#26377;&#20391;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#20445;&#30041;&#20102;&#27425;&#35201;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#20250;&#34987;&#20027;&#35201;&#29992;&#25143;&#25152;&#25513;&#30422;&#65292;&#24182;&#21033;&#29992;&#20102;&#21516;&#19968;&#32858;&#31867;&#20013;&#29992;&#25143;&#30340;&#38598;&#20307;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical scenarios, the effectiveness of sequential recommendation systems is hindered by the user cold-start problem, which arises due to limited interactions for accurately determining user preferences. Previous studies have attempted to address this issue by combining meta-learning with user and item-side information. However, these approaches face inherent challenges in modeling user preference dynamics, particularly for "minor users" who exhibit distinct preferences compared to more common or "major users." To overcome these limitations, we present a novel approach called ClusterSeq, a Meta-Learning Clustering-Based Sequential Recommender System. ClusterSeq leverages dynamic information in the user sequence to enhance item prediction accuracy, even in the absence of side information. This model preserves the preferences of minor users without being overshadowed by major users, and it capitalizes on the collective knowledge of users within the same cluster. Extensive experiment
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#65292;&#35813;&#26041;&#27861;&#22312; Anomaly Detection benchmark &#20013;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>http://arxiv.org/abs/2307.13763</link><description>&lt;p&gt;
&#38544;&#24335;&#24402;&#19968;&#21270;&#26174;&#24335;&#27491;&#21017;&#21270;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Implicitly Normalized Explicitly Regularized Density Estimation. (arXiv:2307.13763v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13763
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#65292;&#35813;&#26041;&#27861;&#22312; Anomaly Detection benchmark &#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#26680;&#23494;&#24230;&#20272;&#35745;&#26377;&#26126;&#26174;&#24046;&#24322;&#65292;&#21487;&#20197;&#28165;&#26224;&#35299;&#37322;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;&#34429;&#28982;&#25105;&#20204;&#26080;&#27861;&#24471;&#21040;&#30456;&#20851;&#26680;&#20989;&#25968;&#30340;&#38381;&#21512;&#35299;&#26512;&#24418;&#24335;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#37319;&#26679;&#36827;&#34892;&#36817;&#20284;&#12290;&#20915;&#23450;&#23494;&#24230;&#30340;&#20248;&#21270;&#38382;&#39064;&#26159;&#38750;&#20984;&#30340;&#65292;&#26631;&#20934;&#30340;&#26799;&#24230;&#26041;&#27861;&#25928;&#26524;&#19981;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#35813;&#26041;&#27861;&#25552;&#20379;&#30340;&#26159;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65292;&#26080;&#27861;&#20351;&#29992;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#37319;&#29992;&#22522;&#20110; Fisher &#25955;&#24230;&#30340;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214; ADBench &#19978;&#35780;&#20272;&#20102;&#24471;&#21040;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#36229;&#36807;15&#20010;&#31639;&#27861;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 al
&lt;/p&gt;</description></item><item><title>UPREVE&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#31471;&#21040;&#31471;&#22240;&#26524;&#21457;&#29616;&#35780;&#20272;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#29992;&#25143;&#21516;&#26102;&#36816;&#34892;&#22810;&#20010;&#31639;&#27861;&#65292;&#21487;&#35270;&#21270;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#22270;&#30340;&#20934;&#30830;&#24615;&#65292;&#26088;&#22312;&#20351;&#22240;&#26524;&#21457;&#29616;&#23545;&#31038;&#20250;&#35745;&#31639;&#21644;&#34892;&#20026;&#25991;&#21270;&#24314;&#27169;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#29992;&#25143;&#21451;&#22909;&#65292;&#20197;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#36827;&#34892;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.13757</link><description>&lt;p&gt;
UPREVE: &#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22240;&#26524;&#21457;&#29616;&#35780;&#20272;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UPREVE: An End-to-End Causal Discovery Benchmarking System. (arXiv:2307.13757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13757
&lt;/p&gt;
&lt;p&gt;
UPREVE&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#31471;&#21040;&#31471;&#22240;&#26524;&#21457;&#29616;&#35780;&#20272;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#29992;&#25143;&#21516;&#26102;&#36816;&#34892;&#22810;&#20010;&#31639;&#27861;&#65292;&#21487;&#35270;&#21270;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#22270;&#30340;&#20934;&#30830;&#24615;&#65292;&#26088;&#22312;&#20351;&#22240;&#26524;&#21457;&#29616;&#23545;&#31038;&#20250;&#35745;&#31639;&#21644;&#34892;&#20026;&#25991;&#21270;&#24314;&#27169;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#29992;&#25143;&#21451;&#22909;&#65292;&#20197;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#36827;&#34892;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#31038;&#20250;&#34892;&#20026;&#31995;&#32479;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#26159;&#20026;&#20102;&#26126;&#26234;&#20915;&#31574;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Upload, PREprocess, Visualize&#21644;Evaluate (UPREVE)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#22522;&#20110;web&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#65292;&#26088;&#22312;&#31616;&#21270;&#22240;&#26524;&#21457;&#29616;&#36807;&#31243;&#12290;UPREVE&#20801;&#35768;&#29992;&#25143;&#21516;&#26102;&#36816;&#34892;&#22810;&#20010;&#31639;&#27861;&#65292;&#21487;&#35270;&#21270;&#22240;&#26524;&#20851;&#31995;&#65292;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#22270;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20854;&#26131;&#20110;&#35775;&#38382;&#30340;&#30028;&#38754;&#21644;&#21487;&#23450;&#21046;&#30340;&#21151;&#33021;&#65292;UPREVE&#20351;&#31038;&#20250;&#35745;&#31639;&#21644;&#34892;&#20026;&#25991;&#21270;&#24314;&#27169; (&#31561;) &#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#33021;&#22815;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26088;&#22312;&#20351;&#22240;&#26524;&#21457;&#29616;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#29992;&#25143;&#21451;&#22909;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#20197;&#36827;&#34892;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal relationships in complex socio-behavioral systems is challenging but essential for informed decision-making. We present Upload, PREprocess, Visualize, and Evaluate (UPREVE), a user-friendly web-based graphical user interface (GUI) designed to simplify the process of causal discovery. UPREVE allows users to run multiple algorithms simultaneously, visualize causal relationships, and evaluate the accuracy of learned causal graphs. With its accessible interface and customizable features, UPREVE empowers researchers and practitioners in social computing and behavioral-cultural modeling (among others) to explore and understand causal relationships effectively. Our proposed solution aims to make causal discovery more accessible and user-friendly, enabling users to gain valuable insights for better decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25512;&#26029;&#31232;&#30095;&#26102;&#38388;&#21464;&#21270;&#30340;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;&#38382;&#39064;&#12290;&#36890;&#36807;&#31163;&#24320;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#33539;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#27714;&#35299;&#25152;&#26377;&#31232;&#30095;&#27700;&#24179;&#30340;&#35299;&#36335;&#24452;&#65292;&#24182;&#24471;&#21040;&#29702;&#24819;&#30340;&#32479;&#35745;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.13750</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#21270;&#30340;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;&#30340;&#35299;&#36335;&#24452;&#19982;&#31163;&#25955;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Solution Path of Time-varying Markov Random Fields with Discrete Regularization. (arXiv:2307.13750v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25512;&#26029;&#31232;&#30095;&#26102;&#38388;&#21464;&#21270;&#30340;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;&#38382;&#39064;&#12290;&#36890;&#36807;&#31163;&#24320;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#33539;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#27714;&#35299;&#25152;&#26377;&#31232;&#30095;&#27700;&#24179;&#30340;&#35299;&#36335;&#24452;&#65292;&#24182;&#24471;&#21040;&#29702;&#24819;&#30340;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21442;&#25968;&#19978;&#24212;&#29992;&#19981;&#21516;&#31163;&#25955;&#21644;&#26102;&#38388;&#27491;&#21017;&#21270;&#30340;&#25512;&#26029;&#31232;&#30095;&#26102;&#38388;&#21464;&#21270;&#30340;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;(MRF)&#38382;&#39064;&#12290;&#30001;&#20110;&#31163;&#25955;&#27491;&#21017;&#21270;&#30340;&#22797;&#26434;&#24615;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#25152;&#35859;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;(MLE)&#19982;&#25918;&#26494;&#27491;&#21017;&#21270;&#65292;&#26082;&#19981;&#33021;&#24471;&#21040;&#29702;&#24819;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#20063;&#19981;&#33021;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#23610;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31163;&#24320;MLE&#30340;&#33539;&#24335;&#65292;&#36716;&#32780;&#27714;&#35299;&#19968;&#31867;&#20855;&#26377;&#31934;&#30830;&#12289;&#31163;&#25955;&#27491;&#21017;&#21270;&#30340;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#20419;&#36827;&#20272;&#35745;&#21442;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#20844;&#24335;&#20855;&#26377;&#38750;&#20984;&#21644;&#31163;&#25955;&#30340;&#29305;&#28857;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#23427;&#21487;&#20197;&#39640;&#25928;&#19988;&#21442;&#25968;&#21270;&#22320;&#27714;&#35299;&#25152;&#26377;&#31232;&#30095;&#27700;&#24179;&#30340;&#26102;&#38388;&#21464;&#21270;MRF&#30340;&#35299;&#36335;&#24452;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#25152;&#26377;&#31232;&#30095;&#27700;&#24179;&#30340;&#26102;&#38388;&#21464;&#21270;MRF&#30340;&#25972;&#20010;&#35299;&#36335;&#24452;&#21487;&#20197;&#22312;O(pT^3)&#30340;&#22797;&#26434;&#24230;&#19979;&#24471;&#21040;&#65292;&#20854;&#20013;T&#26159;&#26102;&#38388;&#30340;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
We study the problem of inferring sparse time-varying Markov random fields (MRFs) with different discrete and temporal regularizations on the parameters. Due to the intractability of discrete regularization, most approaches for solving this problem rely on the so-called maximum-likelihood estimation (MLE) with relaxed regularization, which neither results in ideal statistical properties nor scale to the dimensions encountered in realistic settings. In this paper, we address these challenges by departing from the MLE paradigm and resorting to a new class of constrained optimization problems with exact, discrete regularization to promote sparsity in the estimated parameters. Despite the nonconvex and discrete nature of our formulation, we show that it can be solved efficiently and parametrically for all sparsity levels. More specifically, we show that the entire solution path of the time-varying MRF for all sparsity levels can be obtained in $\mathcal{O}(pT^3)$, where $T$ is the number o
&lt;/p&gt;</description></item><item><title>mL-BFGS&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;L-BFGS&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#37327;&#26041;&#26696;&#21644;&#20943;&#23569;Hessian&#20013;&#30340;&#38543;&#26426;&#22122;&#22768;&#65292;&#31283;&#23450;&#20102;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.13744</link><description>&lt;p&gt;
mL-BFGS:&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;L-BFGS&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization. (arXiv:2307.13744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13744
&lt;/p&gt;
&lt;p&gt;
mL-BFGS&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;L-BFGS&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#37327;&#26041;&#26696;&#21644;&#20943;&#23569;Hessian&#20013;&#30340;&#38543;&#26426;&#22122;&#22768;&#65292;&#31283;&#23450;&#20102;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;Hessian&#30456;&#20851;&#35745;&#31639;&#20013;&#30340;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#21644;&#38543;&#26426;&#35757;&#32451;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25311;&#29275;&#39039;&#26041;&#27861;&#20173;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#24050;&#30693;&#30340;&#39640;&#25928;&#36817;&#20284;Hessian&#30340;L-BFGS&#26041;&#27861;&#65292;&#22312;&#38543;&#26426;&#35757;&#32451;&#20013;&#20250;&#20986;&#29616;&#25910;&#25947;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#23558;L-BFGS&#36866;&#24212;&#20110;&#22823;&#35268;&#27169;&#38543;&#26426;&#35757;&#32451;&#30340;&#23581;&#35797;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#39069;&#22806;&#24320;&#38144;&#65292;&#36825;&#25269;&#28040;&#20102;&#20854;&#22312;&#23454;&#38469;&#26102;&#38388;&#19978;&#30340;&#25910;&#25947;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;mL-BFGS&#31639;&#27861;&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#22522;&#20110;&#21160;&#37327;&#30340;L-BFGS&#31639;&#27861;&#65292;&#20026;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20248;&#21270;&#20013;&#30340;&#25311;&#29275;&#39039;&#26041;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;mL-BFGS&#23558;&#36817;&#20046;&#20813;&#36153;&#30340;&#21160;&#37327;&#26041;&#26696;&#24341;&#20837;&#21040;L-BFGS&#26356;&#26032;&#20013;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;Hessian&#20013;&#30340;&#38543;&#26426;&#22122;&#22768;&#65292;&#20174;&#32780;&#22312;&#38543;&#26426;&#20248;&#21270;&#36807;&#31243;&#20013;&#31283;&#23450;&#25910;&#25947;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#35757;&#32451;&#65292;mL-BFGS&#20351;&#29992;&#22359;&#36817;&#20284;Hessian&#12290;
&lt;/p&gt;
&lt;p&gt;
Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block
&lt;/p&gt;</description></item><item><title>FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13716</link><description>&lt;p&gt;
FedDRL: &#19968;&#31181;&#22522;&#20110;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13716
&lt;/p&gt;
&lt;p&gt;
FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#26679;&#26412;&#25968;&#37327;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#22266;&#23450;&#26435;&#37325;&#20540;&#26469;&#34701;&#21512;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#36129;&#29486;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26679;&#26412;&#37327;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23458;&#25143;&#31471;&#25925;&#24847;&#19978;&#20256;&#20302;&#36136;&#37327;&#25110;&#24694;&#24847;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#23558;&#20005;&#37325;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDRL&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36807;&#28388;&#25481;&#24694;&#24847;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#21487;&#20449;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#19982;&#27169;&#22411;&#34701;&#21512;&#12290;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;FedDRL&#31639;&#27861;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21487;&#20449;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#24182;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ShuttleNet&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#20987;&#29699;&#20449;&#24687;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#39044;&#27979;&#32701;&#27611;&#29699;&#20987;&#29699;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#22312;CoachAI&#32701;&#27611;&#29699;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2307.13715</link><description>&lt;p&gt;
&#12298;2023&#24180;&#25945;&#32451;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;8&#21442;&#21152;CoachAI&#32701;&#27611;&#29699;&#25361;&#25112;&#36187;&#65306;&#29992;&#20110;&#20987;&#29699;&#39044;&#27979;&#30340;&#39640;&#32423;ShuttleNet&#12299;
&lt;/p&gt;
&lt;p&gt;
Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions. (arXiv:2307.13715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ShuttleNet&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#20987;&#29699;&#20449;&#24687;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#39044;&#27979;&#32701;&#27611;&#29699;&#20987;&#29699;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#22312;CoachAI&#32701;&#27611;&#29699;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#20987;&#29699;&#26469;&#25913;&#36827;&#29616;&#26377;&#26694;&#26550;ShuttleNet&#22312;&#39044;&#27979;&#32701;&#27611;&#29699;&#20987;&#29699;&#31867;&#22411;&#21644;&#20301;&#32622;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21442;&#21152;&#20102;2023&#24180;IJCAI&#30340;CoachAI&#32701;&#27611;&#29699;&#25361;&#25112;&#36187;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#22522;&#20934;&#32447;&#30340;&#25104;&#32489;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#27604;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#24182;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, our objective is to improve the performance of the existing framework ShuttleNet in predicting badminton shot types and locations by leveraging past strokes. We participated in the CoachAI Badminton Challenge at IJCAI 2023 and achieved significantly better results compared to the baseline. Ultimately, our team achieved the first position in the competition and we made our code available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#38416;&#36848;&#20102;&#25511;&#21046;&#21644;&#30417;&#25511;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#28418;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25351;&#26631;&#29992;&#20110;&#23457;&#26597;&#27169;&#22411;&#22312;&#28508;&#22312;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13705</link><description>&lt;p&gt;
&#25511;&#21046;&#21644;&#30417;&#25511;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Control and Monitoring of Artificial Intelligence Algorithms. (arXiv:2307.13705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13705
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#38416;&#36848;&#20102;&#25511;&#21046;&#21644;&#30417;&#25511;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#28418;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25351;&#26631;&#29992;&#20110;&#23457;&#26597;&#27169;&#22411;&#22312;&#28508;&#22312;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#22312;&#37096;&#32626;&#21518;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#30417;&#27979;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20043;&#38388;&#30340;&#28508;&#22312;&#27874;&#21160;&#30340;&#37325;&#35201;&#24615;&#12290;&#35814;&#32454;&#35299;&#37322;&#20102;&#25968;&#25454;&#28418;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#22522;&#26412;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#21487;&#29992;&#20110;&#23457;&#26597;&#27169;&#22411;&#22312;&#28508;&#22312;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.13704</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#29616;&#22312;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#33021;&#22815;&#35299;&#37322;&#22797;&#26434;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;XAI&#29305;&#21035;&#36866;&#29992;&#20110;&#21361;&#38505;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#31867;&#30340;&#29983;&#21629;&#20381;&#36182;&#20110;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#12290;&#21307;&#30103;&#30740;&#31350;&#30340;&#19968;&#20010;&#39046;&#22495;&#26159;&#24180;&#40836;&#39044;&#27979;&#21644;&#34928;&#32769;&#21450;&#19982;&#24180;&#40836;&#30456;&#20851;&#30142;&#30149;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#37492;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;XAI&#30340;&#20316;&#29992;&#23578;&#26410;&#30452;&#25509;&#25506;&#35752;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#22120;&#23448;&#31995;&#32479;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;XAI&#22312;&#21307;&#30103;&#24212;&#29992;&#20197;&#21450;&#29305;&#21035;&#26159;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36827;&#34892;&#24544;&#23454;&#30340;&#8220;&#38142;&#24335;&#24605;&#32500;&#8221;&#25512;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23545;&#38142;&#24335;&#24605;&#32500;&#30340;&#20381;&#36182;&#31243;&#24230;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#26356;&#33021;&#21147;&#36234;&#24378;&#65292;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#25512;&#29702;&#36234;&#26469;&#36234;&#19981;&#24544;&#23454;&#12290;</title><link>http://arxiv.org/abs/2307.13702</link><description>&lt;p&gt;
&#27979;&#37327;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#20013;&#30340;&#24544;&#35802;&#24230;
&lt;/p&gt;
&lt;p&gt;
Measuring Faithfulness in Chain-of-Thought Reasoning. (arXiv:2307.13702v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36827;&#34892;&#24544;&#23454;&#30340;&#8220;&#38142;&#24335;&#24605;&#32500;&#8221;&#25512;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23545;&#38142;&#24335;&#24605;&#32500;&#30340;&#20381;&#36182;&#31243;&#24230;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#26356;&#33021;&#21147;&#36234;&#24378;&#65292;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#25512;&#29702;&#36234;&#26469;&#36234;&#19981;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#65292;&#22914;&#26524;&#33021;&#22815;&#20135;&#29983;&#36880;&#27493;&#30340;&#8220;&#38142;&#24335;&#24605;&#32500;&#8221;&#25512;&#29702;&#65292;&#20854;&#34920;&#29616;&#20250;&#26356;&#22909;&#65292;&#20294;&#19981;&#28165;&#26970;&#25152;&#36848;&#30340;&#25512;&#29702;&#26159;&#21542;&#24544;&#23454;&#22320;&#35299;&#37322;&#20102;&#27169;&#22411;&#23454;&#38469;&#30340;&#25512;&#29702;&#36807;&#31243;&#65288;&#21363;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#24335;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#24403;&#20171;&#20837;&#38142;&#24335;&#24605;&#32500;&#26102;&#27169;&#22411;&#39044;&#27979;&#22914;&#20309;&#21457;&#29983;&#21464;&#21270;&#65288;&#20363;&#22914;&#65292;&#28155;&#21152;&#38169;&#35823;&#25110;&#25913;&#20889;&#23427;&#65289;&#65292;&#26469;&#30740;&#31350;&#38142;&#24335;&#24605;&#32500;&#21487;&#33021;&#19981;&#24544;&#23454;&#30340;&#20551;&#35774;&#12290;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#22312;&#39044;&#27979;&#31572;&#26696;&#26102;&#23545;&#38142;&#24335;&#24605;&#32500;&#30340;&#20381;&#36182;&#31243;&#24230;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#26377;&#26102;&#20250;&#20005;&#37325;&#20381;&#36182;&#38142;&#24335;&#24605;&#32500;&#65292;&#32780;&#20854;&#20182;&#26102;&#20505;&#21017;&#20027;&#35201;&#24573;&#35270;&#23427;&#12290;&#38142;&#24335;&#24605;&#32500;&#30340;&#24615;&#33021;&#25552;&#21319;&#20284;&#20046;&#19981;&#20165;&#20165;&#26469;&#33258;&#20110;&#20854;&#22686;&#21152;&#30340;&#27979;&#35797;&#35745;&#31639;&#37327;&#65292;&#20063;&#19981;&#20165;&#20165;&#26469;&#33258;&#20110;&#38142;&#24335;&#24605;&#32500;&#30340;&#29305;&#23450;&#25514;&#36766;&#25152;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#26356;&#26377;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#25512;&#29702;&#36234;&#26469;&#36234;&#19981;&#24544;&#23454;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#29615;&#22659;&#36866;&#24403;&#65292;&#38142;&#24335;&#24605;&#32500;&#21487;&#20197;&#26159;&#24544;&#23454;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\text{EFO}_{k}$-CQA&#26694;&#26550;&#65292;&#29992;&#20110;&#36229;&#36234;&#38598;&#21512;&#25805;&#20316;&#30340;&#30693;&#35782;&#22270;&#35889;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#26041;&#27861;&#35780;&#20272;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#29616;&#26377;&#26597;&#35810;&#31354;&#38388;&#12290;&#20351;&#29992;&#26500;&#24314;&#30340;$\text{EFO}_{k}$-CQA&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#26597;&#35810;&#38590;&#24230;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#35777;&#26126;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#36807;&#31243;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13701</link><description>&lt;p&gt;
$\text{EFO}_{k}$-CQA&#65306;&#36229;&#36234;&#38598;&#21512;&#25805;&#20316;&#30340;&#30693;&#35782;&#22270;&#35889;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
$\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation. (arXiv:2307.13701v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\text{EFO}_{k}$-CQA&#26694;&#26550;&#65292;&#29992;&#20110;&#36229;&#36234;&#38598;&#21512;&#25805;&#20316;&#30340;&#30693;&#35782;&#22270;&#35889;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#26041;&#27861;&#35780;&#20272;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#29616;&#26377;&#26597;&#35810;&#31354;&#38388;&#12290;&#20351;&#29992;&#26500;&#24314;&#30340;$\text{EFO}_{k}$-CQA&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#26597;&#35810;&#38590;&#24230;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#35777;&#26126;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#36807;&#31243;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22238;&#31572;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#38656;&#35201;&#23545;&#19981;&#23436;&#25972;&#30693;&#35782;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#22240;&#20026;&#23384;&#22312;&#30528;&#24320;&#25918;&#19990;&#30028;&#30340;&#20551;&#35774;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23545;&#26410;&#35266;&#23519;&#21040;&#30340;&#30693;&#35782;&#36827;&#34892;&#27867;&#21270;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#65292;&#19968;&#20010;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#33719;&#21462;&#21644;&#35780;&#20272;&#36825;&#26679;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#26041;&#27861;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#23384;&#22312;&#37327;&#21270;&#19968;&#38454;&#26597;&#35810;&#19982;&#22810;&#20010;&#21464;&#37327;($\text{EFO}_{k}$)&#30340;&#32452;&#21512;&#31354;&#38388;&#12290;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;&#32452;&#21512;&#26597;&#35810;&#31354;&#38388;&#26174;&#33879;&#25193;&#23637;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#36890;&#36807;&#38598;&#21512;&#25805;&#20316;&#23450;&#20041;&#30340;&#26597;&#35810;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;741&#31181;&#26597;&#35810;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;$\text{EFO}_{k}$-CQA&#65292;&#20197;&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#32467;&#26524;&#20026;&#29702;&#35299;&#26597;&#35810;&#38590;&#24230;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#36807;&#31243;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge is required due to the open-world assumption. Learning-based methods are essential because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables ($\text{EFO}_{k}$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset, $\text{EFO}_{k}$-CQA, with 741 types of query for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systema
&lt;/p&gt;</description></item><item><title>CAMP&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26495;&#29699;&#29699;&#21592;&#34920;&#29616;&#25351;&#26631;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#23545;&#25163;&#23454;&#21147;&#21644;&#27604;&#36187;&#29615;&#22659;&#31561;&#22240;&#32032;&#65292;&#21487;&#20197;&#37327;&#21270;&#20010;&#20307;&#29699;&#21592;&#23545;&#27604;&#36187;&#32467;&#26524;&#30340;&#36129;&#29486;&#12290;CAMP&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#20026;&#36873;&#25321;&#21644;&#33609;&#26696;&#12289;&#25945;&#32451;&#21644;&#35757;&#32451;&#12289;&#22242;&#38431;&#38453;&#23481;&#21644;&#25112;&#30053;&#21046;&#23450;&#25552;&#20379;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2307.13700</link><description>&lt;p&gt;
CAMP:&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26495;&#29699;&#29699;&#21592;&#34920;&#29616;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
CAMP: A Context-Aware Cricket Players Performance Metric. (arXiv:2307.13700v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13700
&lt;/p&gt;
&lt;p&gt;
CAMP&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26495;&#29699;&#29699;&#21592;&#34920;&#29616;&#25351;&#26631;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#23545;&#25163;&#23454;&#21147;&#21644;&#27604;&#36187;&#29615;&#22659;&#31561;&#22240;&#32032;&#65292;&#21487;&#20197;&#37327;&#21270;&#20010;&#20307;&#29699;&#21592;&#23545;&#27604;&#36187;&#32467;&#26524;&#30340;&#36129;&#29486;&#12290;CAMP&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#20026;&#36873;&#25321;&#21644;&#33609;&#26696;&#12289;&#25945;&#32451;&#21644;&#35757;&#32451;&#12289;&#22242;&#38431;&#38453;&#23481;&#21644;&#25112;&#30053;&#21046;&#23450;&#25552;&#20379;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26495;&#29699;&#26159;&#20165;&#27425;&#20110;&#36275;&#29699;&#22312;&#25910;&#35270;&#29575;&#19978;&#26368;&#21463;&#27426;&#36814;&#30340;&#36816;&#21160;&#12290;&#28982;&#32780;&#65292;&#23545;&#20010;&#20307;&#29699;&#21592;&#34920;&#29616;&#30340;&#35780;&#20272;&#65292;&#20316;&#20026;&#22242;&#38431;&#36816;&#21160;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#30446;&#21069;&#20027;&#35201;&#22522;&#20110;&#32508;&#21512;&#34920;&#29616;&#25968;&#25454;&#65292;&#21253;&#25324;&#24179;&#22343;&#24471;&#20998;&#21644;&#20987;&#29699;&#27425;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CAMP&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29699;&#21592;&#34920;&#29616;&#25351;&#26631;&#65292;&#29992;&#20110;&#34913;&#37327;&#20010;&#20307;&#29699;&#21592;&#23545;&#26495;&#29699;&#27604;&#36187;&#32467;&#26524;&#30340;&#36129;&#29486;&#12290;CAMP&#37319;&#29992;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#24182;&#20026;&#36873;&#25321;&#21644;&#33609;&#26696;&#12289;&#25945;&#32451;&#21644;&#35757;&#32451;&#12289;&#22242;&#38431;&#38453;&#23481;&#21644;&#25112;&#30053;&#21046;&#23450;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#12290;CAMP&#32435;&#20837;&#20102;&#34920;&#29616;&#30340;&#30830;&#20999;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23545;&#25163;&#30340;&#23454;&#21147;&#21644;&#27604;&#36187;&#30340;&#29305;&#23450;&#29615;&#22659;&#65292;&#20363;&#22914;&#21387;&#21147;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;2001&#24180;&#33267;2019&#24180;&#38388;&#30340;&#26377;&#38480;&#36807;&#24230;&#26495;&#29699;&#27604;&#36187;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#22312;&#27599;&#22330;&#27604;&#36187;&#20013;&#65292;&#19968;&#32452;&#19987;&#23478;&#23459;&#24067;&#19968;&#21517;&#29699;&#21592;&#20026;&#26368;&#20339;&#29699;&#21592;&#65292;&#34987;&#31216;&#20026;M}atch&#30340;&#26368;&#20339;&#29699;&#21592;&#65288;MoM&#65289;&#12290;&#36890;&#36807;CAMP&#35780;&#20272;&#65292;&#26368;&#39640;&#35780;&#20998;&#30340;&#20004;&#21517;&#29699;&#21592;&#19982;MoM&#21305;&#37197;&#30340;&#27010;&#29575;&#20026;83&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cricket is the second most popular sport after soccer in terms of viewership. However, the assessment of individual player performance, a fundamental task in team sports, is currently primarily based on aggregate performance statistics, including average runs and wickets taken. We propose Context-Aware Metric of player Performance, CAMP, to quantify individual players' contributions toward a cricket match outcome. CAMP employs data mining methods and enables effective data-driven decision-making for selection and drafting, coaching and training, team line-ups, and strategy development. CAMP incorporates the exact context of performance, such as opponents' strengths and specific circumstances of games, such as pressure situations. We empirically evaluate CAMP on data of limited-over cricket matches between 2001 and 2019. In every match, a committee of experts declares one player as the best player, called Man of the M}atch (MoM). The top two rated players by CAMP match with MoM in 83\% 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24425;&#31080;&#31080;&#25454;&#20551;&#35828;&#22312;&#31232;&#30095;&#32593;&#32476;&#24615;&#33021;&#26041;&#38754;&#30340;&#27934;&#23519;&#65292;&#21457;&#29616;&#20462;&#21098;&#30340;&#32593;&#32476;&#24615;&#33021;&#20250;&#38477;&#20302;&#65292;&#24182;&#19988;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#20135;&#29983;&#30340;&#27010;&#24565;&#21644;&#20687;&#32032;&#19982;&#21407;&#22987;&#32593;&#32476;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13698</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#26041;&#27861;&#25506;&#32034;&#24425;&#31080;&#31080;&#25454;&#20551;&#35828;&#65306;&#23545;&#31232;&#30095;&#32593;&#32476;&#24615;&#33021;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance. (arXiv:2307.13698v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24425;&#31080;&#31080;&#25454;&#20551;&#35828;&#22312;&#31232;&#30095;&#32593;&#32476;&#24615;&#33021;&#26041;&#38754;&#30340;&#27934;&#23519;&#65292;&#21457;&#29616;&#20462;&#21098;&#30340;&#32593;&#32476;&#24615;&#33021;&#20250;&#38477;&#20302;&#65292;&#24182;&#19988;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#20135;&#29983;&#30340;&#27010;&#24565;&#21644;&#20687;&#32032;&#19982;&#21407;&#22987;&#32593;&#32476;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26377;&#38480;&#23384;&#20648;&#33021;&#21147;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#39640;&#24615;&#33021;&#30340;&#31232;&#30095;&#32593;&#32476;&#23545;&#20110;&#22914;&#25163;&#26426;&#31561;&#35774;&#22791;&#38750;&#24120;&#26377;&#21033;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#22521;&#20859;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#24425;&#31080;&#31080;&#25454;&#20551;&#35828;&#65288;LTH&#65289;&#26159;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#25214;&#21040;&#19968;&#20010;&#19982;&#21407;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#40092;&#26377;&#30740;&#31350;&#25506;&#31350;LTH&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25104;&#36133;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20462;&#21098;&#30340;&#32593;&#32476;&#24615;&#33021;&#36880;&#28176;&#25552;&#39640;&#25110;&#38477;&#20302;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#20998;&#21035;&#20351;&#29992;Grad-CAM&#21644;&#21518;&#26399;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;PCBM&#65289;&#26469;&#32771;&#23519;&#20462;&#21098;&#32593;&#32476;&#22312;&#20687;&#32032;&#21644;&#39640;&#32423;&#27010;&#24565;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#38543;&#30528;&#26435;&#37325;&#20462;&#21098;&#30340;&#22686;&#22810;&#65292;&#32593;&#32476;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#20174;&#20462;&#21098;&#30340;&#32593;&#32476;&#20013;&#21457;&#29616;&#30340;&#27010;&#24565;&#21644;&#20687;&#32032;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering a high-performing sparse network within a massive neural network is advantageous for deploying them on devices with limited storage, such as mobile phones. Additionally, model explainability is essential to fostering trust in AI. The Lottery Ticket Hypothesis (LTH) finds a network within a deep network with comparable or superior performance to the original model. However, limited study has been conducted on the success or failure of LTH in terms of explainability. In this work, we examine why the performance of the pruned networks gradually increases or decreases. Using Grad-CAM and Post-hoc concept bottleneck models (PCBMs), respectively, we investigate the explainability of pruned networks in terms of pixels and high-level concepts. We perform extensive experiments across vision and medical imaging datasets. As more weights are pruned, the performance of the network degrades. The discovered concepts and pixels from the pruned networks are inconsistent with the original n
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LtC&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26469;&#26377;&#25928;&#20943;&#23567;&#27969;&#23186;&#20307;&#35270;&#39057;&#30340;&#27969;&#37327;&#65292;&#25552;&#39640;&#27969;&#23186;&#20307;&#35270;&#39057;&#20998;&#26512;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12171</link><description>&lt;p&gt;
&#23398;&#20064;&#21387;&#32553;&#65288;LtC&#65289;&#65306;&#39640;&#25928;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27969;&#23186;&#20307;&#35270;&#39057;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics. (arXiv:2307.12171v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LtC&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26469;&#26377;&#25928;&#20943;&#23567;&#27969;&#23186;&#20307;&#35270;&#39057;&#30340;&#27969;&#37327;&#65292;&#25552;&#39640;&#27969;&#23186;&#20307;&#35270;&#39057;&#20998;&#26512;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#36890;&#24120;&#20316;&#20026;&#36793;&#32536;&#35774;&#32622;&#20013;&#30340;&#20113;&#26381;&#21153;&#36827;&#34892;&#65292;&#20027;&#35201;&#26159;&#20026;&#20102;&#21368;&#36733;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#22312;&#35270;&#39057;&#20256;&#24863;&#22120;&#19981;&#30452;&#25509;&#20351;&#29992;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#20174;&#36793;&#32536;&#35774;&#22791;&#21457;&#36865;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#25968;&#25454;&#22312;&#24102;&#23485;&#21644;&#21151;&#32791;&#26041;&#38754;&#37117;&#20250;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#26500;&#24314;&#19968;&#20010;&#39640;&#25928;&#21033;&#29992;&#36825;&#20123;&#36164;&#28304;&#30340;&#27969;&#23186;&#20307;&#35270;&#39057;&#20998;&#26512;&#27969;&#27700;&#32447;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#20943;&#23567;&#35270;&#39057;&#27969;&#30340;&#22823;&#23567;&#12290;&#20256;&#32479;&#30340;&#35270;&#39057;&#21387;&#32553;&#31639;&#27861;&#23545;&#35270;&#39057;&#30340;&#35821;&#20041;&#19981;&#25935;&#24863;&#65292;&#21487;&#33021;&#26082;&#20302;&#25928;&#21448;&#23545;&#20998;&#26512;&#24615;&#33021;&#26377;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LtC&#65292;&#36825;&#26159;&#35270;&#39057;&#28304;&#21644;&#20998;&#26512;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22312;&#20998;&#26512;&#27969;&#27700;&#32447;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#26469;&#20943;&#23567;&#35270;&#39057;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LtC&#23558;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#21151;&#33021;&#20998;&#26512;&#31639;&#27861;&#20316;&#20026;&#25945;&#24072;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23398;&#29983;&#31070;&#32463;&#32593;&#32476;&#65292;&#28982;&#21518;&#37096;&#32626;&#22312;&#35270;&#39057;&#28304;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics are often performed as cloud services in edge settings, mainly to offload computation, and also in situations where the results are not directly consumed at the video sensors. Sending high-quality video data from the edge devices can be expensive both in terms of bandwidth and power use. In order to build a streaming video analytics pipeline that makes efficient use of these resources, it is therefore imperative to reduce the size of the video stream. Traditional video compression algorithms are unaware of the semantics of the video, and can be both inefficient and harmful for the analytics performance. In this paper, we introduce LtC, a collaborative framework between the video source and the analytics server, that efficiently learns to reduce the video streams within an analytics pipeline. Specifically, LtC uses the full-fledged analytics algorithm at the server as a teacher to train a lightweight student neural network, which is then deployed at the video source. The
&lt;/p&gt;</description></item><item><title>AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.10711</link><description>&lt;p&gt;
AdjointDPM: &#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10711
&lt;/p&gt;
&lt;p&gt;
AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23450;&#21046;&#21270;&#26041;&#27861;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#26679;&#20363;&#26469;&#23558;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#27010;&#24565;&#23545;&#40784;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#24403;&#21807;&#19968;&#21487;&#29992;&#30340;&#30417;&#30563;&#26159;&#23450;&#20041;&#22312;&#29983;&#25104;&#20869;&#23481;&#19978;&#30340;&#21487;&#24494;&#24230;&#37327;&#26102;&#30340;DPM&#23450;&#21046;&#21270;&#25361;&#25112;&#12290;&#30001;&#20110;DPM&#30340;&#37319;&#26679;&#36807;&#31243;&#28041;&#21450;&#23545;&#21435;&#22122;UNet&#30340;&#36882;&#24402;&#35843;&#29992;&#65292;&#26420;&#32032;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#23384;&#20648;&#25152;&#26377;&#36845;&#20195;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#23548;&#33268;&#20869;&#23384;&#28040;&#32791;&#26497;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;AdjointDPM&#65292;&#39318;&#20808;&#36890;&#36807;&#27714;&#35299;&#30456;&#24212;&#30340;&#27010;&#29575;&#27969;ODE&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#28982;&#21518;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#36890;&#36807;&#27714;&#35299;&#21478;&#19968;&#20010;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;(&#21253;&#25324;&#35843;&#21046;&#20449;&#21495;&#12289;&#32593;&#32476;&#26435;&#37325;&#21644;&#21021;&#22987;&#22122;&#22768;)&#12290;&#20026;&#20102;&#20943;&#23569;&#27491;&#21521;&#29983;&#25104;&#21644;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#25968;&#20540;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#36825;&#19968;&#26032;&#26041;&#21521;&#65292;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#38754;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#21364;&#26263;&#20013;&#36827;&#34892;&#38544;&#34255;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2307.10569</link><description>&lt;p&gt;
&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deceptive Alignment Monitoring. (arXiv:2307.10569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#36825;&#19968;&#26032;&#26041;&#21521;&#65292;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#38754;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#21364;&#26263;&#20013;&#36827;&#34892;&#38544;&#34255;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#20197;&#21450;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#33258;&#27835;&#26435;&#19981;&#26029;&#25193;&#22823;&#65292;&#19968;&#20010;&#26032;&#30340;&#23545;&#25163;&#20986;&#29616;&#20102;&#65306;&#27169;&#22411;&#26412;&#36523;&#12290;&#19968;&#20010;&#27169;&#22411;&#30475;&#20284;&#21512;&#29702;&#22320;&#34892;&#20026;&#65292;&#21364;&#26263;&#20013;&#12289;&#24494;&#22937;&#22320;&#20462;&#25913;&#20854;&#34892;&#20026;&#20197;&#36798;&#21040;&#21035;&#30340;&#30446;&#30340;&#30340;&#23041;&#32961;&#65292;&#36890;&#24120;&#22312;AI&#23433;&#20840;&#19982;&#23545;&#40784;&#31038;&#21306;&#20013;&#34987;&#31216;&#20026;&#27450;&#39575;&#24615;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#26041;&#21521;&#31216;&#20026;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26426;&#22120;&#23398;&#20064;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#26041;&#21521;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#23545;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#19988;&#32039;&#23494;&#30456;&#20851;&#65292;&#24182;&#19988;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#39046;&#22495;&#30340;&#36827;&#27493;&#26082;&#25552;&#20986;&#20102;&#38271;&#26399;&#25361;&#25112;&#65292;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21628;&#21505;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#26356;&#22810;&#22320;&#21442;&#19982;&#36825;&#20123;&#26032;&#20852;&#26041;&#21521;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety &amp; Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
&lt;/p&gt;</description></item><item><title>TimeTuner&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#27169;&#22411;&#34892;&#20026;&#19982;&#26102;&#38388;&#34920;&#31034;&#30340;&#20851;&#31995;&#65292;&#24182;&#35299;&#20915;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09916</link><description>&lt;p&gt;
TimeTuner: &#35786;&#26029;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26102;&#38388;&#34920;&#31034;&#30340;&#23545;&#29031;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations. (arXiv:2307.09916v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09916
&lt;/p&gt;
&lt;p&gt;
TimeTuner&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#27169;&#22411;&#34892;&#20026;&#19982;&#26102;&#38388;&#34920;&#31034;&#30340;&#20851;&#31995;&#65292;&#24182;&#35299;&#20915;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#35768;&#22810;&#21162;&#21147;&#33268;&#21147;&#20110;&#35774;&#35745;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#24448;&#24448;&#24402;&#22240;&#20110;&#26377;&#25928;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20419;&#36827;&#20102;&#29305;&#24449;&#24037;&#31243;&#21644;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#34701;&#20837;&#20808;&#39564;&#30693;&#35782;&#12289;&#35782;&#21035;&#21464;&#37327;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#20197;&#30830;&#20445;&#27169;&#22411;&#21487;&#38752;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#21363;TimeTuner&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#19982;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#30340;&#23616;&#37096;&#30456;&#20851;&#24615;&#12289;&#24179;&#31283;&#24615;&#21644;&#31890;&#24230;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#35813;&#31995;&#32479;&#20027;&#35201;&#21253;&#25324;&#20197;&#19979;&#20004;&#20010;&#38454;&#27573;&#25216;&#26415;&#65306;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#23545;&#29031;&#35299;&#37322;&#26469;&#24314;&#31435;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39640;&#20445;&#30495;&#24230;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#20013;&#30340;&#30913;&#22330;&#25299;&#25169;&#36827;&#34892;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22320;&#29699;&#30913;&#23618;&#27169;&#25311;&#20013;&#65292;&#26088;&#22312;&#25361;&#25112;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#36890;&#36807;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#24191;&#27867;&#28508;&#21147;&#24433;&#21709;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09469</link><description>&lt;p&gt;
&#23545;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#39640;&#20445;&#30495;&#24230;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#20013;&#30913;&#22330;&#25299;&#25169;&#30340;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications. (arXiv:2307.09469v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39640;&#20445;&#30495;&#24230;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#20013;&#30340;&#30913;&#22330;&#25299;&#25169;&#36827;&#34892;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22320;&#29699;&#30913;&#23618;&#27169;&#25311;&#20013;&#65292;&#26088;&#22312;&#25361;&#25112;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#36890;&#36807;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#24191;&#27867;&#28508;&#21147;&#24433;&#21709;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27169;&#25311;&#31561;&#31163;&#23376;&#20307;&#20013;&#30340;&#30913;&#22330;&#36827;&#34892;&#25299;&#25169;&#20998;&#26512;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#30740;&#31350;&#22810;&#31181;&#29289;&#29702;&#29616;&#35937;&#12290;&#20854;&#20013;&#19968;&#20010;&#24212;&#29992;&#26159;&#30913;&#37325;&#26032;&#36830;&#25509;&#65292;&#36825;&#26159;&#19982;&#30913;&#22330;&#25299;&#25169;&#21160;&#21147;&#23398;&#30456;&#20851;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#24456;&#38590;&#26816;&#27979;&#21644;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;&#19977;&#32500;&#30913;&#22330;&#30690;&#37327;&#22330;&#36827;&#34892;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#21644;&#26102;&#31354;&#22270;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#22320;&#29699;&#30913;&#23618;&#30340;&#27169;&#25311;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#30001;Vlasiator&#20135;&#29983;&#30340;&#65292;Vlasiator&#26159;&#19968;&#20010;&#22522;&#20110;Vlasov&#29702;&#35770;&#30340;&#36229;&#32423;&#35745;&#31639;&#26426;&#35268;&#27169;&#30340;&#36817;&#22320;&#31354;&#38388;&#27169;&#25311;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#25361;&#25112;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#19968;&#20010;&#22312;&#31185;&#23398;&#19978;&#20855;&#26377;&#24191;&#27867;&#28508;&#21147;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological analysis of the magnetic field in simulated plasmas allows the study of various physical phenomena in a wide range of settings. One such application is magnetic reconnection, a phenomenon related to the dynamics of the magnetic field topology, which is difficult to detect and characterize in three dimensions. We propose a scalable pipeline for topological data analysis and spatiotemporal graph representation of three-dimensional magnetic vector fields. We demonstrate our methods on simulations of the Earth's magnetosphere produced by Vlasiator, a supercomputer-scale Vlasov theory-based simulation for near-Earth space. The purpose of this work is to challenge the machine learning community to explore graph-based machine learning approaches to address a largely open scientific problem with wide-ranging potential impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prompt Generate Train (PGT)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;&#12289;&#23545;&#40784;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#24182;&#22312;&#29983;&#25104;&#30456;&#20851;&#31572;&#26696;&#26041;&#38754;&#20855;&#26377;&#19982;&#22522;&#20110;GPT-4&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05915</link><description>&lt;p&gt;
Prompt Generate Train (PGT): &#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;&#12289;&#23545;&#40784;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering. (arXiv:2307.05915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05915
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prompt Generate Train (PGT)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;&#12289;&#23545;&#40784;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#24182;&#22312;&#29983;&#25104;&#30456;&#20851;&#31572;&#26696;&#26041;&#38754;&#20855;&#26377;&#19982;&#22522;&#20110;GPT-4&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Prompt, Generate, Train (PGT) &#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#24320;&#21457;&#19968;&#20010;&#38024;&#23545;&#19987;&#26377;&#30340;&#25991;&#26412;&#25991;&#26723;&#38598;&#21512;&#36827;&#34892;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23558;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#23545;&#40784;&#12289;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#30456;&#20851;&#31572;&#26696;&#26102;&#20855;&#26377;&#19982;&#22522;&#20110; GPT-4 &#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26381;&#21153;&#25104;&#26412;&#26356;&#20302;&#12290;&#36890;&#36807;&#20351;&#29992;&#20013;&#31561;&#35268;&#27169;&#30340;LLM (Flan-T5 XXL) &#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#26696;&#65292;&#21512;&#25104;&#29983;&#25104;&#31649;&#36947;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#31649;&#36947;&#26088;&#22312;&#29983;&#25104;&#28085;&#30422;&#25972;&#20010;&#35821;&#26009;&#24211;&#30340;&#25277;&#35937;&#21644;&#25552;&#21462;&#24335;&#38382;&#39064;&#12290;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#65292;&#35813;&#26694;&#26550;&#23545;&#19968;&#20010;&#30001;&#31264;&#23494;&#26816;&#32034;&#22120;&#21644;&#36739;&#23567;&#35268;&#27169;&#30340;LLM&#32452;&#25104;&#30340;&#36739;&#23567;&#30340;RAG&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework - Prompt, Generate, Train (PGT) - to efficiently develop a generative question-answering model for open-book question-answering over a proprietary collection of text documents. The framework adapts a retriever augmented generation model to the target domain using supervised finetuning and reinforcement learning with synthetic feedback in a few-shot setting. This yields an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs. The synthetic generation pipeline generates high quality synthetic training data musing a medium sized LLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline is designed to generate both abstractive and extractive questions that span the entire corpus. Using samples from this dataset, the framework fine-tunes a smaller RAG model comprising a dense retriever and a smaller sized LLM on samples from the dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#30340;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#21040;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.04056</link><description>&lt;p&gt;
&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04056
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#30340;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#21040;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;(MNNs)&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#12290;&#36825;&#20010;&#31867;&#21035;&#21253;&#25324;&#20102;Wang&#12289;Ruiz&#21644;Ribeiro&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#32771;&#34385;&#30340;MNNs&#65292;&#27969;&#24418;&#25955;&#23556;&#21464;&#25442;(&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;)&#65292;&#20197;&#21450;&#20854;&#20182;&#26377;&#36259;&#30340;&#20043;&#21069;&#22312;&#25991;&#29486;&#20013;&#26410;&#32771;&#34385;&#30340;&#31034;&#20363;&#65292;&#22914;Kipf&#21644;Welling&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27969;&#24418;&#31561;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#23545;&#27969;&#24418;&#26377;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#32780;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32593;&#32476;&#22312;&#26679;&#26412;&#28857;&#25968;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#33021;&#22815;&#20445;&#35777;&#25910;&#25947;&#21040;&#20854;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;(&#20027;&#35201;&#20851;&#27880;&#29305;&#23450;&#30340;MNN&#32467;&#26500;&#21644;&#22270;&#26500;&#24314;)&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#19981;&#20381;&#36182;&#20110;&#20351;&#29992;&#30340;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;&#32780;&#19988;&#65292;&#23427;&#34920;&#29616;&#20986;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a large class of manifold neural networks (MNNs) which we call Manifold Filter-Combine Networks. This class includes as special cases, the MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold scattering transform (a wavelet-based model of neural networks), and other interesting examples not previously considered in the literature such as the manifold equivalent of Kipf and Welling's graph convolutional network. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific MNN architectures and graph constructions), our rate of convergence does not explicitly depend on the number of filters used. Moreover, it exhibits line
&lt;/p&gt;</description></item><item><title>FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.11650</link><description>&lt;p&gt;
FedNoisy: &#20998;&#24067;&#24335;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11650
&lt;/p&gt;
&lt;p&gt;
FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#22240;&#20026;&#26080;&#38656;&#23545;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25935;&#24863;&#25968;&#25454;&#36827;&#34892;&#32858;&#21512;&#32780;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#25968;&#25454;&#38548;&#31163;&#30340;&#20998;&#24067;&#24335;&#21644;&#23396;&#31435;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#36136;&#37327;&#30340;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#26631;&#31614;&#30340;&#24178;&#25200;&#12290;&#35768;&#22810;&#21162;&#21147;&#37117;&#33268;&#21147;&#20110;&#22312;&#38598;&#20013;&#24335;&#25110;&#32852;&#21512;&#24335;&#29615;&#22659;&#20013;&#38450;&#24481;&#22122;&#22768;&#26631;&#31614;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#32771;&#34385;&#21508;&#31181;&#20856;&#22411;&#32852;&#21512;&#23398;&#20064;&#22330;&#26223;&#20013;&#22122;&#22768;&#26631;&#31614;&#24433;&#21709;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#32852;&#21512;&#22122;&#22768;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25506;&#32034;&#36825;&#20123;&#25968;&#25454;&#35774;&#32622;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#65292;&#36825;&#21487;&#33021;&#25351;&#23548;&#26410;&#26469;&#30340;&#26041;&#27861;&#24320;&#21457;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20986;&#30340;20&#20010;&#22522;&#26412;&#35774;&#32622;&#65292;&#36866;&#29992;&#20110;5&#20010;&#20197;&#19978;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23548;&#20986;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#19968;&#38454;ODR&#65292;&#25551;&#36848;&#20102;&#20854;&#26368;&#20248;&#26435;&#34913;&#26354;&#32447;&#30340;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#26368;&#20248;&#32534;&#30721;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.09790</link><description>&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65306;&#22522;&#20110;&#19968;&#38454;&#26681;&#36712;&#36857;&#30340;&#20449;&#24687;&#29942;&#39048;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Information Bottleneck's Ordinary Differential Equation: First-Order Root-Tracking for the IB. (arXiv:2306.09790v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23548;&#20986;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#19968;&#38454;ODR&#65292;&#25551;&#36848;&#20102;&#20854;&#26368;&#20248;&#26435;&#34913;&#26354;&#32447;&#30340;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#26368;&#20248;&#32534;&#30721;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#26159;&#19968;&#31181;&#26377;&#25439;&#21387;&#32553;&#26041;&#27861;&#65292;&#20854;&#36895;&#29575;-&#22833;&#30495;&#26354;&#32447;&#25551;&#36848;&#20102;&#36755;&#20837;&#21387;&#32553;&#21644;&#30456;&#20851;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#23427;&#25513;&#30422;&#20102;&#26368;&#20248;&#36755;&#20837;&#32534;&#30721;&#30340;&#22522;&#26412;&#21160;&#24577;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#38543;&#30528;&#36755;&#20837;&#20449;&#24687;&#30340;&#21387;&#32553;&#65292;&#36825;&#20123;&#21160;&#24577;&#36890;&#24120;&#36981;&#24490;&#20998;&#27573;&#20809;&#28369;&#30340;&#36712;&#36857;&#65292;&#22914;&#26368;&#36817;&#22312;RD&#20013;&#25152;&#31034;&#12290;&#36825;&#20123;&#20809;&#28369;&#30340;&#21160;&#24577;&#22312;&#26368;&#20248;&#32534;&#30721;&#21457;&#29983;&#23450;&#24615;&#21464;&#21270;&#65288;&#22312;&#20998;&#21449;&#22788;&#65289;&#26102;&#20250;&#20013;&#26029;&#12290;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#19982;RD&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#21487;&#20197;&#30475;&#21040;&#27425;&#20248;&#35299;&#22312;&#37027;&#37324;&#21457;&#29983;&#30896;&#25758;&#25110;&#20132;&#25442;&#26368;&#20248;&#24615;&#12290;&#23613;&#31649;&#20449;&#24687;&#29942;&#39048;&#21450;&#20854;&#24212;&#29992;&#24050;&#34987;&#25509;&#21463;&#65292;&#20294;&#24778;&#20154;&#22320;&#32570;&#20047;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#25968;&#20540;&#25216;&#26415;&#65292;&#21363;&#20351;&#23545;&#20110;&#24050;&#30693;&#20998;&#24067;&#30340;&#26377;&#38480;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#26032;&#23548;&#20986;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#25551;&#36848;&#20102;&#20854;&#26368;&#20248;&#26435;&#34913;&#26354;&#32447;&#30340;&#22522;&#26412;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Information Bottleneck (IB) is a method of lossy compression. Its rate-distortion (RD) curve describes the fundamental tradeoff between input compression and the preservation of relevant information. However, it conceals the underlying dynamics of optimal input encodings. We argue that these typically follow a piecewise smooth trajectory as the input information is being compressed, as recently shown in RD. These smooth dynamics are interrupted when an optimal encoding changes qualitatively, at a bifurcation. By leveraging the IB's intimate relations with RD, sub-optimal solutions can be seen to collide or exchange optimality there.  Despite the acceptance of the IB and its applications, there are surprisingly few techniques to solve it numerically, even for finite problems whose distribution is known. We derive anew the IB's first-order Ordinary Differential Equation, which describes the dynamics underlying its optimal tradeoff curve. To exploit these dynamics, one needs not only 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22914;&#20309;&#34987;&#20445;&#30041;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;OFA&#22312;&#29983;&#25104;&#22270;&#20687;&#30340;&#36807;&#31243;&#20013;&#24536;&#35760;&#20102;&#37096;&#20998;&#23454;&#20307;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.02115</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#30693;&#35782;&#25506;&#31350;&#30340;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models. (arXiv:2306.02115v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22914;&#20309;&#34987;&#20445;&#30041;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;OFA&#22312;&#29983;&#25104;&#22270;&#20687;&#30340;&#36807;&#31243;&#20013;&#24536;&#35760;&#20102;&#37096;&#20998;&#23454;&#20307;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#20197;&#39564;&#35777;&#33258;&#28982;&#35821;&#35328;&#20013;&#33719;&#21462;&#30340;&#20851;&#20110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22914;&#20309;&#34987;&#20445;&#30041;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#65288;V&amp;L&#65289;&#27169;&#22411;&#20013;&#12290;&#35813;&#20219;&#21153;&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#26159;&#29983;&#25104;&#19968;&#20010;&#21253;&#21547;&#20851;&#20110;&#23454;&#20307;&#21450;&#20854;&#30456;&#20851;&#22270;&#20687;&#30340;&#30693;&#35782;&#30340;&#34920;&#26684;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#26159;&#26681;&#25454;&#23454;&#20307;&#12289;&#26631;&#39064;&#21644;&#21253;&#21547;&#30456;&#20851;&#23454;&#20307;&#30693;&#35782;&#30340;&#34920;&#26684;&#29983;&#25104;&#22270;&#20687;&#12290;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#27491;&#30830;&#22320;&#20102;&#35299;&#29992;&#20110;&#25191;&#34892;&#29983;&#25104;&#30340;&#23454;&#20307;&#12290;&#25105;&#20204;&#20174;&#33521;&#25991;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#30340;&#32422;200,000&#20010;&#20449;&#24687;&#26694;&#21019;&#24314;&#20102;&#32500;&#22522;&#30334;&#31185;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;&#65288;WikiTIG&#65289;&#25968;&#25454;&#38598;&#26469;&#25191;&#34892;&#25552;&#20986;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;V&amp;L&#27169;&#22411;OFA&#23545;&#20219;&#21153;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;OFA&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#24536;&#35760;&#20102;&#37096;&#20998;&#23454;&#20307;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#22270;&#20687;&#30456;&#20851;&#24615;&#33021;&#26159;&#19968;&#31181;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision &amp; Language (V&amp;L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V&amp;L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image relat
&lt;/p&gt;</description></item><item><title>SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.19442</link><description>&lt;p&gt;
SimFBO&#65306;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#21452;&#23618;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19442
&lt;/p&gt;
&lt;p&gt;
SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#12289;&#24494;&#35843;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#39046;&#22495;&#20013;&#23884;&#22871;&#20248;&#21270;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32852;&#37030;&#21452;&#23618;&#20248;&#21270;&#65288;FBO&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#26174;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FBO&#31639;&#27861;&#24448;&#24448;&#28041;&#21450;&#22797;&#26434;&#30340;&#35745;&#31639;&#65292;&#24182;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#22810;&#20010;&#23376;&#24490;&#29615;&#65292;&#27599;&#20010;&#23376;&#24490;&#29615;&#21253;&#21547;&#22810;&#20010;&#36890;&#20449;&#36718;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimFBO&#30340;&#31616;&#21333;&#28789;&#27963;&#30340;FBO&#26694;&#26550;&#65292;&#23427;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#23376;&#24490;&#29615;&#65292;&#24182;&#21253;&#25324;&#19968;&#31181;&#24191;&#20041;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#21644;&#26356;&#26032;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#31995;&#32479;&#32423;&#24322;&#26500;&#40065;&#26834;FBO&#65288;ShroFBO&#65289;&#20316;&#20026;SimFBO&#30340;&#21464;&#20307;&#65292;&#20854;&#23545;&#26412;&#22320;&#35745;&#31639;&#30340;&#24322;&#26500;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#21644;&#26080;&#26367;&#25442;&#30340;&#23458;&#25143;&#31471;&#37319;&#26679;&#19979;&#65292;SimFBO&#21644;ShroFBO&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#21152;&#36895;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#20803;&#23398;&#20064;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16573</link><description>&lt;p&gt;
&#25506;&#32034;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26435;&#37325;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16573
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#26368;&#36817;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#20998;&#24067;&#36890;&#24120;&#26159;&#25351;&#25968;&#20998;&#24067;&#65292;&#38500;&#38750;&#26377;&#24847;&#22320;&#35843;&#25972;&#26679;&#26412;&#25968;&#37327;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33879;&#21517;&#30340;&#32463;&#20856;&#27491;&#21017;&#21270;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#20294;&#24050;&#30693;&#20854;&#23545;&#29616;&#26377;&#21508;&#31181;&#19981;&#21516;&#26041;&#27861;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#20026;&#20160;&#20040;&#36825;&#31181;&#26041;&#27861;&#23545;&#38271;&#23614;&#25968;&#25454;&#26377;&#25928;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#27599;&#20010;&#35757;&#32451;&#38454;&#27573;&#30340;&#22278;&#38181;&#25928;&#24212;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#20998;&#35299;&#20026;&#30001;&#26435;&#20540;&#34928;&#20943;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#24341;&#36215;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20013;Fisher&#21028;&#21035;&#27604;&#30340;&#22686;&#21152;&#20197;&#21450;&#30001;&#26435;&#37325;&#34928;&#20943;&#21644;&#31867;&#24179;&#34913;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#38544;&#24335;&#36923;&#36753;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#25104;&#21151;&#32531;&#35299;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38271;&#23614;&#25968;&#25454;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#26816;&#27979;&#39640;&#20135;&#20892;&#30000;&#65292;&#33719;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#26395;&#20026;&#20892;&#27665;&#25552;&#20379;&#24110;&#21161;.</title><link>http://arxiv.org/abs/2305.11990</link><description>&lt;p&gt;
&#39640;&#20135;&#20892;&#30000;&#26816;&#27979;&#65306;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Productive Crop Field Detection: A New Dataset and Deep Learning Benchmark Results. (arXiv:2305.11990v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#26816;&#27979;&#39640;&#20135;&#20892;&#30000;&#65292;&#33719;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#26395;&#20026;&#20892;&#27665;&#25552;&#20379;&#24110;&#21161;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#20934;&#20892;&#19994;&#20013;&#65292;&#26816;&#27979;&#39640;&#20135;&#20892;&#30000;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#23454;&#36341;&#65292;&#20351;&#24471;&#20892;&#27665;&#21487;&#20197;&#21333;&#29420;&#35780;&#20272;&#25805;&#20316;&#32489;&#25928;&#24182;&#27604;&#36739;&#19981;&#21516;&#30340;&#31181;&#23376;&#21697;&#31181;&#12289;&#20892;&#33647;&#21644;&#32933;&#26009;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35782;&#21035;&#39640;&#20135;&#20892;&#30000;&#24448;&#24448;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26816;&#27979;&#20892;&#30000;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#36890;&#36807;&#26426;&#22120;&#25805;&#20316;&#32467;&#21512;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#32780;&#36319;&#36394;&#30340;Sentinel-2&#22270;&#20687;&#29983;&#25104;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#20811;&#26381;&#26631;&#35760;&#26679;&#26412;&#19981;&#36275;&#30340;&#25968;&#25454;&#38598;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#24212;&#29992;&#21322;&#30417;&#30563;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#31867;&#21644;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#33258;&#21160;&#26816;&#27979;&#39640;&#20135;&#20892;&#30000;&#12290;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#22312;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#38750;&#24120;&#36866;&#21512;
&lt;/p&gt;
&lt;p&gt;
In precision agriculture, detecting productive crop fields is an essential practice that allows the farmer to evaluate operating performance separately and compare different seed varieties, pesticides, and fertilizers. However, manually identifying productive fields is often a time-consuming and error-prone task. Previous studies explore different methods to detect crop fields using advanced machine learning algorithms, but they often lack good quality labeled data. In this context, we propose a high-quality dataset generated by machine operation combined with Sentinel-2 images tracked over time. As far as we know, it is the first one to overcome the lack of labeled samples by using this technique. In sequence, we apply a semi-supervised classification of unlabeled data and state-of-the-art supervised and self-supervised deep learning methods to detect productive crop fields automatically. Finally, the results demonstrate high accuracy in Positive Unlabeled learning, which perfectly fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#26032;&#29702;&#35770;&#35299;&#37322;&#21644;&#35777;&#26126;&#65292;&#21363;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#26041;&#24335;&#38544;&#24335;&#20248;&#21270;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#25351;&#20986;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25506;&#32034;&#30340;&#23454;&#36136;&#26159;&#35745;&#31639;&#24403;&#21069;&#31574;&#30053;&#25910;&#30410;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#31574;&#30053;&#30340;&#26041;&#24046;&#24212;&#35813;&#26159;&#21382;&#21490;&#20381;&#36182;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.06851</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#26041;&#24335;&#38544;&#24335;&#20248;&#21270;&#30340;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Algorithms Implicitly Optimize by Continuation. (arXiv:2305.06851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#26032;&#29702;&#35770;&#35299;&#37322;&#21644;&#35777;&#26126;&#65292;&#21363;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#26041;&#24335;&#38544;&#24335;&#20248;&#21270;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#25351;&#20986;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25506;&#32034;&#30340;&#23454;&#36136;&#26159;&#35745;&#31639;&#24403;&#21069;&#31574;&#30053;&#25910;&#30410;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#31574;&#30053;&#30340;&#26041;&#24046;&#24212;&#35813;&#26159;&#21382;&#21490;&#20381;&#36182;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#36890;&#24120;&#36890;&#36807;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#20248;&#21270;&#31574;&#30053;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#24314;&#31435;&#22312;&#20248;&#21270;&#36830;&#32493;&#26694;&#26550;&#19979;&#12290;&#21518;&#32773;&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#38750;&#20984;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20197;&#36830;&#32493;&#30340;&#26367;&#20195;&#30446;&#26631;&#20989;&#25968;&#24207;&#21015;&#20026;&#22522;&#30784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20248;&#21270;&#20223;&#23556;&#39640;&#26031;&#31574;&#30053;&#24182;&#25191;&#34892;&#29109;&#27491;&#21017;&#21270;&#21487;&#20197;&#35299;&#37322;&#20026;&#36890;&#36807;&#36830;&#32493;&#38544;&#24335;&#22320;&#20248;&#21270;&#30830;&#23450;&#24615;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#30340;&#25506;&#32034;&#21253;&#25324;&#35745;&#31639;&#24403;&#21069;&#30340;&#31574;&#30053;&#25910;&#30410;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#31574;&#30053;&#30340;&#26041;&#24046;&#24212;&#35813;&#26159;&#21382;&#21490;&#20381;&#36182;&#24615;&#20989;&#25968;&#65292;&#20197;&#36991;&#20813;&#23616;&#37096;&#26368;&#20540;&#32780;&#19981;&#26159;&#20165;&#20165;&#26368;&#22823;&#21270;&#25919;&#31574;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Kullback-Leibler Maillard Sampling (KL-MS)&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;KL&#31354;&#38388;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14989</link><description>&lt;p&gt;
Kullback-Leibler Maillard&#37319;&#26679;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Kullback-Leibler Maillard Sampling (KL-MS)&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;KL&#31354;&#38388;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22870;&#21169;&#20998;&#24067;&#38598;&#20013;&#22312;&#21306;&#38388;$[0,1]$&#20869;&#30340;$K$&#33218;&#25968;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kullback-Leibler Maillard Sampling (KL-MS)&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#26159;Maillard&#37319;&#26679;&#22312;KL&#31354;&#38388;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;KL-MS&#22312;Bernoulli&#22870;&#21169;&#26102;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#33021;&#65292;&#20854;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#24230;&#19978;&#30028;&#20026;$O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$&#65292;&#20854;&#20013;$\mu^*$&#26159;&#26368;&#20248;&#33218;&#30340;&#26399;&#26395;&#22870;&#21169;&#65292;$T$&#26159;&#26102;&#27573;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$, where $\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#36712;&#36857;&#25130;&#26029;&#26041;&#27861;&#65288;TATU&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#22411;&#39537;&#21160;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;TATU&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.04660</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#20013;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#36712;&#36857;&#25130;&#26029;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning. (arXiv:2304.04660v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#36712;&#36857;&#25130;&#26029;&#26041;&#27861;&#65288;TATU&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#22411;&#39537;&#21160;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;TATU&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#33021;&#22815;&#20174;&#22266;&#23450;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#21040;&#22909;&#30340;&#31574;&#30053;&#65292;&#29978;&#33267;&#19968;&#20123;&#36136;&#37327;&#36739;&#24046;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#21512;&#25104;&#26679;&#26412;&#26159;&#21542;&#21487;&#38752;&#26080;&#27861;&#24471;&#21040;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#19968;&#20123;&#21512;&#25104;&#26679;&#26412;&#21487;&#33021;&#20301;&#20110;&#38745;&#24577;&#25968;&#25454;&#38598;&#25903;&#25345;&#21306;&#22495;&#20043;&#22806;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#36712;&#36857;&#25130;&#26029;&#65288;TATU&#65289;&#26041;&#27861;&#65292;&#22914;&#26524;&#32047;&#31215;&#19981;&#30830;&#23450;&#24615;&#36229;&#36807;&#38408;&#20540;&#65292;&#21017;&#33258;&#36866;&#24212;&#25130;&#26029;&#21512;&#25104;&#36712;&#36857;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;TATU&#30340;&#24615;&#33021;&#36793;&#30028;&#20197;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;&#20026;&#20102;&#22312;&#23454;&#35777;&#19978;&#26174;&#31034;&#20986;TATU&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#19982;&#20004;&#20010;&#32463;&#20856;&#30340;&#27169;&#22411;&#39537;&#21160;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MOPO&#21644;COMBO&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;TATU&#19982;&#22810;&#20010;&#29616;&#25104;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;BCQ&#65289;&#36827;&#34892;&#25972;&#21512;&#12290;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TATU&#30340;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with the trained environmental dynamics, model-based offline reinforcement learning (RL) algorithms can often successfully learn good policies from fixed-sized datasets, even some datasets with poor quality. Unfortunately, however, it can not be guaranteed that the generated samples from the trained dynamics model are reliable (e.g., some synthetic samples may lie outside of the support region of the static dataset). To address this issue, we propose Trajectory Truncation with Uncertainty (TATU), which adaptively truncates the synthetic trajectory if the accumulated uncertainty along the trajectory is too large. We theoretically show the performance bound of TATU to justify its benefits. To empirically show the advantages of TATU, we first combine it with two classical model-based offline RL algorithms, MOPO and COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free offline RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark show that TATU
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25351;&#20986;&#20256;&#32479;&#30340;&#22270;&#29255;&#20998;&#31867;&#22120;&#23398;&#20064;&#36807;&#31243;&#24573;&#35270;&#27880;&#37322;&#36807;&#31243;&#20013;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#26469;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17595</link><description>&lt;p&gt;
&#34987;&#24573;&#35270;&#30340;&#20813;&#36153;&#21320;&#39184;&#8212;&#8212;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts. (arXiv:2303.17595v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25351;&#20986;&#20256;&#32479;&#30340;&#22270;&#29255;&#20998;&#31867;&#22120;&#23398;&#20064;&#36807;&#31243;&#24573;&#35270;&#27880;&#37322;&#36807;&#31243;&#20013;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#26469;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#30417;&#30563;&#23398;&#20064;&#23558;&#20154;&#31867;&#30693;&#35782;&#36890;&#36807;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#65288;X&#65292;Y&#65289;&#30340;&#23545;&#24212;&#20851;&#31995;&#36716;&#21270;&#20026;&#21442;&#25968;&#27169;&#22411;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#31616;&#21333;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#20154;&#31867;&#30693;&#35782;&#34920;&#31034;&#24573;&#35270;&#20102;&#27880;&#37322;&#36807;&#31243;&#20013;&#20016;&#23500;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#20363;&#22914;&#22312;&#22270;&#20687;&#36873;&#25321;&#21518;&#30041;&#19979;&#30340;&#40736;&#26631;&#36712;&#36857;&#21644;&#28857;&#20987;&#30340;&#26102;&#38388;&#24207;&#21015;&#31561;&#12290;&#25105;&#20204;&#30340;&#27934;&#35265;&#26159;&#65292;&#36825;&#20123;&#27880;&#37322;&#21103;&#20135;&#21697;Z&#25552;&#20379;&#20102;&#36817;&#20284;&#30340;&#20154;&#31867;&#20851;&#27880;&#20449;&#24687;&#65292;&#24369;&#21270;&#20102;&#27169;&#22411;&#23545;&#21069;&#26223;&#32447;&#32034;&#30340;&#20851;&#27880;&#65292;&#20943;&#23569;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#38450;&#27490;&#20102;&#25463;&#24452;&#23398;&#20064;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ImageNet-AB&#21644;COCO-AB&#12290;&#23427;&#20204;&#26159;&#36890;&#36807;&#22797;&#21046;&#30456;&#24212;&#30340;&#21407;&#22987;&#27880;&#37322;&#20219;&#21153;&#26469;&#33719;&#24471;&#30340;ImageNet&#21644;COCO&#35757;&#32451;&#38598;&#65292;&#22686;&#21152;&#20102;&#26679;&#26412;&#32423;&#21035;&#30340;&#27880;&#37322;&#21103;&#20135;&#21697;&#12290;&#25105;&#20204;&#31216;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#26469;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#20026;&#23398;&#20064;&#27880;&#37322;&#21103;&#20135;&#21697;&#65288;LUAB&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#25439;&#22833;&#65292;&#29992;&#20110;&#21516;&#26102;&#22238;&#24402;Z&#21644;Y&#24050;&#32463;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning of image classifiers distills human knowledge into a parametric model through pairs of images and corresponding labels (X,Y). We argue that this simple and widely used representation of human knowledge neglects rich auxiliary information from the annotation procedure, such as the time-series of mouse traces and clicks left after image selection. Our insight is that such annotation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues, reducing spurious correlations and discouraging shortcut learning. To verify this, we create ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annotation byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing Z together with Y already improves 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#21450;&#20854;&#27010;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#20844;&#36335;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16109</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Manoeuvre and Trajectory Prediction for Autonomous Vehicles Using Transformer Networks. (arXiv:2303.16109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16109
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#21450;&#20854;&#27010;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#20844;&#36335;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20854;&#20182;&#36947;&#36335;&#29992;&#25143;&#65288;&#21253;&#25324;&#36710;&#36742;&#65289;&#30340;&#34892;&#20026;&#65288;&#21363;&#25805;&#20316;/&#36712;&#36857;&#65289;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#25110;&#33258;&#21160;&#21270;&#39550;&#39542;&#31995;&#32479;&#65288;ADSs&#65289;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#36710;&#36742;&#26410;&#26469;&#34892;&#20026;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#34892;&#39542;&#22330;&#26223;&#65292;&#19968;&#20010;&#36710;&#36742;&#36890;&#24120;&#23384;&#22312;&#22810;&#20010;&#26410;&#26469;&#34892;&#20026;&#27169;&#24335;&#26159;&#21512;&#29702;&#30340;&#12290;&#22240;&#27492;&#65292;&#22810;&#27169;&#24577;&#39044;&#27979;&#21487;&#20197;&#25552;&#20379;&#27604;&#21333;&#27169;&#24577;&#39044;&#27979;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20351;AV&#33021;&#22815;&#26356;&#22909;&#22320;&#35780;&#20272;&#39118;&#38505;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#39044;&#27979;&#26694;&#26550;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#21450;&#20854;&#27010;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#23450;&#21046;&#30340;&#25805;&#20316;&#39044;&#27979;&#38382;&#39064;&#20844;&#24335;&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#31181;&#23450;&#21046;&#30340;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#20844;&#36335;&#39550;&#39542;&#25968;&#25454;&#38598;&#65288;&#21363;NGSIM&#21644;highD&#65289;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#24471;&#21040;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the behaviour (i.e. manoeuvre/trajectory) of other road users, including vehicles, is critical for the safe and efficient operation of autonomous vehicles (AVs), a.k.a. automated driving systems (ADSs). Due to the uncertain future behaviour of vehicles, multiple future behaviour modes are often plausible for a vehicle in a given driving scene. Therefore, multimodal prediction can provide richer information than single-mode prediction enabling AVs to perform a better risk assessment. To this end, we propose a novel multimodal prediction framework that can predict multiple plausible behaviour modes and their likelihoods. The proposed framework includes a bespoke problem formulation for manoeuvre prediction, a novel transformer-based prediction model, and a tailored training method for multimodal manoeuvre and trajectory prediction. The performance of the framework is evaluated using two public benchmark highway driving datasets, namely NGSIM and highD. The results show that th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#29983;&#29289;&#29305;&#24449;&#35748;&#35777;&#30340;&#29275;&#33080;&#23884;&#20837;&#20998;&#24067;&#27169;&#22411;&#65292;&#36890;&#36807;&#35745;&#31639;&#27979;&#35797;&#23884;&#20837;&#21644;&#35757;&#32451;&#23884;&#20837;&#30340;&#39532;&#27663;&#36317;&#31163;&#26469;&#36827;&#34892;&#35748;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#20351;&#29992;VGG16&#27169;&#22411;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;20&#20010;&#29275;&#36523;&#20221;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;1.25%&#30340;FRR&#21644;1.18%&#30340;FAR&#12290;</title><link>http://arxiv.org/abs/2302.14831</link><description>&lt;p&gt;
FacEDiM&#65306;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#29983;&#29289;&#29305;&#24449;&#35748;&#35777;&#30340;&#29275;&#33080;&#23884;&#20837;&#20998;&#24067;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle. (arXiv:2302.14831v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#29983;&#29289;&#29305;&#24449;&#35748;&#35777;&#30340;&#29275;&#33080;&#23884;&#20837;&#20998;&#24067;&#27169;&#22411;&#65292;&#36890;&#36807;&#35745;&#31639;&#27979;&#35797;&#23884;&#20837;&#21644;&#35757;&#32451;&#23884;&#20837;&#30340;&#39532;&#27663;&#36317;&#31163;&#26469;&#36827;&#34892;&#35748;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#20351;&#29992;VGG16&#27169;&#22411;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;20&#20010;&#29275;&#36523;&#20221;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;1.25%&#30340;FRR&#21644;1.18%&#30340;FAR&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#36890;&#36807;&#35745;&#31639;&#27979;&#35797;&#23884;&#20837;&#21644;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CNN&#33719;&#24471;&#30340;&#35757;&#32451;&#23884;&#20837;&#30340;&#22810;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;&#20043;&#38388;&#30340;&#39532;&#27663;&#36317;&#31163;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#29983;&#29289;&#29305;&#24449;&#35748;&#35777;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#22312;&#20154;&#33080;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;VGG16&#27169;&#22411;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;20&#20010;&#29275;&#36523;&#20221;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;1.25%&#30340;FRR&#21644;1.18%&#30340;FAR&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to solve the problem of few-shot biometric authentication by computing the Mahalanobis distance between testing embeddings and a multivariate Gaussian distribution of training embeddings obtained using pre-trained CNNs. Experimental results show that models pre-trained on the ImageNet dataset significantly outperform models pre-trained on human faces. With a VGG16 model, we obtain a FRR of 1.25% for a FAR of 1.18% on a dataset of 20 cattle identities.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#21046;&#36896;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#20197;&#25552;&#21319;&#24615;&#33021;&#36739;&#24046;&#30340;&#21333;&#20803;&#30340;&#22312;&#32447;&#36807;&#31243;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.12004</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#20998;&#25955;&#21046;&#36896;&#31995;&#32479;&#22312;&#32447;&#36807;&#31243;&#30417;&#27979;&#30340;&#20449;&#24687;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System. (arXiv:2302.12004v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12004
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#21046;&#36896;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#20197;&#25552;&#21319;&#24615;&#33021;&#36739;&#24046;&#30340;&#21333;&#20803;&#30340;&#22312;&#32447;&#36807;&#31243;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20808;&#36827;&#21046;&#36896;&#39046;&#22495;&#65292;&#20256;&#24863;&#25216;&#26415;&#30340;&#24212;&#29992;&#20026;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#39640;&#25928;&#30340;&#21407;&#22320;&#36807;&#31243;&#30417;&#27979;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#21516;&#26102;&#65292;&#20449;&#24687;&#25216;&#26415;&#30340;&#36827;&#27493;&#20063;&#20351;&#24471;&#21046;&#36896;&#31995;&#32479;&#33021;&#22815;&#22312;&#36830;&#25509;&#21644;&#20998;&#25955;&#30340;&#29615;&#22659;&#19979;&#24037;&#20316;&#65292;&#20351;&#31995;&#32479;&#20013;&#30340;&#19981;&#21516;&#21046;&#36896;&#21333;&#20803;&#26356;&#21152;&#32039;&#23494;&#22320;&#21512;&#20316;&#12290;&#22312;&#20998;&#25955;&#21046;&#36896;&#31995;&#32479;&#20013;&#65292;&#21442;&#19982;&#30340;&#21333;&#20803;&#21487;&#33021;&#21046;&#36896;&#30456;&#21516;&#25110;&#31867;&#20284;&#30340;&#20135;&#21697;&#65292;&#24182;&#19988;&#37096;&#32626;&#33258;&#24049;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#36807;&#31243;&#30417;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25805;&#20316;&#36807;&#31243;&#20013;&#20219;&#21153;&#36827;&#24230;&#30340;&#21487;&#33021;&#19981;&#19968;&#33268;&#65292;&#26377;&#20123;&#21333;&#20803;&#30340;&#25968;&#25454;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#32780;&#26377;&#20123;&#21333;&#20803;&#30340;&#25968;&#25454;&#20449;&#24687;&#36739;&#23569;&#26159;&#24456;&#24120;&#35265;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010;&#21333;&#20803;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30417;&#27979;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#22312;&#20998;&#25955;&#21046;&#36896;&#31995;&#32479;&#20013;&#23454;&#29616;&#21333;&#20301;&#38388;&#39640;&#25928;&#23433;&#20840;&#30340;&#30693;&#35782;&#20849;&#20139;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#36739;&#24046;&#30340;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
In advanced manufacturing, the incorporation of sensing technology provides an opportunity to achieve efficient in-situ process monitoring using machine learning methods. Meanwhile, the advances of information technologies also enable a connected and decentralized environment for manufacturing systems, making different manufacturing units in the system collaborate more closely. In a decentralized manufacturing system, the involved units may fabricate same or similar products and deploy their own machine learning model for online process monitoring. However, due to the possible inconsistency of task progress during the operation, it is also common that some units have more informative data while some have less informative data. Thus, the monitoring performance of machine learning model for each unit may highly vary. Therefore, it is extremely valuable to achieve efficient and secured knowledge sharing among the units in a decentralized manufacturing system for enhancement of poorly perf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#36229;&#32593;&#26684;&#19978;&#20998;&#24067;&#22343;&#21248;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#23376;&#31435;&#26041;&#20307;&#26465;&#20214;&#25277;&#26679;&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#22810;&#27425;&#26597;&#35810;&#21518;&#21487;&#20197;&#33719;&#24471;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#20998;&#26512;&#35777;&#26126;&#20102;&#36229;&#32593;&#26684;&#19978;&#20989;&#25968;&#30340; Pisier &#19981;&#31561;&#24335;&#30340;&#40065;&#26834;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2302.09013</link><description>&lt;p&gt;
&#24102;&#26377;&#23376;&#31435;&#26041;&#20307;&#26465;&#20214;&#30340;&#36229;&#32593;&#26684;&#22343;&#21248;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Uniformity Testing over Hypergrids with Subcube Conditioning. (arXiv:2302.09013v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#36229;&#32593;&#26684;&#19978;&#20998;&#24067;&#22343;&#21248;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#23376;&#31435;&#26041;&#20307;&#26465;&#20214;&#25277;&#26679;&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#22810;&#27425;&#26597;&#35810;&#21518;&#21487;&#20197;&#33719;&#24471;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#20998;&#26512;&#35777;&#26126;&#20102;&#36229;&#32593;&#26684;&#19978;&#20989;&#25968;&#30340; Pisier &#19981;&#31561;&#24335;&#30340;&#40065;&#26834;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#27979;&#35797;&#25903;&#25345;&#22312;&#36229;&#32593;&#26684;$[m_1] \times \cdots \times [m_n]$&#19978;&#30340;&#20998;&#24067;&#30340;&#22343;&#21248;&#24615;&#65292;&#35813;&#31639;&#27861;&#23545;&#20855;&#26377;$m=\max_i m_i$&#30340;&#23376;&#31435;&#26041;&#20307;&#26465;&#20214;&#25277;&#26679;&#39044;&#35328;&#26426;&#36827;&#34892;$\smash{\widetilde{O}(\text{poly}(m)\sqrt{n}/\epsilon^2)}$&#20010;&#26597;&#35810;&#12290;&#24403;$m$&#26159;&#19968;&#20010;&#24120;&#25968;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#21152;&#24378;&#20102;[CCK+21]&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#30456;&#21516;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#20294;&#20165;&#36866;&#29992;&#20110;&#36229;&#31435;&#26041;&#20307;$\{\pm 1\}^n$&#12290;&#25105;&#20204;&#31639;&#27861;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#20351;&#29992;&#20613;&#37324;&#21494;&#20998;&#26512;&#35777;&#26126;&#20102;&#36229;&#32593;&#26684;&#19978;&#20989;&#25968;&#30340; Pisier &#19981;&#31561;&#24335;&#30340;&#40065;&#26834;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give an algorithm for testing uniformity of distributions supported on hypergrids $[m_1] \times \cdots \times [m_n]$, which makes $\smash{\widetilde{O}(\text{poly}(m)\sqrt{n}/\epsilon^2)}$ many queries to a subcube conditional sampling oracle with $m=\max_i m_i$. When $m$ is a constant, our algorithm is nearly optimal and strengthens the algorithm of [CCK+21] which has the same query complexity but works for hypercubes $\{\pm 1\}^n$ only.  A key technical contribution behind the analysis of our algorithm is a proof of a robust version of Pisier's inequality for functions over hypergrids using Fourier analysis.
&lt;/p&gt;</description></item><item><title>&#22240;&#23376;&#22330;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#20449;&#21495;&#20998;&#35299;&#20026;&#22240;&#23376;&#30340;&#20056;&#31215;&#65292;&#36890;&#36807;&#31070;&#32463;&#22330;&#25110;&#24120;&#35268;&#22330;&#34920;&#31034;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#27010;&#25324;&#22810;&#31181;&#26368;&#36817;&#30340;&#20449;&#21495;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#24378;&#22823;&#30340;&#26032;&#20449;&#21495;&#34920;&#31034;&#65292;&#22914;&#26412;&#25991;&#25552;&#20986;&#30340;&#31995;&#25968;&#22522;&#20989;&#25968;&#20998;&#35299;&#65288;CoBaFa&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CoBaFa&#22312;&#36924;&#36817;&#36136;&#37327;&#12289;&#32039;&#20945;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.01226</link><description>&lt;p&gt;
&#22240;&#23376;&#22330;&#65306;&#31070;&#32463;&#22330;&#21644;&#26356;&#22810;&#39046;&#22495;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Factor Fields: A Unified Framework for Neural Fields and Beyond. (arXiv:2302.01226v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01226
&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#22330;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#20449;&#21495;&#20998;&#35299;&#20026;&#22240;&#23376;&#30340;&#20056;&#31215;&#65292;&#36890;&#36807;&#31070;&#32463;&#22330;&#25110;&#24120;&#35268;&#22330;&#34920;&#31034;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#27010;&#25324;&#22810;&#31181;&#26368;&#36817;&#30340;&#20449;&#21495;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#24378;&#22823;&#30340;&#26032;&#20449;&#21495;&#34920;&#31034;&#65292;&#22914;&#26412;&#25991;&#25552;&#20986;&#30340;&#31995;&#25968;&#22522;&#20989;&#25968;&#20998;&#35299;&#65288;CoBaFa&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CoBaFa&#22312;&#36924;&#36817;&#36136;&#37327;&#12289;&#32039;&#20945;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#23376;&#22330;&#65292;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#21644;&#34920;&#31034;&#20449;&#21495;&#30340;&#26032;&#26694;&#26550;&#12290;&#22240;&#23376;&#22330;&#23558;&#20449;&#21495;&#20998;&#35299;&#20026;&#22240;&#23376;&#30340;&#20056;&#31215;&#65292;&#27599;&#20010;&#22240;&#23376;&#30001;&#31070;&#32463;&#22330;&#25110;&#24120;&#35268;&#22330;&#34920;&#31034;&#65292;&#24182;&#23545;&#36755;&#20837;&#20449;&#21495;&#36827;&#34892;&#22352;&#26631;&#21464;&#25442;&#25805;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#35299;&#20135;&#29983;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#27010;&#25324;&#22810;&#31181;&#26368;&#36817;&#30340;&#20449;&#21495;&#34920;&#31034;&#26041;&#27861;&#65292;&#21253;&#25324;NeRF&#65292;PlenOxels&#65292;EG3D&#65292;Instant-NGP&#21644;TensoRF&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26694;&#26550;&#36824;&#21487;&#20197;&#21019;&#24314;&#24378;&#22823;&#30340;&#26032;&#20449;&#21495;&#34920;&#31034;&#65292;&#20363;&#22914;&#26412;&#25991;&#25552;&#20986;&#30340;&#31995;&#25968;&#22522;&#20989;&#25968;&#20998;&#35299;&#65288;CoBaFa&#65289;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;CoBaFa&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#24555;&#36895;&#37325;&#24314;&#26041;&#27861;&#22312;&#31070;&#32463;&#20449;&#21495;&#34920;&#31034;&#30340;&#19977;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#36924;&#36817;&#36136;&#37327;&#65292;&#32039;&#20945;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;&#22312;&#20108;&#32500;&#22270;&#20687;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#22270;&#20687;&#36924;&#36817;&#36136;&#37327;&#65292;&#21516;&#26102;&#22312;&#20960;&#20309;&#36136;&#37327;&#26041;&#38754;&#20063;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Factor Fields, a novel framework for modeling and representing signals. Factor Fields decomposes a signal into a product of factors, each of which is represented by a neural or regular field representation operating on a coordinate transformed input signal. We show that this decomposition yields a unified framework that generalizes several recent signal representations including NeRF, PlenOxels, EG3D, Instant-NGP, and TensoRF. Moreover, the framework allows for the creation of powerful new signal representations, such as the Coefficient-Basis Factorization (CoBaFa) which we propose in this paper. As evidenced by our experiments, CoBaFa leads to improvements over previous fast reconstruction methods in terms of the three critical goals in neural signal representation: approximation quality, compactness and efficiency. Experimentally, we demonstrate that our representation achieves better image approximation quality on 2D image regression tasks, higher geometric quality when r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#21407;&#21017;&#12289;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#20197;&#21450;&#38024;&#23545;&#23458;&#25143;&#31471;&#24322;&#26500;&#24615;&#38382;&#39064;&#30340;&#21508;&#31181;&#31639;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#24212;&#29992;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26426;&#21046;&#65292;&#24182;&#28608;&#21457;&#30740;&#31350;&#20154;&#21592;&#23545;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2211.01549</link><description>&lt;p&gt;
&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#65306;&#21407;&#21017;&#12289;&#25361;&#25112;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Client Selection in Federated Learning: Principles, Challenges, and Opportunities. (arXiv:2211.01549v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#21407;&#21017;&#12289;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#20197;&#21450;&#38024;&#23545;&#23458;&#25143;&#31471;&#24322;&#26500;&#24615;&#38382;&#39064;&#30340;&#21508;&#31181;&#31639;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#24212;&#29992;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26426;&#21046;&#65292;&#24182;&#28608;&#21457;&#30740;&#31350;&#20154;&#21592;&#23545;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#37117;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#20856;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#23458;&#25143;&#31471;&#22312;&#25968;&#25454;&#20998;&#24067;&#21644;&#30828;&#20214;&#37197;&#32622;&#26041;&#38754;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#24322;&#36136;&#24615;&#12290;&#22240;&#27492;&#65292;&#27599;&#19968;&#36718;&#35757;&#32451;&#20013;&#38543;&#26426;&#36873;&#25321;&#23458;&#25143;&#31471;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#26469;&#33258;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#65292;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#38477;&#20302;&#12289;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12289;&#20844;&#24179;&#24615;&#19979;&#38477;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#36817;&#26399;&#22312;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#36873;&#25321;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#20197;&#21450;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26426;&#20250;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#24212;&#29992;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26426;&#21046;&#65292;&#21516;&#26102;&#28608;&#21457;&#30740;&#31350;&#20154;&#21592;&#21644;&#26032;&#20154;&#23545;&#36825;&#19968;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#39046;&#22495;&#26377;&#26356;&#22909;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a privacy-preserving paradigm for training Machine Learning (ML) models, Federated Learning (FL) has received tremendous attention from both industry and academia. In a typical FL scenario, clients exhibit significant heterogeneity in terms of data distribution and hardware configurations. Thus, randomly sampling clients in each training round may not fully exploit the local updates from heterogeneous clients, resulting in lower model accuracy, slower convergence rate, degraded fairness, etc. To tackle the FL client heterogeneity problem, various client selection algorithms have been developed, showing promising performance improvement. In this paper, we systematically present recent advances in the emerging field of FL client selection and its challenges and research opportunities. We hope to facilitate practitioners in choosing the most suitable client selection mechanisms for their applications, as well as inspire researchers and newcomers to better understand this exciting resea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#19982;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25512;&#23548;&#20102;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;SDE&#36793;&#38469;&#23494;&#24230;&#28436;&#21270;&#30340;&#27721;&#23494;&#23572;&#39039;-&#38597;&#21487;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#24182;&#23558;&#29983;&#25104;&#24314;&#27169;&#34920;&#36848;&#20026;&#23545;&#21512;&#36866;&#24230;&#37327;&#20043;&#38388;Kullback-Leibler&#25955;&#24230;&#30340;&#26368;&#23567;&#21270;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26041;&#27861;&#29992;&#20110;&#37319;&#26679;&#38750;&#24402;&#19968;&#21270;&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.01364</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20248;&#25511;&#21046;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An optimal control perspective on diffusion-based generative modeling. (arXiv:2211.01364v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#19982;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25512;&#23548;&#20102;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;SDE&#36793;&#38469;&#23494;&#24230;&#28436;&#21270;&#30340;&#27721;&#23494;&#23572;&#39039;-&#38597;&#21487;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#24182;&#23558;&#29983;&#25104;&#24314;&#27169;&#34920;&#36848;&#20026;&#23545;&#21512;&#36866;&#24230;&#37327;&#20043;&#38388;Kullback-Leibler&#25955;&#24230;&#30340;&#26368;&#23567;&#21270;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26041;&#27861;&#29992;&#20110;&#37319;&#26679;&#38750;&#24402;&#19968;&#21270;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#19982;&#22522;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20363;&#22914;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#27721;&#23494;&#23572;&#39039;-&#38597;&#21487;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;&#30340;SDE&#36793;&#38469;&#23494;&#24230;&#30340;&#28436;&#21270;&#12290;&#36825;&#20010;&#35270;&#35282;&#20801;&#35768;&#23558;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35777;&#25454;&#19979;&#30028;&#26159;&#25511;&#21046;&#29702;&#35770;&#20013;&#24191;&#20026;&#20154;&#30693;&#30340;&#39564;&#35777;&#23450;&#29702;&#30340;&#30452;&#25509;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24314;&#27169;&#34920;&#36848;&#20026;&#36335;&#24452;&#31354;&#38388;&#20013;&#21512;&#36866;&#24230;&#37327;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#26368;&#23567;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#38750;&#24402;&#19968;&#21270;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#26032;&#22411;&#25193;&#25955;&#26041;&#27861;&#65292;&#36825;&#22312;&#32479;&#35745;&#23398;&#21644;&#35745;&#31639;&#31185;&#23398;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26102;&#24207;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#22120;&#65288;DIS&#65289;&#21487;&#20197;&#32988;&#36807;&#20854;&#20182;&#22522;&#20110;&#25193;&#25955;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton-Jacobi-Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback-Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approache
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36801;&#31227;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#30446;&#26631;&#29366;&#24577;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#36827;&#34892;MTT&#65292;&#24182;&#22312;10&#20010;&#30446;&#26631;&#30340;MTT&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;250&#20010;&#30446;&#26631;&#30340;&#26356;&#22823;MTT&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#39640;&#20102;29%&#12290;</title><link>http://arxiv.org/abs/2210.15539</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#36801;&#31227;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Multi-Target Tracking with Transferable Convolutional Neural Networks. (arXiv:2210.15539v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36801;&#31227;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#30446;&#26631;&#29366;&#24577;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#36827;&#34892;MTT&#65292;&#24182;&#22312;10&#20010;&#30446;&#26631;&#30340;MTT&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;250&#20010;&#30446;&#26631;&#30340;&#26356;&#22823;MTT&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#39640;&#20102;29%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MTT&#65289;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#20174;&#22122;&#22768;&#20256;&#24863;&#22120;&#27979;&#37327;&#20013;&#20272;&#35745;&#20986;&#26410;&#30693;&#25968;&#37327;&#30340;&#31227;&#21160;&#30446;&#26631;&#30340;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;MTT&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#30446;&#26631;&#29366;&#24577;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#34920;&#31034;&#20026;&#22270;&#20687;&#65292;&#24182;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23567;&#33539;&#22260;&#30340;&#36319;&#36394;&#21306;&#22495;&#35757;&#32451;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36801;&#31227;&#21040;&#20855;&#26377;&#22823;&#37327;&#30446;&#26631;&#21644;&#20256;&#24863;&#22120;&#30340;&#26356;&#22823;&#33539;&#22260;&#12290;&#36825;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20351;&#24471;MTT&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#19978;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#26032;&#39062;&#20998;&#26512;&#30340;&#25903;&#25345;&#65292;&#35813;&#20998;&#26512;&#38480;&#21046;&#20102;&#27867;&#21270;&#35823;&#24046;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#21487;&#36801;&#31227;CNN&#26550;&#26500;&#22312;&#20855;&#26377;10&#20010;&#30446;&#26631;&#30340;MTT&#20219;&#21153;&#19978;&#20248;&#20110;&#38543;&#26426;&#26377;&#38480;&#38598;&#28388;&#27874;&#22120;&#65292;&#24182;&#19988;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#36801;&#31227;&#21040;&#20855;&#26377;250&#20010;&#30446;&#26631;&#30340;&#26356;&#22823;&#30340;MTT&#20219;&#21153;&#20013;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;29%&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-target tracking (MTT) is a classical signal processing task, where the goal is to estimate the states of an unknown number of moving targets from noisy sensor measurements. In this paper, we revisit MTT from a deep learning perspective and propose a convolutional neural network (CNN) architecture to tackle it. We represent the target states and sensor measurements as images and recast the problem as an image-to-image prediction task. Then we train a fully convolutional model at small tracking areas and transfer it to much larger areas with numerous targets and sensors. This transfer learning approach enables MTT at a large scale and is also theoretically supported by our novel analysis that bounds the generalization error. In practice, the proposed transferable CNN architecture outperforms random finite set filters on the MTT task with 10 targets and transfers without re-training to a larger MTT task with 250 targets with a 29% performance improvement.
&lt;/p&gt;</description></item><item><title>Teal&#26159;&#19968;&#31181;&#23398;&#20064;&#21152;&#36895;&#30340;&#24191;&#22495;&#32593;&#27969;&#37327;&#24037;&#31243;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;GPU&#30340;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#21152;&#36895;TE&#25511;&#21046;&#12290;&#23427;&#20351;&#29992;&#27969;&#20026;&#20013;&#24515;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#25417;WAN&#36830;&#25509;&#21644;&#32593;&#32476;&#27969;&#37327;&#65292;&#24182;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#29420;&#31435;&#20998;&#37197;&#21644;&#20013;&#24515;&#21270;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#23545;&#20998;&#37197;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2210.13763</link><description>&lt;p&gt;
Teal: &#23398;&#20064;&#21152;&#36895;&#30340;&#24191;&#22495;&#32593;&#27969;&#37327;&#24037;&#31243;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Teal: Learning-Accelerated Optimization of WAN Traffic Engineering. (arXiv:2210.13763v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13763
&lt;/p&gt;
&lt;p&gt;
Teal&#26159;&#19968;&#31181;&#23398;&#20064;&#21152;&#36895;&#30340;&#24191;&#22495;&#32593;&#27969;&#37327;&#24037;&#31243;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;GPU&#30340;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#21152;&#36895;TE&#25511;&#21046;&#12290;&#23427;&#20351;&#29992;&#27969;&#20026;&#20013;&#24515;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#25417;WAN&#36830;&#25509;&#21644;&#32593;&#32476;&#27969;&#37327;&#65292;&#24182;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#29420;&#31435;&#20998;&#37197;&#21644;&#20013;&#24515;&#21270;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#23545;&#20998;&#37197;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#20113;&#24191;&#22495;&#32593;&#65288;WAN&#65289;&#30340;&#24555;&#36895;&#25193;&#23637;&#32473;&#21830;&#19994;&#20248;&#21270;&#24341;&#25806;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22914;&#20309;&#39640;&#25928;&#35299;&#20915;&#35268;&#27169;&#21270;&#30340;&#32593;&#32476;&#27969;&#37327;&#24037;&#31243;&#65288;TE&#65289;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#21152;&#36895;&#31574;&#30053;&#23558;TE&#20248;&#21270;&#20998;&#35299;&#20026;&#24182;&#21457;&#30340;&#23376;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#36816;&#34892;&#26102;&#38388;&#21644;&#20998;&#37197;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#29616;&#30340;&#24182;&#34892;&#21270;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Teal&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;TE&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;GPU&#30340;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#21152;&#36895;TE&#25511;&#21046;&#12290;&#39318;&#20808;&#65292;Teal&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#27969;&#20026;&#20013;&#24515;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#25429;&#25417;WAN&#36830;&#25509;&#21644;&#32593;&#32476;&#27969;&#37327;&#65292;&#23398;&#20064;&#27969;&#29305;&#24449;&#20316;&#20026;&#19979;&#28216;&#20998;&#37197;&#30340;&#36755;&#20837;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#20943;&#23569;&#38382;&#39064;&#35268;&#27169;&#24182;&#20351;&#23398;&#20064;&#21487;&#34892;&#65292;Teal&#37319;&#29992;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#29420;&#31435;&#20998;&#37197;&#27599;&#20010;&#27969;&#37327;&#38656;&#27714;&#65292;&#21516;&#26102;&#20248;&#21270;&#19968;&#20010;&#20013;&#24515;&#30340;TE&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;Teal&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#23545;&#20998;&#37197;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid expansion of global cloud wide-area networks (WANs) has posed a challenge for commercial optimization engines to efficiently solve network traffic engineering (TE) problems at scale. Existing acceleration strategies decompose TE optimization into concurrent subproblems but realize limited parallelism due to an inherent tradeoff between run time and allocation performance.  We present Teal, a learning-based TE algorithm that leverages the parallel processing power of GPUs to accelerate TE control. First, Teal designs a flow-centric graph neural network (GNN) to capture WAN connectivity and network flows, learning flow features as inputs to downstream allocation. Second, to reduce the problem scale and make learning tractable, Teal employs a multi-agent reinforcement learning (RL) algorithm to independently allocate each traffic demand while optimizing a central TE objective. Finally, Teal fine-tunes allocations with ADMM (Alternating Direction Method of Multipliers), a highly 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#21487;&#20197;&#25552;&#39640;&#36793;&#38469;&#20284;&#28982;&#65292;&#20294;&#20132;&#21449;&#39564;&#35777;&#24230;&#37327;&#34920;&#29616;&#20986;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2210.07612</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#21333;&#35843;&#24615;&#21644;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes. (arXiv:2210.07612v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#21487;&#20197;&#25552;&#39640;&#36793;&#38469;&#20284;&#28982;&#65292;&#20294;&#20132;&#21449;&#39564;&#35777;&#24230;&#37327;&#34920;&#29616;&#20986;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35780;&#20272;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#25351;&#26631;&#30452;&#21040;&#26368;&#36817;&#25165;&#24320;&#22987;&#24471;&#21040;&#20005;&#26684;&#30340;&#34920;&#24449;&#12290;&#19968;&#20010;&#26174;&#33879;&#38382;&#39064;&#26159;&#32500;&#24230;&#35781;&#21650;&#65306;&#26222;&#36941;&#35748;&#20026;&#36793;&#32536;&#20284;&#28982;&#24212;&#35813;&#19982;&#20132;&#21449;&#39564;&#35777;&#24230;&#37327;&#31867;&#20284;&#65292;&#24182;&#19988;&#20004;&#32773;&#22312;&#36755;&#20837;&#32500;&#24230;&#36739;&#22823;&#26102;&#37117;&#20250;&#24694;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20197;&#26368;&#22823;&#21270;&#36793;&#38469;&#20284;&#28982;&#65288;&#32463;&#39564;&#36125;&#21494;&#26031;&#36807;&#31243;&#65289;&#65292;&#24615;&#33021;&#65288;&#20197;&#36793;&#38469;&#20284;&#28982;&#27979;&#37327;&#65289;&#38543;&#30528;&#36755;&#20837;&#32500;&#24230;&#30340;&#22686;&#21152;&#21333;&#35843;&#25913;&#21892;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20132;&#21449;&#39564;&#35777;&#24230;&#37327;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#29305;&#24449;&#65292;&#21363;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#26368;&#36817;&#22240;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#24615;&#33021;&#25552;&#39640;&#32780;&#21463;&#21040;&#20851;&#27880;&#30340;&#20919;&#24577;&#21518;&#39564;&#20284;&#20046;&#21152;&#21095;&#20102;&#36825;&#20123;&#29616;&#35937;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#25104;&#31435;&#65292;&#36229;&#20986;&#25105;&#20204;&#32771;&#34385;&#30340;&#20551;&#35774;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their importance for assessing reliability of predictions, uncertainty quantification (UQ) measures for machine learning models have only recently begun to be rigorously characterized. One prominent issue is the curse of dimensionality: it is commonly believed that the marginal likelihood should be reminiscent of cross-validation metrics and that both should deteriorate with larger input dimensions. We prove that by tuning hyperparameters to maximize marginal likelihood (the empirical Bayes procedure), the performance, as measured by the marginal likelihood, improves monotonically} with the input dimension. On the other hand, we prove that cross-validation metrics exhibit qualitatively different behavior that is characteristic of double descent. Cold posteriors, which have recently attracted interest due to their improved performance in certain settings, appear to exacerbate these phenomena. We verify empirically that our results hold for real data, beyond our considered assump
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#26377;&#20851;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#12289;&#25915;&#20987;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#19982;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2210.03829</link><description>&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#26377;&#20851;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#12289;&#25915;&#20987;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#19982;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#24341;&#36215;&#30340;&#26641;&#26408;&#27515;&#20129;&#26041;&#38754;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#20174;&#26641;&#30382;&#30002;&#34411;&#19982;&#23492;&#20027;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12289;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#36827;&#34892;&#20102;&#24635;&#32467;&#12290;&#19982;&#20197;&#24448;&#30340;&#21162;&#21147;&#30456;&#21453;&#65292;&#26412;&#32508;&#36848;&#21253;&#25324;&#20102;&#25152;&#26377;&#36965;&#24863;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26681;&#25454;&#22810;&#20809;&#35889;&#25110;&#39640;&#20809;&#35889;&#20998;&#26512;&#35299;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#24182;&#20174;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#21644;&#25915;&#20987;&#38454;&#27573;&#65292;&#37325;&#28857;&#20851;&#27880;&#25915;&#20987;&#30340;&#26089;&#26399;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#21644;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;/&#31354;&#38388;/&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#20809;&#35889;&#26893;&#34987;&#25351;&#25968;&#65288;SVIs&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#23398;&#20064;&#26041;&#26696;&#12289;&#20219;&#21153;&#31867;&#21035;&#12289;&#27169;&#22411;&#12289;&#31639;&#27861;&#12289;&#31867;&#21035;/&#31751;&#12289;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#19982;&#26550;&#26500;&#26041;&#38754;&#25552;&#21462;&#30693;&#35782;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#21487;&#35265;&#20809;&#21644;&#28909;&#32418;&#22806;&#31561;&#27874;&#27573;&#19978;&#26816;&#27979;&#24494;&#23567;&#21464;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive review of past and current advances in the early detection of bark beetle-induced tree mortality from three primary perspectives: bark beetle &amp; host interactions, RS, and ML/DL. In contrast to prior efforts, this review encompasses all RS systems and emphasizes ML/DL methods to investigate their strengths and weaknesses. We parse existing literature based on multi- or hyper-spectral analyses and distill their knowledge based on: bark beetle species &amp; attack phases with a primary emphasis on early stages of attacks, host trees, study regions, RS platforms &amp; sensors, spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation indices (SVIs), ML approaches, learning schemes, task categories, models, algorithms, classes/clusters, features, and DL networks &amp; architectures. Although DL-based methods and the random forest (RF) algorithm showed promising results, highlighting their potential to detect subtle changes across visible, therma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21464;&#37327;&#25968;&#37327;&#36828;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#22823;&#35268;&#27169;&#22240;&#26524;&#22810;&#26641;&#32467;&#26500;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#20219;&#20309;&#20998;&#24067;&#25110;&#24314;&#27169;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2209.07028</link><description>&lt;p&gt;
&#20174;&#23567;&#26679;&#26412;&#20013;&#20272;&#35745;&#22823;&#30340;&#22240;&#26524;&#22810;&#26641;
&lt;/p&gt;
&lt;p&gt;
Estimating large causal polytrees from small samples. (arXiv:2209.07028v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21464;&#37327;&#25968;&#37327;&#36828;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#22823;&#35268;&#27169;&#22240;&#26524;&#22810;&#26641;&#32467;&#26500;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#20219;&#20309;&#20998;&#24067;&#25110;&#24314;&#27169;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#30456;&#23545;&#36739;&#23567;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#20013;&#20272;&#35745;&#22823;&#30340;&#22240;&#26524;&#22810;&#26641;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#22312;&#21464;&#37327;&#25968;&#37327;&#19982;&#26679;&#26412;&#22823;&#23567;&#30456;&#27604;&#38750;&#24120;&#22823;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#22240;&#26524;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20197;&#39640;&#20934;&#30830;&#24230;&#24674;&#22797;&#26641;&#24418;&#32467;&#26500;&#12290;&#35813;&#31639;&#27861;&#38500;&#20102;&#19968;&#20123;&#28201;&#21644;&#30340;&#38750;&#36864;&#21270;&#26465;&#20214;&#22806;&#65292;&#22522;&#26412;&#19981;&#38656;&#35201;&#20998;&#24067;&#25110;&#24314;&#27169;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating a large causal polytree from a relatively small i.i.d. sample. This is motivated by the problem of determining causal structure when the number of variables is very large compared to the sample size, such as in gene regulatory networks. We give an algorithm that recovers the tree with high accuracy in such settings. The algorithm works under essentially no distributional or modeling assumptions other than some mild non-degeneracy conditions.
&lt;/p&gt;</description></item><item><title>AudioLM&#26159;&#19968;&#20010;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#23454;&#29616;&#38899;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#36755;&#20837;&#38899;&#39057;&#26144;&#23556;&#20026;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#31526;&#21512;&#35821;&#27861;&#21644;&#35821;&#20041;&#35201;&#27714;&#30340;&#36830;&#32493;&#38899;&#39057;&#12290;</title><link>http://arxiv.org/abs/2209.03143</link><description>&lt;p&gt;
AudioLM&#65306;&#19968;&#31181;&#38899;&#39057;&#29983;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AudioLM: a Language Modeling Approach to Audio Generation. (arXiv:2209.03143v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03143
&lt;/p&gt;
&lt;p&gt;
AudioLM&#26159;&#19968;&#20010;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#23454;&#29616;&#38899;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#36755;&#20837;&#38899;&#39057;&#26144;&#23556;&#20026;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#31526;&#21512;&#35821;&#27861;&#21644;&#35821;&#20041;&#35201;&#27714;&#30340;&#36830;&#32493;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;AudioLM&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#38899;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#38271;&#26399;&#19968;&#33268;&#24615;&#12290;AudioLM&#23558;&#36755;&#20837;&#38899;&#39057;&#26144;&#23556;&#20026;&#19968;&#31995;&#21015;&#31163;&#25955;&#30340;&#31526;&#21495;&#65292;&#24182;&#23558;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#38899;&#39057;&#20998;&#35789;&#24037;&#20855;&#22312;&#37325;&#26500;&#36136;&#37327;&#21644;&#38271;&#26399;&#32467;&#26500;&#20043;&#38388;&#25552;&#20379;&#30340;&#19981;&#21516;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20998;&#35789;&#26041;&#26696;&#20197;&#23454;&#29616;&#20004;&#20010;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#38899;&#39057;&#19978;&#39044;&#35757;&#32451;&#30340;&#24102;&#26377;&#25513;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31163;&#25955;&#28608;&#27963;&#26469;&#25429;&#25417;&#38271;&#26399;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#20135;&#29983;&#30340;&#31163;&#25955;&#32534;&#30721;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;&#36890;&#36807;&#22312;&#22823;&#22411;&#21407;&#22987;&#38899;&#39057;&#27874;&#24418;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;AudioLM&#23398;&#20064;&#20102;&#22312;&#32473;&#23450;&#30701;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#33258;&#28982;&#36830;&#32493;&#30340;&#33021;&#21147;&#12290;&#24403;&#22312;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#19988;&#27809;&#26377;&#20219;&#20309;&#36716;&#24405;&#25110;&#27880;&#37322;&#26102;&#65292;AudioLM&#29983;&#25104;&#30340;&#35821;&#38899;&#36830;&#32493;&#20445;&#25345;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#21512;&#29702;&#24615;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#26410;&#35265;&#36807;&#30340;&#35828;&#35805;&#32773;&#36523;&#20221;&#21644;&#38901;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;Shapley&#20540;&#35299;&#37322;&#26041;&#27861;&#19982;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20043;&#38388;&#30340;&#32039;&#23494;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#38543;&#26426;&#21270;&#27979;&#35797;&#30340;&#27979;&#35797;&#36807;&#31243;SHAP-XRT&#65292;&#29992;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;Shapley&#20540;&#30340;&#36793;&#38469;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2207.07038</link><description>&lt;p&gt;
SHAP-XRT: Shapley Value&#36935;&#19978;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SHAP-XRT: The Shapley Value Meets Conditional Independence Testing. (arXiv:2207.07038v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;Shapley&#20540;&#35299;&#37322;&#26041;&#27861;&#19982;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20043;&#38388;&#30340;&#32039;&#23494;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#38543;&#26426;&#21270;&#27979;&#35797;&#30340;&#27979;&#35797;&#36807;&#31243;SHAP-XRT&#65292;&#29992;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;Shapley&#20540;&#30340;&#36793;&#38469;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#24341;&#21457;&#20102;&#23545;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#20851;&#27880;&#12290;Shapley&#20540;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26368;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#20043;&#19968;&#12290;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#29305;&#24449;&#37325;&#35201;&#24615;&#26159;&#36890;&#36807;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23450;&#20041;&#30340;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20004;&#31181;&#35299;&#37322;&#26041;&#27861;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#34987;&#35748;&#20026;&#26159;&#20998;&#24320;&#30340;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;Shapley&#20540;&#35299;&#37322;&#26041;&#27861;&#19982;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20043;&#38388;&#30340;&#32039;&#23494;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SHAPley-E$\textbf{X}$planation $\textbf{R}$andomization $\textbf{T}$est (SHAP-XRT)&#65292;&#19968;&#31181;&#21463;&#26465;&#20214;&#38543;&#26426;&#21270;&#27979;&#35797;(CRT)&#21551;&#21457;&#30340;&#27979;&#35797;&#36807;&#31243;&#65292;&#29992;&#20110;&#26816;&#39564;&#29305;&#23450;&#27010;&#24565;&#30340;&#23616;&#37096;&#65288;&#22312;&#26679;&#26412;&#19978;&#30340;&#65289;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;SHAP-XRT&#65292;&#22312;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;Shapley&#20540;&#30340;&#36793;&#38469;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex nature of artificial neural networks raises concerns on their reliability, trustworthiness, and fairness in real-world scenarios. The Shapley value -- a solution concept from game theory -- is one of the most popular explanation methods for machine learning models. More traditionally, from a statistical perspective, feature importance is defined in terms of conditional independence. So far, these two approaches to interpretability and feature importance have been considered separate and distinct. In this work, we show that Shapley-based explanation methods and conditional independence testing are closely related. We introduce the $\textbf{SHAP}$ley-E$\textbf{X}$planation $\textbf{R}$andomization $\textbf{T}$est (SHAP-XRT), a testing procedure inspired by the Conditional Randomization Test (CRT) for a specific notion of local (i.e., on a sample) conditional independence. With it, we prove that for binary classification problems, the marginal contributions in the Shapley valu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26799;&#24230;&#19979;&#38477;&#23545;&#20110;&#38750;Lipschitz&#26799;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#65292;&#21457;&#29616;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#23613;&#31649;&#23384;&#22312;&#23616;&#37096;&#19981;&#31283;&#23450;&#24615;&#21644;&#25391;&#33633;&#34892;&#20026;&#65292;&#26799;&#24230;&#19979;&#38477;&#20173;&#28982;&#33021;&#22815;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2206.04172</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#27493;&#26799;&#24230;&#26356;&#26032;&#36229;&#36234;&#31283;&#23450;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Beyond the Edge of Stability via Two-step Gradient Updates. (arXiv:2206.04172v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26799;&#24230;&#19979;&#38477;&#23545;&#20110;&#38750;Lipschitz&#26799;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#65292;&#21457;&#29616;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#23613;&#31649;&#23384;&#22312;&#23616;&#37096;&#19981;&#31283;&#23450;&#24615;&#21644;&#25391;&#33633;&#34892;&#20026;&#65292;&#26799;&#24230;&#19979;&#38477;&#20173;&#28982;&#33021;&#22815;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22240;&#20854;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#26799;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;GD&#21482;&#33021;&#25214;&#21040;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#28508;&#22312;&#26799;&#24230;&#27969;&#30340;&#8220;&#30495;&#23454;&#8221;&#31163;&#25955;&#21270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#28041;&#21450;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#24182;&#19981;&#23646;&#20110;&#36825;&#20010;&#38382;&#39064;&#31867;&#21035;&#65292;&#36825;&#20419;&#20351;&#30740;&#31350;&#36229;&#36234;&#25152;&#35859;&#30340;&#8220;&#31283;&#23450;&#36793;&#30028;&#8221;&#65288;Edge of Stability&#65292;EoS&#65289;&#65292;&#20854;&#20013;&#27493;&#38271;&#36234;&#36807;&#19982;Lipschitz&#24120;&#25968;&#25104;&#21453;&#27604;&#30340;&#21487;&#20801;&#35768;&#38408;&#20540;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#32463;&#39564;&#35777;&#26126;&#23613;&#31649;&#23384;&#22312;&#23616;&#37096;&#19981;&#31283;&#23450;&#24615;&#21644;&#25391;&#33633;&#34892;&#20026;&#65292;GD&#20173;&#28982;&#25910;&#25947;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#21021;&#27493;&#29702;&#35770;&#20998;&#26512;&#20027;&#35201;&#38598;&#20013;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#33539;&#22260;&#20869;&#65292;&#22312;&#27492;&#33539;&#22260;&#20869;&#36873;&#25321;&#36739;&#22823;&#30340;&#23398;&#20064;&#29575;&#21487;&#33021;&#19982;&#22312;&#26368;&#23567;&#21270;&#22120;&#27969;&#24418;&#20869;&#38544;&#21547;&#30340;&#8220;&#23574;&#24230;&#26368;&#23567;&#21270;&#8221;&#27491;&#21017;&#21270;&#30456;&#20851;&#65292;&#35831;&#35814;&#35265;&#35770;&#25991;&#20102;&#35299;&#26356;&#22810;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient Descent (GD) is a powerful workhorse of modern machine learning thanks to its scalability and efficiency in high-dimensional spaces. Its ability to find local minimisers is only guaranteed for losses with Lipschitz gradients, where it can be seen as a `bona-fide' discretisation of an underlying gradient flow. Yet, many ML setups involving overparametrised models do not fall into this problem class, which has motivated research beyond the so-called ``Edge of Stability'' (EoS), where the step-size crosses the admissibility threshold inversely proportional to the Lipschitz constant above. Perhaps surprisingly, GD has been empirically observed to still converge regardless of local instability and oscillatory behavior.  The incipient theoretical analysis of this phenomena has mainly focused in the overparametrised regime, where the effect of choosing a large learning rate may be associated to a `Sharpness-Minimisation' implicit regularisation within the manifold of minimisers, unde
&lt;/p&gt;</description></item><item><title>TreeFlow&#26159;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#21644;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#20998;&#24067;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.04140</link><description>&lt;p&gt;
TreeFlow&#65306;&#36229;&#36234;&#22522;&#20110;&#26641;&#30340;&#39640;&#26031;&#27010;&#29575;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
TreeFlow: Going beyond Tree-based Gaussian Probabilistic Regression. (arXiv:2206.04140v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04140
&lt;/p&gt;
&lt;p&gt;
TreeFlow&#26159;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#21644;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#20998;&#24067;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#28151;&#21512;&#31867;&#22411;&#21464;&#37327;&#20026;&#29305;&#24449;&#21521;&#37327;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#65292;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#23427;&#20204;&#20027;&#35201;&#35774;&#35745;&#20026;&#25552;&#20379;&#30830;&#23450;&#24615;&#21709;&#24212;&#25110;&#20351;&#29992;&#39640;&#26031;&#25110;&#21442;&#25968;&#20998;&#24067;&#23545;&#36755;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TreeFlow&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#20351;&#29992;&#26641;&#38598;&#25104;&#21644;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#28789;&#27963;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#35299;&#20915;&#26041;&#26696;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#23558;&#20854;&#19982;&#24402;&#19968;&#21270;&#27969;&#30340;&#26465;&#20214;&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#22238;&#24402;&#36755;&#20986;&#36827;&#34892;&#22797;&#26434;&#20998;&#24067;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#20307;&#31215;&#12289;&#29305;&#24449;&#29305;&#24615;&#21644;&#30446;&#26631;&#32500;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22238;&#24402;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tree-based ensembles are known for their outstanding performance in classification and regression problems characterized by feature vectors represented by mixed-type variables from various ranges and domains. However, considering regression problems, they are primarily designed to provide deterministic responses or model the uncertainty of the output with Gaussian or parametric distribution. In this work, we introduce TreeFlow, the tree-based approach that combines the benefits of using tree ensembles with the capabilities of modeling flexible probability distributions using normalizing flows. The main idea of the solution is to use a tree-based model as a feature extractor and combine it with a conditional variant of normalizing flow. Consequently, our approach is capable of modeling complex distributions for the regression outputs. We evaluate the proposed method on challenging regression benchmarks with varying volume, feature characteristics, and target dimensionality. We obtai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaxNet&#30340;&#29289;&#29702;&#24863;&#30693;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#23398;&#20064;3D&#23567;&#20998;&#23376;&#21644;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;&#30340;&#34920;&#31034;&#12290;PaxNet&#36890;&#36807;&#20998;&#31163;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#24314;&#27169;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#35282;&#24230;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#30690;&#37327;&#23646;&#24615;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.02789</link><description>&lt;p&gt;
&#39640;&#25928;&#20934;&#30830;&#30340;&#29289;&#29702;&#24863;&#30693;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;3D&#23567;&#20998;&#23376;&#21644;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;
&lt;/p&gt;
&lt;p&gt;
Efficient and Accurate Physics-aware Multiplex Graph Neural Networks for 3D Small Molecules and Macromolecule Complexes. (arXiv:2206.02789v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaxNet&#30340;&#29289;&#29702;&#24863;&#30693;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#23398;&#20064;3D&#23567;&#20998;&#23376;&#21644;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;&#30340;&#34920;&#31034;&#12290;PaxNet&#36890;&#36807;&#20998;&#31163;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#24314;&#27169;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#35282;&#24230;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#30690;&#37327;&#23646;&#24615;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#20351;&#29992;GNNs&#23398;&#20064;&#19977;&#32500;&#65288;3D&#65289;&#32467;&#26500;&#34920;&#31034;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GNNs&#23384;&#22312;&#22810;&#26679;&#24615;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#19981;&#36275;&#12289;&#35745;&#31639;&#24320;&#38144;&#22823;&#21644;&#24573;&#35270;&#30690;&#37327;&#20540;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#27169;&#22411;&#65292;&#21363;&#29289;&#29702;&#24863;&#30693;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PaxNet&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#20197;&#20415;&#22312;&#23567;&#26377;&#26426;&#21270;&#21512;&#29289;&#21644;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;&#30340;3D&#20998;&#23376;&#34920;&#31034;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#23398;&#20064;&#12290;PaxNet&#36890;&#36807;&#20998;&#31163;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#24314;&#27169;&#65292;&#21463;&#21040;&#20998;&#23376;&#21147;&#23398;&#30340;&#21551;&#21457;&#65292;&#24182;&#20943;&#23569;&#20102;&#26114;&#36149;&#30340;&#19982;&#35282;&#24230;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;&#38500;&#20102;&#26631;&#37327;&#23646;&#24615;&#22806;&#65292;PaxNet&#36824;&#21487;&#20197;&#36890;&#36807;&#20026;&#27599;&#20010;&#21407;&#23376;&#23398;&#20064;&#19968;&#20010;&#30456;&#20851;&#32852;&#30340;&#30690;&#37327;&#26469;&#39044;&#27979;&#30690;&#37327;&#23646;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;PaxNet&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20004;&#20010;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in applying Graph Neural Networks (GNNs) to molecular science have showcased the power of learning three-dimensional (3D) structure representations with GNNs. However, most existing GNNs suffer from the limitations of insufficient modeling of diverse interactions, computational expensive operations, and ignorance of vectorial values. Here, we tackle these limitations by proposing a novel GNN model, Physics-aware Multiplex Graph Neural Network (PaxNet), to efficiently and accurately learn the representations of 3D molecules for both small organic compounds and macromolecule complexes. PaxNet separates the modeling of local and non-local interactions inspired by molecular mechanics, and reduces the expensive angle-related computations. Besides scalar properties, PaxNet can also predict vectorial properties by learning an associated vector for each atom. To evaluate the performance of PaxNet, we compare it with state-of-the-art baselines in two tasks. On small molecule dat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#38024;&#23545;&#25512;&#33616;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24212;&#29992;&#26469;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13619</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#22522;&#30784;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#38024;&#23545;&#25512;&#33616;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24212;&#29992;&#26469;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26368;&#26222;&#36941;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#25512;&#33616;&#31995;&#32479;&#22312;&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#21644;&#24179;&#21488;&#30340;&#21033;&#30410;&#19982;&#29983;&#25104;&#30340;&#25512;&#33616;&#32467;&#26524;&#30340;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#39640;&#24230;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#21463;&#21040;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#21066;&#24369;&#31995;&#32479;&#30340;&#21487;&#20449;&#36182;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#33616;&#35774;&#32622;&#20013;&#35299;&#20915;&#28508;&#22312;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#32771;&#34385;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#28041;&#21450;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30456;&#23545;&#38646;&#25955;&#19988;&#32570;&#20047;&#31995;&#32479;&#21270;&#25972;&#29702;&#65292;&#22240;&#27492;&#23545;&#20110;&#26032;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#38590;&#20197;&#28145;&#20837;&#39046;&#22495;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23545;&#25512;&#33616;&#20013;&#29616;&#26377;&#20844;&#24179;&#24615;&#20316;&#21697;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24378;&#20581;&#30340;&#25968;&#37327;&#24863;&#30693;&#32858;&#21512;&#31639;&#27861;&#65292;&#21517;&#20026;FedRA&#65292;&#33021;&#22815;&#22312;&#32858;&#21512;&#27169;&#22411;&#26102;&#32771;&#34385;&#26412;&#22320;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#33021;&#22815;&#25269;&#24481;&#25968;&#37327;&#22686;&#24378;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2205.10848</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#25968;&#37327;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Quantity-Aware Aggregation for Federated Learning. (arXiv:2205.10848v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24378;&#20581;&#30340;&#25968;&#37327;&#24863;&#30693;&#32858;&#21512;&#31639;&#27861;&#65292;&#21517;&#20026;FedRA&#65292;&#33021;&#22815;&#22312;&#32858;&#21512;&#27169;&#22411;&#26102;&#32771;&#34385;&#26412;&#22320;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#33021;&#22815;&#25269;&#24481;&#25968;&#37327;&#22686;&#24378;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;FL&#38754;&#20020;&#20005;&#37325;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#24694;&#24847;&#30340;&#23458;&#25143;&#31471;&#21487;&#20197;&#27745;&#26579;&#27169;&#22411;&#26356;&#26032;&#65292;&#24182;&#21516;&#26102;&#34394;&#25253;&#22823;&#37327;&#20197;&#25918;&#22823;&#20854;&#22312;&#27169;&#22411;&#32858;&#21512;&#20013;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;FL&#38450;&#24481;&#26041;&#27861;&#65292;&#34429;&#28982;&#37117;&#33021;&#22788;&#29702;&#24694;&#24847;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20294;&#35201;&#20040;&#23558;&#25152;&#26377;&#25968;&#37327;&#35270;&#20026;&#33391;&#24615;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#24573;&#30053;/&#25130;&#26029;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#12290;&#21069;&#32773;&#23481;&#26131;&#21463;&#21040;&#25968;&#37327;&#22686;&#24378;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#23548;&#33268;&#23376;&#20248;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#26412;&#22320;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181; robust quantity-aware aggregation algorithm for federated learning (FedRA)&#65292;&#36890;&#36807;&#23545;&#26412;&#22320;&#25968;&#25454;&#25968;&#37327;&#30340;&#24863;&#30693;&#26469;&#25191;&#34892;&#32858;&#21512;&#65292;&#24182;&#33021;&#22815;&#25269;&#24481;&#25968;&#37327;&#22686;&#24378;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple clients to collaboratively train models without sharing their local data, and becomes an important privacy-preserving machine learning framework. However, classical FL faces serious security and robustness problem, e.g., malicious clients can poison model updates and at the same time claim large quantities to amplify the impact of their model updates in the model aggregation. Existing defense methods for FL, while all handling malicious model updates, either treat all quantities benign or simply ignore/truncate the quantities of all clients. The former is vulnerable to quantity-enhanced attack, while the latter leads to sub-optimal performance since the local data on different clients is usually in significantly different sizes. In this paper, we propose a robust quantity-aware aggregation algorithm for federated learning, called FedRA, to perform the aggregation with awareness of local data quantities while being able to defend against quantity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#20851;&#20110;&#25968;&#20540;&#29305;&#24449;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26500;&#24314;&#23884;&#20837;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#27169;&#22359;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#22312;&#19968;&#20123;&#20256;&#32479;&#19978;&#36866;&#29992;&#20110;GBDT&#30340;&#22522;&#20934;&#19978;&#19982;&#20043;&#31454;&#20105;&#20855;&#26377;&#37325;&#35201;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2203.05556</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#20540;&#29305;&#24449;&#22312;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
On Embeddings for Numerical Features in Tabular Deep Learning. (arXiv:2203.05556v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#20851;&#20110;&#25968;&#20540;&#29305;&#24449;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26500;&#24314;&#23884;&#20837;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#27169;&#22359;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#22312;&#19968;&#20123;&#20256;&#32479;&#19978;&#36866;&#29992;&#20110;GBDT&#30340;&#22522;&#20934;&#19978;&#19982;&#20043;&#31454;&#20105;&#20855;&#26377;&#37325;&#35201;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31867;&#20284;Transformer&#30340;&#28145;&#24230;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38382;&#39064;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#65288;&#22914;MLP&#65289;&#19981;&#21516;&#65292;&#36825;&#20123;&#26550;&#26500;&#23558;&#25968;&#20540;&#29305;&#24449;&#30340;&#26631;&#37327;&#20540;&#26144;&#23556;&#21040;&#39640;&#32500;&#23884;&#20837;&#20013;&#65292;&#28982;&#21518;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#23558;&#23427;&#20204;&#28151;&#21512;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#25968;&#20540;&#29305;&#24449;&#30340;&#23884;&#20837;&#22312;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;&#20013;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#33258;&#30001;&#24230;&#65292;&#23427;&#20801;&#35768;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#19978;&#36866;&#29992;&#20110;GBDT&#30340;&#22522;&#20934;&#36827;&#34892;&#31454;&#20105;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#26500;&#24314;&#23884;&#20837;&#27169;&#22359;&#30340;&#20004;&#31181;&#27010;&#24565;&#19978;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#31181;&#22522;&#20110;&#26631;&#37327;&#20540;&#30340;&#20998;&#27573;&#32447;&#24615;&#32534;&#30721;&#65292;&#31532;&#20108;&#31181;&#21033;&#29992;&#21608;&#26399;&#24615;&#28608;&#27963;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#19982;&#22522;&#20110;&#32447;&#24615;&#23618;&#21644;ReLU&#28608;&#27963;&#30340;&#20256;&#32479;&#27169;&#22359;&#30456;&#27604;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23884;&#20837;&#25968;&#20540;&#29305;&#24449;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with GBDT on some traditionally GBDT-friendly benchmarks. We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MICDIR&#26041;&#27861;&#65292;&#22312;&#22810;&#23610;&#24230;&#19978;&#20351;&#29992;UNetMSS&#21644;&#33258;&#26500;&#24314;&#22270;&#28508;&#21464;&#37327;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#20013;&#23384;&#22312;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#22823;&#24418;&#21464;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.04317</link><description>&lt;p&gt;
MICDIR: &#20351;&#29992;UNetMSS&#21644;&#33258;&#26500;&#24314;&#22270;&#28508;&#21464;&#37327;&#30340;&#22810;&#23610;&#24230;&#21453;&#21521;&#19968;&#33268;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent. (arXiv:2203.04317v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MICDIR&#26041;&#27861;&#65292;&#22312;&#22810;&#23610;&#24230;&#19978;&#20351;&#29992;UNetMSS&#21644;&#33258;&#26500;&#24314;&#22270;&#28508;&#21464;&#37327;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#20013;&#23384;&#22312;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#22823;&#24418;&#21464;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#37197;&#20934;&#26159;&#23558;&#19981;&#21516;&#22270;&#20687;&#24102;&#20837;&#21040;&#19968;&#20010;&#20849;&#21516;&#22352;&#26631;&#31995;&#30340;&#36807;&#31243;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#22914;&#36965;&#24863;&#12289;&#22270;&#20687;&#26816;&#32034;&#21644;&#21307;&#23398;&#25104;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#22797;&#26434;&#30340;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#25552;&#20986;&#20102;&#20960;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#37197;&#20934;&#25216;&#26415;&#12290;&#20363;&#22914;Voxelmorph&#31561;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#25216;&#26415;&#65292;&#25104;&#21151;&#25429;&#25417;&#21040;&#26356;&#32454;&#24494;&#30340;&#21464;&#21270;&#24182;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#21464;&#24418;&#12290;&#28982;&#32780;&#65292;Voxelmorph&#12289;ICNet&#21644;FIRE&#31561;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#32534;&#30721;&#20840;&#23616;&#20381;&#36182;&#24615;&#65288;&#21363;&#25152;&#25552;&#20379;&#22270;&#20687;&#30340;&#25972;&#20307;&#35299;&#21078;&#35270;&#22270;&#65289;&#65292;&#22240;&#27492;&#19981;&#33021;&#36319;&#36394;&#22823;&#30340;&#24418;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#20197;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;Voxelmorph&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image registration is the process of bringing different images into a common coordinate system - a technique widely used in various applications of computer vision, such as remote sensing, image retrieval, and, most commonly, medical imaging. Deep learning based techniques have been applied successfully to tackle various complex medical image processing problems, including medical image registration. Over the years, several image registration techniques have been proposed using deep learning. Deformable image registration techniques such as Voxelmorph have been successful in capturing finer changes and providing smoother deformations. However, Voxelmorph, as well as ICNet and FIRE, do not explicitly encode global dependencies (i.e. the overall anatomical view of the supplied image) and, therefore, cannot track large deformations. In order to tackle the aforementioned problems, this paper extends the Voxelmorph approach in three different ways. To improve the performance in case of smal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaDT&#30340;&#26032;&#39062;&#20803;&#23398;&#20064;&#20915;&#31574;&#26641;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#21644;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;FSL&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20174;&#32780;&#22312;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#35782;&#21035;&#26032;&#31867;&#21035;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2203.01482</link><description>&lt;p&gt;
MetaDT: &#20351;&#29992;&#31867;&#23618;&#27425;&#32467;&#26500;&#30340;&#20803;&#20915;&#31574;&#26641;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MetaDT: Meta Decision Tree with Class Hierarchy for Interpretable Few-Shot Learning. (arXiv:2203.01482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaDT&#30340;&#26032;&#39062;&#20803;&#23398;&#20064;&#20915;&#31574;&#26641;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#21644;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;FSL&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20174;&#32780;&#22312;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#35782;&#21035;&#26032;&#31867;&#21035;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#35782;&#21035;&#26032;&#31867;&#21035;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#26041;&#27861;&#20174;&#20803;&#23398;&#20064;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;FSL&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#20915;&#31574;&#26641;&#26694;&#26550;MetaDT&#65292;&#20174;&#21487;&#35299;&#37322;&#30340;FSL&#35282;&#24230;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FSL&#30340;&#21487;&#35299;&#37322;&#24615;&#20174;&#27010;&#24565;&#21644;&#35270;&#35273;&#20004;&#20010;&#26041;&#38754;&#23454;&#29616;&#12290;&#22312;&#27010;&#24565;&#26041;&#38754;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#31867;&#20284;&#26641;&#29366;&#30340;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;FSL&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#20808;&#39564;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#23569;&#26679;&#26412;&#20219;&#21153;&#20998;&#21106;&#20026;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#27010;&#24565;&#23618;&#27425;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#20915;&#31574;&#26641;&#27169;&#22411;&#36827;&#34892;&#31867;&#21035;&#39044;&#27979;&#12290;&#36825;&#31181;&#35774;&#35745;&#30340;&#20248;&#21183;&#22312;&#20110;&#21487;&#20197;&#33719;&#24471;&#19968;&#31995;&#21015;&#23548;&#33268;&#26368;&#32456;&#31867;&#21035;&#39044;&#27979;&#30340;&#39640;&#32423;&#27010;&#24565;&#20915;&#31574;&#65292;&#20174;&#32780;&#26126;&#30830;FSL&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#35270;&#35273;&#26041;&#38754;...
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning (FSL) is a challenging task, which aims to recognize novel classes with few examples. Recently, lots of methods have been proposed from the perspective of meta-learning and representation learning. However, few works focus on the interpretability of FSL decision process. In this paper, we take a step towards the interpretable FSL by proposing a novel meta-learning based decision tree framework, namely, MetaDT. In particular, the FSL interpretability is achieved from two aspects, i.e., a concept aspect and a visual aspect. On the concept aspect, we first introduce a tree-like concept hierarchy as FSL prior. Then, resorting to the prior, we split each few-shot task to a set of subtasks with different concept levels and then perform class prediction via a model of decision tree. The advantage of such design is that a sequence of high-level concept decisions that lead up to a final class prediction can be obtained, which clarifies the FSL decision process. On the visual a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#29992;&#25143;&#25351;&#21335;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#31934;&#31639;&#23454;&#36341;&#20013;&#19968;&#33268;&#24615;&#35780;&#20998;&#20989;&#25968;&#30340;&#27604;&#36739;&#21644;&#26657;&#20934;&#35780;&#20272;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20107;&#20808;&#30830;&#23450;&#30446;&#26631;&#39044;&#27979;&#20989;&#25968;&#21644;&#36873;&#25321;&#30456;&#24212;&#35780;&#20998;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.12780</link><description>&lt;p&gt;
&#27169;&#22411;&#27604;&#36739;&#21644;&#26657;&#20934;&#35780;&#20272;&#65306;&#26426;&#22120;&#23398;&#20064;&#21644;&#31934;&#31639;&#23454;&#36341;&#20013;&#19968;&#33268;&#24615;&#35780;&#20998;&#20989;&#25968;&#30340;&#29992;&#25143;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice. (arXiv:2202.12780v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12780
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#29992;&#25143;&#25351;&#21335;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#31934;&#31639;&#23454;&#36341;&#20013;&#19968;&#33268;&#24615;&#35780;&#20998;&#20989;&#25968;&#30340;&#27604;&#36739;&#21644;&#26657;&#20934;&#35780;&#20272;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20107;&#20808;&#30830;&#23450;&#30446;&#26631;&#39044;&#27979;&#20989;&#25968;&#21644;&#36873;&#25321;&#30456;&#24212;&#35780;&#20998;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#31639;&#24072;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#20027;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#26500;&#24314;&#36866;&#29992;&#20110;&#20445;&#38505;&#39046;&#22495;&#20013;&#35832;&#22914;&#32034;&#36180;&#37329;&#39069;&#25110;&#32034;&#36180;&#25968;&#37327;&#31561;&#29616;&#35937;&#30340;&#26377;&#25928;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#29702;&#24819;&#24773;&#20917;&#19979;&#21033;&#29992;&#32473;&#23450;&#30340;&#29305;&#24449;&#20449;&#24687;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#29992;&#25143;&#25351;&#21335;&#37325;&#26032;&#23457;&#35270;&#21644;&#26126;&#30830;&#20102;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#26657;&#20934;&#24615;&#25110;&#36866;&#24403;&#24615;&#30340;&#32479;&#35745;&#25216;&#26415;&#65292;&#20197;&#21450;&#27604;&#36739;&#21644;&#25490;&#21517;&#19981;&#21516;&#27169;&#22411;&#30340;&#25216;&#26415;&#12290;&#22312;&#36825;&#26679;&#20570;&#26102;&#65292;&#24378;&#35843;&#20102;&#20107;&#20808;&#26126;&#30830;&#25351;&#23450;&#30446;&#26631;&#39044;&#27979;&#20989;&#25968;&#65288;&#22914;&#24179;&#22343;&#20540;&#25110;&#20998;&#20301;&#25968;&#65289;&#20197;&#21450;&#36873;&#25321;&#19982;&#27492;&#30446;&#26631;&#20989;&#25968;&#19968;&#33268;&#30340;&#35780;&#20998;&#20989;&#25968;&#22312;&#27169;&#22411;&#27604;&#36739;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25552;&#20379;&#20102;&#23454;&#38469;&#36873;&#25321;&#35780;&#20998;&#20989;&#25968;&#30340;&#25351;&#23548;&#12290;&#33268;&#21147;&#20110;&#22635;&#34917;&#31185;&#23398;&#19982;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24046;&#36317;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#29616;&#26377;&#32467;&#26524;&#21644;&#26368;&#20339;&#23454;&#36341;&#30340;&#25945;&#23398;&#23637;&#31034;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#26696;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the main tasks of actuaries and data scientists is to build good predictive models for certain phenomena such as the claim size or the number of claims in insurance. These models ideally exploit given feature information to enhance the accuracy of prediction. This user guide revisits and clarifies statistical techniques to assess the calibration or adequacy of a model on the one hand, and to compare and rank different models on the other hand. In doing so, it emphasises the importance of specifying the prediction target functional at hand a priori (e.g. the mean or a quantile) and of choosing the scoring function in model comparison in line with this target functional. Guidance for the practical choice of the scoring function is provided. Striving to bridge the gap between science and daily practice in application, it focuses mainly on the pedagogical presentation of existing results and of best practice. The results are accompanied and illustrated by two real data case studies 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.11104</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11104
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#25509;&#22270;&#20013;&#25214;&#21040;&#26368;&#20248;&#36335;&#24452;&#38656;&#35201;&#30830;&#23450;&#27839;&#30528;&#22270;&#30340;&#36793;&#32536;&#34892;&#36827;&#30340;&#26368;&#23567;&#24635;&#25104;&#26412;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20960;&#31181;&#32463;&#20856;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#36890;&#24120;&#25152;&#26377;&#36793;&#32536;&#30340;&#25104;&#26412;&#37117;&#26159;&#39044;&#20808;&#23450;&#20041;&#22909;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#24819;&#35201;&#26681;&#25454;&#26576;&#20010;&#20219;&#21153;&#30340;&#35201;&#27714;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#25913;&#21464;&#25104;&#26412;&#26102;&#65292;&#36890;&#24120;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31361;&#35302;&#26435;&#37325;&#26469;&#23450;&#20041;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#36825;&#20801;&#35768;&#20351;&#29992;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#36827;&#34892;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#24403;&#20174;&#19968;&#20010;&#21021;&#22987;&#27963;&#36291;&#24230;&#20540;&#20026;1&#24320;&#22987;&#26102;&#65292;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#30340;&#27963;&#21160;&#20256;&#25773;&#23558;&#23548;&#33268;&#19982;Bellman-Ford&#31639;&#27861;&#25214;&#21040;&#30340;&#35299;&#30456;&#21516;&#30340;&#35299;&#12290;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19982;Bellman-Ford&#30456;&#21516;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#65288;&#22914;&#36203;&#24067;&#23398;&#20064;&#65289;&#21487;&#20197;&#35843;&#25972;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal paths in connected graphs requires determining the smallest total cost for traveling along the graph's edges. This problem can be solved by several classical algorithms where, usually, costs are predefined for all edges. Conventional planning methods can, thus, normally not be used when wanting to change costs in an adaptive way following the requirements of some task. Here we show that one can define a neural network representation of path finding problems by transforming cost values into synaptic weights, which allows for online weight adaptation using network learning mechanisms. When starting with an initial activity value of one, activity propagation in this network will lead to solutions, which are identical to those found by the Bellman-Ford algorithm. The neural network has the same algorithmic complexity as Bellman-Ford and, in addition, we can show that network learning mechanisms (such as Hebbian learning) can adapt the weights in the network augmenting the r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#26368;&#20248;&#33218;&#35782;&#21035;&#30340;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#31639;&#27861;&#65292;&#20854;&#21305;&#37197;&#20102;&#19979;&#30028;&#65292;&#21482;&#24046;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2111.09885</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#26368;&#20248;&#33218;&#35782;&#21035;&#20013;&#30340;&#26368;&#20248;&#31616;&#21333;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Optimal Simple Regret in Bayesian Best Arm Identification. (arXiv:2111.09885v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09885
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#26368;&#20248;&#33218;&#35782;&#21035;&#30340;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#31639;&#27861;&#65292;&#20854;&#21305;&#37197;&#20102;&#19979;&#30028;&#65292;&#21482;&#24046;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#12290;&#22312;&#20808;&#39564;&#26465;&#20214;&#20855;&#26377;&#19968;&#23450;&#30340;&#36830;&#32493;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#36125;&#21494;&#26031;&#31616;&#21333;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#19982;&#36125;&#21494;&#26031;&#36951;&#25022;&#26368;&#23567;&#21270;&#19981;&#21516;&#65292;&#36125;&#21494;&#26031;&#31616;&#21333;&#36951;&#25022;&#30340;&#20027;&#23548;&#39033;&#26469;&#28304;&#20110;&#26368;&#20248;&#33218;&#21644;&#27425;&#20248;&#33218;&#20043;&#38388;&#38388;&#38553;&#23567;&#20110;$\sqrt{\frac{\log T}{T}}$&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#35745;&#31639;&#31639;&#27861;&#65292;&#20854;&#20027;&#23548;&#39033;&#21305;&#37197;&#20102;&#19979;&#30028;&#65292;&#21482;&#24046;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#65307;&#27169;&#25311;&#32467;&#26524;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider best arm identification in the multi-armed bandit problem. Assuming certain continuity conditions of the prior, we characterize the rate of the Bayesian simple regret. Differing from Bayesian regret minimization (Lai, 1987), the leading term in the Bayesian simple regret derives from the region where the gap between optimal and suboptimal arms is smaller than $\sqrt{\frac{\log T}{T}}$. We propose a simple and easy-to-compute algorithm with its leading term matching with the lower bound up to a constant factor; simulation results support our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20316;&#20026;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#31867;&#20284;&#20110;ResNet&#30340;&#26550;&#26500;&#21644;&#36866;&#24212;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;Transformer&#26550;&#26500;&#12290;&#36825;&#20123;&#22522;&#20934;&#27169;&#22411;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.11959</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20316;&#20026;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#31867;&#20284;&#20110;ResNet&#30340;&#26550;&#26500;&#21644;&#36866;&#24212;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;Transformer&#26550;&#26500;&#12290;&#36825;&#20123;&#22522;&#20934;&#27169;&#22411;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#29486;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#24120;&#27809;&#26377;&#36827;&#34892;&#36866;&#24403;&#30340;&#27604;&#36739;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24120;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#21644;&#23454;&#39564;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#32780;&#35328;&#65292;&#20160;&#20040;&#26679;&#30340;&#27169;&#22411;&#24615;&#33021;&#26368;&#22909;&#26159;&#19981;&#28165;&#26970;&#30340;&#12290;&#21478;&#22806;&#65292;&#35813;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#21363;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#31454;&#20105;&#21147;&#24615;&#33021;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20027;&#35201;DL&#26550;&#26500;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#30830;&#23450;&#20004;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#28145;&#24230;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#34920;&#26684;DL&#30340;&#22522;&#20934;&#12290;&#31532;&#19968;&#31181;&#26159;&#31867;&#20284;&#20110;ResNet&#30340;&#26550;&#26500;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#26159;&#24120;&#35265;&#30340;&#20808;&#21069;&#24037;&#20316;&#20013;&#24120;&#32570;&#22833;&#30340;&#24378;&#22522;&#20934;&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#25105;&#20204;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;Transformer&#26550;&#26500;&#30340;&#31616;&#21333;&#36866;&#24212;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.  In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31232;&#30095;&#20381;&#36182;&#32467;&#26500;&#30340;&#21487;&#21387;&#32553;&#39057;&#35889;&#28151;&#21512;&#26680;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#65292;&#36890;&#36807;&#25913;&#36827;&#21407;&#22987;&#26680;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#20132;&#21449;&#21327;&#26041;&#24046;&#21644;&#20132;&#21449;&#21367;&#31215;&#27867;&#21270;&#20381;&#36182;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21442;&#25968;&#21270;&#26102;&#38388;&#21644;&#30456;&#20301;&#24310;&#36831;&#25552;&#39640;&#20381;&#36182;&#32467;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/1808.00560</link><description>&lt;p&gt;
&#24102;&#26377;&#31232;&#30095;&#20381;&#36182;&#32467;&#26500;&#30340;&#21487;&#21387;&#32553;&#39057;&#35889;&#28151;&#21512;&#26680;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Compressible Spectral Mixture Kernels with Sparse Dependency Structures for Gaussian Processes. (arXiv:1808.00560v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1808.00560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31232;&#30095;&#20381;&#36182;&#32467;&#26500;&#30340;&#21487;&#21387;&#32553;&#39057;&#35889;&#28151;&#21512;&#26680;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#65292;&#36890;&#36807;&#25913;&#36827;&#21407;&#22987;&#26680;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#20132;&#21449;&#21327;&#26041;&#24046;&#21644;&#20132;&#21449;&#21367;&#31215;&#27867;&#21270;&#20381;&#36182;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21442;&#25968;&#21270;&#26102;&#38388;&#21644;&#30456;&#20301;&#24310;&#36831;&#25552;&#39640;&#20381;&#36182;&#32467;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39057;&#35889;&#28151;&#21512;&#65288;SM&#65289;&#26680;&#26159;&#19968;&#31181;&#25551;&#36848;&#22797;&#26434;&#27169;&#24335;&#30340;&#36890;&#29992;&#20869;&#26680;&#31867;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#12290;&#26412;&#25991;&#36890;&#36807;&#27169;&#22411;&#21387;&#32553;&#21644;&#26102;&#38388;&#30456;&#20301;&#65288;TP&#65289;&#35843;&#21046;&#20381;&#36182;&#32467;&#26500;&#65292;&#25913;&#36827;&#20102;&#21407;&#22987;&#30340;&#65288;SM&#65289;&#26680;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#37319;&#29992;Bienaym&#233;s&#24658;&#31561;&#24335;&#65292;&#25105;&#20204;&#36890;&#36807;SM&#32452;&#20214;&#20043;&#38388;&#30340;&#20132;&#21449;&#21327;&#26041;&#24046;&#26469;&#27867;&#21270;&#20381;&#36182;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#20381;&#36182;&#32467;&#26500;&#65288;SMD&#65289;&#30340;SM&#26680;&#65292;&#36890;&#36807;SM&#32452;&#20214;&#20043;&#38388;&#30340;&#20132;&#21449;&#21367;&#31215;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21442;&#25968;&#21270;&#26102;&#38388;&#21644;&#30456;&#20301;&#24310;&#36831;&#26469;&#25913;&#21892;&#20381;&#36182;&#32467;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#35813;&#20381;&#36182;&#32467;&#26500;&#22312;&#39057;&#35889;&#23494;&#24230;&#12289;&#21327;&#26041;&#24046;&#34892;&#20026;&#21644;&#37319;&#26679;&#36335;&#24452;&#26041;&#38754;&#20855;&#26377;&#28165;&#26224;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#20016;&#23500;SMD&#30340;&#26377;&#25928;&#36229;&#21442;&#25968;&#21021;&#22987;&#21270;&#12289;&#21487;&#21387;&#32553;&#30340;SM&#26680;&#32452;&#20214;&#21644;&#31232;&#30095;&#30340;&#20381;&#36182;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#36866;&#24212;&#65288;SA&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral mixture (SM) kernels comprise a powerful class of generalized kernels for Gaussian processes (GPs) to describe complex patterns. This paper introduces model compression and time- and phase (TP) modulated dependency structures to the original (SM) kernel for improved generalization of GPs. Specifically, by adopting Bienaym\'es identity, we generalize the dependency structure through cross-covariance between the SM components. Then, we propose a novel SM kernel with a dependency structure (SMD) by using cross-convolution between the SM components. Furthermore, we ameliorate the expressiveness of the dependency structure by parameterizing it with time and phase delays. The dependency structure has clear interpretations in terms of spectral density, covariance behavior, and sampling path. To enrich the SMD with effective hyperparameter initialization, compressible SM kernel components, and sparse dependency structures, we introduce a novel structure adaptation (SA) algorithm in th
&lt;/p&gt;</description></item></channel></rss>