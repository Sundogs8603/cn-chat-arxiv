<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#32858;&#21512;&#38450;&#24481;&#31574;&#30053;&#30340;&#23454;&#36341;&#26041;&#38754;&#65292;&#24182;&#38024;&#23545;Deep Partition Aggregation&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#25928;&#29575;&#12289;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#32553;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#32858;&#21512;&#38450;&#24481;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.16415</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#32858;&#21512;&#38450;&#24481;&#30340;&#23454;&#36341;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
On Practical Aspects of Aggregation Defenses against Data Poisoning Attacks. (arXiv:2306.16415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#32858;&#21512;&#38450;&#24481;&#31574;&#30053;&#30340;&#23454;&#36341;&#26041;&#38754;&#65292;&#24182;&#38024;&#23545;Deep Partition Aggregation&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#25928;&#29575;&#12289;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#32553;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#32858;&#21512;&#38450;&#24481;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#26469;&#35828;&#65292;&#25968;&#25454;&#30340;&#22686;&#21152;&#19981;&#20165;&#24102;&#26469;&#26426;&#20250;&#65292;&#20063;&#24102;&#26469;&#39118;&#38505;&#65292;&#22240;&#20026;&#24694;&#24847;&#35757;&#32451;&#26679;&#26412;&#21487;&#20197;&#25805;&#32437;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#25915;&#20987;&#34987;&#31216;&#20026;&#25968;&#25454;&#20013;&#27602;&#12290;&#36817;&#26399;&#23545;&#25239;&#25968;&#25454;&#20013;&#27602;&#30340;&#38450;&#24481;&#31574;&#30053;&#30340;&#36827;&#23637;&#31361;&#20986;&#20102;&#32858;&#21512;&#26041;&#26696;&#22312;&#23454;&#29616;&#35748;&#35777;&#20013;&#27602;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#36341;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;Deep Partition Aggregation&#65292;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#32858;&#21512;&#38450;&#24481;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#23454;&#38469;&#26041;&#38754;&#65292;&#21253;&#25324;&#25928;&#29575;&#12289;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#34987;&#35843;&#25972;&#21040;64&#215;64&#20998;&#36776;&#29575;&#30340;ImageNet&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#22312;&#27604;&#20197;&#21069;&#26356;&#22823;&#30340;&#35268;&#27169;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#23454;&#29992;&#30340;&#22522;&#20110;&#32553;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#25913;&#21892;&#20102;&#32858;&#21512;&#38450;&#24481;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#25968;&#25454;&#21078;&#20998;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing access to data poses both opportunities and risks in deep learning, as one can manipulate the behaviors of deep learning models with malicious training samples. Such attacks are known as data poisoning. Recent advances in defense strategies against data poisoning have highlighted the effectiveness of aggregation schemes in achieving state-of-the-art results in certified poisoning robustness. However, the practical implications of these approaches remain unclear. Here we focus on Deep Partition Aggregation, a representative aggregation defense, and assess its practical aspects, including efficiency, performance, and robustness. For evaluations, we use ImageNet resized to a resolution of 64 by 64 to enable evaluations at a larger scale than previous ones. Firstly, we demonstrate a simple yet practical approach to scaling base models, which improves the efficiency of training and inference for aggregation defenses. Secondly, we provide empirical evidence supporting the data
&lt;/p&gt;</description></item><item><title>MultiZoo&#21644;MultiBench&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#22810;&#27169;&#24577;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20419;&#36827;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#30830;&#20445;&#26131;&#29992;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16413</link><description>&lt;p&gt;
MultiZoo &amp; MultiBench: &#29992;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
MultiZoo &amp; MultiBench: A Standardized Toolkit for Multimodal Deep Learning. (arXiv:2306.16413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16413
&lt;/p&gt;
&lt;p&gt;
MultiZoo&#21644;MultiBench&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#22810;&#27169;&#24577;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20419;&#36827;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#30830;&#20445;&#26131;&#29992;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#28041;&#21450;&#25972;&#21512;&#26469;&#33258;&#22810;&#31181;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#21152;&#24555;&#23545;&#23569;&#30740;&#31350;&#30340;&#27169;&#24577;&#21644;&#20219;&#21153;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#30830;&#20445;&#29616;&#23454;&#19990;&#30028;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;MultiZoo&#65292;&#19968;&#20010;&#20844;&#20849;&#24037;&#20855;&#21253;&#65292;&#20854;&#20013;&#21253;&#21547;&gt; 20&#20010;&#26680;&#24515;&#22810;&#27169;&#24577;&#31639;&#27861;&#30340;&#26631;&#20934;&#21270;&#23454;&#29616;&#65292;&#20197;&#21450;MultiBench&#65292;&#19968;&#20010;&#28085;&#30422;15&#20010;&#25968;&#25454;&#38598;&#65292;10&#20010;&#27169;&#24577;&#65292;20&#20010;&#39044;&#27979;&#20219;&#21153;&#21644;6&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20123;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#31616;&#21270;&#21644;&#26631;&#20934;&#21270;&#25968;&#25454;&#21152;&#36733;&#12289;&#23454;&#39564;&#35774;&#32622;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;&#20026;&#20102;&#23454;&#29616;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#65288;1&#65289;&#27867;&#21270;&#33021;&#21147;&#65292;&#65288;2&#65289;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#21644;&#65288;3&#65289;&#27169;&#24577;&#40065;&#26834;&#24615;&#12290;MultiBench&#20026;&#26356;&#22909;&#22320;&#20102;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#38480;&#21046;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#21516;&#26102;&#30830;&#20445;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#21253;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of &gt; 20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26080;&#38480;&#21608;&#26399;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#22810;&#20010;&#39640;&#25928;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#21253;&#25324;&#22312;&#32447;&#35774;&#32622;&#21644;&#20223;&#30495;&#22120;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#36951;&#25022;&#21644;&#26679;&#26412;&#20351;&#29992;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16394</link><description>&lt;p&gt;
&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26356;&#31934;&#30830;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes. (arXiv:2306.16394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26080;&#38480;&#21608;&#26399;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#22810;&#20010;&#39640;&#25928;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#21253;&#25324;&#22312;&#32447;&#35774;&#32622;&#21644;&#20223;&#30495;&#22120;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#36951;&#25022;&#21644;&#26679;&#26412;&#20351;&#29992;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#32463;&#36807;&#39564;&#35777;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#38480;&#21608;&#26399;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#35774;&#32622;&#21644;&#25317;&#26377;&#20223;&#30495;&#22120;&#30340;&#35774;&#32622;&#12290;&#22312;&#22312;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21442;&#32771;&#20248;&#21183;&#20998;&#35299;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;T&#27493;&#20043;&#21518;&#36798;&#21040; $\widetilde{O}(S^5A^2\mathrm{sp}(h^*)\sqrt{T})$ &#30340;&#36951;&#25022;&#65292;&#20854;&#20013; $S\times A$ &#26159;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292; $\mathrm{sp}(h^*)$ &#26159;&#26368;&#20248;&#20559;&#24046;&#20989;&#25968;&#30340;&#36328;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#22312;&#24369;&#36890;&#20449;MDPs&#20013;&#36798;&#21040;T&#30340;&#26368;&#20339;&#20381;&#36182;&#24615;&#30340;&#12290;&#22312;&#20223;&#30495;&#22120;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992; $\widetilde{O} \left(\frac{SA\mathrm{sp}^2(h^*)}{\epsilon^2}+\frac{S^2A\mathrm{sp}(h^*)}{\epsilon} \right)$ &#20010;&#26679;&#26412;&#25214;&#21040;&#19968;&#20010; $\epsilon$-&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#26497;&#23567;&#21270;&#19979;&#30028;&#26159; $\Omega\left(\frac{SA\mathrm{sp}(h^*)}{\epsilon^2}\right)$&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#20004;&#20010;&#26032;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
We develop several provably efficient model-free reinforcement learning (RL) algorithms for infinite-horizon average-reward Markov Decision Processes (MDPs). We consider both online setting and the setting with access to a simulator. In the online setting, we propose model-free RL algorithms based on reference-advantage decomposition. Our algorithm achieves $\widetilde{O}(S^5A^2\mathrm{sp}(h^*)\sqrt{T})$ regret after $T$ steps, where $S\times A$ is the size of state-action space, and  $\mathrm{sp}(h^*)$ the span of the optimal bias function. Our results are the first to achieve optimal dependence in $T$ for weakly communicating MDPs.  In the simulator setting, we propose a model-free RL algorithm that finds an $\epsilon$-optimal policy using $\widetilde{O} \left(\frac{SA\mathrm{sp}^2(h^*)}{\epsilon^2}+\frac{S^2A\mathrm{sp}(h^*)}{\epsilon} \right)$ samples, whereas the minimax lower bound is $\Omega\left(\frac{SA\mathrm{sp}(h^*)}{\epsilon^2}\right)$.  Our results are based on two new te
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26469;&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#19978;&#26102;CPU&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16384</link><description>&lt;p&gt;
&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#65306;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;
&lt;/p&gt;
&lt;p&gt;
Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses. (arXiv:2306.16384v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26469;&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#19978;&#26102;CPU&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#27491;&#22312;&#25104;&#20026;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#21644;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#19968;&#20010;&#24378;&#22823;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;GNNs&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#22270;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#35757;&#32451;&#20173;&#28982;&#38754;&#20020;&#30528;&#25968;&#25454;&#35775;&#38382;&#21644;&#25968;&#25454;&#31227;&#21160;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#29616;&#26377;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#20351;&#29992;CPU&#36827;&#34892;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#65292;&#32780;&#27169;&#22411;&#26435;&#37325;&#30340;&#35757;&#32451;&#21644;&#26356;&#26032;&#21017;&#30001;GPU&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;CPU&#26080;&#27861;&#23454;&#29616;&#25152;&#38656;&#30340;&#21534;&#21520;&#37327;&#20197;&#20805;&#20998;&#21033;&#29992;&#26114;&#36149;&#30340;GPU&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#24403;&#22270;&#21644;&#20854;&#23884;&#20837;&#19981;&#33021;&#36866;&#24212;CPU&#20869;&#23384;&#26102;&#65292;&#25805;&#20316;&#31995;&#32479;&#24341;&#20837;&#30340;&#24320;&#38144;&#65292;&#22914;&#22788;&#29702;&#39029;&#38754;&#38169;&#35823;&#65292;&#20250;&#25104;&#20026;&#20851;&#38190;&#36335;&#24452;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPU&#21457;&#36215;&#30340;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are emerging as a powerful tool for learning from graph-structured data and performing sophisticated inference tasks in various application domains. Although GNNs have been shown to be effective on modest-sized graphs, training them on large-scale graphs remains a significant challenge due to lack of efficient data access and data movement methods. Existing frameworks for training GNNs use CPUs for graph sampling and feature aggregation, while the training and updating of model weights are executed on GPUs. However, our in-depth profiling shows the CPUs cannot achieve the throughput required to saturate GNN model training throughput, causing gross under-utilization of expensive GPU resources. Furthermore, when the graph and its embeddings do not fit in the CPU memory, the overhead introduced by the operating system, say for handling page-faults, comes in the critical path of execution.  To address these issues, we propose the GPU Initiated Direct Storage Ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31449;&#28857;&#20020;&#24202;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36882;&#24402;&#21644;&#27880;&#24847;&#21147;&#27169;&#22411;&#20197;&#21450;NVFlare&#26694;&#26550;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#31034;&#20363;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;LSTM&#27169;&#22411;&#21644;BERT&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16367</link><description>&lt;p&gt;
&#22810;&#31449;&#28857;&#20020;&#24202;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#36882;&#24402;&#21644;&#27880;&#24847;&#21147;&#27169;&#22411;&#20197;&#21450;NVFlare
&lt;/p&gt;
&lt;p&gt;
Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare. (arXiv:2306.16367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31449;&#28857;&#20020;&#24202;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36882;&#24402;&#21644;&#27880;&#24847;&#21147;&#27169;&#22411;&#20197;&#21450;NVFlare&#26694;&#26550;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#31034;&#20363;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;LSTM&#27169;&#22411;&#21644;BERT&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20581;&#24247;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#26469;&#23457;&#26597;&#21307;&#30103;&#35760;&#24405;&#12289;&#20020;&#24202;&#31508;&#35760;&#21644;&#20854;&#20182;&#22522;&#20110;&#25991;&#26412;&#30340;&#20581;&#24247;&#20449;&#24687;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#22686;&#24378;&#24739;&#32773;&#25252;&#29702;&#21644;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#25968;&#25454;&#38544;&#31169;&#21644;&#36981;&#23432;&#27861;&#35268;&#20173;&#28982;&#26159;&#20851;&#38190;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#22810;&#20010;&#32452;&#32455;&#33021;&#22815;&#22312;&#19981;&#20256;&#25773;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#32852;&#37030;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;&#30001;NVIDIA&#24320;&#21457;&#30340;NVFlare&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#31034;&#20363;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#27169;&#22411;&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#65288;BERT&#65289;&#65292;&#23427;&#20204;&#22312;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986; exceptional &#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prodigious growth of digital health data has precipitated a mounting interest in harnessing machine learning methodologies, such as natural language processing (NLP), to scrutinize medical records, clinical notes, and other text-based health information. Although NLP techniques have exhibited substantial potential in augmenting patient care and informing clinical decision-making, data privacy and adherence to regulations persist as critical concerns. Federated learning (FL) emerges as a viable solution, empowering multiple organizations to train machine learning models collaboratively without disseminating raw data. This paper proffers a pragmatic approach to medical NLP by amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA. We introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-based model and Bidirectional Encoder Representations from Transformers (BERT), which have demonstrated exceptional performance in comprehending context and semant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22810;&#39033;&#24335;&#23485;&#24230;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25237;&#24433;&#26799;&#24230;&#27969;&#30340;&#22343;&#22330;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#19982;NTK&#20043;&#38388;&#30340;&#26126;&#26174;&#24046;&#24322;&#65292;&#21363;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#27604;&#26680;&#26041;&#27861;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16361</link><description>&lt;p&gt;
&#36229;&#36234;NTK&#19982;&#33539;&#24335;&#26799;&#24230;&#19979;&#38477;&#65306;&#23545;&#20855;&#26377;&#22810;&#39033;&#24335;&#23485;&#24230;&#12289;&#26679;&#26412;&#21644;&#26102;&#38388;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time. (arXiv:2306.16361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22810;&#39033;&#24335;&#23485;&#24230;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25237;&#24433;&#26799;&#24230;&#27969;&#30340;&#22343;&#22330;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#19982;NTK&#20043;&#38388;&#30340;&#26126;&#26174;&#24046;&#24322;&#65292;&#21363;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#27604;&#26680;&#26041;&#27861;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#38750;&#20984;&#20248;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#21462;&#24471;&#20102;&#29702;&#35770;&#19978;&#30340;&#36827;&#23637;&#65292;&#20294;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#26159;&#21542;&#21487;&#20197;&#27604;&#26680;&#26041;&#27861;&#23454;&#29616;&#26356;&#39640;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22810;&#39033;&#24335;&#23485;&#24230;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25237;&#24433;&#26799;&#24230;&#27969;&#30340;&#28165;&#26224;&#22343;&#22330;&#20998;&#26512;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#19981;&#38656;&#35201;&#23545;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#19981;&#33258;&#28982;&#30340;&#20462;&#25913;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#26679;&#26412;&#22823;&#23567;$n = O(d^{3.1})$&#65292;&#20854;&#20013;$d$&#26159;&#36755;&#20837;&#30340;&#32500;&#24230;&#65292;&#32593;&#32476;&#22312;&#22810;&#39033;&#24335;&#27425;&#36845;&#20195;&#20013;&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#38169;&#35823;&#65292;&#36825;&#20010;&#38169;&#35823;&#26080;&#27861;&#36890;&#36807;&#20351;&#29992;$n \ll d^4$&#20010;&#26679;&#26412;&#30340;&#26680;&#26041;&#27861;&#23454;&#29616;&#65292;&#22240;&#27492;&#28165;&#26970;&#22320;&#35777;&#26126;&#20102;&#21407;&#22987;&#26799;&#24230;&#19979;&#38477;&#21644;NTK&#20043;&#38388;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent theoretical progress on the non-convex optimization of two-layer neural networks, it is still an open question whether gradient descent on neural networks without unnatural modifications can achieve better sample complexity than kernel methods. This paper provides a clean mean-field analysis of projected gradient flow on polynomial-width two-layer neural networks. Different from prior works, our analysis does not require unnatural modifications of the optimization algorithm. We prove that with sample size $n = O(d^{3.1})$ where $d$ is the dimension of the inputs, the network converges in polynomially many iterations to a non-trivial error that is not achievable by kernel methods using $n \ll d^4$ samples, hence demonstrating a clear separation between unmodified gradient descent and NTK.
&lt;/p&gt;</description></item><item><title>cuSLINK&#26159;&#19968;&#31181;&#22312;GPU&#19978;&#23454;&#29616;&#30340;&#21333;&#38142;&#25509;&#32858;&#31867;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.16354</link><description>&lt;p&gt;
cuSLINK: GPU&#19978;&#30340;&#21333;&#38142;&#25509;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
cuSLINK: Single-linkage Agglomerative Clustering on the GPU. (arXiv:2306.16354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16354
&lt;/p&gt;
&lt;p&gt;
cuSLINK&#26159;&#19968;&#31181;&#22312;GPU&#19978;&#23454;&#29616;&#30340;&#21333;&#38142;&#25509;&#32858;&#31867;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cuSLINK&#65292;&#35813;&#31639;&#27861;&#26159;&#23545;GPU&#19978;&#30340;SLINK&#31639;&#27861;&#30340;&#19968;&#31181;&#26032;&#22411;&#21644;&#26368;&#20808;&#36827;&#30340;&#25913;&#36827;&#65292;&#23427;&#21482;&#38656;&#35201;$O(Nk)$&#30340;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#21442;&#25968;$k$&#26469;&#26435;&#34913;&#31354;&#38388;&#21644;&#26102;&#38388;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#39062;&#19988;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36825;&#20123;&#26500;&#24314;&#27169;&#22359;&#21253;&#25324;&#38024;&#23545;$k$-NN&#22270;&#26500;&#24314;&#12289;&#29983;&#25104;&#26641;&#21644;&#26641;&#29366;&#22270;&#32858;&#31867;&#30340;&#39640;&#24230;&#20248;&#21270;&#30340;&#35745;&#31639;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#22522;&#26412;&#27169;&#22359;&#22312;GPU&#19978;&#20840;&#38754;&#23454;&#29616;cuSLINK&#65292;&#36827;&#19968;&#27493;&#20351;&#24471;&#21407;&#26412;&#38590;&#20197;&#22788;&#29702;&#30340;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#38500;&#20102;&#22312;&#27969;&#34892;&#30340;HDBSCAN&#31639;&#27861;&#20013;&#26159;&#20027;&#35201;&#30340;&#35745;&#31639;&#29942;&#39048;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;cuSLINK&#31639;&#27861;&#30340;&#20840;&#38754;&#24433;&#21709;&#36824;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#21644;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#30340;&#32858;&#31867;&#20998;&#26512;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#29992;&#25143;&#21487;&#20197;&#22312;https://docs.rapids.ai/api/cuml/latest/api/#agg&#33719;&#21462;cuSLINK&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose cuSLINK, a novel and state-of-the-art reformulation of the SLINK algorithm on the GPU which requires only $O(Nk)$ space and uses a parameter $k$ to trade off space and time. We also propose a set of novel and reusable building blocks that compose cuSLINK. These building blocks include highly optimized computational patterns for $k$-NN graph construction, spanning trees, and dendrogram cluster extraction. We show how we used our primitives to implement cuSLINK end-to-end on the GPU, further enabling a wide range of real-world data mining and machine learning applications that were once intractable. In addition to being a primary computational bottleneck in the popular HDBSCAN algorithm, the impact of our end-to-end cuSLINK algorithm spans a large range of important applications, including cluster analysis in social and computer networks, natural language processing, and computer vision. Users can obtain cuSLINK at https://docs.rapids.ai/api/cuml/latest/api/#agg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#38543;&#26426;&#20998;&#31867;&#22122;&#22768;&#30340;&#36793;&#30028;&#21322;&#31354;&#38388;&#30340;&#20449;&#24687;-&#35745;&#31639;&#26435;&#34913;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#31639;&#27861;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.16352</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#38543;&#26426;&#20998;&#31867;&#22122;&#22768;&#30340;&#36793;&#30028;&#21322;&#31354;&#38388;&#30340;&#20449;&#24687;-&#35745;&#31639;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Information-Computation Tradeoffs for Learning Margin Halfspaces with Random Classification Noise. (arXiv:2306.16352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#38543;&#26426;&#20998;&#31867;&#22122;&#22768;&#30340;&#36793;&#30028;&#21322;&#31354;&#38388;&#30340;&#20449;&#24687;-&#35745;&#31639;&#26435;&#34913;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#31639;&#27861;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22122;&#22768;&#23398;&#20064;&#947;-&#36793;&#30028;&#21322;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20449;&#24687;-&#35745;&#31639;&#26435;&#34913;&#65292;&#34920;&#26126;&#20102;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19982;&#35745;&#31639;&#25928;&#29575;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20026;&#920;(1/ (&#947;^2 &#949;))&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24615;&#20026;O(1/ (&#947;^2 &#949;^2))&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#31639;&#27861;&#21644;&#20302;&#27425;&#22810;&#39033;&#24335;&#27979;&#35797;&#30340;&#19979;&#30028;&#65292;&#36825;&#34920;&#26126;&#22312;&#35745;&#31639;&#25928;&#29575;&#31639;&#27861;&#20013;&#65292;&#26679;&#26412;&#22797;&#26434;&#24615;&#23545;1/&#949;&#30340;&#20108;&#27425;&#20381;&#36182;&#26159;&#22266;&#26377;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#20102;&#20219;&#20309;&#26377;&#25928;&#30340;SQ&#23398;&#20064;&#22120;&#25110;&#20302;&#27425;&#27979;&#35797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#19979;&#30028;&#20026;&#937;(1/ (&#947;^(1/2) &#949;^2))&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of PAC learning $\gamma$-margin halfspaces with Random Classification Noise. We establish an information-computation tradeoff suggesting an inherent gap between the sample complexity of the problem and the sample complexity of computationally efficient algorithms. Concretely, the sample complexity of the problem is $\widetilde{\Theta}(1/(\gamma^2 \epsilon))$. We start by giving a simple efficient algorithm with sample complexity $\widetilde{O}(1/(\gamma^2 \epsilon^2))$. Our main result is a lower bound for Statistical Query (SQ) algorithms and low-degree polynomial tests suggesting that the quadratic dependence on $1/\epsilon$ in the sample complexity is inherent for computationally efficient algorithms. Specifically, our results imply a lower bound of $\widetilde{\Omega}(1/(\gamma^{1/2} \epsilon^2))$ on the sample complexity of any efficient SQ learner or low-degree test.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;mNARX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#27969;&#24418;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36817;&#20284;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#21709;&#24212;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#25972;&#20010;&#38382;&#39064;&#20998;&#35299;&#25104;&#36739;&#23567;&#30340;&#23376;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36866;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.16335</link><description>&lt;p&gt;
&#20351;&#29992;&#27969;&#24418;&#19978;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;mNARX&#65289;&#27169;&#25311;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX). (arXiv:2306.16335v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;mNARX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#27969;&#24418;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36817;&#20284;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#21709;&#24212;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#25972;&#20010;&#38382;&#39064;&#20998;&#35299;&#25104;&#36739;&#23567;&#30340;&#23376;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36866;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36817;&#20284;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#23545;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22806;&#37096;&#21050;&#28608;&#30340;&#21709;&#24212;&#65292;&#24182;&#19988;&#21487;&#20197;&#24310;&#38271;&#26102;&#38388;&#27573;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#8220;&#24102;&#22806;&#37096;&#36755;&#20837;&#30340;&#27969;&#24418;&#38750;&#32447;&#24615;&#33258;&#22238;&#24402;&#24314;&#27169;&#8221;&#65288;mNARX&#65289;&#65292;&#23427;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#38382;&#39064;&#29305;&#23450;&#30340;&#22806;&#37096;&#36755;&#20837;&#27969;&#24418;&#65292;&#20197;&#20415;&#26500;&#36896;&#33258;&#22238;&#24402;&#26367;&#20195;&#27169;&#22411;&#12290;&#36825;&#20010;&#27969;&#24418;&#26159;mNARX&#30340;&#26680;&#24515;&#65292;&#36890;&#36807;&#32467;&#21512;&#31995;&#32479;&#30340;&#29289;&#29702;&#24615;&#36136;&#20197;&#21450;&#20808;&#21069;&#30340;&#19987;&#23478;&#21644;&#39046;&#22495;&#30693;&#35782;&#26469;&#36880;&#27493;&#26500;&#24314;&#12290;&#22240;&#20026;mNARX&#23558;&#25972;&#20010;&#38382;&#39064;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#26356;&#23567;&#30340;&#23376;&#38382;&#39064;&#65292;&#27599;&#20010;&#23376;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#27604;&#21407;&#26469;&#30340;&#20302;&#65292;&#25152;&#20197;&#23427;&#22312;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#26080;&#35770;&#26159;&#26368;&#32456;&#26367;&#20195;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#36824;&#26159;&#35780;&#20272;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;mNARX&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#38750;&#24120;&#22865;&#21512;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel surrogate modelling approach to efficiently and accurately approximate the response of complex dynamical systems driven by time-varying exogenous excitations over extended time periods. Our approach, that we name \emph{manifold nonlinear autoregressive modelling with exogenous input} (mNARX), involves constructing a problem-specific exogenous input manifold that is optimal for constructing autoregressive surrogates. The manifold, which forms the core of mNARX, is constructed incrementally by incorporating the physics of the system, as well as prior expert- and domainknowledge. Because mNARX decomposes the full problem into a series of smaller sub-problems, each with a lower complexity than the original, it scales well with the complexity of the problem, both in terms of training and evaluation costs of the final surrogate. Furthermore, mNARX synergizes well with traditional dimensionality reduction techniques, making it highly suitable for modelling 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.16334</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#26631;&#24535;&#26816;&#27979;&#26469;&#35782;&#21035;&#31163;&#25955;&#21270;&#28508;&#22312;&#22352;&#26631;&#31995;&#32479;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection. (arXiv:2306.16334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#26088;&#22312;&#20165;&#20174;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#20013;&#24674;&#22797;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#30495;&#23454;&#22240;&#32032;&#12290; &#21487;&#35782;&#21035;&#24615;&#20026;&#35299;&#32544;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290; &#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#33258;&#36866;&#24212;&#29420;&#31435;&#28508;&#21464;&#37327;&#22240;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#22240;&#23376;&#21040;&#35266;&#27979;&#30340;&#26144;&#23556;&#19979;&#65292;&#26080;&#30417;&#30563;&#30340;&#21487;&#35782;&#21035;&#24615;&#22312;i.i.d.&#35774;&#32622;&#19979;&#26159;&#29702;&#35770;&#19978;&#19981;&#21487;&#33021;&#30340;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#24120;&#24778;&#20154;&#30340;&#26159;&#65292;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#26144;&#23556;&#65288;&#19968;&#20010;&#24494;&#20998;&#21516;&#32986;&#65289;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26144;&#23556;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290; &#36825;&#26159;&#22312;&#20551;&#35774;&#28508;&#22312;&#23494;&#24230;&#20855;&#26377;&#36724;&#23545;&#40784;&#30340;&#19981;&#36830;&#32493;&#26631;&#24535;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#19981;&#20570;&#22240;&#32032;&#30340;&#32479;&#35745;&#29420;&#31435;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#23545;&#24674;&#22797;&#31163;&#25955;&#22352;&#26631;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentanglement aims to recover meaningful latent ground-truth factors from only the observed distribution. Identifiability provides the theoretical grounding for disentanglement to be well-founded. Unfortunately, unsupervised identifiability of independent latent factors is a theoretically proven impossibility in the i.i.d. setting under a general nonlinear smooth map from factors to observations. In this work, we show that, remarkably, it is possible to recover discretized latent coordinates under a highly generic nonlinear smooth mapping (a diffeomorphism) without any additional inductive bias on the mapping. This is, assuming that latent density has axis-aligned discontinuity landmarks, but without making the unrealistic assumption of statistical independence of the factors. We introduce this novel form of identifiability, termed quantized coordinate identifiability, and provide a comprehensive proof of the recovery of discretized coordinates.
&lt;/p&gt;</description></item><item><title>VBN&#26159;&#19968;&#31181;&#21033;&#29992;&#23618;&#27425;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#8220;&#38271;&#23614;&#8221;&#23454;&#20307;&#24314;&#27169;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20808;&#39564;&#21644;&#26126;&#30830;&#20851;&#31995;&#32422;&#26463;&#65292;VBN&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23494;&#24230;&#34920;&#31034;&#23454;&#20307;&#65292;&#23545;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.16326</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#36125;&#21494;&#26031;&#32593;&#32476;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Variational Bayesian Networks. (arXiv:2306.16326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16326
&lt;/p&gt;
&lt;p&gt;
VBN&#26159;&#19968;&#31181;&#21033;&#29992;&#23618;&#27425;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#8220;&#38271;&#23614;&#8221;&#23454;&#20307;&#24314;&#27169;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20808;&#39564;&#21644;&#26126;&#30830;&#20851;&#31995;&#32422;&#26463;&#65292;VBN&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23494;&#24230;&#34920;&#31034;&#23454;&#20307;&#65292;&#23545;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;-&#21464;&#20998;&#36125;&#21494;&#26031;&#32593;&#32476; (VBN)&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#23618;&#27425;&#21644;&#20851;&#31995;&#20449;&#24687;&#65292;&#24182;&#23545;&#8220;&#38271;&#23614;&#8221;&#20013;&#30340;&#23454;&#20307;&#24314;&#27169;&#29305;&#21035;&#26377;&#29992;&#65292;&#22240;&#20026;&#36825;&#31181;&#24773;&#20917;&#19979;&#25968;&#25454;&#31232;&#32570;&#12290;VBN&#36890;&#36807;&#20004;&#31181;&#20114;&#34917;&#26426;&#21046;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38271;&#23614;&#23454;&#20307;&#24314;&#27169;&#65306;&#39318;&#20808;&#65292;VBN&#37319;&#29992;&#20102;&#20449;&#24687;&#20016;&#23500;&#30340;&#23618;&#27425;&#20808;&#39564;&#65292;&#20351;&#20849;&#20139;&#20849;&#21516;&#31062;&#20808;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;VBN&#24314;&#27169;&#20102;&#23454;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#20851;&#31995;&#65292;&#24378;&#21046;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#34917;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#65292;&#24341;&#23548;&#23398;&#20064;&#30340;&#34920;&#31034;&#21521;&#26356;&#26377;&#24847;&#20041;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;&#20854;&#27425;&#65292;VBN&#36890;&#36807;&#23494;&#24230;&#34920;&#31034;&#23454;&#20307;&#65288;&#32780;&#19981;&#26159;&#21521;&#37327;&#65289;&#65292;&#20174;&#32780;&#23545;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#36215;&#21040;&#34917;&#20805;&#20316;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;VBN&#22312;&#35821;&#35328;&#23398;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Variational Bayesian Network (VBN) - a novel Bayesian entity representation learning model that utilizes hierarchical and relational side information and is particularly useful for modeling entities in the ``long-tail'', where the data is scarce. VBN provides better modeling for long-tail entities via two complementary mechanisms: First, VBN employs informative hierarchical priors that enable information propagation between entities sharing common ancestors. Additionally, VBN models explicit relations between entities that enforce complementary structure and consistency, guiding the learned representations towards a more meaningful arrangement in space. Second, VBN represents entities by densities (rather than vectors), hence modeling uncertainty that plays a complementary role in coping with data scarcity. Finally, we propose a scalable Variational Bayes optimization algorithm that enables fast approximate Bayesian inference. We evaluate the effectiveness of VBN on linguist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Stein&#26041;&#27861;&#25512;&#23548;&#20986;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#36890;&#36807;&#39640;&#26031;&#24179;&#28369;&#25216;&#26415;&#23558;&#24179;&#28369;&#24230;&#37327;&#36716;&#21270;&#20026;Wasserstein&#36317;&#31163;&#12290;&#36890;&#36807;&#29305;&#27530;&#21270;&#32467;&#26524;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24191;&#20041;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#39640;&#26031;&#38543;&#26426;&#22330;&#36924;&#36817;&#30340;&#39318;&#20010;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.16308</link><description>&lt;p&gt;
&#36890;&#36807;Stein&#26041;&#27861;&#23545;&#39640;&#26031;&#38543;&#26426;&#22330;&#36827;&#34892;&#36924;&#36817;&#21450;&#20854;&#22312;&#24191;&#20041;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Gaussian random field approximation via Stein's method with applications to wide random neural networks. (arXiv:2306.16308v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Stein&#26041;&#27861;&#25512;&#23548;&#20986;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#36890;&#36807;&#39640;&#26031;&#24179;&#28369;&#25216;&#26415;&#23558;&#24179;&#28369;&#24230;&#37327;&#36716;&#21270;&#20026;Wasserstein&#36317;&#31163;&#12290;&#36890;&#36807;&#29305;&#27530;&#21270;&#32467;&#26524;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24191;&#20041;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#39640;&#26031;&#38543;&#26426;&#22330;&#36924;&#36817;&#30340;&#39318;&#20010;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;Stein&#26041;&#27861;&#25512;&#23548;&#20986;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#65288;$W_1$&#65289;&#30340;&#19978;&#30028;&#65292;&#35813;&#36317;&#31163;&#26159;&#36830;&#32493;&#38543;&#26426;&#22330;&#19982;&#39640;&#26031;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#26031;&#24179;&#28369;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#24179;&#28369;&#24230;&#37327;&#20013;&#30340;&#19978;&#30028;&#36716;&#21270;&#20026;$W_1$&#36317;&#31163;&#12290;&#24179;&#28369;&#24615;&#26159;&#22522;&#20110;&#20351;&#29992;Laplacian&#31639;&#23376;&#30340;&#24130;&#26500;&#24314;&#30340;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#35774;&#35745;&#25104;&#19982;Cameron-Martin&#25110;Reproducing Kernel Hilbert Space&#30456;&#20851;&#32852;&#30340;&#39640;&#26031;&#36807;&#31243;&#20855;&#26377;&#26131;&#25805;&#20316;&#30340;&#29305;&#24449;&#12290;&#36825;&#20010;&#29305;&#24449;&#20351;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#20043;&#21069;&#25991;&#29486;&#20013;&#32771;&#34385;&#30340;&#19968;&#32500;&#21306;&#38388;&#22411;&#25351;&#26631;&#38598;&#12290;&#36890;&#36807;&#29305;&#21270;&#25105;&#20204;&#30340;&#19968;&#33324;&#32467;&#26524;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#22312;&#20219;&#24847;&#28145;&#24230;&#21644;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;&#24191;&#20041;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#39640;&#26031;&#38543;&#26426;&#22330;&#36924;&#36817;&#30340;&#39318;&#20010;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#19978;&#30028;&#26126;&#30830;&#22320;&#29992;&#32593;&#32476;&#23485;&#24230;&#21644;&#38543;&#26426;&#26435;&#37325;&#30340;&#30697;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive upper bounds on the Wasserstein distance ($W_1$), with respect to $\sup$-norm, between any continuous $\mathbb{R}^d$ valued random field indexed by the $n$-sphere and the Gaussian, based on Stein's method. We develop a novel Gaussian smoothing technique that allows us to transfer a bound in a smoother metric to the $W_1$ distance. The smoothing is based on covariance functions constructed using powers of Laplacian operators, designed so that the associated Gaussian process has a tractable Cameron-Martin or Reproducing Kernel Hilbert Space. This feature enables us to move beyond one dimensional interval-based index sets that were previously considered in the literature. Specializing our general result, we obtain the first bounds on the Gaussian random field approximation of wide random neural networks of any depth and Lipschitz activation functions at the random field level. Our bounds are explicitly expressed in terms of the widths of the network and moments of the random wei
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#20462;&#21098;&#30456;&#20851;&#23454;&#20307;&#65292;&#20174;&#32780;&#24341;&#23548;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#39046;&#22495;&#21644;&#24322;&#36136;&#31181;&#23376;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24212;&#29992;&#31867;&#27604;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.16296</link><description>&lt;p&gt;
&#30456;&#20851;&#23454;&#20307;&#36873;&#25321;&#65306;&#36890;&#36807;&#38646;&#26679;&#26412;&#31867;&#27604;&#20462;&#21098;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning. (arXiv:2306.16296v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#20462;&#21098;&#30456;&#20851;&#23454;&#20307;&#65292;&#20174;&#32780;&#24341;&#23548;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#39046;&#22495;&#21644;&#24322;&#36136;&#31181;&#23376;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24212;&#29992;&#31867;&#27604;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#39640;&#36136;&#37327;&#30340;&#26680;&#24515;&#24320;&#22987;&#65292;&#36890;&#36807;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#19981;&#26029;&#25913;&#36827;&#12290;&#36825;&#26679;&#30340;&#26680;&#24515;&#21487;&#20197;&#20174;&#20687;Wikidata&#36825;&#26679;&#30340;&#24320;&#25918;&#24335;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35268;&#27169;&#65292;&#23558;&#20854;&#20316;&#20026;&#25972;&#20307;&#38598;&#25104;&#21487;&#33021;&#20250;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#20174;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#24863;&#20852;&#36259;&#31181;&#23376;&#23454;&#20307;&#24320;&#22987;&#65292;&#24182;&#20445;&#30041;&#25110;&#20462;&#21098;&#20854;&#30456;&#37051;&#23454;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598; &#22312;Wikidata&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#39046;&#22495;&#21516;&#36136;&#25110;&#24322;&#36136;&#30340;&#31181;&#23376;&#23454;&#20307;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#20248;&#20110;LSTM&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#19988;&#21442;&#25968;&#25968;&#37327;&#22823;&#22823;&#20943;&#23569;&#12290;&#25105;&#20204;&#36824;&#22312;&#36801;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#36827;&#19968;&#27493;&#23558;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#38598;&#25104;&#21040;&#30456;&#20851;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Construction (KGC) can be seen as an iterative process starting from a high quality nucleus that is refined by knowledge extraction approaches in a virtuous loop. Such a nucleus can be obtained from knowledge existing in an open KG like Wikidata. However, due to the size of such generic KGs, integrating them as a whole may entail irrelevant content and scalability issues. We propose an analogy-based approach that starts from seed entities of interest in a generic KG, and keeps or prunes their neighboring entities. We evaluate our approach on Wikidata through two manually labeled datasets that contain either domain-homogeneous or -heterogeneous seed entities. We empirically show that our analogy-based approach outperforms LSTM, Random Forest, SVM, and MLP, with a drastically lower number of parameters. We also evaluate its generalization potential in a transfer learning setting. These results advocate for the further integration of analogy-based inference in tasks relate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#35782;&#21035;&#26497;&#31471;&#20107;&#20214;&#21069;&#20806;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#32858;&#31867;&#31995;&#32479;&#29366;&#24577;&#12289;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#21644;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#35782;&#21035;&#20102;&#20004;&#20010;&#19981;&#21516;&#28151;&#27788;&#31995;&#32479;&#20013;&#30340;&#26497;&#31471;&#20107;&#20214;&#21069;&#20806;&#12290;</title><link>http://arxiv.org/abs/2306.16291</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#28151;&#27788;&#31995;&#32479;&#26497;&#31471;&#20107;&#20214;&#21069;&#20806;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Clustering-based Identification of Precursors of Extreme Events in Chaotic Systems. (arXiv:2306.16291v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#35782;&#21035;&#26497;&#31471;&#20107;&#20214;&#21069;&#20806;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#32858;&#31867;&#31995;&#32479;&#29366;&#24577;&#12289;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#21644;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#35782;&#21035;&#20102;&#20004;&#20010;&#19981;&#21516;&#28151;&#27788;&#31995;&#32479;&#20013;&#30340;&#26497;&#31471;&#20107;&#20214;&#21069;&#20806;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#30028;&#20013;&#65292;&#22914;&#21095;&#28872;&#30340;&#27668;&#20505;&#27169;&#24335;&#12289;&#32597;&#35265;&#24040;&#28010;&#25110;&#38634;&#23849;&#31561;&#36807;&#31243;&#20013;&#65292;&#21160;&#21147;&#31995;&#32479;&#29366;&#24577;&#30340;&#31361;&#28982;&#32780;&#36805;&#36895;&#30340;&#39640;&#25391;&#24133;&#21464;&#21270;&#34987;&#31216;&#20026;&#26497;&#31471;&#20107;&#20214;&#12290;&#36825;&#20123;&#20107;&#20214;&#24448;&#24448;&#20855;&#26377;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#25551;&#36848;&#21644;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#28151;&#27788;&#29305;&#24615;&#65292;&#23427;&#20204;&#30340;&#24314;&#27169;&#33267;&#20170;&#20173;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;&#27169;&#22359;&#21270;&#30340;&#32858;&#31867;&#25216;&#26415;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#35782;&#21035;&#32597;&#35265;&#21644;&#26497;&#31471;&#20107;&#20214;&#21069;&#20806;&#30340;&#24212;&#29992;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#31995;&#32479;&#29366;&#24577;&#12289;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#21644;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#30340;&#35782;&#21035;&#26694;&#26550;&#65292;&#24182;&#22312;&#20004;&#20010;&#23637;&#31034;&#26497;&#31471;&#20107;&#20214;&#30340;&#19981;&#21516;&#28151;&#27788;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;Moehliss-Faisst-Eckhardt&#33258;&#25345;&#28237;&#27969;&#27169;&#22411;&#21644;&#20108;&#32500;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#27969;&#21160;&#27169;&#22411;&#12290;&#20004;&#32773;&#37117;&#23637;&#31034;&#20102;&#33021;&#37327;&#21644;&#32791;&#25955;&#20013;&#30340;&#31361;&#21457;&#26497;&#31471;&#20107;&#20214;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#26497;&#31471;&#20107;&#20214;&#30340;&#21069;&#20806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abrupt and rapid high-amplitude changes in a dynamical system's states known as extreme event appear in many processes occurring in nature, such as drastic climate patterns, rogue waves, or avalanches. These events often entail catastrophic effects, therefore their description and prediction is of great importance. However, because of their chaotic nature, their modelling represents a great challenge up to this day. The applicability of a data-driven modularity-based clustering technique to identify precursors of rare and extreme events in chaotic systems is here explored. The proposed identification framework based on clustering of system states, probability transition matrices and state space tessellation was developed and tested on two different chaotic systems that exhibit extreme events: the Moehliss-Faisst-Eckhardt model of self-sustained turbulence and the 2D Kolmogorov flow. Both exhibit extreme events in the form of bursts in kinetic energy and dissipation. It is shown that th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#19982;ChatGPT&#25110;GPT-4&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;(PSG)&#24320;&#21457;&#20013;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.16275</link><description>&lt;p&gt;
&#21033;&#29992;GPT-4&#36827;&#34892;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#20197;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#22686;&#24378;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting. (arXiv:2306.16275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#19982;ChatGPT&#25110;GPT-4&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;(PSG)&#24320;&#21457;&#20013;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#24433;&#21709;&#20174;&#26032;&#33647;&#30003;&#35831;(NDA)&#20013;&#30340;&#25688;&#35201;&#26159;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;(PSG)&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20174;&#22823;&#37327;&#33647;&#29289;&#30003;&#35831;&#23457;&#26597;&#25991;&#20214;&#20013;&#25163;&#21160;&#25688;&#35201;&#39135;&#29289;&#24433;&#21709;&#26159;&#32791;&#26102;&#30340;&#65292;&#36825;&#24341;&#21457;&#20102;&#24320;&#21457;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#21644;GPT-4&#30340;&#36827;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#25913;&#21892;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#25928;&#26524;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#22312;PSG&#35780;&#20272;&#20013;&#20934;&#30830;&#27010;&#25324;&#39135;&#29289;&#24433;&#21709;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#36845;&#20195;&#25552;&#31034;&#65292;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#19982;ChatGPT&#25110;GPT-4&#36827;&#34892;&#20114;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#36718;&#36845;&#20195;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#65292;&#20854;&#20013;&#22312;&#36830;&#32493;&#30340;&#36718;&#27425;&#20013;&#20998;&#21035;&#25552;&#20379;&#20102;&#20851;&#38190;&#23383;&#32858;&#28966;&#21644;&#38271;&#24230;&#25511;&#21046;&#30340;&#25552;&#31034;&#20197;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment. However, manual summarization of food effect from extensive drug application review documents is time-consuming, which arouses a need to develop automated methods. Recent advances in large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability regarding the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach, iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the 
&lt;/p&gt;</description></item><item><title>S2SNet&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#36229;&#23548;&#24615;&#12290;&#23427;&#21033;&#29992;&#20102;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22522;&#20110;&#21253;&#21547;&#26230;&#20307;&#32467;&#26500;&#21644;&#36229;&#23548;&#20020;&#30028;&#28201;&#24230;&#30340;&#26032;&#25968;&#25454;&#38598;S2S&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.16270</link><description>&lt;p&gt;
S2SNet&#65306;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36229;&#23548;&#24615;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
S2SNet: A Pretrained Neural Network for Superconductivity Discovery. (arXiv:2306.16270v1 [cond-mat.supr-con])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16270
&lt;/p&gt;
&lt;p&gt;
S2SNet&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#36229;&#23548;&#24615;&#12290;&#23427;&#21033;&#29992;&#20102;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22522;&#20110;&#21253;&#21547;&#26230;&#20307;&#32467;&#26500;&#21644;&#36229;&#23548;&#20020;&#30028;&#28201;&#24230;&#30340;&#26032;&#25968;&#25454;&#38598;S2S&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#23548;&#24615;&#33021;&#20351;&#24471;&#30005;&#27969;&#33021;&#22815;&#22312;&#27809;&#26377;&#33021;&#37327;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#27969;&#21160;&#65292;&#22240;&#27492;&#20351;&#22266;&#20307;&#26448;&#26009;&#20855;&#26377;&#36229;&#23548;&#24615;&#26159;&#29289;&#29702;&#23398;&#12289;&#26448;&#26009;&#31185;&#23398;&#21644;&#30005;&#27668;&#24037;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#24050;&#32463;&#26377;16&#20301;&#35834;&#36125;&#23572;&#22870;&#24471;&#20027;&#22240;&#20026;&#20854;&#23545;&#36229;&#23548;&#24615;&#30740;&#31350;&#30340;&#36129;&#29486;&#32780;&#33719;&#24471;&#20102;&#22870;&#39033;&#12290;&#36229;&#23548;&#20307;&#23545;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDGs&#65289;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#22914;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12289;&#21487;&#36127;&#25285;&#24471;&#36215;&#30340;&#28165;&#27905;&#33021;&#28304;&#12289;&#24037;&#19994;&#12289;&#21019;&#26032;&#21644;&#22522;&#30784;&#35774;&#26045;&#31561;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#27809;&#26377;&#19968;&#20010;&#32479;&#19968;&#30340;&#29289;&#29702;&#29702;&#35770;&#33021;&#22815;&#35299;&#37322;&#25152;&#26377;&#36229;&#23548;&#24615;&#26426;&#21046;&#12290;&#20154;&#20204;&#35748;&#20026;&#65292;&#36229;&#23548;&#24615;&#30340;&#24494;&#35266;&#26426;&#21046;&#19981;&#20165;&#19982;&#20998;&#23376;&#32452;&#25104;&#26377;&#20851;&#65292;&#36824;&#19982;&#26230;&#20307;&#32467;&#26500;&#26377;&#20851;&#12290;&#22240;&#27492;&#65292;&#22312;SuperCon&#21644;Material Project&#30340;&#22522;&#30784;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;S2S&#65292;&#20854;&#20013;&#21253;&#21547;&#26230;&#20307;&#32467;&#26500;&#21644;&#36229;&#23548;&#20020;&#30028;&#28201;&#24230;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;S2SNet&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#27880;&#24847;&#26426;&#21046;&#26469;&#36827;&#34892;&#36229;&#23548;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Superconductivity allows electrical current to flow without any energy loss, and thus making solids superconducting is a grand goal of physics, material science, and electrical engineering. More than 16 Nobel Laureates have been awarded for their contribution to superconductivity research. Superconductors are valuable for sustainable development goals (SDGs), such as climate change mitigation, affordable and clean energy, industry, innovation and infrastructure, and so on. However, a unified physics theory explaining all superconductivity mechanism is still unknown. It is believed that superconductivity is microscopically due to not only molecular compositions but also the geometric crystal structure. Hence a new dataset, S2S, containing both crystal structures and superconducting critical temperature, is built upon SuperCon and Material Project. Based on this new dataset, we propose a novel model, S2SNet, which utilizes the attention mechanism for superconductivity prediction. To over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#27169;&#25311;&#21452;&#20998;&#23700;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#21495;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31639;&#27861;&#21644;&#20351;&#29992;&#28145;&#24230;&#23637;&#24320;&#25216;&#26415;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16264</link><description>&lt;p&gt;
&#28145;&#24230;&#23637;&#24320;&#27169;&#25311;&#30340;&#21452;&#20998;&#23700;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Unfolded Simulated Bifurcation for Massive MIMO Signal Detection. (arXiv:2306.16264v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#27169;&#25311;&#21452;&#20998;&#23700;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#21495;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31639;&#27861;&#21644;&#20351;&#29992;&#28145;&#24230;&#23637;&#24320;&#25216;&#26415;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#36890;&#20449;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#37327;&#23376;&#65288;&#21551;&#21457;&#24335;&#65289;&#31639;&#27861;&#30340;&#21508;&#31181;MIMO&#20449;&#21495;&#26816;&#27979;&#22120;&#24050;&#34987;&#25552;&#20986;&#65292;&#20197;&#25913;&#21892;&#19982;&#20256;&#32479;&#26816;&#27979;&#22120;&#30456;&#27604;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#20851;&#27880;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#20013;&#30340;&#27169;&#25311;&#21452;&#20998;&#23700;&#65288;SB&#65289;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#25913;&#21892;&#20854;&#26816;&#27979;&#24615;&#33021;&#12290;&#31532;&#19968;&#31181;&#26159;&#20462;&#25913;&#21463;Levenberg-Marquardt&#31639;&#27861;&#21551;&#21457;&#30340;&#31639;&#27861;&#65292;&#20197;&#28040;&#38500;&#26368;&#22823;&#20284;&#28982;&#26816;&#27979;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#28145;&#24230;&#23637;&#24320;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#26469;&#35757;&#32451;&#36845;&#20195;&#31639;&#27861;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23637;&#24320;&#30340;SB&#65292;&#36890;&#36807;&#20351;SB&#30340;&#26356;&#26032;&#35268;&#21017;&#21487;&#24494;&#20998;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#25552;&#20986;&#30340;&#26816;&#27979;&#22120;&#26174;&#33879;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#30340;&#20449;&#21495;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple-input multiple-output (MIMO) is a key ingredient of next-generation wireless communications. Recently, various MIMO signal detectors based on deep learning techniques and quantum(-inspired) algorithms have been proposed to improve the detection performance compared with conventional detectors. This paper focuses on the simulated bifurcation (SB) algorithm, a quantum-inspired algorithm. This paper proposes two techniques to improve its detection performance. The first is modifying the algorithm inspired by the Levenberg-Marquardt algorithm to eliminate local minima of maximum likelihood detection. The second is the use of deep unfolding, a deep learning technique to train the internal parameters of an iterative algorithm. We propose a deep-unfolded SB by making the update rule of SB differentiable. The numerical results show that these proposed detectors significantly improve the signal detection performance in massive MIMO systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#28508;&#22312;SDE&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20301;&#29699;&#19978;&#30340;SDE&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#34920;&#36798;&#24335;&#26469;&#35745;&#31639;&#36817;&#20284;&#21518;&#39564;&#21644;&#20808;&#39564;&#36807;&#31243;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.16248</link><description>&lt;p&gt;
&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#28508;&#22312;SDE
&lt;/p&gt;
&lt;p&gt;
Latent SDEs on Homogeneous Spaces. (arXiv:2306.16248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#28508;&#22312;SDE&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20301;&#29699;&#19978;&#30340;SDE&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#34920;&#36798;&#24335;&#26469;&#35745;&#31639;&#36817;&#20284;&#21518;&#39564;&#21644;&#20808;&#39564;&#36807;&#31243;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#65288;&#21487;&#33021;&#26159;&#22797;&#26434;&#30340;&#65289;&#35266;&#27979;&#38543;&#26426;&#36807;&#31243;&#30001;&#28508;&#22312;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#25152;&#39537;&#21160;&#12290;&#21463;&#21040;&#23398;&#20064;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#65288;&#20960;&#20046;&#20219;&#24847;&#65289;&#28508;&#22312;&#31070;&#32463;SDE&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#20363;&#22914;&#25928;&#29575;&#26799;&#24230;&#35745;&#31639;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#24182;&#30740;&#31350;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#23376;&#31867;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;SDE&#22312;&#19968;&#20010;&#40784;&#27425;&#28508;&#22312;&#31354;&#38388;&#19978;&#28436;&#21464;&#65292;&#24182;&#30001;&#30456;&#24212;&#65288;&#30697;&#38453;&#65289;Lie&#32676;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#25152;&#35825;&#23548;&#12290;&#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21333;&#20301;$n$-&#29699;&#19978;&#30340;SDE&#21487;&#20197;&#35828;&#26159;&#36825;&#19968;&#35774;&#32622;&#20013;&#26368;&#30456;&#20851;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#65292;&#21333;&#20301;&#29699;&#19981;&#20165;&#26377;&#21161;&#20110;&#20351;&#29992;&#30495;&#27491;&#26080;&#20449;&#24687;&#30340;&#20808;&#39564;SDE&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#33719;&#24471;&#20102;&#20851;&#20110;&#36817;&#20284;&#21518;&#39564;&#21644;&#20808;&#39564;&#36807;&#31243;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#29305;&#21035;&#31616;&#21333;&#21644;&#30452;&#35266;&#30340;&#34920;&#36798;&#24335;&#65292;&#36825;&#22312;&#35777;&#25454;&#19979;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of variational Bayesian inference in a latent variable model where a (possibly complex) observed stochastic process is governed by the solution of a latent stochastic differential equation (SDE). Motivated by the challenges that arise when trying to learn an (almost arbitrary) latent neural SDE from large-scale data, such as efficient gradient computation, we take a step back and study a specific subclass instead. In our case, the SDE evolves on a homogeneous latent space and is induced by stochastic dynamics of the corresponding (matrix) Lie group. In learning problems, SDEs on the unit $n$-sphere are arguably the most relevant incarnation of this setup. Notably, for variational inference, the sphere not only facilitates using a truly uninformative prior SDE, but we also obtain a particularly simple and intuitive expression for the Kullback-Leibler divergence between the approximate posterior and prior process in the evidence lower bound. Experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;q-learning&#22312;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#29992;&#20110;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;q&#20989;&#25968;&#30340;&#23384;&#22312;&#21450;&#20854;&#31215;&#20998;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.16208</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;q-learning&#29992;&#20110;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Continuous-Time q-learning for McKean-Vlasov Control Problems. (arXiv:2306.16208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;q-learning&#22312;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#29992;&#20110;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;q&#20989;&#25968;&#30340;&#23384;&#22312;&#21450;&#20854;&#31215;&#20998;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;q-learning&#65292;&#22312;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;&#12290;&#19982;Jia&#21644;Zhou&#65288;2022c&#65289;&#30340;&#21333;&#20010;&#20195;&#29702;&#25511;&#21046;&#38382;&#39064;&#19981;&#21516;&#65292;&#20195;&#29702;&#20043;&#38388;&#30340;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#20351;&#24471;q&#20989;&#25968;&#30340;&#23450;&#20041;&#26356;&#21152;&#22797;&#26434;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#28982;&#20135;&#29983;&#20004;&#31181;&#19981;&#21516;q&#20989;&#25968;&#30340;&#24773;&#20917;&#65306;&#65288;i&#65289;&#34987;&#31216;&#20026;&#38598;&#25104;q&#20989;&#25968;&#65288;&#29992;$q$&#34920;&#31034;&#65289;&#65292;&#20316;&#20026;Gu&#12289;Guo&#12289;Wei&#21644;Xu&#65288;2023&#65289;&#24341;&#20837;&#30340;&#38598;&#25104;Q&#20989;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#21487;&#20197;&#36890;&#36807;&#28041;&#21450;&#27979;&#35797;&#31574;&#30053;&#30340;&#24369;&#38789;&#26465;&#20214;&#36827;&#34892;&#23398;&#20064;&#65307;&#65288;ii&#65289;&#20316;&#20026;&#31574;&#30053;&#25913;&#36827;&#36845;&#20195;&#20013;&#25152;&#20351;&#29992;&#30340;&#23454;&#36136;q&#20989;&#25968;&#65288;&#29992;$q_e$&#34920;&#31034;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;q&#20989;&#25968;&#22312;&#25152;&#26377;&#27979;&#35797;&#31574;&#30053;&#19979;&#36890;&#36807;&#31215;&#20998;&#34920;&#31034;&#30456;&#20851;&#32852;&#12290;&#22522;&#20110;&#38598;&#25104;q&#20989;&#25968;&#30340;&#24369;&#38789;&#26465;&#20214;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31639;&#27861;&#26469;&#23398;&#20064;&#20004;&#20010;q&#20989;&#25968;&#20197;&#35299;&#20915;Mckean-Vlasov&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the q-learning, recently coined as the continuous-time counterpart of Q-learning by Jia and Zhou (2022c), for continuous time Mckean-Vlasov control problems in the setting of entropy-regularized reinforcement learning. In contrast to the single agent's control problem in Jia and Zhou (2022c), the mean-field interaction of agents render the definition of q-function more subtle, for which we reveal that two distinct q-functions naturally arise: (i) the integrated q-function (denoted by $q$) as the first-order approximation of the integrated Q-function introduced in Gu, Guo, Wei and Xu (2023) that can be learnt by a weak martingale condition involving test policies; and (ii) the essential q-function (denoted by $q_e$) that is employed in the policy improvement iterations. We show that two q-functions are related via an integral representation under all test policies. Based on the weak martingale condition of the integrated q-function and our proposed searching method of
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16177</link><description>&lt;p&gt;
&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65306;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defining data science: a new field of inquiry. (arXiv:2306.16177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16177
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#12290;&#23427;&#30340;&#21147;&#37327;&#12289;&#33539;&#22260;&#21644;&#35268;&#27169;&#23558;&#36229;&#36234;&#31185;&#23398;&#65292;&#25104;&#20026;&#20419;&#20351;&#30693;&#35782;&#21457;&#29616;&#24182;&#25913;&#21464;&#19990;&#30028;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#25105;&#20204;&#23578;&#26410;&#29702;&#35299;&#21644;&#23450;&#20041;&#23427;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#20854;&#28508;&#21147;&#21644;&#31649;&#29702;&#20854;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#33258;1962&#24180;&#20197;&#26469;&#32531;&#24930;&#21457;&#23637;&#65292;&#24182;&#19988;&#33258;2000&#24180;&#20197;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#23427;&#26159;&#19968;&#31181;&#26681;&#26412;&#24615;&#30340;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26159;21&#19990;&#32426;&#26368;&#27963;&#36291;&#12289;&#26368;&#24378;&#22823;&#21644;&#21457;&#23637;&#26368;&#24555;&#30340;&#21019;&#26032;&#20043;&#19968;&#12290;&#30001;&#20110;&#20854;&#20215;&#20540;&#12289;&#21147;&#37327;&#21644;&#36866;&#29992;&#24615;&#65292;&#23427;&#27491;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#25968;&#25454;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#21253;&#21547;&#20102;&#26080;&#25968;&#20851;&#20110;&#25968;&#25454;&#31185;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#23450;&#20041;&#12290;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#35768;&#22810;&#23450;&#20041;&#26159;&#29420;&#31435;&#30340;&#12289;&#24212;&#29992;&#29305;&#23450;&#30340;&#12289;&#30456;&#20114;&#19981;&#23436;&#25972;&#30340;&#12289;&#20887;&#20313;&#30340;&#25110;&#19981;&#19968;&#33268;&#30340;&#65292;&#22240;&#27492;&#25968;&#25454;&#31185;&#23398;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#22810;&#37325;&#23450;&#20041;&#25361;&#25112;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in 40+ disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application-specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#25945;&#24072;&#23545;&#25239;&#40065;&#26834;&#24615;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#24178;&#20928;&#26679;&#26412;&#25945;&#24072;&#21644;&#40065;&#26834;&#24615;&#25945;&#24072;&#26469;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.16170</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#25945;&#24072;&#23545;&#25239;&#33976;&#39311;&#20943;&#36731;&#20934;&#30830;&#24615;&#19982;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation. (arXiv:2306.16170v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#25945;&#24072;&#23545;&#25239;&#40065;&#26834;&#24615;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#24178;&#20928;&#26679;&#26412;&#25945;&#24072;&#21644;&#40065;&#26834;&#24615;&#25945;&#24072;&#26469;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#19968;&#31181;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#34429;&#28982;&#26377;&#25928;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#24178;&#20928;&#26679;&#26412;&#30340;&#24615;&#33021;&#21364;&#26377;&#25152;&#19979;&#38477;&#65292;&#36825;&#24847;&#21619;&#30528;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26435;&#34913;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#20294;&#24182;&#27809;&#26377;&#26174;&#33879;&#25913;&#21892;&#23545;&#24178;&#20928;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#22810;&#25945;&#24072;&#23545;&#25239;&#40065;&#26834;&#24615;&#33976;&#39311;&#65288;MTARD&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#24378;&#22823;&#30340;&#24178;&#20928;&#26679;&#26412;&#25945;&#24072;&#21644;&#24378;&#22823;&#30340;&#40065;&#26834;&#26679;&#26412;&#25945;&#24072;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#25945;&#24072;&#26174;&#31034;&#30456;&#20284;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#29109;&#30340;&#24179;&#34913;&#31639;&#27861;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is a practical approach for improving the robustness of deep neural networks against adversarial attacks. Although bringing reliable robustness, the performance toward clean examples is negatively affected after adversarial training, which means a trade-off exists between accuracy and robustness. Recently, some studies have tried to use knowledge distillation methods in adversarial training, achieving competitive performance in improving the robustness but the accuracy for clean samples is still limited. In this paper, to mitigate the accuracy-robustness trade-off, we introduce the Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the model's adversarial training process by applying a strong clean teacher and a strong robust teacher to handle the clean examples and adversarial examples, respectively. During the optimization process, to ensure that different teachers show similar knowledge scales, we design the Entropy-Based Balance algorithm to adj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#36890;&#20449;&#36164;&#28304;&#21463;&#38480;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#21644;&#27169;&#22411;&#32858;&#21512;&#26469;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#22312;CARLA&#20223;&#30495;&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16169</link><description>&lt;p&gt;
&#36890;&#20449;&#36164;&#28304;&#21463;&#38480;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Communication Resources Constrained Hierarchical Federated Learning for End-to-End Autonomous Driving. (arXiv:2306.16169v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#36890;&#20449;&#36164;&#28304;&#21463;&#38480;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#21644;&#27169;&#22411;&#32858;&#21512;&#26469;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#22312;CARLA&#20223;&#30495;&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#27169;&#22411;&#32858;&#21512;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20256;&#32479;&#30340;&#21333;&#36339;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#30001;&#20110;&#36710;&#36742;&#21644;&#20113;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36828;&#31243;&#36890;&#20449;&#32780;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#36890;&#36807;&#24341;&#20837;&#20013;&#38388;&#36793;&#32536;&#26381;&#21153;&#22120;&#20811;&#26381;&#20102;&#36825;&#20123;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#21463;&#38480;&#30340;&#36890;&#20449;&#36164;&#28304;&#21644;HFL&#24615;&#33021;&#20043;&#38388;&#30340;&#21327;&#35843;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#36890;&#20449;&#36164;&#28304;&#21463;&#38480;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;CRCHFL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#25968;&#25454;&#21644;&#27169;&#22411;&#32858;&#21512;&#26469;&#26368;&#23567;&#21270;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#36890;&#36807;&#22312;CARLA&#20223;&#30495;&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;CRCHFL&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CRCHFL&#26082;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#21448;&#22686;&#24378;&#20102;&#32852;&#37030;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While federated learning (FL) improves the generalization of end-to-end autonomous driving by model aggregation, the conventional single-hop FL (SFL) suffers from slow convergence rate due to long-range communications among vehicles and cloud server. Hierarchical federated learning (HFL) overcomes such drawbacks via introduction of mid-point edge servers. However, the orchestration between constrained communication resources and HFL performance becomes an urgent problem. This paper proposes an optimization-based Communication Resource Constrained Hierarchical Federated Learning (CRCHFL) framework to minimize the generalization error of the autonomous driving model using hybrid data and model aggregation. The effectiveness of the proposed CRCHFL is evaluated in the Car Learning to Act (CARLA) simulation platform. Results show that the proposed CRCHFL both accelerates the convergence rate and enhances the generalization of federated learning autonomous driving model. Moreover, under the 
&lt;/p&gt;</description></item><item><title>&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#39046;&#22495;&#65292;&#24182;&#19988;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#21457;&#23637;&#20063;&#19982;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#30456;&#20114;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16156</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#20248;&#36755;&#36816;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Optimal Transport for Machine Learning. (arXiv:2306.16156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16156
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#39046;&#22495;&#65292;&#24182;&#19988;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#21457;&#23637;&#20063;&#19982;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26368;&#20248;&#36755;&#36816;&#34987;&#25552;&#20986;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#27604;&#36739;&#21644;&#25805;&#20316;&#27010;&#29575;&#20998;&#24067;&#30340;&#27010;&#29575;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#28304;&#20110;&#20854;&#20016;&#23500;&#30340;&#21382;&#21490;&#21644;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#29983;&#25104;&#24314;&#27169;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;2012&#24180;&#33267;2022&#24180;&#26399;&#38388;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#36129;&#29486;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#30340;&#22235;&#20010;&#23376;&#39046;&#22495;&#65306;&#26377;&#30417;&#30563;&#23398;&#20064;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#31361;&#20986;&#20102;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#19982;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Optimal Transport has been proposed as a probabilistic framework in Machine Learning for comparing and manipulating probability distributions. This is rooted in its rich history and theory, and has offered new solutions to different problems in machine learning, such as generative modeling and transfer learning. In this survey we explore contributions of Optimal Transport for Machine Learning over the period 2012 -- 2022, focusing on four sub-fields of Machine Learning: supervised, unsupervised, transfer and reinforcement learning. We further highlight the recent development in computational Optimal Transport, and its interplay with Machine Learning practice.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#25104;&#29087;&#24230;&#27169;&#22411;&#65288;MLSMM&#65289;&#65292;&#36890;&#36807;&#23558;&#23433;&#20840;&#23454;&#36341;&#27839;&#30528;ML&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#36827;&#34892;&#32452;&#32455;&#21644;&#24314;&#31435;&#25104;&#29087;&#24230;&#27700;&#24179;&#65292;&#20197;&#35780;&#20272;&#23433;&#20840;&#23454;&#36341;&#30340;&#25104;&#29087;&#24230;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#20419;&#36827;&#20135;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#26356;&#32039;&#23494;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16127</link><description>&lt;p&gt;
MLSMM&#65306;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#25104;&#29087;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MLSMM: Machine Learning Security Maturity Model. (arXiv:2306.16127v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16127
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#25104;&#29087;&#24230;&#27169;&#22411;&#65288;MLSMM&#65289;&#65292;&#36890;&#36807;&#23558;&#23433;&#20840;&#23454;&#36341;&#27839;&#30528;ML&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#36827;&#34892;&#32452;&#32455;&#21644;&#24314;&#31435;&#25104;&#29087;&#24230;&#27700;&#24179;&#65292;&#20197;&#35780;&#20272;&#23433;&#20840;&#23454;&#36341;&#30340;&#25104;&#29087;&#24230;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#20419;&#36827;&#20135;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#26356;&#32039;&#23494;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22522;&#20110;&#36719;&#20214;&#32452;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#35780;&#20272;&#23433;&#20840;&#23454;&#36341;&#30340;&#25104;&#29087;&#24230;&#27809;&#26377;&#24471;&#21040;&#20687;&#20256;&#32479;&#36719;&#20214;&#24320;&#21457;&#37027;&#26679;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#31687;&#34013;&#22825;&#24819;&#27861;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#25104;&#29087;&#24230;&#27169;&#22411;&#65288;MLSMM&#65289;&#65292;&#35813;&#27169;&#22411;&#23558;&#23433;&#20840;&#23454;&#36341;&#27839;&#30528;ML&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#36827;&#34892;&#32452;&#32455;&#65292;&#24182;&#20026;&#27599;&#20010;&#23454;&#36341;&#24314;&#31435;&#20102;&#19977;&#20010;&#25104;&#29087;&#24230;&#27700;&#24179;&#12290;&#25105;&#20204;&#23558;MLSMM&#35270;&#20026;&#20135;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#26356;&#32039;&#23494;&#21512;&#20316;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing the maturity of security practices during the development of Machine Learning (ML) based software components has not gotten as much attention as traditional software development. In this Blue Sky idea paper, we propose an initial Machine Learning Security Maturity Model (MLSMM) which organizes security practices along the ML-development lifecycle and, for each, establishes three levels of maturity. We envision MLSMM as a step towards closer collaboration between industry and academia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#33258;&#21160;&#36716;&#24405;&#34920;&#26684;&#25968;&#25454;&#30340;&#20154;&#24037;&#23457;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36716;&#24405;&#22823;&#37327;&#30340;&#25163;&#20889;&#32844;&#19994;&#20195;&#30721;&#24182;&#36827;&#34892;&#25163;&#21160;&#23457;&#26680;&#65292;&#36798;&#21040;&#20102;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2306.16126</link><description>&lt;p&gt;
&#26356;&#39640;&#25928;&#30340;&#33258;&#21160;&#36716;&#24405;&#34920;&#26684;&#25968;&#25454;&#30340;&#20154;&#24037;&#23457;&#26680;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
More efficient manual review of automatically transcribed tabular data. (arXiv:2306.16126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#33258;&#21160;&#36716;&#24405;&#34920;&#26684;&#25968;&#25454;&#30340;&#20154;&#24037;&#23457;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36716;&#24405;&#22823;&#37327;&#30340;&#25163;&#20889;&#32844;&#19994;&#20195;&#30721;&#24182;&#36827;&#34892;&#25163;&#21160;&#23457;&#26680;&#65292;&#36798;&#21040;&#20102;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36716;&#24405;&#21382;&#21490;&#25968;&#25454;&#26041;&#38754;&#24050;&#32463;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#39640;&#31934;&#24230;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#20063;&#38656;&#35201;&#25163;&#21160;&#39564;&#35777;&#21644;&#32416;&#27491;&#12290;&#36825;&#26679;&#30340;&#20154;&#24037;&#23457;&#26680;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#65292;&#22240;&#27492;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#20808;&#21069;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#39640;&#31934;&#24230;&#36716;&#24405;&#20102;&#25386;&#23041; 1950 &#24180;&#20154;&#21475;&#26222;&#26597;&#20013;&#30340; 230 &#19975;&#20221;&#25163;&#20889;&#32844;&#19994;&#20195;&#30721;&#65292;&#28982;&#21518;&#25163;&#21160;&#23457;&#26680;&#20102; 3%&#65288;9 &#19975;&#20221;&#65289;&#32622;&#20449;&#24230;&#26368;&#20302;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#23558;&#36825; 9 &#19975;&#20221;&#20195;&#30721;&#20998;&#37197;&#32473;&#20154;&#24037;&#23457;&#26680;&#21592;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#24037;&#20855;&#36827;&#34892;&#23457;&#26680;&#12290;&#20026;&#20102;&#35780;&#20272;&#23457;&#26680;&#21592;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#37096;&#20998;&#20195;&#30721;&#34987;&#20998;&#37197;&#32473;&#22810;&#20301;&#23457;&#26680;&#21592;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23457;&#26680;&#32467;&#26524;&#20197;&#20102;&#35299;&#20934;&#30830;&#24615;&#25913;&#36827;&#19982;&#24037;&#20316;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#35775;&#20102;&#23457;&#26680;&#21592;&#20197;&#25913;&#21892;&#24037;&#20316;&#27969;&#31243;&#12290;&#23457;&#26680;&#21592;&#26356;&#27491;&#20102; 62.8% &#30340;&#26631;&#31614;&#65292;&#24182;&#22312; 31.9% &#30340;&#26696;&#20363;&#20013;&#19982;&#27169;&#22411;&#30340;&#26631;&#31614;&#36798;&#25104;&#19968;&#33268;&#12290;&#32422; 0.2% &#30340;&#22270;&#29255;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#20154;&#24037;&#23457;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods have proven useful in transcribing historical data. However, results from even highly accurate methods require manual verification and correction. Such manual review can be time-consuming and expensive, therefore the objective of this paper was to make it more efficient. Previously, we used machine learning to transcribe 2.3 million handwritten occupation codes from the Norwegian 1950 census with high accuracy (97%). We manually reviewed the 90,000 (3%) codes with the lowest model confidence. We allocated those 90,000 codes to human reviewers, who used our annotation tool to review the codes. To assess reviewer agreement, some codes were assigned to multiple reviewers. We then analyzed the review results to understand the relationship between accuracy improvements and effort. Additionally, we interviewed the reviewers to improve the workflow. The reviewers corrected 62.8% of the labels and agreed with the model label in 31.9% of cases. About 0.2% of the images 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#27491;&#21521;&#23545;&#38598;&#21512;&#65288;SPPS&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#27491;&#21521;&#23454;&#20363;&#65292;&#20174;&#32780;&#20943;&#23569;&#20002;&#24323;&#37325;&#35201;&#29305;&#24449;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16122</link><description>&lt;p&gt;
&#22686;&#24378;&#23545;&#27604;&#23454;&#20363;&#21306;&#20998;&#30340;&#35821;&#20041;&#27491;&#21521;&#23545;
&lt;/p&gt;
&lt;p&gt;
Semantic Positive Pairs for Enhancing Contrastive Instance Discrimination. (arXiv:2306.16122v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#27491;&#21521;&#23545;&#38598;&#21512;&#65288;SPPS&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#27491;&#21521;&#23454;&#20363;&#65292;&#20174;&#32780;&#20943;&#23569;&#20002;&#24323;&#37325;&#35201;&#29305;&#24449;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23454;&#20363;&#21306;&#20998;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26377;&#25928;&#22320;&#38450;&#27490;&#34920;&#31034;&#22349;&#32553;&#65292;&#24182;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#20135;&#29983;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21560;&#24341;&#27491;&#21521;&#23545;&#65288;&#21363;&#30456;&#21516;&#23454;&#20363;&#30340;&#20004;&#20010;&#35270;&#22270;&#65289;&#24182;&#25490;&#26021;&#25152;&#26377;&#20854;&#20182;&#23454;&#20363;&#65288;&#21363;&#36127;&#21521;&#23545;&#65289;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#31867;&#21035;&#65292;&#21487;&#33021;&#23548;&#33268;&#20002;&#24323;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#27491;&#21521;&#23454;&#20363;&#65292;&#21629;&#21517;&#20026;&#35821;&#20041;&#27491;&#21521;&#23545;&#38598;&#21512;&#65288;SPPS&#65289;&#65292;&#20174;&#32780;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#20943;&#23569;&#20102;&#20002;&#24323;&#37325;&#35201;&#29305;&#24449;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#23545;&#27604;&#23454;&#20363;&#21306;&#20998;&#26694;&#26550;&#65288;&#22914;SimCLR&#25110;MOCO&#65289;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;ImageNet&#12289;STL-10&#21644;CIFAR-10&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;vanilla&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning algorithms based on instance discrimination effectively prevent representation collapse and produce promising results in representation learning. However, the process of attracting positive pairs (i.e., two views of the same instance) in the embedding space and repelling all other instances (i.e., negative pairs) irrespective of their categories could result in discarding important features. To address this issue, we propose an approach to identifying those images with similar semantic content and treating them as positive instances, named semantic positive pairs set (SPPS), thereby reducing the risk of discarding important features during representation learning. Our approach could work with any contrastive instance discrimination framework such as SimCLR or MOCO. We conduct experiments on three datasets: ImageNet, STL-10 and CIFAR-10 to evaluate our approach. The experimental results show that our approach consistently outperforms the baseline method vanilla 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#27491;&#21017;&#21270;&#22312;&#26368;&#20248;&#26102;&#38388;&#21464;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#33539;&#22260;&#30452;&#25509;&#30456;&#20851;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#20943;&#23569;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16111</link><description>&lt;p&gt;
&#26102;&#38388;&#27491;&#21017;&#21270;&#22312;&#26368;&#20248;&#26102;&#38388;&#21464;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Time Regularization in Optimal Time Variable Learning. (arXiv:2306.16111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#27491;&#21017;&#21270;&#22312;&#26368;&#20248;&#26102;&#38388;&#21464;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#33539;&#22260;&#30452;&#25509;&#30456;&#20851;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#20943;&#23569;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;arXiv:2204.08528&#20013;&#24341;&#20837;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#30340;&#26368;&#20248;&#26102;&#38388;&#21464;&#37327;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#33539;&#22260;&#30452;&#25509;&#30456;&#20851;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#25193;&#23637;&#20102;&#36825;&#20010;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;Residual Neural Networks (ResNets)&#65292;&#21487;&#20197;&#20943;&#23569;&#32593;&#32476;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#33879;&#21517;&#30340;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;PyTorch&#20195;&#30721;&#21487;&#22312;https://github.com/frederikkoehne/time_variable_learning&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, optimal time variable learning in deep neural networks (DNNs) was introduced in arXiv:2204.08528. In this manuscript we extend the concept by introducing a regularization term that directly relates to the time horizon in discrete dynamical systems. Furthermore, we propose an adaptive pruning approach for Residual Neural Networks (ResNets), which reduces network complexity without compromising expressiveness, while simultaneously decreasing training time. The results are illustrated by applying the proposed concepts to classification tasks on the well known MNIST and Fashion MNIST data sets. Our PyTorch code is available on https://github.com/frederikkoehne/time_variable_learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#24369;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#30340;&#21508;&#31181;&#25512;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#21387;&#32553;&#24863;&#30693;&#21644;&#24863;&#30693;&#22120;&#23398;&#20064;&#12290;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16097</link><description>&lt;p&gt;
&#31232;&#30095;&#34920;&#31034;&#12289;&#25512;&#29702;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sparse Representations, Inference and Learning. (arXiv:2306.16097v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#24369;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#30340;&#21508;&#31181;&#25512;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#21387;&#32553;&#24863;&#30693;&#21644;&#24863;&#30693;&#22120;&#23398;&#20064;&#12290;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32479;&#35745;&#29289;&#29702;&#23398;&#24050;&#32463;&#35777;&#26126;&#26159;&#25506;&#31350;&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#22823;&#32500;&#25512;&#29702;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;&#32479;&#35745;&#29289;&#29702;&#23398;&#25552;&#20379;&#20102;&#20998;&#26512;&#24037;&#20855;&#26469;&#30740;&#31350;&#20854;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#31639;&#27861;&#26469;&#35299;&#20915;&#20010;&#21035;&#23454;&#20363;&#12290;&#22312;&#36825;&#20123;&#31508;&#35760;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;2022&#24180;Les Houches&#22799;&#23395;&#23398;&#26657;&#20013;Marc M\'ezard&#30340;&#35762;&#24231;&#65292;&#20171;&#32461;&#19968;&#31181;&#21487;&#20197;&#22312;&#24369;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#30340;&#21508;&#31181;&#38382;&#39064;&#20013;&#20351;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#25110;&#24863;&#30693;&#22120;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#30475;&#21040;&#36825;&#20123;&#38382;&#39064;&#22914;&#20309;&#22312;&#22797;&#21046;&#23545;&#31216;&#32423;&#21035;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#20351;&#29992;&#20013;&#33108;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#26082;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#20063;&#20316;&#20026;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years statistical physics has proven to be a valuable tool to probe into large dimensional inference problems such as the ones occurring in machine learning. Statistical physics provides analytical tools to study fundamental limitations in their solutions and proposes algorithms to solve individual instances. In these notes, based on the lectures by Marc M\'ezard in 2022 at the summer school in Les Houches, we will present a general framework that can be used in a large variety of problems with weak long-range interactions, including the compressed sensing problem, or the problem of learning in a perceptron. We shall see how these problems can be studied at the replica symmetric level, using developments of the cavity methods, both as a theoretical tool and as an algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#20102;&#21452;&#26354;&#27491;&#20999;&#12289;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21644;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#28608;&#27963;&#20989;&#25968;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26354;&#38754;&#65292;&#21457;&#29616;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21576;&#29616;&#26368;&#20984;&#22411;&#26354;&#38754;&#65292;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#21576;&#29616;&#26368;&#24179;&#22374;&#26354;&#38754;&#24182;&#20855;&#26377;&#26356;&#20248;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25152;&#26377;&#28608;&#27963;&#20989;&#25968;&#30340;&#25439;&#22833;&#26354;&#38754;&#20013;&#23384;&#22312;&#23485;&#38420;&#21644;&#29421;&#31364;&#30340;&#23665;&#35895;&#65292;&#32780;&#29421;&#31364;&#30340;&#23665;&#35895;&#19982;&#39281;&#21644;&#31070;&#32463;&#20803;&#21644;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#32593;&#32476;&#32467;&#26500;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2306.16090</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32463;&#39564;&#25439;&#22833;&#26354;&#38754;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Loss Landscape Analysis of Neural Network Activation Functions. (arXiv:2306.16090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#20102;&#21452;&#26354;&#27491;&#20999;&#12289;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21644;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#28608;&#27963;&#20989;&#25968;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26354;&#38754;&#65292;&#21457;&#29616;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21576;&#29616;&#26368;&#20984;&#22411;&#26354;&#38754;&#65292;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#21576;&#29616;&#26368;&#24179;&#22374;&#26354;&#38754;&#24182;&#20855;&#26377;&#26356;&#20248;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25152;&#26377;&#28608;&#27963;&#20989;&#25968;&#30340;&#25439;&#22833;&#26354;&#38754;&#20013;&#23384;&#22312;&#23485;&#38420;&#21644;&#29421;&#31364;&#30340;&#23665;&#35895;&#65292;&#32780;&#29421;&#31364;&#30340;&#23665;&#35895;&#19982;&#39281;&#21644;&#31070;&#32463;&#20803;&#21644;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#32593;&#32476;&#32467;&#26500;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#20250;&#24433;&#21709;&#25439;&#22833;&#26354;&#38754;&#30340;&#24615;&#36136;&#12290;&#20102;&#35299;&#28608;&#27963;&#20989;&#25968;&#19982;&#25439;&#22833;&#26354;&#38754;&#24615;&#36136;&#30340;&#20851;&#31995;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#35774;&#35745;&#26159;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#19978;&#20998;&#26512;&#20102;&#19982;&#21452;&#26354;&#27491;&#20999;&#12289;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21644;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#28608;&#27963;&#20989;&#25968;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26354;&#38754;&#12290;&#23454;&#39564;&#35777;&#26126;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#20135;&#29983;&#26368;&#20984;&#22411;&#30340;&#25439;&#22833;&#26354;&#38754;&#65292;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#20135;&#29983;&#26368;&#24179;&#22374;&#30340;&#25439;&#22833;&#26354;&#38754;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;&#26356;&#20248;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#20110;&#25152;&#26377;&#28608;&#27963;&#20989;&#25968;&#65292;&#25439;&#22833;&#26354;&#38754;&#20013;&#23384;&#22312;&#23485;&#38420;&#21644;&#29421;&#31364;&#30340;&#23665;&#35895;&#65292;&#24182;&#19988;&#29421;&#31364;&#30340;&#23665;&#35895;&#19982;&#39281;&#21644;&#31070;&#32463;&#20803;&#21644;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#32593;&#32476;&#32467;&#26500;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions play a significant role in neural network design by enabling non-linearity. The choice of activation function was previously shown to influence the properties of the resulting loss landscape. Understanding the relationship between activation functions and loss landscape properties is important for neural architecture and training algorithm design. This study empirically investigates neural network loss landscapes associated with hyperbolic tangent, rectified linear unit, and exponential linear unit activation functions. Rectified linear unit is shown to yield the most convex loss landscape, and exponential linear unit is shown to yield the least flat loss landscape, and to exhibit superior generalisation performance. The presence of wide and narrow valleys in the loss landscape is established for all activation functions, and the narrow valleys are shown to correlate with saturated neurons and implicitly regularised network configurations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#27169;&#24335;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36136;&#35889;&#39044;&#27979;&#32593;&#32476;&#65288;MoMS-Net&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#23376;&#32467;&#26500;&#22312;&#22270;&#32423;&#21035;&#30340;&#26041;&#24335;&#23454;&#29616;&#36136;&#35889;&#39044;&#27979;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25193;&#23637;&#36136;&#35889;&#24211;&#24182;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16085</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#27169;&#24335;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#36136;&#35889;
&lt;/p&gt;
&lt;p&gt;
Mass Spectra Prediction with Structural Motif-based Graph Neural Networks. (arXiv:2306.16085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16085
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#27169;&#24335;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36136;&#35889;&#39044;&#27979;&#32593;&#32476;&#65288;MoMS-Net&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#23376;&#32467;&#26500;&#22312;&#22270;&#32423;&#21035;&#30340;&#26041;&#24335;&#23454;&#29616;&#36136;&#35889;&#39044;&#27979;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25193;&#23637;&#36136;&#35889;&#24211;&#24182;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#35889;&#26159;&#26469;&#33258;&#30446;&#26631;&#20998;&#23376;&#30340;&#31163;&#23376;&#21270;&#30862;&#29255;&#30340;&#22242;&#22359;&#65292;&#23545;&#20110;&#20998;&#23376;&#32467;&#26500;&#30340;&#37492;&#23450;&#22312;&#21508;&#20010;&#39046;&#22495;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#20998;&#26512;&#26041;&#27861;&#26159;&#20351;&#29992;&#20809;&#35889;&#24211;&#25628;&#32034;&#65292;&#23558;&#26410;&#30693;&#30340;&#36136;&#35889;&#19982;&#25968;&#25454;&#24211;&#36827;&#34892;&#20132;&#21449;&#24341;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#29616;&#26377;&#36136;&#35889;&#25968;&#25454;&#24211;&#33539;&#22260;&#30340;&#38480;&#21046;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#36136;&#35889;&#39044;&#27979;&#25193;&#23637;&#25968;&#25454;&#24211;&#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#27169;&#24335;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23454;&#29616;&#30340;&#36136;&#35889;&#39044;&#27979;&#32593;&#32476;&#65288;MoMS-Net&#65289;&#12290;&#25105;&#20204;&#24050;&#32463;&#22312;&#22810;&#26679;&#30340;&#36136;&#35889;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#22312;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;MoMS-Net&#22312;&#22270;&#32423;&#21035;&#32771;&#34385;&#20102;&#23376;&#32467;&#26500;&#65292;&#36825;&#26377;&#21161;&#20110;&#34701;&#20837;&#38271;&#31243;&#20381;&#36182;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mass spectra, which are agglomerations of ionized fragments from targeted molecules, play a crucial role across various fields for the identification of molecular structures. A prevalent analysis method involves spectral library searches,where unknown spectra are cross-referenced with a database. The effectiveness of such search-based approaches, however, is restricted by the scope of the existing mass spectra database, underscoring the need to expand the database via mass spectra prediction. In this research, we propose the Motif-based Mass Spectrum Prediction Network (MoMS-Net), a system that predicts mass spectra using the information derived from structural motifs and the implementation of Graph Neural Networks (GNNs). We have tested our model across diverse mass spectra and have observed its superiority over other existing models. MoMS-Net considers substructure at the graph level, which facilitates the incorporation of long-range dependencies while using less memory compared to t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#20445;&#25252;&#38544;&#31169;&#24182;&#22312;&#19978;&#28216;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ZOO-based VFL&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16077</link><description>&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#24322;&#27493;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;:&#22522;&#20110;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization. (arXiv:2306.16077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#20445;&#25252;&#38544;&#31169;&#24182;&#22312;&#19978;&#28216;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ZOO-based VFL&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;(VFL)&#22240;&#33021;&#22815;&#22312;&#22402;&#30452;&#20998;&#21106;&#30340;&#25968;&#25454;&#19978;&#32852;&#21512;&#35757;&#32451;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;(ZOO)&#22312;&#26500;&#24314;&#23454;&#29992;&#30340;VFL&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;ZOO&#30340;VFL&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20854;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#22788;&#29702;&#29616;&#20195;&#22823;&#22411;&#27169;&#22411;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;VFL&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20013;&#65292;&#19979;&#28216;&#27169;&#22411;&#65288;&#23458;&#25143;&#31471;&#65289;&#20351;&#29992;ZOO&#36827;&#34892;&#35757;&#32451;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#30830;&#20445;&#19981;&#20849;&#20139;&#20869;&#37096;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#19978;&#28216;&#27169;&#22411;&#65288;&#26381;&#21153;&#22120;&#65289;&#22312;&#26412;&#22320;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;(FOO)&#36827;&#34892;&#26356;&#26032;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#21069;&#25552;&#19979;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;VFL&#26694;&#26550;&#27604;&#22522;&#20110;ZOO&#30340;VFL&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) attracts increasing attention because it empowers multiple parties to jointly train a privacy-preserving model over vertically partitioned data. Recent research has shown that applying zeroth-order optimization (ZOO) has many advantages in building a practical VFL algorithm. However, a vital problem with the ZOO-based VFL is its slow convergence rate, which limits its application in handling modern large models. To address this problem, we propose a cascaded hybrid optimization method in VFL. In this method, the downstream models (clients) are trained with ZOO to protect privacy and ensure that no internal information is shared. Meanwhile, the upstream model (server) is updated with first-order optimization (FOO) locally, which significantly improves the convergence rate, making it feasible to train the large models without compromising privacy and security. We theoretically prove that our VFL framework converges faster than the ZOO-based VFL, as the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20256;&#36755;&#25552;&#31034;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#25552;&#21319;&#24615;&#33021;&#12289;&#21152;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.16064</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Generative Learning with Foundation Models. (arXiv:2306.16064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20256;&#36755;&#25552;&#31034;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#25552;&#21319;&#24615;&#33021;&#12289;&#21152;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#29305;&#24449;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#65292;&#36825;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#20302;&#25928;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;&#20511;&#21161;&#26032;&#20852;&#30340;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#65292;&#23427;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#25552;&#31034;&#12290;&#36890;&#36807;&#25509;&#25910;&#21040;&#30340;&#21253;&#21547;&#36739;&#23569;&#38544;&#31169;&#20449;&#24687;&#30340;&#25552;&#31034;&#20197;&#21450;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20010;&#26032;&#26694;&#26550;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#26356;&#22909;&#30340;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#21152;&#24378;&#20102;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;ImageNet&#21644;DomainNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing federated learning solutions focus on transmitting features, parameters or gadients between clients and server, which suffer from serious low-efficiency and privacy-leakage problems. Thanks to the emerging foundation generative models, we propose a novel federated learning framework, namely Federated Generative Learning, that transmits prompts associated with distributed training data between clients and server. The informative training data can be synthesized remotely based on received prompts containing little privacy and the foundation generative models. The new framework possesses multiple advantages, including improved communication efficiency, better resilience to distribution shift, substantial performance gains, and enhanced privacy protection, which are verified in extensive experiments on ImageNet and DomainNet datasets.
&lt;/p&gt;</description></item><item><title>DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16058</link><description>&lt;p&gt;
DUET: 2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16058
&lt;/p&gt;
&lt;p&gt;
DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;(MSSL)&#22522;&#20110;&#23398;&#20064;&#30456;&#23545;&#20110;&#19968;&#32452;&#36755;&#20837;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#21464;&#24615;&#20174;&#34920;&#31034;&#20013;&#37096;&#20998;&#25110;&#23436;&#20840;&#31227;&#38500;&#19982;&#21464;&#25442;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#38656;&#35201;&#36825;&#20123;&#20449;&#24687;&#30340;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#36896;&#25104;&#25439;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;2D&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#34920;&#31034;&#65292;&#31216;&#20026;DUET&#65292;&#23427;&#20204;&#26159;&#20197;&#30697;&#38453;&#32467;&#26500;&#32452;&#32455;&#30340;2D&#34920;&#31034;&#65292;&#24182;&#19988;&#23545;&#20316;&#29992;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#25442;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;DUET&#34920;&#31034;&#20445;&#30041;&#26377;&#20851;&#36755;&#20837;&#21464;&#25442;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#34920;&#36798;&#33021;&#21147;&#12290;&#19982;SimCLR&#65288;Chen&#31561;&#65292;2020&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#19981;&#21464;&#24615;&#65289;&#21644;ESSL&#65288;Dangovski&#31561;&#65292;2022&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#31561;&#21464;&#24615;&#65289;&#30456;&#27604;&#65292;DUET&#34920;&#31034;&#30340;&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#24615;&#20351;&#24471;&#29983;&#25104;&#20855;&#26377;&#26356;&#20302;&#30340;&#37325;&#24314;&#35823;&#24046;&#30340;&#21487;&#25511;&#24615;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;SimCLR&#25110;ESSL&#21017;&#26080;&#27861;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;DUET&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiview Self-Supervised Learning (MSSL) is based on learning invariances with respect to a set of input transformations. However, invariance partially or totally removes transformation-related information from the representations, which might harm performance for specific downstream tasks that require such information. We propose 2D strUctured and EquivarianT representations (coined DUET), which are 2d representations organized in a matrix structure, and equivariant with respect to transformations acting on the input data. DUET representations maintain information about an input transformation, while remaining semantically expressive. Compared to SimCLR (Chen et al., 2020) (unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured and equivariant), the structured and equivariant nature of DUET representations enables controlled generation with lower reconstruction error, while controllability is not possible with SimCLR or ESSL. DUET also achieves higher accuracy fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#39044;&#25490;&#24207;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#28789;&#38271;&#31867;&#22768;&#38899;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#8220;ComparE 2021&#8221;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;UAR&#20998;&#25968;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.16054</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#39044;&#25490;&#24207;&#26469;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#28789;&#38271;&#31867;&#22768;&#38899;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Primate Sounds Classification using Binary Presorting for Deep Learning. (arXiv:2306.16054v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#39044;&#25490;&#24207;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#28789;&#38271;&#31867;&#22768;&#38899;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#8220;ComparE 2021&#8221;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;UAR&#20998;&#25968;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#29983;&#21160;&#29289;&#35266;&#23519;&#21644;&#20445;&#25252;&#39046;&#22495;&#65292;&#20351;&#29992;&#38899;&#39057;&#35760;&#24405;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26469;&#33258;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#21487;&#29992;&#25968;&#25454;&#38598;&#36890;&#24120;&#19981;&#26159;&#26368;&#20339;&#30340;&#23398;&#20064;&#26448;&#26009;&#65307;&#26679;&#26412;&#21487;&#33021;&#34987;&#24369;&#26631;&#35760;&#65292;&#38271;&#24230;&#19981;&#21516;&#25110;&#20449;&#22122;&#27604;&#36739;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24191;&#20041;&#26041;&#27861;&#65292;&#39318;&#20808;&#37325;&#26032;&#26631;&#35760;MEL&#35889;&#22270;&#34920;&#31034;&#30340;&#23376;&#27573;&#65292;&#20197;&#22312;&#23454;&#38469;&#30340;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#20108;&#36827;&#21046;&#39044;&#25490;&#24207;&#21644;&#20998;&#31867;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#8220;ComparE 2021&#8221;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#22312;&#20998;&#31867;&#19981;&#21516;&#28789;&#38271;&#31867;&#29289;&#31181;&#22768;&#38899;&#30340;&#20219;&#21153;&#20013;&#65292;&#25253;&#21578;&#20102;&#19982;&#30456;&#27604;&#36739;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#26174;&#33879;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;UAR&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of wildlife observation and conservation, approaches involving machine learning on audio recordings are becoming increasingly popular. Unfortunately, available datasets from this field of research are often not optimal learning material; Samples can be weakly labeled, of different lengths or come with a poor signal-to-noise ratio. In this work, we introduce a generalized approach that first relabels subsegments of MEL spectrogram representations, to achieve higher performances on the actual multi-class classification tasks. For both the binary pre-sorting and the classification, we make use of convolutional neural networks (CNN) and various data-augmentation techniques. We showcase the results of this approach on the challenging \textit{ComparE 2021} dataset, with the task of classifying between different primate species sounds, and report significantly higher Accuracy and UAR scores in contrast to comparatively equipped model baselines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#35780;&#20272;&#20102;&#28145;&#24230;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#23481;&#26131;&#34987;&#25915;&#20987;&#12290;&#36824;&#30740;&#31350;&#20102;&#21435;&#22122;&#27169;&#22411;&#30340;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#20013;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16050</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#35780;&#20272;&#28145;&#24230;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack. (arXiv:2306.16050v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16050
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#35780;&#20272;&#20102;&#28145;&#24230;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#23481;&#26131;&#34987;&#25915;&#20987;&#12290;&#36824;&#30740;&#31350;&#20102;&#21435;&#22122;&#27169;&#22411;&#30340;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#20013;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#21435;&#22122;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#30340;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#26356;&#20248;&#36234;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#21487;&#36991;&#20813;&#22320;&#26174;&#31034;&#20986;&#24369;&#40065;&#26834;&#24615;&#65292;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#26131;&#21463;&#25439;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;&#29616;&#26377;&#28145;&#24230;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#23427;&#20204;&#37117;&#23481;&#26131;&#34987;&#23545;&#25239;&#25915;&#20987;&#27450;&#39575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;-PGD&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20840;&#23545;&#25239;&#30340;&#21435;&#22122;&#27169;&#22411;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#38750;&#30450;&#21435;&#22122;&#27169;&#22411;&#65288;DnCNN&#65292;FFDNet&#65292;ECNDNet&#65292;BRDNet&#65289;&#65292;&#30450;&#21435;&#22122;&#27169;&#22411;&#65288;DnCNN-B&#65292;Noise2Noise&#65292;RDDCNN-B&#65292;FAN&#65289;&#65292;&#21363;&#25554;&#21363;&#29992;&#65288;DPIR&#65292;CurvPnP&#65289;&#21644;&#23637;&#24320;&#21435;&#22122;&#27169;&#22411;&#65288;DeamNet&#65289;&#24212;&#29992;&#20110;&#28784;&#24230;&#21644;&#24425;&#33394;&#22270;&#20687;&#37117;&#21487;&#20197;&#34987;&#21516;&#19968;&#32452;&#26041;&#27861;&#25915;&#20987;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#21435;&#22122;-PGD&#30340;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#20013;&#24456;&#31361;&#20986;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#25506;&#32034;&#36801;&#31227;&#24615;&#19979;&#30340;&#28508;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have a wide range of applications in the field of image denoising, and they are superior to traditional image denoising. However, DNNs inevitably show vulnerability, which is the weak robustness in the face of adversarial attacks. In this paper, we find some similitudes between existing deep image denoising methods, as they are consistently fooled by adversarial attacks. First, denoising-PGD is proposed which is a denoising model full adversarial method. The current mainstream non-blind denoising models (DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise, RDDCNN-B, FAN), and plug-and-play (DPIR, CurvPnP) and unfolding denoising models (DeamNet) applied to grayscale and color images can be attacked by the same set of methods. Second, since the transferability of denoising-PGD is prominent in the image denoising task, we design experiments to explore the characteristic of the latent under the transferability. We correlate transferabi
&lt;/p&gt;</description></item><item><title>OpenNDD&#26159;&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.16045</link><description>&lt;p&gt;
OpenNDD:&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16045
&lt;/p&gt;
&lt;p&gt;
OpenNDD&#26159;&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;(NDDs)&#26159;&#19968;&#32452;&#39640;&#24739;&#30149;&#29575;&#30340;&#38556;&#30861;&#65292;&#34920;&#29616;&#20986;&#20020;&#24202;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#24471;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#30340;NDDs&#65288;&#22914;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#21644;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#65288;ADHD&#65289;&#65289;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;NDDs&#35786;&#26029;&#24182;&#27809;&#26377;&#21487;&#38752;&#30340;&#29983;&#29702;&#26631;&#24535;&#29289;&#65292;&#32780;&#20165;&#20381;&#36182;&#20110;&#24515;&#29702;&#35780;&#20272;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26234;&#33021;&#36741;&#21161;&#35786;&#26029;&#26469;&#38450;&#27490;&#35823;&#35786;&#21644;&#28431;&#35786;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#19982;&#38543;&#21518;&#30340;&#30456;&#24212;&#27835;&#30103;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;NDDs&#31579;&#26597;&#21644;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#36825;&#26159;&#22312;&#35813;&#39046;&#22495;&#20013;&#39318;&#27425;&#24212;&#29992;&#24320;&#25918;&#24615;&#35782;&#21035;&#12290;&#23427;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#36807;&#21435;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurodevelopmental disorders (NDDs) are a highly prevalent group of disorders and represent strong clinical behavioral similarities, and that make it very challenging for accurate identification of different NDDs such as autism spectrum disorder (ASD) and attention-deficit hyperactivity disorder (ADHD). Moreover, there is no reliable physiological markers for NDDs diagnosis and it solely relies on psychological evaluation criteria. However, it is crucial to prevent misdiagnosis and underdiagnosis by intelligent assisted diagnosis, which is closely related to the follow-up corresponding treatment. In order to relieve these issues, we propose a novel open set recognition framework for NDDs screening and detection, which is the first application of open set recognition in this field. It combines auto encoder and adversarial reciprocal points open set recognition to accurately identify known classes as well as recognize classes never encountered. And considering the strong similarities bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#29992;&#25143;&#19978;&#19979;&#25991;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#21644;&#34394;&#25311;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#37319;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#25512;&#26029;&#36807;&#31243;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16029</link><description>&lt;p&gt;
&#29992;&#25143;&#19978;&#19979;&#25991;&#30340;&#36731;&#37327;&#32423;&#24314;&#27169;&#65306;&#32467;&#21512;&#29289;&#29702;&#21644;&#34394;&#25311;&#20256;&#24863;&#22120;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Lightweight Modeling of User Context Combining Physical and Virtual Sensor Data. (arXiv:2306.16029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#29992;&#25143;&#19978;&#19979;&#25991;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#21644;&#34394;&#25311;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#37319;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#25512;&#26029;&#36807;&#31243;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#35774;&#22791;&#19978;&#20256;&#24863;&#22120;&#20135;&#29983;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#25903;&#25345;&#19978;&#19979;&#25991;&#24863;&#30693;&#26381;&#21153;&#65292;&#35782;&#21035;&#29992;&#25143;&#30340;&#24403;&#21069;&#24773;&#20917;&#65288;&#21363;&#29289;&#29702;&#19978;&#19979;&#25991;&#65289;&#65292;&#24182;&#20248;&#21270;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#33021;&#20027;&#35201;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#25512;&#26029;&#36807;&#31243;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#19982;&#22823;&#35268;&#27169;&#21644;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#32039;&#23494;&#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#21457;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25910;&#38598;&#21253;&#21547;&#20010;&#20154;&#31227;&#21160;&#35774;&#22791;&#23548;&#20986;&#30340;&#22810;&#26679;&#21270;&#20256;&#24863;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#26694;&#26550;&#24050;&#32463;&#34987;3&#20010;&#24535;&#24895;&#32773;&#29992;&#25143;&#20351;&#29992;&#20102;&#20004;&#20010;&#26143;&#26399;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;36K&#20010;&#26679;&#26412;&#21644;1331&#20010;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#33021;&#22815;&#22312;&#29992;&#25143;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#22320;&#25191;&#34892;&#25972;&#20010;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20845;&#31181;&#32500;&#24230;&#20943;&#23569;&#25216;&#26415;&#65292;&#20197;&#20248;&#21270;&#19978;&#19979;&#25991;&#20998;&#31867;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multitude of data generated by sensors available on users' mobile devices, combined with advances in machine learning techniques, support context-aware services in recognizing the current situation of a user (i.e., physical context) and optimizing the system's personalization features. However, context-awareness performances mainly depend on the accuracy of the context inference process, which is strictly tied to the availability of large-scale and labeled datasets. In this work, we present a framework developed to collect datasets containing heterogeneous sensing data derived from personal mobile devices. The framework has been used by 3 voluntary users for two weeks, generating a dataset with more than 36K samples and 1331 features. We also propose a lightweight approach to model the user context able to efficiently perform the entire reasoning process on the user mobile device. To this aim, we used six dimensionality reduction techniques in order to optimize the context classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#20856;&#23398;&#20064;&#32773;&#21644;&#37327;&#23376;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#25351;&#25968;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#23398;&#20064;&#20998;&#31163;&#38382;&#39064;&#65292;&#20854;&#20013;&#32463;&#20856;&#22256;&#38590;&#20027;&#35201;&#22312;&#20110;&#35782;&#21035;&#29983;&#25104;&#25968;&#25454;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.16028</link><description>&lt;p&gt;
&#32463;&#20856;&#23398;&#20064;&#32773;&#19982;&#37327;&#23376;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#25351;&#25968;&#21306;&#21035;
&lt;/p&gt;
&lt;p&gt;
Exponential separations between classical and quantum learners. (arXiv:2306.16028v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#20856;&#23398;&#20064;&#32773;&#21644;&#37327;&#23376;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#25351;&#25968;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#23398;&#20064;&#20998;&#31163;&#38382;&#39064;&#65292;&#20854;&#20013;&#32463;&#20856;&#22256;&#38590;&#20027;&#35201;&#22312;&#20110;&#35782;&#21035;&#29983;&#25104;&#25968;&#25454;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#22312;&#22788;&#29702;&#32463;&#20856;&#25968;&#25454;&#26102;&#24050;&#32463;&#23637;&#31034;&#20102;&#37327;&#23376;&#23398;&#20064;&#20248;&#21183;&#65292;&#20294;&#22312;&#23547;&#25214;&#37327;&#23376;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22312;&#32463;&#20856;&#23398;&#20064;&#31639;&#27861;&#19978;&#23454;&#29616;&#21487;&#35777;&#26126;&#25351;&#25968;&#21152;&#36895;&#30340;&#23398;&#20064;&#38382;&#39064;&#26041;&#38754;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#35752;&#35770;&#19982;&#27492;&#38382;&#39064;&#30456;&#20851;&#30340;&#35745;&#31639;&#23398;&#20064;&#29702;&#35770;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#23450;&#20041;&#19978;&#30340;&#24494;&#22937;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#32773;&#38656;&#35201;&#28385;&#36275;&#21644;&#35299;&#20915;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24050;&#26377;&#30340;&#20855;&#26377;&#21487;&#35777;&#26126;&#37327;&#23376;&#21152;&#36895;&#24615;&#36136;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#35745;&#31639;&#29983;&#25104;&#25968;&#25454;&#30340;&#20989;&#25968;&#30340;&#32463;&#20856;&#38590;&#24230;&#65292;&#32780;&#19981;&#26159;&#35782;&#21035;&#36825;&#20010;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#23398;&#20064;&#20998;&#31163;&#38382;&#39064;&#65292;&#20854;&#20013;&#32463;&#20856;&#22256;&#38590;&#20027;&#35201;&#22312;&#20110;&#35782;&#21035;&#29983;&#25104;&#25968;&#25454;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant effort, the quantum machine learning community has only demonstrated quantum learning advantages for artificial cryptography-inspired datasets when dealing with classical data. In this paper we address the challenge of finding learning problems where quantum learning algorithms can achieve a provable exponential speedup over classical learning algorithms. We reflect on computational learning theory concepts related to this question and discuss how subtle differences in definitions can result in significantly different requirements and tasks for the learner to meet and solve. We examine existing learning problems with provable quantum speedups and find that they largely rely on the classical hardness of evaluating the function that generates the data, rather than identifying it. To address this, we present two new learning separations where the classical difficulty primarily lies in identifying the function generating the data. Furthermore, we explore computational h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#32852;&#30431;&#21306;&#22359;&#38142;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#21464;&#38382;&#39064;&#20013;&#24322;&#26500;&#27169;&#22411;&#21644;&#27169;&#22411;&#38388;&#21327;&#20316;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16023</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#27169;&#22411;&#38598;&#25104;&#24322;&#26500;&#27169;&#22411;&#21644;&#32852;&#30431;&#21306;&#22359;&#38142;&#35299;&#20915;&#26102;&#21464;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Distributed Computation Model Based on Federated Learning Integrates Heterogeneous models and Consortium Blockchain for Solving Time-Varying Problems. (arXiv:2306.16023v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#32852;&#30431;&#21306;&#22359;&#38142;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#21464;&#38382;&#39064;&#20013;&#24322;&#26500;&#27169;&#22411;&#21644;&#27169;&#22411;&#38388;&#21327;&#20316;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26102;&#21464;&#38382;&#39064;&#65292;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#20013;&#24335;&#22788;&#29702;&#30340;&#26041;&#24335;&#38480;&#21046;&#65292;&#27169;&#22411;&#24615;&#33021;&#21463;&#21040;&#29616;&#23454;&#20013;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#23396;&#31435;&#38382;&#39064;&#31561;&#22240;&#32032;&#30340;&#26497;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;&#20363;&#22914;&#32852;&#37030;&#23398;&#20064;&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#27169;&#22411;&#38388;&#30340;&#21160;&#24577;&#32858;&#21512;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#38598;&#25104;&#36807;&#31243;&#20173;&#28982;&#20381;&#36182;&#20110;&#26381;&#21153;&#22120;&#65292;&#21487;&#33021;&#32473;&#25972;&#20307;&#27169;&#22411;&#24102;&#26469;&#24456;&#22823;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#21482;&#20801;&#35768;&#21516;&#36136;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#24182;&#19988;&#27809;&#26377;&#24456;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#24322;&#26500;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#30431;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#27169;&#22411;&#65288;DCM&#65289;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#24322;&#26500;&#27169;&#22411;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23618;&#32423;&#38598;&#25104;&#65288;DHI&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recurrent neural network has been greatly developed for effectively solving time-varying problems corresponding to complex environments. However, limited by the way of centralized processing, the model performance is greatly affected by factors like the silos problems of the models and data in reality. Therefore, the emergence of distributed artificial intelligence such as federated learning (FL) makes it possible for the dynamic aggregation among models. However, the integration process of FL is still server-dependent, which may cause a great risk to the overall model. Also, it only allows collaboration between homogeneous models, and does not have a good solution for the interaction between heterogeneous models. Therefore, we propose a Distributed Computation Model (DCM) based on the consortium blockchain network to improve the credibility of the overall model and effective coordination among heterogeneous models. In addition, a Distributed Hierarchical Integration (DHI) algorith
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16021</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32467;&#26500;&#65306;&#35843;&#26597;&#19982;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Structure in Reinforcement Learning: A Survey and Open Problems. (arXiv:2306.16021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16021
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#20989;&#25968;&#36924;&#36817;&#26041;&#38754;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#23545;&#22810;&#26679;&#19988;&#19981;&#21487;&#39044;&#27979;&#30340;&#21160;&#24577;&#12289;&#22024;&#26434;&#20449;&#21495;&#20197;&#21450;&#24222;&#22823;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#31561;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#26102;&#65292;&#20854;&#23454;&#29992;&#24615;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;&#35832;&#22914;&#25968;&#25454;&#25928;&#29575;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12289;&#32570;&#23569;&#23433;&#20840;&#20445;&#35777;&#21644;&#19981;&#21487;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#22312;&#36825;&#20123;&#20851;&#38190;&#25351;&#26631;&#19978;&#25552;&#39640;&#24615;&#33021;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#23558;&#38382;&#39064;&#30340;&#38468;&#21152;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#21508;&#20010;&#23376;&#39046;&#22495;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#32435;&#20837;&#36825;&#26679;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#19979;&#65292;&#25581;&#31034;&#32467;&#26500;&#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing a wide range of real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning prob
&lt;/p&gt;</description></item><item><title>BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2306.16015</link><description>&lt;p&gt;
BayesFlow: &#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
BayesFlow: Amortized Bayesian Workflows With Neural Networks. (arXiv:2306.16015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16015
&lt;/p&gt;
&lt;p&gt;
BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36125;&#21494;&#26031;&#25512;&#26029;&#28041;&#21450;&#19968;&#31995;&#21015;&#35745;&#31639;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#12289;&#39564;&#35777;&#21644;&#20174;&#27010;&#29575;&#27169;&#22411;&#20013;&#24471;&#20986;&#32467;&#35770;&#65292;&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20013;&#26377;&#21407;&#21017;&#30340;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#12290;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#20013;&#30340;&#20856;&#22411;&#38382;&#39064;&#21253;&#25324;&#36817;&#20284;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#22411;&#31867;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#22797;&#26434;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#27604;&#36739;&#21516;&#19968;&#36807;&#31243;&#30340;&#31454;&#20105;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Python&#24211;BayesFlow&#65292;&#29992;&#20110;&#22522;&#20110;&#20223;&#30495;&#35757;&#32451;&#24050;&#24314;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#25674;&#36824;&#25968;&#25454;&#21387;&#32553;&#21644;&#25512;&#26029;&#12290;&#22312;BayesFlow&#20013;&#23454;&#29616;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#37325;&#29992;&#20110;&#27169;&#22411;&#30340;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#30001;&#20110;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#21487;&#20197;&#20960;&#20046;&#21363;&#26102;&#22320;&#25191;&#34892;&#25512;&#26029;&#65292;&#22240;&#27492;&#21069;&#26399;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24456;&#24555;&#23601;&#33021;&#22815;&#25674;&#36824;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Bayesian inference involves a mixture of computational techniques for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows for data analysis. Typical problems in Bayesian workflows are the approximation of intractable posterior distributions for diverse model types and the comparison of competing models of the same process in terms of their complexity and predictive performance. This manuscript introduces the Python library BayesFlow for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#26631;&#31614;&#22122;&#22768;&#20462;&#27491;&#25216;&#26415;&#22312;&#20445;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#36890;&#36807;&#24320;&#21457;&#32463;&#39564;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#20462;&#27491;&#26041;&#27861;&#23545;&#20462;&#27491;&#26377;&#20559;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.15994</link><description>&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#20462;&#27491;&#23545;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#24433;&#21709;&#30340;&#31995;&#32479;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Systematic analysis of the impact of label noise correction on ML Fairness. (arXiv:2306.15994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#26631;&#31614;&#22122;&#22768;&#20462;&#27491;&#25216;&#26415;&#22312;&#20445;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#36890;&#36807;&#24320;&#21457;&#32463;&#39564;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#20462;&#27491;&#26041;&#27861;&#23545;&#20462;&#27491;&#26377;&#20559;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#12289;&#19981;&#19968;&#33268;&#25110;&#38169;&#35823;&#30340;&#20915;&#31574;&#24341;&#21457;&#20102;&#20005;&#37325;&#20851;&#20999;&#65292;&#38450;&#27490;&#19981;&#20844;&#24179;&#27169;&#22411;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#26085;&#30410;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#25968;&#25454;&#32463;&#24120;&#21453;&#26144;&#20102;&#36807;&#21435;&#30340;&#27495;&#35270;&#34892;&#20026;&#65292;&#35757;&#32451;&#22312;&#27492;&#31867;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#21487;&#33021;&#21453;&#26144;&#20102;&#23545;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#24180;&#40836;&#65289;&#30340;&#20559;&#35265;&#12290;&#24320;&#21457;&#20844;&#24179;&#27169;&#22411;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#21435;&#38500;&#28508;&#22312;&#30340;&#20559;&#35265;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#65292;&#20363;&#22914;&#36890;&#36807;&#32416;&#27491;&#26377;&#20559;&#35265;&#30340;&#26631;&#31614;&#12290;&#34429;&#28982;&#26377;&#22810;&#31181;&#26631;&#31614;&#22122;&#22768;&#20462;&#27491;&#26041;&#27861;&#21487;&#29992;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#22312;&#35782;&#21035;&#27495;&#35270;&#26041;&#38754;&#30340;&#34892;&#20026;&#30340;&#20449;&#24687;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32463;&#39564;&#26041;&#27861;&#26469;&#31995;&#32479;&#35780;&#20272;&#26631;&#31614;&#22122;&#22768;&#20462;&#27491;&#25216;&#26415;&#22312;&#30830;&#20445;&#35757;&#32451;&#20110;&#26377;&#20559;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#25805;&#32437;&#26631;&#31614;&#22122;&#22768;&#30340;&#31243;&#24230;&#65292;&#21487;&#29992;&#20110;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#35813;&#26041;&#27861;&#26469;&#35780;&#20272;&#20960;&#31181;&#24120;&#35265;&#30340;&#26631;&#31614;&#22122;&#22768;&#20462;&#27491;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20844;&#24179;&#24615;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#36825;&#20123;&#20462;&#27491;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arbitrary, inconsistent, or faulty decision-making raises serious concerns, and preventing unfair models is an increasingly important challenge in Machine Learning. Data often reflect past discriminatory behavior, and models trained on such data may reflect bias on sensitive attributes, such as gender, race, or age. One approach to developing fair models is to preprocess the training data to remove the underlying biases while preserving the relevant information, for example, by correcting biased labels. While multiple label noise correction methods are available, the information about their behavior in identifying discrimination is very limited. In this work, we develop an empirical methodology to systematically evaluate the effectiveness of label noise correction techniques in ensuring the fairness of models trained on biased datasets. Our methodology involves manipulating the amount of label noise and can be used with fairness benchmarks but also with standard ML datasets. We apply t
&lt;/p&gt;</description></item><item><title>"MyDigitalFootprint" &#26159;&#19968;&#20010;&#20197;&#36793;&#32536;&#35745;&#31639;&#20026;&#24212;&#29992;&#22330;&#26223;&#30340;&#20840;&#38754;&#32972;&#26223;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35782;&#21035;&#21644;&#31038;&#20132;&#20851;&#31995;&#24314;&#27169;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#29289;&#29702;&#25509;&#36817;&#20449;&#24687;&#21644;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20114;&#21160;&#12290;&#36890;&#36807;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#22312;&#29992;&#25143;&#30340;&#33258;&#28982;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#21644;&#19978;&#19979;&#25991;&#36866;&#24212;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15990</link><description>&lt;p&gt;
"MyDigitalFootprint: &#29992;&#20110;&#36793;&#32536;&#26222;&#36866;&#35745;&#31639;&#24212;&#29992;&#30340;&#20840;&#38754;&#32972;&#26223;&#25968;&#25454;&#38598;"
&lt;/p&gt;
&lt;p&gt;
MyDigitalFootprint: an extensive context dataset for pervasive computing applications at the edge. (arXiv:2306.15990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15990
&lt;/p&gt;
&lt;p&gt;
"MyDigitalFootprint" &#26159;&#19968;&#20010;&#20197;&#36793;&#32536;&#35745;&#31639;&#20026;&#24212;&#29992;&#22330;&#26223;&#30340;&#20840;&#38754;&#32972;&#26223;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35782;&#21035;&#21644;&#31038;&#20132;&#20851;&#31995;&#24314;&#27169;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#29289;&#29702;&#25509;&#36817;&#20449;&#24687;&#21644;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20114;&#21160;&#12290;&#36890;&#36807;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#22312;&#29992;&#25143;&#30340;&#33258;&#28982;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#21644;&#19978;&#19979;&#25991;&#36866;&#24212;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#26234;&#33021;&#35774;&#22791;&#30340;&#24191;&#27867;&#20256;&#25773;&#20419;&#36827;&#20102;&#20114;&#32852;&#32593;&#22312;&#36793;&#32536;&#30340;&#24555;&#36895;&#25193;&#23637;&#21644;&#28436;&#21464;&#12290;&#20010;&#20154;&#31227;&#21160;&#35774;&#22791;&#19982;&#21608;&#22260;&#30340;&#20854;&#20182;&#26234;&#33021;&#23545;&#35937;&#36827;&#34892;&#20132;&#20114;&#65292;&#26681;&#25454;&#24555;&#36895;&#21464;&#21270;&#30340;&#29992;&#25143;&#19978;&#19979;&#25991;&#35843;&#25972;&#34892;&#20026;&#12290;&#31227;&#21160;&#35774;&#22791;&#26412;&#22320;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#30340;&#33021;&#21147;&#23545;&#20110;&#24555;&#36895;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#24212;&#29992;&#20013;&#38598;&#25104;&#30340;&#21333;&#20010;&#35814;&#32454;&#22788;&#29702;&#36807;&#31243;&#25110;&#29992;&#20110;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#20013;&#38388;&#20214;&#24179;&#21488;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#29615;&#22659;&#20013;&#32570;&#20047;&#32771;&#34385;&#29992;&#25143;&#19978;&#19979;&#25991;&#22797;&#26434;&#24615;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MyDigitalFootprint&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#29289;&#29702;&#25509;&#36817;&#20449;&#24687;&#21644;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20114;&#21160;&#12290;&#35813;&#25968;&#25454;&#38598;&#25903;&#25345;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35782;&#21035;&#21644;&#31038;&#20132;&#20851;&#31995;&#24314;&#27169;&#12290;&#23427;&#22218;&#25324;&#20102;31&#20301;&#24535;&#24895;&#32773;&#29992;&#25143;&#22312;&#20854;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#20004;&#20010;&#26376;&#30340;&#27979;&#37327;&#25968;&#25454;&#65292;&#20801;&#35768;&#26080;&#38480;&#21046;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread diffusion of connected smart devices has contributed to the rapid expansion and evolution of the Internet at its edge. Personal mobile devices interact with other smart objects in their surroundings, adapting behavior based on rapidly changing user context. The ability of mobile devices to process this data locally is crucial for quick adaptation. This can be achieved through a single elaboration process integrated into user applications or a middleware platform for context processing. However, the lack of public datasets considering user context complexity in the mobile environment hinders research progress. We introduce MyDigitalFootprint, a large-scale dataset comprising smartphone sensor data, physical proximity information, and Online Social Networks interactions. This dataset supports multimodal context recognition and social relationship modeling. It spans two months of measurements from 31 volunteer users in their natural environment, allowing for unrestricted be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#21464;&#25442;&#22120;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#34880;&#27969;&#21644;&#31070;&#32463;&#27963;&#21160;&#26469;&#25512;&#26029;&#24403;&#21069;&#34880;&#27969;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#34880;&#28082;&#21160;&#21147;&#23398;&#21709;&#24212;&#31070;&#32463;&#27963;&#21160;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2306.15971</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#27169;&#24577;&#21464;&#25442;&#22120;&#37325;&#24314;&#34880;&#28082;&#21160;&#21147;&#23398;&#21709;&#24212;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer. (arXiv:2306.15971v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#21464;&#25442;&#22120;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#34880;&#27969;&#21644;&#31070;&#32463;&#27963;&#21160;&#26469;&#25512;&#26029;&#24403;&#21069;&#34880;&#27969;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#34880;&#28082;&#21160;&#21147;&#23398;&#21709;&#24212;&#31070;&#32463;&#27963;&#21160;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34880;&#27969;&#19982;&#31070;&#32463;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#34987;&#24191;&#27867;&#35748;&#21487;&#65292;&#22312;fMRI&#30740;&#31350;&#20013;&#65292;&#34880;&#27969;&#32463;&#24120;&#34987;&#29992;&#20316;&#31070;&#32463;&#27963;&#21160;&#30340;&#26367;&#20195;&#25351;&#26631;&#12290;&#22312;&#24494;&#35266;&#27700;&#24179;&#19978;&#65292;&#24050;&#32463;&#26174;&#31034;&#31070;&#32463;&#27963;&#21160;&#20250;&#24433;&#21709;&#38468;&#36817;&#34880;&#31649;&#30340;&#34880;&#27969;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#30452;&#25509;&#22312;&#26126;&#30830;&#30340;&#31070;&#32463;&#20803;&#32676;&#20307;&#27700;&#24179;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20351;&#29992;&#28165;&#37266;&#23567;&#40736;&#30340;&#20307;&#20869;&#35760;&#24405;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21452;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#26681;&#25454;&#21382;&#21490;&#34880;&#27969;&#21644;&#25345;&#32493;&#33258;&#21457;&#31070;&#32463;&#27963;&#21160;&#26469;&#25512;&#26029;&#24403;&#21069;&#30340;&#34880;&#27969;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#32467;&#21512;&#31070;&#32463;&#27963;&#21160;&#26126;&#26174;&#25913;&#21892;&#20102;&#27169;&#22411;&#39044;&#27979;&#34880;&#27969;&#20540;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#34880;&#28082;&#21160;&#21147;&#23398;&#21709;&#24212;&#31070;&#32463;&#27963;&#21160;&#30340;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#24615;&#36136;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between blood flow and neuronal activity is widely recognized, with blood flow frequently serving as a surrogate for neuronal activity in fMRI studies. At the microscopic level, neuronal activity has been shown to influence blood flow in nearby blood vessels. This study introduces the first predictive model that addresses this issue directly at the explicit neuronal population level. Using in vivo recordings in awake mice, we employ a novel spatiotemporal bimodal transformer architecture to infer current blood flow based on both historical blood flow and ongoing spontaneous neuronal activity. Our findings indicate that incorporating neuronal activity significantly enhances the model's ability to predict blood flow values. Through analysis of the model's behavior, we propose hypotheses regarding the largely unexplored nature of the hemodynamic response to neuronal activity.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15969</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#26377;&#24076;&#26395;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#21508;&#31181;PDE&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;PINNs&#26469;&#35299;&#20915;&#22810;&#32500;PDE&#21644;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#35299;&#20989;&#25968;&#23384;&#22312;&#26681;&#26412;&#38480;&#21046;&#12290;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PDE&#19978;&#25152;&#38656;&#30340;&#35757;&#32451;&#28857;&#25968;&#37327;(&#37197;&#28857;)&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24222;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20854;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PINNs&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20998;&#31163;&#30340;PINN (SPINN)&#65292;&#22312;&#22810;&#32500;PDE&#20013;&#25353;&#36724;&#36880;&#20010;&#22788;&#29702;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#20256;&#25773;&#30340;&#25968;&#37327;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;PINNs&#20013;&#30340;&#36880;&#28857;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#26469;&#38477;&#20302;&#35745;&#31639;PDE&#27531;&#24046;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#22312;&#21333;&#20010;&#26222;&#36890;GPU&#19978;&#21487;&#20197;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;(&gt;10^7)&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (&gt;10^7) on a single commodity GPU. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#34892;&#21160;&#21644;&#36712;&#36857;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#24863;&#30693;&#20449;&#24687;&#21644;&#20998;&#23618;&#27169;&#22411;&#26469;&#23398;&#20064;&#21644;&#35268;&#21010;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#34892;&#20026;&#65292;&#24182;&#35299;&#20915;&#22797;&#26434;&#22478;&#24066;&#22330;&#26223;&#19979;&#30340;&#34892;&#39542;&#20219;&#21153;&#21644;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.15968</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#34892;&#21160;&#21644;&#36712;&#36857;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Action and Trajectory Planning for Urban Autonomous Driving with Hierarchical Reinforcement Learning. (arXiv:2306.15968v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#34892;&#21160;&#21644;&#36712;&#36857;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#24863;&#30693;&#20449;&#24687;&#21644;&#20998;&#23618;&#27169;&#22411;&#26469;&#23398;&#20064;&#21644;&#35268;&#21010;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#34892;&#20026;&#65292;&#24182;&#35299;&#20915;&#22797;&#26434;&#22478;&#24066;&#22330;&#26223;&#19979;&#30340;&#34892;&#39542;&#20219;&#21153;&#21644;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#31616;&#21333;&#39550;&#39542;&#22330;&#26223;&#20013;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#35268;&#21010;&#21644;&#20915;&#31574;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#30340;&#22478;&#24066;&#22330;&#26223;&#20013;&#26080;&#27861;&#23398;&#20064;&#20851;&#38190;&#30340;&#39550;&#39542;&#25216;&#33021;&#12290;&#39318;&#20808;&#65292;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#35201;&#27714;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22788;&#29702;&#22810;&#20010;&#39550;&#39542;&#20219;&#21153;&#65292;&#32780;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26080;&#27861;&#32988;&#20219;&#12290;&#20854;&#27425;&#65292;&#22478;&#24066;&#22330;&#26223;&#20013;&#20854;&#20182;&#36710;&#36742;&#30340;&#23384;&#22312;&#23548;&#33268;&#29615;&#22659;&#19981;&#26029;&#21464;&#21270;&#65292;&#36825;&#23545;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35268;&#21010;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#21160;&#21644;&#36712;&#36857;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#34892;&#21160;&#21644;&#36712;&#36857;&#35268;&#21010;&#22120;(atHRL)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28608;&#20809;&#38647;&#36798;&#21644;&#40479;&#30640;&#22270;&#30340;&#24863;&#30693;&#26469;&#24314;&#27169;&#20195;&#29702;&#34892;&#20026;&#12290;&#25152;&#25552;&#20986;&#30340;atHRL&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;DDPG&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;&#24773;&#20917;&#19979;&#23398;&#20064;&#20570;&#20986;&#20851;&#20110;&#20195;&#29702;&#26410;&#26469;&#36712;&#36857;&#30340;&#20915;&#31574;&#65292;&#24182;&#22522;&#20110;&#27492;&#35745;&#31639;&#30446;&#26631;&#36335;&#24452;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has made promising progress in planning and decision-making for Autonomous Vehicles (AVs) in simple driving scenarios. However, existing RL algorithms for AVs fail to learn critical driving skills in complex urban scenarios. First, urban driving scenarios require AVs to handle multiple driving tasks of which conventional RL algorithms are incapable. Second, the presence of other vehicles in urban scenarios results in a dynamically changing environment, which challenges RL algorithms to plan the action and trajectory of the AV. In this work, we propose an action and trajectory planner using Hierarchical Reinforcement Learning (atHRL) method, which models the agent behavior in a hierarchical model by using the perception of the lidar and birdeye view. The proposed atHRL method learns to make decisions about the agent's future trajectory and computes target waypoints under continuous settings based on a hierarchical DDPG algorithm. The waypoints planned by the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24555;&#36895;&#34701;&#21512;Gromov&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#25554;&#20540;&#21644;&#22270;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#32771;&#34385;&#22270;&#32467;&#26500;&#21644;&#20449;&#21495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#37197;&#33410;&#28857;&#20043;&#38388;&#30340;&#26368;&#20248;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;FGW&#27714;&#35299;&#22120;&#26469;&#21152;&#36895;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.15963</link><description>&lt;p&gt;
&#36890;&#36807;&#24555;&#36895;&#34701;&#21512;Gromov&#21270;&#23454;&#29616;&#22270;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Graph Interpolation via Fast Fused-Gromovization. (arXiv:2306.15963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24555;&#36895;&#34701;&#21512;Gromov&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#25554;&#20540;&#21644;&#22270;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#32771;&#34385;&#22270;&#32467;&#26500;&#21644;&#20449;&#21495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#37197;&#33410;&#28857;&#20043;&#38388;&#30340;&#26368;&#20248;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;FGW&#27714;&#35299;&#22120;&#26469;&#21152;&#36895;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#22686;&#24378;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#22312;&#22270;&#32423;&#20998;&#31867;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#29420;&#31435;&#22320;&#22686;&#24378;&#22270;&#20449;&#21495;&#31354;&#38388;&#21644;&#22270;&#32467;&#26500;&#31354;&#38388;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#20849;&#21516;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#21305;&#37197;&#22270;&#20043;&#38388;&#33410;&#28857;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#32771;&#34385;&#22270;&#32467;&#26500;&#21644;&#20449;&#21495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;mixup&#31639;&#27861;&#65292;&#31216;&#20026;FGWMixup&#65292;&#23427;&#21033;&#29992;&#34701;&#21512;Gromov-Wasserstein&#65288;FGW&#65289;&#24230;&#37327;&#31354;&#38388;&#26469;&#35782;&#21035;&#28304;&#22270;&#30340;&#8220;&#20013;&#28857;&#8221;&#12290;&#20026;&#20102;&#25552;&#39640;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25918;&#26494;&#30340;FGW&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#25910;&#25947;&#36895;&#24230;&#20174;O(t^-1)&#21152;&#36895;&#21040;O(t^-2)&#65292;&#25552;&#39640;&#20102;FGWMixup&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data augmentation has proven to be effective in enhancing the generalizability and robustness of graph neural networks (GNNs) for graph-level classifications. However, existing methods mainly focus on augmenting the graph signal space and the graph structure space independently, overlooking their joint interaction. This paper addresses this limitation by formulating the problem as an optimal transport problem that aims to find an optimal strategy for matching nodes between graphs considering the interactions between graph structures and signals. To tackle this problem, we propose a novel graph mixup algorithm dubbed FGWMixup, which leverages the Fused Gromov-Wasserstein (FGW) metric space to identify a "midpoint" of the source graphs. To improve the scalability of our approach, we introduce a relaxed FGW solver that accelerates FGWMixup by enhancing the convergence rate from $\mathcal{O}(t^{-1})$ to $\mathcal{O}(t^{-2})$. Extensive experiments conducted on five datasets, utilizin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15951</link><description>&lt;p&gt;
&#36890;&#36807;&#36339;&#36807;&#38646;&#20803;&#32032;&#38477;&#20302;&#21367;&#31215;&#23618;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#24182;&#34892;&#22788;&#29702;&#22120;&#36827;&#34892;&#21152;&#36895;&#12290;&#20026;&#20102;&#20026;&#20854;&#35774;&#35745;&#36816;&#31639;&#31526;&#65292;&#38656;&#35201;&#19981;&#20165;&#26377;&#20248;&#21270;&#31639;&#27861;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#36824;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#21367;&#31215;&#23618;&#20027;&#35201;&#21253;&#21547;&#19977;&#31181;&#36816;&#31639;&#31526;&#65306;&#21069;&#21521;&#20256;&#25773;&#30340;&#21367;&#31215;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#12290;&#24403;&#25191;&#34892;&#36825;&#20123;&#36816;&#31639;&#26102;&#65292;&#22987;&#32456;&#20250;&#21521;&#24352;&#37327;&#20013;&#28155;&#21152;0&#20803;&#32032;&#65292;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65288;ConvV2, KS-deconv, Sk-dilated&#65289;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#36339;&#36807;&#36825;&#20123;0&#20803;&#32032;&#65306;&#20462;&#21098;&#28388;&#27874;&#22120;&#20197;&#25490;&#38500;&#22635;&#20805;&#30340;0&#20803;&#32032;&#65307;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#20026;&#31264;&#23494;&#24352;&#37327;&#65292;&#36991;&#20813;&#22312;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#20013;&#25554;&#20837;0&#20803;&#32032;&#12290;&#19982;&#26222;&#36890;&#21367;&#31215;&#30456;&#27604;&#65292;&#21453;&#21367;&#31215;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#21152;&#36895;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;C-K-S&#30340;&#39640;&#24615;&#33021;GPU&#23454;&#29616;&#65292;&#24182;&#36890;&#36807;&#19982;PyTorch&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
&lt;/p&gt;</description></item><item><title>Pb-Hash&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21306;b&#20301;&#21704;&#24076;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;B&#20301;&#21704;&#24076;&#20998;&#25104;m&#20010;&#22359;&#26469;&#37325;&#22797;&#20351;&#29992;&#24050;&#26377;&#30340;&#21704;&#24076;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.15944</link><description>&lt;p&gt;
Pb-Hash: &#20998;&#21306;b&#20301;&#21704;&#24076;
&lt;/p&gt;
&lt;p&gt;
Pb-Hash: Partitioned b-bit Hashing. (arXiv:2306.15944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15944
&lt;/p&gt;
&lt;p&gt;
Pb-Hash&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21306;b&#20301;&#21704;&#24076;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;B&#20301;&#21704;&#24076;&#20998;&#25104;m&#20010;&#22359;&#26469;&#37325;&#22797;&#20351;&#29992;&#24050;&#26377;&#30340;&#21704;&#24076;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21704;&#24076;&#31639;&#27861;&#65292;&#21253;&#25324;minwise&#21704;&#24076;&#65288;MinHash&#65289;&#65292;&#19968;&#27425;&#32622;&#25442;&#21704;&#24076;&#65288;OPH&#65289;&#21644;&#19968;&#33268;&#21152;&#26435;&#37319;&#26679;&#65288;CWS&#65289;&#65292;&#29983;&#25104;B&#20301;&#25972;&#25968;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#21521;&#37327;&#30340;k&#20010;&#21704;&#24076;&#65292;&#23384;&#20648;&#31354;&#38388;&#23558;&#26159;B&#215;k&#20301;&#65307;&#24403;&#29992;&#20110;&#22823;&#35268;&#27169;&#23398;&#20064;&#26102;&#65292;&#27169;&#22411;&#22823;&#23567;&#23558;&#26159;2^B&#215;k&#65292;&#36825;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#19968;&#31181;&#26631;&#20934;&#31574;&#30053;&#26159;&#20165;&#20351;&#29992;B&#20301;&#20013;&#30340;&#26368;&#20302;b&#20301;&#65292;&#24182;&#30053;&#24494;&#22686;&#21152;&#21704;&#24076;&#30340;&#25968;&#37327;k&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;B&#20301;&#20998;&#25104;m&#20010;&#22359;&#65292;&#20363;&#22914;b&#215;m=B&#65292;&#26469;&#37325;&#22797;&#20351;&#29992;&#21704;&#24076;&#12290;&#23545;&#24212;&#22320;&#65292;&#27169;&#22411;&#22823;&#23567;&#21464;&#20026;m&#215;2^b&#215;k&#65292;&#36825;&#21487;&#33021;&#27604;&#21407;&#26469;&#30340;2^B&#215;k&#35201;&#23567;&#24471;&#22810;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;&#65292;&#36890;&#36807;&#23558;&#21704;&#24076;&#20540;&#20998;&#25104;m&#20010;&#22359;&#65292;&#20934;&#30830;&#24615;&#20250;&#19979;&#38477;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20351;&#29992;B/m&#20301;&#30340;m&#20010;&#22359;&#23558;&#19981;&#22914;&#30452;&#25509;&#20351;&#29992;B&#20301;&#31934;&#30830;&#12290;&#36825;&#26159;&#30001;&#20110;&#36890;&#36807;&#37325;&#26032;&#20351;&#29992;&#30456;&#21516;&#30340;&#21704;&#24076;&#20540;&#24341;&#36215;&#30340;&#30456;&#20851;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;
&lt;/p&gt;
&lt;p&gt;
Many hashing algorithms including minwise hashing (MinHash), one permutation hashing (OPH), and consistent weighted sampling (CWS) generate integers of $B$ bits. With $k$ hashes for each data vector, the storage would be $B\times k$ bits; and when used for large-scale learning, the model size would be $2^B\times k$, which can be expensive. A standard strategy is to use only the lowest $b$ bits out of the $B$ bits and somewhat increase $k$, the number of hashes. In this study, we propose to re-use the hashes by partitioning the $B$ bits into $m$ chunks, e.g., $b\times m =B$. Correspondingly, the model size becomes $m\times 2^b \times k$, which can be substantially smaller than the original $2^B\times k$.  Our theoretical analysis reveals that by partitioning the hash values into $m$ chunks, the accuracy would drop. In other words, using $m$ chunks of $B/m$ bits would not be as accurate as directly using $B$ bits. This is due to the correlation from re-using the same hash. On the other h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#27010;&#24565;&#22312;&#25163;&#26426;&#32593;&#32476;&#20013;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#21644;Z&#20998;&#25968;&#26469;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;K-means&#31639;&#27861;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24322;&#24120;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#20026;&#25163;&#26426;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#24555;&#19988;&#33258;&#20027;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#22823;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15938</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#27010;&#24565;&#22312;&#25163;&#26426;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Anomaly Detection in Cellular Networks by Learning Concepts in Variational Autoencoders. (arXiv:2306.15938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#27010;&#24565;&#22312;&#25163;&#26426;&#32593;&#32476;&#20013;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#21644;Z&#20998;&#25968;&#26469;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;K-means&#31639;&#27861;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24322;&#24120;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#20026;&#25163;&#26426;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#24555;&#19988;&#33258;&#20027;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#22823;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#25163;&#26426;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAEs)&#23398;&#20064;&#27599;&#20010;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;(KPI)&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#37325;&#26500;&#25439;&#22833;&#21644;Z&#20998;&#25968;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;K-means&#31639;&#27861;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#38468;&#21152;&#20449;&#24687;&#20013;&#24515;&#28857;(c)&#30830;&#20445;&#24322;&#24120;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#29305;&#23450;KPI&#30340;&#28508;&#22312;&#32500;&#24230;&#20013;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#24322;&#24120;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#20026;&#22312;&#25163;&#26426;&#32593;&#32476;&#20013;&#26816;&#27979;&#24322;&#24120;&#25552;&#20379;&#20102;&#26356;&#24555;&#19988;&#33258;&#20027;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#22788;&#29702;&#22823;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenges of detecting anomalies in cellular networks in an interpretable way and proposes a new approach using variational autoencoders (VAEs) that learn interpretable representations of the latent space for each Key Performance Indicator (KPI) in the dataset. This enables the detection of anomalies based on reconstruction loss and Z-scores. We ensure the interpretability of the anomalies via additional information centroids (c) using the K-means algorithm to enhance representation learning. We evaluate the performance of the model by analyzing patterns in the latent dimension for specific KPIs and thereby demonstrate the interpretability and anomalies. The proposed framework offers a faster and autonomous solution for detecting anomalies in cellular networks and showcases the potential of deep learning-based algorithms in handling big data.
&lt;/p&gt;</description></item><item><title>&#22909;&#22855;&#22238;&#25918;&#26159;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#30340;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22909;&#22855;&#24230;&#22522;&#30784;&#30340;&#20248;&#20808;&#20449;&#21495;&#65292;&#23427;&#25552;&#39640;&#20102;&#25506;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;Crafter&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2306.15934</link><description>&lt;p&gt;
&#23545;&#20110;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#36866;&#24212;&#24615;&#30340;&#22909;&#22855;&#22238;&#25918;
&lt;/p&gt;
&lt;p&gt;
Curious Replay for Model-based Adaptation. (arXiv:2306.15934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15934
&lt;/p&gt;
&lt;p&gt;
&#22909;&#22855;&#22238;&#25918;&#26159;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#30340;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22909;&#22855;&#24230;&#22522;&#30784;&#30340;&#20248;&#20808;&#20449;&#21495;&#65292;&#23427;&#25552;&#39640;&#20102;&#25506;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;Crafter&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#24517;&#39035;&#33021;&#22815;&#22312;&#29615;&#22659;&#25913;&#21464;&#26102;&#24555;&#36895;&#36866;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#36825;&#26041;&#38754;&#20570;&#24471;&#19981;&#22909;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#23427;&#20204;&#22914;&#20309;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#35757;&#32451;&#20854;&#19990;&#30028;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22909;&#22855;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#19968;&#31181;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22909;&#22855;&#24230;&#22522;&#30784;&#30340;&#20248;&#20808;&#20449;&#21495;&#12290;&#20351;&#29992;&#22909;&#22855;&#22238;&#25918;&#30340;&#20195;&#29702;&#22312;&#21463;&#21040;&#21160;&#29289;&#34892;&#20026;&#21551;&#21457;&#30340;&#25506;&#32034;&#33539;&#24335;&#21644;Crafter&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#24102;&#26377;&#22909;&#22855;&#22238;&#25918;&#30340;DreamerV3&#22312;Crafter&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;19.4&#30340;&#24179;&#22343;&#20998;&#25968;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#20043;&#21069;DreamerV3&#20351;&#29992;&#22343;&#21248;&#22238;&#25918;&#26102;&#30340;&#26368;&#39640;&#20998;&#25968;14.5&#65292;&#24182;&#19988;&#22312;Deepmind Control Suite&#19978;&#30340;&#24615;&#33021;&#20063;&#30456;&#20284;&#12290;&#22909;&#22855;&#22238;&#25918;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/AutonomousAgentsLab/curiousreplay&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents must be able to adapt quickly as an environment changes. We find that existing model-based reinforcement learning agents are unable to do this well, in part because of how they use past experiences to train their world model. Here, we present Curious Replay -- a form of prioritized experience replay tailored to model-based agents through use of a curiosity-based priority signal. Agents using Curious Replay exhibit improved performance in an exploration paradigm inspired by animal behavior and on the Crafter benchmark. DreamerV3 with Curious Replay surpasses state-of-the-art performance on Crafter, achieving a mean score of 19.4 that substantially improves on the previous high score of 14.5 by DreamerV3 with uniform replay, while also maintaining similar performance on the Deepmind Control Suite. Code for Curious Replay is available at https://github.com/AutonomousAgentsLab/curiousreplay
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#26469;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15933</link><description>&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#32416;&#27491;&#25552;&#31034;&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting. (arXiv:2306.15933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#26469;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#36755;&#20837;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#65288;&#31216;&#20026;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#25324;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#38454;&#27573;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#19968;&#27425;&#24615;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;VCP&#65288;&#39564;&#35777;&#21644;&#32416;&#27491;&#25552;&#31034;&#65289;&#65292;&#20174;&#27169;&#22411;&#29983;&#25104;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#39564;&#35777;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#27491;&#30830;&#24615;&#12290;&#39564;&#35777;&#27493;&#39588;&#30340;&#35266;&#23519;&#32467;&#26524;&#34987;&#36716;&#21270;&#20026;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#65292;&#35813;&#25552;&#31034;&#25351;&#31034;&#27169;&#22411;&#22312;&#37325;&#26032;&#29983;&#25104;&#36755;&#20986;&#26102;&#32771;&#34385;&#24050;&#35782;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#32416;&#27491;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22521;&#35757;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#20351;&#27169;&#22411;&#33021;&#22815;&#34701;&#20837;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#21892;&#36755;&#20986;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advancements in existing models, generating text descriptions from structured data input, known as data-to-text generation, remains a challenging task. In this paper, we propose a novel approach that goes beyond traditional one-shot generation methods by introducing a multi-step process consisting of generation, verification, and correction stages. Our approach, VCP(Verification and Correction Prompting), begins with the model generating an initial output. We then proceed to verify the correctness of different aspects of the generated text. The observations from the verification step are converted into a specialized error-indication prompt, which instructs the model to regenerate the output while considering the identified errors. To enhance the model's correction ability, we have developed a carefully designed training procedure. This procedure enables the model to incorporate feedback from the error-indication prompt, resulting in improved output generation. Throu
&lt;/p&gt;</description></item><item><title>NIPD&#26159;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#38750;&#29420;&#31435;&#19982;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#20154;&#20307;&#26816;&#27979;&#22522;&#20934;&#65292;&#24320;&#28304;&#20102;&#19968;&#20010;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#29289;&#32852;&#32593;&#20154;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.15932</link><description>&lt;p&gt;
NIPD&#65288;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#38750;&#29420;&#31435;&#19982;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#20154;&#20307;&#26816;&#27979;&#22522;&#20934;&#65289;
&lt;/p&gt;
&lt;p&gt;
NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data. (arXiv:2306.15932v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15932
&lt;/p&gt;
&lt;p&gt;
NIPD&#26159;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#38750;&#29420;&#31435;&#19982;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#20154;&#20307;&#26816;&#27979;&#22522;&#20934;&#65292;&#24320;&#28304;&#20102;&#19968;&#20010;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#29289;&#32852;&#32593;&#20154;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#12290;FL&#20351;&#24471;&#29289;&#32852;&#32593;&#23458;&#25143;&#31471;&#21487;&#20197;&#22312;&#38450;&#27490;&#38544;&#31169;&#27844;&#28431;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#33391;&#22909;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#22914;&#26524;&#23558;FL&#19982;&#36793;&#32536;&#35774;&#22791;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#22312;&#36793;&#32536;&#30452;&#25509;&#22788;&#29702;&#35270;&#39057;&#25968;&#25454;&#65292;&#20174;&#32780;&#23558;&#20154;&#21592;&#26816;&#27979;&#37096;&#32626;&#22312;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30456;&#26426;&#30340;&#30828;&#20214;&#21644;&#37096;&#32626;&#22330;&#26223;&#19981;&#21516;&#65292;&#30456;&#26426;&#25152;&#25910;&#38598;&#30340;&#25968;&#25454;&#21576;&#29616;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#30340;&#29305;&#24615;&#65292;FL&#32858;&#21512;&#24471;&#21040;&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#25928;&#26524;&#36739;&#24046;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#29992;&#20110;&#30740;&#31350;&#29289;&#32852;&#32593;&#30456;&#26426;&#19978;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#38382;&#39064;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#19968;&#20010;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#29289;&#32852;&#32593;&#20154;&#20307;&#26816;&#27979;&#65288;NIPD&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#20116;&#20010;&#19981;&#21516;&#30340;&#30456;&#26426;&#25910;&#38598;&#32780;&#26469;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30495;&#27491;&#22522;&#20110;&#35774;&#22791;&#30340;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#20154;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), a privacy-preserving distributed machine learning, has been rapidly applied in wireless communication networks. FL enables Internet of Things (IoT) clients to obtain well-trained models while preventing privacy leakage. Person detection can be deployed on edge devices with limited computing power if combined with FL to process the video data directly at the edge. However, due to the different hardware and deployment scenarios of different cameras, the data collected by the camera present non-independent and identically distributed (non-IID), and the global model derived from FL aggregation is less effective. Meanwhile, existing research lacks public data set for real-world FL object detection, which is not conducive to studying the non-IID problem on IoT cameras. Therefore, we open source a non-IID IoT person detection (NIPD) data set, which is collected from five different cameras. To our knowledge, this is the first true device-based non-IID person detection 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24537;&#30860;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BysGNN&#65289;&#30340;&#20020;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#25152;&#26377;&#32972;&#26223;&#20449;&#24687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#23398;&#20064;&#20852;&#36259;&#28857;&#20043;&#38388;&#30340;&#22810;&#32972;&#26223;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#35775;&#38382;&#37327;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.15927</link><description>&lt;p&gt;
&#20174;&#25152;&#26377;&#32972;&#26223;&#20449;&#24687;&#20013;&#23398;&#20064;&#21160;&#24577;&#22270;&#20197;&#20934;&#30830;&#39044;&#27979;&#20852;&#36259;&#28857;&#30340;&#35775;&#38382;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamic Graphs from All Contextual Information for Accurate Point-of-Interest Visit Forecasting. (arXiv:2306.15927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15927
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24537;&#30860;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BysGNN&#65289;&#30340;&#20020;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#25152;&#26377;&#32972;&#26223;&#20449;&#24687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#23398;&#20064;&#20852;&#36259;&#28857;&#20043;&#38388;&#30340;&#22810;&#32972;&#26223;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#35775;&#38382;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#22320;&#21306;&#39044;&#27979;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#35775;&#38382;&#37327;&#23545;&#20110;&#35268;&#21010;&#21644;&#20915;&#31574;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#28041;&#21450;&#39046;&#22495;&#21253;&#25324;&#22478;&#24066;&#35268;&#21010;&#12289;&#20132;&#36890;&#31649;&#29702;&#12289;&#20844;&#20849;&#21355;&#29983;&#21644;&#31038;&#20250;&#30740;&#31350;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;POIs&#20043;&#38388;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#32972;&#26223;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24537;&#30860;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BysGNN&#65289;&#30340;&#20020;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#23398;&#20064;&#21644;&#25581;&#31034;POIs&#20043;&#38388;&#30340;&#28508;&#22312;&#22810;&#32972;&#26223;&#30456;&#20851;&#24615;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#35775;&#38382;&#37327;&#12290;&#19982;&#20854;&#20182;&#20165;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;BysGNN&#21033;&#29992;&#25152;&#26377;&#32972;&#26223;&#20449;&#24687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#36890;&#36807;&#32467;&#21512;&#25152;&#26377;&#32972;&#26223;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#21495;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#27979;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting the number of visits to Points-of-Interest (POI) in an urban area is critical for planning and decision-making for various application domains, from urban planning and transportation management to public health and social studies. Although this forecasting problem can be formulated as a multivariate time-series forecasting task, the current approaches cannot fully exploit the ever-changing multi-context correlations among POIs. Therefore, we propose Busyness Graph Neural Network (BysGNN), a temporal graph neural network designed to learn and uncover the underlying multi-context correlations between POIs for accurate visit forecasting. Unlike other approaches where only time-series data is used to learn a dynamic graph, BysGNN utilizes all contextual information and time-series data to learn an accurate dynamic graph representation. By incorporating all contextual, temporal, and spatial signals, we observe a significant improvement in our forecasting accuracy over state-of-t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#36807;&#28388;&#20989;&#25968;&#26469;&#29983;&#25104;&#26377;&#38480;&#32422;&#26463;&#25991;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#38656;&#27714;&#29983;&#25104;&#24102;&#26377;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.15926</link><description>&lt;p&gt;
&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#25104;&#20026;&#35799;&#20154;&#65306;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#21644;&#26377;&#38480;&#25991;&#26412;&#29983;&#25104;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio. (arXiv:2306.15926v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15926
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#36807;&#28388;&#20989;&#25968;&#26469;&#29983;&#25104;&#26377;&#38480;&#32422;&#26463;&#25991;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#38656;&#27714;&#29983;&#25104;&#24102;&#26377;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#20851;&#26377;&#38480;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#23545;&#24050;&#34987;&#35789;&#27719;&#12289;&#35821;&#20041;&#25110;&#38899;&#38901;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#30740;&#31350;&#26102;&#38388;&#24456;&#23569;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#21363;&#20351;&#22312;&#26174;&#33879;&#32422;&#26463;&#19979;&#20063;&#33021;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26222;&#36866;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#25991;&#26412;&#21333;&#20803;&#20043;&#21069;&#32452;&#21512;&#24212;&#29992;&#36807;&#28388;&#20989;&#25968;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#65292;&#26469;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#21363;&#25554;&#21363;&#29992;&#30340;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#20462;&#25913;&#12290;&#20026;&#23637;&#31034;&#36825;&#31181;&#25216;&#26415;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#21517;&#20026;&#26377;&#38480;&#25991;&#26412;&#29983;&#25104;&#24037;&#20855;&#65288;CTGS&#65289;&#12290;CTGS&#20801;&#35768;&#29992;&#25143;&#29983;&#25104;&#25110;&#36873;&#25321;&#20855;&#26377;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#30340;&#25991;&#26412;&#65292;&#20363;&#22914;&#31105;&#27490;&#26576;&#20010;&#23383;&#27597;&#65292;&#24378;&#21046;&#29983;&#25104;&#30340;&#21333;&#35789;&#20855;&#26377;&#19968;&#23450;&#30340;&#38899;&#33410;&#25968;&#65292;&#25110;&#24378;&#21046;&#29983;&#25104;&#19982;&#32473;&#23450;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21333;&#35789;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite rapid advancement in the field of Constrained Natural Language Generation, little time has been spent on exploring the potential of language models which have had their vocabularies lexically, semantically, and/or phonetically constrained. We find that most language models generate compelling text even under significant constraints. We present a simple and universally applicable technique for modifying the output of a language model by compositionally applying filter functions to the language models vocabulary before a unit of text is generated. This approach is plug-and-play and requires no modification to the model. To showcase the value of this technique, we present an easy to use AI writing assistant called Constrained Text Generation Studio (CTGS). CTGS allows users to generate or choose from text with any combination of a wide variety of constraints, such as banning a particular letter, forcing the generated words to have a certain number of syllables, and/or forcing the 
&lt;/p&gt;</description></item><item><title>&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#32500;&#24230;&#35781;&#21650;&#65292;&#20294;&#23545;&#20110;&#30001;Hamilton-Jacobi&#26041;&#31243;&#23450;&#20041;&#30340;&#35299;&#31639;&#23376;&#21487;&#20197;&#20811;&#26381;&#32500;&#24230;&#35781;&#21650;&#12290;</title><link>http://arxiv.org/abs/2306.15924</link><description>&lt;p&gt;
&#36816;&#31639;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
The curse of dimensionality in operator learning. (arXiv:2306.15924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15924
&lt;/p&gt;
&lt;p&gt;
&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#32500;&#24230;&#35781;&#21650;&#65292;&#20294;&#23545;&#20110;&#30001;Hamilton-Jacobi&#26041;&#31243;&#23450;&#20041;&#30340;&#35299;&#31639;&#23376;&#21487;&#20197;&#20811;&#26381;&#32500;&#24230;&#35781;&#21650;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#26144;&#23556;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#31639;&#23376;&#65292;&#21487;&#20197;&#29992;&#20110;&#36890;&#36807;&#27169;&#25311;&#21152;&#36895;&#27169;&#22411;&#35780;&#20272;&#65292;&#25110;&#32773;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24341;&#21457;&#20102;&#31639;&#23376;&#23398;&#20064;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#39033;&#36129;&#29486;&#26159;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#33324;&#30340;&#21482;&#30001;&#20854; $C^r$ &#25110; Lipschitz &#27491;&#21017;&#24615;&#29305;&#24449;&#21270;&#30340;&#31639;&#23376;&#31867;&#65292;&#31639;&#23376;&#23398;&#20064;&#36973;&#21463;&#20102;&#32500;&#24230;&#35781;&#21650;&#65292;&#36825;&#37324;&#36890;&#36807;&#26080;&#31351;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#20989;&#25968;&#31354;&#38388;&#30340;&#34920;&#24449;&#26469;&#31934;&#30830;&#23450;&#20041;&#32500;&#24230;&#35781;&#21650;&#12290;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#21253;&#25324; PCA-Net&#12289;DeepONet &#21644; FNO &#22312;&#20869;&#30340;&#22810;&#31181;&#29616;&#26377;&#31070;&#32463;&#31639;&#23376;&#12290;&#26412;&#25991;&#30340;&#31532;&#20108;&#39033;&#36129;&#29486;&#26159;&#35777;&#26126;&#20102;&#23545;&#20110;&#30001;Hamilton-Jacobi&#26041;&#31243;&#23450;&#20041;&#30340;&#35299;&#31639;&#23376;&#65292;&#21487;&#20197;&#20811;&#26381;&#19968;&#33324;&#30340;&#32500;&#24230;&#35781;&#21650;&#65307;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#34920;&#31034;&#26041;&#27861;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operator architectures employ neural networks to approximate operators mapping between Banach spaces of functions; they may be used to accelerate model evaluations via emulation, or to discover models from data. Consequently, the methodology has received increasing attention over recent years, giving rise to the rapidly growing field of operator learning. The first contribution of this paper is to prove that for general classes of operators which are characterized only by their $C^r$- or Lipschitz-regularity, operator learning suffers from a curse of dimensionality, defined precisely here in terms of representations of the infinite-dimensional input and output function spaces. The result is applicable to a wide variety of existing neural operators, including PCA-Net, DeepONet and the FNO. The second contribution of the paper is to prove that the general curse of dimensionality can be overcome for solution operators defined by the Hamilton-Jacobi equation; this is achieved by lev
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25429;&#33719;&#30340;&#20449;&#24687;&#65292;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#27530;&#12289;&#27169;&#31946;&#25110;&#23569;&#25968;&#23376;&#32676;&#20307;&#31034;&#20363;&#19978;&#30340;&#34892;&#20026;&#65292;&#22312;&#27867;&#21270;&#20013;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#20449;&#24687;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.15918</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#25152;&#25429;&#33719;&#30340;&#20449;&#24687;&#65306;&#19982;&#35760;&#24518;&#21644;&#27867;&#21270;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On information captured by neural networks: connections with memorization and generalization. (arXiv:2306.15918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15918
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25429;&#33719;&#30340;&#20449;&#24687;&#65292;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#27530;&#12289;&#27169;&#31946;&#25110;&#23569;&#25968;&#23376;&#32676;&#20307;&#31034;&#20363;&#19978;&#30340;&#34892;&#20026;&#65292;&#22312;&#27867;&#21270;&#20013;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#20449;&#24687;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#24456;&#21463;&#27426;&#36814;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#12289;&#22914;&#20309;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#22815;&#27867;&#21270;&#21040;&#26410;&#35265;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22240;&#20026;&#23398;&#20064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#25152;&#20197;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25429;&#33719;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20174;&#22122;&#22768;&#26631;&#31614;&#30340;&#23384;&#22312;&#19979;&#35266;&#23519;&#23398;&#20064;&#65292;&#24182;&#23548;&#20986;&#20102;&#23558;&#26631;&#31614;&#22122;&#22768;&#20449;&#24687;&#38480;&#21046;&#22312;&#26435;&#37325;&#20013;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20010;&#20307;&#26679;&#26412;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#25552;&#20379;&#30340;&#21807;&#19968;&#20449;&#24687;&#30340;&#27010;&#24565;&#65292;&#20026;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#20856;&#22411;&#12289;&#27169;&#31946;&#25110;&#23646;&#20110;&#23569;&#25968;&#23376;&#32676;&#20307;&#30340;&#31034;&#20363;&#19978;&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#20123;&#21551;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#24179;&#20961;&#30340;&#27867;&#21270;&#24046;&#36317;&#30028;&#38480;&#65292;&#23558;&#31034;&#20363;&#20449;&#24687;&#24615;&#19982;&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#30740;&#31350;&#30693;&#35782;&#25552;&#21462;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25968;&#25454;&#21644;&#26631;&#31614;&#22797;&#26434;&#24615;&#22312;&#27867;&#21270;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the popularity and success of deep learning, there is limited understanding of when, how, and why neural networks generalize to unseen examples. Since learning can be seen as extracting information from data, we formally study information captured by neural networks during training. Specifically, we start with viewing learning in presence of noisy labels from an information-theoretic perspective and derive a learning algorithm that limits label noise information in weights. We then define a notion of unique information that an individual sample provides to the training of a deep network, shedding some light on the behavior of neural networks on examples that are atypical, ambiguous, or belong to underrepresented subpopulations. We relate example informativeness to generalization by deriving nonvacuous generalization gap bounds. Finally, by studying knowledge distillation, we highlight the important role of data and label complexity in generalization. Overall, our findings contr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#38543;&#26426;&#31995;&#25968;&#23725;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20272;&#35745;&#39118;&#38505;&#25110;&#39044;&#27979;&#39118;&#38505;&#26469;&#30830;&#23450;&#30446;&#26631;&#27169;&#22411;&#21644;&#28304;&#27169;&#22411;&#30340;&#22238;&#24402;&#31995;&#25968;&#30340;&#26368;&#20248;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#24471;&#20986;&#20102;&#26368;&#20248;&#26435;&#37325;&#30340;&#26497;&#38480;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15915</link><description>&lt;p&gt;
&#29992;&#38543;&#26426;&#31995;&#25968;&#23725;&#22238;&#24402;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning with Random Coefficient Ridge Regression. (arXiv:2306.15915v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#38543;&#26426;&#31995;&#25968;&#23725;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20272;&#35745;&#39118;&#38505;&#25110;&#39044;&#27979;&#39118;&#38505;&#26469;&#30830;&#23450;&#30446;&#26631;&#27169;&#22411;&#21644;&#28304;&#27169;&#22411;&#30340;&#22238;&#24402;&#31995;&#25968;&#30340;&#26368;&#20248;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#24471;&#20986;&#20102;&#26368;&#20248;&#26435;&#37325;&#30340;&#26497;&#38480;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#65292;&#38543;&#26426;&#31995;&#25968;&#23725;&#22238;&#24402;&#25552;&#20379;&#20102;&#22266;&#23450;&#31995;&#25968;&#22238;&#24402;&#30340;&#19968;&#20010;&#37325;&#35201;&#26367;&#20195;&#26041;&#26696;&#65292;&#24403;&#25928;&#26524;&#34987;&#26399;&#26395;&#20026;&#23567;&#20294;&#19981;&#20026;&#38646;&#26102;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#36801;&#31227;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#21644;&#39044;&#27979;&#38543;&#26426;&#31995;&#25968;&#23725;&#22238;&#24402;&#65292;&#22312;&#30446;&#26631;&#27169;&#22411;&#30340;&#35266;&#27979;&#20540;&#20043;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#26469;&#33258;&#19981;&#21516;&#20294;&#21487;&#33021;&#30456;&#20851;&#30340;&#22238;&#24402;&#27169;&#22411;&#30340;&#28304;&#26679;&#26412;&#12290;&#28304;&#27169;&#22411;&#23545;&#30446;&#26631;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#21487;&#20197;&#36890;&#36807;&#22238;&#24402;&#31995;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#37327;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#30446;&#26631;&#27169;&#22411;&#22238;&#24402;&#31995;&#25968;&#30340;&#20272;&#35745;&#22120;&#65292;&#23427;&#20204;&#26159;&#30446;&#26631;&#27169;&#22411;&#21644;&#28304;&#27169;&#22411;&#23725;&#20272;&#35745;&#30340;&#21152;&#26435;&#21644;&#65292;&#20854;&#20013;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#20272;&#35745;&#39118;&#38505;&#25110;&#39044;&#27979;&#39118;&#38505;&#26469;&#30830;&#23450;&#12290;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#65292;&#22312;$p/n \rightarrow \gamma$&#30340;&#26465;&#20214;&#19979;&#65292;&#24471;&#20986;&#20102;&#26368;&#20248;&#26435;&#37325;&#30340;&#26497;&#38480;&#20540;&#65292;&#20854;&#20013;$p$&#26159;...
&lt;/p&gt;
&lt;p&gt;
Ridge regression with random coefficients provides an important alternative to fixed coefficients regression in high dimensional setting when the effects are expected to be small but not zeros. This paper considers estimation and prediction of random coefficient ridge regression in the setting of transfer learning, where in addition to observations from the target model, source samples from different but possibly related regression models are available. The informativeness of the source model to the target model can be quantified by the correlation between the regression coefficients. This paper proposes two estimators of regression coefficients of the target model as the weighted sum of the ridge estimates of both target and source models, where the weights can be determined by minimizing the empirical estimation risk or prediction risk. Using random matrix theory, the limiting values of the optimal weights are derived under the setting when $p/n \rightarrow \gamma$, where $p$ is the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36890;&#36947;&#21160;&#20316;&#23884;&#20837;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#31283;&#20581;&#31574;&#30053;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#22312;2D&#36855;&#23467;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30005;&#23376;&#21830;&#21153;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.15913</link><description>&lt;p&gt;
DCT: &#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#24378;&#21270;&#23398;&#20064;&#30340;&#21452;&#36890;&#36947;&#21160;&#20316;&#23884;&#20837;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DCT: Dual Channel Training of Action Embeddings for Reinforcement Learning with Large Discrete Action Spaces. (arXiv:2306.15913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36890;&#36947;&#21160;&#20316;&#23884;&#20837;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#31283;&#20581;&#31574;&#30053;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#22312;2D&#36855;&#23467;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30005;&#23376;&#21830;&#21153;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#20020;&#32500;&#24230;&#28798;&#38590;&#30340;&#22024;&#26434;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#31283;&#20581;&#31574;&#30053;&#24182;&#24191;&#20041;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#26159;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#21516;&#26102;&#23454;&#29616;&#23545;&#21407;&#22987;&#21160;&#20316;&#30340;&#37325;&#26500;&#20197;&#21450;&#23545;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#36827;&#34892;&#21160;&#20316;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#21452;&#36890;&#36947;&#25439;&#22833;&#26469;&#24179;&#34913;&#21160;&#20316;&#37325;&#26500;&#21644;&#29366;&#24577;&#39044;&#27979;&#31934;&#24230;&#12290;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#35299;&#30721;&#22120;&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#19968;&#20010;&#20855;&#26377;4000&#22810;&#20010;&#31163;&#25955;&#22122;&#22768;&#21160;&#20316;&#30340;2D&#36855;&#23467;&#29615;&#22659;&#21644;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30005;&#23376;&#21830;&#21153;&#20132;&#26131;&#25968;&#25454;&#30340;&#20135;&#21697;&#25512;&#33616;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20004;&#20010;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn robust policies while generalizing over large discrete action spaces is an open challenge for intelligent systems, especially in noisy environments that face the curse of dimensionality. In this paper, we present a novel framework to efficiently learn action embeddings that simultaneously allow us to reconstruct the original action as well as to predict the expected future state. We describe an encoder-decoder architecture for action embeddings with a dual channel loss that balances between action reconstruction and state prediction accuracy. We use the trained decoder in conjunction with a standard reinforcement learning algorithm that produces actions in the embedding space. Our architecture is able to outperform two competitive baselines in two diverse environments: a 2D maze environment with more than 4000 discrete noisy actions, and a product recommendation task that uses real-world e-commerce transaction data. Empirical results show that the model results in 
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.15907</link><description>&lt;p&gt;
&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#27700;&#20301;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Models for Water Stage Predictions in South Florida. (arXiv:2306.15907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21644;&#39044;&#27979;&#27827;&#27969;&#31995;&#32479;&#30340;&#27700;&#20301;&#23545;&#20110;&#27946;&#27700;&#35686;&#25253;&#12289;&#27700;&#21147;&#25805;&#20316;&#21644;&#27946;&#27700;&#20943;&#36731;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;HEC-RAS&#12289;MIKE&#21644;SWMM&#31561;&#24037;&#20855;&#24314;&#31435;&#35814;&#32454;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27700;&#25991;&#21644;&#27700;&#21147;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#25972;&#20010;&#27969;&#22495;&#65292;&#20174;&#32780;&#39044;&#27979;&#31995;&#32479;&#20013;&#20219;&#24847;&#28857;&#30340;&#27700;&#20301;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#35745;&#31639;&#37327;&#22823;&#65292;&#23588;&#20854;&#23545;&#20110;&#22823;&#27969;&#22495;&#21644;&#38271;&#26102;&#38388;&#27169;&#25311;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20960;&#20010;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#27700;&#20301;&#12290;&#26412;&#25991;&#20197;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#30340;&#19979;&#28216;&#27700;&#20301;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#26469;&#33258;&#21335;&#20315;&#32599;&#37324;&#36798;&#27700;&#31649;&#29702;&#21306;&#65288;SFWMD&#65289;&#30340;DBHYDRO&#25968;&#25454;&#24211;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2010&#24180;1&#26376;1&#26085;&#33267;2020&#24180;12&#26376;31&#26085;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DL&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating and predicting water levels in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. In the engineering field, tools such as HEC-RAS, MIKE, and SWMM are used to build detailed physics-based hydrological and hydraulic computational models to simulate the entire watershed, thereby predicting the water stage at any point in the system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. The downstream stage of the Miami River in South Florida is chosen as a case study for this paper. The dataset is from January 1, 2010, to December 31, 2020, downloaded from the DBHYDRO database of the South Florida Water Management District (SFWMD). Extensive experiments show that the performance of the DL models is comparable to that of the physics-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#35757;&#32451;&#20013;&#32500;&#24230;&#26080;&#20851;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#28151;&#21512;&#26041;&#27861;&#65288;DINS&#65289;&#65292;&#36890;&#36807;&#23545;&#37319;&#26679;&#21306;&#22495;&#30340;&#26032;&#35270;&#35282;&#36827;&#34892;&#37325;&#26032;&#23457;&#35270;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DINS&#20248;&#20110;&#20854;&#20182;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15905</link><description>&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#20013;&#32500;&#24230;&#26080;&#20851;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dimension Independent Mixup for Hard Negative Sample in Collaborative Filtering. (arXiv:2306.15905v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#35757;&#32451;&#20013;&#32500;&#24230;&#26080;&#20851;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#28151;&#21512;&#26041;&#27861;&#65288;DINS&#65289;&#65292;&#36890;&#36807;&#23545;&#37319;&#26679;&#21306;&#22495;&#30340;&#26032;&#35270;&#35282;&#36827;&#34892;&#37325;&#26032;&#23457;&#35270;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DINS&#20248;&#20110;&#20854;&#20182;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22522;&#20110;&#36807;&#21435;&#30340;&#20114;&#21160;&#39044;&#27979;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#36127;&#37319;&#26679;&#22312;&#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#35757;&#32451;&#22522;&#20110;CF&#30340;&#27169;&#22411;&#26102;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21306;&#22495;&#30340;&#26032;&#35270;&#35282;&#26469;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#30446;&#21069;&#30340;&#37319;&#26679;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#37319;&#26679;&#25110;&#32447;&#37319;&#26679;&#19978;&#65292;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#26377;&#30456;&#24403;&#22823;&#19968;&#37096;&#20998;&#22256;&#38590;&#37319;&#26679;&#21306;&#22495;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#26080;&#20851;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#28151;&#21512;&#26041;&#27861;&#65288;DINS&#65289;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#35757;&#32451;&#22522;&#20110;CF&#30340;&#27169;&#22411;&#30340;&#21306;&#22495;&#37319;&#26679;&#26041;&#27861;&#12290;DINS&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#22256;&#38590;&#36793;&#30028;&#23450;&#20041;&#12289;&#32500;&#24230;&#26080;&#20851;&#28151;&#21512;&#21644;&#22810;&#36339;&#27744;&#21270;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DINS&#20248;&#20110;&#20854;&#20182;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is a widely employed technique that predicts user preferences based on past interactions. Negative sampling plays a vital role in training CF-based models with implicit feedback. In this paper, we propose a novel perspective based on the sampling area to revisit existing sampling methods. We point out that current sampling methods mainly focus on Point-wise or Line-wise sampling, lacking flexibility and leaving a significant portion of the hard sampling area un-explored. To address this limitation, we propose Dimension Independent Mixup for Hard Negative Sampling (DINS), which is the first Area-wise sampling method for training CF-based models. DINS comprises three modules: Hard Boundary Definition, Dimension Independent Mixup, and Multi-hop Pooling. Experiments with real-world datasets on both matrix factorization and graph-based models demonstrate that DINS outperforms other negative sampling methods, establishing its effectiveness and superiority. Our wo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#20010;&#21035;&#21644;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;IS-GIB&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22495;&#22806;&#22270;&#20687;&#36890;&#29992;&#21270;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20002;&#24323;&#34394;&#20551;&#29305;&#24449;&#21644;&#21033;&#29992;&#32467;&#26500;&#20851;&#32852;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15902</link><description>&lt;p&gt;
&#20010;&#21035;&#21644;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#23545;&#20110;&#22495;&#22806;&#36890;&#29992;&#21270;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Individual and Structural Graph Information Bottlenecks for Out-of-Distribution Generalization. (arXiv:2306.15902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15902
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#20010;&#21035;&#21644;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;IS-GIB&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22495;&#22806;&#22270;&#20687;&#36890;&#29992;&#21270;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20002;&#24323;&#34394;&#20551;&#29305;&#24449;&#21644;&#21033;&#29992;&#32467;&#26500;&#20851;&#32852;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#22806;&#22270;&#20687;&#36890;&#29992;&#21270;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20002;&#24323;&#19982;&#26631;&#31614;&#26080;&#20851;&#30340;&#36755;&#20837;&#20013;&#30340;&#34394;&#20551;&#25110;&#22122;&#22768;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20027;&#35201;&#36827;&#34892;&#23454;&#20363;&#32423;&#21035;&#30340;&#31867;&#19981;&#21464;&#22270;&#23398;&#20064;&#65292;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#22270;&#23454;&#20363;&#20043;&#38388;&#30340;&#32467;&#26500;&#21270;&#31867;&#21035;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#31216;&#20026;&#20010;&#21035;&#21644;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;IS-GIB&#65289;&#12290;&#20026;&#20102;&#28040;&#38500;&#30001;&#20998;&#24067;&#20559;&#31227;&#24341;&#36215;&#30340;&#31867;&#21035;&#34394;&#20551;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20010;&#21035;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;I-GIB&#65289;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#22270;&#19982;&#20854;&#23884;&#20837;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20002;&#24323;&#19981;&#30456;&#20851;&#20449;&#24687;&#12290;&#20026;&#20102;&#21033;&#29992;&#32467;&#26500;&#20869;&#37096;&#21644;&#36328;&#22495;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;S-GIB&#65289;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#19968;&#25209;&#20855;&#26377;&#22810;&#20010;&#22495;&#30340;&#22270;&#65292;S-GIB&#39318;&#20808;&#35745;&#31639;&#25104;&#23545;&#30340;&#36755;&#20837;-&#36755;&#20837;&#12289;&#23884;&#20837;-&#23884;&#20837;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) graph generalization are critical for many real-world applications. Existing methods neglect to discard spurious or noisy features of inputs, which are irrelevant to the label. Besides, they mainly conduct instance-level class-invariant graph learning and fail to utilize the structural class relationships between graph instances. In this work, we endeavor to address these issues in a unified framework, dubbed Individual and Structural Graph Information Bottlenecks (IS-GIB). To remove class spurious feature caused by distribution shifts, we propose Individual Graph Information Bottleneck (I-GIB) which discards irrelevant information by minimizing the mutual information between the input graph and its embeddings. To leverage the structural intra- and inter-domain correlations, we propose Structural Graph Information Bottleneck (S-GIB). Specifically for a batch of graphs with multiple domains, S-GIB first computes the pair-wise input-input, embedding-embedding, a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15895</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#65306;&#22810;&#26679;&#24615;&#21644;&#20559;&#24046;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#32487;&#25215;&#20102;LLM&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#23646;&#24615;&#30340;&#25552;&#31034;(&#20363;&#22914;&#25351;&#23450;&#38271;&#24230;&#21644;&#39118;&#26684;&#31561;&#23646;&#24615;)&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#65292;&#36825;&#26377;&#28508;&#21147;&#20135;&#29983;&#22810;&#26679;&#21644;&#24402;&#22240;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20855;&#26377;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23646;&#24615;&#21270;&#25552;&#31034;&#22312;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#39033;&#21253;&#25324;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#31561;&#20851;&#38190;&#26041;&#38754;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#39318;&#20808;&#65292;&#31995;&#32479;&#24615;&#20559;&#24046;&#22312;&#29983;&#25104;&#25968;&#25454;&#20013;&#23384;&#22312;&#65307;&#20854;&#27425;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65307;&#26368;&#21518;&#65292;&#36827;&#34892;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65288;APCONs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#26102;&#21464;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#12289;&#27744;&#21270;&#21644;&#28608;&#27963;&#25805;&#20316;&#26469;&#25429;&#25417;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#30340;&#25193;&#25955;&#34892;&#20026;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15891</link><description>&lt;p&gt;
&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#25429;&#25417;&#22810;&#23610;&#24230;&#32447;&#24615;&#36755;&#36816;&#26041;&#31243;&#30340;&#25193;&#25955;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Asymptotic-Preserving Convolutional DeepONets Capture the Diffusive Behavior of the Multiscale Linear Transport Equations. (arXiv:2306.15891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65288;APCONs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#26102;&#21464;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#12289;&#27744;&#21270;&#21644;&#28608;&#27963;&#25805;&#20316;&#26469;&#25429;&#25417;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#30340;&#25193;&#25955;&#34892;&#20026;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65288;APCONs&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#23610;&#24230;&#26102;&#21464;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#20462;&#25913;&#36807;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#22522;&#26412;&#29289;&#29702;&#32422;&#26463;DeepONets&#21487;&#33021;&#22312;&#20445;&#25345;&#26399;&#26395;&#30340;&#23439;&#35266;&#34892;&#20026;&#19978;&#34920;&#29616;&#20986;&#19981;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20351;&#29992;&#28176;&#36817;&#20445;&#25345;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#21463;&#25193;&#25955;&#26041;&#31243;&#20013;&#30340;&#28909;&#26680;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65292;&#23427;&#22312;&#27599;&#20010;&#28388;&#27874;&#22120;&#23618;&#20013;&#37319;&#29992;&#22810;&#20010;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#32780;&#19981;&#26159;&#20840;&#23616;&#28909;&#26680;&#65292;&#24182;&#32467;&#21512;&#27744;&#21270;&#21644;&#28608;&#27963;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;APCON&#26041;&#27861;&#30340;&#21442;&#25968;&#25968;&#37327;&#19982;&#32593;&#26684;&#22823;&#23567;&#26080;&#20851;&#65292;&#24182;&#33021;&#22815;&#25429;&#25417;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#30340;&#25193;&#25955;&#34892;&#20026;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce two types of novel Asymptotic-Preserving Convolutional Deep Operator Networks (APCONs) designed to address the multiscale time-dependent linear transport problem. We observe that the vanilla physics-informed DeepONets with modified MLP may exhibit instability in maintaining the desired limiting macroscopic behavior. Therefore, this necessitates the utilization of an asymptotic-preserving loss function. Drawing inspiration from the heat kernel in the diffusion equation, we propose a new architecture called Convolutional Deep Operator Networks, which employ multiple local convolution operations instead of a global heat kernel, along with pooling and activation operations in each filter layer. Our APCON methods possess a parameter count that is independent of the grid size and are capable of capturing the diffusive behavior of the linear transport problem. Finally, we validate the effectiveness of our methods through several numerical examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21453;&#24212;&#21644;&#36870;&#21512;&#25104;&#39044;&#27979;&#20013;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#36890;&#36807;&#35843;&#26597;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#35774;&#35745;&#26426;&#21046;&#12289;&#20248;&#21183;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#21516;&#26102;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#21644;&#38382;&#39064;&#26412;&#36523;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.15890</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21453;&#24212;&#21644;&#36870;&#21512;&#25104;&#39044;&#27979;&#20013;&#30340;&#32479;&#19968;&#35270;&#35282;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Unified View of Deep Learning for Reaction and Retrosynthesis Prediction: Current Status and Future Challenges. (arXiv:2306.15890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21453;&#24212;&#21644;&#36870;&#21512;&#25104;&#39044;&#27979;&#20013;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#36890;&#36807;&#35843;&#26597;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#35774;&#35745;&#26426;&#21046;&#12289;&#20248;&#21183;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#21516;&#26102;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#21644;&#38382;&#39064;&#26412;&#36523;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#24212;&#21644;&#36870;&#21512;&#25104;&#39044;&#27979;&#26159;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#30340;&#20851;&#27880;&#12290;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#21021;&#27493;&#30340;&#25104;&#21151;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21453;&#24212;&#21644;&#36870;&#21512;&#25104;&#39044;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#35774;&#35745;&#26426;&#21046;&#12289;&#20248;&#21183;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#38382;&#39064;&#26412;&#36523;&#30340;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#21033;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#20840;&#38754;&#21644;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#26088;&#22312;&#25552;&#20379;&#21453;&#24212;&#21644;&#36870;&#21512;&#25104;&#39044;&#27979;&#30340;&#32479;&#19968;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reaction and retrosynthesis prediction are fundamental tasks in computational chemistry that have recently garnered attention from both the machine learning and drug discovery communities. Various deep learning approaches have been proposed to tackle these problems, and some have achieved initial success. In this survey, we conduct a comprehensive investigation of advanced deep learning-based models for reaction and retrosynthesis prediction. We summarize the design mechanisms, strengths, and weaknesses of state-of-the-art approaches. Then, we discuss the limitations of current solutions and open challenges in the problem itself. Finally, we present promising directions to facilitate future research. To our knowledge, this paper is the first comprehensive and systematic survey that seeks to provide a unified understanding of reaction and retrosynthesis prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26412;&#22320;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#29992;&#25143;&#20043;&#38388;&#30340;&#24433;&#21709;&#27010;&#29575;&#29983;&#25104;&#22810;&#20010;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26102;&#24207;&#27880;&#24847;&#26426;&#21046;&#21306;&#20998;&#19981;&#21516;&#26102;&#21051;&#39044;&#27979;&#28304;&#30340;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#26102;&#21464;&#24863;&#26579;&#24773;&#26223;&#19979;&#29992;&#25143;&#20132;&#20114;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#23545;&#26032;&#22330;&#26223;&#30340;&#26816;&#27979;&#20063;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15886</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#34920;&#31034;&#30340;&#26102;&#24207;&#20851;&#27880;&#28304;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Sequential Attention Source Identification Based on Feature Representation. (arXiv:2306.15886v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26412;&#22320;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#29992;&#25143;&#20043;&#38388;&#30340;&#24433;&#21709;&#27010;&#29575;&#29983;&#25104;&#22810;&#20010;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26102;&#24207;&#27880;&#24847;&#26426;&#21046;&#21306;&#20998;&#19981;&#21516;&#26102;&#21051;&#39044;&#27979;&#28304;&#30340;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#26102;&#21464;&#24863;&#26579;&#24773;&#26223;&#19979;&#29992;&#25143;&#20132;&#20114;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#23545;&#26032;&#22330;&#26223;&#30340;&#26816;&#27979;&#20063;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24555;&#29031;&#35266;&#23519;&#30340;&#28304;&#23450;&#20301;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20854;&#26131;&#24471;&#21644;&#20302;&#25104;&#26412;&#30340;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#21450;&#26102;&#35299;&#20915;&#26102;&#21464;&#24863;&#26579;&#24773;&#26223;&#19979;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24322;&#36136;&#20132;&#20114;&#24773;&#26223;&#20013;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26412;&#22320;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#26102;&#24207;&#22270;&#27880;&#24847;&#21147;&#30340;&#28304;&#35782;&#21035;&#65288;TGASI&#65289;&#65292;&#22522;&#20110;&#24402;&#32435;&#23398;&#20064;&#24605;&#24819;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32534;&#30721;&#22120;&#36890;&#36807;&#20272;&#35745;&#20004;&#20010;&#29992;&#25143;&#20043;&#38388;&#30340;&#24433;&#21709;&#27010;&#29575;&#65292;&#38598;&#20013;&#29983;&#25104;&#22810;&#20010;&#29305;&#24449;&#65292;&#35299;&#30721;&#22120;&#36890;&#36807;&#35774;&#35745;&#30340;&#26102;&#24207;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#19981;&#21516;&#26102;&#21051;&#21306;&#20998;&#39044;&#27979;&#28304;&#30340;&#37325;&#35201;&#24615;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;&#24402;&#32435;&#23398;&#20064;&#24605;&#24819;&#30830;&#20445;&#20102;TGASI&#22312;&#19981;&#30693;&#36947;&#20854;&#20182;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33021;&#22312;&#26032;&#22330;&#26223;&#20013;&#26816;&#27979;&#28304;&#65292;&#35777;&#26126;&#20102;TGASI&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TGASI&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Snapshot observation based source localization has been widely studied due to its accessibility and low cost. However, the interaction of users in existing methods does not be addressed in time-varying infection scenarios. So these methods have a decreased accuracy in heterogeneous interaction scenarios. To solve this critical issue, this paper proposes a sequence-to-sequence based localization framework called Temporal-sequence based Graph Attention Source Identification (TGASI) based on an inductive learning idea. More specifically, the encoder focuses on generating multiple features by estimating the influence probability between two users, and the decoder distinguishes the importance of prediction sources in different timestamps by a designed temporal attention mechanism. It's worth mentioning that the inductive learning idea ensures that TGASI can detect the sources in new scenarios without knowing other prior knowledge, which proves the scalability of TGASI. Comprehensive experim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22359;&#29366;&#29305;&#24449;&#20132;&#20114; (BFI) &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20132;&#20114;&#36807;&#31243;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#20197;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#36127;&#25285;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BFI&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#25509;&#36817;&#26631;&#20934;DCNv2&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#21442;&#25968;&#25968;&#37327;&#65292;&#20026;&#39640;&#25928;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.15881</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22359;&#29366;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Blockwise Feature Interaction in Recommendation Systems. (arXiv:2306.15881v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22359;&#29366;&#29305;&#24449;&#20132;&#20114; (BFI) &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20132;&#20114;&#36807;&#31243;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#20197;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#36127;&#25285;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BFI&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#25509;&#36817;&#26631;&#20934;DCNv2&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#21442;&#25968;&#25968;&#37327;&#65292;&#20026;&#39640;&#25928;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#20132;&#20114;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#25429;&#25417;&#20102;&#29992;&#25143;&#20559;&#22909;&#21644;&#29289;&#21697;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#28145;&#24230;&#21644;&#20132;&#21449;&#32593;&#32476; DCNv2&#65289;&#21487;&#33021;&#30001;&#20110;&#20854;&#36328;&#23618;&#25805;&#20316;&#32780;&#38754;&#20020;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22359;&#29366;&#29305;&#24449;&#20132;&#20114; (BFI)&#65292;&#20197;&#24110;&#21161;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#29305;&#24449;&#20132;&#20114;&#36807;&#31243;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22235;&#20010;&#21464;&#20307;&#65288;&#20998;&#21035;&#20026; P&#12289;Q&#12289;T&#12289;S&#65289;&#30340; BFI&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19982;&#26631;&#20934; DCNv2 &#30456;&#27604;&#26102;&#33021;&#22815;&#23454;&#29616;&#25509;&#36817;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#21442;&#25968;&#25968;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#39640;&#25928;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature interactions can play a crucial role in recommendation systems as they capture complex relationships between user preferences and item characteristics. Existing methods such as Deep &amp; Cross Network (DCNv2) may suffer from high computational requirements due to their cross-layer operations. In this paper, we propose a novel approach called blockwise feature interaction (BFI) to help alleviate this issue. By partitioning the feature interaction process into smaller blocks, we can significantly reduce both the memory footprint and the computational burden. Four variants (denoted by P, Q, T, S, respectively) of BFI have been developed and empirically compared. Our experimental results demonstrate that the proposed algorithms achieves close accuracy compared to the standard DCNv2, while greatly reducing the computational overhead and the number of parameters. This paper contributes to the development of efficient recommendation systems by providing a practical solution for improving
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#36716;&#25442;&#29983;&#25104;&#26679;&#26412;&#29305;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#32469;&#36807;&#20102;&#28145;&#24230;&#35821;&#38899;&#20998;&#31867;&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#19988;&#19981;&#20250;&#24341;&#20837;&#39069;&#22806;&#30340;&#21487;&#21548;&#22122;&#38899;&#12290;</title><link>http://arxiv.org/abs/2306.15875</link><description>&lt;p&gt;
&#36890;&#36807;&#22768;&#38899;&#36716;&#25442;&#23545;&#28145;&#24230;&#35821;&#38899;&#20998;&#31867;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#65292;&#30495;&#23454;&#19982;&#34394;&#20551;&#20043;&#38388;&#30340;&#36739;&#37327;
&lt;/p&gt;
&lt;p&gt;
Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion. (arXiv:2306.15875v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#36716;&#25442;&#29983;&#25104;&#26679;&#26412;&#29305;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#32469;&#36807;&#20102;&#28145;&#24230;&#35821;&#38899;&#20998;&#31867;&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#19988;&#19981;&#20250;&#24341;&#20837;&#39069;&#22806;&#30340;&#21487;&#21548;&#22122;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#35821;&#38899;&#20998;&#31867;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25512;&#21160;&#20102;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#21518;&#38376;&#25915;&#20987;&#23545;&#20854;&#36896;&#25104;&#20102;&#26032;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21487;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#24179;&#21488;&#19978;&#65292;&#25915;&#20987;&#32773;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#21487;&#20197;&#28608;&#27963;&#21518;&#38376;&#12290;&#29616;&#26377;&#35821;&#38899;&#21518;&#38376;&#25915;&#20987;&#20013;&#30340;&#22823;&#22810;&#25968;&#35302;&#21457;&#22120;&#37117;&#26159;&#19982;&#26679;&#26412;&#26080;&#20851;&#30340;&#65292;&#21363;&#20351;&#36825;&#20123;&#35302;&#21457;&#22120;&#35774;&#35745;&#24471;&#19981;&#21487;&#23519;&#35273;&#65292;&#23427;&#20204;&#20173;&#28982;&#21487;&#20197;&#21548;&#21040;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#22768;&#38899;&#36716;&#25442;&#30340;&#26679;&#26412;&#29305;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22768;&#38899;&#36716;&#25442;&#27169;&#22411;&#29983;&#25104;&#35302;&#21457;&#22120;&#65292;&#30830;&#20445;&#27602;&#21270;&#26679;&#26412;&#19981;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#30340;&#21487;&#21548;&#22122;&#38899;&#12290;&#20004;&#20010;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#28608;&#27963;&#25552;&#20986;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#20855;&#20307;&#22330;&#26223;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#23545;&#24494;&#35843;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep speech classification has achieved tremendous success and greatly promoted the emergence of many real-world applications. However, backdoor attacks present a new security threat to it, particularly with untrustworthy third-party platforms, as pre-defined triggers set by the attacker can activate the backdoor. Most of the triggers in existing speech backdoor attacks are sample-agnostic, and even if the triggers are designed to be unnoticeable, they can still be audible. This work explores a backdoor attack that utilizes sample-specific triggers based on voice conversion. Specifically, we adopt a pre-trained voice conversion model to generate the trigger, ensuring that the poisoned samples does not introduce any additional audible noise. Extensive experiments on two speech classification tasks demonstrate the effectiveness of our attack. Furthermore, we analyzed the specific scenarios that activated the proposed backdoor and verified its resistance against fine-tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#38543;&#26426;&#24494;&#31215;&#20998;&#12289;&#21464;&#20998;&#36125;&#21494;&#26031;&#29702;&#35770;&#21644;&#31232;&#30095;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#20934;&#30830;&#22320;&#21457;&#29616;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;SPDEs&#65289;&#12290;&#20316;&#32773;&#24212;&#29992;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#38543;&#26426;&#28909;&#26041;&#31243;&#12289;&#38543;&#26426;Allen-Cahn&#26041;&#31243;&#21644;&#38543;&#26426;Nagumo&#26041;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15873</link><description>&lt;p&gt;
&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#21457;&#29616;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Discovering stochastic partial differential equations from limited data using variational Bayes inference. (arXiv:2306.15873v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#38543;&#26426;&#24494;&#31215;&#20998;&#12289;&#21464;&#20998;&#36125;&#21494;&#26031;&#29702;&#35770;&#21644;&#31232;&#30095;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#20934;&#30830;&#22320;&#21457;&#29616;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;SPDEs&#65289;&#12290;&#20316;&#32773;&#24212;&#29992;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#38543;&#26426;&#28909;&#26041;&#31243;&#12289;&#38543;&#26426;Allen-Cahn&#26041;&#31243;&#21644;&#38543;&#26426;Nagumo&#26041;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;SPDEs&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#38543;&#26426;&#24494;&#31215;&#20998;&#12289;&#21464;&#20998;&#36125;&#21494;&#26031;&#29702;&#35770;&#21644;&#31232;&#30095;&#23398;&#20064;&#30340;&#27010;&#24565;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;Kramers-Moyal&#23637;&#24320;&#24418;&#24335;&#65292;&#20197;&#21709;&#24212;&#29366;&#24577;&#26469;&#34920;&#31034;SPDE&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#39033;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#31232;&#30095;&#23398;&#20064;&#25216;&#26415;&#30340;Spike-and-Slab&#20808;&#39564;&#26469;&#39640;&#25928;&#20934;&#30830;&#22320;&#21457;&#29616;&#28508;&#22312;&#30340;SPDEs&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24050;&#32463;&#24212;&#29992;&#20110;&#19977;&#20010;&#20856;&#22411;&#30340;SPDEs&#65292;&#20998;&#21035;&#26159;&#38543;&#26426;&#28909;&#26041;&#31243;&#12289;&#38543;&#26426;Allen-Cahn&#26041;&#31243;&#21644;&#38543;&#26426;Nagumo&#26041;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#26377;&#38480;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;SPDEs&#12290;&#36825;&#26159;&#39318;&#27425;&#23581;&#35797;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;SPDEs&#65292;&#23545;&#20110;&#21508;&#31181;&#31185;&#23398;&#24212;&#29992;&#65292;&#22914;&#27668;&#20505;&#24314;&#27169;&#12289;&#37329;&#34701;&#39044;&#27979;&#21644;&#21270;&#23398;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for discovering Stochastic Partial Differential Equations (SPDEs) from data. The proposed approach combines the concepts of stochastic calculus, variational Bayes theory, and sparse learning. We propose the extended Kramers-Moyal expansion to express the drift and diffusion terms of an SPDE in terms of state responses and use Spike-and-Slab priors with sparse learning techniques to efficiently and accurately discover the underlying SPDEs. The proposed approach has been applied to three canonical SPDEs, (a) stochastic heat equation, (b) stochastic Allen-Cahn equation, and (c) stochastic Nagumo equation. Our results demonstrate that the proposed approach can accurately identify the underlying SPDEs with limited data. This is the first attempt at discovering SPDEs from data, and it has significant implications for various scientific applications, such as climate modeling, financial forecasting, and chemical kinetics.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#65288;GraSS&#65289;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#27491;&#26679;&#26412;&#28151;&#28102;&#21644;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15868</link><description>&lt;p&gt;
GraSS:&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GraSS: Contrastive Learning with Gradient Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation. (arXiv:2306.15868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15868
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#65288;GraSS&#65289;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#27491;&#26679;&#26412;&#28151;&#28102;&#21644;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SSCL&#65289;&#22312;&#36965;&#24863;&#22270;&#20687;&#65288;RSI&#65289;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#37324;&#31243;&#30865;&#12290;&#20854;&#26680;&#24515;&#22312;&#20110;&#35774;&#35745;&#19968;&#31181;&#26080;&#30417;&#30563;&#23454;&#20363;&#21306;&#20998;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#21033;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#23454;&#20363;&#21306;&#20998;&#30340;SSCL&#22312;&#24212;&#29992;&#20110;RSI&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#26102;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;1&#65289;&#27491;&#26679;&#26412;&#28151;&#28102;&#38382;&#39064;&#65307;2&#65289;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#12290;&#22312;&#38656;&#35201;&#20687;&#32032;&#32423;&#25110;&#30446;&#26631;&#32423;&#29305;&#24449;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#23427;&#24341;&#20837;&#20102;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#37492;&#21035;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#30340;&#26799;&#24230;&#26144;&#23556;&#21040;RSI&#30340;&#29305;&#23450;&#21306;&#22495;&#65292;&#36825;&#20123;&#29305;&#23450;&#21306;&#22495;&#24448;&#24448;&#21253;&#21547;&#29305;&#27530;&#30340;&#22320;&#38754;&#23545;&#35937;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;GraSS&#65289;&#29992;&#20110;RSI&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised contrastive learning (SSCL) has achieved significant milestones in remote sensing image (RSI) understanding. Its essence lies in designing an unsupervised instance discrimination pretext task to extract image features from a large number of unlabeled images that are beneficial for downstream tasks. However, existing instance discrimination based SSCL suffer from two limitations when applied to the RSI semantic segmentation task: 1) Positive sample confounding issue; 2) Feature adaptation bias. It introduces a feature adaptation bias when applied to semantic segmentation tasks that require pixel-level or object-level features. In this study, We observed that the discrimination information can be mapped to specific regions in RSI through the gradient of unsupervised contrastive loss, these specific regions tend to contain singular ground objects. Based on this, we propose contrastive learning with Gradient guided Sampling Strategy (GraSS) for RSI semantic segmentation. Gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15865</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20132;&#25442;&#20449;&#24687;&#26469;&#20272;&#35745;&#20174;&#20854;&#31169;&#19979;&#35266;&#23519;&#30340;&#26679;&#26412;&#20013;&#26410;&#30693;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#20294;&#20182;&#20204;&#20063;&#38754;&#20020;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#26696;&#30340;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#21644;&#32593;&#32476;&#20013;&#39640;&#25928;&#22320;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#65292;&#21516;&#26102;&#28385;&#36275;&#20195;&#29702;&#30340;&#38544;&#31169;&#38656;&#27714;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#36229;&#36234;&#20182;&#20204;&#26412;&#22320;&#38468;&#36817;&#30340;&#21327;&#35843;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#21442;&#19982;&#30340;&#20195;&#29702;&#33021;&#22815;&#20174;&#31163;&#32447;&#25110;&#38543;&#26102;&#38388;&#22312;&#32447;&#33719;&#21462;&#30340;&#31169;&#26377;&#20449;&#21495;&#20013;&#20272;&#35745;&#23436;&#25972;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#24182;&#20445;&#25252;&#20854;&#20449;&#21495;&#21644;&#32593;&#32476;&#38468;&#36817;&#30340;&#38544;&#31169;&#12290;&#36825;&#26159;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#23454;&#29616;&#30340;&#65292;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#20132;&#25442;&#30340;&#20272;&#35745;&#25968;&#25454;&#20013;&#20197;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23454;&#29616;&#20960;&#20309;&#20449;&#24687;&#26377;&#26681;&#25454;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.15858</link><description>&lt;p&gt;
&#29992;&#20110;&#25163;&#20013;&#29289;&#20307;&#33258;&#24863;&#30693;6D&#23039;&#24577;&#20272;&#35745;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects. (arXiv:2306.15858v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23454;&#29616;&#20960;&#20309;&#20449;&#24687;&#26377;&#26681;&#25454;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#29305;&#21035;&#26159;&#25163;&#20013;&#29289;&#20307;&#30340;&#25805;&#20316;&#65292;&#36890;&#24120;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#29289;&#20307;&#30340;6D&#23039;&#24577;&#12290;&#20026;&#20102;&#25552;&#39640;&#20272;&#35745;&#23039;&#24577;&#30340;&#20934;&#30830;&#24615;&#65292;&#30446;&#21069;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20351;&#29992;&#26469;&#33258;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20363;&#22914;RGB&#22270;&#20687;&#12289;&#28145;&#24230;&#21644;&#35302;&#35273;&#35835;&#25968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#36825;&#20123;&#27169;&#24577;&#25429;&#33719;&#30340;&#29289;&#20307;&#30340;&#22522;&#26412;&#20960;&#20309;&#32467;&#26500;&#30340;&#21033;&#29992;&#26377;&#38480;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#23545;&#35270;&#35273;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;&#36825;&#23548;&#33268;&#24403;&#38754;&#23545;&#32570;&#20047;&#36825;&#31181;&#35270;&#35273;&#29305;&#24449;&#30340;&#29289;&#20307;&#25110;&#32773;&#35270;&#35273;&#29305;&#24449;&#34987;&#36974;&#25377;&#26102;&#65292;&#24615;&#33021;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#20063;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#25163;&#25351;&#20301;&#32622;&#20013;&#23884;&#20837;&#30340;&#24863;&#35273;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32467;&#21512;&#22810;&#27169;&#24577;&#65288;&#35270;&#35273;&#21644;&#35302;&#35273;&#65289;&#25968;&#25454;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20960;&#20309;&#20449;&#24687;&#26377;&#26681;&#25454;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation, in particular in-hand object manipulation, often requires an accurate estimate of the object's 6D pose. To improve the accuracy of the estimated pose, state-of-the-art approaches in 6D object pose estimation use observational data from one or more modalities, e.g., RGB images, depth, and tactile readings. However, existing approaches make limited use of the underlying geometric structure of the object captured by these modalities, thereby, increasing their reliance on visual features. This results in poor performance when presented with objects that lack such visual features or when visual features are simply occluded. Furthermore, current approaches do not take advantage of the proprioceptive information embedded in the position of the fingers. To address these limitations, in this paper: (1) we introduce a hierarchical graph neural network architecture for combining multimodal (vision and touch) data that allows for a geometrically informed 6D object pose estima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30450;&#30446;&#37319;&#26679;&#22120;&#24212;&#29992;&#20197;&#35299;&#20915;&#20998;&#31163;&#35774;&#32622;&#19979;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20855;&#26377;&#20302;&#31209;&#24207;&#21015;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#32431;&#25506;&#32034;&#38382;&#39064;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.15856</link><description>&lt;p&gt;
&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#30450;&#30446;&#37319;&#26679;&#22120;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Pure exploration in multi-armed bandits with low rank structure using oblivious sampler. (arXiv:2306.15856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30450;&#30446;&#37319;&#26679;&#22120;&#24212;&#29992;&#20197;&#35299;&#20915;&#20998;&#31163;&#35774;&#32622;&#19979;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20855;&#26377;&#20302;&#31209;&#24207;&#21015;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#32431;&#25506;&#32034;&#38382;&#39064;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#22870;&#21169;&#24207;&#21015;&#30340;&#20302;&#31209;&#32467;&#26500;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#30340;&#20998;&#31163;&#35774;&#32622;&#65292;&#20854;&#20013;&#25506;&#32034;&#31574;&#30053;&#26080;&#27861;&#25509;&#25910;&#20854;&#25506;&#32034;&#30340;&#21453;&#39304;&#12290;&#30001;&#20110;&#36825;&#31181;&#20998;&#31163;&#35774;&#32622;&#65292;&#25506;&#32034;&#31574;&#30053;&#38656;&#35201;&#30450;&#30446;&#22320;&#37319;&#26679;&#33218;&#12290;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#21521;&#37327;&#30340;&#26680;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#26102;&#38388;&#21464;&#21270;&#21644;&#22266;&#23450;&#24773;&#20917;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#20026;$O(d\sqrt{(\ln N)/n})$&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20302;&#31209;&#24207;&#21015;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#19978;&#30028;&#19982;&#19979;&#30028;&#20043;&#38388;&#23384;&#22312;$O(\sqrt{\ln N})$&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the low rank structure of the reward sequence of the pure exploration problems. Firstly, we propose the separated setting in pure exploration problem, where the exploration strategy cannot receive the feedback of its explorations. Due to this separation, it requires that the exploration strategy to sample the arms obliviously. By involving the kernel information of the reward vectors, we provide efficient algorithms for both time-varying and fixed cases with regret bound $O(d\sqrt{(\ln N)/n})$. Then, we show the lower bound to the pure exploration in multi-armed bandits with low rank sequence. There is an $O(\sqrt{\ln N})$ gap between our upper bound and the lower bound.
&lt;/p&gt;</description></item><item><title>GoalieNet&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#32852;&#21512;&#25512;&#27979;&#20912;&#29699;&#20013;&#23432;&#38376;&#21592;&#12289;&#35013;&#22791;&#21644;&#29699;&#32593;&#30340;&#23039;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GoalieNet&#22312;&#22823;&#37327;&#20851;&#38190;&#28857;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;80%&#30340;&#24773;&#20917;&#19979;&#65292;&#25972;&#20307;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;84%&#65292;&#34920;&#26126;&#36825;&#31181;&#32852;&#21512;&#23039;&#21183;&#25512;&#27979;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15853</link><description>&lt;p&gt;
GoalieNet: &#20912;&#29699;&#20013;&#29992;&#20110;&#32852;&#21512;&#25512;&#27979;&#23432;&#38376;&#21592;&#12289;&#35013;&#22791;&#21644;&#29699;&#32593;&#23039;&#21183;&#30340;&#22810;&#38454;&#27573;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GoalieNet: A Multi-Stage Network for Joint Goalie, Equipment, and Net Pose Estimation in Ice Hockey. (arXiv:2306.15853v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15853
&lt;/p&gt;
&lt;p&gt;
GoalieNet&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#32852;&#21512;&#25512;&#27979;&#20912;&#29699;&#20013;&#23432;&#38376;&#21592;&#12289;&#35013;&#22791;&#21644;&#29699;&#32593;&#30340;&#23039;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GoalieNet&#22312;&#22823;&#37327;&#20851;&#38190;&#28857;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;80%&#30340;&#24773;&#20917;&#19979;&#65292;&#25972;&#20307;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;84%&#65292;&#34920;&#26126;&#36825;&#31181;&#32852;&#21512;&#23039;&#21183;&#25512;&#27979;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39537;&#21160;&#30340;&#20912;&#29699;&#20998;&#26512;&#39046;&#22495;&#20013;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#19988;&#30740;&#31350;&#26368;&#23569;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#23432;&#38376;&#21592;&#23039;&#21183;&#25512;&#27979;&#12290;&#19982;&#19968;&#33324;&#30340;&#20154;&#20307;&#23039;&#21183;&#25512;&#27979;&#19981;&#21516;&#65292;&#23432;&#38376;&#21592;&#23039;&#21183;&#25512;&#27979;&#26356;&#21152;&#22797;&#26434;&#65292;&#23427;&#19981;&#20165;&#28041;&#21450;&#26816;&#27979;&#38544;&#34255;&#22312;&#21402;&#21402;&#20869;&#34924;&#21644;&#38754;&#20855;&#19979;&#30340;&#23432;&#38376;&#21592;&#20851;&#38190;&#28857;&#65292;&#36824;&#28041;&#21450;&#22823;&#37327;&#19982;&#23432;&#38376;&#21592;&#36523;&#31359;&#30340;&#21402;&#37325;&#25252;&#33151;&#21644;&#25163;&#22871;&#12289;&#29699;&#26834;&#20197;&#21450;&#20912;&#29699;&#32593;&#30456;&#20851;&#30340;&#38750;&#20154;&#31867;&#20851;&#38190;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GoalieNet&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#32852;&#21512;&#25512;&#27979;&#23432;&#38376;&#21592;&#23039;&#21183;&#12289;&#23432;&#38376;&#21592;&#35013;&#22791;&#21644;&#29699;&#32593;&#30340;&#22810;&#38454;&#27573;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20351;&#29992;NHL&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;GoalieNet&#21487;&#20197;&#22312;&#25152;&#26377;&#20851;&#38190;&#28857;&#19978;&#23454;&#29616;84%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;&#26377;22&#20010;&#20851;&#38190;&#28857;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;80%&#12290;&#36825;&#34920;&#26126;&#65292;&#36825;&#31181;&#32852;&#21512;&#23039;&#21183;&#25512;&#27979;&#26041;&#27861;&#21487;&#33021;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of computer vision-driven ice hockey analytics, one of the most challenging and least studied tasks is goalie pose estimation. Unlike general human pose estimation, goalie pose estimation is much more complex as it involves not only the detection of keypoints corresponding to the joints of the goalie concealed under thick padding and mask, but also a large number of non-human keypoints corresponding to the large leg pads and gloves worn, the stick, as well as the hockey net. To tackle this challenge, we introduce GoalieNet, a multi-stage deep neural network for jointly estimating the pose of the goalie, their equipment, and the net. Experimental results using NHL benchmark data demonstrate that the proposed GoalieNet can achieve an average of 84\% accuracy across all keypoints, where 22 out of 29 keypoints are detected with more than 80\% accuracy. This indicates that such a joint pose estimation approach can be a promising research direction.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#38750;&#26367;&#20195;&#24615;SGD&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#25490;&#24207;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15848</link><description>&lt;p&gt;
&#38750;&#26367;&#20195;&#24615;SGD&#30340;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Ordering for Non-Replacement SGD. (arXiv:2306.15848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15848
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#38750;&#26367;&#20195;&#24615;SGD&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#25490;&#24207;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#25552;&#39640;&#25928;&#29575;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20943;&#23569;&#25152;&#20351;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#27927;&#29260;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#25216;&#26415;&#65292;&#20294;&#22312;&#29702;&#35770;&#19978;&#21364;&#21482;&#22312;&#26368;&#36817;&#20960;&#24180;&#25165;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#12290;&#38024;&#23545;&#38543;&#26426;&#27927;&#29260;&#21644;&#22686;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#21516;&#25910;&#25947;&#36895;&#24230;&#65292;&#25105;&#20204;&#23547;&#27714;&#25214;&#21040;&#19968;&#31181;&#25490;&#24207;&#26041;&#24335;&#65292;&#20197;&#25913;&#21892;&#38750;&#26367;&#20195;&#24418;&#24335;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22522;&#20110;&#24050;&#26377;&#30340;&#26368;&#20248;&#19982;&#24403;&#21069;&#36845;&#20195;&#20043;&#38388;&#30340;&#36317;&#31163;&#30028;&#38480;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#21462;&#20915;&#20110;&#36845;&#20195;&#24320;&#22987;&#26102;&#30340;&#26799;&#24230;&#12290;&#36890;&#36807;&#23545;&#35813;&#30028;&#38480;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#33021;&#22815;&#20026;&#24378;&#20984;&#21644;&#20984;&#20989;&#25968;&#24320;&#21457;&#20986;&#24120;&#25968;&#21644;&#36882;&#20943;&#27493;&#38271;&#30340;&#26368;&#20248;&#25490;&#24207;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#33021;&#23558;&#36825;&#31181;&#25490;&#24207;&#26041;&#24335;&#19982;&#23567;&#25209;&#37327;&#21644;&#22686;&#22823;&#25209;&#27425;&#26799;&#24230;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
One approach for reducing run time and improving efficiency of machine learning is to reduce the convergence rate of the optimization algorithm used. Shuffling is an algorithm technique that is widely used in machine learning, but it only started to gain attention theoretically in recent years. With different convergence rates developed for random shuffling and incremental gradient descent, we seek to find an ordering that can improve the convergence rates for the non-replacement form of the algorithm. Based on existing bounds of the distance between the optimal and current iterate, we derive an upper bound that is dependent on the gradients at the beginning of the epoch. Through analysis of the bound, we are able to develop optimal orderings for constant and decreasing step sizes for strongly convex and convex functions. We further test and verify our results through experiments on synthesis and real data sets. In addition, we are able to combine the ordering with mini-batch and furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;&#30340;&#35745;&#31639;&#24265;&#20215;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#26469;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15832</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Easing Color Shifts in Score-Based Diffusion Models. (arXiv:2306.15832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;&#30340;&#35745;&#31639;&#24265;&#20215;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#26469;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24471;&#20998;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#33021;&#20250;&#22240;&#31354;&#38388;&#22343;&#20540;&#30340;&#38169;&#35823;&#32780;&#20986;&#29616;&#39068;&#33394;&#20559;&#31227;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#36739;&#22823;&#30340;&#22270;&#20687;&#20013;&#20250;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#36731;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#39068;&#33394;&#20559;&#31227;&#12290;&#25105;&#20204;&#22312;&#24471;&#20998;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#65292;&#29992;&#20110;&#22788;&#29702;&#36755;&#20837;&#30340;&#31354;&#38388;&#22343;&#20540;&#65292;&#24182;&#39044;&#27979;&#24471;&#20998;&#20989;&#25968;&#30340;&#22343;&#20540;&#12290;&#36825;&#31181;&#32593;&#32476;&#26550;&#26500;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25913;&#36827;&#19982;&#29983;&#25104;&#22270;&#20687;&#22823;&#23567;&#30340;&#20851;&#31995;&#36817;&#20284;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#36328;&#22270;&#20687;&#23610;&#23544;&#30340;&#39068;&#33394;&#20559;&#31227;&#38382;&#39064;&#25552;&#20379;&#20102;&#30456;&#23545;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#29702;&#24819;&#21270;&#24773;&#20917;&#19979;&#39068;&#33394;&#20559;&#31227;&#30340;&#36215;&#28304;&#65292;&#20197;&#25512;&#21160;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generated images of score-based models can suffer from errors in their spatial means, an effect, referred to as a color shift, which grows for larger images. This paper introduces a computationally inexpensive solution to mitigate color shifts in score-based diffusion models. We propose a simple nonlinear bypass connection in the score network, designed to process the spatial mean of the input and to predict the mean of the score function. This network architecture substantially improves the resulting spatial means of the generated images, and we show that the improvement is approximately independent of the size of the generated images. As a result, our solution offers a comparatively inexpensive solution for the color shift problem across image sizes. Lastly, we discuss the origin of color shifts in an idealized setting in order to motivate our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#26368;&#26377;&#20449;&#24515;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15824</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Confidence-based Ensembles of End-to-End Speech Recognition Models. (arXiv:2306.15824v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#26368;&#26377;&#20449;&#24515;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#34987;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#25110;&#35821;&#35328;&#65292;&#23548;&#33268;&#19987;&#23478;&#31995;&#32479;&#22823;&#37327;&#28044;&#29616;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#33258;&#24049;&#19987;&#19994;&#39046;&#22495;&#20043;&#22806;&#30340;&#24615;&#33021;&#21364;&#30456;&#23545;&#36739;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#38598;&#25104;&#26469;&#25506;&#32034;&#36825;&#20123;&#19987;&#23478;&#27169;&#22411;&#30340;&#32452;&#21512;&#65306;&#22312;&#27169;&#22411;&#20013;&#21482;&#20351;&#29992;&#26368;&#26377;&#20449;&#24515;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#20551;&#35774;&#27169;&#22411;&#30340;&#30446;&#26631;&#25968;&#25454;&#38500;&#20102;&#19968;&#20010;&#23567;&#30340;&#39564;&#35777;&#38598;&#22806;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#30001;5&#20010;&#21333;&#35821;&#27169;&#22411;&#32452;&#25104;&#30340;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#38598;&#25104;&#20248;&#20110;&#36890;&#36807;&#19987;&#29992;&#35821;&#35328;&#35782;&#21035;&#27169;&#22359;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#30340;&#31995;&#32479;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#23558;&#22522;&#30784;&#27169;&#22411;&#21644;&#35843;&#25972;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#21407;&#22987;&#21644;&#30446;&#26631;&#25968;&#25454;&#19978;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#19978;&#39564;&#35777;&#20102;&#25152;&#26377;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of end-to-end speech recognition models grows every year. These models are often adapted to new domains or languages resulting in a proliferation of expert systems that achieve great results on target data, while generally showing inferior performance outside of their domain of expertise. We explore combination of such experts via confidence-based ensembles: ensembles of models where only the output of the most-confident model is used. We assume that models' target data is not available except for a small validation set. We demonstrate effectiveness of our approach with two applications. First, we show that a confidence-based ensemble of 5 monolingual models outperforms a system where model selection is performed via a dedicated language identification block. Second, we demonstrate that it is possible to combine base and adapted models to achieve strong results on both original and target data. We validate all our results on multiple datasets and model architectures.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#21516;&#28304;&#33041;&#32467;&#26500;&#27491;&#24120;&#19981;&#23545;&#31216;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#21644;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;Siamese&#26550;&#26500;&#23558;&#33041;&#32467;&#26500;&#30340;&#24038;&#21491;&#21322;&#29699;&#26144;&#23556;&#21040;&#19968;&#20010;&#27491;&#24120;&#19981;&#23545;&#31216;&#23884;&#20837;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#37327;&#21270;&#19982;&#27491;&#24120;&#19981;&#23545;&#31216;&#24615;&#20559;&#31163;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.15811</link><description>&lt;p&gt;
&#23398;&#20064;&#21516;&#28304;&#33041;&#32467;&#26500;&#30340;&#27491;&#24120;&#19981;&#23545;&#31216;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning normal asymmetry representations for homologous brain structures. (arXiv:2306.15811v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15811
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#21516;&#28304;&#33041;&#32467;&#26500;&#27491;&#24120;&#19981;&#23545;&#31216;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#21644;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;Siamese&#26550;&#26500;&#23558;&#33041;&#32467;&#26500;&#30340;&#24038;&#21491;&#21322;&#29699;&#26144;&#23556;&#21040;&#19968;&#20010;&#27491;&#24120;&#19981;&#23545;&#31216;&#23884;&#20837;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#37327;&#21270;&#19982;&#27491;&#24120;&#19981;&#23545;&#31216;&#24615;&#20559;&#31163;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27491;&#24120;&#30340;&#21516;&#28304;&#33041;&#32467;&#26500;&#22312;&#23450;&#20041;&#19978;&#26159;&#36817;&#20284;&#23545;&#31216;&#30340;&#65292;&#20294;&#23427;&#20204;&#20063;&#20250;&#22240;&#33258;&#28982;&#32769;&#21270;&#31561;&#21407;&#22240;&#32780;&#23384;&#22312;&#24418;&#29366;&#24046;&#24322;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#20250;&#23548;&#33268;&#19981;&#23545;&#31216;&#31243;&#24230;&#22686;&#21152;&#25110;&#25913;&#21464;&#20854;&#20301;&#32622;&#12290;&#30446;&#21069;&#65292;&#30830;&#23450;&#36825;&#20123;&#21464;&#21270;&#26159;&#21542;&#30001;&#30149;&#29702;&#24615;&#24694;&#21270;&#24341;&#36215;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#20020;&#24202;&#24037;&#20855;&#35201;&#20040;&#20381;&#36182;&#20110;&#20027;&#35266;&#35780;&#20272;&#12289;&#22522;&#26412;&#20307;&#31215;&#27979;&#37327;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#29305;&#23450;&#30142;&#30149;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#26816;&#27979;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21516;&#28304;&#33041;&#32467;&#26500;&#20013;&#27491;&#24120;&#30340;&#19981;&#23545;&#31216;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;Siamese&#26550;&#26500;&#65292;&#23558;&#33041;&#32467;&#26500;&#24038;&#21491;&#21322;&#29699;&#30340;3D&#20998;&#21106;&#26144;&#23556;&#21040;&#19968;&#20010;&#22522;&#20110;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#30446;&#26631;&#23398;&#20064;&#30340;&#27491;&#24120;&#19981;&#23545;&#31216;&#23884;&#20837;&#31354;&#38388;&#12290;&#30001;&#20110;&#21482;&#20351;&#29992;&#20581;&#24247;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#37327;&#21270;&#19982;&#27491;&#24120;&#19981;&#23545;&#31216;&#24615;&#20559;&#31163;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although normal homologous brain structures are approximately symmetrical by definition, they also have shape differences due to e.g. natural ageing. On the other hand, neurodegenerative conditions induce their own changes in this asymmetry, making them more pronounced or altering their location. Identifying when these alterations are due to a pathological deterioration is still challenging. Current clinical tools rely either on subjective evaluations, basic volume measurements or disease-specific deep learning models. This paper introduces a novel method to learn normal asymmetry patterns in homologous brain structures based on anomaly detection and representation learning. Our framework uses a Siamese architecture to map 3D segmentations of left and right hemispherical sides of a brain structure to a normal asymmetry embedding space, learned using a support vector data description objective. Being trained using healthy samples only, it can quantify deviations-from-normal-asymmetry pa
&lt;/p&gt;</description></item><item><title>FLuRKA&#26159;&#19968;&#31181;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;transformer&#31867;&#21035;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.15799</link><description>&lt;p&gt;
FLuRKA: &#24555;&#36895;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
FLuRKA: Fast fused Low-Rank &amp; Kernel Attention. (arXiv:2306.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15799
&lt;/p&gt;
&lt;p&gt;
FLuRKA&#26159;&#19968;&#31181;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;transformer&#31867;&#21035;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;transformer&#32467;&#26500;&#30340;&#25552;&#20986;&#20197;&#26469;&#65292;&#35768;&#22810;&#39640;&#25928;&#30340;&#36817;&#20284;&#33258;&#27880;&#24847;&#21147;&#25216;&#26415;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#20854;&#20013;&#20004;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#31867;&#21035;&#26159;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#30456;&#20114;&#34917;&#20805;&#65292;&#21033;&#29992;&#36825;&#20123;&#21327;&#21516;&#25928;&#24212;&#26469;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#65292;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#30340;transformer&#31867;&#21035;&#65306;FLuRKA&#65288;&#24555;&#36895;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#65289;&#12290;FLuRKA&#30456;&#23545;&#20110;&#36825;&#20123;&#36817;&#20284;&#25216;&#26415;&#25552;&#20379;&#20102;&#21487;&#35266;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#35780;&#20272;&#20102;FLuRKA&#30340;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#25552;&#20379;&#20102;&#22810;&#31181;&#21442;&#25968;&#37197;&#32622;&#65292;&#22312;&#36825;&#20123;&#37197;&#32622;&#19979;&#65292;FLuRKA&#20855;&#26377;&#21152;&#36895;&#25928;&#26524;&#65307;&#25105;&#20204;&#30340;&#20934;&#30830;&#24615;&#20998;&#26512;&#38480;&#23450;&#20102;FLuRKA&#30456;&#23545;&#20110;&#20840;&#27880;&#24847;&#21147;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19977;&#31181;FLuRKA&#21464;&#20307;&#65292;&#30456;&#23545;&#20110;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#20998;&#21035;&#23454;&#29616;&#20102;&#39640;&#36798;3.3&#20493;&#21644;1.7&#20493;&#30340;&#32463;&#39564;&#21152;&#36895;&#12290;&#36825;&#24847;&#21619;&#30528;&#26356;&#24555;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#32780;&#19988;&#36136;&#37327;&#20173;&#28982;&#20445;&#25345;&#19981;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many efficient approximate self-attention techniques have become prevalent since the inception of the transformer architecture. Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its own strengths. We observe these strengths synergistically complement each other and exploit these synergies to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA provide sizable performance gains over these approximate techniques and are of high quality. We theoretically and empirically evaluate both the runtime performance and quality of FLuRKA. Our runtime analysis posits a variety of parameter configurations where FLuRKA exhibit speedups and our accuracy analysis bounds the error of FLuRKA with respect to full-attention. We instantiate three FLuRKA variants which experience empirical speedups of up to 3.3x and 1.7x over low-rank and kernel methods respectively. This translates to spe
&lt;/p&gt;</description></item><item><title>HyenaDNA&#26159;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#21367;&#31215;&#30340;&#22522;&#22240;&#32452;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#26680;&#33527;&#37240;&#20998;&#36776;&#29575;&#19979;&#23545;&#38271;&#33539;&#22260;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.15794</link><description>&lt;p&gt;
HyenaDNA&#65306;&#21333;&#26680;&#33527;&#37240;&#20998;&#36776;&#29575;&#19979;&#30340;&#38271;&#33539;&#22260;&#22522;&#22240;&#32452;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution. (arXiv:2306.15794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15794
&lt;/p&gt;
&lt;p&gt;
HyenaDNA&#26159;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#21367;&#31215;&#30340;&#22522;&#22240;&#32452;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#26680;&#33527;&#37240;&#20998;&#36776;&#29575;&#19979;&#23545;&#38271;&#33539;&#22260;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32452;&#65288;DNA&#65289;&#24207;&#21015;&#32534;&#30721;&#20102;&#22823;&#37327;&#20851;&#20110;&#22522;&#22240;&#35843;&#25511;&#21644;&#34507;&#30333;&#36136;&#21512;&#25104;&#30340;&#20449;&#24687;&#12290;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22522;&#22240;&#32452;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#38750;&#26631;&#35760;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#22914;&#35782;&#21035;&#35843;&#25511;&#20803;&#20214;&#12290;&#30001;&#20110;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#25193;&#23637;&#65292;&#20808;&#21069;&#22522;&#20110;Transformer&#30340;&#22522;&#22240;&#32452;&#27169;&#22411;&#20165;&#20351;&#29992;512&#21040;4k&#20010;&#26631;&#35760;&#20316;&#20026;&#19978;&#19979;&#25991;&#65288;&lt;0.001%&#30340;&#20154;&#31867;&#22522;&#22240;&#32452;&#65289;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;DNA&#30340;&#38271;&#33539;&#22260;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#26631;&#35760;&#22120;&#26469;&#32858;&#21512;&#26377;&#24847;&#20041;&#30340;DNA&#21333;&#20803;&#65292;&#20002;&#22833;&#20102;&#21333;&#26680;&#33527;&#37240;&#20998;&#36776;&#29575;&#65292;&#20854;&#20013;&#24494;&#23567;&#30340;&#36951;&#20256;&#21464;&#24322;&#21487;&#20197;&#36890;&#36807;&#21333;&#26680;&#33527;&#37240;&#22810;&#24577;&#24615;&#65288;SNP&#65289;&#23436;&#20840;&#25913;&#21464;&#34507;&#30333;&#36136;&#21151;&#33021;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#38544;&#24335;&#21367;&#31215;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Hyena&#26174;&#31034;&#20986;&#33021;&#22815;&#19982;&#27880;&#24847;&#21147;&#36136;&#37327;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#20801;&#35768;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#26356;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;&#21033;&#29992;Hyenas n
&lt;/p&gt;
&lt;p&gt;
Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (&lt;0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#21160;&#21147;&#23398;&#22312;&#31283;&#20581;&#27493;&#34892;&#26426;&#22120;&#20154;&#20013;&#30340;&#20154;&#32676;&#23618;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#25511;&#21046;&#22120;&#30340;&#25299;&#25169;&#32467;&#26500;&#23545;&#24179;&#34913;&#33021;&#21147;&#30340;&#24433;&#21709;&#65307;&#36890;&#36807;&#24212;&#29992;&#31070;&#32463;&#24178;&#25200;&#25506;&#31350;&#31995;&#32479;&#30340;&#24378;&#36843;&#21709;&#24212;&#65292;&#21457;&#29616;&#24490;&#29615;&#29366;&#24577;&#21160;&#21147;&#23398;&#20855;&#26377;&#32467;&#26500;&#21270;&#21644;&#20302;&#32500;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#30340;&#23384;&#22312;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.15793</link><description>&lt;p&gt;
&#19968;&#20010;&#23545;&#31283;&#20581;&#24577;&#27493;&#34892;&#26426;&#22120;&#20154;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#20154;&#32676;&#23618;&#38754;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Population-Level Analysis of Neural Dynamics in Robust Legged Robots. (arXiv:2306.15793v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#21160;&#21147;&#23398;&#22312;&#31283;&#20581;&#27493;&#34892;&#26426;&#22120;&#20154;&#20013;&#30340;&#20154;&#32676;&#23618;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#25511;&#21046;&#22120;&#30340;&#25299;&#25169;&#32467;&#26500;&#23545;&#24179;&#34913;&#33021;&#21147;&#30340;&#24433;&#21709;&#65307;&#36890;&#36807;&#24212;&#29992;&#31070;&#32463;&#24178;&#25200;&#25506;&#31350;&#31995;&#32479;&#30340;&#24378;&#36843;&#21709;&#24212;&#65292;&#21457;&#29616;&#24490;&#29615;&#29366;&#24577;&#21160;&#21147;&#23398;&#20855;&#26377;&#32467;&#26500;&#21270;&#21644;&#20302;&#32500;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#30340;&#23384;&#22312;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#24378;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#23436;&#25104;&#22797;&#26434;&#30340;&#36816;&#21160;&#25511;&#21046;&#20219;&#21153;&#65292;&#22914;&#27493;&#24577;&#21644;&#25805;&#20316;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#26041;&#27861;&#26469;&#29702;&#35299;&#31283;&#20581;&#26426;&#22120;&#20154;&#27493;&#34892;&#25511;&#21046;&#22120;&#30340;&#20154;&#32676;&#23618;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#20998;&#26512;&#25299;&#25169;&#32467;&#26500;&#24320;&#22987;&#65292;&#21457;&#29616;&#33030;&#24369;&#30340;&#25511;&#21046;&#22120;&#20855;&#26377;&#26356;&#22810;&#30340;&#22266;&#23450;&#28857;&#21644;&#19981;&#31283;&#23450;&#26041;&#21521;&#65292;&#23548;&#33268;&#22312;&#25351;&#23548;&#19979;&#20445;&#25345;&#24179;&#34913;&#26102;&#26356;&#24046;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20027;&#23548;&#20154;&#32676;&#23618;&#27963;&#21160;&#26041;&#21521;&#19978;&#24212;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#31070;&#32463;&#24178;&#25200;&#26469;&#20998;&#26512;&#31995;&#32479;&#30340;&#24378;&#36843;&#21709;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#24490;&#29615;&#29366;&#24577;&#21160;&#21147;&#23398;&#22312;&#34892;&#36208;&#36807;&#31243;&#20013;&#20855;&#26377;&#32467;&#26500;&#21270;&#21644;&#20302;&#32500;&#29305;&#24449;&#65292;&#19982;&#28789;&#38271;&#31867;&#21160;&#29289;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#31526;&#12290;&#27492;&#22806;&#65292;&#24403;&#24490;&#29615;&#29366;&#24577;&#25200;&#21160;&#20026;&#38646;&#26102;&#65292;&#33030;&#24369;&#30340;&#25511;&#21046;&#22120;&#20173;&#33021;&#22815;&#34892;&#36208;&#65292;&#36825;&#34920;&#26126;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural network-based reinforcement learning systems are capable of complex motor control tasks such as locomotion and manipulation, however, much of their underlying mechanisms still remain difficult to interpret. Our aim is to leverage computational neuroscience methodologies to understanding the population-level activity of robust robot locomotion controllers. Our investigation begins by analyzing topological structure, discovering that fragile controllers have a higher number of fixed points with unstable directions, resulting in poorer balance when instructed to stand in place. Next, we analyze the forced response of the system by applying targeted neural perturbations along directions of dominant population-level activity. We find evidence that recurrent state dynamics are structured and low-dimensional during walking, which aligns with primate studies. Additionally, when recurrent states are perturbed to zero, fragile agents continue to walk, which is indicative of a st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26694;&#26550;&#30340;&#19981;&#36275;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;DP&#26426;&#21046;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30456;&#37051;&#25968;&#25454;&#38598;&#19978;&#30340;&#8220;&#35206;&#30422;&#31243;&#24230;&#8221;&#12290;&#36890;&#36807;&#36830;&#25509;&#35206;&#30422;&#31243;&#24230;&#25351;&#26631;&#21644;&#24050;&#26377;&#30740;&#31350;&#65292;&#25105;&#20204;&#25490;&#21517;&#20102;&#35757;&#32451;&#38598;&#20013;&#20010;&#21035;&#26679;&#26412;&#30340;&#38544;&#31169;&#65292;&#24182;&#24418;&#25104;&#20102;&#19968;&#20010;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#21487;&#20197;&#29992;&#26469;&#25506;&#27979;&#35266;&#23519;&#21040;&#30340;&#38544;&#31169;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2306.15790</link><description>&lt;p&gt;
&#21033;&#29992;&#36755;&#20986;&#29305;&#23450;&#21644;&#25968;&#25454;&#35299;&#26512;&#30340;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#38598;&#32423;&#38544;&#31169;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Probing the Transition to Dataset-Level Privacy in ML Models Using an Output-Specific and Data-Resolved Privacy Profile. (arXiv:2306.15790v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26694;&#26550;&#30340;&#19981;&#36275;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;DP&#26426;&#21046;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30456;&#37051;&#25968;&#25454;&#38598;&#19978;&#30340;&#8220;&#35206;&#30422;&#31243;&#24230;&#8221;&#12290;&#36890;&#36807;&#36830;&#25509;&#35206;&#30422;&#31243;&#24230;&#25351;&#26631;&#21644;&#24050;&#26377;&#30740;&#31350;&#65292;&#25105;&#20204;&#25490;&#21517;&#20102;&#35757;&#32451;&#38598;&#20013;&#20010;&#21035;&#26679;&#26412;&#30340;&#38544;&#31169;&#65292;&#24182;&#24418;&#25104;&#20102;&#19968;&#20010;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#21487;&#20197;&#29992;&#26469;&#25506;&#27979;&#35266;&#23519;&#21040;&#30340;&#38544;&#31169;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26159;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#29992;&#25143;&#25968;&#25454;&#30340;&#20027;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26694;&#26550;&#23384;&#22312;&#36873;&#25321;&#38544;&#31169;&#39044;&#31639;&#949;&#30340;&#19981;&#28165;&#26224;&#20197;&#21450;&#29305;&#23450;&#35757;&#32451;&#27169;&#22411;&#23545;&#29305;&#23450;&#25968;&#25454;&#34892;&#30340;&#38544;&#31169;&#27844;&#38706;&#37327;&#30340;&#32570;&#20047;&#37327;&#21270;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#23558;&#20351;&#29992;DP&#26426;&#21046;&#22312;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#35757;&#32451;&#30456;&#37051;&#25968;&#25454;&#38598;&#20135;&#29983;&#30340;&#20998;&#24067;&#30340;&#8220;&#35206;&#30422;&#31243;&#24230;&#8221;&#36827;&#34892;&#37327;&#21270;&#30340;&#38544;&#31169;&#25351;&#26631;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#26469;&#21487;&#35270;&#21270;DP&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#35206;&#30422;&#31243;&#24230;&#25351;&#26631;&#19982;&#25991;&#29486;&#20013;&#30340;&#24050;&#26377;&#30740;&#31350;&#30456;&#36830;&#25509;&#65292;&#24182;&#29992;&#23427;&#26469;&#23545;&#35757;&#32451;&#38598;&#20013;&#20010;&#21035;&#26679;&#26412;&#30340;&#38544;&#31169;&#36827;&#34892;&#25490;&#21517;&#65292;&#24418;&#25104;&#20102;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#21487;&#20197;&#29992;&#26469;&#25506;&#27979;&#35266;&#23519;&#21040;&#30340;&#37051;&#36817;&#20998;&#24067;&#20013;&#21457;&#29983;&#30340;&#19981;&#21487;&#21306;&#20998;&#24615;&#36716;&#25442;&#38543;&#30528;&#949;&#30340;&#36882;&#20943;
&lt;/p&gt;
&lt;p&gt;
Differential privacy (DP) is the prevailing technique for protecting user data in machine learning models. However, deficits to this framework include a lack of clarity for selecting the privacy budget $\epsilon$ and a lack of quantification for the privacy leakage for a particular data row by a particular trained model. We make progress toward these limitations and a new perspective by which to visualize DP results by studying a privacy metric that quantifies the extent to which a model trained on a dataset using a DP mechanism is ``covered" by each of the distributions resulting from training on neighboring datasets. We connect this coverage metric to what has been established in the literature and use it to rank the privacy of individual samples from the training set in what we call a privacy profile. We additionally show that the privacy profile can be used to probe an observed transition to indistinguishability that takes place in the neighboring distributions as $\epsilon$ decrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20316;&#20026;&#22810;&#23454;&#20363;&#23398;&#20064;&#22120;&#65292;&#29992;&#20110;&#39640;&#25928;&#24314;&#27169;&#21644;&#20998;&#31867;&#25972;&#20010;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#32452;&#32455;&#26001;&#22359;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2306.15789</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structured State Space Models for Multiple Instance Learning in Digital Pathology. (arXiv:2306.15789v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20316;&#20026;&#22810;&#23454;&#20363;&#23398;&#20064;&#22120;&#65292;&#29992;&#20110;&#39640;&#25928;&#24314;&#27169;&#21644;&#20998;&#31867;&#25972;&#20010;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#32452;&#32455;&#26001;&#22359;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#26159;&#19968;&#31181;&#29702;&#24819;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#20998;&#26512;&#27169;&#24335;&#65292;&#20854;&#20013;&#22823;&#37327;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#36890;&#24120;&#29992;&#21333;&#20010;&#20840;&#23616;&#26631;&#31614;&#36827;&#34892;&#27880;&#37322;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#34987;&#24314;&#27169;&#20026;&#19968;&#32452;&#35201;&#36827;&#34892;&#32858;&#21512;&#21644;&#20998;&#31867;&#30340;&#32452;&#32455;&#26001;&#22359;&#12290;&#29992;&#20110;&#36827;&#34892;&#27492;&#20998;&#31867;&#30340;&#24120;&#35265;&#27169;&#22411;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;transformer&#12290;&#23613;&#31649;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#22914;&#28145;&#24230;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#38477;&#20302;&#27599;&#20010;&#26001;&#22359;&#30340;&#32500;&#24230;&#65292;&#20294;&#26469;&#33258;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#30340;&#24207;&#21015;&#20173;&#28982;&#36807;&#38271;&#65292;&#36890;&#24120;&#21253;&#21547;&#25968;&#19975;&#20010;&#26001;&#22359;&#12290;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#24207;&#21015;&#24314;&#27169;&#26367;&#20195;&#26041;&#26696;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#39640;&#25928;&#24314;&#27169;&#38271;&#24207;&#21015;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23558;&#36755;&#20837;&#24207;&#21015;&#26368;&#20248;&#25237;&#24433;&#21040;&#21387;&#32553;&#25972;&#20010;&#24207;&#21015;&#30340;&#20869;&#23384;&#21333;&#20803;&#20013;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#29992;&#20316;&#22810;&#23454;&#20363;&#23398;&#20064;&#22120;&#65292;&#20197;&#23454;&#29616;&#21487;&#21464;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple instance learning is an ideal mode of analysis for histopathology data, where vast whole slide images are typically annotated with a single global label. In such cases, a whole slide image is modelled as a collection of tissue patches to be aggregated and classified. Common models for performing this classification include recurrent neural networks and transformers. Although powerful compression algorithms, such as deep pre-trained neural networks, are used to reduce the dimensionality of each patch, the sequences arising from whole slide images remain excessively long, routinely containing tens of thousands of patches. Structured state space models are an emerging alternative for sequence modelling, specifically designed for the efficient modelling of long sequences. These models invoke an optimal projection of an input sequence into memory units that compress the entire sequence. In this paper, we propose the use of state space models as a multiple instance learner to a vari
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#36825;&#20026;&#20043;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.15786</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#32599;&#29983;&#38376;&#25928;&#24212;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning. (arXiv:2306.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#36825;&#20026;&#20043;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32599;&#29983;&#38376;&#25928;&#24212;&#25551;&#36848;&#20102;&#20197;&#19979;&#29616;&#35937;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#21516;&#33391;&#22909;&#24615;&#33021;&#20294;&#37319;&#29992;&#19981;&#21516;&#35299;&#20915;&#31574;&#30053;&#30340;&#27169;&#22411;&#12290;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#35299;&#37322;&#30340;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#19981;&#21516;&#27604;&#36739;&#22330;&#26223;&#25552;&#20379;&#20102;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#12289;&#24402;&#22240;&#26041;&#27861;&#21644;&#25351;&#26631;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36229;&#21442;&#25968;&#35843;&#25972;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#65292;&#25351;&#26631;&#36873;&#25321;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20808;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Rashomon Effect describes the following phenomenon: for a given dataset there may exist many models with equally good performance but with different solution strategies. The Rashomon Effect has implications for Explainable Machine Learning, especially for the comparability of explanations. We provide a unified view on three different comparison scenarios and conduct a quantitative evaluation across different datasets, models, attribution methods, and metrics. We find that hyperparameter-tuning plays a role and that metric selection matters. Our results provide empirical support for previously anecdotal evidence and exhibit challenges for both scientists and practitioners.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.15782</link><description>&lt;p&gt;
UTRNet: &#21360;&#21047;&#25991;&#26723;&#20013;&#39640;&#20998;&#36776;&#29575;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#23610;&#24230;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;UTRNet&#26550;&#26500;&#65292;&#19968;&#20010;&#28151;&#21512;CNN-RNN&#27169;&#22411;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#24037;&#20316;&#24456;&#38590;&#25512;&#24191;&#21040;&#20044;&#23572;&#37117;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#36275;&#22815;&#30340;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UTRSet-Real&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;11,000&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;UTRSet-Synth&#65292;&#19968;&#20010;&#19982;&#23454;&#38469;&#19990;&#30028;&#38750;&#24120;&#30456;&#20284;&#30340;&#21547;&#26377;20,000&#34892;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;IIITH&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#30495;&#23454;&#24615;&#36827;&#34892;&#20102;&#20462;&#27491;&#65292;&#20351;&#20854;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#26356;&#21487;&#38752;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;UrduDoc&#65292;&#19968;&#31181;&#29992;&#20110;&#25195;&#25551;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#24037;&#20855;&#65292;&#36890;&#36807;&#23558;UTRNet&#19982;&#25991;&#26412;&#30340;&#31471;&#21040;&#31471;&#20044;&#23572;&#37117;OCR&#38598;&#25104;&#22312;&#21360;&#21047;&#25991;&#26723;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multi-scale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a tex
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#25216;&#26415;&#35282;&#24230;&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#30340;&#19979;&#19968;&#27493;&#24037;&#20316;&#65292;&#21253;&#25324;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12289;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15774</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19979;&#19968;&#27493;&#65306;&#25216;&#26415;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#25216;&#26415;&#35282;&#24230;&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#30340;&#19979;&#19968;&#27493;&#24037;&#20316;&#65292;&#21253;&#25324;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12289;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#22797;&#36328;&#23398;&#31185;&#35752;&#35770;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#35282;&#24230;&#20026;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#19979;&#19968;&#27493;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#36335;&#32447;&#22270;&#65292;&#27010;&#36848;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19977;&#20010;&#23618;&#38754;&#19978;&#30340;&#26410;&#26469;&#26041;&#21521;&#65306;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65307;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#65307;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#35813;&#36335;&#32447;&#22270;&#26088;&#22312;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#35752;&#35770;&#65292;&#21516;&#26102;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through iterative, cross-disciplinary discussions, we define and propose next-steps for Human-centered Generative AI (HGAI) from a technical perspective. We contribute a roadmap that lays out future directions of Generative AI spanning three levels: Aligning with human values; Accommodating humans' expression of intents; and Augmenting humans' abilities in a collaborative workflow. This roadmap intends to draw interdisciplinary research teams to a comprehensive list of emergent ideas in HGAI, identifying their interested topics while maintaining a coherent big picture of the future work landscape.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#25628;&#32034;&#22823;&#35268;&#27169;&#30340;LAION&#25968;&#25454;&#38598;&#65292;&#23581;&#35797;&#37325;&#26032;&#21019;&#24314;&#22270;&#20687;&#32593;&#65292;&#24182;&#21457;&#29616;&#19982;&#21407;&#22987;&#22270;&#20687;&#32593;&#30456;&#27604;&#65292;&#26032;&#24314;&#30340;LAIONet&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#20043;&#22788;&#12290;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#26159;&#65292;&#22312;&#22522;&#20110;&#22270;&#20687;&#25551;&#36848;&#36827;&#34892;&#25628;&#32034;&#26102;&#65292;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36873;&#25321;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.15769</link><description>&lt;p&gt;
&#22270;&#20687;&#32593;&#20026;&#20309;&#19982;LAION&#32593;&#32476;&#25130;&#28982;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
What Makes ImageNet Look Unlike LAION. (arXiv:2306.15769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#25628;&#32034;&#22823;&#35268;&#27169;&#30340;LAION&#25968;&#25454;&#38598;&#65292;&#23581;&#35797;&#37325;&#26032;&#21019;&#24314;&#22270;&#20687;&#32593;&#65292;&#24182;&#21457;&#29616;&#19982;&#21407;&#22987;&#22270;&#20687;&#32593;&#30456;&#27604;&#65292;&#26032;&#24314;&#30340;LAIONet&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#20043;&#22788;&#12290;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#26159;&#65292;&#22312;&#22522;&#20110;&#22270;&#20687;&#25551;&#36848;&#36827;&#34892;&#25628;&#32034;&#26102;&#65292;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36873;&#25321;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32593;&#26159;&#36890;&#36807;Flickr&#22270;&#20687;&#25628;&#32034;&#32467;&#26524;&#21019;&#24314;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#20165;&#26681;&#25454;&#22270;&#20687;&#25551;&#36848;&#37325;&#26032;&#21019;&#24314;&#22270;&#20687;&#32593;&#65292;&#25628;&#32034;&#22823;&#35268;&#27169;&#30340;LAION&#25968;&#25454;&#38598;&#20250;&#21457;&#29983;&#20160;&#20040;&#21602;&#65311;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#36825;&#20010;&#21453;&#20107;&#23454;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#37325;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#32593;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;LAIONet&#65292;&#19982;&#21407;&#22987;&#22270;&#20687;&#32593;&#26377;&#26126;&#26174;&#19981;&#21516;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21407;&#22987;&#22270;&#20687;&#32593;&#20013;&#22270;&#20687;&#30340;&#31867;&#20869;&#30456;&#20284;&#24615;&#36828;&#39640;&#20110;LAIONet&#12290;&#22240;&#27492;&#65292;&#22312;&#22270;&#20687;&#32593;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;LAIONet&#19978;&#34920;&#29616;&#26126;&#26174;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#35299;&#37322;&#36825;&#31181;&#24046;&#24322;&#30340;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#20104;&#20197;&#25903;&#25345;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#20165;&#22522;&#20110;&#22270;&#20687;&#25551;&#36848;&#36827;&#34892;&#25628;&#32034;&#20250;&#20135;&#29983;&#20449;&#24687;&#29942;&#39048;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#22522;&#20110;&#22270;&#20687;&#36807;&#28388;&#26102;&#23384;&#22312;&#30340;&#36873;&#25321;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;&#38271;&#26399;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
ImageNet was famously created from Flickr image search results. What if we recreated ImageNet instead by searching the massive LAION dataset based on image captions alone? In this work, we carry out this counterfactual investigation. We find that the resulting ImageNet recreation, which we call LAIONet, looks distinctly unlike the original. Specifically, the intra-class similarity of images in the original ImageNet is dramatically higher than it is for LAIONet. Consequently, models trained on ImageNet perform significantly worse on LAIONet. We propose a rigorous explanation for the discrepancy in terms of a subtle, yet important, difference in two plausible causal data-generating processes for the respective datasets, that we support with systematic experimentation. In a nutshell, searching based on an image caption alone creates an information bottleneck that mitigates the selection bias otherwise present in image-based filtering. Our explanation formalizes a long-held intuition in th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#25104;&#26412;&#25552;&#21319;NLP&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#24046;&#24322;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#25490;&#24207;&#20219;&#21153;&#19978;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15766</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26631;&#27880;&#32773;&#65306;&#20197;&#26368;&#23567;&#25104;&#26412;&#22686;&#24378;NLP&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost. (arXiv:2306.15766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#25104;&#26412;&#25552;&#21319;NLP&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#24046;&#24322;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#25490;&#24207;&#20219;&#21153;&#19978;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24335;NLP&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#24456;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#23545;&#20110;&#20302;&#25968;&#25454;&#39046;&#22495;&#30340;&#36755;&#20837;&#20063;&#23481;&#26131;&#20986;&#29616;&#22833;&#36133;&#65292;&#20363;&#22914;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#21253;&#21547;&#30340;&#39046;&#22495;&#12290;&#20316;&#20026;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23545;&#36755;&#20837;&#36827;&#34892;&#26631;&#27880;&#24182;&#25552;&#21319;NLP&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;LLM&#26631;&#27880;&#30340;&#39044;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36755;&#20837;&#36827;&#34892;&#26631;&#27880;&#21644;&#37325;&#26032;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#27969;&#34892;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#25928;&#26524;&#19981;&#20339;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#24494;&#35843;NLP&#27169;&#22411;&#20043;&#38388;&#39044;&#27979;&#20998;&#25968;&#24046;&#24322;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#21033;&#29992;&#20102;&#22823;&#22810;&#25968;NLP&#27169;&#22411;&#37117;&#26159;&#20174;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#32780;&#26469;&#30340;&#20107;&#23454;&#12290;&#20998;&#31867;(&#35821;&#20041;&#30456;&#20284;&#24230;)&#21644;&#25490;&#24207;(&#35821;&#20041;&#25628;&#32034;)&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#31574;&#30053;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art supervised NLP models achieve high accuracy but are also susceptible to failures on inputs from low-data regimes, such as domains that are not represented in training data. As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models. Specifically, given a budget for LLM annotations, we present an algorithm for sampling the most informative inputs to annotate and retrain the NLP model. We find that popular active learning strategies such as uncertainty-based sampling do not work well. Instead, we propose a sampling strategy based on the difference in prediction scores between the base model and the finetuned NLP model, utilizing the fact that most NLP models are finetuned from a base model. Experiments with classification (semantic similarity) and ranking (semantic search) tasks show that our sampling strategy leads to significant gain
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#24605;&#24819;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#28145;&#24230;&#22240;&#26524;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15764</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#22240;&#26524;&#27169;&#22411;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
High Fidelity Image Counterfactuals with Probabilistic Causal Models. (arXiv:2306.15764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15764
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#24605;&#24819;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#28145;&#24230;&#22240;&#26524;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#20855;&#26377;&#28145;&#24230;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;&#23545;&#20110;&#39640;&#32500;&#32467;&#26500;&#21464;&#37327;&#65288;&#22914;&#22270;&#20687;&#65289;&#30340;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#20272;&#35745;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#30340;&#24605;&#24819;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#65292;&#20026;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21464;&#37327;&#35774;&#35745;&#20102;&#26032;&#30340;&#28145;&#24230;&#22240;&#26524;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26426;&#21046;&#33021;&#22815;&#20934;&#30830;&#22320;&#25512;&#26029;&#21644;&#20272;&#35745;&#30452;&#25509;&#12289;&#38388;&#25509;&#21644;&#24635;&#25928;&#24212;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#21453;&#20107;&#23454;&#30340;&#20844;&#29702;&#20005;&#23494;&#24615;&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general causal generative modelling framework for accurate estimation of high fidelity image counterfactuals with deep structural causal models. Estimation of interventional and counterfactual queries for high-dimensional structured variables, such as images, remains a challenging task. We leverage ideas from causal mediation analysis and advances in generative modelling to design new deep causal mechanisms for structured variables in causal models. Our experiments demonstrate that our proposed mechanisms are capable of accurate abduction and estimation of direct, indirect and total effects as measured by axiomatic soundness of counterfactuals.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#37325;&#26500;&#20195;&#30721;&#24322;&#21619;&#23545;&#24212;&#29992;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#31639;&#27861;&#26469;&#39044;&#27979;&#20195;&#30721;&#37325;&#26500;&#23545;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;31&#20010;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#30340;16&#31181;&#20195;&#30721;&#24322;&#21619;&#31867;&#22411;&#30340;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#29420;&#31435;&#21644;&#25209;&#37327;&#37325;&#26500;&#29305;&#23450;&#20195;&#30721;&#24322;&#21619;&#21518;&#30340;CPU&#21644;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.15763</link><description>&lt;p&gt;
&#39044;&#27979;&#25209;&#37327;&#37325;&#26500;&#20195;&#30721;&#24322;&#21619;&#23545;&#24212;&#29992;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Predicting the Impact of Batch Refactoring Code Smells on Application Resource Consumption. (arXiv:2306.15763v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#37325;&#26500;&#20195;&#30721;&#24322;&#21619;&#23545;&#24212;&#29992;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#31639;&#27861;&#26469;&#39044;&#27979;&#20195;&#30721;&#37325;&#26500;&#23545;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;31&#20010;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#30340;16&#31181;&#20195;&#30721;&#24322;&#21619;&#31867;&#22411;&#30340;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#29420;&#31435;&#21644;&#25209;&#37327;&#37325;&#26500;&#29305;&#23450;&#20195;&#30721;&#24322;&#21619;&#21518;&#30340;CPU&#21644;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25209;&#37327;&#37325;&#26500;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#26032;&#26550;&#26500;&#36719;&#20214;&#30340;&#20107;&#23454;&#19978;&#30340;&#26426;&#21046;&#65292;&#21487;&#33021;&#20250;&#23545;&#20195;&#30721;&#36136;&#37327;&#21644;&#21487;&#32500;&#25252;&#24615;&#20135;&#29983;&#37325;&#22823;&#35774;&#35745;&#32570;&#38519;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#23613;&#31649;&#24050;&#30693;&#33258;&#21160;&#21270;&#25209;&#37327;&#37325;&#26500;&#25216;&#26415;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36719;&#20214;&#30340;&#36136;&#37327;&#21644;&#21487;&#32500;&#25252;&#24615;&#65292;&#20294;&#20854;&#23545;&#36164;&#28304;&#21033;&#29992;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#33391;&#22909;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#25209;&#37327;&#37325;&#26500;&#20195;&#30721;&#24322;&#21619;&#21644;&#36164;&#28304;&#28040;&#32791;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#30830;&#23450;&#20102;&#36719;&#20214;&#20195;&#30721;&#24322;&#21619;&#25209;&#37327;&#37325;&#26500;&#19982;&#36164;&#28304;&#28040;&#32791;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#26088;&#22312;&#35774;&#35745;&#31639;&#27861;&#26469;&#39044;&#27979;&#20195;&#30721;&#24322;&#21619;&#37325;&#26500;&#23545;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;31&#20010;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#30340;16&#31181;&#20195;&#30721;&#24322;&#21619;&#31867;&#22411;&#21450;&#20854;&#23545;&#36164;&#28304;&#21033;&#29992;&#30340;&#32852;&#21512;&#25928;&#26524;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23427;&#35814;&#32454;&#22320;&#20998;&#26512;&#20102;&#22312;&#29420;&#31435;&#21644;&#25209;&#37327;&#37325;&#26500;&#29305;&#23450;&#20195;&#30721;&#24322;&#21619;&#21518;&#24212;&#29992;&#30340;CPU&#21644;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;&#21464;&#21270;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20010;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Automated batch refactoring has become a de-facto mechanism to restructure software that may have significant design flaws negatively impacting the code quality and maintainability. Although automated batch refactoring techniques are known to significantly improve overall software quality and maintainability, their impact on resource utilization is not well studied. This paper aims to bridge the gap between batch refactoring code smells and consumption of resources. It determines the relationship between software code smell batch refactoring, and resource consumption. Next, it aims to design algorithms to predict the impact of code smell refactoring on resource consumption. This paper investigates 16 code smell types and their joint effect on resource utilization for 31 open source applications. It provides a detailed empirical analysis of the change in application CPU and memory utilization after refactoring specific code smells in isolation and in batches. This analysis is then used 
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31080;&#25454;&#21270;&#23398;&#20064;-&#36951;&#24536;&#27169;&#22411;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#21521;&#27599;&#20010;&#21442;&#19982;&#35757;&#32451;&#31034;&#20363;&#21457;&#36865;&#39069;&#22806;&#20449;&#24687;&#24182;&#20445;&#30041;&#19968;&#37096;&#20998;&#20013;&#22830;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31080;&#25454;&#21270;&#23398;&#20064;-&#36951;&#24536;&#26041;&#26696;&#65292;&#29992;&#20110;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.15744</link><description>&lt;p&gt;
&#31080;&#25454;&#21270;&#23398;&#20064;-&#36951;&#24536;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Ticketed Learning-Unlearning Schemes. (arXiv:2306.15744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31080;&#25454;&#21270;&#23398;&#20064;-&#36951;&#24536;&#27169;&#22411;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#21521;&#27599;&#20010;&#21442;&#19982;&#35757;&#32451;&#31034;&#20363;&#21457;&#36865;&#39069;&#22806;&#20449;&#24687;&#24182;&#20445;&#30041;&#19968;&#37096;&#20998;&#20013;&#22830;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31080;&#25454;&#21270;&#23398;&#20064;-&#36951;&#24536;&#26041;&#26696;&#65292;&#29992;&#20110;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#25152;&#23450;&#20041;&#30340;&#23398;&#20064;-&#36951;&#24536;&#33539;&#24335;&#12290;&#39318;&#20808;&#65292;&#32473;&#23450;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#22909;&#30340;&#39044;&#27979;&#22120;&#65292;&#20363;&#22914;&#26368;&#23567;&#21270;&#26576;&#20010;&#25439;&#22833;&#20989;&#25968;&#30340;&#39044;&#27979;&#22120;&#12290;&#38543;&#21518;&#65292;&#32473;&#23450;&#20219;&#20309;&#24076;&#26395;&#36951;&#24536;&#30340;&#31034;&#20363;&#23376;&#38598;&#65292;&#30446;&#26631;&#26159;&#22312;&#19981;&#30693;&#36947;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#19982;&#22312;&#24184;&#23384;&#31034;&#20363;&#19978;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#26102;&#25152;&#29983;&#25104;&#30340;&#39044;&#27979;&#22120;&#23436;&#20840;&#30456;&#21516;&#30340;&#22909;&#30340;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31080;&#25454;&#21270;&#23398;&#20064;-&#36951;&#24536;&#27169;&#22411;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#23567;&#22411;&#65288;&#21152;&#23494;&#30340;&#65289;&#8220;&#31080;&#25454;&#8221;&#21521;&#27599;&#20010;&#21442;&#19982;&#35757;&#32451;&#31034;&#20363;&#21457;&#36865;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#24182;&#20445;&#30041;&#19968;&#23567;&#37096;&#20998;&#8220;&#20013;&#22830;&#8221;&#20449;&#24687;&#20197;&#20379;&#20197;&#21518;&#20351;&#29992;&#12290;&#38543;&#21518;&#65292;&#24076;&#26395;&#36951;&#24536;&#30340;&#31034;&#20363;&#23558;&#20854;&#31080;&#25454;&#25552;&#20132;&#32473;&#36951;&#24536;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36824;&#20351;&#29992;&#20013;&#22830;&#20449;&#24687;&#36820;&#22238;&#19968;&#20010;&#26032;&#30340;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31080;&#25454;&#21270;&#23398;&#20064;-&#36951;&#24536;&#26041;&#26696;&#65292;&#29992;&#20110;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the learning--unlearning paradigm defined as follows. First given a dataset, the goal is to learn a good predictor, such as one minimizing a certain loss. Subsequently, given any subset of examples that wish to be unlearnt, the goal is to learn, without the knowledge of the original training dataset, a good predictor that is identical to the predictor that would have been produced when learning from scratch on the surviving examples.  We propose a new ticketed model for learning--unlearning wherein the learning algorithm can send back additional information in the form of a small-sized (encrypted) ``ticket'' to each participating training example, in addition to retaining a small amount of ``central'' information for later. Subsequently, the examples that wish to be unlearnt present their tickets to the unlearning algorithm, which additionally uses the central information to return a new predictor. We provide space-efficient ticketed learning--unlearning schemes for a broad
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#27604;&#29575;&#27169;&#25311;&#25512;&#26029;&#31639;&#27861;&#19982;&#38543;&#26426;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#30340;&#37325;&#35201;&#36830;&#25509;&#65292;&#23558;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745; (BOED) &#25193;&#23637;&#21040;&#27169;&#25311;&#25512;&#26029;&#24212;&#29992;&#20013;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#20248;&#21270;&#23454;&#39564;&#35774;&#35745;&#21644;&#25674;&#36824;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15731</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#23454;&#39564;&#35774;&#35745;&#30340;&#27169;&#25311;&#25512;&#26029;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Bayesian Optimal Experimental Designs for Simulation-based Inference. (arXiv:2306.15731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#27604;&#29575;&#27169;&#25311;&#25512;&#26029;&#31639;&#27861;&#19982;&#38543;&#26426;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#30340;&#37325;&#35201;&#36830;&#25509;&#65292;&#23558;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745; (BOED) &#25193;&#23637;&#21040;&#27169;&#25311;&#25512;&#26029;&#24212;&#29992;&#20013;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#20248;&#21270;&#23454;&#39564;&#35774;&#35745;&#21644;&#25674;&#36824;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36870;&#38382;&#39064;&#30340;&#22797;&#26434;&#31185;&#23398;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38750;&#21487;&#24494;&#24615;&#36136;&#65292;&#27169;&#25311;&#25512;&#26029;&#27169;&#22411;&#24120;&#24120;&#38754;&#20020;&#37325;&#22823;&#22256;&#38590;&#65292;&#36825;&#38459;&#30861;&#20102;&#26799;&#24230;&#20248;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745; (BOED) &#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#23454;&#39564;&#36164;&#28304;&#65292;&#20197;&#25913;&#36827;&#25512;&#26029;&#32467;&#26524;&#12290;&#23613;&#31649;&#38543;&#26426;&#26799;&#24230; BOED &#26041;&#27861;&#22312;&#39640;&#32500;&#35774;&#35745;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#35768;&#22810;&#27169;&#25311;&#25512;&#26029;&#27169;&#22411;&#30340;&#38590;&#20197;&#22788;&#29702;&#30340;&#38750;&#21487;&#24494;&#24615;&#36136;&#65292;BOED &#19982;&#27169;&#25311;&#25512;&#26029;&#30340;&#25972;&#21512;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20114;&#20449;&#24687;&#30028;&#38480;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#27604;&#29575;&#30340;&#27169;&#25311;&#25512;&#26029;&#25512;&#29702;&#31639;&#27861;&#19982;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#37325;&#35201;&#36830;&#25509;&#12290;&#36825;&#31181;&#36830;&#25509;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;BOED&#25193;&#23637;&#21040;&#27169;&#25311;&#25512;&#26029;&#24212;&#29992;&#39046;&#22495;&#65292;&#23454;&#29616;&#23454;&#39564;&#35774;&#35745;&#21644;&#25674;&#36824;&#25512;&#26029;&#26041;&#27861;&#30340;&#21516;&#26102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based inference (SBI) methods tackle complex scientific models with challenging inverse problems. However, SBI models often face a significant hurdle due to their non-differentiable nature, which hampers the use of gradient-based optimization techniques. Bayesian Optimal Experimental Design (BOED) is a powerful approach that aims to make the most efficient use of experimental resources for improved inferences. While stochastic gradient BOED methods have shown promising results in high-dimensional design problems, they have mostly neglected the integration of BOED with SBI due to the difficult non-differentiable property of many SBI simulators. In this work, we establish a crucial connection between ratio-based SBI inference algorithms and stochastic gradient-based variational inference by leveraging mutual information bounds. This connection allows us to extend BOED to SBI applications, enabling the simultaneous optimization of experimental designs and amortized inference fu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#38381;&#29615;&#35757;&#32451;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#35757;&#32451;&#22522;&#20934;&#35774;&#35745;&#21644;&#27969;&#34892;RL&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#39550;&#39542;&#20195;&#29702;TRAVL&#65292;&#36890;&#36807;&#22810;&#27493;&#39044;&#27979;&#21644;&#21033;&#29992;&#34394;&#25311;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.15713</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#38381;&#29615;&#35757;&#32451;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rethinking Closed-loop Training for Autonomous Driving. (arXiv:2306.15713v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15713
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#38381;&#29615;&#35757;&#32451;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#35757;&#32451;&#22522;&#20934;&#35774;&#35745;&#21644;&#27969;&#34892;RL&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#39550;&#39542;&#20195;&#29702;TRAVL&#65292;&#36890;&#36807;&#22810;&#27493;&#39044;&#27979;&#21644;&#21033;&#29992;&#34394;&#25311;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#39550;&#39542;&#20195;&#29702;&#30340;&#38381;&#29615;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#28508;&#22312;&#22320;&#35299;&#20915;&#20102;&#35757;&#32451;&#19982;&#37096;&#32626;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#23433;&#20840;&#19988;&#24265;&#20215;&#22320;&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22914;&#20309;&#26500;&#24314;&#38381;&#29615;&#35757;&#32451;&#30340;&#26377;&#25928;&#35757;&#32451;&#22522;&#20934;&#32570;&#20047;&#20102;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#35757;&#32451;&#22522;&#20934;&#35774;&#35745;&#23545;&#20110;&#23398;&#20064;&#20195;&#29702;&#25104;&#21151;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#22914;&#20309;&#35774;&#35745;&#20132;&#36890;&#22330;&#26223;&#21644;&#35268;&#27169;&#21270;&#35757;&#32451;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#27969;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#26080;&#27861;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#38271;&#26399;&#35268;&#21010;&#65292;&#24182;&#19988;&#35757;&#32451;&#26102;&#38388;&#38750;&#24120;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36712;&#36857;&#20215;&#20540;&#23398;&#20064;&#65288;TRAVL&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#39550;&#39542;&#20195;&#29702;&#65292;&#36890;&#36807;&#22810;&#27493;&#39044;&#27979;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#21033;&#29992;&#24265;&#20215;&#29983;&#25104;&#30340;&#34394;&#25311;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in high-fidelity simulators have enabled closed-loop training of autonomous driving agents, potentially solving the distribution shift in training v.s. deployment and allowing training to be scaled both safely and cheaply. However, there is a lack of understanding of how to build effective training benchmarks for closed-loop training. In this work, we present the first empirical study which analyzes the effects of different training benchmark designs on the success of learning agents, such as how to design traffic scenarios and scale training environments. Furthermore, we show that many popular RL algorithms cannot achieve satisfactory performance in the context of autonomous driving, as they lack long-term planning and take an extremely long time to train. To address these issues, we propose trajectory value learning (TRAVL), an RL-based driving agent that performs planning with multistep look-ahead and exploits cheaply generated imagined data for efficient learning. O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26412;&#22320;&#20998;&#24067;&#22810;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38544;&#31169;&#20445;&#25252;&#26469;&#36827;&#34892;&#20849;&#35782;&#31038;&#21306;&#26816;&#27979;&#21644;&#20272;&#35745;&#12290;&#37319;&#29992;&#38543;&#26426;&#21709;&#24212;&#26426;&#21046;&#23545;&#32593;&#32476;&#36793;&#36827;&#34892;&#25200;&#21160;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#35889;&#32858;&#31867;&#31639;&#27861;&#22312;&#25200;&#21160;&#37051;&#25509;&#30697;&#38453;&#19978;&#25191;&#34892;&#65292;&#20197;&#38450;&#27490;&#31038;&#21306;&#20043;&#38388;&#30340;&#25269;&#28040;&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#20004;&#27493;&#20559;&#24046;&#35843;&#25972;&#36807;&#31243;&#26469;&#28040;&#38500;&#25200;&#21160;&#21644;&#32593;&#32476;&#30697;&#38453;&#24102;&#26469;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.15709</link><description>&lt;p&gt;
&#20445;&#25252;&#38544;&#31169;&#30340;&#26412;&#22320;&#20998;&#24067;&#22810;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Community Detection for Locally Distributed Multiple Networks. (arXiv:2306.15709v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26412;&#22320;&#20998;&#24067;&#22810;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38544;&#31169;&#20445;&#25252;&#26469;&#36827;&#34892;&#20849;&#35782;&#31038;&#21306;&#26816;&#27979;&#21644;&#20272;&#35745;&#12290;&#37319;&#29992;&#38543;&#26426;&#21709;&#24212;&#26426;&#21046;&#23545;&#32593;&#32476;&#36793;&#36827;&#34892;&#25200;&#21160;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#35889;&#32858;&#31867;&#31639;&#27861;&#22312;&#25200;&#21160;&#37051;&#25509;&#30697;&#38453;&#19978;&#25191;&#34892;&#65292;&#20197;&#38450;&#27490;&#31038;&#21306;&#20043;&#38388;&#30340;&#25269;&#28040;&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#20004;&#27493;&#20559;&#24046;&#35843;&#25972;&#36807;&#31243;&#26469;&#28040;&#38500;&#25200;&#21160;&#21644;&#32593;&#32476;&#30697;&#38453;&#24102;&#26469;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#32593;&#32476;&#30001;&#20110;&#38544;&#31169;&#12289;&#25152;&#26377;&#26435;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#21407;&#22240;&#65292;&#24120;&#24120;&#20197;&#26412;&#22320;&#21644;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#20998;&#26512;&#12290;&#20851;&#20110;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#27169;&#22411;&#21270;&#32479;&#35745;&#26041;&#27861;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#30340;&#25991;&#29486;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26412;&#22320;&#23384;&#20648;&#21644;&#35745;&#31639;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#22810;&#23618;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#20849;&#35782;&#31038;&#21306;&#26816;&#27979;&#21644;&#20272;&#35745;&#65292;&#24182;&#37319;&#29992;&#38544;&#31169;&#20445;&#25252;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#35889;&#32858;&#31867;&#65288;ppDSC&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;&#20026;&#20102;&#20445;&#25252;&#36793;&#30340;&#38544;&#31169;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#38543;&#26426;&#21709;&#24212;&#65288;RR&#65289;&#26426;&#21046;&#26469;&#25200;&#21160;&#32593;&#32476;&#36793;&#65292;&#35813;&#26426;&#21046;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#30340;&#24378;&#27010;&#24565;&#12290;ppDSC&#31639;&#27861;&#22312;&#24179;&#26041;&#30340;RR&#25200;&#21160;&#37051;&#25509;&#30697;&#38453;&#19978;&#25191;&#34892;&#65292;&#20197;&#38450;&#27490;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#31038;&#21306;&#30456;&#20114;&#25269;&#28040;&#12290;&#20026;&#20102;&#28040;&#38500;RR&#21644;&#24179;&#26041;&#32593;&#32476;&#30697;&#38453;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#27493;&#20559;&#24046;&#35843;&#25972;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer networks are commonly stored and analyzed in a local and distributed fashion because of the privacy, ownership, and communication costs. The literature on the model-based statistical methods for community detection based on these data is still limited. This paper proposes a new method for consensus community detection and estimation in a multi-layer stochastic block model using locally stored and computed network data with privacy protection. A novel algorithm named privacy-preserving Distributed Spectral Clustering (ppDSC) is developed. To preserve the edges' privacy, we adopt the randomized response (RR) mechanism to perturb the network edges, which satisfies the strong notion of differential privacy. The ppDSC algorithm is performed on the squared RR-perturbed adjacency matrices to prevent possible cancellation of communities among different layers. To remove the bias incurred by RR and the squared network matrices, we develop a two-step bias-adjustment procedure.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;QFL&#26694;&#26550;&#30340;&#35774;&#35745;&#24605;&#36335;&#12289;&#24212;&#29992;&#26696;&#20363;&#21644;&#20851;&#38190;&#22240;&#32032;&#30340;&#32771;&#34385;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#21508;&#31181;QFL&#30740;&#31350;&#39033;&#30446;&#30340;&#25216;&#26415;&#36129;&#29486;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15708</link><description>&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65306;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#29616;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Quantum Federated Learning: Analysis, Design and Implementation Challenges. (arXiv:2306.15708v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;QFL&#26694;&#26550;&#30340;&#35774;&#35745;&#24605;&#36335;&#12289;&#24212;&#29992;&#26696;&#20363;&#21644;&#20851;&#38190;&#22240;&#32032;&#30340;&#32771;&#34385;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#21508;&#31181;QFL&#30740;&#31350;&#39033;&#30446;&#30340;&#25216;&#26415;&#36129;&#29486;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#38543;&#30528;&#23545;QFL&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#36843;&#20999;&#38656;&#35201;&#20102;&#35299;&#20854;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#20171;&#32461;&#24403;&#21069;QFL&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#22635;&#34917;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#37325;&#35201;&#30693;&#35782;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;QFL&#26694;&#26550;&#30340;&#24605;&#36335;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#24212;&#29992;&#26696;&#20363;&#65292;&#24182;&#32771;&#34385;&#20102;&#24433;&#21709;&#20854;&#35774;&#35745;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#23545;&#21508;&#31181;QFL&#30740;&#31350;&#39033;&#30446;&#30340;&#25216;&#26415;&#36129;&#29486;&#21644;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#32771;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Federated Learning (QFL) has gained significant attention due to quantum computing and machine learning advancements. As the demand for QFL continues to surge, there is a pressing need to comprehend its intricacies in distributed environments. This paper aims to provide a comprehensive overview of the current state of QFL, addressing a crucial knowledge gap in the existing literature. We develop ideas for new QFL frameworks, explore diverse use cases of applications, and consider the critical factors influencing their design. The technical contributions and limitations of various QFL research projects are examined while presenting future research directions and open questions for further exploration.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#30340;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#19982;&#39640;&#32500;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#21521;&#37327;&#30340;&#20851;&#31995;&#65292;&#35745;&#31639;&#20986;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAPs&#65289;&#12290;&#22522;&#20110;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;UAPs&#23545;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#30340;&#21453;&#24212;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#20855;&#20307;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32500;&#25345;&#20102;&#19982;&#27491;&#24120;&#25512;&#26029;&#30456;&#31561;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.15705</link><description>&lt;p&gt;
&#20851;&#20110;&#29992;&#20110;&#39640;&#25928;&#30340;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection. (arXiv:2306.15705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#30340;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#19982;&#39640;&#32500;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#21521;&#37327;&#30340;&#20851;&#31995;&#65292;&#35745;&#31639;&#20986;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAPs&#65289;&#12290;&#22522;&#20110;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;UAPs&#23545;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#30340;&#21453;&#24212;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#20855;&#20307;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32500;&#25345;&#20102;&#19982;&#27491;&#24120;&#25512;&#26029;&#30456;&#31561;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#26679;&#26412;&#20197;&#27450;&#39575;&#27169;&#22411;&#26159;&#30830;&#20445;&#31038;&#20132;&#23433;&#20840;&#24212;&#29992;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#24341;&#21457;&#20102;&#19982;&#38544;&#31169;&#27844;&#38706;&#21644;&#36890;&#29992;&#24615;&#30456;&#20851;&#30340;&#26126;&#26174;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25915;&#20987;&#31639;&#27861;&#20135;&#29983;&#30340;&#23545;&#25239;&#26679;&#26412;&#19982;&#39640;&#32500;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#21521;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#20123;&#21521;&#37327;&#31216;&#20026;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAPs&#65289;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#24471;&#20986;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#25968;&#25454;&#23545;&#25239;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#23545;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#20135;&#29983;&#19981;&#21516;&#30340;&#21453;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#32500;&#25345;&#20102;&#19982;&#27491;&#24120;&#25512;&#26029;&#30456;&#31561;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications. However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability. In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs. Such vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated without original training data. Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces different responses between normal and adversarial samples to UAPs. Experimental results show that our method achieves competitive detection performance on various text classification tasks, and maintains an equivalent time consumption to normal inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;NuPlan&#25361;&#25112;&#36187;&#20013;&#30340;&#31532;&#20108;&#21517;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;-&#26102;&#38388;&#28909;&#21147;&#22270;&#39044;&#27979;&#26410;&#26469;&#22810;&#27169;&#24577;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#36712;&#36857;&#20248;&#21270;&#25216;&#26415;&#23454;&#29616;&#23433;&#20840;&#30340;&#33258;&#20027;&#39550;&#39542;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2306.15700</link><description>&lt;p&gt;
Imitation with Spatial-Temporal Heatmap: NuPlan&#25361;&#25112;&#31532;&#20108;&#21517;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Imitation with Spatial-Temporal Heatmap: 2nd Place Solution for NuPlan Challenge. (arXiv:2306.15700v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;NuPlan&#25361;&#25112;&#36187;&#20013;&#30340;&#31532;&#20108;&#21517;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;-&#26102;&#38388;&#28909;&#21147;&#22270;&#39044;&#27979;&#26410;&#26469;&#22810;&#27169;&#24577;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#36712;&#36857;&#20248;&#21270;&#25216;&#26415;&#23454;&#29616;&#23433;&#20840;&#30340;&#33258;&#20027;&#39550;&#39542;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;NuPlan&#25361;&#25112;&#36187;2023&#20013;&#30340;&#31532;&#20108;&#21517;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#33258;&#20027;&#39550;&#39542;&#26159;&#38750;&#24120;&#22797;&#26434;&#21644;&#19981;&#30830;&#23450;&#30340;&#12290;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22330;&#26223;&#20013;&#23454;&#29616;&#23433;&#20840;&#35268;&#21010;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#31354;&#38388;-&#26102;&#38388;&#28909;&#21147;&#22270;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21019;&#26032;&#24615;&#22320;&#20351;&#29992;&#28909;&#21147;&#22270;&#34920;&#31034;&#39044;&#27979;&#26410;&#26469;&#30340;&#22810;&#27169;&#24577;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#36712;&#36857;&#20248;&#21270;&#25216;&#26415;&#26469;&#30830;&#20445;&#26368;&#32456;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#36710;&#36742;&#30340;&#36827;&#23637;&#21644;&#23433;&#20840;&#24615;&#65292;&#29983;&#25104;&#20102;&#23433;&#20840;&#21448;&#33298;&#36866;&#30340;&#36712;&#36857;&#12290;&#22312;NuPlan&#27604;&#36187;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#31532;&#20108;&#39640;&#30340;&#24635;&#20998;&#65292;&#21516;&#26102;&#22312;&#33258;&#25105;&#36827;&#23637;&#21644;&#33298;&#36866;&#24230;&#25351;&#26631;&#20013;&#33719;&#24471;&#20102;&#26368;&#20339;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our 2nd place solution for the NuPlan Challenge 2023. Autonomous driving in real-world scenarios is highly complex and uncertain. Achieving safe planning in the complex multimodal scenarios is a highly challenging task. Our approach, Imitation with Spatial-Temporal Heatmap, adopts the learning form of behavior cloning, innovatively predicts the future multimodal states with a heatmap representation, and uses trajectory refinement techniques to ensure final safety. The experiment shows that our method effectively balances the vehicle's progress and safety, generating safe and comfortable trajectories. In the NuPlan competition, we achieved the second highest overall score, while obtained the best scores in the ego progress and comfort metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;&#65292;&#20351;&#29992;&#21442;&#25968;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20026;&#30410;&#26234;&#28216;&#25103;Lily's Garden&#29983;&#25104;&#20851;&#21345;&#12290;&#34429;&#28982;GAN&#22312;&#36924;&#36817;&#22320;&#22270;&#24418;&#29366;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#36924;&#36817;&#26041;&#22359;&#20998;&#24067;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21487;&#33021;&#36890;&#36807;&#23581;&#35797;&#26367;&#20195;GAN&#30340;&#26550;&#26500;&#26469;&#25913;&#36827;&#36825;&#19968;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.15696</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#30410;&#26234;&#28216;&#25103;&#30340;&#36807;&#31243;&#21270;&#20869;&#23481;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Procedural content generation of puzzle games using conditional generative adversarial networks. (arXiv:2306.15696v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;&#65292;&#20351;&#29992;&#21442;&#25968;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20026;&#30410;&#26234;&#28216;&#25103;Lily's Garden&#29983;&#25104;&#20851;&#21345;&#12290;&#34429;&#28982;GAN&#22312;&#36924;&#36817;&#22320;&#22270;&#24418;&#29366;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#36924;&#36817;&#26041;&#22359;&#20998;&#24067;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21487;&#33021;&#36890;&#36807;&#23581;&#35797;&#26367;&#20195;GAN&#30340;&#26550;&#26500;&#26469;&#25913;&#36827;&#36825;&#19968;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20026;&#30410;&#26234;&#28216;&#25103;Lily's Garden&#29983;&#25104;&#20851;&#21345;&#30340;&#23454;&#39564;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#30495;&#23454;&#20851;&#21345;&#20013;&#25552;&#21462;&#20004;&#20010;&#26465;&#20214;&#21521;&#37327;&#65292;&#20197;&#25511;&#21046;GAN&#36755;&#20986;&#30340;&#32454;&#33410;&#12290;&#34429;&#28982;GAN&#22312;&#36924;&#36817;&#31532;&#19968;&#20010;&#26465;&#20214;&#65288;&#22320;&#22270;&#24418;&#29366;&#65289;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#36924;&#36817;&#31532;&#20108;&#20010;&#26465;&#20214;&#65288;&#26041;&#22359;&#20998;&#24067;&#65289;&#26041;&#38754;&#21364;&#26377;&#22256;&#38590;&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#23581;&#35797;&#26367;&#20195;GAN&#30340;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26550;&#26500;&#21487;&#33021;&#20250;&#25913;&#36827;&#36825;&#19968;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present an experimental approach to using parameterized Generative Adversarial Networks (GANs) to produce levels for the puzzle game Lily's Garden. We extract two condition vectors from the real levels in an effort to control the details of the GAN's outputs. While the GANs perform well in approximating the first condition (map shape), they struggle to approximate the second condition (piece distribution). We hypothesize that this might be improved by trying out alternative architectures for both the Generator and Discriminator of the GANs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#25299;&#25169;&#21644;&#28151;&#21512;&#33286;&#35770;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#25913;&#36827;&#32593;&#32476;&#21644;&#26356;&#26032;&#35268;&#21017;&#30340;&#21021;&#22987;&#20272;&#35745;&#65292;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15695</link><description>&lt;p&gt;
&#22522;&#20110;&#36172;&#21338;&#31639;&#27861;&#30340;&#32593;&#32476;&#25299;&#25169;&#21644;&#33286;&#35770;&#21160;&#21147;&#23398;&#30340;&#32852;&#21512;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Network Topology and Opinion Dynamics Based on Bandit Algorithms. (arXiv:2306.15695v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#25299;&#25169;&#21644;&#28151;&#21512;&#33286;&#35770;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#25913;&#36827;&#32593;&#32476;&#21644;&#26356;&#26032;&#35268;&#21017;&#30340;&#21021;&#22987;&#20272;&#35745;&#65292;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#28151;&#21512;&#33286;&#35770;&#21160;&#21147;&#23398;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#26356;&#26032;&#35268;&#21017;&#12290;&#36825;&#31181;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#20010;&#20307;&#20114;&#21160;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#20174;&#20960;&#20010;&#20505;&#36873;&#35268;&#21017;&#20013;&#25214;&#21040;&#27599;&#20010;&#20195;&#29702;&#20154;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#24182;&#23398;&#20064;&#24213;&#23618;&#32593;&#32476;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#31639;&#27861;&#20551;&#35774;&#27599;&#20010;&#20195;&#29702;&#20154;&#37117;&#26377;&#19968;&#20010;&#26356;&#26032;&#30340;&#35268;&#21017;&#65292;&#28982;&#21518;&#20462;&#25913;&#32593;&#32476;&#20272;&#35745;&#20197;&#20943;&#23569;&#39564;&#35777;&#35823;&#24046;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25913;&#36827;&#20102;&#32593;&#32476;&#21644;&#26356;&#26032;&#35268;&#21017;&#30340;&#21021;&#22987;&#20272;&#35745;&#65292;&#20943;&#23569;&#20102;&#39044;&#27979;&#35823;&#24046;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65289;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study joint learning of network topology and a mixed opinion dynamics, in which agents may have different update rules. Such a model captures the diversity of real individual interactions. We propose a learning algorithm based on multi-armed bandit algorithms to address the problem. The goal of the algorithm is to find each agent's update rule from several candidate rules and to learn the underlying network. At each iteration, the algorithm assumes that each agent has one of the updated rules and then modifies network estimates to reduce validation error. Numerical experiments show that the proposed algorithm improves initial estimates of the network and update rules, decreases prediction error, and performs better than other methods such as sparse linear regression and Gaussian process regression.
&lt;/p&gt;</description></item><item><title>Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15687</link><description>&lt;p&gt;
Voicebox&#65306;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15687
&lt;/p&gt;
&lt;p&gt;
Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;GPT&#21644;DALL-E&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25110;&#22270;&#20687;&#36755;&#20986;&#65292;&#32780;&#19988;&#36824;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#34987;&#26126;&#30830;&#25945;&#25480;&#30340;&#20219;&#21153;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#20219;&#21153;&#36890;&#29992;&#21270;&#26041;&#38754;&#20173;&#28982;&#27604;&#36739;&#21407;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Voicebox&#65292;&#36825;&#26159;&#26368;&#22810;&#21151;&#33021;&#30340;&#38754;&#21521;&#35268;&#27169;&#30340;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;Voicebox&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#38899;&#39057;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;50,000&#23567;&#26102;&#30340;&#26410;&#32463;&#36807;&#28388;&#25110;&#22686;&#24378;&#30340;&#35821;&#38899;&#36827;&#34892;&#22635;&#20805;&#12290;&#19982;GPT&#31867;&#20284;&#65292;Voicebox&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#22810;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#26356;&#21152;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#36824;&#21487;&#20197;&#23545;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;Voicebox&#21487;&#20197;&#29992;&#20110;&#21333;&#35821;&#25110;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;Voicebox
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models such as GPT and DALL-E have revolutionized natural language processing and computer vision research. These models not only generate high fidelity text or image outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#32467;&#21512;&#23454;&#39564;&#25968;&#25454;&#21644;&#33258;&#20030;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35780;&#20272;&#21333;&#20809;&#23376;&#28304;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#22810;&#20809;&#23376;&#21457;&#23556;&#20107;&#20214;&#27010;&#29575;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#36136;&#37327;&#35780;&#20272;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.15683</link><description>&lt;p&gt;
&#24555;&#36895;&#30830;&#23450;&#21333;&#20809;&#23376;&#28304;&#36136;&#37327;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Challenge of Quickly Determining the Quality of a Single-Photon Source. (arXiv:2306.15683v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15683
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#32467;&#21512;&#23454;&#39564;&#25968;&#25454;&#21644;&#33258;&#20030;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35780;&#20272;&#21333;&#20809;&#23376;&#28304;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#22810;&#20809;&#23376;&#21457;&#23556;&#20107;&#20214;&#27010;&#29575;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#36136;&#37327;&#35780;&#20272;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#29486;&#25552;&#20986;&#20102;&#26032;&#39062;&#26041;&#27861;&#26469;&#24555;&#36895;&#35780;&#20272;&#21333;&#20809;&#23376;&#28304;&#65288;SPS&#65289;&#30340;&#36136;&#37327;&#65292;&#20363;&#22914;&#37327;&#23376;&#28857;&#65292;&#20197;&#35299;&#20915;&#23454;&#39564;&#39564;&#35777;&#36890;&#36807;&#20809;&#24378;&#24178;&#28041;&#20202;&#39564;&#35777;&#25928;&#29575;&#20302;&#21644;&#32791;&#26102;&#38271;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#35752;&#35770;&#21644;&#21487;&#37325;&#22797;&#24615;&#32454;&#33410;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#24341;&#21457;&#20102;&#20851;&#20999;&#12290;&#26412;&#30740;&#31350;&#23545;&#20174;&#21457;&#23556;&#27874;&#38271;&#20026;1.3&#956;m&#65292;&#30001;80MHz&#28608;&#20809;&#22120;&#28608;&#21457;&#30340;InGaAs/GaAs&#22806;&#24310;&#37327;&#23376;&#28857;&#33719;&#24471;&#30340;&#20843;&#32452;&#25968;&#25454;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#23454;&#39564;&#25968;&#25454;&#19982;&#33258;&#20030;&#26679;&#26412;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36129;&#29486;&#12290;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#39640;&#25928;&#30452;&#26041;&#22270;&#25311;&#21512;&#23548;&#20986;&#30340;SPS&#36136;&#37327;&#25351;&#26631;&#65292;&#21363;&#22810;&#20809;&#23376;&#21457;&#23556;&#20107;&#20214;&#30340;&#27010;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30001;&#25551;&#36848;&#25506;&#27979;&#29575;&#30340;&#27850;&#26494;&#36807;&#31243;&#20013;&#38543;&#26426;&#21464;&#24322;&#24615;&#36129;&#29486;&#30340;&#26174;&#33879;&#19981;&#30830;&#23450;&#24615;&#12290;&#24573;&#35270;&#36825;&#20010;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#23545;SPS&#36136;&#37327;&#30340;&#38169;&#35823;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel methods for rapidly estimating single-photon source (SPS) quality, e.g. of quantum dots, have been promoted in recent literature to address the expensive and time-consuming nature of experimental validation via intensity interferometry. However, the frequent lack of uncertainty discussions and reproducible details raises concerns about their reliability. This study investigates one such proposal on eight datasets obtained from an InGaAs/GaAs epitaxial quantum dot that emits at 1.3 {\mu}m and is excited by an 80 MHz laser. The study introduces a novel contribution by employing data augmentation, a machine learning technique, to supplement experimental data with bootstrapped samples. Analysis of the SPS quality metric, i.e. the probability of multi-photon emission events, as derived from efficient histogram fitting of the synthetic samples, reveals significant uncertainty contributed by stochastic variability in the Poisson processes that describe detection rates. Ignoring this sou
&lt;/p&gt;</description></item><item><title>ECG-QA&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#24515;&#30005;&#22270;&#20998;&#26512;&#35774;&#35745;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#28085;&#30422;&#24191;&#27867;&#20020;&#24202;&#30456;&#20851;ECG&#20027;&#39064;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#22810;&#26679;&#21270;&#30340;ECG&#35299;&#35835;&#38382;&#39064;&#12290;&#36825;&#19968;&#36164;&#28304;&#23558;&#20026;&#26410;&#26469;&#30340;&#21307;&#30103;&#20445;&#20581;&#38382;&#31572;&#30740;&#31350;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.15681</link><description>&lt;p&gt;
ECG-QA&#65306;&#32467;&#21512;&#24515;&#30005;&#22270;&#30340;&#32508;&#21512;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram. (arXiv:2306.15681v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15681
&lt;/p&gt;
&lt;p&gt;
ECG-QA&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#24515;&#30005;&#22270;&#20998;&#26512;&#35774;&#35745;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#28085;&#30422;&#24191;&#27867;&#20020;&#24202;&#30456;&#20851;ECG&#20027;&#39064;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#22810;&#26679;&#21270;&#30340;ECG&#35299;&#35835;&#38382;&#39064;&#12290;&#36825;&#19968;&#36164;&#28304;&#23558;&#20026;&#26410;&#26469;&#30340;&#21307;&#30103;&#20445;&#20581;&#38382;&#31572;&#30740;&#31350;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#38382;&#31572;&#38382;&#39064;&#65288;QA&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21307;&#30103;&#20445;&#20581;QA&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#21307;&#23398;&#24433;&#20687;&#12289;&#20020;&#24202;&#35760;&#24405;&#25110;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#34920;&#19978;&#12290;&#36825;&#20351;&#24471;&#23558;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#19982;&#36825;&#20123;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#24040;&#22823;&#28508;&#21147;&#20960;&#20046;&#26410;&#34987;&#21033;&#29992;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ECG-QA&#65292;&#36825;&#26159;&#19987;&#38376;&#38024;&#23545;ECG&#20998;&#26512;&#35774;&#35745;&#30340;&#31532;&#19968;&#20010;QA&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20849;70&#20010;&#28085;&#30422;&#20102;&#24191;&#27867;&#20020;&#24202;&#30456;&#20851;ECG&#20027;&#39064;&#30340;&#38382;&#39064;&#27169;&#26495;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#32463;&#36807;&#19968;&#21517;ECG&#19987;&#23478;&#30340;&#39564;&#35777;&#65292;&#20197;&#30830;&#20445;&#20854;&#20020;&#24202;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#26679;&#21270;&#30340;ECG&#35299;&#35835;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;ECG&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35768;&#22810;&#23454;&#39564;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;ECG-QA&#23558;&#25104;&#20026;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20379;&#30740;&#31350;&#32773;&#20204;&#25506;&#32034;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#21442;&#25968;&#21270;&#30340;BRDFs&#65292;&#20026;&#33402;&#26415;&#24615;&#22320;&#21019;&#20316;3D&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15679</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#21442;&#25968;&#21270;BRDFs
&lt;/p&gt;
&lt;p&gt;
Generating Parametric BRDFs from Natural Language Descriptions. (arXiv:2306.15679v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15679
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#21442;&#25968;&#21270;&#30340;BRDFs&#65292;&#20026;&#33402;&#26415;&#24615;&#22320;&#21019;&#20316;3D&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#24615;&#22320;&#21019;&#20316;3D&#29615;&#22659;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#29087;&#32451;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35299;&#20915;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#29983;&#25104;&#32593;&#26684;&#12289;&#25490;&#21015;&#20960;&#20309;&#12289;&#21512;&#25104;&#32441;&#29702;&#31561;&#26041;&#38754;&#24050;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#21452;&#21521;&#21453;&#23556;&#20998;&#24067;&#20989;&#25968;&#65288;BRDFs&#65289;&#12290; BRDFs&#26159;&#34920;&#24449;&#20809;&#19982;&#34920;&#38754;&#26448;&#26009;&#30456;&#20114;&#20316;&#29992;&#30340;&#22235;&#32500;&#27010;&#29575;&#20998;&#24067;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#21442;&#25968;&#21270;&#30340;&#24418;&#24335;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#21015;&#20030;&#27599;&#23545;&#20837;&#23556;&#21644;&#20986;&#23556;&#35282;&#24230;&#30340;&#27010;&#29575;&#23494;&#24230;&#26469;&#34920;&#31034;&#12290;&#21069;&#32773;&#36866;&#29992;&#20110;&#33402;&#26415;&#32534;&#36753;&#65292;&#32780;&#21518;&#32773;&#21017;&#29992;&#20110;&#27979;&#37327;&#23454;&#38469;&#26448;&#26009;&#30340;&#22806;&#35266;&#12290;&#35768;&#22810;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20174;&#26448;&#26009;&#22270;&#20687;&#20551;&#35774;BRDF&#27169;&#22411;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#20174;&#26448;&#26009;&#30340;&#25991;&#26412;&#25551;&#36848;&#21040;&#21442;&#25968;&#21270;BRDF&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Artistic authoring of 3D environments is a laborious enterprise that also requires skilled content creators. There have been impressive improvements in using machine learning to address different aspects of generating 3D content, such as generating meshes, arranging geometry, synthesizing textures, etc. In this paper we develop a model to generate Bidirectional Reflectance Distribution Functions (BRDFs) from descriptive textual prompts. BRDFs are four dimensional probability distributions that characterize the interaction of light with surface materials. They are either represented parametrically, or by tabulating the probability density associated with every pair of incident and outgoing angles. The former lends itself to artistic editing while the latter is used when measuring the appearance of real materials. Numerous works have focused on hypothesizing BRDF models from images of materials. We learn a mapping from textual descriptions of materials to parametric BRDFs. Our model is f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KAPLA&#65292;&#19968;&#20010;&#29992;&#20110;&#21487;&#25193;&#23637;NN&#21152;&#36895;&#22120;&#25968;&#25454;&#27969;&#20248;&#21270;&#30340;&#24555;&#36895;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23454;&#29992;&#30340;&#25351;&#20196;&#21644;&#20840;&#38754;&#30340;&#25968;&#25454;&#27969;&#34920;&#31034;&#65292;KAPLA&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#35774;&#35745;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#24182;&#24555;&#36895;&#30830;&#23450;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.15676</link><description>&lt;p&gt;
KAPLA&#65306;&#21487;&#25193;&#23637;NN&#21152;&#36895;&#22120;&#25968;&#25454;&#27969;&#30340;&#23454;&#29992;&#21270;&#34920;&#31034;&#21644;&#24555;&#36895;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
KAPLA: Pragmatic Representation and Fast Solving of Scalable NN Accelerator Dataflow. (arXiv:2306.15676v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KAPLA&#65292;&#19968;&#20010;&#29992;&#20110;&#21487;&#25193;&#23637;NN&#21152;&#36895;&#22120;&#25968;&#25454;&#27969;&#20248;&#21270;&#30340;&#24555;&#36895;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23454;&#29992;&#30340;&#25351;&#20196;&#21644;&#20840;&#38754;&#30340;&#25968;&#25454;&#27969;&#34920;&#31034;&#65292;KAPLA&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#35774;&#35745;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#24182;&#24555;&#36895;&#30830;&#23450;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#35843;&#24230;&#20915;&#31574;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21152;&#36895;&#22120;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#21487;&#25193;&#23637;&#30340;NN&#21152;&#36895;&#22120;&#25903;&#25345;&#19968;&#32452;&#20016;&#23500;&#30340;&#20808;&#36827;&#25968;&#25454;&#27969;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#20840;&#38754;&#34920;&#31034;&#21644;&#24555;&#36895;&#25214;&#21040;&#20248;&#21270;&#30340;&#25968;&#25454;&#27969;&#26041;&#26696;&#30340;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#22810;&#33410;&#28857;NN&#26550;&#26500;&#19978;&#26102;&#38388;&#21644;&#31354;&#38388;&#35843;&#24230;&#30340;&#20840;&#38754;&#23454;&#29992;&#21270;&#25968;&#25454;&#27969;&#34920;&#31034;&#12290;&#19968;&#20221;&#38750;&#27491;&#24335;&#30340;&#20998;&#23618;&#20998;&#31867;&#34920;&#26126;&#65292;&#25968;&#25454;&#27969;&#31354;&#38388;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#30340;&#32039;&#23494;&#32806;&#21512;&#26159;&#24555;&#36895;&#35774;&#35745;&#25506;&#32034;&#30340;&#20027;&#35201;&#38590;&#28857;&#12290;&#19968;&#32452;&#24418;&#24335;&#21270;&#30340;&#24352;&#37327;&#20013;&#24515;&#25351;&#20196;&#20934;&#30830;&#22320;&#34920;&#31034;&#21508;&#31181;&#23618;&#38388;&#21644;&#23618;&#20869;&#26041;&#26696;&#65292;&#24182;&#20801;&#35768;&#24555;&#36895;&#30830;&#23450;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#20248;&#21270;&#30340;&#21644;&#24555;&#36895;&#30340;&#25968;&#25454;&#27969;&#27714;&#35299;&#22120;KAPLA&#65292;&#21033;&#29992;&#23454;&#29992;&#30340;&#25351;&#20196;&#26469;&#36827;&#34892;&#35774;&#35745;&#31354;&#38388;&#30340;&#26377;&#25928;&#26377;&#25928;&#24615;&#26816;&#26597;&#21644;&#20248;&#21270;&#26041;&#26696;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataflow scheduling decisions are of vital importance to neural network (NN) accelerators. Recent scalable NN accelerators support a rich set of advanced dataflow techniques. The problems of comprehensively representing and quickly finding optimized dataflow schemes thus become significantly more complicated and challenging. In this work, we first propose comprehensive and pragmatic dataflow representations for temporal and spatial scheduling on scalable multi-node NN architectures. An informal hierarchical taxonomy highlights the tight coupling across different levels of the dataflow space as the major difficulty for fast design exploration. A set of formal tensor-centric directives accurately express various inter-layer and intra-layer schemes, and allow for quickly determining their validity and efficiency. We then build a generic, optimized, and fast dataflow solver, KAPLA, which makes use of the pragmatic directives to explore the design space with effective validity check and eff
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.15632</link><description>&lt;p&gt;
&#24322;&#27493;&#31639;&#27861;&#19982;Cocycles&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;&#20294;&#26159;&#65292;&#20856;&#22411;&#30340;GNN&#22312;&#23450;&#20041;&#21644;&#35843;&#29992;&#28040;&#24687;&#20989;&#25968;&#20043;&#38388;&#27169;&#31946;&#20102;&#21306;&#21035;&#65292;&#36843;&#20351;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#37117;&#21521;&#20854;&#37051;&#23621;&#21457;&#36865;&#28040;&#24687;&#65292;&#21516;&#27493;&#22320;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;GNN&#24212;&#29992;&#20110;&#23398;&#20064;&#25191;&#34892;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26102;&#65292;&#22823;&#22810;&#25968;&#27493;&#39588;&#21482;&#26377;&#23569;&#25968;&#20960;&#20010;&#33410;&#28857;&#20250;&#26377;&#26377;&#24847;&#20041;&#30340;&#26356;&#26032;&#35201;&#21457;&#36865;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#21457;&#36865;&#22826;&#22810;&#26080;&#20851;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#23548;&#33268;&#20302;&#25928;&#29575;&#65292;&#32780;&#35768;&#22810;&#20013;&#38388;&#30340;GNN&#27493;&#39588;&#24517;&#39035;&#23398;&#20064;&#36523;&#20221;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#20998;&#31163;&#20102;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#36825;&#31181;&#20998;&#31163;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#25968;&#23398;&#34920;&#36798;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#24605;&#32771;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#27493;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15595</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15595
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#25554;&#20540;&#65288;PI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMA&#27169;&#22411;&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#19988;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#30340;&#21508;&#31181;&#20219;&#21153;&#65288;&#21253;&#25324;&#23494;&#38053;&#26816;&#32034;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31687;&#25991;&#26723;&#25688;&#35201;&#31561;&#65289;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#30340;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#30340;&#20219;&#21153;&#20013;&#30456;&#23545;&#20445;&#25345;&#33391;&#22909;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20301;&#32622;&#25554;&#20540;&#32447;&#24615;&#22320;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#20197;&#21305;&#37197;&#21407;&#22987;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#65292;&#32780;&#19981;&#26159;&#36229;&#36807;&#35757;&#32451;&#26102;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#23436;&#20840;&#30772;&#22351;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#25554;&#20540;&#30340;&#19978;&#30028;&#33267;&#23569;&#26159;&#25512;&#26029;&#30340;&#19978;&#30028;&#30340;$\sim 600 \times$&#35201;&#23567;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#26694;&#26550;&#30340;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20381;&#36182;&#21040;&#36798;&#26102;&#38388;&#24046;&#20449;&#24687;&#23454;&#29616;&#24494;&#27668;&#27873;&#30340;&#23450;&#20301;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15548</link><description>&lt;p&gt;
&#20960;&#20309;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;
&lt;/p&gt;
&lt;p&gt;
Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#26694;&#26550;&#30340;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20381;&#36182;&#21040;&#36798;&#26102;&#38388;&#24046;&#20449;&#24687;&#23454;&#29616;&#24494;&#27668;&#27873;&#30340;&#23450;&#20301;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#22686;&#24378;&#36229;&#22768;&#65288;CEUS&#65289;&#24050;&#25104;&#20026;&#26080;&#21019;&#21160;&#24577;&#21487;&#35270;&#21270;&#21307;&#23398;&#35786;&#26029;&#26041;&#27861;&#65292;&#28982;&#32780;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;&#65288;ULM&#65289;&#36890;&#36807;&#25552;&#20379;&#21313;&#20493;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#23454;&#29616;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24310;&#36831;&#21644;&#27714;&#21644;&#65288;DAS&#65289;&#27874;&#26463;&#24418;&#25104;&#22120;&#34987;&#29992;&#20110;&#28210;&#26579;ULM&#24103;&#65292;&#26368;&#32456;&#30830;&#23450;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;ULM&#65292;&#26412;&#30740;&#31350;&#36136;&#30097;&#27874;&#26463;&#24418;&#25104;&#26159;&#21542;&#26159;ULM&#26368;&#26377;&#25928;&#30340;&#22788;&#29702;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;TDoA&#65289;&#20449;&#24687;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#26925;&#22278;&#20132;&#28857;&#23450;&#20301;&#24494;&#27668;&#27873;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#29616;&#26377;&#27874;&#26463;&#24418;&#25104;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#20960;&#20309;ULM&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#20102;&#37096;&#20998;&#21487;&#29992;&#30340;&#25442;&#33021;&#22120;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound Localization Microscopy (ULM) has enabled a revolutionary breakthrough by offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers are used to render ULM frames, ultimately determining the image resolution capability. To take full advantage of ULM, this study questions whether beamforming is the most effective processing step for ULM, suggesting an alternative approach that relies solely on Time-Difference-of-Arrival (TDoA) information. To this end, a novel geometric framework for micro bubble localization via ellipse intersections is proposed to overcome existing beamforming limitations. We present a benchmark comparison based on a public dataset for which our geometric ULM outperforms existing baseline methods in terms of accuracy and reliability while only utilizing a portion of the available transducer data.
&lt;/p&gt;</description></item><item><title>MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15128</link><description>&lt;p&gt;
MIMIC: &#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15128
&lt;/p&gt;
&lt;p&gt;
MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20687;&#32032;&#32423;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#8212;&#8212;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#8212;&#8212;&#22914;&#20170;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31579;&#36873;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20165;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;3D&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#30456;&#26426;&#21442;&#25968;&#31579;&#36873;&#32780;&#26469;&#65292;&#24182;&#19981;&#20855;&#22791;&#22810;&#35270;&#35282;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#31579;&#36873;&#26426;&#21046;&#12290;&#25105;&#20204;&#20174;&#24320;&#28304;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#30340;3D&#29615;&#22659;&#20013;&#25366;&#25496;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;MIMIC-1M(&#21253;&#21547;1.3M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#21644;MIMIC-3M(&#21253;&#21547;3.1M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;MIMIC-3M&#35757;&#32451;&#30340;&#34920;&#31034;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#34920;&#38754;&#27861;&#32447;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#20197;Transformer&#20026;&#22522;&#30784;&#27169;&#22411;&#24182;&#32467;&#21512;&#35760;&#24518;&#65292;&#25193;&#23637;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#19982;ResNet50&#30456;&#32467;&#21512;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#30340;&#35748;&#30693;&#26550;&#26500;GAMR&#65292;&#23427;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#32452;&#21512;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#24182;&#20855;&#26377;&#23545;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.14650</link><description>&lt;p&gt;
&#21338;&#22763;&#35770;&#25991;&#65306;&#25506;&#32034;&#35748;&#30693;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26550;&#26500;&#20013;&#30340;(&#33258;&#25105;)&#27880;&#24847;&#21147;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture. (arXiv:2306.14650v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#20197;Transformer&#20026;&#22522;&#30784;&#27169;&#22411;&#24182;&#32467;&#21512;&#35760;&#24518;&#65292;&#25193;&#23637;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#19982;ResNet50&#30456;&#32467;&#21512;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#30340;&#35748;&#30693;&#26550;&#26500;GAMR&#65292;&#23427;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#32452;&#21512;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#24182;&#20855;&#26377;&#23545;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#22522;&#20110;Transformer&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#35760;&#24518;&#30456;&#32467;&#21512;&#26469;&#25193;&#23637;&#23427;&#12290;&#36890;&#36807;&#30740;&#31350;&#21512;&#25104;&#35270;&#35273;&#25512;&#29702;&#27979;&#35797;&#65292;&#25105;&#20204;&#23436;&#21892;&#20102;&#25512;&#29702;&#20219;&#21153;&#30340;&#20998;&#31867;&#27861;&#12290;&#36890;&#36807;&#23558;&#33258;&#25105;&#27880;&#24847;&#21147;&#19982;ResNet50&#32467;&#21512;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#22686;&#24378;&#29305;&#24449;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#30340;&#39640;&#25928;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#29702;&#35299;SVRT&#20219;&#21153;&#23545;&#27880;&#24847;&#21147;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GAMR&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#28789;&#24863;&#26469;&#33258;&#20027;&#21160;&#35270;&#35273;&#29702;&#35770;&#12290;GAMR&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#32452;&#21512;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#24182;&#22312;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of attention and memory in complex reasoning tasks. We analyze Transformer-based self-attention as a model and extend it with memory. By studying a synthetic visual reasoning test, we refine the taxonomy of reasoning tasks. Incorporating self-attention with ResNet50, we enhance feature maps using feature-based and spatial attention, achieving efficient solving of challenging visual reasoning tasks. Our findings contribute to understanding the attentional needs of SVRT tasks. Additionally, we propose GAMR, a cognitive architecture combining attention and memory, inspired by active vision theory. GAMR outperforms other architectures in sample efficiency, robustness, and compositionality, and shows zero-shot generalization on new reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#36755;&#20837;&#20013;&#36827;&#34892;&#30446;&#26631;&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.14325</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21453;&#21521;&#35268;&#21010;&#24341;&#25806;&#65288;NIPE&#65289;&#65306;&#22522;&#20110;&#35821;&#35328;&#36755;&#20837;&#30340;&#27010;&#29575;&#31038;&#20132;&#25512;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs. (arXiv:2306.14325v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#36755;&#20837;&#20013;&#36827;&#34892;&#30446;&#26631;&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#31038;&#20132;&#24615;&#30340;&#29983;&#29289;&#12290;&#25105;&#20204;&#32463;&#24120;&#25512;&#29702;&#20854;&#20182;&#26234;&#33021;&#20307;&#65292;&#32780;&#36825;&#31181;&#31038;&#20132;&#25512;&#29702;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#22312;&#20102;&#35299;&#20182;&#20204;&#30340;&#34892;&#20026;&#26102;&#25512;&#26029;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#35821;&#35328;&#25551;&#36848;&#30340;&#26234;&#33021;&#20307;&#12289;&#21160;&#20316;&#21644;&#32972;&#26223;&#29615;&#22659;&#20013;&#36827;&#34892;&#30452;&#35266;&#20294;&#21487;&#38752;&#30340;&#30446;&#26631;&#25512;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#27010;&#29575;&#30446;&#26631;&#25512;&#26029;&#39046;&#22495;&#20013;&#65292;&#35821;&#35328;&#39537;&#21160;&#21644;&#24433;&#21709;&#31038;&#20132;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#35821;&#35328;&#36755;&#20837;&#20013;&#36827;&#34892;&#30446;&#26631;&#25512;&#26029;&#12290;&#20854;&#20013;&#30340;&#8220;&#31070;&#32463;&#8221;&#37096;&#20998;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23558;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#20195;&#30721;&#34920;&#31034;&#65292;&#32780;&#8220;&#31526;&#21495;&#8221;&#37096;&#20998;&#21017;&#26159;&#19968;&#20010;&#36125;&#21494;&#26031;&#21453;&#21521;&#35268;&#21010;&#24341;&#25806;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#36827;&#34892;&#20102;&#19968;&#20010;&#20851;&#20110;&#35821;&#35328;&#30446;&#26631;&#25512;&#26029;&#20219;&#21153;&#30340;&#20154;&#31867;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#21453;&#24212;&#27169;&#24335;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#19988;&#27604;&#21333;&#29420;&#20351;&#29992;LLM&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#20154;&#31867;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human beings are social creatures. We routinely reason about other agents, and a crucial component of this social reasoning is inferring people's goals as we learn about their actions. In many settings, we can perform intuitive but reliable goal inference from language descriptions of agents, actions, and the background environments. In this paper, we study this process of language driving and influencing social reasoning in a probabilistic goal inference domain. We propose a neuro-symbolic model that carries out goal inference from linguistic inputs of agent scenarios. The "neuro" part is a large language model (LLM) that translates language descriptions to code representations, and the "symbolic" part is a Bayesian inverse planning engine. To test our model, we design and run a human experiment on a linguistic goal inference task. Our model closely matches human response patterns and better predicts human judgements than using an LLM alone.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;FedINIBoost&#65292;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21305;&#37197;&#26500;&#24314;&#26377;&#25928;&#30340;&#25552;&#21462;&#27169;&#22359;&#65292;&#22312;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.12088</link><description>&lt;p&gt;
&#19968;&#31181;&#20943;&#23569;&#32852;&#37030;&#23398;&#20064;&#36890;&#20449;&#30340;&#39640;&#25928;&#34394;&#25311;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning. (arXiv:2306.12088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;FedINIBoost&#65292;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21305;&#37197;&#26500;&#24314;&#26377;&#25928;&#30340;&#25552;&#21462;&#27169;&#22359;&#65292;&#22312;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#24320;&#38144;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#19968;&#20123;&#32463;&#20856;&#30340;&#26041;&#26696;&#20551;&#35774;&#26381;&#21153;&#22120;&#21487;&#20197;&#20174;&#26412;&#22320;&#27169;&#22411;&#20013;&#25552;&#21462;&#21442;&#19982;&#32773;&#35757;&#32451;&#25968;&#25454;&#30340;&#36741;&#21161;&#20449;&#24687;&#26469;&#26500;&#24314;&#20013;&#22830;&#34394;&#25311;&#25968;&#25454;&#38598;&#12290;&#26381;&#21153;&#22120;&#20351;&#29992;&#34394;&#25311;&#25968;&#25454;&#38598;&#26469;&#24494;&#35843;&#32858;&#21512;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20197;&#22312;&#36739;&#23569;&#30340;&#36890;&#20449;&#36718;&#27425;&#20869;&#36798;&#21040;&#30446;&#26631;&#27979;&#35797;&#31934;&#24230;&#12290;&#26412;&#25991;&#23558;&#19978;&#36848;&#35299;&#20915;&#26041;&#26696;&#27010;&#25324;&#20026;&#22522;&#20110;&#25968;&#25454;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25552;&#20986;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#35774;&#35745;&#19968;&#20010;&#26377;&#25928;&#30340;&#25552;&#21462;&#27169;&#22359;&#65288;EM&#65289;&#65292;&#23427;&#30830;&#20445;&#34394;&#25311;&#25968;&#25454;&#38598;&#23545;&#24494;&#35843;&#32858;&#21512;&#30340;&#20840;&#23616;&#27169;&#22411;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#22120;&#26469;&#35774;&#35745;EM&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;FedINIBoost&#20511;&#37492;&#20102;&#26799;&#24230;&#21305;&#37197;&#30340;&#24605;&#24819;&#26469;&#26500;&#24314;EM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedINIBoost&#22312;&#27599;&#20010;&#36890;&#20449;&#36718;&#27425;&#30340;&#27599;&#20010;&#21442;&#19982;&#32773;&#20013;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#26500;&#24314;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20195;&#29702;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#26381;&#21153;&#22120;&#32858;&#21512;&#25152;&#26377;&#30340;&#20195;&#29702;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#20013;&#22830;&#34394;&#25311;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication overhead is one of the major challenges in Federated Learning(FL). A few classical schemes assume the server can extract the auxiliary information about training data of the participants from the local models to construct a central dummy dataset. The server uses the dummy dataset to finetune aggregated global model to achieve the target test accuracy in fewer communication rounds. In this paper, we summarize the above solutions into a data-based communication-efficient FL framework. The key of the proposed framework is to design an efficient extraction module(EM) which ensures the dummy dataset has a positive effect on finetuning aggregated global model. Different from the existing methods that use generator to design EM, our proposed method, FedINIBoost borrows the idea of gradient match to construct EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two steps for each participant at each communication round. Then the server aggregates all the pr
&lt;/p&gt;</description></item><item><title>G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11667</link><description>&lt;p&gt;
G-NM&#65306;&#19968;&#32452;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11667
&lt;/p&gt;
&lt;p&gt;
G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#19968;&#20010;&#32508;&#21512;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#65292;&#32479;&#31216;&#20026;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#32452;&#65288;G-NM&#65289;&#12290;&#35813;&#38598;&#21512;&#21253;&#25324;&#20256;&#32479;&#27169;&#22411;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#65288;ARIMA&#65289;&#12289;Holt-Winters&#26041;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#20197;&#21450;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;G-NM&#26126;&#30830;&#26500;&#24314;&#20197;&#22686;&#24378;&#25105;&#20204;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#22266;&#26377;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#36825;&#20123;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;G-NM&#20415;&#20110;&#23545;&#27492;&#31867;&#29616;&#35937;&#22312;&#24310;&#38271;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#25105;&#20204;&#23545;&#27492;&#31867;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#24182;&#22823;&#24133;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;G-NM&#21253;&#25324;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#23395;&#33410;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#38382;&#39064;&#21644;&#39118;&#38505;&#25935;&#24863;MDP&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.11626</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#40065;&#26834;&#30340;MDPs&#21644;&#39118;&#38505;&#25935;&#24863;&#30340;MDPs&#65306;&#31561;&#20215;&#24615;&#12289;&#31574;&#30053;&#26799;&#24230;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity. (arXiv:2306.11626v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#38382;&#39064;&#21644;&#39118;&#38505;&#25935;&#24863;MDP&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20851;&#27880;&#20110;&#27491;&#21017;&#21270;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#23427;&#26159;&#40065;&#26834;MDP&#26694;&#26550;&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#39118;&#38505;&#25935;&#24863;MDP&#65292;&#24182;&#24314;&#31435;&#20102;&#39118;&#38505;&#25935;&#24863;MDP&#21644;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#36825;&#31181;&#31561;&#20215;&#24615;&#20026;&#35299;&#20915;&#27491;&#21017;&#21270;RMDP&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#19988;&#20351;&#24471;&#35774;&#35745;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#31181;&#31561;&#20215;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#24182;&#22312;&#20855;&#26377;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#34920;&#26684;&#35774;&#32622;&#19979;&#35777;&#26126;&#20102;&#31934;&#30830;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#40065;&#26834;&#30340;FZI&#36845;&#20195;&#65292;&#29992;&#20110;&#20855;&#26377;KL&#25955;&#24230;&#27491;&#21017;&#21270;&#39033;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#24471;&#21040;&#20102;&#25968;&#20540;&#27169;&#25311;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on reinforcement learning for the regularized robust Markov decision process (MDP) problem, an extension of the robust MDP framework. We first introduce the risk-sensitive MDP and establish the equivalence between risk-sensitive MDP and regularized robust MDP. This equivalence offers an alternative perspective for addressing the regularized RMDP and enables the design of efficient learning algorithms. Given this equivalence, we further derive the policy gradient theorem for the regularized robust MDP problem and prove the global convergence of the exact policy gradient method under the tabular setting with direct parameterization. We also propose a sample-based offline learning algorithm, namely the robust fitted-Z iteration (RFZI), for a specific regularized robust MDP problem with a KL-divergence regularization term and analyze the sample complexity of the algorithm. Our results are also supported by numerical simulations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32858;&#21512;&#35266;&#27979;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#23454;&#20363;&#30340;&#27599;&#20010;&#26631;&#31614;&#26435;&#34913;&#37325;&#35201;&#24615;&#65292;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#32431;&#21270;&#30340;&#30417;&#30563;&#26469;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20219;&#24847;&#25439;&#22833;&#30340;&#20998;&#31867;&#39118;&#38505;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.11343</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32858;&#21512;&#35266;&#27979;&#30340;&#20998;&#31867;&#30340;&#36890;&#29992;&#26080;&#20559;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Universal Unbiased Method for Classification from Aggregate Observations. (arXiv:2306.11343v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11343
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32858;&#21512;&#35266;&#27979;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#23454;&#20363;&#30340;&#27599;&#20010;&#26631;&#31614;&#26435;&#34913;&#37325;&#35201;&#24615;&#65292;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#32431;&#21270;&#30340;&#30417;&#30563;&#26469;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20219;&#24847;&#25439;&#22833;&#30340;&#20998;&#31867;&#39118;&#38505;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#30417;&#30563;&#20998;&#31867;&#20013;&#65292;&#38656;&#35201;&#23545;&#20010;&#20307;&#23454;&#20363;&#36827;&#34892;&#30495;&#23454;&#26631;&#31614;&#30340;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#20986;&#20110;&#38544;&#31169;&#21644;&#26114;&#36149;&#30340;&#27880;&#37322;&#25104;&#26412;&#31561;&#21407;&#22240;&#65292;&#20026;&#20010;&#20307;&#23454;&#20363;&#25910;&#38598;&#30495;&#23454;&#26631;&#31614;&#21487;&#33021;&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#28608;&#21457;&#20102;&#23545;&#22522;&#20110;&#32858;&#21512;&#35266;&#27979;&#30340;&#20998;&#31867;&#65288;CFAO&#65289;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#30417;&#30563;&#26159;&#20197;&#32452;&#30340;&#24418;&#24335;&#25552;&#20379;&#30340;&#65292;&#32780;&#19981;&#26159;&#20010;&#20307;&#23454;&#20363;&#12290;CFAO&#26159;&#19968;&#20010;&#24191;&#20041;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#65292;&#22914;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#20174;&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;CFAO&#36890;&#29992;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20219;&#24847;&#25439;&#22833;&#30340;&#20998;&#31867;&#39118;&#38505;&#30340;&#26080;&#20559;&#20272;&#35745;&#22120;--&#20197;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26435;&#34913;&#32676;&#32452;&#20013;&#27599;&#20010;&#23454;&#20363;&#30340;&#27599;&#20010;&#26631;&#31614;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#32431;&#21270;&#30340;&#30417;&#30563;&#26469;&#23398;&#20064;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#35777;&#20102;&#39118;&#38505;&#30340;&#25511;&#21046;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#30830;&#23450;&#39118;&#38505;&#19978;&#30028;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conventional supervised classification, true labels are required for individual instances. However, it could be prohibitive to collect the true labels for individual instances, due to privacy concerns or unaffordable annotation costs. This motivates the study on classification from aggregate observations (CFAO), where the supervision is provided to groups of instances, instead of individual instances. CFAO is a generalized learning framework that contains various learning problems, such as multiple-instance learning and learning from label proportions. The goal of this paper is to present a novel universal method of CFAO, which holds an unbiased estimator of the classification risk for arbitrary losses -- previous research failed to achieve this goal. Practically, our method works by weighing the importance of each label for each instance in the group, which provides purified supervision for the classifier to learn. Theoretically, our proposed method not only guarantees the risk con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10946</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#30456;&#23545;&#25104;&#29087;&#38454;&#27573;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#33616;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#26053;&#28216;&#39046;&#22495;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#26053;&#28216;&#26223;&#28857;&#23646;&#24615;&#27969;&#31243;&#20316;&#20026;&#25512;&#33616;&#22522;&#30784;&#36739;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(Att-KGCN)&#65292;&#33258;&#21160;&#35821;&#20041;&#22320;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#23558;&#30456;&#23545;&#30456;&#20284;&#30340;&#20301;&#32622;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#26053;&#23458;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#32034;&#31185;&#29305;&#25289;&#23707;-&#20063;&#38376;&#30340;&#26053;&#28216;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#26053;&#28216;&#39046;&#22495;&#30340;&#26223;&#28857;&#25512;&#33616;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09364</link><description>&lt;p&gt;
TSMixer: &#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09364
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22240;&#20854;&#33021;&#22815;&#25429;&#25417;&#38271;&#24207;&#21015;&#20132;&#20114;&#32780;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#23545;&#38271;&#26399;&#39044;&#27979;&#26500;&#25104;&#20102;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TSMixer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#26550;&#26500;&#65292;&#19987;&#20026;&#22810;&#20803;&#39044;&#27979;&#21644;&#34917;&#19969;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#26159;Transformers&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20511;&#37492;&#20102;MLP-Mixer&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25104;&#21151;&#32463;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#35270;&#35273;MLP-Mixer&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#32452;&#20214;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#21363;&#23558;&#22312;&#32447;&#21327;&#35843;&#22836;&#38468;&#21152;&#21040;MLP-Mixer&#39592;&#24178;&#19978;&#65292;&#20197;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#23646;&#24615;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36890;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;&#32534;&#30721;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#36890;&#36947;&#21644;&#20445;&#30041;&#21333;&#20010;&#36890;&#36947;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TSMixer&#22312;&#19968;&#20803;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#22522;&#20110;Transformers&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24102;&#26631;&#31614;&#30340;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#31561;&#24335;&#32422;&#26463;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;DeepLDE&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06674</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#20248;&#21270;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization. (arXiv:2306.06674v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24102;&#26631;&#31614;&#30340;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#31561;&#24335;&#32422;&#26463;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;DeepLDE&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#20256;&#32479;&#27714;&#35299;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#37327;&#36739;&#22823;&#65292;&#29305;&#21035;&#26159;&#22312;&#35268;&#27169;&#36739;&#22823;&#12289;&#26102;&#38388;&#25935;&#24863;&#30340;&#38382;&#39064;&#19978;&#26356;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#24555;&#36895;&#26368;&#20248;&#35299;&#36924;&#36817;&#22120;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22823;&#20852;&#36259;&#65292;&#20294;&#26159;&#23558;&#32422;&#26463;&#26465;&#20214;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DeepLDE&#30340;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#22312;&#19981;&#20351;&#29992;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23547;&#25214;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23558;&#31561;&#24335;&#32422;&#26463;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#23545;&#19981;&#31561;&#24335;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DeepLDE&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#20165;&#38752;&#21407;&#22987;-&#23545;&#20598;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#30830;&#20445;&#31561;&#24335;&#32422;&#26463;&#65292;&#38656;&#35201;&#31561;&#24335;&#23884;&#20837;&#30340;&#24110;&#21161;&#12290;&#22312;&#20984;&#12289;&#38750;&#20984;&#21644;&#20132;&#27969;&#26368;&#20248;&#28526;&#27969;&#65288;AC-OPF&#65289;&#38382;&#39064;&#30340;&#27169;&#25311;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DeepLDE&#30340;&#26368;&#20248;&#24615;&#33021;&#32780;&#19988;&#22987;&#32456;&#20445;&#35777;&#21487;&#34892;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional solvers are often computationally expensive for constrained optimization, particularly in large-scale and time-critical problems. While this leads to a growing interest in using neural networks (NNs) as fast optimal solution approximators, incorporating the constraints with NNs is challenging. In this regard, we propose deep Lagrange dual with equality embedding (DeepLDE), a framework that learns to find an optimal solution without using labels. To ensure feasible solutions, we embed equality constraints into the NNs and train the NNs using the primal-dual method to impose inequality constraints. Furthermore, we prove the convergence of DeepLDE and show that the primal-dual learning method alone cannot ensure equality constraints without the help of equality embedding. Simulation results on convex, non-convex, and AC optimal power flow (AC-OPF) problems show that the proposed DeepLDE achieves the smallest optimality gap among all the NN-based approaches while always ensuri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.06238</link><description>&lt;p&gt;
&#29702;&#35299;&#38271;&#23614;&#25928;&#24212;&#23545;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#21387;&#32553;&#29616;&#22312;&#26159;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#19968;&#20010;&#25104;&#29087;&#30340;&#23376;&#39046;&#22495;&#65292;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#21152;&#36895;&#25512;&#26029;&#20026;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#20165;&#20851;&#27880;&#24635;&#20307;&#20934;&#30830;&#24615;&#21487;&#33021;&#26159;&#35823;&#23548;&#30340;&#12290;&#20363;&#22914;&#65292;&#24050;&#32463;&#35777;&#26126;&#20840;&#27169;&#22411;&#21644;&#21387;&#32553;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#20302;&#39057;&#30340;&#31867;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#8220;&#25105;&#20204;&#33021;&#21542;&#22312;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#35821;&#20041;&#31561;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32593;&#32476;&#21387;&#32553;&#65311;&#8221;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;Feldman&#31561;&#20154;&#35266;&#23519;&#21040;&#30340;&#8220;&#38271;&#23614;&#8221;&#29616;&#35937;&#12290;&#20182;&#20204;&#35748;&#20026;&#65292;&#26576;&#20123;&#36755;&#20837;&#65288;&#36866;&#24403;&#23450;&#20041;&#65289;&#30340;&#35760;&#24518;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#30001;&#20110;&#21387;&#32553;&#38480;&#21046;&#20102;&#32593;&#32476;&#30340;&#23481;&#37327;&#65288;&#22240;&#27492;&#20063;&#38480;&#21046;&#20102;&#20854;&#35760;&#24518;&#33021;&#21147;&#65289;&#65292;&#25152;&#20197;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#8221;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#23545;&#22522;&#22240;&#32452;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#24182;&#21457;&#29616;&#22522;&#22240;&#35843;&#25511;&#30340;&#23618;&#27425;&#20381;&#36182;&#20851;&#31995;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05143</link><description>&lt;p&gt;
&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#65306;&#19968;&#31181;&#24102;&#26377;1D&#31227;&#21160;&#31383;&#21475;&#21464;&#25442;&#22120;&#30340;&#23618;&#27425;&#22522;&#22240;&#32452;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer. (arXiv:2306.05143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#8221;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#23545;&#22522;&#22240;&#32452;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#24182;&#21457;&#29616;&#22522;&#22240;&#35843;&#25511;&#30340;&#23618;&#27425;&#20381;&#36182;&#20851;&#31995;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#22240;&#32452;&#25968;&#25454;&#37327;&#21644;&#36136;&#37327;&#30340;&#22686;&#21152;&#65292;&#25552;&#21462;&#26032;&#30340;&#27934;&#35265;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#22240;&#32452;&#27979;&#23450;&#39044;&#27979;&#32467;&#26500;&#65306;&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#12290;&#35813;&#27169;&#22411;&#22312;&#22522;&#22240;&#32452;&#27979;&#23450;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;&#22522;&#22240;&#32452;&#20301;&#28857;&#30340;&#23618;&#27425;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26159;&#36890;&#36807;1D-Swin&#36827;&#34892;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#38271;&#33539;&#22260;&#23618;&#27425;&#25968;&#25454;&#30340;&#26032;&#22411;&#21464;&#25442;&#22120;&#22359;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;38,171&#20010;17K&#30897;&#22522;&#23545;&#30340;DNA&#29255;&#27573;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#22312;&#26579;&#33394;&#36136;&#21487;&#36798;&#24615;&#21644;&#22522;&#22240;&#34920;&#36798;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#22522;&#22240;&#35843;&#25511;&#30340;&#28508;&#22312;&#8220;&#35821;&#27861;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the increasing volume and quality of genomics data, extracting new insights requires interpretable machine-learning models. This work presents Genomic Interpreter: a novel architecture for genomic assay prediction. This model outperforms the state-of-the-art models for genomic assay prediction tasks. Our model can identify hierarchical dependencies in genomic sites. This is achieved through the integration of 1D-Swin, a novel Transformer-based block designed by us for modelling long-range hierarchical data. Evaluated on a dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter demonstrates superior performance in chromatin accessibility and gene expression prediction and unmasks the underlying `syntax' of gene regulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04502</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21487;&#38752;&#21644;&#39640;&#24615;&#33021;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21363;&#20415;&#26159;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20063;&#20250;&#21253;&#21547;&#38169;&#35823;&#65292;&#26356;&#19981;&#29992;&#35828;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20102;&#12290;&#29616;&#26377;&#30340;&#19968;&#20123;&#25968;&#25454;&#21435;&#22122;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26816;&#27979;&#24322;&#24120;&#20540;&#24182;&#36827;&#34892;&#27704;&#20037;&#24615;&#21435;&#38500;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#36807;&#24230;&#25110;&#32773;&#27424;&#24230;&#36807;&#28388;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65288;AGRA&#65289;&#65292;&#19981;&#21516;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#28165;&#27927;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#32452;&#26679;&#26412;&#30340;&#32047;&#31215;&#26799;&#24230;&#21644;&#21333;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#22312;&#24403;&#21069;&#26356;&#26032;&#26102;&#20445;&#30041;&#23545;&#24212;&#30340;&#26679;&#26412;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#23427;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AGRA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20840;&#38754;&#30340;&#32467;&#26524;&#20998;&#26512;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01843</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Training of Autoencoders. (arXiv:2306.01843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#20855;&#26377;&#20248;&#24322;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#20013;&#38750;&#24120;&#27969;&#34892;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;&#26377;&#26395;&#27604;&#24402;&#19968;&#21270;&#27969;&#26356;&#39640;&#25928;&#12290;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#30340;&#25104;&#21151;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#23558;&#36825;&#20004;&#31181;&#33539;&#24335;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35782;&#21035;&#24182;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#29616;&#26377;&#30340;&#33258;&#30001;&#26684;&#24335;&#32593;&#32476;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#36807;&#20110;&#32531;&#24930;&#65292;&#20381;&#36182;&#20110;&#36845;&#20195;&#26041;&#26696;&#65292;&#20854;&#25104;&#26412;&#38543;&#28508;&#22312;&#32500;&#24230;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#20272;&#35745;&#22120;&#65292;&#28040;&#38500;&#20102;&#36845;&#20195;&#65292;&#20174;&#32780;&#20351;&#25104;&#26412;&#20445;&#25345;&#19981;&#21464;&#65288;&#27599;&#20010;&#25209;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#22823;&#32422;&#26159;&#26222;&#36890;&#33258;&#32534;&#30721;&#22120;&#30340;&#20004;&#20493;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#26420;&#32032;&#22320;&#23558;&#26368;&#22823;&#20284;&#28982;&#24212;&#29992;&#20110;&#33258;&#32534;&#30721;&#22120;&#21487;&#33021;&#23548;&#33268;&#21457;&#25955;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#24819;&#27861;&#26469;&#25512;&#21160;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#29983;&#25104;&#22270;&#20687;&#12289;&#25554;&#20540;&#21644;&#21464;&#25442;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood training has favorable statistical properties and is popular for generative modeling, especially with normalizing flows. On the other hand, generative autoencoders promise to be more efficient than normalizing flows due to the manifold hypothesis. In this work, we introduce successful maximum likelihood training of unconstrained autoencoders for the first time, bringing the two paradigms together. To do so, we identify and overcome two challenges: Firstly, existing maximum likelihood estimators for free-form networks are unacceptably slow, relying on iteration schemes whose cost scales linearly with latent dimension. We introduce an improved estimator which eliminates iteration, resulting in constant cost (roughly double the runtime per batch of a vanilla autoencoder). Secondly, we demonstrate that naively applying maximum likelihood to autoencoders can lead to divergent solutions and use this insight to motivate a stable maximum likelihood training objective. We per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00074</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#40784;&#26657;&#20934;&#29992;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26102;&#65292;&#23427;&#36890;&#24120;&#25552;&#20379;&#26631;&#31614;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20540;&#12290;&#28982;&#21518;&#65292;&#20915;&#31574;&#32773;&#24212;&#20351;&#29992;&#32622;&#20449;&#24230;&#20540;&#26469;&#26657;&#20934;&#23545;&#39044;&#27979;&#30340;&#20449;&#20219;&#31243;&#24230;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#32463;&#24120;&#35748;&#20026;&#32622;&#20449;&#24230;&#20540;&#24212;&#23545;&#39044;&#27979;&#26631;&#31614;&#19982;&#23454;&#38469;&#26631;&#31614;&#21305;&#37197;&#30340;&#27010;&#29575;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22810;&#26465;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20915;&#31574;&#32773;&#38590;&#20197;&#20351;&#29992;&#36825;&#20123;&#32622;&#20449;&#24230;&#20540;&#24456;&#22909;&#22320;&#30830;&#23450;&#20309;&#26102;&#20449;&#20219;&#39044;&#27979;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#39318;&#20808;&#26159;&#29702;&#35299;&#20026;&#20160;&#20040;&#65292;&#28982;&#21518;&#30740;&#31350;&#22914;&#20309;&#26500;&#24314;&#26356;&#26377;&#29992;&#30340;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#39318;&#20808;&#35748;&#20026;&#65292;&#22312;&#24191;&#27867;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20013;&#65292;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#65292;&#23545;&#20110;&#36825;&#20123;&#20998;&#24067;&#65292;&#29702;&#24615;&#20915;&#31574;&#32773;&#36890;&#24120;&#38590;&#20197;&#20351;&#29992;&#20197;&#19978;&#32622;&#20449;&#24230;&#20540;&#21457;&#29616;&#26368;&#20339;&#20915;&#31574;&#25919;&#31574;&#8212;&#8212;&#26368;&#20339;&#30340;&#20915;&#31574;&#32773;&#38656;&#35201;&#20154;&#31867;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20182;&#20204;&#22312;&#25152;&#38754;&#20020;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#20915;&#31574;&#19978;&#30340;&#20010;&#20154;&#20559;&#22909;&#30340;&#26032;&#26041;&#27861;&#26469;&#26500;&#36896;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#32622;&#20449;&#24230;&#20540;&#27604;&#20351;&#29992;&#26631;&#20934;&#32622;&#20449;&#24230;&#24230;&#37327;&#23548;&#33268;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#20272;&#35745;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#24555;&#30340;&#22870;&#21169;&#31215;&#32047;&#12290;</title><link>http://arxiv.org/abs/2305.19535</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32447;&#23398;&#20064;&#30340;&#20302;&#31209;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-rank extended Kalman filtering for online learning of neural networks from streaming data. (arXiv:2305.19535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#20272;&#35745;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#24555;&#30340;&#22870;&#21169;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#21487;&#33021;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#20013;&#20272;&#35745;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;EKF&#65289;&#65292;&#20294;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#30340;&#21518;&#39564;&#31934;&#24230;&#30697;&#38453;&#20998;&#35299;&#65292;&#20854;&#27599;&#27493;&#30340;&#25104;&#26412;&#19982;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#19982;&#22522;&#20110;&#38543;&#26426;&#21464;&#20998;&#25512;&#29702;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#30830;&#23450;&#30340;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#27493;&#38271;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#23548;&#33268;&#26356;&#24555;&#65288;&#26356;&#39640;&#25928;&#65289;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#29992;&#20316;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#30340;&#19968;&#37096;&#20998;&#26102;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#24555;&#30340;&#22870;&#21169;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient online approximate Bayesian inference algorithm for estimating the parameters of a nonlinear function from a potentially non-stationary data stream. The method is based on the extended Kalman filter (EKF), but uses a novel low-rank plus diagonal decomposition of the posterior precision matrix, which gives a cost per step which is linear in the number of model parameters. In contrast to methods based on stochastic variational inference, our method is fully deterministic, and does not require step-size tuning. We show experimentally that this results in much faster (more sample efficient) learning, which results in more rapid adaptation to changing distributions, and faster accumulation of reward when used as part of a contextual bandit algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#27969;&#24418;&#37319;&#26679;&#28857;&#26500;&#36896;&#30340;&#22270;&#19982;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#22270;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;&#36830;&#32493;&#27969;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#28388;&#27874;&#22120;&#30340;&#21487;&#20998;&#24615;&#21644;&#36817;&#20284;&#27969;&#24418;&#28388;&#27874;&#22120;&#25152;&#38656;&#34892;&#20026;&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#26435;&#34913;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#20110;&#38750;&#32447;&#24615;&#30340;&#39057;&#29575;&#28151;&#21512;&#23646;&#24615;&#32780;&#24471;&#21040;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.18467</link><description>&lt;p&gt;
&#20960;&#20309;&#22270;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#65306;&#26497;&#38480;&#24615;&#36136;&#21644;&#21028;&#21035;&#24230;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Geometric Graph Filters and Neural Networks: Limit Properties and Discriminability Trade-offs. (arXiv:2305.18467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#27969;&#24418;&#37319;&#26679;&#28857;&#26500;&#36896;&#30340;&#22270;&#19982;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#22270;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;&#36830;&#32493;&#27969;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#28388;&#27874;&#22120;&#30340;&#21487;&#20998;&#24615;&#21644;&#36817;&#20284;&#27969;&#24418;&#28388;&#27874;&#22120;&#25152;&#38656;&#34892;&#20026;&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#26435;&#34913;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#20110;&#38750;&#32447;&#24615;&#30340;&#39057;&#29575;&#28151;&#21512;&#23646;&#24615;&#32780;&#24471;&#21040;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;MNN&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24403;&#22270;&#26159;&#20174;&#27969;&#24418;&#37319;&#26679;&#28857;&#26500;&#36896;&#32780;&#25104;&#26102;&#65292;&#20174;&#32780;&#32534;&#30721;&#20960;&#20309;&#20449;&#24687;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21367;&#31215;MNN&#21644;GNN&#65292;&#20854;&#20013;&#27969;&#24418;&#21644;&#22270;&#21367;&#31215;&#20998;&#21035;&#20197;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#21644;&#22270;Laplacian&#20026;&#23450;&#20041;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#26680;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23494;&#38598;&#21644;&#20013;&#31561;&#31232;&#30095;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#65292;&#34920;&#26126;&#36825;&#20123;&#22270;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;&#36830;&#32493;&#27969;&#24418;&#19978;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#36825;&#20010;&#20998;&#26512;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#22270;&#28388;&#27874;&#22120;&#30340;&#21487;&#20998;&#24615;&#21644;&#36817;&#20284;&#27969;&#24418;&#28388;&#27874;&#22120;&#25152;&#38656;&#34892;&#20026;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#37325;&#35201;&#26435;&#34913;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#26435;&#34913;&#22914;&#20309;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#20110;&#38750;&#32447;&#24615;&#30340;&#39057;&#29575;&#28151;&#21512;&#23646;&#24615;&#32780;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the relationship between a graph neural network (GNN) and a manifold neural network (MNN) when the graph is constructed from a set of points sampled from the manifold, thus encoding geometric information. We consider convolutional MNNs and GNNs where the manifold and the graph convolutions are respectively defined in terms of the Laplace-Beltrami operator and the graph Laplacian. Using the appropriate kernels, we analyze both dense and moderately sparse graphs. We prove non-asymptotic error bounds showing that convolutional filters and neural networks on these graphs converge to convolutional filters and neural networks on the continuous manifold. As a byproduct of this analysis, we observe an important trade-off between the discriminability of graph filters and their ability to approximate the desired behavior of manifold filters. We then discuss how this trade-off is ameliorated in neural networks due to the frequency mixing property of nonlinearities. We further d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20869;&#23384;&#21463;&#21040;&#26497;&#31471;&#38480;&#21046;&#26102;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.18418</link><description>&lt;p&gt;
&#19968;&#30629;&#65306;&#37325;&#26032;&#24605;&#32771;&#35270;&#39057;&#19981;&#26029;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Just a Glimpse: Rethinking Temporal Information for Video Continual Learning. (arXiv:2305.18418v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20869;&#23384;&#21463;&#21040;&#26497;&#31471;&#38480;&#21046;&#26102;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26159;&#36830;&#32493;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#37325;&#35201;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#23494;&#20999;&#30456;&#20851;&#12290;&#38543;&#30528;&#31867;&#21035;/&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#21463;&#21040;&#20869;&#23384;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#20250;&#20986;&#29616;&#12290;&#22312;&#35270;&#39057;&#39046;&#22495;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35270;&#39057;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#24103;&#65292;&#36825;&#20250;&#20351;&#22238;&#25918;&#35760;&#24518;&#36127;&#25285;&#26356;&#37325;&#12290;&#30446;&#21069;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#20174;&#35270;&#39057;&#27969;&#20013;&#23545;&#24103;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#22238;&#25918;&#35760;&#24518;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26497;&#31471;&#20869;&#23384;&#38480;&#21046;&#19979;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20174;&#20195;&#34920;&#22823;&#37327;&#29420;&#29305;&#35270;&#39057;&#30340;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#35270;&#39057;&#25968;&#25454;&#38598;Kin&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning is one of the most important settings for the study of Continual Learning, as it closely resembles real-world application scenarios. With constrained memory sizes, catastrophic forgetting arises as the number of classes/tasks increases. Studying continual learning in the video domain poses even more challenges, as video data contains a large number of frames, which places a higher burden on the replay memory. The current common practice is to sub-sample frames from the video stream and store them in the replay memory. In this paper, we propose SMILE a novel replay mechanism for effective video continual learning based on individual/single frames. Through extensive experimentation, we show that under extreme memory constraints, video diversity plays a more significant role than temporal information. Therefore, our method focuses on learning from a small number of frames that represent a large number of unique videos. On three representative video datasets, Kin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15408</link><description>&lt;p&gt;
&#20174;&#29702;&#35770;&#35282;&#24230;&#25581;&#31034;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;"&#24605;&#32500;&#38142;"&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#25968;&#23398;&#25110;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#26426;&#21046;&#20197;&#21450;&#23427;&#22914;&#20309;&#37322;&#25918;LLMs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#31070;&#31192;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20219;&#20309;&#26377;&#38480;&#28145;&#24230;&#30340;Transformer&#37117;&#19981;&#33021;&#30452;&#25509;&#36755;&#20986;&#27491;&#30830;&#30340;&#22522;&#26412;&#31639;&#26415;/&#26041;&#31243;&#20219;&#21153;&#30340;&#31572;&#26696;&#65292;&#38500;&#38750;&#27169;&#22411;&#22823;&#23567;&#38543;&#30528;&#36755;&#20837;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#36229;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#65292;&#22823;&#23567;&#24658;&#23450;&#30340;&#33258;&#22238;&#24402;Transformer&#36275;&#20197;&#36890;&#36807;&#20351;&#29992;&#24120;&#29992;&#30340;&#25968;&#23398;&#35821;&#35328;&#24418;&#24335;&#29983;&#25104;&#8220;&#24605;&#32500;&#38142;&#8221;&#25512;&#23548;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.12809</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#26469;&#32763;&#36716;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yang&#31561;&#20154;&#21457;&#29616;&#65292;&#20165;&#21024;&#38500;1%&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#32763;&#36716;&#12290;&#37492;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#30340;&#26222;&#36941;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#19968;&#20010;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#21487;&#21542;&#23548;&#33268;&#27979;&#35797;&#32467;&#26524;&#32763;&#36716;&#65311;&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#36825;&#31181;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22987;&#32456;&#33021;&#22815;&#20135;&#29983;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26426;&#21046;&#26377;&#22810;&#37325;&#20316;&#29992;&#65306;&#65288;1&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24674;&#22797;&#21487;&#33021;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#65307;&#65288;2&#65289;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#26412;&#25991;&#21457;&#29616;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#35757;&#32451;&#38598;&#20013;&#22122;&#22768;&#25968;&#25454;&#30340;&#27604;&#20363;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65307;&#65288;3&#65289;&#25552;&#20379;&#20102;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#23545;&#35782;&#21035;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#38382;&#39064;&#30340;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
&lt;/p&gt;</description></item><item><title>&#22810;&#20219;&#21153;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;MH-AIRL&#65289;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#21644;&#22522;&#26412;&#25216;&#33021;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#20219;&#21153;&#25110;&#25216;&#33021;&#27880;&#37322;&#30340;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.12633</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-task Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2305.12633v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12633
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;MH-AIRL&#65289;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#21644;&#22522;&#26412;&#25216;&#33021;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#20219;&#21153;&#25110;&#25216;&#33021;&#27880;&#37322;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22810;&#20219;&#21153;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#20986;&#33021;&#22815;&#25191;&#34892;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#36825;&#23545;&#20110;&#36890;&#29992;&#30446;&#30340;&#30340;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#22797;&#26434;&#38271;&#26102;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#20219;&#21153;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;MH-AIRL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#20998;&#23618;&#32467;&#26500;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#32452;&#21512;&#20219;&#21153;&#26356;&#21152;&#26377;&#30410;&#65292;&#24182;&#19988;&#36890;&#36807;&#35782;&#21035;&#21644;&#20256;&#36882;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#22522;&#26412;&#25216;&#33021;&#26469;&#25552;&#39640;&#19987;&#23478;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;MH-AIRL&#26377;&#25928;&#22320;&#21512;&#25104;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;AIRL&#65288;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65289;&#21644;&#20998;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;MH-AIRL&#36824;&#21487;&#20197;&#36866;&#24212;&#27809;&#26377;&#20219;&#21153;&#25110;&#25216;&#33021;&#27880;&#37322;&#30340;&#28436;&#31034;&#65288;&#21363;&#20165;&#21253;&#21547;&#29366;&#24577;-&#21160;&#20316;&#23545;&#65289;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26356;&#26131;&#20110;&#33719;&#21462;&#12290;&#23545;&#20110;MH-AIRL&#30340;&#27599;&#20010;&#27169;&#22359;&#37117;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task Imitation Learning (MIL) aims to train a policy capable of performing a distribution of tasks based on multi-task expert demonstrations, which is essential for general-purpose robots. Existing MIL algorithms suffer from low data efficiency and poor performance on complex long-horizontal tasks. We develop Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) to learn hierarchically-structured multi-task policies, which is more beneficial for compositional tasks with long horizons and has higher expert data efficiency through identifying and transferring reusable basic skills across tasks. To realize this, MH-AIRL effectively synthesizes context-based multi-task learning, AIRL (an IL approach), and hierarchical policy learning. Further, MH-AIRL can be adopted to demonstrations without the task or skill annotations (i.e., state-action pairs only) which are more accessible in practice. Theoretical justifications are provided for each module of MH-AIRL, and
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.11531</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#26032;&#22411;&#30005;&#30913;&#37327;&#35745;&#20960;&#20309;&#27169;&#25311;&#30340;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11531
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#29983;&#25104;&#23545;&#25758;&#20135;&#29289;&#30340;&#27169;&#25311;&#25506;&#27979;&#22120;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#12290;&#20854;&#20013;&#19968;&#20010;&#23376;&#25506;&#27979;&#22120;&#65292;&#30005;&#30913;&#37327;&#35745;&#30001;&#20110;&#20854;&#21333;&#20803;&#26684;&#30340;&#39640;&#31890;&#24230;&#21644;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#32780;&#21344;&#25454;&#20102;&#35745;&#31639;&#26102;&#38388;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#24555;&#30340;&#26679;&#26412;&#29983;&#25104;&#65292;&#20294;&#30446;&#21069;&#38656;&#35201;&#22823;&#37327;&#21162;&#21147;&#26469;&#20248;&#21270;&#29305;&#23450;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#32593;&#32476;&#26469;&#25551;&#36848;&#19981;&#21516;&#30340;&#21333;&#20803;&#26684;&#22823;&#23567;&#21644;&#25490;&#21015;&#26041;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#33021;&#25512;&#24191;&#21040;&#20854;&#20182;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#8220;&#20960;&#20309;&#24863;&#30693;&#8221;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#29983;&#25104;&#30475;&#19981;&#35265;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#27169;&#25311;&#21709;&#24212;&#32780;&#26080;&#38656;&#20854;&#20182;&#35757;&#32451;&#12290;&#35813;&#20960;&#20309;&#24863;&#30693;&#27169;&#22411;&#22312;&#28041;&#21450;&#20851;&#38190;&#21709;&#24212;&#30340;&#29983;&#25104;&#21644;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#31561;&#25351;&#26631;&#19978;&#27604;&#22522;&#32447;&#27169;&#22411;&#20248;&#36234;50&#65285;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#25193;&#23637;&#21040;&#38750;&#24179;&#38754;&#20960;&#20309;&#24418;&#29366;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of simulated detector response to collision products is crucial to data analysis in particle physics, but computationally very expensive. One subdetector, the calorimeter, dominates the computational time due to the high granularity of its cells and complexity of the interaction. Generative models can provide more rapid sample production, but currently require significant effort to optimize performance for specific detector geometries, often requiring many networks to describe the varying cell sizes and arrangements, which do not generalize to other geometries. We develop a {\it geometry-aware} autoregressive model, which learns how the calorimeter response varies with geometry, and is capable of generating simulated responses to unseen geometries without additional training. The geometry-aware model outperforms a baseline, unaware model by 50\% in metrics such as the Wasserstein distance between generated and true distributions of key quantities which summarize the simulate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#30340;&#30524;&#24213;&#34880;&#31649;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#25552;&#21462;&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#21644;Dropout&#23618;&#35299;&#20915;&#20809;&#29031;&#21464;&#21270;&#21644;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03617</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#30340;&#30524;&#24213;&#34880;&#31649;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segmentation of fundus vascular images based on a dual-attention mechanism. (arXiv:2305.03617v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#30340;&#30524;&#24213;&#34880;&#31649;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#25552;&#21462;&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#21644;Dropout&#23618;&#35299;&#20915;&#20809;&#29031;&#21464;&#21270;&#21644;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#20998;&#21106;&#35270;&#32593;&#33180;&#30524;&#24213;&#22270;&#20687;&#20013;&#30340;&#34880;&#31649;&#23545;&#20110;&#26089;&#26399;&#31579;&#26597;&#12289;&#35786;&#26029;&#21644;&#35780;&#20272;&#26576;&#20123;&#30524;&#37096;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#20809;&#29031;&#21464;&#21270;&#21644;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#65292;&#36825;&#20351;&#24471;&#20998;&#21106;&#21464;&#24471;&#38750;&#24120;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#27880;&#24847;&#34701;&#21512;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#32467;&#21512;&#20102;Transformer&#26500;&#24314;&#30340;&#36890;&#36947;&#27880;&#24847;&#21644;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#25552;&#21462;&#35270;&#32593;&#33180;&#30524;&#24213;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#28040;&#38500;&#32534;&#30721;&#22120;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#22312;&#36339;&#36291;&#36830;&#25509;&#20013;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;Dropout&#23618;&#38543;&#26426;&#33293;&#24323;&#19968;&#20123;&#31070;&#32463;&#20803;&#65292;&#20197;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#25311;&#21512;&#24182;&#25552;&#39640;&#20854;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;DERIVE&#12289;STARE&#21644;CHASEDB1&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#19968;&#20123;&#26368;&#36817;&#30340;&#35270;&#32593;&#33180;&#30524;&#24213;&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately segmenting blood vessels in retinal fundus images is crucial in the early screening, diagnosing, and evaluating some ocular diseases. However, significant light variations and non-uniform contrast in these images make segmentation quite challenging. Thus, this paper employ an attention fusion mechanism that combines the channel attention and spatial attention mechanisms constructed by Transformer to extract information from retinal fundus images in both spatial and channel dimensions. To eliminate noise from the encoder image, a spatial attention mechanism is introduced in the skip connection. Moreover, a Dropout layer is employed to randomly discard some neurons, which can prevent overfitting of the neural network and improve its generalization performance. Experiments were conducted on publicly available datasets DERIVE, STARE, and CHASEDB1. The results demonstrate that our method produces satisfactory results compared to some recent retinal fundus image segmentation algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#20294;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.02657</link><description>&lt;p&gt;
&#28145;&#24230;&#23485;&#26494;&#24347;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#20248;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#20294;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;$\mathcal X \subset \mathbb R^{d}$&#19978;&#30340;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#34987;&#30456;&#24212;&#30340;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#22238;&#24402;&#25152;&#23436;&#20840;&#25551;&#32472;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#35889;&#29305;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#22312;$\mathcal{X}$&#19978;&#20026;&#27491;&#23450;&#65292;&#20854;&#29305;&#24449;&#20540;&#34928;&#20943;&#29575;&#20026;$(d+1)/d$&#12290;&#30001;&#20110;&#26680;&#22238;&#24402;&#20013;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.14807</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning assisted microwave-plasma interaction based technique for plasma density estimation. (arXiv:2304.14807v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#23494;&#24230;&#26159;&#34920;&#24449;&#20219;&#20309;&#31561;&#31163;&#23376;&#20307;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#24212;&#29992;&#21644;&#30740;&#31350;&#37117;&#22522;&#20110;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#21644;&#31561;&#31163;&#23376;&#20307;&#28201;&#24230;&#12290;&#20256;&#32479;&#30340;&#30005;&#23376;&#23494;&#24230;&#27979;&#37327;&#26041;&#27861;&#38024;&#23545;&#32473;&#23450;&#32447;&#24615;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#35774;&#22791;&#25552;&#20379;&#36724;&#21521;&#21644;&#24452;&#21521;&#21078;&#38754;&#12290;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#25805;&#20316;&#33539;&#22260;&#36739;&#23567;&#12289;&#20202;&#22120;&#27785;&#37325;&#20197;&#21450;&#25968;&#25454;&#20998;&#26512;&#36807;&#31243;&#22797;&#26434;&#31561;&#20027;&#35201;&#32570;&#28857;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#23454;&#38469;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#30830;&#23450;&#31561;&#31163;&#23376;&#20307;&#20869;&#30005;&#23376;&#23494;&#24230;&#21078;&#38754;&#12290;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;&#35813;&#31574;&#30053;&#38024;&#23545;&#19968;&#20010;&#27169;&#25311;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#65292;&#20854;&#20013;&#21253;&#25324;&#20302;&#28201;&#12289;&#38750;&#30913;&#21270;&#21644;&#30896;&#25758;&#24615;&#31561;&#31163;&#23376;&#20307;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#39640;&#26031;&#24418;&#29366;&#23494;&#24230;&#21078;&#38754;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electron density is a key parameter to characterize any plasma. Most of the plasma applications and research in the area of low-temperature plasmas (LTPs) is based on plasma density and plasma temperature. The conventional methods for electron density measurements offer axial and radial profiles for any given linear LTP device. These methods have major disadvantages of operational range (not very wide), cumbersome instrumentation, and complicated data analysis procedures. To address such practical concerns, the article proposes a novel machine learning (ML) assisted microwave-plasma interaction based strategy which is capable enough to determine the electron density profile within the plasma. The electric field pattern due to microwave scattering is measured to estimate the density profile. The proof of concept is tested for a simulated training data set comprising a low-temperature, unmagnetized, collisional plasma. Different types of Gaussian-shaped density profiles, in the range
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26412;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39135;&#35889;&#65292;&#26088;&#22312;&#38477;&#20302;&#33258;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#30340;&#38376;&#27099;&#65292;&#24182;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20102;&#35299;&#21508;&#31181;&#36873;&#25321;&#21644;&#21442;&#25968;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.12210</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#39135;&#35889;
&lt;/p&gt;
&lt;p&gt;
A Cookbook of Self-Supervised Learning. (arXiv:2304.12210v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26412;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39135;&#35889;&#65292;&#26088;&#22312;&#38477;&#20302;&#33258;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#30340;&#38376;&#27099;&#65292;&#24182;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20102;&#35299;&#21508;&#31181;&#36873;&#25321;&#21644;&#21442;&#25968;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#34987;&#31216;&#20026;&#26234;&#33021;&#30340;&#26263;&#29289;&#36136;&#65292;&#26159;&#25512;&#36827;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#26465;&#26377;&#21069;&#36884;&#30340;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#20687;&#28921;&#39274;&#19968;&#26679;&#65292;&#35757;&#32451;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#38376;&#38656;&#35201;&#39640;&#38376;&#27099;&#30340;&#31934;&#32454;&#33402;&#26415;&#12290;&#34429;&#28982;&#35768;&#22810;&#32452;&#20214;&#26159;&#29087;&#24713;&#30340;&#65292;&#20294;&#25104;&#21151;&#35757;&#32451;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#28041;&#21450;&#21040;&#20174;&#21069;&#36235;&#20219;&#21153;&#21040;&#35757;&#32451;&#36229;&#21442;&#25968;&#30340;&#20196;&#20154;&#30524;&#33457;&#32557;&#20081;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20197;&#39135;&#35889;&#30340;&#24418;&#24335;&#22880;&#23450;&#22522;&#30784;&#21644;&#20171;&#32461;&#26368;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#38477;&#20302;&#36827;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#30340;&#38376;&#27099;&#12290;&#25105;&#20204;&#24076;&#26395;&#36171;&#20104;&#22909;&#22855;&#30340;&#30740;&#31350;&#20154;&#21592;&#22312;&#26041;&#27861;&#39046;&#22495;&#20013;&#33322;&#34892;&#65292;&#29702;&#35299;&#21508;&#31181;&#35843;&#33410;&#38062;&#30340;&#20316;&#29992;&#65292;&#24182;&#33719;&#24471;&#25506;&#32034;&#33258;&#30417;&#30563;&#23398;&#20064;&#32654;&#21619;&#20043;&#22788;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.02836</link><description>&lt;p&gt;
&#38271;&#26399;&#30340;&#22810;&#27169;&#24335;&#21464;&#21387;&#22120;&#25972;&#21512;EHR&#20013;&#25104;&#20687;&#21644;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#65292;&#29992;&#20110;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37325;&#22797;&#25104;&#20687;&#21644;&#21307;&#30103;&#32972;&#26223;&#65288;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65289;&#32435;&#20837;&#39044;&#27979;&#24615;&#23396;&#31435;&#24615;&#32954;&#37096;&#32467;&#33410;&#65288;SPN&#65289;&#35786;&#26029;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20687;&#25104;&#20687;&#21644;&#35786;&#26029;&#20195;&#30721;&#36825;&#26679;&#30340;&#20020;&#24202;&#24120;&#35268;&#27169;&#24335;&#21487;&#33021;&#26159;&#24322;&#27493;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#19981;&#35268;&#21017;&#37319;&#26679;&#65292;&#36825;&#26159;&#38271;&#26399;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#27169;&#24577;&#31574;&#30053;&#65292;&#23558;&#37325;&#22797;&#25104;&#20687;&#19982;&#26085;&#24120;&#25910;&#38598;&#30340;EHR&#20013;&#30340;&#38271;&#26399;&#20020;&#24202;&#29305;&#24449;&#30456;&#25972;&#21512;&#65292;&#20197;&#36827;&#34892;SPN&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#35299;&#32544;&#32538;&#65292;&#24182;&#21033;&#29992;&#26102;&#38388;&#36317;&#31163;&#32553;&#25918;&#33258;&#27880;&#24847;&#21147;&#26469;&#32852;&#21512;&#23398;&#20064;&#20020;&#24202;&#29305;&#24449;&#34920;&#36798;&#21644;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#26159;&#22312;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;2,668&#20010;&#25195;&#25551;&#21644;1,149&#21517;&#24535;&#24895;&#32773;&#30340;&#38271;&#26399;&#33016;&#37096;CT&#12289;&#36134;&#21333;&#20195;&#30721;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#35760;&#24405;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2303.17503</link><description>&lt;p&gt;
Pgx:&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#30340;&#24182;&#34892;&#28216;&#25103;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17503
&lt;/p&gt;
&lt;p&gt;
Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pgx&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#26827;&#30424;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#12290;&#30001;&#20110;JAX&#30340;&#33258;&#21160;&#21521;&#37327;&#21270;&#21644;&#21363;&#26102;&#32534;&#35793;&#21151;&#33021;&#65292;Pgx&#26131;&#20110;&#22312;GPU/TPU&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;A100 GPU&#19978;&#30340;Pgx&#27169;&#25311;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290;Pgx&#23454;&#29616;&#20102;&#34987;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#28216;&#25103;&#65292;&#22914;Backgammon&#65292;Shogi&#21644;Go&#12290; Pgx&#21487;&#22312;https://github.com/sotetsuk/pgx&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
&lt;/p&gt;</description></item><item><title>PeakNet&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;Bragg&#23792;&#28857;&#23547;&#25214;&#22120;&#65292;&#23427;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#26469;&#36866;&#24212;&#36880;&#21457;&#24378;&#32972;&#26223;&#25955;&#23556;&#30340;&#27874;&#21160;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#31639;&#27861;&#21442;&#25968;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#35823;&#25253;&#23792;&#28857;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.15301</link><description>&lt;p&gt;
PeakNet&#65306;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;Bragg&#23792;&#28857;&#23547;&#25214;&#22120;
&lt;/p&gt;
&lt;p&gt;
PeakNet: An Autonomous Bragg Peak Finder with Deep Neural Networks. (arXiv:2303.15301v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15301
&lt;/p&gt;
&lt;p&gt;
PeakNet&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;Bragg&#23792;&#28857;&#23547;&#25214;&#22120;&#65292;&#23427;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#26469;&#36866;&#24212;&#36880;&#21457;&#24378;&#32972;&#26223;&#25955;&#23556;&#30340;&#27874;&#21160;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#31639;&#27861;&#21442;&#25968;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#35823;&#25253;&#23792;&#28857;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;X&#23556;&#32447;&#33258;&#30001;&#30005;&#23376;&#28608;&#20809;&#22120;&#65288;XFEL&#65289;&#21644;&#21516;&#27493;&#36752;&#23556;&#35774;&#26045;&#20013;&#30340;&#20018;&#34892;&#26230;&#20307;&#23398;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#22823;&#20998;&#23376;&#32467;&#26500;&#21644;&#20998;&#23376;&#36807;&#31243;&#36827;&#34892;&#26032;&#39062;&#30340;&#31185;&#23398;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#39564;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#32473;&#25968;&#25454;&#20943;&#23569;&#21644;&#23454;&#26102;&#21453;&#39304;&#24102;&#26469;&#20102;&#35745;&#31639;&#25361;&#25112;&#12290;Bragg&#23792;&#28857;&#23547;&#25214;&#31639;&#27861;&#29992;&#20110;&#35782;&#21035;&#26377;&#29992;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#21629;&#20013;&#29575;&#21644;&#20998;&#36776;&#29575;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;&#26469;&#33258;&#32531;&#20914;&#28342;&#28082;&#12289;&#27880;&#23556;&#21943;&#22068;&#21644;&#20854;&#20182;&#23631;&#34109;&#26448;&#26009;&#30340;&#36880;&#21457;&#24378;&#24230;&#27874;&#21160;&#21644;&#24378;&#32972;&#26223;&#25955;&#23556;&#20351;&#24471;&#36825;&#25104;&#20026;&#19968;&#20010;&#32791;&#26102;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PeakNet&#65292;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;Bragg&#23792;&#28857;&#23547;&#25214;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Serial crystallography at X-ray free electron laser (XFEL) and synchrotron facilities has experienced tremendous progress in recent times enabling novel scientific investigations into macromolecular structures and molecular processes. However, these experiments generate a significant amount of data posing computational challenges in data reduction and real-time feedback. Bragg peak finding algorithm is used to identify useful images and also provide real-time feedback about hit-rate and resolution. Shot-to-shot intensity fluctuations and strong background scattering from buffer solution, injection nozzle and other shielding materials make this a time-consuming optimization problem. Here, we present PeakNet, an autonomous Bragg peak finder that utilizes deep neural networks. The development of this system 1) eliminates the need for manual algorithm parameter tuning, 2) reduces false-positive peaks by adjusting to shot-to-shot variations in strong background scattering in real-time, 3) e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFP&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#26032;&#29305;&#24449;&#21464;&#25442;&#20026;&#26087;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#26469;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#26032;&#29305;&#24449;&#26041;&#21521;&#30340;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.14595</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#29305;&#24449;&#25237;&#24433;&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;
&lt;/p&gt;
&lt;p&gt;
Preserving Linear Separability in Continual Learning by Backward Feature Projection. (arXiv:2303.14595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFP&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#26032;&#29305;&#24449;&#21464;&#25442;&#20026;&#26087;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#26469;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#26032;&#29305;&#24449;&#26041;&#21521;&#30340;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#38656;&#35201;&#22312;&#26377;&#38480;&#25110;&#27809;&#26377;&#20197;&#21069;&#26597;&#30475;&#20219;&#21153;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#24182;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#30452;&#25509;&#32422;&#26463;&#26032;&#29305;&#24449;&#20197;&#21305;&#37197;&#26087;&#29305;&#24449;&#65292;&#24573;&#35270;&#20102;&#21487;&#22609;&#24615;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Backward Feature Projection&#65288;BFP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#26032;&#29305;&#24449;&#22312;&#26087;&#29305;&#24449;&#30340;&#21487;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#20013;&#21457;&#29983;&#21464;&#21270;&#12290;BFP&#20445;&#30041;&#26087;&#31867;&#21035;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#21516;&#26102;&#20801;&#35768;&#26032;&#30340;&#29305;&#24449;&#26041;&#21521;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#30340;&#31867;&#21035;&#12290;BFP&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;&#38598;&#25104;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;BFP&#26377;&#21161;&#20110;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space,
&lt;/p&gt;</description></item><item><title>Xplainer&#26159;&#19968;&#20010;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23384;&#22312;&#30340;&#25551;&#36848;&#24615;&#35266;&#23519;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#33258;&#21160;&#35786;&#26029;&#38598;&#25104;&#21040;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13391</link><description>&lt;p&gt;
Xplainer&#65306;&#20174;X&#23556;&#32447;&#35266;&#23519;&#21040;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis. (arXiv:2303.13391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13391
&lt;/p&gt;
&lt;p&gt;
Xplainer&#26159;&#19968;&#20010;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23384;&#22312;&#30340;&#25551;&#36848;&#24615;&#35266;&#23519;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#33258;&#21160;&#35786;&#26029;&#38598;&#25104;&#21040;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#35786;&#26029;&#39044;&#27979;&#65292;&#26159;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#22312;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#21307;&#23398;&#39046;&#22495;&#30340;&#27880;&#37322;&#25968;&#25454;&#24448;&#24448;&#24456;&#23569;&#12290;&#38646;&#26679;&#26412;&#26041;&#27861;&#36890;&#36807;&#20801;&#35768;&#22312;&#19981;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#20020;&#24202;&#32467;&#26524;&#30340;&#26032;&#35774;&#32622;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#33258;&#21160;&#35786;&#26029;&#38598;&#25104;&#21040;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#26041;&#27861;&#24212;&#35813;&#26159;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#65292;&#22686;&#21152;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#20449;&#20219;&#24182;&#20419;&#36827;&#27491;&#30830;&#24615;&#39564;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Xplainer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;&#30340;&#26032;&#26694;&#26550;&#12290;Xplainer&#23558;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#21363;&#25551;&#36848;&#26041;&#27861;&#36866;&#24212;&#20110;&#22810;&#26631;&#31614;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#23545;&#23384;&#22312;&#30340;&#25551;&#36848;&#24615;&#35266;&#23519;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#39044;&#27979;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated diagnosis prediction from medical images is a valuable resource to support clinical decision-making. However, such systems usually need to be trained on large amounts of annotated data, which often is scarce in the medical domain. Zero-shot methods address this challenge by allowing a flexible adaption to new settings with different clinical findings without relying on labeled data. Further, to integrate automated diagnosis in the clinical workflow, methods should be transparent and explainable, increasing medical professionals' trust and facilitating correctness verification. In this work, we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in the clinical setting. Xplainer adapts the classification-by-description approach of contrastive vision-language models to the multi-label medical diagnosis task. Specifically, instead of directly predicting a diagnosis, we prompt the model to classify the existence of descriptive observations, which a radiologi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;</title><link>http://arxiv.org/abs/2303.11702</link><description>&lt;p&gt;
&#36830;&#25509;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25506;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#20197;&#21069;&#27809;&#26377;&#27491;&#24335;&#23558;SSL&#21644;OSR&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#21508;&#33258;&#30340;&#26041;&#27861;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSL-GAN&#21644;OSR-GAN&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;SSL&#21644;OSR&#20998;&#31867;&#22120;&#37117;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;SSL&#21644;OSR&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;SSL-GAN&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;OSR-GAN&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#29486;&#22522;&#30784;&#26356;&#21152;&#29282;&#22266;&#30340;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;&#26576;&#20123;&#19968;&#33324;&#30340;OSR&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;OSR&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#20114;&#24800;&#28857;&#65288;ARP&#65289;-GAN&#22312;&#19968;&#20123;OSR&#20219;&#21153;&#20013;&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
&lt;/p&gt;</description></item><item><title>PyVBMC&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Python&#24037;&#20855;&#65292;&#29992;&#20110;&#40657;&#30418;&#35745;&#31639;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#27169;&#22411;&#36873;&#25321;&#65292;&#21487;&#20197;&#22788;&#29702;&#36830;&#32493;&#21442;&#25968;&#19981;&#36229;&#36807;&#32422;10-15&#20010;&#30340;&#35745;&#31639;&#25110;&#32479;&#35745;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09519</link><description>&lt;p&gt;
PyVBMC&#65306;Python&#20013;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
PyVBMC: Efficient Bayesian inference in Python. (arXiv:2303.09519v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09519
&lt;/p&gt;
&lt;p&gt;
PyVBMC&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Python&#24037;&#20855;&#65292;&#29992;&#20110;&#40657;&#30418;&#35745;&#31639;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#27169;&#22411;&#36873;&#25321;&#65292;&#21487;&#20197;&#22788;&#29702;&#36830;&#32493;&#21442;&#25968;&#19981;&#36229;&#36807;&#32422;10-15&#20010;&#30340;&#35745;&#31639;&#25110;&#32479;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyVBMC&#26159;Variational Bayesian Monte Carlo&#65288;VBMC&#65289;&#31639;&#27861;&#30340;Python&#23454;&#29616;&#65292;&#29992;&#20110;&#40657;&#30418;&#35745;&#31639;&#27169;&#22411;&#30340;&#21518;&#39564;&#21644;&#27169;&#22411;&#25512;&#26029;&#12290;VBMC&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#20272;&#35745;&#21644;&#27169;&#22411;&#35780;&#20272;&#30340;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#65292;&#24403;&#27169;&#22411;&#35780;&#20272;&#26159;&#26377;&#28857;&#21040;&#38750;&#24120;&#26114;&#36149;&#65288;&#20363;&#22914;&#31532;&#20108;&#27425;&#25110;&#26356;&#22810;&#27425;&#65289;&#21644;/&#25110;&#22024;&#26434;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;VBMC&#35745;&#31639;&#65306;
&lt;/p&gt;
&lt;p&gt;
PyVBMC is a Python implementation of the Variational Bayesian Monte Carlo (VBMC) algorithm for posterior and model inference for black-box computational models (Acerbi, 2018, 2020). VBMC is an approximate inference method designed for efficient parameter estimation and model assessment when model evaluations are mildly-to-very expensive (e.g., a second or more) and/or noisy. Specifically, VBMC computes:  - a flexible (non-Gaussian) approximate posterior distribution of the model parameters, from which statistics and posterior samples can be easily extracted;  - an approximation of the model evidence or marginal likelihood, a metric used for Bayesian model selection.  PyVBMC can be applied to any computational or statistical model with up to roughly 10-15 continuous parameters, with the only requirement that the user can provide a Python function that computes the target log likelihood of the model, or an approximation thereof (e.g., an estimate of the likelihood obtained via simulation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DeepONets&#30340;&#22810;&#20449;&#24230;&#26041;&#27861;&#26469;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#38477;&#38454;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#27531;&#24046;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;&#24182;&#25512;&#26029;&#26032;&#39044;&#27979;&#30340;&#35823;&#24046;&#12290;&#35813;&#26694;&#26550;&#26368;&#22823;&#21270;&#21033;&#29992;&#39640;&#20449;&#24230;&#20449;&#24687;&#65292;&#29992;&#20110;&#26500;&#24314;&#38477;&#38454;&#27169;&#22411;&#21644;&#23398;&#20064;&#27531;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12682</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38477;&#38454;&#24314;&#27169;&#20013;&#27531;&#24046;&#23398;&#20064;&#30340;&#22810;&#20449;&#24230;DeepONet&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A DeepONet multi-fidelity approach for residual learning in reduced order modeling. (arXiv:2302.12682v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DeepONets&#30340;&#22810;&#20449;&#24230;&#26041;&#27861;&#26469;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#38477;&#38454;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#27531;&#24046;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;&#24182;&#25512;&#26029;&#26032;&#39044;&#27979;&#30340;&#35823;&#24046;&#12290;&#35813;&#26694;&#26550;&#26368;&#22823;&#21270;&#21033;&#29992;&#39640;&#20449;&#24230;&#20449;&#24687;&#65292;&#29992;&#20110;&#26500;&#24314;&#38477;&#38454;&#27169;&#22411;&#21644;&#23398;&#20064;&#27531;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#20449;&#24230;&#35270;&#35282;&#21644;DeepONets&#26469;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#31934;&#24230;&#30340;&#26032;&#26041;&#27861;&#12290;&#38477;&#38454;&#27169;&#22411;&#36890;&#36807;&#31616;&#21270;&#21407;&#22987;&#27169;&#22411;&#25552;&#20379;&#23454;&#26102;&#25968;&#20540;&#36817;&#20284;&#12290;&#36890;&#24120;&#24573;&#30053;&#36825;&#31181;&#25805;&#20316;&#24341;&#20837;&#30340;&#35823;&#24046;&#20197;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#27169;&#22411;&#38477;&#38454;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#27531;&#24046;&#23398;&#20064;&#30456;&#32806;&#21512;&#65292;&#20197;&#20415;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#24182;&#25512;&#26029;&#26032;&#39044;&#27979;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#24378;&#35843;&#35813;&#26694;&#26550;&#26368;&#22823;&#21270;&#21033;&#29992;&#39640;&#20449;&#24230;&#20449;&#24687;&#65292;&#29992;&#20110;&#26500;&#24314;&#38477;&#38454;&#27169;&#22411;&#21644;&#23398;&#20064;&#27531;&#24046;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25506;&#32034;&#20102;&#36866;&#29992;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20027;&#20803;&#27491;&#20132;&#20998;&#35299;&#65288;POD&#65289;&#21644;&#32570;&#22833;POD&#19982;&#26368;&#26032;&#30340;DeepONet&#26550;&#26500;&#30340;&#38598;&#25104;&#12290;&#23545;&#19968;&#20010;&#21442;&#25968;&#22522;&#20934;&#20989;&#25968;&#21644;&#19968;&#20010;&#38750;&#32447;&#24615;&#21442;&#25968;Navier-Stokes&#38382;&#39064;&#36827;&#34892;&#20102;&#25968;&#20540;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present work, we introduce a novel approach to enhance the precision of reduced order models by exploiting a multi-fidelity perspective and DeepONets. Reduced models provide a real-time numerical approximation by simplifying the original model. The error introduced by the such operation is usually neglected and sacrificed in order to reach a fast computation. We propose to couple the model reduction to a machine learning residual learning, such that the above-mentioned error can be learned by a neural network and inferred for new predictions. We emphasize that the framework maximizes the exploitation of high-fidelity information, using it for building the reduced order model and for learning the residual. In this work, we explore the integration of proper orthogonal decomposition (POD), and gappy POD for sensors data, with the recent DeepONet architecture. Numerical investigations for a parametric benchmark function and a nonlinear parametric Navier-Stokes problem are presented.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#27880;&#20876;&#25968;&#25454;&#26469;&#35782;&#21035;&#26032;&#29992;&#25143;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#19988;&#22312;&#20165;&#26377;&#23569;&#37327;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#26356;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2302.07517</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#29305;&#23450;&#36816;&#21160;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Extensible Motion-based Identification of XR Users using Non-Specific Motion Data. (arXiv:2302.07517v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07517
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#27880;&#20876;&#25968;&#25454;&#26469;&#35782;&#21035;&#26032;&#29992;&#25143;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#19988;&#22312;&#20165;&#26377;&#23569;&#37327;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#24335;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#36317;&#31163;&#21644;&#20998;&#31867;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#36890;&#36807;&#29992;&#25143;&#30340;&#36816;&#21160;&#26469;&#35782;&#21035;&#25193;&#23637;&#29616;&#23454;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#8220;&#21322;&#34928;&#26399;&#65306;Alyx&#8221;VR&#28216;&#25103;&#30340;&#29992;&#25143;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#30340;&#22522;&#32447;&#20998;&#31867;&#27169;&#22411;&#20316;&#20026;&#23545;&#27604;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21482;&#20351;&#29992;&#20960;&#20998;&#38047;&#30340;&#27880;&#20876;&#25968;&#25454;&#65292;&#35782;&#21035;&#26032;&#29992;&#25143;&#30340;&#38750;&#29305;&#23450;&#36816;&#21160;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#37325;&#26032;&#35757;&#32451;&#22522;&#32447;&#26041;&#27861;&#38656;&#35201;&#33457;&#36153;&#23558;&#36817;&#19968;&#22825;&#30340;&#26102;&#38388;&#65292;&#24403;&#21482;&#26377;&#24456;&#23569;&#30340;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#21487;&#38752;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#20351;&#29992;&#19981;&#21516;VR&#35774;&#22791;&#35760;&#24405;&#30340;&#26032;&#29992;&#25143;&#25968;&#25454;&#38598;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#26131;&#20110;&#25193;&#23637;&#30340;XR&#29992;&#25143;&#35782;&#21035;&#31995;&#32479;&#22880;&#23450;&#22522;&#30784;&#65292;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we combine the strengths of distance-based and classification-based approaches for the task of identifying extended reality users by their movements. For this we present an embedding-based approach that leverages deep metric learning. We train the model on a dataset of users playing the VR game ``Half-Life: Alyx'' and conduct multiple experiments and analyses using a state of the art classification-based model as baseline. The results show that the embedding-based method 1) is able to identify new users from non-specific movements using only a few minutes of enrollment data, 2) can enroll new users within seconds, while retraining the baseline approach takes almost a day, 3) is more reliable than the baseline approach when only little enrollment data is available, 4) can be used to identify new users from another dataset recorded with different VR devices.  Altogether, our solution is a foundation for easily extensible XR user identification systems, applicable to a wide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#33719;&#24471;&#24179;&#22343;&#22238;&#25253;MDPs&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#35299;&#20915;&#20102;&#36817;&#20284;&#31574;&#30053;&#36845;&#20195;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#24179;&#22343;&#22238;&#25253;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#30028;&#38480;&#38382;&#39064;</title><link>http://arxiv.org/abs/2302.01450</link><description>&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#30340;&#24179;&#22343;&#22238;&#25253;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms. (arXiv:2302.01450v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#33719;&#24471;&#24179;&#22343;&#22238;&#25253;MDPs&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#35299;&#20915;&#20102;&#36817;&#20284;&#31574;&#30053;&#36845;&#20195;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#24179;&#22343;&#22238;&#25253;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#30028;&#38480;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#36817;&#20284;&#31574;&#30053;&#36845;&#20195;&#30340;&#23454;&#20363;&#65292;&#21363;&#31574;&#30053;&#25913;&#36827;&#21644;&#31574;&#30053;&#35780;&#20272;&#37117;&#26159;&#36817;&#20284;&#36827;&#34892;&#30340;&#12290;&#22312;&#24179;&#22343;&#22238;&#25253;&#30446;&#26631;&#26159;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#24230;&#37327;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#25240;&#25187;&#22238;&#25253;&#20844;&#24335;&#65292;&#24182;&#20351;&#25240;&#25187;&#22240;&#23376;&#25509;&#36817;1&#65292;&#36825;&#30456;&#24403;&#20110;&#20351;&#39044;&#26399;&#30340;&#26102;&#38388;&#38271;&#30340;&#26080;&#38480;&#22823;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35823;&#24046;&#24615;&#33021;&#65292;&#30456;&#24212;&#30340;&#29702;&#35770;&#30028;&#38480;&#19982;&#26102;&#38388;&#38271;&#30340;&#24179;&#26041;&#25104;&#27604;&#20363;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#23558;&#24635;&#22238;&#25253;&#38500;&#20197;&#26102;&#38388;&#38271;&#65292;&#24179;&#22343;&#22238;&#25253;&#38382;&#39064;&#30340;&#30456;&#24212;&#24615;&#33021;&#30028;&#38480;&#20173;&#36235;&#20110;&#26080;&#31351;&#22823;&#12290;&#22240;&#27492;&#65292;&#33719;&#24471;&#36817;&#20284;&#31574;&#30053;&#36845;&#20195;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#24179;&#22343;&#22238;&#25253;&#35774;&#32622;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#30028;&#38480;&#19968;&#30452;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#33719;&#24471;&#24179;&#22343;&#22238;&#25253;MDPs&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#26469;&#35299;&#20915;&#36825;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#28176;&#36817;&#24773;&#20917;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
Many policy-based reinforcement learning (RL) algorithms can be viewed as instantiations of approximate policy iteration (PI), i.e., where policy improvement and policy evaluation are both performed approximately. In applications where the average reward objective is the meaningful performance metric, discounted reward formulations are often used with the discount factor being close to $1,$ which is equivalent to making the expected horizon very large. However, the corresponding theoretical bounds for error performance scale with the square of the horizon. Thus, even after dividing the total reward by the length of the horizon, the corresponding performance bounds for average reward problems go to infinity. Therefore, an open problem has been to obtain meaningful performance bounds for approximate PI and RL algorithms for the average-reward setting. In this paper, we solve this open problem by obtaining the first finite-time error bounds for average-reward MDPs, and show that the asymp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#22810;&#20010;&#34987;&#21160;&#23545;&#31216;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#23562;&#37325;&#34987;&#21160;&#23545;&#31216;&#24615;&#30340; dos and don'ts&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#34987;&#21160;&#23545;&#31216;&#24615;&#19982;&#22240;&#26524;&#24314;&#27169;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#22312;&#23398;&#20064;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#26679;&#26412;&#22806;&#25512;&#24191;&#26102;&#65292;&#23454;&#29616;&#34987;&#21160;&#23545;&#31216;&#24615;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2301.13724</link><description>&lt;p&gt;
&#36808;&#21521;&#23436;&#20840;&#21327;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards fully covariant machine learning. (arXiv:2301.13724v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#22810;&#20010;&#34987;&#21160;&#23545;&#31216;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#23562;&#37325;&#34987;&#21160;&#23545;&#31216;&#24615;&#30340; dos and don'ts&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#34987;&#21160;&#23545;&#31216;&#24615;&#19982;&#22240;&#26524;&#24314;&#27169;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#22312;&#23398;&#20064;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#26679;&#26412;&#22806;&#25512;&#24191;&#26102;&#65292;&#23454;&#29616;&#34987;&#21160;&#23545;&#31216;&#24615;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#23545;&#25968;&#25454;&#30340;&#34920;&#31034;&#37117;&#28041;&#21450;&#21040;&#20219;&#24847;&#30340;&#30740;&#31350;&#32773;&#36873;&#25321;&#12290;&#30001;&#20110;&#36825;&#20123;&#36873;&#25321;&#26159;&#22806;&#37096;&#20110;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#65292;&#27599;&#20010;&#36873;&#25321;&#37117;&#23548;&#33268;&#20102;&#19968;&#20010;&#30830;&#20999;&#30340;&#23545;&#31216;&#24615;&#65292;&#23545;&#24212;&#20110;&#23558;&#19968;&#20010;&#21487;&#33021;&#30340;&#34920;&#31034;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#34920;&#31034;&#30340;&#21464;&#25442;&#32676;&#12290;&#36825;&#20123;&#34987;&#21160;&#23545;&#31216;&#24615;&#21253;&#25324;&#22352;&#26631;&#33258;&#30001;&#24230;&#12289;&#35268;&#33539;&#23545;&#31216;&#24615;&#21644;&#21333;&#20301;&#21327;&#21464;&#24615;&#65292;&#22312;&#29289;&#29702;&#23398;&#20013;&#37117;&#20135;&#29983;&#20102;&#37325;&#35201;&#30340;&#32467;&#26524;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#26368;&#26126;&#26174;&#30340;&#34987;&#21160;&#23545;&#31216;&#24615;&#26159;&#22270;&#30340;&#37325;&#26032;&#26631;&#35760;&#25110;&#32622;&#25442;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#36825;&#20123;&#34987;&#21160;&#23545;&#31216;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#22914;&#26524;&#35201;&#23562;&#37325;&#34987;&#21160;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#30340;&#24212;&#35813;&#21644;&#19981;&#24212;&#35813;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19982;&#22240;&#26524;&#24314;&#27169;&#30340;&#32852;&#31995;&#65292;&#24182;&#35748;&#20026;&#22312;&#23398;&#20064;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#26679;&#26412;&#22806;&#25512;&#24191;&#26102;&#65292;&#23454;&#29616;&#34987;&#21160;&#23545;&#31216;&#24615;&#29305;&#21035;&#26377;&#20215;&#20540;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#27010;&#24565;&#24615;&#30340;&#65306;&#23427;&#22312;&#29289;&#29702;&#23398;&#30340;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. Our goal is to understand the implications for machine learning of the many passive symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling, and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. This paper is conceptual: It translates among the languages of physic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#26550;&#26500;&#22312;&#35813;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#35777;&#25454;&#65292;&#21457;&#29616;&#20102;&#19968;&#31867;&#32593;&#32476;&#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#23398;&#20064;&#20102;&#26368;&#23567;&#24230;&#25554;&#20540;&#22120;&#65292;&#24182;&#23545;&#38271;&#24230;&#26222;&#36890;&#21270;&#29616;&#35937;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2301.13105</link><description>&lt;p&gt;
&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#23398;&#20301;&#35838;&#31243;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generalization on the Unseen, Logic Reasoning and Degree Curriculum. (arXiv:2301.13105v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#26550;&#26500;&#22312;&#35813;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#35777;&#25454;&#65292;&#21457;&#29616;&#20102;&#19968;&#31867;&#32593;&#32476;&#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#23398;&#20064;&#20102;&#26368;&#23567;&#24230;&#25554;&#20540;&#22120;&#65292;&#24182;&#23545;&#38271;&#24230;&#26222;&#36890;&#21270;&#29616;&#35937;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#36923;&#36753;&#65288;&#24067;&#23572;&#65289;&#20989;&#25968;&#30340;&#23398;&#20064;&#65292;&#37325;&#28857;&#22312;&#20110;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#65288;GOTU&#65289;&#35774;&#23450;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#30340;&#26696;&#20363;&#12290;&#36825;&#26159;&#30001;&#20110;&#26576;&#20123;&#25512;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;&#31639;&#26415;/&#36923;&#36753;&#65289;&#20013;&#25968;&#25454;&#30340;&#20016;&#23500;&#32452;&#21512;&#24615;&#36136;&#20351;&#24471;&#20195;&#34920;&#24615;&#25968;&#25454;&#37319;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#22312;GOTU&#19979;&#25104;&#21151;&#23398;&#20064;&#20026;&#31532;&#19968;&#20010;&#8220;&#25512;&#29702;&#8221;&#23398;&#20064;&#32773;&#23637;&#31034;&#20102;&#19968;&#20010;&#23567;&#25554;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;(S)GD&#35757;&#32451;&#30340;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;GOTU&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#31867;&#21035;&#30340;&#32593;&#32476;&#27169;&#22411;&#65288;&#21253;&#25324;Transformer&#30340;&#23454;&#20363;&#12289;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#65289;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#23398;&#20064;&#20102;&#26368;&#23567;&#24230;&#25554;&#20540;&#22120;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#20854;&#20182;&#20855;&#26377;&#26356;&#22823;&#23398;&#20064;&#36895;&#29575;&#25110;&#22343;&#22330;&#32593;&#32476;&#30340;&#23454;&#20363;&#36798;&#21040;&#20102;&#28183;&#28431;&#26368;&#23567;&#24230;&#35299;&#12290;&#36825;&#20123;&#21457;&#29616;&#24102;&#26469;&#20102;&#20004;&#20010;&#24433;&#21709;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#38271;&#24230;&#26222;&#36890;&#21270;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an 'extrapolating' or 'reasoning' learner. We then study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. We also provide evidence that other instances with larger learning rates or mean-field networks reach leaky min-degree solutions. These findings lead to two implications: (1) we provide an explanation to the length genera
&lt;/p&gt;</description></item><item><title>Reef-Insight&#26159;&#19968;&#31181;&#21033;&#29992;&#32858;&#31867;&#26041;&#27861;&#21644;&#36965;&#24863;&#25216;&#26415;&#36827;&#34892;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#36965;&#24863;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2301.10876</link><description>&lt;p&gt;
Reef-insight:&#19968;&#31181;&#36890;&#36807;&#36965;&#24863;&#36827;&#34892;&#27700;&#22495;&#26646;&#24687;&#22320;&#26144;&#23556;&#30340;&#32858;&#31867;&#26041;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing. (arXiv:2301.10876v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10876
&lt;/p&gt;
&lt;p&gt;
Reef-Insight&#26159;&#19968;&#31181;&#21033;&#29992;&#32858;&#31867;&#26041;&#27861;&#21644;&#36965;&#24863;&#25216;&#26415;&#36827;&#34892;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#36965;&#24863;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#25439;&#23475;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#20851;&#27880;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#28023;&#23736;&#21306;&#22495;&#21644;&#28023;&#27915;&#20013;&#65292;&#32771;&#34385;&#21040;&#27668;&#20505;&#21464;&#21270;&#21644;&#27745;&#26579;&#21450;&#26497;&#31471;&#27668;&#20505;&#20107;&#20214;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#20998;&#26512;&#33021;&#21147;&#20197;&#21450;&#36965;&#24863;&#31561;&#20449;&#24687;&#33719;&#21462;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#21487;&#20197;&#29992;&#20110;&#31649;&#29702;&#21644;&#30740;&#31350;&#29642;&#29786;&#30977;&#29983;&#24577;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reef-Insight&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#20808;&#36827;&#30340;&#32858;&#31867;&#26041;&#27861;&#21644;&#36965;&#24863;&#25216;&#26415;&#65292;&#29992;&#20110;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#36965;&#24863;&#25968;&#25454;&#27604;&#36739;&#19981;&#21516;&#30340;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#23450;&#24615;&#21644;&#35270;&#35273;&#35780;&#20272;&#30340;&#22235;&#31181;&#20027;&#35201;&#32858;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;k-means&#12289;&#23618;&#27425;&#32858;&#31867;&#12289;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#23494;&#24230;&#32858;&#31867;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#28595;&#22823;&#21033;&#20122;&#21335;&#22823;&#22561;&#30977;&#30340;One Tree Island&#29642;&#29786;&#30977;&#30340;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#35797;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#36827;&#34892;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Environmental damage has been of much concern, particularly in coastal areas and the oceans, given climate change and the drastic effects of pollution and extreme climate events. Our present-day analytical capabilities, along with advancements in information acquisition techniques such as remote sensing, can be utilised for the management and study of coral reef ecosystems. In this paper, we present Reef-Insight, an unsupervised machine learning framework that features advanced clustering methods and remote sensing for reef habitat mapping. Our framework compares different clustering methods for reef habitat mapping using remote sensing data. We evaluate four major clustering approaches based on qualitative and visual assessments which include k-means, hierarchical clustering, Gaussian mixture model, and density-based clustering. We utilise remote sensing data featuring the One Tree Island reef in Australia's Southern Great Barrier Reef. Our results indicate that clustering methods usi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#22823;&#23567;&#21644;&#36235;&#21183;&#65292;&#24182;&#25454;&#27492;&#20026;&#22320;&#29699;&#23545;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#24433;&#21709;&#20570;&#20934;&#22791;&#12290;</title><link>http://arxiv.org/abs/2301.06732</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#20885;&#31354;&#27934;
&lt;/p&gt;
&lt;p&gt;
Solar Coronal Hole Analysis and Prediction using Computer Vision and LSTM Neural Network. (arXiv:2301.06732v4 [astro-ph.SR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#22823;&#23567;&#21644;&#36235;&#21183;&#65292;&#24182;&#25454;&#27492;&#20026;&#22320;&#29699;&#23545;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#24433;&#21709;&#20570;&#20934;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#24320;&#22987;&#25506;&#32034;&#22826;&#31354;&#65292;&#22826;&#31354;&#22825;&#27668;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#26126;&#26174;&#12290;&#24050;&#32463;&#30830;&#31435;&#20102;&#22826;&#38451;&#20885;&#31354;&#27934;&#36825;&#19968;&#31181;&#22826;&#31354;&#22825;&#27668;&#29616;&#35937;&#20250;&#23545;&#39134;&#26426;&#21644;&#21355;&#26143;&#30340;&#36816;&#20316;&#20135;&#29983;&#24433;&#21709;&#12290;&#22826;&#38451;&#20885;&#31354;&#27934;&#26159;&#22826;&#38451;&#19978;&#30340;&#19968;&#29255;&#21306;&#22495;&#65292;&#20854;&#29305;&#28857;&#26159;&#30913;&#22330;&#32447;&#24320;&#25918;&#32780;&#28201;&#24230;&#30456;&#23545;&#36739;&#20302;&#65292;&#23548;&#33268;&#22826;&#38451;&#39118;&#20197;&#39640;&#20110;&#24179;&#22343;&#36895;&#29575;&#36827;&#34892;&#21457;&#23556;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#20934;&#22791;&#22909;&#24212;&#23545;&#22826;&#38451;&#20885;&#31354;&#27934;&#23545;&#22320;&#29699;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26816;&#27979;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#22270;&#20687;&#20013;&#30340;&#22826;&#38451;&#20885;&#31354;&#27934;&#21306;&#22495;&#24182;&#35745;&#31639;&#20854;&#22823;&#23567;&#12290;&#25105;&#20204;&#23545;&#22826;&#38451;&#27599;&#20010;&#21306;&#22495;&#30340;&#22826;&#38451;&#20885;&#31354;&#27934;&#36827;&#34892;&#27604;&#36739;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#27861;&#20998;&#26512;&#22826;&#38451;&#20885;&#31354;&#27934;&#38754;&#31215;&#25968;&#25454;&#30340;&#36235;&#21183;&#65292;&#24182;&#39044;&#27979;&#19981;&#21516;&#22826;&#38451;&#21306;&#22495;&#26410;&#26469;7&#22825;&#30340;&#22826;&#38451;&#20885;&#31354;&#27934;&#22823;&#23567;&#12290;&#36890;&#36807;&#20998;&#26512;&#22826;&#38451;&#20885;&#31354;&#27934;&#38754;&#31215;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36776;&#35782;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#21464;&#21270;&#36235;&#21183;&#21644;&#39044;&#27979;&#20854;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
As humanity has begun to explore space, the significance of space weather has become apparent. It has been established that coronal holes, a type of space weather phenomenon, can impact the operation of aircraft and satellites. The coronal hole is an area on the sun characterized by open magnetic field lines and relatively low temperatures, which result in the emission of the solar wind at higher than average rates. In this study, To prepare for the impact of coronal holes on the Earth, we use computer vision to detect the coronal hole region and calculate its size based on images from the Solar Dynamics Observatory (SDO). We compare the coronal holes for each region of the Sun and analyze the correlation. We then implement deep learning techniques, specifically the Long Short-Term Memory (LSTM) method, to analyze trends in the coronal hole area data and predict its size for different sun regions over 7 days. By analyzing time series data on the coronal hole area, this study aims to id
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#39044;&#27979;&#65292;&#23558;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#20132;&#21449;&#26399;&#26679;&#26412;&#30340;&#26631;&#27880;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#39044;&#27979;&#27010;&#29575;&#22686;&#24378;&#65292;&#24182;&#24212;&#29992;&#32047;&#31215;&#20915;&#31574;&#35268;&#21017;&#65292;&#25104;&#21151;&#32553;&#30701;&#20102;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2301.03465</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#39044;&#27979;&#32553;&#30701;&#23454;&#26102;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Shorter Latency of Real-time Epileptic Seizure Detection via Probabilistic Prediction. (arXiv:2301.03465v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03465
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#39044;&#27979;&#65292;&#23558;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#20132;&#21449;&#26399;&#26679;&#26412;&#30340;&#26631;&#27880;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#39044;&#27979;&#27010;&#29575;&#22686;&#24378;&#65292;&#24182;&#24212;&#29992;&#32047;&#31215;&#20915;&#31574;&#35268;&#21017;&#65292;&#25104;&#21151;&#32553;&#30701;&#20102;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20855;&#26377;&#33391;&#22909;&#28789;&#25935;&#24230;&#24615;&#33021;&#30340;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#31639;&#27861;&#65292;&#20294;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#23454;&#29616;&#26174;&#33879;&#32553;&#30701;&#30340;&#26816;&#27979;&#24310;&#36831;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27010;&#29575;&#39044;&#27979;&#26469;&#32553;&#30701;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#24310;&#36831;&#12290;&#25105;&#20204;&#39318;&#27425;&#23558;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#20219;&#21153;&#20174;&#20256;&#32479;&#30340;&#20108;&#20998;&#31867;&#36716;&#21270;&#20026;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#30315;&#30187;&#23450;&#21521;&#33041;&#30005;&#22270;&#35760;&#24405;&#30340;&#20132;&#21449;&#26399;&#24182;&#25552;&#20986;&#20351;&#29992;&#36719;&#26631;&#31614;&#23545;&#20132;&#21449;&#26399;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#30340;&#35268;&#21017;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;STFT&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#32467;&#21512;3D-CNN&#26550;&#26500;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#26679;&#26412;&#30340;&#39044;&#27979;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20462;&#27491;&#21152;&#26435;&#31574;&#30053;&#20197;&#22686;&#24378;&#39044;&#27979;&#27010;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#32047;&#31215;&#20915;&#31574;&#35268;&#21017;&#20197;&#23454;&#29616;&#26174;&#33879;&#32553;&#30701;&#30340;&#26816;&#27979;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although recent studies have proposed seizure detection algorithms with good sensitivity performance, there is a remained challenge that they were hard to achieve significantly short detection latency in real-time scenarios. In this manuscript, we propose a novel deep learning framework intended for shortening epileptic seizure detection latency via probabilistic prediction. We are the first to convert the seizure detection task from traditional binary classification to probabilistic prediction by introducing a crossing period from seizure-oriented EEG recording and proposing a labeling rule using soft-label for crossing period samples. And, a novel multiscale STFT-based feature extraction method combined with 3D-CNN architecture is proposed to accurately capture predictive probabilities of samples. Furthermore, we also propose rectified weighting strategy to enhance predictive probabilities, and accumulative decision-making rule to achieve significantly shorter detection latency. We i
&lt;/p&gt;</description></item><item><title>&#35813;&#35838;&#31243;&#20171;&#32461;&#20102;&#27969;&#34892;&#30340;&#25968;&#25454;&#31185;&#23398;&#35821;&#35328;R&#65292;&#24182;&#26088;&#22312;&#22521;&#20859;&#23398;&#29983;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#32773;&#25104;&#20026;&#29420;&#31435;&#30340;R&#35821;&#35328;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2301.01188</link><description>&lt;p&gt;
&#28145;&#24230;R&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep R Programming. (arXiv:2301.01188v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35838;&#31243;&#20171;&#32461;&#20102;&#27969;&#34892;&#30340;&#25968;&#25454;&#31185;&#23398;&#35821;&#35328;R&#65292;&#24182;&#26088;&#22312;&#22521;&#20859;&#23398;&#29983;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#32773;&#25104;&#20026;&#29420;&#31435;&#30340;R&#35821;&#35328;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;R&#32534;&#31243;&#26159;&#19968;&#38376;&#20840;&#38754;&#30340;&#35838;&#31243;&#65292;&#37325;&#28857;&#20171;&#32461;&#25968;&#25454;&#31185;&#23398;&#20013;&#26368;&#27969;&#34892;&#30340;&#35821;&#35328;&#20043;&#19968;&#8212;&#8212;R&#35821;&#35328;&#65288;&#32479;&#35745;&#35745;&#31639;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#28165;&#27927;&#21644;&#20998;&#26512;&#65289;&#12290;&#23427;&#28145;&#20837;&#20171;&#32461;&#20102;R&#35821;&#35328;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#26088;&#22312;&#22521;&#20859;&#26377;&#25265;&#36127;&#30340;&#23398;&#29983;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#32773;&#65292;&#35753;&#20182;&#20204;&#25104;&#20026;&#29420;&#31435;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#29615;&#22659;&#30340;&#29992;&#25143;&#12290;&#36825;&#20010;&#25945;&#26448;&#26159;&#19968;&#20010;&#38750;&#30408;&#21033;&#39033;&#30446;&#65292;&#23427;&#30340;&#22312;&#32447;&#21644;PDF&#29256;&#26412;&#21487;&#20197;&#22312; &lt;https://deepr.gagolewski.com/&gt; &#20813;&#36153;&#33719;&#21462;&#12290;&#24076;&#26395;&#36825;&#20010;&#26089;&#26399;&#33609;&#26696;&#21457;&#25918;&#20986;&#26469;&#21518;&#33021;&#23545;&#35835;&#32773;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep R Programming is a comprehensive course on one of the most popular languages in data science (statistical computing, graphics, machine learning, data wrangling and analytics). It introduces the base language in-depth and is aimed at ambitious students, practitioners, and researchers who would like to become independent users of this powerful environment. This textbook is a non-profit project. Its online and PDF versions are freely available at &lt;https://deepr.gagolewski.com/&gt;. This early draft is distributed in the hope that it will be useful.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#36884;&#24452;&#26469;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#20135;&#29983;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;&#19968;&#31181;&#26159;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23558;&#33521;&#25991;&#36164;&#28304;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#27880;&#37325;&#25968;&#37327;&#65307;&#21478;&#19968;&#31181;&#26159;&#30452;&#25509;&#22522;&#20110;&#39640;&#36136;&#37327;&#12289;&#29421;&#35889;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#36164;&#28304;&#36739;&#23569;&#35821;&#35328;&#22914;&#24847;&#22823;&#21033;&#35821;&#30340;&#39046;&#22495;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10422</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#30340;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models. (arXiv:2212.10422v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#36884;&#24452;&#26469;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#20135;&#29983;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;&#19968;&#31181;&#26159;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23558;&#33521;&#25991;&#36164;&#28304;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#27880;&#37325;&#25968;&#37327;&#65307;&#21478;&#19968;&#31181;&#26159;&#30452;&#25509;&#22522;&#20110;&#39640;&#36136;&#37327;&#12289;&#29421;&#35889;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#36164;&#28304;&#36739;&#23569;&#35821;&#35328;&#22914;&#24847;&#22823;&#21033;&#35821;&#30340;&#39046;&#22495;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21307;&#30103;&#26102;&#20195;&#65292;&#21307;&#38498;&#27599;&#22825;&#20135;&#29983;&#30340;&#22823;&#37327;&#25991;&#26412;&#20449;&#24687;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#36164;&#20135;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#12289;&#31934;&#32454;&#35843;&#25972;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#26469;&#21033;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#21644;&#31649;&#29702;&#12290;&#23545;&#20110;&#36825;&#20123;&#19987;&#38376;&#39046;&#22495;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26469;&#33258;&#24191;&#35206;&#30422;&#28857;&#26816;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#39046;&#22495;&#20869;&#36164;&#28304;&#30340;&#39069;&#22806;&#35757;&#32451;&#36718;&#27425;&#19978;&#21487;&#20197;&#33719;&#30410;&#24456;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36164;&#28304;&#36890;&#24120;&#23545;&#20110;&#20687;&#24847;&#22823;&#21033;&#36825;&#26679;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#26159;&#19981;&#21487;&#21450;&#30340;&#65292;&#20351;&#24471;&#24403;&#22320;&#21307;&#30103;&#26426;&#26500;&#26080;&#27861;&#36827;&#34892;&#39046;&#22495;&#20869;&#36866;&#24212;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#20004;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#29983;&#25104;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#24847;&#22823;&#21033;&#35821;&#20026;&#20855;&#20307;&#26696;&#20363;&#65306;&#19968;&#31181;&#22522;&#20110;&#33521;&#25991;&#36164;&#28304;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#36861;&#27714;&#25968;&#37327;&#32780;&#19981;&#26159;&#36136;&#37327;&#65307;&#21478;&#19968;&#31181;&#22522;&#20110;&#39640;&#36136;&#37327;&#12289;&#29421;&#35889;&#30340;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital healthcare, the huge volumes of textual information generated every day in hospitals constitute an essential but underused asset that could be exploited with task-specific, fine-tuned biomedical language representation models, improving patient care and management. For such specialized domains, previous research has shown that fine-tuning models stemming from broad-coverage checkpoints can largely benefit additional training rounds over large-scale in-domain resources. However, these resources are often unreachable for less-resourced languages like Italian, preventing local medical institutions to employ in-domain adaptation. In order to reduce this gap, our work investigates two accessible approaches to derive biomedical language models in languages other than English, taking Italian as a concrete use-case: one based on neural machine translation of English resources, favoring quantity over quality; the other based on a high-grade, narrow-scoped corpus natively w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#39118;&#38505;&#22235;&#26041;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;SVR&#30340;&#20004;&#31181;&#24418;&#24335;&#23545;&#24212;&#20110;&#31561;&#25928;&#35823;&#24046;&#24230;&#37327;&#30340;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#21152;&#19978;&#27491;&#21017;&#21270;&#24809;&#32602;&#39033;&#12290;&#36890;&#36807;&#26500;&#36896;&#22522;&#26412;&#39118;&#38505;&#22235;&#26041;&#26694;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SVR&#26159;&#23545;&#20004;&#20010;&#23545;&#31216;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#24179;&#22343;&#25968;&#30340;&#28176;&#36817;&#26080;&#20559;&#20272;&#35745;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$\varepsilon$-SVR&#21644;$\nu$-SVR&#22312;&#19968;&#33324;&#38543;&#26426;&#29615;&#22659;&#19979;&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09178</link><description>&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;: &#39118;&#38505;&#22235;&#26041;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Support Vector Regression: Risk Quadrangle Framework. (arXiv:2212.09178v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#39118;&#38505;&#22235;&#26041;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;SVR&#30340;&#20004;&#31181;&#24418;&#24335;&#23545;&#24212;&#20110;&#31561;&#25928;&#35823;&#24046;&#24230;&#37327;&#30340;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#21152;&#19978;&#27491;&#21017;&#21270;&#24809;&#32602;&#39033;&#12290;&#36890;&#36807;&#26500;&#36896;&#22522;&#26412;&#39118;&#38505;&#22235;&#26041;&#26694;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SVR&#26159;&#23545;&#20004;&#20010;&#23545;&#31216;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#24179;&#22343;&#25968;&#30340;&#28176;&#36817;&#26080;&#20559;&#20272;&#35745;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$\varepsilon$-SVR&#21644;$\nu$-SVR&#22312;&#19968;&#33324;&#38543;&#26426;&#29615;&#22659;&#19979;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22522;&#26412;&#30340;&#39118;&#38505;&#22235;&#26041;&#29702;&#35770;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#35813;&#29702;&#35770;&#23558;&#20248;&#21270;&#12289;&#39118;&#38505;&#31649;&#29702;&#21644;&#32479;&#35745;&#20272;&#35745;&#32852;&#31995;&#36215;&#26469;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;SVR&#30340;&#20004;&#31181;&#24418;&#24335;&#65292;$\varepsilon$-SVR&#21644;$\nu$-SVR&#65292;&#37117;&#23545;&#24212;&#20110;&#31561;&#25928;&#35823;&#24046;&#24230;&#37327;&#65288;&#20998;&#21035;&#20026;Vapnik&#35823;&#24046;&#21644;CVaR&#33539;&#25968;&#65289;&#30340;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#21152;&#19978;&#27491;&#21017;&#21270;&#24809;&#32602;&#39033;&#12290;&#36825;&#20123;&#35823;&#24046;&#24230;&#37327;&#21448;&#23450;&#20041;&#20102;&#30456;&#24212;&#30340;&#39118;&#38505;&#22235;&#26041;&#26694;&#12290;&#36890;&#36807;&#26500;&#36896;&#19982;SVR&#23545;&#24212;&#30340;&#22522;&#26412;&#39118;&#38505;&#22235;&#26041;&#26694;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SVR&#26159;&#20004;&#20010;&#23545;&#31216;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#24179;&#22343;&#25968;&#30340;&#28176;&#36817;&#26080;&#20559;&#20272;&#35745;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#19968;&#33324;&#38543;&#26426;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;$\varepsilon$-SVR&#21644;$\nu$-SVR&#30340;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;SVR&#34987;&#34920;&#36848;&#20026;&#24102;&#26377;&#27491;&#21017;&#21270;&#24809;&#32602;&#39033;&#30340;&#27491;&#21017;&#20559;&#31163;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25512;&#23548;&#20102;&#22312;&#39118;&#38505;&#22235;&#26041;&#26694;&#26550;&#20013;&#30340;SVR&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates Support Vector Regression (SVR) in the context of the fundamental risk quadrangle theory, which links optimization, risk management, and statistical estimation. It is shown that both formulations of SVR, $\varepsilon$-SVR and $\nu$-SVR, correspond to the minimization of equivalent error measures (Vapnik error and CVaR norm, respectively) with a regularization penalty. These error measures, in turn, define the corresponding risk quadrangles. By constructing the fundamental risk quadrangle, which corresponds to SVR, we show that SVR is the asymptotically unbiased estimator of the average of two symmetric conditional quantiles. Further, we prove the equivalence of the $\varepsilon$-SVR and $\nu$-SVR in a general stochastic setting. Additionally, SVR is formulated as a regular deviation minimization problem with a regularization penalty. Finally, the dual formulation of SVR in the risk quadrangle framework is derived.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#24494;&#20998;&#30340;&#29992;&#25143;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24191;&#27867;&#24212;&#29992;&#30340;&#21487;&#24494;&#20998;&#26367;&#20195;&#21697;&#35299;&#20915;&#20102;&#29616;&#20195;&#20808;&#36827;&#29992;&#25143;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#19981;&#20860;&#23481;&#24615;&#21644;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32447;&#24212;&#29992;&#20013;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#26080;&#20284;&#28982;&#25512;&#29702;&#26041;&#27861;&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33756;&#21333;&#25628;&#32034;&#20219;&#21153;&#20013;&#22914;&#20309;&#21033;&#29992;&#35748;&#30693;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2211.16277</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#29992;&#25143;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentiable User Models. (arXiv:2211.16277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#24494;&#20998;&#30340;&#29992;&#25143;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24191;&#27867;&#24212;&#29992;&#30340;&#21487;&#24494;&#20998;&#26367;&#20195;&#21697;&#35299;&#20915;&#20102;&#29616;&#20195;&#20808;&#36827;&#29992;&#25143;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#19981;&#20860;&#23481;&#24615;&#21644;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32447;&#24212;&#29992;&#20013;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#26080;&#20284;&#28982;&#25512;&#29702;&#26041;&#27861;&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33756;&#21333;&#25628;&#32034;&#20219;&#21153;&#20013;&#22914;&#20309;&#21033;&#29992;&#35748;&#30693;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#29992;&#25143;&#24314;&#27169;&#23545;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#20808;&#36827;&#30340;&#29992;&#25143;&#27169;&#22411;&#36890;&#24120;&#34987;&#35774;&#35745;&#20026;&#35748;&#30693;&#34892;&#20026;&#27169;&#25311;&#22120;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#19981;&#20860;&#23481;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#24191;&#27867;&#24212;&#29992;&#30340;&#21487;&#24494;&#20998;&#26367;&#20195;&#21697;&#65292;&#32469;&#36807;&#35745;&#31639;&#29942;&#39048;&#65292;&#20351;&#29616;&#20195;&#35748;&#30693;&#27169;&#22411;&#30340;&#25512;&#29702;&#26356;&#39640;&#25928;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20197;&#36866;&#29992;&#20110;&#22312;&#32447;&#24212;&#29992;&#30340;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#19982;&#29616;&#26377;&#30340;&#26080;&#20284;&#28982;&#25512;&#29702;&#26041;&#27861;&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22914;&#20309;&#22312;&#33756;&#21333;&#25628;&#32034;&#20219;&#21153;&#20013;&#20351;&#29992;&#35748;&#30693;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#65292;&#32780;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#36890;&#24120;&#38656;&#35201;&#25968;&#23567;&#26102;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic user modeling is essential for building machine learning systems in the ubiquitous cases with humans in the loop. However, modern advanced user models, often designed as cognitive behavior simulators, are incompatible with modern machine learning pipelines and computationally prohibitive for most practical applications. We address this problem by introducing widely-applicable differentiable surrogates for bypassing this computational bottleneck; the surrogates enable computationally efficient inference with modern cognitive models. We show experimentally that modeling capabilities comparable to the only available solution, existing likelihood-free inference methods, are achievable with a computational cost suitable for online applications. Finally, we demonstrate how AI-assistants can now use cognitive models for online interaction in a menu-search task, which has so far required hours of computation during interaction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#26799;&#24230;&#32534;&#30721;&#21644;&#36873;&#25321;&#24615;&#37325;&#22797;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25302;&#36710;&#20943;&#36731;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.13802</link><description>&lt;p&gt;
&#29992;&#20110;&#20943;&#36731;&#24930;&#33410;&#28857;&#24433;&#21709;&#21147;&#30340;&#39034;&#24207;&#26799;&#24230;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sequential Gradient Coding For Straggler Mitigation. (arXiv:2211.13802v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#26799;&#24230;&#32534;&#30721;&#21644;&#36873;&#25321;&#24615;&#37325;&#22797;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25302;&#36710;&#20943;&#36731;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#20013;&#65292;&#24930;&#33410;&#28857;&#65288;&#21363;&#25302;&#25289;&#26426;&#65289;&#36890;&#24120;&#25104;&#20026;&#29942;&#39048;&#12290;Tandon&#31561;&#20154;&#25552;&#20986;&#30340;&#26799;&#24230;&#32534;&#30721;&#65288;GC&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#32416;&#38169;&#30721;&#21407;&#29702;&#22312;&#25302;&#36710;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20998;&#24067;&#26799;&#24230;&#35745;&#31639;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#26412;&#25991;&#32771;&#34385;&#35745;&#31639;&#19968;&#20010;&#26799;&#24230;&#24207;&#21015;$\{g(1),g(2),\ldots,g(J)\}$&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#20854;&#20013;&#27599;&#20010;&#26799;&#24230;$g(t)$&#30340;&#22788;&#29702;&#20174;&#31532;$t$&#36718;&#24320;&#22987;&#65292;&#21040;&#31532;$(t+T)$&#36718;&#32467;&#26463;&#12290;&#36825;&#37324;$T\geq 0$&#34920;&#31034;&#19968;&#20010;&#24310;&#36831;&#21442;&#25968;&#12290;&#23545;&#20110;GC&#26041;&#26696;&#65292;&#32534;&#30721;&#20165;&#22312;&#35745;&#31639;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#65292;&#23548;&#33268;$T=0$&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;$T&gt;0$&#20801;&#35768;&#35774;&#35745;&#21033;&#29992;&#26102;&#38388;&#32500;&#24230;&#30340;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#26696;&#65292;&#30456;&#27604;&#20110;GC&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#26041;&#26696;&#23558;GC&#19982;&#36873;&#25321;&#24615;&#37325;&#22797;&#20043;&#21069;&#26410;&#23436;&#25104;&#30340;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#25302;&#36710;&#20943;&#36731;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#26041;&#26696;&#20013;&#65292;&#26500;&#25104;&#20102;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In distributed computing, slower nodes (stragglers) usually become a bottleneck. Gradient Coding (GC), introduced by Tandon et al., is an efficient technique that uses principles of error-correcting codes to distribute gradient computation in the presence of stragglers. In this paper, we consider the distributed computation of a sequence of gradients $\{g(1),g(2),\ldots,g(J)\}$, where processing of each gradient $g(t)$ starts in round-$t$ and finishes by round-$(t+T)$. Here $T\geq 0$ denotes a delay parameter. For the GC scheme, coding is only across computing nodes and this results in a solution where $T=0$. On the other hand, having $T&gt;0$ allows for designing schemes which exploit the temporal dimension as well. In this work, we propose two schemes that demonstrate improved performance compared to GC. Our first scheme combines GC with selective repetition of previously unfinished tasks and achieves improved straggler mitigation. In our second scheme, which constitutes our main contri
&lt;/p&gt;</description></item><item><title>QueryForm&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#34920;&#21333;&#23454;&#20307;&#26597;&#35810;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#25552;&#31034;&#26426;&#21046;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#26597;&#35810;-&#23454;&#20307;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#33021;&#22815;&#20174;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20540;&#65292;&#26080;&#38656;&#30446;&#26631;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.07730</link><description>&lt;p&gt;
QueryForm: &#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#34920;&#21333;&#23454;&#20307;&#26597;&#35810;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QueryForm: A Simple Zero-shot Form Entity Query Framework. (arXiv:2211.07730v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07730
&lt;/p&gt;
&lt;p&gt;
QueryForm&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#34920;&#21333;&#23454;&#20307;&#26597;&#35810;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#25552;&#31034;&#26426;&#21046;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#26597;&#35810;-&#23454;&#20307;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#33021;&#22815;&#20174;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20540;&#65292;&#26080;&#38656;&#30446;&#26631;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#23545;&#20110;&#25991;&#26723;&#29702;&#35299;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#22330;&#26223;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#26631;&#27880;&#25991;&#26723;&#23454;&#20307;&#25152;&#38656;&#30340;&#39640;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#26694;&#26550;QueryForm&#65292;&#35813;&#26694;&#26550;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#20174;&#31867;&#20284;&#34920;&#21333;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20540;&#12290;QueryForm&#21253;&#21547;&#19968;&#20010;&#21452;&#37325;&#25552;&#31034;&#26426;&#21046;&#65292;&#23558;&#25991;&#26723;&#27169;&#24335;&#21644;&#29305;&#23450;&#23454;&#20307;&#31867;&#22411;&#32452;&#21512;&#25104;&#19968;&#20010;&#26597;&#35810;&#65292;&#29992;&#20110;&#25552;&#31034;Transformer&#27169;&#22411;&#25191;&#34892;&#21333;&#20010;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#20174;&#31867;&#20284;&#34920;&#21333;&#30340;&#32593;&#39029;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#26597;&#35810;-&#23454;&#20307;&#23545;&#36827;&#34892;QueryForm&#30340;&#39044;&#35757;&#32451;&#65292;&#36825;&#20123;&#32593;&#39029;&#24102;&#26377;&#24369;HTML&#27880;&#37322;&#12290;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32479;&#19968;&#21040;&#30456;&#21516;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#26694;&#26550;&#20013;&#65292;QueryForm&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21253;&#21547;&#21508;&#31181;&#23454;&#20307;&#21644;&#24067;&#23616;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#30446;&#26631;&#25991;&#26723;&#31867;&#22411;&#65292;&#26080;&#38656;&#30446;&#26631;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;QueryForm&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#27867;&#21270;&#30340;&#31070;&#32463;&#20803;&#38598;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#27963;&#21160;&#38750;&#24179;&#31283;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.05634</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#38598;&#21512;&#25512;&#29702;&#26041;&#27861;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization of generative model for neuronal ensemble inference method. (arXiv:2211.05634v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05634
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#27867;&#21270;&#30340;&#31070;&#32463;&#20803;&#38598;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#27963;&#21160;&#38750;&#24179;&#31283;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#31070;&#32463;&#20803;&#30340;&#30456;&#20114;&#20316;&#29992;&#26500;&#25104;&#20102;&#32500;&#25345;&#29983;&#21629;&#27963;&#21160;&#25152;&#24517;&#38656;&#30340;&#21508;&#31181;&#33041;&#21151;&#33021;&#65292;&#22240;&#27492;&#65292;&#20998;&#26512;&#21151;&#33021;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#38416;&#26126;&#22823;&#33041;&#21151;&#33021;&#30340;&#26426;&#21046;&#65292;&#21253;&#25324;&#31070;&#32463;&#31185;&#23398;&#21508;&#20010;&#39046;&#22495;&#22312;&#20869;&#30340;&#35768;&#22810;&#30740;&#31350;&#27491;&#22312;&#31215;&#26497;&#24320;&#23637;&#21151;&#33021;&#31070;&#32463;&#20803;&#38598;&#21512;&#21644;&#20013;&#24515;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21151;&#33021;&#31070;&#32463;&#20803;&#38598;&#21512;&#21644;&#20013;&#24515;&#30340;&#23384;&#22312;&#26377;&#21161;&#20110;&#25552;&#39640;&#20449;&#24687;&#22788;&#29702;&#25928;&#29575;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20174;&#31070;&#32463;&#20803;&#27963;&#21160;&#25968;&#25454;&#20013;&#25512;&#26029;&#21151;&#33021;&#31070;&#32463;&#20803;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#24314;&#31435;&#27963;&#21160;&#27169;&#22411;&#23384;&#22312;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#27963;&#21160;&#29305;&#24449;&#21462;&#20915;&#20110;&#29983;&#29702;&#23454;&#39564;&#26465;&#20214;&#65292;&#22240;&#27492;&#20854;&#20855;&#26377;&#38750;&#24179;&#31283;&#24615;&#12290;&#32467;&#26524;&#65292;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#27169;&#22411;&#20013;&#20551;&#35774;&#30340;&#24179;&#31283;&#24615;&#20250;&#22952;&#30861;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various brain functions that are necessary to maintain life activities materialize through the interaction of countless neurons. Therefore, it is important to analyze the structure of functional neuronal network. To elucidate the mechanism of brain function, many studies are being actively conducted on the structure of functional neuronal ensemble and hub, including all areas of neuroscience. In addition, recent study suggests that the existence of functional neuronal ensembles and hubs contributes to the efficiency of information processing. For these reasons, there is a demand for methods to infer functional neuronal ensembles from neuronal activity data, and methods based on Bayesian inference have been proposed. However, there is a problem in modeling the activity in Bayesian inference. The features of each neuron's activity have non-stationarity depending on physiological experimental conditions. As a result, the assumption of stationarity in Bayesian inference model impedes infer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#39640;&#32500;&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#22810;&#37325;&#26816;&#39564;&#31243;&#24207;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25511;&#21046;&#39057;&#29575;FDR&#65292;&#24182;&#33021;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.02778</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#20013;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;FDR&#25511;&#21046;&#30340;&#36817;&#20284;&#26368;&#20248;&#22810;&#37325;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Near-optimal multiple testing in Bayesian linear models with finite-sample FDR control. (arXiv:2211.02778v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39640;&#32500;&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#22810;&#37325;&#26816;&#39564;&#31243;&#24207;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25511;&#21046;&#39057;&#29575;FDR&#65292;&#24182;&#33021;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#20013;&#65292;&#32479;&#35745;&#23398;&#23478;&#36890;&#24120;&#33268;&#21147;&#20110;&#35774;&#35745;&#33021;&#25511;&#21046;&#34394;&#35686;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#30340;&#22810;&#37325;&#26816;&#39564;&#36807;&#31243;&#65292;&#21516;&#26102;&#35782;&#21035;&#26356;&#22810;&#30456;&#20851;&#21464;&#37327;&#12290;&#27169;&#22411;-X&#26041;&#27861;&#65288;&#22914;Knockoffs&#21644;&#26465;&#20214;&#38543;&#26426;&#21270;&#26816;&#39564;&#65289;&#22312;&#20551;&#35774;&#24050;&#30693;&#21327;&#21464;&#37327;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26679;&#26412;FDR&#25511;&#21046;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#36824;&#33021;&#23454;&#29616;&#26368;&#22823;&#21270;&#21457;&#29616;&#30340;&#27425;&#35201;&#30446;&#26631;&#20173;&#19981;&#30830;&#23450;&#12290;&#20107;&#23454;&#19978;&#65292;&#35774;&#35745;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;FDR&#25511;&#21046;&#24182;&#21457;&#29616;&#26356;&#22810;&#30456;&#20851;&#21464;&#37327;&#30340;&#31243;&#24207;&#65292;&#22312;&#21487;&#33021;&#26159;&#26368;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#20013;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#39640;&#32500;&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#19982;&#21508;&#21521;&#21516;&#24615;&#21327;&#21464;&#37327;&#65292;&#24320;&#21457;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#37325;&#26816;&#39564;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33021;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#20445;&#35777;&#39057;&#29575;FDR&#25511;&#21046;&#30340;&#27169;&#22411;-X&#31243;&#24207;&#65292;&#21363;&#20351;&#27169;&#22411;&#34987;&#38169;&#35823;&#25351;&#23450;&#65292;&#20063;&#33021;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high dimensional variable selection problems, statisticians often seek to design multiple testing procedures that control the False Discovery Rate (FDR), while concurrently identifying a greater number of relevant variables. Model-X methods, such as Knockoffs and conditional randomization tests, achieve the primary goal of finite-sample FDR control, assuming a known distribution of covariates. However, whether these methods can also achieve the secondary goal of maximizing discoveries remains uncertain. In fact, designing procedures to discover more relevant variables with finite-sample FDR control is a largely open question, even within the arguably simplest linear models.  In this paper, we develop near-optimal multiple testing procedures for high dimensional Bayesian linear models with isotropic covariates. We introduce Model-X procedures that provably control the frequentist FDR from finite samples, even when the model is misspecified, and conjecturally achieve near-optimal powe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#65292;&#21457;&#29616;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#20248;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#20316;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#30340;&#21478;&#19968;&#20010;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2211.00181</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Numerical Stability of Hyperbolic Representation Learning. (arXiv:2211.00181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#65292;&#21457;&#29616;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#20248;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#20316;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#30340;&#21478;&#19968;&#20010;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36229;&#29699;&#30340;&#23481;&#37327;&#38543;&#21322;&#24452;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#33021;&#22815;&#23558;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#23884;&#20837;&#20854;&#20013;&#32780;&#19981;&#22833;&#30495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25351;&#25968;&#22686;&#38271;&#30340;&#24615;&#36136;&#24120;&#24120;&#23548;&#33268;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#20351;&#24471;&#35757;&#32451;&#36229;&#20960;&#20309;&#23398;&#20064;&#27169;&#22411;&#26377;&#26102;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;NaN&#38382;&#39064;&#21644;&#28014;&#28857;&#31639;&#26415;&#20013;&#36935;&#21040;&#26080;&#27861;&#34920;&#31034;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;&#8212;&#8212;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#65292;&#22312;64&#20301;&#31639;&#26415;&#31995;&#32479;&#19979;&#65292;Poincar\'e&#29699;&#30456;&#23545;&#20110;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22823;&#30340;&#33021;&#21147;&#26469;&#27491;&#30830;&#34920;&#31034;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;Lorentz&#27169;&#22411;&#20248;&#20110;Poincar\'e&#29699;&#30340;&#20248;&#36234;&#24615;&#12290;&#37492;&#20110;&#20004;&#31181;&#27169;&#22411;&#30340;&#25968;&#20540;&#38480;&#21046;&#65292;&#25105;&#20204;&#30830;&#23450;&#19968;&#31181;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#65292;&#22312;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#20043;&#22806;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the exponential growth of the volume of the ball w.r.t. its radius, the hyperbolic space is capable of embedding trees with arbitrarily small distortion and hence has received wide attention for representing hierarchical datasets. However, this exponential growth property comes at a price of numerical instability such that training hyperbolic learning models will sometimes lead to catastrophic NaN problems, encountering unrepresentable values in floating point arithmetic. In this work, we carefully analyze the limitation of two popular models for the hyperbolic space, namely, the Poincar\'e ball and the Lorentz model. We first show that, under the 64 bit arithmetic system, the Poincar\'e ball has a relatively larger capacity than the Lorentz model for correctly representing points. Then, we theoretically validate the superiority of the Lorentz model over the Poincar\'e ball from the perspective of optimization. Given the numerical limitations of both models, we identify one Eucli
&lt;/p&gt;</description></item><item><title>&#36328;&#23458;&#25143;&#26631;&#31614;&#20256;&#25773;&#65288;XCLP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#35774;&#22791;&#21644;&#21322;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.06434</link><description>&lt;p&gt;
&#36328;&#23458;&#25143;&#26631;&#31614;&#20256;&#25773;&#29992;&#20110;&#36328;&#35774;&#22791;&#21644;&#21322;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-client Label Propagation for Transductive and Semi-Supervised Federated Learning. (arXiv:2210.06434v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06434
&lt;/p&gt;
&lt;p&gt;
&#36328;&#23458;&#25143;&#26631;&#31614;&#20256;&#25773;&#65288;XCLP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#35774;&#22791;&#21644;&#21322;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65306;&#36328;&#23458;&#25143;&#26631;&#31614;&#20256;&#25773;&#65288;XCLP&#65289;&#12290;XCLP&#36890;&#36807;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20849;&#21516;&#20272;&#35745;&#25968;&#25454;&#22270;&#65292;&#24182;&#36890;&#36807;&#22312;&#22270;&#19978;&#20256;&#25773;&#26631;&#31614;&#20449;&#24687;&#26469;&#35745;&#31639;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#36991;&#20813;&#23458;&#25143;&#31471;&#38656;&#35201;&#19982;&#20182;&#20154;&#20849;&#20139;&#25968;&#25454;&#65292;XCLP&#37319;&#29992;&#20102;&#20004;&#20010;&#23494;&#30721;&#23398;&#23433;&#20840;&#21327;&#35758;&#65306;&#23433;&#20840;&#30340;&#27721;&#26126;&#36317;&#31163;&#35745;&#31639;&#21644;&#23433;&#20840;&#27714;&#21644;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;XCLP&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#22312;&#31532;&#19968;&#20010;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#20197;&#19968;&#27425;&#24615;&#30340;&#26041;&#24335;&#39044;&#27979;&#26410;&#35265;&#27979;&#35797;&#28857;&#30340;&#26631;&#31614;&#12290;&#22312;&#31532;&#20108;&#20010;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#22312;&#32852;&#37030;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#37325;&#22797;&#20266;&#26631;&#35760;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#30495;&#23454;&#30340;&#32852;&#37030;&#21644;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;XCLP&#22312;&#36825;&#20004;&#20010;&#24212;&#29992;&#20013;&#27604;&#26367;&#20195;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Cross-Client Label Propagation(XCLP), a new method for transductive federated learning. XCLP estimates a data graph jointly from the data of multiple clients and computes labels for the unlabeled data by propagating label information across the graph. To avoid clients having to share their data with anyone, XCLP employs two cryptographically secure protocols: secure Hamming distance computation and secure summation. We demonstrate two distinct applications of XCLP within federated learning. In the first, we use it in a one-shot way to predict labels for unseen test points. In the second, we use it to repeatedly pseudo-label unlabeled training data in a federated semi-supervised setting. Experiments on both real federated and standard benchmark datasets show that in both applications XCLP achieves higher classification accuracy than alternative approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#22797;&#21512;&#26680;&#26469;&#23558;&#20808;&#39564;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#38544;&#24335;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#26680;&#20989;&#25968;&#21644;&#36873;&#25321;&#30340;&#31532;&#20108;&#20010;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#27169;&#25311;&#24050;&#30693;&#29305;&#24615;&#65292;&#24182;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.07384</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#22797;&#21512;&#26680;&#23558;&#20808;&#39564;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel. (arXiv:2205.07384v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#22797;&#21512;&#26680;&#26469;&#23558;&#20808;&#39564;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#38544;&#24335;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#26680;&#20989;&#25968;&#21644;&#36873;&#25321;&#30340;&#31532;&#20108;&#20010;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#27169;&#25311;&#24050;&#30693;&#29305;&#24615;&#65292;&#24182;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#23398;&#20064;&#20197;&#20808;&#39564;&#30693;&#35782;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35768;&#22810;&#24050;&#30693;&#29305;&#24615;&#65292;&#22914;&#31354;&#38388;&#24179;&#28369;&#24615;&#25110;&#23395;&#33410;&#24615;&#65292;&#22312;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20013;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#26680;&#20989;&#25968;&#26469;&#24314;&#27169;&#26159;&#30452;&#25509;&#30340;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#36825;&#20123;&#24050;&#30693;&#29305;&#24615;&#26469;&#25913;&#36827;&#12290;&#20363;&#22914;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#24191;&#27867;&#29992;&#20110;&#36965;&#24863;&#65292;&#36825;&#21463;&#21040;&#24378;&#28872;&#30340;&#23395;&#33410;&#25928;&#24212;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#30001;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#23450;&#20041;&#30340;&#26680;&#20989;&#25968;&#19982;&#36873;&#25321;&#29992;&#20110;&#24314;&#27169;&#24050;&#30693;&#29305;&#24615;&#30340;&#31532;&#20108;&#20010;&#26680;&#20989;&#25968;&#65288;&#20363;&#22914;&#23395;&#33410;&#24615;&#65289;&#30456;&#32467;&#21512;&#30340;&#22797;&#21512;&#26680;&#26469;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;GP&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#28145;&#24230;&#32593;&#32476;&#21644;&#22522;&#20110;Nystrom&#36817;&#20284;&#30340;&#39640;&#25928;&#26144;&#23556;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#24819;&#27861;&#65292;&#23558;&#20854;&#31216;&#20026;&#38544;&#24335;&#22797;&#21512;&#26680;&#65288;ICK&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#26679;&#26412;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#23436;&#25972;&#30340;GP&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ICK&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#36965;&#24863;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of deep learning and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping based on the Nystrom approximation, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demons
&lt;/p&gt;</description></item><item><title>PARNN&#26159;&#19968;&#31181;&#27010;&#29575;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20855;&#26377;&#38750;&#24179;&#31283;&#24615;&#12289;&#38750;&#32447;&#24615;&#12289;&#38750;&#21608;&#26399;&#24615;&#12289;&#38271;&#26399;&#20381;&#36182;&#21644;&#28151;&#27788;&#27169;&#24335;&#30340;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#21306;&#38388;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2204.09640</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#30340;&#27010;&#29575;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Probabilistic AutoRegressive Neural Networks for Accurate Long-range Forecasting. (arXiv:2204.09640v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09640
&lt;/p&gt;
&lt;p&gt;
PARNN&#26159;&#19968;&#31181;&#27010;&#29575;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20855;&#26377;&#38750;&#24179;&#31283;&#24615;&#12289;&#38750;&#32447;&#24615;&#12289;&#38750;&#21608;&#26399;&#24615;&#12289;&#38271;&#26399;&#20381;&#36182;&#21644;&#28151;&#27788;&#27169;&#24335;&#30340;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#21306;&#38388;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#32929;&#31080;&#20215;&#26684;&#21040;&#26089;&#26399;&#20256;&#26579;&#30149;&#39044;&#27979;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#23454;&#38469;&#39044;&#27979;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#23558;&#20256;&#32479;&#39044;&#27979;&#26041;&#27861;&#21644;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27010;&#29575;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;PARNN&#65289;&#65292;&#33021;&#22815;&#22788;&#29702;&#34920;&#29616;&#20986;&#38750;&#24179;&#31283;&#24615;&#12289;&#38750;&#32447;&#24615;&#12289;&#38750;&#21608;&#26399;&#24615;&#12289;&#38271;&#26399;&#20381;&#36182;&#21644;&#28151;&#27788;&#27169;&#24335;&#30340;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;PARNN&#26159;&#36890;&#36807;&#25913;&#36827;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;ARNN&#65289;&#20351;&#29992;&#33258;&#22238;&#24402;&#31215;&#20998;&#31227;&#21160;&#24179;&#22343;&#65288;ARIMA&#65289;&#21453;&#39304;&#35823;&#24046;&#36827;&#34892;&#26500;&#24314;&#30340;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#8220;&#30333;&#30418;&#23376;&#33324;&#8221;&#30340;&#39044;&#27979;&#34892;&#20026;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PARNN&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#21306;&#38388;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20351;&#20854;&#19982;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#21306;&#21035;&#24320;&#26469;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting time series data is a critical area of research with applications spanning from stock prices to early epidemic prediction. While numerous statistical and machine learning methods have been proposed, real-life prediction problems often require hybrid solutions that bridge classical forecasting approaches and modern neural network models. In this study, we introduce the Probabilistic AutoRegressive Neural Networks (PARNN), capable of handling complex time series data exhibiting non-stationarity, nonlinearity, non-seasonality, long-range dependence, and chaotic patterns. PARNN is constructed by improving autoregressive neural networks (ARNN) using autoregressive integrated moving average (ARIMA) feedback error, combining the explainability, scalability, and "white-box-like" prediction behavior of both models. Notably, the PARNN model provides uncertainty quantification through prediction intervals, setting it apart from advanced deep learning tools. Through comprehensive compu
&lt;/p&gt;</description></item><item><title>PyDTS&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#29983;&#23384;&#25968;&#25454;&#21322;&#21442;&#25968;&#31454;&#20105;&#39118;&#38505;&#27169;&#22411;&#30340;Python&#21253;&#65292;&#25903;&#25345;&#21253;&#25324;LASSO&#21644;&#24377;&#24615;&#32593;&#31561;&#27491;&#21017;&#21270;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.05731</link><description>&lt;p&gt;
PyDTS&#65306;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31454;&#20105;&#39118;&#38505;&#65288;&#27491;&#21017;&#21270;&#65289;&#22238;&#24402;&#30340; Python &#21253;
&lt;/p&gt;
&lt;p&gt;
PyDTS: A Python Package for Discrete-Time Survival (Regularized) Regression with Competing Risks. (arXiv:2204.05731v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05731
&lt;/p&gt;
&lt;p&gt;
PyDTS&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#29983;&#23384;&#25968;&#25454;&#21322;&#21442;&#25968;&#31454;&#20105;&#39118;&#38505;&#27169;&#22411;&#30340;Python&#21253;&#65292;&#25903;&#25345;&#21253;&#25324;LASSO&#21644;&#24377;&#24615;&#32593;&#31561;&#27491;&#21017;&#21270;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#33267;&#20107;&#20214;&#20998;&#26512;&#65288;&#29983;&#23384;&#20998;&#26512;&#65289;&#29992;&#20110;&#21709;&#24212;&#26102;&#38388;&#26159;&#25351;&#39044;&#23450;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#30001;&#20110;&#26102;&#38388;&#26412;&#36523;&#26159;&#31163;&#25955;&#30340;&#25110;&#30001;&#20110;&#23558;&#22833;&#36133;&#26102;&#38388;&#20998;&#32452;&#20026;&#38388;&#38548;&#25110;&#33293;&#20837;&#27979;&#37327;&#65292;&#22240;&#27492;&#26102;&#38388;&#33267;&#20107;&#20214;&#25968;&#25454;&#26377;&#26102;&#26159;&#31163;&#25955;&#30340;&#12290;&#27492;&#22806;&#65292;&#20010;&#20307;&#30340;&#22833;&#36133;&#21487;&#33021;&#26159;&#20960;&#31181;&#19981;&#21516;&#30340;&#22833;&#36133;&#31867;&#22411;&#20043;&#19968;&#65292;&#31216;&#20026;&#31454;&#20105;&#39118;&#38505;&#65288;&#20107;&#20214;&#65289;&#12290;&#22823;&#22810;&#25968;&#29983;&#23384;&#22238;&#24402;&#20998;&#26512;&#30340;&#26041;&#27861;&#21644;&#36719;&#20214;&#21253;&#20551;&#23450;&#26102;&#38388;&#26159;&#22312;&#36830;&#32493;&#23610;&#24230;&#19978;&#27979;&#37327;&#30340;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#23558;&#26631;&#20934;&#30340;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#30340;&#20272;&#35745;&#22120;&#23384;&#22312;&#20559;&#24046;&#12290;&#20171;&#32461;&#20102; Python &#21253; PyDTS&#65292;&#29992;&#20110;&#27169;&#25311;&#65292;&#20272;&#35745;&#21644;&#35780;&#20272;&#31163;&#25955;&#26102;&#38388;&#29983;&#23384;&#25968;&#25454;&#30340;&#21322;&#21442;&#25968;&#31454;&#20105;&#39118;&#38505;&#27169;&#22411;&#12290;&#35813;&#21253;&#23454;&#29616;&#20102;&#24555;&#36895;&#36807;&#31243;&#65292;&#20351;&#26377;&#25928;&#22320;&#21253;&#25324;&#27491;&#21017;&#21270;&#22238;&#24402;&#26041;&#27861;&#65292;&#22914; LASSO &#21644;&#24377;&#24615;&#32593;&#32476;&#31561;&#12290;&#19968;&#20010;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Time-to-event analysis (survival analysis) is used when the response of interest is the time until a pre-specified event occurs. Time-to-event data are sometimes discrete either because time itself is discrete or due to grouping of failure times into intervals or rounding off measurements. In addition, the failure of an individual could be one of several distinct failure types, known as competing risks (events). Most methods and software packages for survival regression analysis assume that time is measured on a continuous scale. It is well-known that naively applying standard continuous-time models with discrete-time data may result in biased estimators of the discrete-time models. The Python package PyDTS, for simulating, estimating and evaluating semi-parametric competing-risks models for discrete-time survival data, is introduced. The package implements a fast procedure that enables including regularized regression methods, such as LASSO and elastic net, among others. A simulation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#20998;&#24067;&#22806;&#30340;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#25439;&#22833;&#21644;&#24314;&#27169;&#20559;&#24046;&#65292;&#35813;&#27169;&#22411;&#22312;&#22330;&#26223;&#20998;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.11194</link><description>&lt;p&gt;
&#20197;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#30340;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Test-time Adaptation with Slot-Centric Models. (arXiv:2203.11194v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#20998;&#24067;&#22806;&#30340;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#25439;&#22833;&#21644;&#24314;&#27169;&#20559;&#24046;&#65292;&#35813;&#27169;&#22411;&#22312;&#22330;&#26223;&#20998;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35270;&#35273;&#26816;&#27979;&#22120;&#22312;&#35757;&#32451;&#20998;&#24067;&#20869;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#23558;&#20998;&#24067;&#22806;&#30340;&#22330;&#26223;&#35299;&#26512;&#20026;&#20854;&#32452;&#25104;&#23454;&#20307;&#12290;&#26368;&#36817;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#20351;&#29992;&#36741;&#21161;&#30340;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#29420;&#31435;&#22320;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#24050;&#32463;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#22312;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#27867;&#21270;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25439;&#22833;&#23545;&#20110;&#22330;&#26223;&#20998;&#35299;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#36275;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#24314;&#27169;&#20559;&#24046;&#12290;&#26368;&#36817;&#30340;&#20197;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#29983;&#25104;&#27169;&#22411;&#23581;&#35797;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#22330;&#26223;&#20998;&#35299;&#20026;&#23454;&#20307;&#65292;&#36890;&#36807;&#37325;&#24314;&#20687;&#32032;&#26469;&#23454;&#29616;&#12290;&#32467;&#21512;&#36825;&#20004;&#20010;&#24037;&#20316;&#32447;&#36335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#22330;&#26223;&#20998;&#35299;&#27169;&#22411;&#65292;&#21363;Slot-TTA&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#37325;&#24314;&#25110;&#20132;&#21449;&#35270;&#22270;&#32508;&#21512;&#30446;&#26631;&#22312;&#27599;&#20010;&#22330;&#26223;&#19978;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#36866;&#24212;&#12290;&#25105;&#20204;&#23545;Slot-TTA&#22312;&#22810;&#31181;&#36755;&#20837;&#27169;&#24335;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current visual detectors, though impressive within their training distribution, often fail to parse out-of-distribution scenes into their constituent entities. Recent test-time adaptation methods use auxiliary self-supervised losses to adapt the network parameters to each test example independently and have shown promising results towards generalization outside the training distribution for the task of image classification. In our work, we find evidence that these losses are insufficient for the task of scene decomposition, without also considering architectural inductive biases. Recent slot-centric generative models attempt to decompose scenes into entities in a self-supervised manner by reconstructing pixels. Drawing upon these two lines of work, we propose Slot-TTA, a semi-supervised slot-centric scene decomposition model that at test time is adapted per scene through gradient descent on reconstruction or cross-view synthesis objectives. We evaluate Slot-TTA across multiple input mo
&lt;/p&gt;</description></item><item><title>SUPERNOVA&#26159;&#19968;&#20010;&#36127;&#36131;&#33258;&#21160;&#21270;&#27979;&#35797;&#36873;&#25321;&#21644;&#32570;&#38519;&#39044;&#38450;&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#39118;&#38505;&#30340;&#27979;&#35797;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#35270;&#39057;&#28216;&#25103;&#27979;&#35797;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2203.05566</link><description>&lt;p&gt;
SUPERNOVA: &#20351;&#29992;&#22522;&#20110;&#39118;&#38505;&#30340;&#27979;&#35797;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;AAA&#28216;&#25103;&#20013;&#33258;&#21160;&#21270;&#27979;&#35797;&#36873;&#25321;&#21644;&#32570;&#38519;&#39044;&#38450;
&lt;/p&gt;
&lt;p&gt;
SUPERNOVA: Automating Test Selection and Defect Prevention in AAA Video Games Using Risk Based Testing and Machine Learning. (arXiv:2203.05566v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05566
&lt;/p&gt;
&lt;p&gt;
SUPERNOVA&#26159;&#19968;&#20010;&#36127;&#36131;&#33258;&#21160;&#21270;&#27979;&#35797;&#36873;&#25321;&#21644;&#32570;&#38519;&#39044;&#38450;&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#39118;&#38505;&#30340;&#27979;&#35797;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#35270;&#39057;&#28216;&#25103;&#27979;&#35797;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36719;&#20214;&#31995;&#32479;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#27979;&#35797;&#35270;&#39057;&#28216;&#25103;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#25193;&#23637;&#12290;&#25163;&#21160;&#27979;&#35797;&#26159;&#19968;&#20010;&#38750;&#24120;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#22240;&#27492;&#24456;&#24555;&#23601;&#21464;&#24471;&#25104;&#26412;&#19981;&#21487;&#34892;&#12290;&#20351;&#29992;&#33050;&#26412;&#36827;&#34892;&#33258;&#21160;&#21270;&#27979;&#35797;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#33050;&#26412;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#19988;&#30693;&#36947;&#20309;&#26102;&#36816;&#34892;&#27599;&#20010;&#27979;&#35797;&#26159;&#21478;&#19968;&#20010;&#38382;&#39064;&#12290;&#29616;&#20195;&#28216;&#25103;&#30340;&#22797;&#26434;&#24615;&#12289;&#33539;&#22260;&#21644;&#29609;&#23478;&#26399;&#26395;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#65292;&#36136;&#37327;&#25511;&#21046;&#21344;&#25454;&#20102;&#29983;&#20135;&#25104;&#26412;&#21644;&#20132;&#20184;&#39118;&#38505;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#12290;&#20943;&#23569;&#36825;&#31181;&#39118;&#38505;&#24182;&#23454;&#29616;&#29983;&#20135;&#23545;&#20110;&#35813;&#34892;&#19994;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#22312;&#21457;&#24067;&#21069;&#21518;&#20445;&#25345;&#29983;&#20135;&#25104;&#26412;&#30340;&#23454;&#38469;&#24615;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#39044;&#38450;&#24615;&#36136;&#37327;&#20445;&#35777;&#31574;&#30053;&#20197;&#21450;&#27979;&#35797;&#21644;&#25968;&#25454;&#20998;&#26512;&#33258;&#21160;&#21270;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SUPERNOVA&#65288;&#29992;&#20110;&#26032;&#39062;&#36719;&#20214;&#24322;&#24120;&#30340;&#27979;&#35797;&#36873;&#25321;&#21644;&#36890;&#29992;&#32570;&#38519;&#39044;&#38450;&#22312;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#30340;&#31995;&#32479;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36127;&#36131;
&lt;/p&gt;
&lt;p&gt;
Testing video games is an increasingly difficult task as traditional methods fail to scale with growing software systems. Manual testing is a very labor-intensive process, and therefore quickly becomes cost prohibitive. Using scripts for automated testing is affordable, however scripts are ineffective in non-deterministic environments, and knowing when to run each test is another problem altogether. The modern game's complexity, scope, and player expectations are rapidly increasing where quality control is a big portion of the production cost and delivery risk. Reducing this risk and making production happen is a big challenge for the industry currently. To keep production costs realistic up-to and after release, we are focusing on preventive quality assurance tactics alongside testing and data analysis automation. We present SUPERNOVA (Selection of tests and Universal defect Prevention in External Repositories for Novel Objective Verification of software Anomalies), a system responsib
&lt;/p&gt;</description></item><item><title>CoCoFL&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#20923;&#32467;&#21644;&#37327;&#21270;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#36866;&#24212;&#35774;&#22791;&#30340;&#24322;&#26500;&#36164;&#28304;&#65292;&#24182;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2203.05468</link><description>&lt;p&gt;
CoCoFL: &#22522;&#20110;&#37096;&#20998;&#31070;&#32463;&#32593;&#32476;&#20923;&#32467;&#21644;&#37327;&#21270;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CoCoFL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization. (arXiv:2203.05468v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05468
&lt;/p&gt;
&lt;p&gt;
CoCoFL&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#20923;&#32467;&#21644;&#37327;&#21270;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#36866;&#24212;&#35774;&#22791;&#30340;&#24322;&#26500;&#36164;&#28304;&#65292;&#24182;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#35774;&#22791;&#36890;&#24120;&#20855;&#26377;&#24322;&#26500;&#30340;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25152;&#26377;&#35774;&#22791;&#38656;&#35201;&#22312;&#30001;&#26381;&#21153;&#22120;&#35268;&#23450;&#30340;&#30456;&#21516;&#25130;&#27490;&#26085;&#26399;&#20043;&#21069;&#23436;&#25104;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21463;&#38480;&#35774;&#22791;&#19978;&#35757;&#32451;&#36739;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#23376;&#38598;&#65288;&#21363;&#36890;&#36807;&#21024;&#38500;&#31070;&#32463;&#20803;/&#28388;&#27874;&#22120;&#65289;&#26159;&#20302;&#25928;&#30340;&#65292;&#38459;&#27490;&#20102;&#36825;&#20123;&#35774;&#22791;&#23545;&#27169;&#22411;&#30340;&#26377;&#25928;&#36129;&#29486;&#12290;&#36825;&#23548;&#33268;&#20102;&#38024;&#23545;&#21463;&#38480;&#35774;&#22791;&#30340;&#21487;&#36798;&#20934;&#30830;&#24230;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35774;&#22791;&#20043;&#38388;&#23384;&#22312;&#31867;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;CoCoFL&#65292;&#35813;&#25216;&#26415;&#22312;&#25152;&#26377;&#35774;&#22791;&#19978;&#20445;&#25345;&#23436;&#25972;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#20026;&#20102;&#36866;&#24212;&#35774;&#22791;&#30340;&#24322;&#26500;&#36164;&#28304;&#65292;CoCoFL&#20923;&#32467;&#21644;&#37327;&#21270;&#36873;&#25321;&#30340;&#23618;&#65292;&#38477;&#20302;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#32780;&#20854;&#20182;&#23618;&#20173;&#28982;&#20197;&#20840;&#31934;&#24230;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Devices participating in federated learning (FL) typically have heterogeneous communication, computation, and memory resources. However, in synchronous FL, all devices need to finish training by the same deadline dictated by the server. Our results show that training a smaller subset of the neural network (NN) at constrained devices, i.e., dropping neurons/filters as proposed by state of the art, is inefficient, preventing these devices to make an effective contribution to the model. This causes unfairness w.r.t the achievable accuracies of constrained devices, especially in cases with a skewed distribution of class labels across devices. We present a novel FL technique, CoCoFL, which maintains the full NN structure on all devices. To adapt to the devices' heterogeneous resources, CoCoFL freezes and quantizes selected layers, reducing communication, computation, and memory requirements, whereas other layers are still trained in full precision, enabling to reach a high accuracy. Thereby
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#24577;&#21644;&#20132;&#20114;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#21152;&#36895;&#23545;&#22810;&#26679;&#21270;&#21644;&#23569;&#34987;&#30740;&#31350;&#30340;&#27169;&#24577;&#30340;&#25512;&#24191;&#12290; (arXiv:2203.01311v4 [cs.LG] UPDATED)</title><link>http://arxiv.org/abs/2203.01311</link><description>&lt;p&gt;
&#39640;&#27169;&#24577;&#22810;&#27169;&#24577;Transformer&#65306;&#37327;&#21270;&#27169;&#24577;&#19982;&#20132;&#20114;&#24322;&#36136;&#24615;&#20197;&#36827;&#34892;&#39640;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-Modality Multimodal Transformer: Quantifying Modality &amp; Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#24577;&#21644;&#20132;&#20114;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#21152;&#36895;&#23545;&#22810;&#26679;&#21270;&#21644;&#23569;&#34987;&#30740;&#31350;&#30340;&#27169;&#24577;&#30340;&#25512;&#24191;&#12290; (arXiv:2203.01311v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#20363;&#22914;&#20154;&#31867;&#29992;&#20110;&#20132;&#27969;&#30340;&#21475;&#35821;&#12289;&#25163;&#21183;&#21644;&#35821;&#29992;&#23398;&#65292;&#20197;&#21450;&#26426;&#22120;&#20154;&#19978;&#30340;&#21147;&#12289;&#26412;&#20307;&#24863;&#21644;&#35270;&#35273;&#20256;&#24863;&#22120;&#12290;&#34429;&#28982;&#22810;&#27169;&#24577;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#19968;&#23567;&#32452;&#27169;&#24577;&#65292;&#20027;&#35201;&#26159;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#12290;&#20026;&#20102;&#21152;&#36895;&#21521;&#22810;&#26679;&#21270;&#21644;&#23569;&#34987;&#30740;&#31350;&#30340;&#27169;&#24577;&#25512;&#24191;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#28041;&#21450;&#19968;&#20010;&#22823;&#37327;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#30001;&#20110;&#20026;&#27599;&#20010;&#26032;&#27169;&#24577;&#28155;&#21152;&#26032;&#27169;&#22411;&#21464;&#24471;&#20195;&#20215;&#36807;&#39640;&#65292;&#20851;&#38190;&#30340;&#25216;&#26415;&#25361;&#25112;&#26159;&#24322;&#36136;&#24615;&#37327;&#21270;&#65306;&#25105;&#20204;&#22914;&#20309;&#34913;&#37327;&#21738;&#20123;&#27169;&#24577;&#32534;&#30721;&#20102;&#31867;&#20284;&#30340;&#20449;&#24687;&#21644;&#20132;&#20114;&#65292;&#20197;&#20415;&#20801;&#35768;&#19982;&#20808;&#21069;&#30340;&#27169;&#24577;&#20849;&#20139;&#21442;&#25968;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#24322;&#36136;&#24615;&#65306;(1)&#27169;&#24577;&#24322;&#36136;&#24615;&#30740;&#31350;&#20102;&#20004;&#20010;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalitie
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#20449;&#20219;&#24230;&#37327;&#26469;&#25903;&#25345;&#22810;&#30446;&#26631;&#21644;&#22810;&#25968;&#25454;&#26469;&#28304;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20449;&#20219;&#22686;&#30410;&#20316;&#20026;&#30446;&#26631;&#20043;&#19968;&#65292;&#23558;&#22810;&#20445;&#30495;&#24230;&#32771;&#34385;&#22312;&#20869;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#22810;&#30446;&#26631;&#21644;&#22810;&#25968;&#25454;&#26469;&#28304;&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;&#27604;&#36739;&#20102;&#20004;&#31181;&#20248;&#21270;&#26041;&#27861;&#65306;&#21516;&#26102;&#36873;&#25321;&#36755;&#20837;&#21442;&#25968;&#21644;&#20445;&#30495;&#24230;&#26631;&#20934;&#30340;&#25972;&#20307;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#24085;&#32047;&#25176;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.13901</link><description>&lt;p&gt;
&#21033;&#29992;&#20449;&#20219;&#36827;&#34892;&#22810;&#30446;&#26631;&#21644;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Trust for Joint Multi-Objective and Multi-Fidelity Optimization. (arXiv:2112.13901v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13901
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#20449;&#20219;&#24230;&#37327;&#26469;&#25903;&#25345;&#22810;&#30446;&#26631;&#21644;&#22810;&#25968;&#25454;&#26469;&#28304;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20449;&#20219;&#22686;&#30410;&#20316;&#20026;&#30446;&#26631;&#20043;&#19968;&#65292;&#23558;&#22810;&#20445;&#30495;&#24230;&#32771;&#34385;&#22312;&#20869;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#22810;&#30446;&#26631;&#21644;&#22810;&#25968;&#25454;&#26469;&#28304;&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;&#27604;&#36739;&#20102;&#20004;&#31181;&#20248;&#21270;&#26041;&#27861;&#65306;&#21516;&#26102;&#36873;&#25321;&#36755;&#20837;&#21442;&#25968;&#21644;&#20445;&#30495;&#24230;&#26631;&#20934;&#30340;&#25972;&#20307;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#24085;&#32047;&#25176;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#39640;&#25928;&#35780;&#20272;&#20195;&#20215;&#26114;&#36149;&#31995;&#32479;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#22810;&#30446;&#26631;&#21644;&#22810;&#20445;&#30495;&#24230;&#65288;MOMF&#65289;&#20248;&#21270;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#20248;&#21270;&#26041;&#27861;&#22312;&#22810;&#32500;&#24230;&#20248;&#21270;&#19968;&#20010;&#25110;&#22810;&#20010;&#30446;&#26631;&#26102;&#24120;&#24120;&#36935;&#21040;&#26497;&#39640;&#30340;&#25104;&#26412;&#12290;&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#25104;&#26412;&#36739;&#20302;&#30340;&#20449;&#24687;&#26469;&#28304;&#65288;&#22914;&#20302;&#20998;&#36776;&#29575;&#27169;&#25311;&#65289;&#25552;&#20379;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20004;&#31181;&#31574;&#30053;&#25972;&#21512;&#36215;&#26469;&#21364;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#20449;&#20219;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#21644;&#25968;&#25454;&#26469;&#28304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20462;&#25913;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#31574;&#30053;&#65292;&#23558;&#27599;&#27425;&#35780;&#20272;&#25104;&#26412;&#30340;&#20449;&#20219;&#22686;&#30410;&#20316;&#20026;&#19968;&#20010;&#30446;&#26631;&#32435;&#20837;&#24085;&#32047;&#25176;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;MOMF&#20248;&#21270;&#24182;&#38477;&#20302;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;MOMF&#20248;&#21270;&#26041;&#27861;&#65306;&#19968;&#31181;&#25972;&#20307;&#26041;&#27861;&#26082;&#36873;&#25321;&#36755;&#20837;&#21442;&#25968;&#21448;&#36873;&#25321;&#20445;&#30495;&#24230;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of efficient optimization of expensive-to-evaluate systems, this paper investigates a novel approach to Bayesian multi-objective and multi-fidelity (MOMF) optimization. Traditional optimization methods, while effective, often encounter prohibitively high costs in multi-dimensional optimizations of one or more objectives. Multi-fidelity approaches offer potential remedies by utilizing multiple, less costly information sources, such as low-resolution simulations. However, integrating these two strategies presents a significant challenge. We suggest the innovative use of a trust metric to support simultaneous optimization of multiple objectives and data sources. Our method modifies a multi-objective optimization policy to incorporate the trust gain per evaluation cost as one objective in a Pareto optimization problem, enabling simultaneous MOMF at lower costs. We present and compare two MOMF optimization methods: a holistic approach selecting both the input parameters and t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#20013;&#23545;&#26799;&#24230;&#36827;&#34892;&#38543;&#26426;&#31232;&#30095;&#21270;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#20010;&#35843;&#25972;&#25910;&#25947;&#30028;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#22122;&#22768;&#21344;&#20027;&#23548;&#22320;&#20301;&#26102;&#33719;&#24471;&#26356;&#23567;&#30340;&#19978;&#30028;&#12290;&#36825;&#20010;&#35266;&#23519;&#34920;&#26126;&#65292;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#26377;&#29305;&#27530;&#30340;&#26799;&#24230;&#21387;&#32553;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2112.00845</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#31232;&#30095;&#21270;&#26799;&#24230;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Differentially Private SGD via Randomly Sparsified Gradients. (arXiv:2112.00845v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.00845
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#20013;&#23545;&#26799;&#24230;&#36827;&#34892;&#38543;&#26426;&#31232;&#30095;&#21270;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#20010;&#35843;&#25972;&#25910;&#25947;&#30028;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#22122;&#22768;&#21344;&#20027;&#23548;&#22320;&#20301;&#26102;&#33719;&#24471;&#26356;&#23567;&#30340;&#19978;&#30028;&#12290;&#36825;&#20010;&#35266;&#23519;&#34920;&#26126;&#65292;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#26377;&#29305;&#27530;&#30340;&#26799;&#24230;&#21387;&#32553;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;DP-SGD&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#25552;&#20379;&#20005;&#26684;&#23450;&#20041;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#35813;&#26041;&#27861;&#35201;&#27714;&#23545;&#26799;&#24230;&#36827;&#34892;&#21098;&#20999;&#20197;&#38480;&#21046;&#20010;&#20307;&#26799;&#24230;&#30340;&#26368;&#22823;&#33539;&#25968;&#65292;&#24182;&#28155;&#21152;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#22122;&#22768;&#12290;&#36890;&#36807;&#22312;&#38750;&#20984;&#29615;&#22659;&#20013;&#20998;&#26512;DP-SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21098;&#20999;&#21644;&#28155;&#21152;&#22122;&#22768;&#20043;&#21069;&#23545;&#26799;&#24230;&#36827;&#34892;&#38543;&#26426;&#31232;&#30095;&#21270;&#21487;&#20197;&#35843;&#25972;&#25910;&#25947;&#30028;&#30340;&#20869;&#37096;&#25104;&#20998;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#22312;&#22122;&#22768;&#21344;&#20027;&#23548;&#22320;&#20301;&#26102;&#23548;&#33268;&#26356;&#23567;&#30340;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26435;&#34913;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#32780;&#21487;&#33021;&#26159;DP-SGD&#30340;&#19968;&#31181;&#29420;&#29305;&#23646;&#24615;&#65292;&#22240;&#20026;&#21462;&#28040;&#22122;&#22768;&#21270;&#25110;&#26799;&#24230;&#21098;&#20999;&#37117;&#20250;&#28040;&#38500;&#30028;&#38480;&#20013;&#30340;&#26435;&#34913;&#12290;&#36825;&#19968;&#35266;&#23519;&#26159;&#26377;&#25351;&#31034;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#24847;&#21619;&#30528;DP-SGD&#20855;&#26377;&#29305;&#27530;&#30340;&#20869;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#36827;&#34892;&#65288;&#29978;&#33267;&#20165;&#20165;&#26159;&#38543;&#26426;&#30340;&#65289;&#26799;&#24230;&#21387;&#32553;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#35266;&#23519;&#24182;&#21033;&#29992;&#23427;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially private stochastic gradient descent (DP-SGD) has been widely adopted in deep learning to provide rigorously defined privacy, which requires gradient clipping to bound the maximum norm of individual gradients and additive isotropic Gaussian noise. With analysis of the convergence rate of DP-SGD in a non-convex setting, we identify that randomly sparsifying gradients before clipping and noisification adjusts a trade-off between internal components of the convergence bound and leads to a smaller upper bound when the noise is dominant. Additionally, our theoretical analysis and empirical evaluations show that the trade-off is not trivial but possibly a unique property of DP-SGD, as either canceling noisification or gradient clipping eliminates the trade-off in the bound. This observation is indicative, as it implies DP-SGD has special inherent room for (even simply random) gradient compression. To verify the observation and utilize it, we propose an efficient and lightweight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#38408;&#20540;&#30005;&#36335;&#21644;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#20027;&#35201;&#32467;&#26524;&#35777;&#26126;&#65292;&#20219;&#20309;&#38408;&#20540;&#30005;&#36335;$C$&#30340;&#22823;&#23567;&#12289;&#28145;&#24230;&#12289;&#33021;&#37327;&#21644;&#26435;&#37325;&#28385;&#36275;$\log(rk(M_C)) \le ed (\log s + \log w + \log n)$&#12290;&#36825;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#23376;&#32447;&#24615;&#28145;&#24230;&#30340;&#38408;&#20540;&#30005;&#36335;&#30340;&#22823;&#23567;&#20063;&#23384;&#22312;&#25351;&#25968;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2107.00223</link><description>&lt;p&gt;
&#38408;&#20540;&#30005;&#36335;&#21644;&#23376;&#32447;&#24615;&#28145;&#24230;&#19982;&#33021;&#37327;&#30340;&#25351;&#25968;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Exponential Lower Bounds for Threshold Circuits of Sub-Linear Depth and Energy. (arXiv:2107.00223v2 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.00223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#38408;&#20540;&#30005;&#36335;&#21644;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#20027;&#35201;&#32467;&#26524;&#35777;&#26126;&#65292;&#20219;&#20309;&#38408;&#20540;&#30005;&#36335;$C$&#30340;&#22823;&#23567;&#12289;&#28145;&#24230;&#12289;&#33021;&#37327;&#21644;&#26435;&#37325;&#28385;&#36275;$\log(rk(M_C)) \le ed (\log s + \log w + \log n)$&#12290;&#36825;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#23376;&#32447;&#24615;&#28145;&#24230;&#30340;&#38408;&#20540;&#30005;&#36335;&#30340;&#22823;&#23567;&#20063;&#23384;&#22312;&#25351;&#25968;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38408;&#20540;&#30005;&#36335;&#21644;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20197;&#20197;&#19979;&#22235;&#20010;&#22797;&#26434;&#24230;&#24230;&#37327;&#20026;&#22522;&#30784;&#65306;&#22823;&#23567;&#65288;&#38376;&#25968;&#37327;&#65289;&#12289;&#28145;&#24230;&#12289;&#26435;&#37325;&#21644;&#33021;&#37327;&#12290;&#20854;&#20013;&#30005;&#36335;&#30340;&#33021;&#37327;&#22797;&#26434;&#24230;&#34913;&#37327;&#20102;&#23427;&#20204;&#35745;&#31639;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#34987;&#23450;&#20041;&#20026;&#22312;&#25152;&#26377;&#36755;&#20837;&#20998;&#37197;&#20013;&#20135;&#29983;&#38750;&#38646;&#20540;&#30340;&#38376;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#22823;&#23567;&#20026;$s$&#65292;&#28145;&#24230;&#20026;$d$&#65292;&#33021;&#37327;&#20026;$e$&#65292;&#26435;&#37325;&#20026;$w$&#30340;&#38408;&#20540;&#30005;&#36335;$C$&#28385;&#36275;$\log(rk(M_C)) \le ed (\log s + \log w + \log n)$&#65292;&#20854;&#20013;$rk(M_C)$&#26159;$C$&#35745;&#31639;&#30340;&#19968;&#20010;$2n$&#21464;&#37327;&#24067;&#23572;&#20989;&#25968;&#30340;&#36890;&#20449;&#30697;&#38453;$M_C$&#30340;&#31209;&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#38408;&#20540;&#30005;&#36335;$C$&#21482;&#33021;&#35745;&#31639;&#20855;&#26377;&#36890;&#20449;&#30697;&#38453;&#30340;&#31209;&#21463;&#21040;$w$&#21644;$d,e$&#30340;&#23545;&#25968;&#22240;&#23376;&#21644;$s$&#30340;&#32447;&#24615;&#22240;&#23376;&#30340;&#38480;&#21046;&#30340;&#24067;&#23572;&#20989;&#25968;&#12290;&#36825;&#24847;&#21619;&#30528;&#29978;&#33267;&#23376;&#32447;&#24615;&#28145;&#24230;&#30340;&#38408;&#20540;&#30005;&#36335;&#30340;&#22823;&#23567;&#23384;&#22312;&#25351;&#25968;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate computational power of threshold circuits and other theoretical models of neural networks in terms of the following four complexity measures: size (the number of gates), depth, weight and energy. Here the energy complexity of a circuit measures sparsity of their computation, and is defined as the maximum number of gates outputting non-zero values taken over all the input assignments. As our main result, we prove that any threshold circuit $C$ of size $s$, depth $d$, energy $e$ and weight $w$ satisfies $\log (rk(M_C)) \le ed (\log s + \log w + \log n)$, where $rk(M_C)$ is the rank of the communication matrix $M_C$ of a $2n$-variable Boolean function that $C$ computes. Thus, such a threshold circuit $C$ is able to compute only a Boolean function of which communication matrix has rank bounded by a product of logarithmic factors of $s,w$ and linear factors of $d,e$. This implies an exponential lower bound on the size of even sublinear-depth threshold circuit i
&lt;/p&gt;</description></item><item><title>GeoT&#26159;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#38752;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#21644;&#21270;&#23398;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#20998;&#23376;&#22270;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#35757;&#32451;&#30446;&#26631;&#30456;&#20851;&#32852;&#30340;&#27880;&#24847;&#21147;&#22270;&#12290;&#19982;&#22522;&#20110;MPNN&#27169;&#22411;&#30456;&#27604;&#65292;GeoT&#20855;&#26377;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.15516</link><description>&lt;p&gt;
GeoT:&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#30340;Transformer&#27169;&#22411;&#29992;&#20110;&#21487;&#38752;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#21644;&#21270;&#23398;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GeoT: A Geometry-aware Transformer for Reliable Molecular Property Prediction and Chemically Interpretable Representation Learning. (arXiv:2106.15516v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.15516
&lt;/p&gt;
&lt;p&gt;
GeoT&#26159;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#38752;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#21644;&#21270;&#23398;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#20998;&#23376;&#22270;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#35757;&#32451;&#30446;&#26631;&#30456;&#20851;&#32852;&#30340;&#27880;&#24847;&#21147;&#22270;&#12290;&#19982;&#22522;&#20110;MPNN&#27169;&#22411;&#30456;&#27604;&#65292;GeoT&#20855;&#26377;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#21508;&#31181;&#21270;&#23398;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#20998;&#23376;&#32467;&#26500;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#23548;&#33268;&#34920;&#31034;&#19981;&#22815;&#30452;&#35266;&#12290;&#27492;&#22806;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#23616;&#38480;&#20110;&#20174;&#21270;&#23398;&#35282;&#24230;&#35299;&#37322;&#23454;&#39564;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#20960;&#20309;&#24863;&#30693;Transformer&#65288;GeoT&#65289;&#12290;GeoT&#36890;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#20998;&#23376;&#22270;&#32467;&#26500;&#65292;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#35299;&#37322;&#24615;&#20197;&#21450;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;GeoT&#33021;&#22815;&#29983;&#25104;&#19982;&#35757;&#32451;&#30446;&#26631;&#30456;&#20851;&#32852;&#30340;&#21407;&#23376;&#38388;&#20851;&#31995;&#30340;&#27880;&#24847;&#21147;&#22270;&#12290;&#27492;&#22806;&#65292;GeoT&#23637;&#31034;&#20102;&#19982;&#22522;&#20110;MPNN&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#38477;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, molecular representation learning has emerged as a key area of focus in various chemical tasks. However, many existing models fail to fully consider the geometric information of molecular structures, resulting in less intuitive representations. Moreover, the widely used message-passing mechanism is limited to provide the interpretation of experimental results from a chemical perspective. To address these challenges, we introduce a novel Transformer-based framework for molecular representation learning, named the Geometry-aware Transformer (GeoT). GeoT learns molecular graph structures through attention-based mechanisms specifically designed to offer reliable interpretability, as well as molecular property prediction. Consequently, GeoT can generate attention maps of interatomic relationships associated with training objectives. In addition, GeoT demonstrates comparable performance to MPNN-based models while achieving reduced computational complexity. Our comprehensive 
&lt;/p&gt;</description></item><item><title>Open-LACU&#26159;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#19981;&#21516;&#30340;&#32972;&#26223;&#21644;&#26410;&#30693;&#31867;&#21035;&#26469;&#25552;&#39640;&#35757;&#32451;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2002.01368</link><description>&lt;p&gt;
&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#25193;&#23637;&#31867;&#21035;&#30340;&#24320;&#25918;&#38598;&#23398;&#20064;&#65288;Open-LACU&#65289;
&lt;/p&gt;
&lt;p&gt;
Open-set learning with augmented category by exploiting unlabeled data (Open-LACU). (arXiv:2002.01368v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.01368
&lt;/p&gt;
&lt;p&gt;
Open-LACU&#26159;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#19981;&#21516;&#30340;&#32972;&#26223;&#21644;&#26410;&#30693;&#31867;&#21035;&#26469;&#25552;&#39640;&#35757;&#32451;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#24335;&#35782;&#21035;&#65288;OSR&#65289;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#23581;&#35797;&#20197;&#21512;&#25104;&#21333;&#20010;&#35757;&#32451;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#27599;&#27425;&#23581;&#35797;&#37117;&#36829;&#21453;&#20102;&#24320;&#25918;&#38598;&#23450;&#20041;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#22312;&#26410;&#26631;&#35760;&#30340;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#26032;&#39062;&#30340;&#31867;&#21035;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#25512;&#24191;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#35266;&#23519;&#21040;&#26032;&#39062;&#31867;&#21035;&#30340;&#32972;&#26223;&#31867;&#21035;&#21644;&#26410;&#35266;&#23519;&#21040;&#26032;&#39062;&#31867;&#21035;&#30340;&#26410;&#30693;&#31867;&#21035;&#12290;&#36890;&#36807;&#20998;&#31867;&#36825;&#20004;&#31181;&#26032;&#39062;&#31867;&#21035;&#30340;&#26041;&#24335;&#65292;Open-LACU&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#24182;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several efforts have been made to synthesize semi-supervised learning (SSL) and open set recognition (OSR) within a single training policy. However, each attempt violated the definition of an open set by incorporating novel categories within the unlabeled training set. Although such \textit{observed} novel categories are undoubtedly prevalent in application-grade datasets, they should not be conflated with the OSR-defined \textit{unobserved} novel categories, which only emerge during testing. This study proposes a new learning policy wherein classifiers generalize between observed and unobserved novel categories. Specifically, our open-set learning with augmented category by exploiting unlabeled data (Open-LACU) policy defines a background category for observed novel categories and an unknown category for unobserved novel categories. By separating these novel category types, Open-LACU promotes cost-efficient training by eliminating the need to label every category and ensures safe clas
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#23545;&#31216;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31867;&#38750;&#23545;&#31216;&#23567;&#27874;&#65292;&#23427;&#32479;&#19968;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#22270;&#24418;&#25955;&#23556;&#26550;&#26500;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20026;&#22270;&#24418;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/1911.06253</link><description>&lt;p&gt;
&#20102;&#35299;&#20855;&#26377;&#38750;&#23545;&#31216;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Understanding Graph Neural Networks with Asymmetric Geometric Scattering Transforms. (arXiv:1911.06253v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.06253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#23545;&#31216;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31867;&#38750;&#23545;&#31216;&#23567;&#27874;&#65292;&#23427;&#32479;&#19968;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#22270;&#24418;&#25955;&#23556;&#26550;&#26500;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20026;&#22270;&#24418;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25955;&#23556;&#21464;&#25442;&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20316;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#26377;&#20960;&#31687;&#24037;&#20316;&#24341;&#20837;&#20102;&#25955;&#23556;&#21464;&#25442;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#35774;&#32622;&#65288;&#22914;&#22270;&#24418;&#65289;&#20013;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#36825;&#20123;&#26500;&#36896;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#38750;&#24120;&#19968;&#33324;&#30340;&#38750;&#23545;&#31216;&#23567;&#27874;&#31867;&#30340;&#22270;&#24418;&#31383;&#21475;&#21270;&#21644;&#38750;&#31383;&#21475;&#21270;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#38750;&#23545;&#31216;&#22270;&#24418;&#25955;&#23556;&#21464;&#25442;&#19982;&#23545;&#31216;&#25955;&#23556;&#21464;&#25442;&#26377;&#35768;&#22810;&#30456;&#21516;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#30340;&#26500;&#36896;&#32479;&#19968;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#22270;&#24418;&#25955;&#23556;&#26550;&#26500;&#30340;&#24050;&#30693;&#29702;&#35770;&#32467;&#26524;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#22823;&#37327;&#24102;&#26377;&#21487;&#35777;&#26126;&#31283;&#23450;&#24615;&#21644;&#19981;&#21464;&#24615;&#20445;&#35777;&#30340;&#32593;&#32476;&#65292;&#26377;&#21161;&#20110;&#24357;&#21512;&#20960;&#20309;&#25955;&#23556;&#21644;&#20854;&#20182;&#22270;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#26410;&#26469;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20026;&#22270;&#24418;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scattering transform is a multilayered wavelet-based deep learning architecture that acts as a model of convolutional neural networks. Recently, several works have introduced generalizations of the scattering transform for non-Euclidean settings such as graphs. Our work builds upon these constructions by introducing windowed and non-windowed geometric scattering transforms for graphs based upon a very general class of asymmetric wavelets. We show that these asymmetric graph scattering transforms have many of the same theoretical guarantees as their symmetric counterparts. As a result, the proposed construction unifies and extends known theoretical results for many of the existing graph scattering architectures. In doing so, this work helps bridge the gap between geometric scattering and other graph neural networks by introducing a large family of networks with provable stability and invariance guarantees. These results lay the groundwork for future deep learning architectures for g
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#22810;&#21464;&#37327;&#23545;&#31216;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#22312;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26465;&#20214;&#65292;&#33021;&#22815;&#22312;&#23646;&#24615;&#25968;&#37327;&#12289;&#22522;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#19981;&#21516;&#32452;&#21512;&#19979;&#20445;&#25345;&#36825;&#19968;&#24230;&#37327;&#30340;&#33391;&#22909;&#36136;&#37327;&#65292;&#20026;&#38477;&#32500;&#36807;&#31243;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/1709.08730</link><description>&lt;p&gt;
&#20998;&#26512;&#22810;&#21464;&#37327;&#23545;&#31216;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#22312;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Understanding a Version of Multivariate Symmetric Uncertainty to assist in Feature Selection. (arXiv:1709.08730v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1709.08730
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#22810;&#21464;&#37327;&#23545;&#31216;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#22312;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26465;&#20214;&#65292;&#33021;&#22815;&#22312;&#23646;&#24615;&#25968;&#37327;&#12289;&#22522;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#19981;&#21516;&#32452;&#21512;&#19979;&#20445;&#25345;&#36825;&#19968;&#24230;&#37327;&#30340;&#33391;&#22909;&#36136;&#37327;&#65292;&#20026;&#38477;&#32500;&#36807;&#31243;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#27169;&#25311;&#25216;&#26415;&#22312;&#20449;&#24687;&#21644;&#38750;&#20449;&#24687;&#38543;&#26426;&#29983;&#25104;&#30340;&#29305;&#24449;&#28151;&#21512;&#20013;&#20998;&#26512;&#20102;&#22810;&#21464;&#37327;&#23545;&#31216;&#19981;&#30830;&#23450;&#24615;&#65288;MSU&#65289;&#24230;&#37327;&#30340;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#23646;&#24615;&#25968;&#37327;&#12289;&#22522;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#23545;MSU&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26465;&#20214;&#65292;&#33021;&#22815;&#22312;&#36825;&#19977;&#20010;&#22240;&#32032;&#30340;&#19981;&#21516;&#32452;&#21512;&#19979;&#20445;&#25345;MSU&#30340;&#33391;&#22909;&#36136;&#37327;&#65292;&#20026;&#39537;&#21160;&#38477;&#32500;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#26032;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical simulation techniques under various mixes of informative and non-informative randomly generated features. Experiments show how the number of attributes, their cardinalities, and the sample size affect the MSU. We discovered a condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction.
&lt;/p&gt;</description></item></channel></rss>